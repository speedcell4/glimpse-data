{
  "https://openreview.net/forum?id=YVPb6tyRJu": {
    "title": "FREED++: Improving RL Agents for Fragment-Based Molecule Generation by Thorough Reproduction",
    "volume": "main",
    "abstract": "A rational design of new therapeutic drugs aims to find a molecular structure with desired biological functionality, e.g., an ability to activate or suppress a specific protein via binding to it. Molecular docking is a common technique for evaluating protein-molecule interactions. Recently, Reinforcement Learning (RL) has emerged as a promising approach to generating molecules with the docking score (DS) as a reward. In this work, we reproduce, scrutinize and improve the recent RL model for molecule generation called FREED (Yang et al., 2021). Extensive evaluation of the proposed method reveals several limitations and challenges despite the outstanding results reported for three target proteins. Our contributions include fixing numerous implementation bugs and simplifying the model while increasing its quality, significantly extending experiments, and conducting an accurate comparison with current state-of-the-art methods for protein-conditioned molecule generation. We show that the resulting fixed model is capable of producing molecules with superior docking scores compared to alternative approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Telepov",
      "Artem Tsypin",
      "Kuzma Khrabrov",
      "Sergey Yakukhnov",
      "Pavel Strashnov",
      "Petr Zhilyaev",
      "Egor Rumiantsev",
      "Daniel Ezhov",
      "Manvel Avetisian",
      "Olga Popova",
      "Artur Kadurin"
    ]
  },
  "https://openreview.net/forum?id=Ls1E16bTj8": {
    "title": "In search of projectively equivariant networks",
    "volume": "main",
    "abstract": "Equivariance of linear neural network layers is well studied. In this work, we relax the equivariance condition to only be true in a projective sense. Hereby, we introduce the topic of projective equivariance to the machine learning audience. We theoretically study the relation of projectively and linearly equivariant linear layers. We find that in some important cases, surprisingly, the two types of layers coincide. We also propose a way to construct a projectively equivariant neural network, which boils down to building a standard equivariant network where the linear group representations acting on each intermediate feature space are lifts of projective group representations. Projective equivariance is showcased in two simple experiments. Code for the experiments is provided in the supplementary material",
    "checked": true,
    "id": "87e4e0723dea55ad3779ffe9a047f495990ddd1c",
    "semantic_title": "in search of projectively equivariant networks",
    "citation_count": 0,
    "authors": [
      "Georg Bökman",
      "Axel Flinth",
      "Fredrik Kahl"
    ]
  },
  "https://openreview.net/forum?id=2wecNCpZ7Y": {
    "title": "Improving Native CNN Robustness with Filter Frequency Regularization",
    "volume": "main",
    "abstract": "Neural networks tend to overfit the training distribution and perform poorly on out-of-distribution data. A conceptually simple solution lies in adversarial training, which introduces worst-case perturbations into the training data and thus improves model generalization to some extent. However, it is only one ingredient towards generally more robust models and requires knowledge about the potential attacks or inference time data corruptions during model training. This paper focuses on the native robustness of models that can learn robust behavior directly from conventional training data without out-of-distribution examples. To this end, we study the frequencies in learned convolution filters. Clean-trained models often prioritize high-frequency information, whereas adversarial training enforces models to shift the focus to low-frequency details during training. By mimicking this behavior through frequency regularization in learned convolution weights, we achieve improved native robustness to adversarial attacks, common corruptions, and other out-of-distribution tests. Additionally, this method leads to more favorable shifts in decision-making towards low-frequency information, such as shapes, which inherently aligns more closely with human vision",
    "checked": true,
    "id": "f2b15de1db12bfaeab8365202e03f364caa1dee2",
    "semantic_title": "improving native cnn robustness with filter frequency regularization",
    "citation_count": 9,
    "authors": [
      "Jovita Lukasik",
      "Paul Gavrikov",
      "Janis Keuper",
      "Margret Keuper"
    ]
  },
  "https://openreview.net/forum?id=wzzrs5QH5k": {
    "title": "Resmax: An Alternative Soft-Greedy Operator for Reinforcement Learning",
    "volume": "main",
    "abstract": "Soft-greedy operators, namely $\\varepsilon$-greedy and softmax, remain a common choice to induce a basic level of exploration for action-value methods in reinforcement learning. These operators, however, have a few critical limitations. In this work, we investigate a simple soft-greedy operator, which we call resmax, that takes actions proportionally to their max action gap: the residual to the estimated maximal value. It is simple to use and ensures coverage of the state-space like $\\varepsilon$-greedy, but focuses exploration more on potentially promising actions like softmax. Further, it does not concentrate probability as quickly as softmax, and so better avoids overemphasizing sub-optimal actions that appear high-valued during learning. Additionally, we prove it is a non-expansion for any fixed exploration hyperparameter, unlike the softmax policy which requires a state-action specific temperature to obtain a non-expansion (called mellowmax). We empirically validate that resmax is comparable to or outperforms $\\varepsilon$-greedy and softmax across a variety of environments in tabular and deep RL",
    "checked": true,
    "id": "8321641f955713efa53caba33b6d7cbf87964f38",
    "semantic_title": "resmax: an alternative soft-greedy operator for reinforcement learning",
    "citation_count": 2,
    "authors": [
      "Erfan Miahi",
      "Revan MacQueen",
      "Alex Ayoub",
      "Abbas Masoumzadeh",
      "Martha White"
    ]
  },
  "https://openreview.net/forum?id=SnPEhMyuYX": {
    "title": "Privacy Budget Tailoring in Private Data Analysis",
    "volume": "main",
    "abstract": "We consider the problem of learning differentially private linear and logistic regression models that do not exhibit disparate performance for minority groups in the data. Small-sized datasets pose a challenging regime for differential privacy; that is, satisfying differential privacy while learning models from data can lead to models with worse accuracy for minority---in size---subgroups. To address this challenge, inspired by Abowd & Schmutte (2018), we propose: (i) to systematically tailor the privacy budget to the different groups, (ii) use linear optimization oracles in a grid to optimize Lagrangian objectives that correspond to fair learning and optimization. We present efficient differentially private algorithms for linear and logistic regression subject to fairness constraints (e.g., bounded group loss) that allocate the privacy budget based on the private standard error of each subgroup in the data. Consequently, the formulation reduces the amount of noise added to these groups, which leads to more accurate models for such groups. We validate the proposed, group-aware budget allocation, method on synthetic and real-world datasets where we show significant reductions in prediction error for the smallest groups, while still preserving sufficient privacy to protect the minority group from re-identification attacks. In addition, we provide sample complexity lower bounds for our problem formulation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Alabi",
      "Chris Wiggins"
    ]
  },
  "https://openreview.net/forum?id=CviCLt44Em": {
    "title": "Smoothed Differential Privacy",
    "volume": "main",
    "abstract": "Differential privacy (DP) is a widely-accepted and widely-applied notion of privacy based on worst-case analysis. Often, DP classifies most mechanisms without additive noise as non-private (Dwork et al., 2014). Thus, additive noises are added to improve privacy (to achieve DP). However, in many real-world applications, adding additive noise is undesirable (Bagdasaryan et al., 2019) and sometimes prohibited (Liu et al., 2020). In this paper, we propose a natural extension of DP following the worst average-case idea behind the celebrated smoothed analysis (Spielman & Teng, May 2004). Our notion, smoothed DP, can effectively measure the privacy leakage of mechanisms without additive noises under realistic settings. We prove that any discrete mechanism with sampling procedures is more private than what DP predicts, while many continuous mechanisms with sampling procedures are still non-private under smoothed DP. In addition, we prove several desirable properties of smoothed DP, including composition, robustness to post-processing, and distribution reduction. Based on those properties, we propose an efficient algorithm to calculate the privacy parameters for smoothed DP. Experimentally, we verify that, according to smoothed DP, the discrete sampling mechanisms are private in real-world elections, and some discrete neural networks can be private without adding any additive noise. We believe that these results contribute to the theoretical foundation of realistic privacy measures beyond worst-case analysis",
    "checked": true,
    "id": "a2beb939daba8c6a835accb98afe1d79990e7aae",
    "semantic_title": "smoothed differential privacy",
    "citation_count": 0,
    "authors": [
      "Ao Liu",
      "Yu-Xiang Wang",
      "Lirong Xia"
    ]
  },
  "https://openreview.net/forum?id=sY75NqDRk1": {
    "title": "Distributed Architecture Search Over Heterogeneous Distributions",
    "volume": "main",
    "abstract": "Federated learning (FL) is an efficient learning framework that assists distributed machine learning when data cannot be shared with a centralized server. Recent advancements in FL use predefined architecture-based learning for all clients. However, given that clients' data are invisible to the server and data distributions are non-identical across clients, a predefined architecture discovered in a centralized setting may not be an optimal solution for all the clients in FL. Motivated by this challenge, we introduce SPIDER, an algorithmic framework that aims to Search PersonalIzed neural architecture for feDERated learning. SPIDER is designed based on two unique features: (1) alternately optimizing one architecture-homogeneous global model in a generic FL manner and architecture-heterogeneous local models that are connected to the global model by weight-sharing-based regularization, (2) achieving architecture-heterogeneous local models by a perturbation-based neural architecture search method. Experimental results demonstrate superior prediction performance compared with other state-of-the-art personalization methods",
    "checked": true,
    "id": "e3d4c7af9fe350ec99e77aff9c1e729da6363c04",
    "semantic_title": "distributed architecture search over heterogeneous distributions",
    "citation_count": 0,
    "authors": [
      "Erum Mushtaq",
      "Chaoyang He",
      "Jie Ding",
      "Salman Avestimehr"
    ]
  },
  "https://openreview.net/forum?id=P9haooN9v2": {
    "title": "DreamEdit: Subject-driven Image Editing",
    "volume": "main",
    "abstract": "Subject-driven image generation aims at generating images containing customized subjects, which has recently drawn enormous attention from the research community. Nevertheless, the previous works cannot precisely control the background and position of the target subject. In this work, we aspire to fill the void of the existing subject-driven generation tasks. To this end, we propose two novel subject-driven editing sub-tasks, i.e., Subject Replacement and Subject Addition. The new tasks are challenging in multiple aspects: replacing a subject with a customized one can totally change its shape, texture, and color, while adding a target subject to a designated position in a provided scene necessitates a rational context-aware posture of the subject. To conquer these two novel tasks, we first manually curate a new dataset called DreamEditBench containing 22 different types of subjects, and 440 source images, which cover diverse scenarios with different difficulty levels. We plan to host DreamEditBench as a platform and hire trained evaluators for standardized human evaluation. We also devise an innovative method DreamEditor to resolve these tasks by performing iterative generation, which enables a smooth adaptation to the customized subject. In this project, we conduct automatic and human evaluations to understand the performance of our DreamEditor and baselines on DreamEditBench. We found that the new tasks are challenging for the existing models. For Subject Replacement, we found that the existing models are particularly sensitive to the shape and color of the original subject. When the original subject and the customized subject are highly different, the model failure rate will dramatically increase. For Subject Addition, we found that the existing models cannot easily blend the customized subjects into the background smoothly, which causes noticeable artifacts in the generated image. We hope that DreamEditBench can become a standardized platform to enable future investigations towards building more controllable subject-driven image editing. Our project and benchmark homepage is https://dreameditbenchteam.github.io/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianle Li",
      "Max Ku",
      "Cong Wei",
      "Wenhu Chen"
    ]
  },
  "https://openreview.net/forum?id=4uflhObpcp": {
    "title": "UnIVAL: Unified Model for Image, Video, Audio and Language Tasks",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have made the ambitious quest for generalist agents significantly far from being a fantasy. A key hurdle for building such general models is the diversity and heterogeneity of tasks and modalities. A promising solution is unification, allowing the support of a myriad of tasks and modalities within one unified framework. While few large models (e.g., Flamingo (Alayrac et al. 2022)), trained on massive datasets, can support more than two modalities, current small to mid-scale unified models are still limited to 2 modalities, usually image-text or video-text. The question that we ask is: is it possible to build efficiently a unified model that can support all modalities? To answer this, we propose UnIVAL, a step further towards this ambitious goal. Without relying on fancy datasets sizes or models with billions of parameters, the ~ 0.25B parameter UnIVAL model goes beyond two modalities and unifies text, images, video, and audio into a single model. Our model is efficiently pretrained on many tasks, based on task balancing and multimodal curriculum learning. UnIVAL shows competitive performance to existing state-of-the-art approaches, across image and video-text tasks. The feature representations learned from image and video-text modalities, allows the model to achieve competitive performance when finetuned on audio-text tasks, despite not being pretrained on audio. Thanks to the unified model, we propose a novel study on multimodal model merging via weight interpolation of models trained on different multimodal tasks, showing their benefits in particular for out-of-distribution generalization. Finally, we motivate unification by showing the synergy between tasks. The model weights and code are available at: https://github.com/mshukor/UnIVAL",
    "checked": false,
    "id": "0fe88452660cb8a0e37f54bcd44f3cd6504354b5",
    "semantic_title": "unified model for image, video, audio and language tasks",
    "citation_count": 46,
    "authors": [
      "Mustafa Shukor",
      "Corentin Dancette",
      "Alexandre Rame",
      "Matthieu Cord"
    ]
  },
  "https://openreview.net/forum?id=vfT4YuzAYA": {
    "title": "IndicTrans2: Towards High-Quality and Accessible Machine Translation Models for all 22 Scheduled Indian Languages",
    "volume": "main",
    "abstract": "India has a rich linguistic landscape, with languages from 4 major language families spoken by over a billion people. 22 of these languages listed in the Constitution of India (referred to as scheduled languages) are the focus of this work. Given the linguistic diversity, high-quality and accessible Machine Translation (MT) systems are essential in a country like India. Before this work, there was (i) no parallel training data spanning all 22 languages, (ii) no robust benchmarks covering all these languages and containing content relevant to India, and (iii) no existing translation models that support all 22 scheduled languages of India. In this work, we aim to address this gap by focusing on the missing pieces required for enabling wide, easy, and open access to good machine translation systems for all 22 scheduled Indian languages. We identify four key areas of improvement: curating and creating larger training datasets, creating diverse and high-quality benchmarks, training multilingual models, and releasing models with open access. Our first contribution is the release of the Bharat Parallel Corpus Collection (BPCC), the largest publicly available parallel corpora for Indic languages. BPCC contains a total of 230M bitext pairs, of which a total of 126M were newly added, including 644K manually translated sentence pairs created as part of this work. Our second contribution is the release of the first $n$-way parallel benchmark covering all 22 Indian languages, featuring diverse domains, Indian-origin content, and conversational test sets. Next, we present IndicTrans2, the first translation model to support all 22 languages, surpassing existing models in performance on multiple existing and new benchmarks created as a part of this work. Lastly, to promote accessibility and collaboration, we release our models and associated data with permissive licenses at https://github.com/AI4Bharat/IndicTrans2",
    "checked": true,
    "id": "59919e80ede70e82a70e8f76f533360c22f37275",
    "semantic_title": "indictrans2: towards high-quality and accessible machine translation models for all 22 scheduled indian languages",
    "citation_count": 138,
    "authors": [
      "Jay Gala",
      "Pranjal A Chitale",
      "A K Raghavan",
      "Varun Gumma",
      "Sumanth Doddapaneni",
      "Aswanth Kumar M",
      "Janki Atul Nawale",
      "Anupama Sujatha",
      "Ratish Puduppully",
      "Vivek Raghavan",
      "Pratyush Kumar",
      "Mitesh M Khapra",
      "Raj Dabre",
      "Anoop Kunchukuttan"
    ]
  },
  "https://openreview.net/forum?id=4Hq816XDDG": {
    "title": "Towards Optimization-Friendly Binary Neural Network",
    "volume": "main",
    "abstract": "Binary neural networks (BNNs) are a promising approach for compressing and accelerating deep learning models, especially in resource-constrained environments. However, the optimization gap between BNNs and their full-precision counterparts has long been an open problem limiting their performance. In this work, we propose a novel optimization pipeline to enhance the performance of BNNs. The main approach includes three key components: (1) BNext, a strong binary baseline based on an optimization-friendly basic block design, (2) knowledge complexity, a simple yet effective teacher-selection metric taking the capacity gap between teachers and binary students under consideration, (3) consecutive knowledge distillation (CKD), a novel multi-round optimization technique to transfer high-confidence knowledge from strong teachers to low-capacity BNNs. We empirically validate the superiority of the method on several vision classification tasks CIFAR-10/100 & ImageNet. For instance, the BNext family outperforms previous BNNs under different capacity levels and contributes the first binary neural network to reach the state-of-the-art 80.57\\% Top-1 accuracy on ImageNet with 0.82 GOPS, which verifies the potential of BNNs and already contributes a strong baseline for future research on high-accuracy BNNs. The code will be publicly available at (blind URL, see supplementary material)",
    "checked": true,
    "id": "7f9e7c1497e87e6b584046bb94009da92fe0d90a",
    "semantic_title": "towards optimization-friendly binary neural network",
    "citation_count": 2,
    "authors": [
      "Nianhui Guo",
      "Joseph Bethge",
      "Hong Guo",
      "Christoph Meinel",
      "Haojin Yang"
    ]
  },
  "https://openreview.net/forum?id=ExbGarTbLE": {
    "title": "Equivariant MuZero",
    "volume": "main",
    "abstract": "Deep reinforcement learning has shown lots of success in closed, well-defined domains such as games (Chess, Go, StarCraft). The next frontier is real-world scenarios, where setups are numerous and varied. For this, agents need to learn the underlying environment dynamics, so as to robustly generalise to conditions that differ from those they were trained on. Model-based reinforcement learning algorithms, such as MuZero or Dreamer, aim to accomplish this by learning a world model. However, leveraging a world model has not yet consistently shown greater generalisation capabilities compared to model-free alternatives. In this work, we propose improving the data efficiency and generalisation capabilities of MuZero by explicitly incorporating the \\emph{symmetries} of the environment in its world-model architecture. We prove that, so long as the neural networks used by MuZero are equivariant to a particular symmetry group acting on the environment, the entirety of MuZero's action-selection algorithm will also be equivariant to that group. As such, Equivariant MuZero is guaranteed to behave symmetrically in symmetrically-transformed states, and will hence be more data-efficient when learning its world models. We evaluate Equivariant MuZero on procedurally-generated MiniPacman and on Chaser from the ProcGen suite: training on a set of mazes, and then testing on unseen rotated versions, demonstrating the benefits of equivariance. We verify that our improvements hold even when only some of the components of Equivariant MuZero obey strict equivariance, which highlights the robustness of our construction",
    "checked": true,
    "id": "2e8355d05976a96018757af12177a78f1911e912",
    "semantic_title": "equivariant muzero",
    "citation_count": 4,
    "authors": [
      "Andreea Deac",
      "Theophane Weber",
      "George Papamakarios"
    ]
  },
  "https://openreview.net/forum?id=hFsr59Imzm": {
    "title": "On the Efficacy of Differentially Private Few-shot Image Classification",
    "volume": "main",
    "abstract": "There has been significant recent progress in training differentially private (DP) models which achieve accuracy that approaches the best non-private models. These DP models are typically pretrained on large public datasets and then fine-tuned on private downstream datasets that are relatively large and similar in distribution to the pretraining data. However, in many applications including personalization and federated learning, it is crucial to perform well (i) in the few-shot setting, as obtaining large amounts of labeled data may be problematic; and (ii) on datasets from a wide variety of domains for use in various specialist settings. To understand under which conditions few-shot DP can be effective, we perform an exhaustive set of experiments that reveals how the accuracy and vulnerability to attack of few-shot DP image classification models are affected as the number of shots per class, privacy level, model architecture, downstream dataset, and subset of learnable parameters in the model vary. We show that to achieve DP accuracy on par with non-private models, the shots per class must be increased as the privacy level increases. We also show that learning parameter-efficient FiLM adapters under DP is competitive with learning just the final classifier layer or learning all of the network parameters. Finally, we evaluate DP federated learning systems and establish state-of-the-art performance on the challenging FLAIR benchmark",
    "checked": true,
    "id": "947cf0610c58fb87e9b4d09d3e4e99311d3a7a05",
    "semantic_title": "on the efficacy of differentially private few-shot image classification",
    "citation_count": 12,
    "authors": [
      "Marlon Tobaben",
      "Aliaksandra Shysheya",
      "John F Bronskill",
      "Andrew Paverd",
      "Shruti Tople",
      "Santiago Zanella-Beguelin",
      "Richard E Turner",
      "Antti Honkela"
    ]
  },
  "https://openreview.net/forum?id=sbkZKBVC31": {
    "title": "Mixture of Dynamical Variational Autoencoders for Multi-Source Trajectory Modeling and Separation",
    "volume": "main",
    "abstract": "In this paper, we propose a latent-variable generative model called mixture of dynamical variational autoencoders (MixDVAE) to model the dynamics of a system composed of multiple moving sources. A DVAE model is pre-trained on a single-source dataset to capture the source dynamics. Then, multiple instances of the pre-trained DVAE model are integrated into a multi-source mixture model with a discrete observation-to-source assignment latent variable. The posterior distributions of both the discrete observation-to-source assignment variable and the continuous DVAE variables representing the sources content/position are estimated using the variational expectation-maximization algorithm, leading to multi-source trajectories estimation. We illustrate the versatility of the proposed MixDVAE model on two tasks: a computer vision task, namely multi-object tracking, and an audio processing task, namely single-channel audio source separation. Experimental results show that the proposed method works well on these two tasks, and outperforms several baseline methods",
    "checked": true,
    "id": "b0218ce7d6f38137ba8523b1d268e688052519f2",
    "semantic_title": "mixture of dynamical variational autoencoders for multi-source trajectory modeling and separation",
    "citation_count": 2,
    "authors": [
      "Xiaoyu Lin",
      "Laurent Girin",
      "Xavier Alameda-Pineda"
    ]
  },
  "https://openreview.net/forum?id=f7a8XCRtUu": {
    "title": "Fast Slate Policy Optimization: Going Beyond Plackett-Luce",
    "volume": "main",
    "abstract": "An increasingly important building block of large scale machine learning systems is based on returning slates; an ordered lists of items given a query. Applications of this technology include: search, information retrieval and recommender systems. When the action space is large, decision systems are restricted to a particular structure to complete online queries quickly. This paper addresses the optimization of these large scale decision systems given an arbitrary reward function. We cast this learning problem in a policy optimization framework and propose a new class of policies, born from a novel relaxation of decision functions. This results in a simple, yet efficient learning algorithm that scales to massive action spaces. We compare our method to the commonly adopted Plackett-Luce policy class and demonstrate the effectiveness of our approach on problems with action space sizes in the order of millions",
    "checked": true,
    "id": "583eec43ce04e3b7c58247832d766c4e257fb98a",
    "semantic_title": "fast slate policy optimization: going beyond plackett-luce",
    "citation_count": 4,
    "authors": [
      "Otmane Sakhi",
      "David Rohde",
      "Nicolas Chopin"
    ]
  },
  "https://openreview.net/forum?id=QCjMJfSnYk": {
    "title": "Error bounds and dynamics of bootstrapping in actor-critic reinforcement learning",
    "volume": "main",
    "abstract": "Actor-critic algorithms such as DDPG, TD3, and SAC, which are built on Silver's deterministic policy gradient theorem, are among the most successful reinforcement-learning methods, but their mathematical basis is not entirely clear. In particular, the critic networks in these algorithms learn to estimate action-value functions by a \"bootstrapping\" technique based on Bellman error, and it is unclear why this approach works so well in practice, given that Bellman error is only very loosely related to value error, i.e. to the inaccuracy of the action-value estimate. Here we show that policy training in this class of actor-critic methods depends not on the accuracy of the critic's action-value estimate but on how well the critic estimates the gradient of the action-value, which is better assessed using what we call difference error. We show that this difference error is closely related to the Bellman error — a finding which helps to explain why Bellman-based bootstrapping leads to good policies. Further, we show that value error and difference error show different dynamics along on-policy trajectories through state-action space: value error is a low-pass anticausal (i.e., backward-in-time) filter of Bellman error, and therefore accumulates along trajectories, whereas difference error is a high-pass filter of Bellman error. It follows that techniques which reduce the high-frequency Fourier components of the Bellman error may improve policy training even if they increase the actual size of the Bellman errors. These findings help to explain certain aspects of actor-critic methods that are otherwise theoretically puzzling, such as the use of policy (as distinct from exploratory) noise, and they suggest other measures that may improve these methods",
    "checked": true,
    "id": "d4c92347bff3a51245f2c9f5e2af861cf264d94b",
    "semantic_title": "error bounds and dynamics of bootstrapping in actor-critic reinforcement learning",
    "citation_count": 0,
    "authors": [
      "Ahmed J Zerouali",
      "Douglas Blair Tweed"
    ]
  },
  "https://openreview.net/forum?id=NnUmg1chLL": {
    "title": "Federated Minimax Optimization with Client Heterogeneity",
    "volume": "main",
    "abstract": "Minimax optimization has seen a surge in interest with the advent of modern applications such as GANs, and it is inherently more challenging than simple minimization. The difficulty is exacerbated by the training data residing at multiple edge devices or \\textit{clients}, especially when these clients can have heterogeneous datasets and heterogeneous local computation capabilities. We propose a general federated minimax optimization framework that subsumes such settings and several existing methods like Local SGDA. We show that naive aggregation of model updates made by clients running unequal number of local steps can result in optimizing a mismatched objective function -- a phenomenon previously observed in standard federated minimization. To fix this problem, we propose normalizing the client updates by the number of local steps. We analyze the convergence of the proposed algorithm for classes of nonconvex-concave and nonconvex-nonconcave functions and characterize the impact of heterogeneous client data, partial client participation, and heterogeneous local computations. For all the function classes considered, we significantly improve the existing computation and communication complexity results. Experimental results support our theoretical claims",
    "checked": true,
    "id": "410dc1db47548691bb5a1de2db247bae81324098",
    "semantic_title": "federated minimax optimization with client heterogeneity",
    "citation_count": 9,
    "authors": [
      "Pranay Sharma",
      "Rohan Panda",
      "Gauri Joshi"
    ]
  },
  "https://openreview.net/forum?id=Uj6MRfR1P5": {
    "title": "Towards Fair Video Summarization",
    "volume": "main",
    "abstract": "Automated video summarization is a vision task that aims to generate concise summaries of lengthy videos. Recent advancements in deep learning have led to highly performant video summarization models; however, there has been a lack of attention given to fairness and unbiased representation in the generated summaries. To bridge this gap, we introduce and analytically define the fair video summarization problem, and demonstrate its connections to the well-established problem of fair clustering. To facilitate fair model development, we also introduce the FairVidSum dataset, which is similar in design to state-of-the-art video summarization datasets such as TVSum and SumMe, but also includes annotations for sensitive attributes and individuals alongside frame importance scores. Finally, we propose the SumBal metric for quantifying the fairness of an outputted video summary. We conduct extensive experiments to benchmark the fairness of various state-of-the-art video summarization models. Our results highlight the need for better models that balance accuracy and fairness to ensure equitable representation and inclusion in summaries. For completeness, we also provide a novel fair-only baseline, FVS-LP, to showcase the fairness-utility gap models can improve upon",
    "checked": true,
    "id": "a0786692ef0768176bf1f59489d95dab69f43d28",
    "semantic_title": "towards fair video summarization",
    "citation_count": 3,
    "authors": [
      "Anshuman Chhabra",
      "Kartik Patwari",
      "Chandana Kuntala",
      "Sristi",
      "Deepak Kumar Sharma",
      "Prasant Mohapatra"
    ]
  },
  "https://openreview.net/forum?id=FWyabz82fH": {
    "title": "Uncertainty Estimation for Computed Tomography with a Linearised Deep Image Prior",
    "volume": "main",
    "abstract": "Existing deep-learning based tomographic image reconstruction methods do not provide accurate uncertainty estimates of their reconstructions, hindering their real-world deployment. This paper develops a method, termed as linearised deep image prior (DIP), to estimate the uncertainty associated with reconstructions produced by the DIP with total variation (TV) regularisation. We endow the DIP with conjugate Gaussian-linear model type error-bars computed from a local linearisation of the neural network around its optimised parameters. To preserve conjugacy, we approximate the TV regulariser with a Gaussian surrogate. This approach provides pixel-wise uncertainty estimates and a marginal likelihood objective for hyperparameter optimisation. We demonstrate the method on synthetic data and real-measured high-resolution 2D $\\mu$CT data, and show that it provides superior calibration of uncertainty estimates relative to previous probabilistic formulations of the~DIP. Our code is available at https://github.com/educating-dip/bayes_dip",
    "checked": true,
    "id": "2eec71f84d06d4f554a4243874fe3d1780ba90c2",
    "semantic_title": "uncertainty estimation for computed tomography with a linearised deep image prior",
    "citation_count": 10,
    "authors": [
      "Javier Antoran",
      "Riccardo Barbano",
      "Johannes Leuschner",
      "José Miguel Hernández-Lobato",
      "Bangti Jin"
    ]
  },
  "https://openreview.net/forum?id=231ZzrLC8X": {
    "title": "Early Stopping for Deep Image Prior",
    "volume": "main",
    "abstract": "Deep image prior (DIP) and its variants have shown remarkable potential to solve inverse problems in computational imaging (CI), needing no separate training data. Practical DIP models are often substantially overparameterized. During the learning process, these models first learn the desired visual content and then pick up potential modeling and observational noise, i.e., performing early learning then overfitting. Thus, the practicality of DIP hinges on early stopping (ES) that can capture the transition period. In this regard, most previous DIP works for CI tasks only demonstrate the potential of the models, reporting the peak performance against the ground truth but providing no clue about how to operationally obtain near-peak performance without access to the ground truth. In this paper, we set to break this practicality barrier of DIP, and propose an effective ES strategy that consistently detects near-peak performance across several CI tasks and DIP variants. Simply based on the running variance of DIP intermediate reconstructions, our ES method not only outpaces the existing ones---which only work in very narrow regimes, but also remains effective when combined with methods that try to mitigate overfitting",
    "checked": true,
    "id": "1f82c4a2962a3e6fb0800661ae7239092037bd08",
    "semantic_title": "early stopping for deep image prior",
    "citation_count": 64,
    "authors": [
      "Hengkang Wang",
      "Taihui Li",
      "Zhong Zhuang",
      "Tiancong Chen",
      "Hengyue Liang",
      "Ju Sun"
    ]
  },
  "https://openreview.net/forum?id=xflYdGZMpv": {
    "title": "Image retrieval outperforms diffusion models on data augmentation",
    "volume": "main",
    "abstract": "Many approaches have been proposed to use diffusion models to augment training datasets for downstream tasks, such as classification. However, diffusion models are themselves trained on large datasets, often with noisy annotations, and it remains an open question to which extent these models contribute to downstream classification performance. In particular, it remains unclear if they generalize enough to improve over directly using the additional data of their pre-training process for augmentation. We systematically evaluate a range of existing methods to generate images from diffusion models and study new extensions to assess their benefit for data augmentation. Personalizing diffusion models towards the target data outperforms simpler prompting strategies. However, using the pre-training data of the diffusion model alone, via a simple nearest-neighbor retrieval procedure, leads to even stronger downstream performance. Our study explores the potential of diffusion models in generating new training data, and surprisingly finds that these sophisticated models are not yet able to beat a simple and strong image retrieval baseline on simple downstream vision tasks",
    "checked": true,
    "id": "c22dfd7dd7364d96e03e47998819cc188a53d3f4",
    "semantic_title": "image retrieval outperforms diffusion models on data augmentation",
    "citation_count": 17,
    "authors": [
      "Max F Burg",
      "Florian Wenzel",
      "Dominik Zietlow",
      "Max Horn",
      "Osama Makansi",
      "Francesco Locatello",
      "Chris Russell"
    ]
  },
  "https://openreview.net/forum?id=Mbc58EzF5q": {
    "title": "Transport with Support: Data-Conditional Diffusion Bridges",
    "volume": "main",
    "abstract": "The dynamic Schrödinger bridge problem provides an appealing setting for solving constrained time-series data generation tasks posed as optimal transport problems. It consists of learning non-linear diffusion processes using efficient iterative solvers. Recent works have demonstrated state-of-the-art results (eg., in modelling single-cell embryo RNA sequences or sampling from complex posteriors) but are limited to learning bridges with only initial and terminal constraints. Our work extends this paradigm by proposing the Iterative Smoothing Bridge (ISB). We integrate Bayesian filtering and optimal control into learning the diffusion process, enabling the generation of constrained stochastic processes governed by sparse observations at intermediate stages and terminal constraints. We assess the effectiveness of our method on synthetic and real-world data generation tasks and we show that the ISB generalises well to high-dimensional data, is computationally efficient, and provides accurate estimates of the marginals at intermediate and terminal times",
    "checked": true,
    "id": "8d61874776bf54ced20fffe3b2258da85090d7b8",
    "semantic_title": "transport with support: data-conditional diffusion bridges",
    "citation_count": 7,
    "authors": [
      "Ella Tamir",
      "Martin Trapp",
      "Arno Solin"
    ]
  },
  "https://openreview.net/forum?id=w4MoQ39zmc": {
    "title": "Local Function Complexity for Active Learning via Mixture of Gaussian Processes",
    "volume": "main",
    "abstract": "Inhomogeneities in real-world data, e.g., due to changes in the observation noise level or variations in the structural complexity of the source function, pose a unique set of challenges for statistical inference. Accounting for them can greatly improve predictive power when physical resources or computation time is limited. In this paper, we draw on recent theoretical results on the estimation of local function complexity (LFC), derived from the domain of local polynomial smoothing (LPS), to establish a notion of local structural complexity, which is used to develop a model-agnostic active learning (AL) framework. Due to its reliance on pointwise estimates, the LPS model class is not robust and scalable concerning large input space dimensions that typically come along with real-world problems. Here, we derive and estimate the Gaussian process regression (GPR)-based analog of the LPS-based LFC and use it as a substitute in the above framework to make it robust and scalable. We assess the effectiveness of our LFC estimate in an AL application on a prototypical low-dimensional synthetic dataset, before taking on the challenging real-world task of reconstructing a quantum chemical force field for a small organic molecule and demonstrating state-of-the-art performance with a significantly reduced training demand",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Danny Panknin",
      "Stefan Chmiela",
      "Klaus Robert Muller",
      "Shinichi Nakajima"
    ]
  },
  "https://openreview.net/forum?id=vJcTm2v9Ku": {
    "title": "Towards a General Transfer Approach for Policy-Value Networks",
    "volume": "main",
    "abstract": "Transferring trained policies and value functions from one task to another, such as one game to another with a different board size, board shape, or more substantial rule changes, is a challenging problem. Popular benchmarks for reinforcement learning (RL), such as Atari games and ProcGen, have limited variety especially in terms of action spaces. Due to a focus on such benchmarks, the development of transfer methods that can also handle changes in action spaces has received relatively little attention. Furthermore, we argue that progress towards more general methods should include benchmarks where new problem instances can be described by domain experts, rather than machine learning experts, using convenient, high-level domain specific languages (DSLs). In addition to enabling end users to more easily describe their problems, user-friendly DSLs also contain relevant task information which can be leveraged to make effective zero-shot transfer plausibly achievable. As an example, we use the Ludii general game system, which includes a highly varied set of over 1000 distinct games described in such a language. We propose a simple baseline approach for transferring fully convolutional policy-value networks, which are used to guide search agents similar to AlphaZero, between any pair of games modelled in this system. Extensive results---including various cases of highly successful zero-shot transfer---are provided for a wide variety of source and target games",
    "checked": true,
    "id": "80453413e567db49cb355520d9c7aa063138b85b",
    "semantic_title": "towards a general transfer approach for policy-value networks",
    "citation_count": 3,
    "authors": [
      "Dennis J. N. J. Soemers",
      "Vegard Mella",
      "Eric Piette",
      "Matthew Stephenson",
      "Cameron Browne",
      "Olivier Teytaud"
    ]
  },
  "https://openreview.net/forum?id=Id10mlBjcx": {
    "title": "ProtoCaps: A Fast and Non-Iterative Capsule Network Routing Method",
    "volume": "main",
    "abstract": "Capsule Networks have emerged as a powerful class of deep learning architectures, known for robust performance with relatively few parameters compared to Convolutional Neural Networks (CNNs). However, their inherent efficiency is often overshadowed by their slow, iterative routing mechanisms which establish connections between Capsule layers, posing computational challenges resulting in an inability to scale. In this paper, we introduce a novel, non-iterative routing mechanism, inspired by trainable prototype clustering. This innovative approach aims to mitigate computational complexity, while retaining, if not enhancing, performance efficacy. Furthermore, we harness a shared Capsule subspace, negating the need to project each lower-level Capsule to each higher-level Capsule, thereby significantly reducing memory requisites during training. Our approach demonstrates superior results compared to the current best non-iterative Capsule Network and tests on the Imagewoof dataset, which is too computationally demanding to handle efficiently by iterative approaches. Our findings underscore the potential of our proposed methodology in enhancing the operational efficiency and performance of Capsule Networks, paving the way for their application in increasingly complex computational scenarios. Code is available at https://github.com/mileseverett/ProtoCaps",
    "checked": true,
    "id": "3f870b61566940acdb565e88bd4f5c0183c03827",
    "semantic_title": "protocaps: a fast and non-iterative capsule network routing method",
    "citation_count": 4,
    "authors": [
      "Miles Everett",
      "Mingjun Zhong",
      "Georgios Leontidis"
    ]
  },
  "https://openreview.net/forum?id=t4p612DftO": {
    "title": "Detecting danger in gridworlds using Gromov's Link Condition",
    "volume": "main",
    "abstract": "Gridworlds have been long-utilised in AI research, particularly in reinforcement learning, as they provide simple yet scalable models for many real-world applications such as robot navigation, emergent behaviour, and operations research. We initiate a study of gridworlds using the mathematical framework of reconfigurable systems and state complexes due to Abrams, Ghrist & Peterson. State complexes, a higher-dimensional analogue of state graphs, represent all possible configurations of a system as a single geometric space, thus making them conducive to study using geometric, topological, or combinatorial methods. The main contribution of this work is a modification to the original Abrams, Ghrist & Peterson setup which we introduce to capture agent braiding and thereby more naturally represent the topology of gridworlds. With this modification, the state complexes may exhibit geometric defects (failure of Gromov's Link Condition). Serendipitously, we discover these failures for agent-only cases occur exactly where undesirable or dangerous states appear in the gridworld. Our results therefore provide a novel method for seeking guaranteed safety limitations in discrete task environments with single or multiple agents, and offer useful safety information (in geometric and topological forms) for incorporation in or analysis of machine learning systems. More broadly, our work introduces tools from geometric group theory and combinatorics to the AI community and demonstrates a proof-of-concept for this geometric viewpoint of the task domain through the example of simple environments",
    "checked": true,
    "id": "7d2d1e91f66790aaeefa123304be153297c8a82b",
    "semantic_title": "detecting danger in gridworlds using gromov's link condition",
    "citation_count": 2,
    "authors": [
      "Thomas F Burns",
      "Robert Tang"
    ]
  },
  "https://openreview.net/forum?id=75CcopPxIr": {
    "title": "Partial Optimal Transport for Support Subset Selection",
    "volume": "main",
    "abstract": "In probabilistic terms, optimal transport aims to find a joint distribution that couples two distributions and minimizes the cost of transforming one distribution to another. Any feasible coupling necessarily maintains the support of both distributions. However, maintaining the entire support is not ideal when only a subset of one of the distributions, namely the source, is assumed to align with the other target distribution. For these cases, which are common in machine learning applications, we study the semi-relaxed partial optimal transport problem that relaxes the constraints on the joint distribution allowing it to under-represent a subset of the source by over-representing other subsets of the source by a constant factor. In the discrete distribution case, such as in the case of two samples from continuous random variables, optimal transport with the relaxed constraints is a linear program. When sufficiently relaxed, the solution has a source marginal with only a subset of its original support. We investigate the scaling path of solutions, specifically the relaxed marginal distribution for the source, across different relaxations and show that it is distinct from the solutions from penalty-based semi-relaxed unbalanced optimal transport problems and fully-relaxed partial optimal transport, which have previously been explored. We demonstrate the usefulness of this support subset selection in applications such as color transfer, partial point cloud alignment, and semi-supervised machine learning, where a part of data is curated to have reliable labels and another part is unlabeled or has unreliable labels. Our experiments show that optimal transport under the relaxed constraint can improve the performance of these applications by allowing for more flexible alignment between distributions",
    "checked": true,
    "id": "7d2880a2eea9196780d33e7173699d2b9d80e5b6",
    "semantic_title": "partial optimal transport for support subset selection",
    "citation_count": 2,
    "authors": [
      "Bilal Riaz",
      "Yuksel Karahan",
      "Austin J. Brockmeier"
    ]
  },
  "https://openreview.net/forum?id=KrequDpWzt": {
    "title": "Wrapped $\\beta$-Gaussians with compact support for exact probabilistic modeling on manifolds",
    "volume": "main",
    "abstract": "We introduce wrapped $\\beta$-Gaussians, a family of wrapped distributions on Riemannian manifolds, supporting efficient reparametrized sampling, as well as exact density estimation, effortlessly supporting high dimensions and anisotropic scale parameters. We extend Fenchel-Young losses for geometry-aware learning with wrapped $\\beta$-Gaussians, and demonstrate the efficacy of our proposed family in a suite of experiments on hypersphere and rotation manifolds: data fitting, hierarchy encoding, generative modeling with variational autoencoders, and multilingual word embedding alignment",
    "checked": false,
    "id": "1ece7fca5488633dda8fd9b16305caabc82f6810",
    "semantic_title": "wrapped β-gaussians with compact support for exact probabilistic modeling on manifolds",
    "citation_count": 1,
    "authors": [
      "Sergey Troshin",
      "Vlad Niculae"
    ]
  },
  "https://openreview.net/forum?id=0WKTmrVkd2": {
    "title": "GIT-Net: Generalized Integral Transform for Operator Learning",
    "volume": "main",
    "abstract": "This article introduces GIT-Net, a deep neural network architecture for approximating Partial Differential Equation (PDE) operators, inspired by integral transform operators. GIT-NET harnesses the fact that common differential operators commonly used for defining PDEs can often be represented parsimoniously when expressed in specialized functional bases (e.g., Fourier basis). Unlike rigid integral transforms, GIT-Net parametrizes adaptive generalized integral transforms with deep neural networks. When compared to several recently proposed alternatives, GIT-Net's computational and memory requirements scale gracefully with mesh discretizations, facilitating its application to PDE problems on complex geometries. Numerical experiments demonstrate that GIT-Net is a competitive neural network operator, exhibiting small test errors and low evaluations across a range of PDE problems. This stands in contrast to existing neural network operators, which typically excel in just one of these areas",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Wang",
      "Alexandre H. Thiery"
    ]
  },
  "https://openreview.net/forum?id=sUlbRfLijj": {
    "title": "Semi-Supervised Single Domain Generalization with Label-Free Adversarial Data Augmentation",
    "volume": "main",
    "abstract": "Domain generalization (DG) has attracted increasing attention recently, as it seeks to improve the generalization ability of visual recognition models to unseen target domains. DG leverages multiple source domains for model training, while single domain generalization (SDG) further restricts such setting by exploiting only a single source domain. Nevertheless, both DG and SDG assume that the source domains are fully labeled, which might not be practical in many real world scenarios. In this paper, we present a new problem, i.e., semi-supervised single domain generalization (SS-SDG), which aims to train a model with a partially labeled single source domain to generalize to multiple unseen testing domains. We propose an effective framework to address this problem. In particular, we design a label-free adversarial data augmentation strategy to diversify the source domain, and propose a novel multi-pair FixMatch loss to generalize classifiers to unseen testing domains. Extensive experiments on OfficeHome, PACS and DomainNet20 datasets show that our method surpasses the latest SDG and semi-supervised methods. Moreover, on PACS and DomainNet20, our method approaches the fully supervised ERM upper bound within $5\\%$ gap, but only uses less than $8\\%$ of the labels",
    "checked": true,
    "id": "8fb4eda80a0fe08b32e3c016ad6ed96131568ac4",
    "semantic_title": "semi-supervised single domain generalization with label-free adversarial data augmentation",
    "citation_count": 2,
    "authors": [
      "Ronghang Zhu",
      "Xiang Yu",
      "Sheng Li"
    ]
  },
  "https://openreview.net/forum?id=jpZmhiIys1": {
    "title": "Beyond Boundaries: A Novel Data-Augmentation Discourse for Open Domain Generalization",
    "volume": "main",
    "abstract": "The problem of Open Domain Generalization (ODG) is multifaceted, encompassing shifts in domains and labels across all source and target domains. Existing approaches have encountered challenges such as style bias towards training domains, insufficient feature-space disentanglement to highlight semantic features, and discriminativeness of the latent space. Additionally, they rely on a confidence-based target outlier detection approach, which can lead to misclassifications when target open samples visually align with the source domain data. In response to these challenges, we present a solution named \\textsc{ODG-Net}. We aim to create a direct open-set classifier within a \\textit{discriminative}, \\textit{unbiased}, and \\textit{disentangled} semantic embedding space. To enrich data density and diversity, we introduce a generative augmentation framework that produces \\textit{style-interpolated} novel domains for closed-set images and novel pseudo-open images by \\textit{interpolating the contents of paired training images}. Our augmentation strategy skillfully utilizes \\textit{disentangled style and content information} to synthesize images effectively. Furthermore, we tackle the issue of style bias by representing all images in relation to all source domain properties, which effectively accentuates complementary visual features. Consequently, we train a multi-class semantic object classifier, incorporating both closed and open class classification capabilities, along with a style classifier to identify style primitives. The joint use of style and semantic classifiers facilitates the disentanglement of the latent space, thereby enhancing the generalization performance of the semantic classifier. To ensure discriminativeness in both closed and open spaces, we optimize the semantic feature space using novel metric losses. The experimental results on six benchmark datasets convincingly demonstrate that \\textsc{ODG-Net} surpasses the state-of-the-art by an impressive margin of $1-4\\%$ in both open and closed-set DG scenarios",
    "checked": true,
    "id": "af5dea08a858c96c2311e5de6a41e8fbf2ad0dcc",
    "semantic_title": "beyond boundaries: a novel data-augmentation discourse for open domain generalization",
    "citation_count": 5,
    "authors": [
      "Shirsha Bose",
      "Ankit Jha",
      "Hitesh Kandala",
      "Biplab Banerjee"
    ]
  },
  "https://openreview.net/forum?id=T55dLSgsEf": {
    "title": "Accelerating Batch Active Learning Using Continual Learning Techniques",
    "volume": "main",
    "abstract": "A major problem with Active Learning (AL) is high training costs since models are typically retrained from scratch after every query round. We start by demonstrating that standard AL on neural networks with warm starting fails, both to accelerate training and to avoid catastrophic forgetting when using fine-tuning over AL query rounds. We then develop a new class of techniques, circumventing this problem, by biasing further training towards previously labeled sets. We accomplish this by employing existing, and developing novel, replay-based Continual Learning (CL) algorithms that are effective at quickly learning the new without forgetting the old, especially when data comes from an evolving distribution. We call this paradigm \\textit{\"Continual Active Learning\" (CAL)}. We show CAL achieves significant speedups using a plethora of replay schemes that use model distillation and that select diverse/uncertain points from the history. We conduct experiments across many data domains, including natural language, vision, medical imaging, and computational biology, each with different neural architectures and dataset sizes. CAL consistently provides a $\\sim$3x reduction in training time, while retaining performance and out-of-distribution robustness, showing its wide applicability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arnav Mohanty Das",
      "Gantavya Bhatt",
      "Megh Manoj Bhalerao",
      "Vianne R. Gao",
      "Rui Yang",
      "Jeff Bilmes"
    ]
  },
  "https://openreview.net/forum?id=lXBEwFfxpA": {
    "title": "Revisiting Topic-Guided Language Models",
    "volume": "main",
    "abstract": "A recent line of work in natural language processing has aimed to combine language models and topic models. These \\textit{topic-guided language models} augment neural language models with topic models, unsupervised learning methods that can discover document-level patterns of word use. This paper compares the effectiveness of these methods in a standardized setting. We study four topic-guided language models and two baselines, evaluating the held-out predictive performance of each model on four corpora. Surprisingly, we find that \\textit{none of these methods outperform a standard LSTM language model baseline}, and most fail to learn good topics. Further, we train a probe of the neural language model that shows that the baseline's hidden states already encode topic information. We make public all code used for this study",
    "checked": true,
    "id": "2ccb7e10e1f5bebe7e203a2e99a23c4998287e4d",
    "semantic_title": "revisiting topic-guided language models",
    "citation_count": 1,
    "authors": [
      "Carolina Zheng",
      "Keyon Vafa",
      "David Blei"
    ]
  },
  "https://openreview.net/forum?id=LfQ6uAVAEo": {
    "title": "Two-Level Actor-Critic Using Multiple Teachers",
    "volume": "main",
    "abstract": "Deep reinforcement learning has successfully allowed agents to learn complex behaviors for many tasks. However, a key limitation of current learning approaches is the sample-inefficiency problem, which limits performance of the learning agent. This paper considers how agents can benefit from improved learning via teachers' advice. In particular, we consider the setting with multiple sub-optimal teachers, as opposed to having a single near-optimal teacher. We propose a flexible two-level actor-critic algorithm where the high-level network learns to choose the best teacher in the current situation while the low-level network learns the control policy",
    "checked": true,
    "id": "ce7be018cc30bd2bb195e7f97849ef004fdc33e9",
    "semantic_title": "two-level actor-critic using multiple teachers",
    "citation_count": 0,
    "authors": [
      "Su Zhang",
      "Srijita Das",
      "Sriram Ganapathi Subramanian",
      "Matthew E. Taylor"
    ]
  },
  "https://openreview.net/forum?id=UxmvCwuTMG": {
    "title": "ECG Representation Learning with Multi-Modal EHR Data",
    "volume": "main",
    "abstract": "Electronic Health Records (EHRs) provide a rich source of medical information across different modalities such as electrocardiograms (ECG), structured EHRs (sEHR), and unstructured EHRs (text). Inspired by the fact that many cardiac and non-cardiac diseases influence the behavior of the ECG, we leverage structured EHRs and unstructured EHRs from multiple sources by pairing with ECGs and propose a set of three new multi-modal contrastive learning models that combine ECG, sEHR, and text modalities. The performance of these models is compared against different baseline models such as supervised learning models trained from scratch with random weights initialization, and self-supervised learning models trained only on ECGs. We pre-train the models on a large proprietary dataset of about 9 $million$ ECGs from around 2.4 $million$ patients and evaluate the pre-trained models on various downstream tasks such as classification, zero-shot retrieval, and out-of-distribution detection involving the prediction of various heart conditions using ECG waveforms as input, and demonstrate that the models presented in this work show significant improvements compared to all baseline modes",
    "checked": true,
    "id": "15a7b048c5ad3f3b37844426a94fb66c3074e568",
    "semantic_title": "ecg representation learning with multi-modal ehr data",
    "citation_count": 10,
    "authors": [
      "Sravan Kumar Lalam",
      "Hari Krishna Kunderu",
      "Shayan Ghosh",
      "Harish Kumar A",
      "Samir Awasthi",
      "Ashim Prasad",
      "Francisco Lopez-Jimenez",
      "Zachi I Attia",
      "Samuel Asirvatham",
      "Paul Friedman",
      "Rakesh Barve",
      "Melwin Babu"
    ]
  },
  "https://openreview.net/forum?id=V9tQKYYNK1": {
    "title": "Variational Causal Dynamics: Discovering Modular World Models from Interventions",
    "volume": "main",
    "abstract": "Latent world models allow agents to reason about complex environments with high-dimensional observations. However, adapting to new environments and effectively leveraging previous knowledge remain significant challenges. We present Variational Causal Dynamics (VCD), a structured world model that exploits the invariance of causal mechanisms across environments to achieve fast and modular adaptation. By causally factorising a transition model, VCD is able to identify reusable components across different environments. This is achieved by combining causal discovery and variational inference to learn a latent representation and transition model jointly in an unsupervised manner. Specifically, we optimise the evidence lower bound jointly over a representation model and a transition model structured as a causal graphical model. In evaluations on simulated environments with state and image observations, we show that VCD is able to successfully identify causal variables, and to discover consistent causal structures across different environments. Moreover, given a small number of observations in a previously unseen, intervened environment, VCD is able to identify the sparse changes in the dynamics and to adapt efficiently. In doing so, VCD significantly extends the capabilities of the current state-of-the-art in latent world models while also comparing favourably in terms of prediction accuracy",
    "checked": true,
    "id": "73f959402b39929dbad93610436ca078262c0fbf",
    "semantic_title": "variational causal dynamics: discovering modular world models from interventions",
    "citation_count": 9,
    "authors": [
      "Anson Lei",
      "Bernhard Schölkopf",
      "Ingmar Posner"
    ]
  },
  "https://openreview.net/forum?id=F74ZZk5hPa": {
    "title": "RCT Rejection Sampling for Causal Estimation Evaluation",
    "volume": "main",
    "abstract": "Confounding is a significant obstacle to unbiased estimation of causal effects from observational data. For settings with high-dimensional covariates---such as text data, genomics, or the behavioral social sciences---researchers have proposed methods to adjust for confounding by adapting machine learning methods to the goal of causal estimation. However, empirical evaluation of these adjustment methods has been challenging and limited. In this work, we build on a promising empirical evaluation strategy that simplifies evaluation design and uses real data: subsampling randomized controlled trials (RCTs) to create confounded observational datasets while using the average causal effects from the RCTs as ground-truth. We contribute a new sampling algorithm, which we call RCT rejection sampling, and provide theoretical guarantees that causal identification holds in the observational data to allow for valid comparisons to the ground-truth RCT. Using synthetic data, we show our algorithm indeed results in low bias when oracle estimators are evaluated on the confounded samples, which is not always the case for a previously proposed algorithm. In addition to this identification result, we highlight several finite data considerations for evaluation designers who plan to use RCT rejection sampling on their own datasets. As a proof of concept, we implement an example evaluation pipeline and walk through these finite data considerations with a novel, real-world RCT---which we release publicly---consisting of approximately 70k observations and text data as high-dimensional covariates. Together, these contributions build towards a broader agenda of improved empirical evaluation for causal estimation",
    "checked": true,
    "id": "f2410b3da60ba080031d50b6e2217003173c0eb0",
    "semantic_title": "rct rejection sampling for causal estimation evaluation",
    "citation_count": 7,
    "authors": [
      "Katherine A. Keith",
      "Sergey Feldman",
      "David Jurgens",
      "Jonathan Bragg",
      "Rohit Bhattacharya"
    ]
  },
  "https://openreview.net/forum?id=qM7JPBYROr": {
    "title": "Tight conditions for when the NTK approximation is valid",
    "volume": "main",
    "abstract": "We study when the neural tangent kernel (NTK) approximation is valid for training a model with the square loss. In the lazy training setting of Chizat et al. 2019, we show that rescaling the model by a factor of $\\alpha = O(T)$ suffices for the NTK approximation to be valid until training time $T$. Our bound is tight and improves on the previous bound of Chizat et al. 2019, which required a larger rescaling factor of $\\alpha = O(T^2)$",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Enric Boix-Adserà",
      "Etai Littwin"
    ]
  },
  "https://openreview.net/forum?id=ORMlg4g3mG": {
    "title": "Data-Free Diversity-Based Ensemble Selection for One-Shot Federated Learning",
    "volume": "main",
    "abstract": "The emerging availability of various machine learning models creates a great demand to harness the collective intelligence of many independently well-trained models to improve overall performance. Considering the privacy concern and non-negligible communication costs, one-shot federated learning and ensemble learning in a data-free manner attract significant attention. However, conventional ensemble selection approaches are neither training efficient nor applicable to federated learning due to the risk of privacy leakage from local clients; meanwhile, the \"many could be better than all\" principle under data-free constraints makes it even more challenging. Therefore, it becomes crucial to design an effective ensemble selection strategy to find a good subset of the base models as the ensemble team for the federated learning scenario. In this paper, we propose a novel data-free diversity-based framework, DeDES, to address the ensemble selection problem with diversity consideration for models under the one-shot federated learning setting. Experimental results show that our method can achieve both better performance and higher efficiency over 5 datasets, 4 different model structures, and both homogeneous and heterogeneous model groups under four different data-partition strategies",
    "checked": true,
    "id": "23234175cbdb8664bfbca42a28e37bf39ccff4ec",
    "semantic_title": "data-free diversity-based ensemble selection for one-shot federated learning",
    "citation_count": 5,
    "authors": [
      "Naibo Wang",
      "Wenjie Feng",
      "yuchen deng",
      "Moming Duan",
      "Fusheng Liu",
      "See-Kiong Ng"
    ]
  },
  "https://openreview.net/forum?id=wzRE5kTnl3": {
    "title": "Universal Graph Continual Learning",
    "volume": "main",
    "abstract": "We address catastrophic forgetting issues in graph learning as the arrival of new data from diverse task distributions often leads graph models to prioritize the current task, causing them to forget valuable insights from previous tasks. Whereas prior studies primarily tackle one setting of graph continual learning such as incremental node classification, we focus on a universal approach wherein each data point in a task can be a node or a graph, and the task varies from node to graph classification. We refer to this setting as Universal Graph Continual Learning (UGCL), which includes node-unit node classification (NUNC), graph-unit node classification (GUNC), and graph-unit graph classification (GUGC). Our novel method maintains a replay memory of nodes and neighbours to remind the model of past graph structures through distillation. Emphasizing the importance of preserving distinctive graph structures across tasks, we enforce that coarse-to-grain graph representations stay close to previous ones by minimizing our proposed global and local structure losses. We benchmark our method against various continual learning baselines in 8 real-world graph datasets and achieve significant improvement in average performance and forgetting across tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thanh Duc Hoang",
      "Do Viet Tung",
      "Duy-Hung Nguyen",
      "Bao-Sinh Nguyen",
      "Huy Hoang Nguyen",
      "Hung Le"
    ]
  },
  "https://openreview.net/forum?id=gY04GX8R5k": {
    "title": "Cross-client Label Propagation for Transductive and Semi-Supervised Federated Learning",
    "volume": "main",
    "abstract": "We present Cross-Client Label Propagation (XCLP), a new method for transductive and semi-supervised federated learning. XCLP estimates a data graph jointly from the data of multiple clients and computes labels for the unlabeled data by propagating label information across the graph. To avoid clients having to share their data with anyone, XCLP employs two cryptographically secure protocols: secure Hamming distance computation and secure summation. We demonstrate two distinct applications of XCLP within federated learning. In the first, we use it in a one-shot way to predict labels for unseen test points. In the second, we use it to repeatedly pseudo-label unlabeled training data in a federated semi-supervised setting. Experiments on both real federated and standard benchmark datasets show that in both applications XCLP achieves higher classification accuracy than alternative approaches",
    "checked": true,
    "id": "cbf044732c521a8ee3b2fcf3abc8d98d587f49ec",
    "semantic_title": "cross-client label propagation for transductive and semi-supervised federated learning",
    "citation_count": 0,
    "authors": [
      "Jonathan Scott",
      "Michelle Yeo",
      "Christoph H Lampert"
    ]
  },
  "https://openreview.net/forum?id=H5VRvCXCzf": {
    "title": "MERMAIDE: Learning to Align Learners using Model-Based Meta-Learning",
    "volume": "main",
    "abstract": "We study how a principal can efficiently and effectively intervene on the rewards of a previously unseen learning agent in order to induce desirable outcomes. This is relevant to many real-world settings like auctions or taxation, where the principal may not know the learning behavior nor the rewards of real people. Moreover, the principal should be few-shot adaptable and minimize the number of interventions, because interventions are often costly. We introduce MERMAIDE, a model-based meta-learning framework to train a principal that can quickly adapt to out-of-distribution agents with different learning strategies and reward functions. We validate this approach step-by-step. First, in a Stackelberg setting with a best-response agent, we show that meta-learning enables quick convergence to the theoretically known Stackelberg equilibrium at test time, although noisy observations severely increase the sample complexity. We then show that our model-based meta-learning approach is cost-effective in intervening on bandit agents with unseen explore-exploit strategies. Finally, we outperform baselines that use either meta-learning or agent behavior modeling, in both $0$-shot and $1$-shot settings with partial agent information",
    "checked": true,
    "id": "2226ab21e8059a2e154655e0c8d19779f0fc30fb",
    "semantic_title": "mermaide: learning to align learners using model-based meta-learning",
    "citation_count": 1,
    "authors": [
      "Arundhati Banerjee",
      "Soham Rajesh Phade",
      "Stefano Ermon",
      "Stephan Zheng"
    ]
  },
  "https://openreview.net/forum?id=8tnrh56P5W": {
    "title": "Meta Continual Learning on Graphs with Experience Replay",
    "volume": "main",
    "abstract": "Continual learning is a machine learning approach where the challenge is that a constructed learning model executes incoming tasks while maintaining its performance over the earlier tasks. In order to address this issue, we devise a technique that combines two uniquely important concepts in machine learning, namely \"replay buffer\" and \"meta learning\", aiming to exploit the best of two worlds. In this method, the model weights are initially computed by using the current task dataset. Next, the dataset of the current task is merged with the stored samples from the earlier tasks and the model weights are updated using the combined dataset. This aids in preventing the model weights converging to the optimal parameters of the current task and enables the preservation of information from earlier tasks. We choose to adapt our technique to graph data structure and the task of node classification on graphs. We introduce MetaCLGraph, which outperforms the baseline methods over various graph datasets including Citeseer, Corafull, Arxiv, and Reddit. This method illustrates the potential of combining replay buffer and meta learning in the field of continual learning on graphs",
    "checked": true,
    "id": "65894534b1fb06a4bfa80d409d29048572ca7a73",
    "semantic_title": "meta continual learning on graphs with experience replay",
    "citation_count": 3,
    "authors": [
      "Altay Unal",
      "Abdullah Akgül",
      "Melih Kandemir",
      "Gozde Unal"
    ]
  },
  "https://openreview.net/forum?id=0ck7hJ8EVC": {
    "title": "Improved identification accuracy in equation learning via comprehensive $\\boldsymbol{R^2}$-elimination and Bayesian model selection",
    "volume": "main",
    "abstract": "In the field of equation learning, exhaustively considering all possible combinations derived from a basis function dictionary is infeasible. Sparse regression and greedy algorithms have emerged as popular approaches to tackle this challenge. However, the presence of strong collinearities poses difficulties for sparse regression techniques, and greedy steps may inadvertently exclude important components of the true equation, leading to reduced identification accuracy. In this article, we present a novel algorithm that strikes a balance between comprehensiveness and efficiency in equation learning. Inspired by stepwise regression, our approach combines the coefficient of determination, $R^2$, and the Bayesian model evidence, $p(y|\\mathcal{M})$, in a novel way. Through three extensive numerical experiments involving random polynomials and dynamical systems, we compare our method against two standard approaches, four state-of-the-art methods, and bidirectional stepwise regression incorporating $p(y|\\mathcal{M})$. The results demonstrate that our less greedy algorithm surpasses all other methods in terms of identification accuracy. Furthermore, we discover a heuristic approach to mitigate the overfitting penalty associated with $R^2$ and propose an equation learning procedure solely based on $R^2$, which achieves high rates of exact equation recovery",
    "checked": false,
    "id": "0684e51641821819f73f47679d4d67a0725186f4",
    "semantic_title": "improved identification accuracy in equation learning via comprehensive r2-elimination and bayesian model selection",
    "citation_count": 0,
    "authors": [
      "Daniel Nickelsen",
      "Bubacarr Bah"
    ]
  },
  "https://openreview.net/forum?id=dN9YICB6hN": {
    "title": "Reliable Active Learning via Influence Functions",
    "volume": "main",
    "abstract": "Due to the high cost and time-consuming nature of collecting labeled data, having insufficient labeled data is a common challenge that can negatively impact the performance of deep learning models when applied to real-world applications. Active learning (AL) aims to reduce the cost and time required for obtaining labeled data by selecting valuable samples during model training. However, recent works have pointed out the performance unreliability of existing AL algorithms for deep learning (DL) architectures under different scenarios, which manifests as their performance being comparable (or worse) to that of basic random selection. This behavior compromises the applicability of these approaches. We address this problem by proposing a theoretically motivated AL framework for DL architectures. We demonstrate that the most valuable samples for the model are those that, unsurprisingly, improve its performance on the entire dataset, most of which is unlabeled, and present a framework to efficiently estimate such performance (or loss) via influence functions, pseudo labels and diversity selection. Experimental results show that the proposed reliable active learning via influence functions (RALIF) can consistently outperform the random selection baseline as well as other existing and state-of-the art active learning approaches",
    "checked": true,
    "id": "55736f19cdd0a6bd0946041f806335995190893c",
    "semantic_title": "reliable active learning via influence functions",
    "citation_count": 0,
    "authors": [
      "Meng Xia",
      "Ricardo Henao"
    ]
  },
  "https://openreview.net/forum?id=dZugyhbNFY": {
    "title": "Personalized Federated Learning with Communication Compression",
    "volume": "main",
    "abstract": "In contrast to training traditional machine learning~(ML) models in data centers, federated learning~(FL) trains ML models over local datasets contained on resource-constrained heterogeneous edge devices. Existing FL algorithms aim to learn a single global model for all participating devices, which may not be helpful to all devices participating in the training due to the heterogeneity of the data across the devices. Recently, Hanzely and Richt\\'{a}rik (2020) proposed a new formulation for training personalized FL models aimed at balancing the trade-off between the traditional global model and the local models that could be trained by individual devices using their private data only. They derived a new algorithm, called {\\em loopless gradient descent}~(L2GD), to solve it and showed that this algorithms leads to improved communication complexity guarantees in regimes when more personalization is required. In this paper, we equip their L2GD algorithm with a {\\em bidirectional} compression mechanism to further reduce the communication bottleneck between the local devices and the server. Unlike other compression-based algorithms used in the FL-setting, our compressed L2GD algorithm operates on a probabilistic communication protocol, where communication does not happen on a fixed schedule. Moreover, our compressed L2GD algorithm maintains a similar convergence rate as vanilla SGD without compression. To empirically validate the efficiency of our algorithm, we perform diverse numerical experiments on both convex and non-convex problems and using various compression techniques",
    "checked": true,
    "id": "9849705c3afaa6a147cc0c4d95d97130132e3199",
    "semantic_title": "personalized federated learning with communication compression",
    "citation_count": 10,
    "authors": [
      "El houcine Bergou",
      "Konstantin Pavlovich Burlachenko",
      "Aritra Dutta",
      "Peter Richtárik"
    ]
  },
  "https://openreview.net/forum?id=LT4DXqUJTD": {
    "title": "Uncovering Unique Concept Vectors through Latent Space Decomposition",
    "volume": "main",
    "abstract": "Interpreting the inner workings of deep learning models is crucial for establishing trust and ensuring model safety. Concept-based explanations have emerged as a superior approach that is more interpretable than feature attribution estimates such as pixel saliency. However, defining the concepts for the interpretability analysis biases the explanations by the user's expectations on the concepts. To address this, we propose a novel post-hoc unsupervised method that automatically uncovers the concepts learned by deep models during training. By decomposing the latent space of a layer in singular vectors and refining them by unsupervised clustering, we uncover concept vectors aligned with directions of high variance that are relevant to the model prediction, and that point to semantically distinct concepts. Our extensive experiments reveal that the majority of our concepts are readily understandable to humans, exhibit coherency, and bear relevance to the task at hand. Moreover, we showcase the practical utility of our method in dataset exploration, where our concept vectors successfully identify outlier training samples affected by various confounding factors. This novel exploration technique has remarkable versatility to data types and model architectures and it will facilitate the identification of biases and the discovery of sources of error within training data",
    "checked": true,
    "id": "a0f7ebe4c978c8c9e42e4e5004b968d58a9e901e",
    "semantic_title": "uncovering unique concept vectors through latent space decomposition",
    "citation_count": 5,
    "authors": [
      "Mara Graziani",
      "Laura O'Mahony",
      "An-phi Nguyen",
      "Henning Müller",
      "Vincent Andrearczyk"
    ]
  },
  "https://openreview.net/forum?id=V7guVYzvE4": {
    "title": "SANTA: Source Anchoring Network and Target Alignment for Continual Test Time Adaptation",
    "volume": "main",
    "abstract": "Adapting a trained model to perform satisfactorily on continually changing test environments is an important and challenging task. In this work, we propose a novel framework, SANTA, which aims to satisfy the following characteristics required for online adaptation: 1) can work effectively for different (even small) batch sizes; 2) should continue to work well on the source domain; 3) should have minimal tunable hyperparameters and storage requirements. Given a pre-trained network trained on source domain data, the proposed framework modifies the affine parameters of the batch normalization layers using source anchoring based self-distillation. This ensures that the model incorporates knowledge from the newly encountered domains, without catastrophically forgetting the previously seen domains. We also propose a source-prototype driven contrastive alignment to ensure natural grouping of the target samples, while maintaining the already learnt semantic information. Extensive evaluation on three benchmark datasets under challenging settings justify the effectiveness of SANTA for real-world applications. Code here: https://github.com/goirik-chakrabarty/SANTA",
    "checked": true,
    "id": "a08f8b9355eff9135970c378bddc3252ecf61236",
    "semantic_title": "santa: source anchoring network and target alignment for continual test time adaptation",
    "citation_count": 8,
    "authors": [
      "Goirik Chakrabarty",
      "Manogna Sreenivas",
      "Soma Biswas"
    ]
  },
  "https://openreview.net/forum?id=gvqzvUVPiQ": {
    "title": "The Analysis of the Expected Change in the Classification Probability of the Predicted Label",
    "volume": "main",
    "abstract": "We present a formalism for estimating the expected change in the probability distribution of the predicted label of an object, with respect to all small perturbations to the object. We first derive analytically an estimate of the expected probability change as a function of the input noise. We then conduct three empirical studies: in the first study, experimental results on image classification show that the proposed measure can be used to distinguish the not-robust label predictions from those that are robust, even when they are all predicted with high confidence. The second study shows that the proposed robustness measure is almost always higher for the predictions on the corrupted images, compared to the predictions on the original versions of them. The final study shows that the proposed measure is lower for models when they are trained using adversarial training approaches",
    "checked": true,
    "id": "17f3ca0e7489faf6b3f53cfa327c7bca2d936e30",
    "semantic_title": "the analysis of the expected change in the classification probability of the predicted label",
    "citation_count": 0,
    "authors": [
      "Ruo Yang",
      "Ping Liu",
      "Mustafa Bilgic"
    ]
  },
  "https://openreview.net/forum?id=NE2xXWo0LF": {
    "title": "Latent State Models of Training Dynamics",
    "volume": "main",
    "abstract": "The impact of randomness on model training is poorly understood. How do differences in data order and initialization actually manifest in the model, such that some training runs outperform others or converge faster? Furthermore, how can we interpret the resulting training dynamics and the phase transitions that characterize different trajectories? To understand the effect of randomness on the dynamics and outcomes of neural network training, we train models multiple times with different random seeds and compute a variety of metrics throughout training, such as the $L_2$ norm, mean, and variance of the neural network's weights. We then fit a hidden Markov model (HMM) over the resulting sequences of metrics. The HMM represents training as a stochastic process of transitions between latent states, providing an intuitive overview of significant changes during training. Using our method, we produce a low-dimensional, discrete representation of training dynamics on grokking tasks, image classification, and masked language modeling. We use the HMM representation to study phase transitions and identify latent \"detour\" states that slow down convergence",
    "checked": true,
    "id": "01bc697a530bf5b15ec0f20b6419947d66af4d83",
    "semantic_title": "latent state models of training dynamics",
    "citation_count": 8,
    "authors": [
      "Michael Y. Hu",
      "Angelica Chen",
      "Naomi Saphra",
      "Kyunghyun Cho"
    ]
  },
  "https://openreview.net/forum?id=o8VgRNYh6n": {
    "title": "Differentially Private Optimizers Can Learn Adversarially Robust Models",
    "volume": "main",
    "abstract": "Machine learning models have shone in a variety of domains and attracted increasing attention from both the security and the privacy communities. One important yet worrying question is: Will training models under the differential privacy (DP) constraint have an unfavorable impact on their adversarial robustness? While previous works have postulated that privacy comes at the cost of worse robustness, we give the first theoretical analysis to show that DP models can indeed be robust and accurate, even sometimes more robust than their naturally-trained non-private counterparts. We observe three key factors that influence the privacy-robustness-accuracy tradeoff: (1) hyper-parameters for DP optimizers are critical; (2) pre-training on public data significantly mitigates the accuracy and robustness drop; (3) choice of DP optimizers makes a difference. With these factors set properly, we achieve 90\\% natural accuracy, 72\\% robust accuracy ($+9\\%$ than the non-private model) under $l_2(0.5)$ attack, and 69\\% robust accuracy ($+16\\%$ than the non-private model) with pre-trained SimCLRv2 model under $l_\\infty(4/255)$ attack on CIFAR10 with $\\epsilon=2$. In fact, we show both theoretically and empirically that DP models are Pareto optimal on the accuracy-robustness tradeoff. Empirically, the robustness of DP models is consistently observed across various datasets and models. We believe our encouraging results are a significant step towards training models that are private as well as robust",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiqi Bu",
      "Yuan Zhang"
    ]
  },
  "https://openreview.net/forum?id=oyfRWeoUJY": {
    "title": "Addressing caveats of neural persistence with deep graph persistence",
    "volume": "main",
    "abstract": "Neural Persistence is a prominent measure for quantifying neural network complexity, proposed in the emerging field of topological data analysis in deep learning. In this work, however, we find both theoretically and empirically that the variance of network weights and spatial concentration of large weights are the main factors that impact neural persistence. Whilst this captures useful information for linear classifiers, we find that no relevant spatial structure is present in later layers of deep neural networks, making neural persistence roughly equivalent to the variance of weights. Additionally, the proposed averaging procedure across layers for deep neural networks does not consider interaction between layers. Based on our analysis, we propose an extension of the filtration underlying neural persistence to the whole neural network instead of single layers, which is equivalent to calculating neural persistence on one particular matrix. This yields our deep graph persistence measure, which implicitly incorporates persistent paths through the network and alleviates variance-related issues through standardisation. Code is available at https://github.com/ExplainableML/Deep-Graph-Persistence",
    "checked": true,
    "id": "926070acfff1ba430944f0773dd41a3d56e15cb3",
    "semantic_title": "addressing caveats of neural persistence with deep graph persistence",
    "citation_count": 1,
    "authors": [
      "Leander Girrbach",
      "Anders Christensen",
      "Ole Winther",
      "Zeynep Akata",
      "A. Sophia Koepke"
    ]
  },
  "https://openreview.net/forum?id=91hfMEUukm": {
    "title": "Replay-enhanced Continual Reinforcement Learning",
    "volume": "main",
    "abstract": "Replaying past experiences has proven to be a highly effective approach for averting catastrophic forgetting in supervised continual learning. However, some crucial factors are still largely ignored, making it vulnerable to serious failure, when used as a solution to forgetting in continual reinforcement learning, even in the context of perfect memory where all data of previous tasks are accessible in the current task. On the one hand, since most reinforcement learning algorithms are not invariant to the reward scale, the previously well-learned tasks (with high rewards) may appear to be more salient to the current learning process than the current task (with small initial rewards). This causes the agent to concentrate on those salient tasks at the expense of generality on the current task. On the other hand, offline learning on replayed tasks while learning a new task may induce a distributional shift between the dataset and the learned policy on old tasks, resulting in forgetting. In this paper, we introduce RECALL, a replay-enhanced method that greatly improves the plasticity of existing replay-based methods on new tasks while effectively avoiding the recurrence of catastrophic forgetting in continual reinforcement learning. RECALL leverages adaptive normalization on approximate targets and policy distillation on old tasks to enhance generality and stability, respectively. Extensive experiments on the Continual World benchmark show that RECALL performs significantly better than purely perfect memory replay, and achieves comparable or better overall performance against state-of-the-art continual learning methods",
    "checked": true,
    "id": "f5bbd862df67bc681957e8c1bc64159c5ac38020",
    "semantic_title": "replay-enhanced continual reinforcement learning",
    "citation_count": 7,
    "authors": [
      "Tiantian Zhang",
      "Kevin Zehua Shen",
      "Zichuan Lin",
      "Bo Yuan",
      "Xueqian Wang",
      "Xiu Li",
      "Deheng Ye"
    ]
  },
  "https://openreview.net/forum?id=JllRdycmLk": {
    "title": "The (Un)Scalability of Informed Heuristic Function Estimation in NP-Hard Search Problems",
    "volume": "main",
    "abstract": "The A* algorithm is commonly used to solve NP-hard combinatorial optimization problems. When provided with a completely informed heuristic function, A* can solve such problems in time complexity that is polynomial in the solution cost and branching factor. In light of this fact, we examine a line of recent publications that propose fitting deep neural networks to the completely informed heuristic function. We assert that these works suffer from inherent scalability limitations since --- under the assumption of NP $\\not \\subseteq$ P/poly --- such approaches result in either (a) network sizes that scale super-polynomially in the instance sizes or (b) the accuracy of the fitted deep neural networks scales inversely with the instance sizes. Complementing our theoretical claims, we provide experimental results for three representative NP-hard search problems. The results suggest that fitting deep neural networks to informed heuristic functions requires network sizes that grow quickly with the problem instance size. We conclude by suggesting that the research community should focus on scalable methods for integrating heuristic search with machine learning, as opposed to methods relying on informed heuristic estimation",
    "checked": true,
    "id": "394bcb1ed8b7ad8d4580ab7e78cfc40ea528d78a",
    "semantic_title": "the (un)scalability of informed heuristic function estimation in np-hard search problems",
    "citation_count": 1,
    "authors": [
      "Sumedh Pendurkar",
      "Taoan Huang",
      "Brendan Juba",
      "Jiapeng Zhang",
      "Sven Koenig",
      "Guni Sharon"
    ]
  },
  "https://openreview.net/forum?id=ndw90pkNM9": {
    "title": "A Combinatorial Semi-Bandit Approach to Charging Station Selection for Electric Vehicles",
    "volume": "main",
    "abstract": "In this work, we address the problem of long-distance navigation for battery electric vehicles (BEVs), where one or more charging sessions are required to reach the intended destination. We consider the availability and performance of the charging stations to be unknown and stochastic, and develop a combinatorial semi-bandit framework for exploring the road network to learn the parameters of the queue time and charging power distributions. Within this framework, we first outline a method for transforming the road network graph into a graph of feasible paths between charging stations to handle the constrained combinatorial optimization problem in an efficient way. Then, for the feasibility graph, we use a Bayesian approach to model the stochastic edge weights, utilizing conjugate priors for the one-parameter exponential and two-parameter gamma distributions, the latter of which is novel to multi-armed bandit literature. Finally, we apply combinatorial versions of Thompson Sampling, BayesUCB and Epsilon-greedy to the problem. We demonstrate the performance of our framework on long-distance navigation problem instances in large-scale country-sized road networks, with simulation experiments in Norway, Sweden and Finland",
    "checked": true,
    "id": "969676d0f6002d57b487d220ba400ee6d90b8e5b",
    "semantic_title": "a combinatorial semi-bandit approach to charging station selection for electric vehicles",
    "citation_count": 0,
    "authors": [
      "Niklas Åkerblom",
      "Morteza Haghir Chehreghani"
    ]
  },
  "https://openreview.net/forum?id=4rkKN4tM63": {
    "title": "Invertible Hierarchical Generative Model for Images",
    "volume": "main",
    "abstract": "Normalizing flows (NFs) as generative models enjoy desirable properties such as exact invertibility and exact likelihood evaluation, while being efficient to sample from. These properties, however, come at the cost of heavy restrictions on the architecture. Due to these limitations, modeling multi-modal probability distributions can yield poor results even with low-dimensional data. Additionally, typical flow architectures employed on real image datasets produce samples with visible aliasing artifacts and limited variation. The latent decomposition of flow-models also falls short on that of competing methods, with uneven contribution to a decoded image. In this work we build an invertible generative model using conditional normalizing flows in a hierarchical fashion to circumvent the aforementioned limitations. We show that we can achieve superior sample quality among flow-based models with fewer parameters compared to the state of the art. We demonstrate ability to control individual levels of detail via the latent decomposition of our model",
    "checked": true,
    "id": "781fde26e007ba48b2759169fb2321a8671344f1",
    "semantic_title": "invertible hierarchical generative model for images",
    "citation_count": 0,
    "authors": [
      "Heikki Timonen",
      "Miika Aittala",
      "Jaakko Lehtinen"
    ]
  },
  "https://openreview.net/forum?id=BxdrpnRHNh": {
    "title": "Using Representation Expressiveness and Learnability to Evaluate Self-Supervised Learning Methods",
    "volume": "main",
    "abstract": "We address the problem of evaluating the quality of self-supervised learning (SSL) models without access to supervised labels, while being agnostic to the architecture, learning algorithm or data manipulation used during training. We argue that representations can be evaluated through the lens of expressiveness and learnability. We propose to use the Intrinsic Dimension (ID) to assess expressiveness and introduce Cluster Learnability (CL) to assess learnability. CL is measured in terms of the performance of a KNN classifier trained to predict labels obtained by clustering the representations with K-means. We thus combine CL and ID into a single predictor – CLID. Through a large-scale empirical study with a diverse family of SSL algorithms, we find that CLID better correlates with in-distribution model performance than other competing recent evaluation schemes. We also benchmark CLID on out-of-domain generalization, where CLID serves as a predictor of the transfer performance of SSL models on several visual classification tasks, yielding improvements with respect to the competing baselines",
    "checked": true,
    "id": "f3f3cacc5e36ab98e719b896c99ca757fee592a8",
    "semantic_title": "using representation expressiveness and learnability to evaluate self-supervised learning methods",
    "citation_count": 0,
    "authors": [
      "Yuchen Lu",
      "Zhen Liu",
      "Aristide Baratin",
      "Romain Laroche",
      "Aaron Courville",
      "Alessandro Sordoni"
    ]
  },
  "https://openreview.net/forum?id=SQnPE63jtA": {
    "title": "Learning Multiscale Non-stationary Causal Structures",
    "volume": "main",
    "abstract": "This paper addresses a gap in the current state of the art by providing a solution for modeling causal relationships that evolve over time and occur at different time scales. Specifically, we introduce the multiscale non-stationary directed acyclic graph (MN-DAG), a framework for modeling multivariate time series data. Our contribution is twofold. Firstly, we expose a probabilistic generative model by leveraging results from spectral and causality theories. Our model allows sampling an MN-DAG according to user-specified priors on the time-dependence and multiscale properties of the causal graph. Secondly, we devise a Bayesian method named Multiscale Non-stationary Causal Structure Learner (MN-CASTLE) that uses stochastic variational inference to estimate MN-DAGs. The method also exploits information from the local partial correlation between time series over different time resolutions. The data generated from an MN-DAG reproduces well-known features of time series in different domains, such as volatility clustering and serial correlation. Additionally, we show the superior performance of MN-CASTLE on synthetic data with different multiscale and non-stationary properties compared to baseline models. Finally, we apply MN-CASTLE to identify the drivers of the natural gas prices in the US market. Causal relationships have strengthened during the COVID-19 outbreak and the Russian invasion of Ukraine, a fact that baseline methods fail to capture. MN-CASTLE identifies the causal impact of critical economic drivers on natural gas prices, such as seasonal factors, economic uncertainty, oil prices, and gas storage deviations",
    "checked": true,
    "id": "7706ef7e213cbb099dfeb1861dae82971de50780",
    "semantic_title": "learning multiscale non-stationary causal structures",
    "citation_count": 4,
    "authors": [
      "Gabriele D'Acunto",
      "Gianmarco De Francisci Morales",
      "Paolo Bajardi",
      "Francesco Bonchi"
    ]
  },
  "https://openreview.net/forum?id=r06xREo3QG": {
    "title": "Bag of Image Patch Embedding Behind the Success of Self-Supervised Learning",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL) has recently achieved tremendous empirical advancements in learning image representation. However, our understanding of the principle behind learning such a representation is still limited. This work shows that joint-embedding SSL approaches learn a representation of image patches, which reflects their co-occurrence. Such a connection to co-occurrence modeling can be established formally, and it supplements the prevailing invariance perspective. We empirically show that learning a representation for fixed-scale patches and aggregating local patch representations as the image representation achieves similar or even better results than the baseline methods. We denote this process as {\\it BagSSL}. Even with $32\\times 32$ patch representation, BagSSL achieves $62\\%$ top-1 linear probing accuracy on ImageNet. On the other hand, with a multi-scale pretrained model, we show that the whole image embedding is approximately the average of local patch embeddings. While the SSL representation is relatively invariant at the global scale, we show that locality is preserved when we zoom into local patch-level representation. Further, we show that patch representation aggregation can improve various SOTA baseline methods by a large margin. The patch representation is considerably easier to understand, and this work makes a step to demystify self-supervised representation learning",
    "checked": true,
    "id": "0bbb60fd0fe2e0ffefacb16fc2c527cb2f01a71e",
    "semantic_title": "bag of image patch embedding behind the success of self-supervised learning",
    "citation_count": 7,
    "authors": [
      "Yubei Chen",
      "Adrien Bardes",
      "ZENGYI LI",
      "Yann LeCun"
    ]
  },
  "https://openreview.net/forum?id=8HQCOMRa7g": {
    "title": "One-Round Active Learning through Data Utility Learning and Proxy Models",
    "volume": "main",
    "abstract": "While active learning (AL) techniques have demonstrated the potential to produce high-performance models with fewer labeled data, their application remains limited due to the necessity for multiple rounds of interaction with annotators. This paper studies the problem of one-round AL, which aims at selecting a subset of unlabeled points and querying their labels \\emph{all at once}. A fundamental challenge is how to measure the utility of different choices of labeling queries for learning a target model. Our key idea is to learn such a utility metric from a small initial labeled set. We demonstrate that our approach leads to state-of-the-art performance on various AL benchmarks and is more robust to the lack of initial labeled data. In addition to algorithmic development and evaluation, we introduce a novel metric for quantifying `\\emph{utility transferability}' -- the degree of correlation between the performance changes of two learning algorithms due to variations in training data selection. Previous studies have often observed a notable utility transferability between models, even those with differing complexities. Such transferability enabled our approach, as well as other techniques such as coresets, hyperparameter tuning, and data valuation, to scale up to more sophisticated target models by substituting them with smaller proxy models. Nevertheless, utility transferability has not yet been rigorously defined within a formal mathematical framework, a gap that our work addresses innovatively. We further propose two Monte Carlo-based methods for efficiently comparing utility transferability for different proxy models, thereby facilitating a more informed selection of proxy models",
    "checked": true,
    "id": "89ffeb90dcdf7823844fbc0dd3a8bb3a60d48b0f",
    "semantic_title": "one-round active learning through data utility learning and proxy models",
    "citation_count": 1,
    "authors": [
      "Jiachen T. Wang",
      "Si Chen",
      "Ruoxi Jia"
    ]
  },
  "https://openreview.net/forum?id=J3veZdVpts": {
    "title": "Bridging the Gap Between Offline and Online Reinforcement Learning Evaluation Methodologies",
    "volume": "main",
    "abstract": "Reinforcement learning (RL) has shown great promise with algorithms learning in environments with large state and action spaces purely from scalar reward signals. A crucial challenge for current deep RL algorithms is that they require a tremendous amount of environment interactions for learning. This can be infeasible in situations where such interactions are expensive, such as in robotics. Offline RL algorithms try to address this issue by bootstrapping the learning process from existing logged data without needing to interact with the environment from the very beginning. While online RL algorithms are typically evaluated as a function of the number of environment interactions, there isn't a single established protocol for evaluating offline RL methods. In this paper, we propose a sequential approach to evaluate offline RL algorithms as a function of the training set size and thus by their data efficiency. Sequential evaluation provides valuable insights into the data efficiency of the learning process and the robustness of algorithms to distribution changes in the dataset while also harmonizing the visualization of the offline and online learning phases. Our approach is generally applicable and easy to implement. We compare several existing offline RL algorithms using this approach and present insights from a variety of tasks and offline datasets",
    "checked": true,
    "id": "0f1947fe9ae67eccd2c09057ab00553ebf8c318b",
    "semantic_title": "bridging the gap between offline and online reinforcement learning evaluation methodologies",
    "citation_count": 1,
    "authors": [
      "Shivakanth Sujit",
      "Pedro Braga",
      "Jorg Bornschein",
      "Samira Ebrahimi Kahou"
    ]
  },
  "https://openreview.net/forum?id=hjYmsV6nXZ": {
    "title": "RLTF: Reinforcement Learning from Unit Test Feedback",
    "volume": "main",
    "abstract": "The goal of program synthesis, or code generation, is to generate executable code based on given descriptions. Recently, there has been an increasing number of studies employing reinforcement learning (RL) to improve the performance of large language models (LLMs) for code. However, some of the current representative RL methods have only used offline frameworks, limiting the exploration of new sample spaces. Additionally, the utilization of unit test signals is limited, not accounting for specific error locations within the code. To address these issues, we proposed RLTF, i.e., Reinforcement Learning from Unit Test Feedback, a novel online RL framework with unit test feedback of multi-granularity for refining code LLMs. Our approach generates data in real-time during training and simultaneously utilizes fine-grained feedback signals to guide the model towards producing higher-quality code. Extensive experiments show that RLTF achieves state-of-the-art performance on the APPS and the MBPP benchmarks. Our code is available at: \\url{https://github.com/Zyq-scut/RLTF}",
    "checked": true,
    "id": "a669ea57529f4db630043c8c75d8f840c485d24d",
    "semantic_title": "rltf: reinforcement learning from unit test feedback",
    "citation_count": 62,
    "authors": [
      "Jiate Liu",
      "Yiqin Zhu",
      "Kaiwen Xiao",
      "QIANG FU",
      "Xiao Han",
      "Yang Wei",
      "Deheng Ye"
    ]
  },
  "https://openreview.net/forum?id=ZSxvyWrX6k": {
    "title": "Visualizing the Diversity of Representations Learned by Bayesian Neural Networks",
    "volume": "main",
    "abstract": "Explainable Artificial Intelligence (XAI) aims to make learning machines less opaque, and offers researchers and practitioners various tools to reveal the decision-making strategies of neural networks. In this work, we investigate how XAI methods can be used for exploring and visualizing the diversity of feature representations learned by Bayesian Neural Networks (BNNs). Our goal is to provide a global understanding of BNNs by making their decision-making strategies a) visible and tangible through feature visualizations and b) quantitatively measurable with a distance measure learned by contrastive learning. Our work provides new insights into the posterior distribution in terms of human-understandable feature information with regard to the underlying decision-making strategies. The main findings of our work are the following: 1) global XAI methods can be applied to explain the diversity of decision-making strategies of BNN instances, 2) Monte Carlo dropout with commonly used Dropout rates exhibit increased diversity in feature representations compared to the multimodal posterior approximation of MultiSWAG, 3) the diversity of learned feature representations highly correlates with the uncertainty estimate for the output and 4) the inter-mode diversity of the multimodal posterior decreases as the network width increases, while the intra-mode diversity increases. These findings are consistent with the recent Deep Neural Networks theory, providing additional intuitions about what the theory implies in terms of humanly understandable concepts",
    "checked": true,
    "id": "bd1486c78d5a16b544e6d9d014d96681fc7dd56b",
    "semantic_title": "visualizing the diversity of representations learned by bayesian neural networks",
    "citation_count": 5,
    "authors": [
      "Dennis Grinwald",
      "Kirill Bykov",
      "Shinichi Nakajima",
      "Marina MC Höhne"
    ]
  },
  "https://openreview.net/forum?id=cdRYoTyHZh": {
    "title": "Automated Detection of Causal Inference Opportunities: Regression Discontinuity Subgroup Discovery",
    "volume": "main",
    "abstract": "The gold standard for the identification of causal effects are randomized controlled trials (RCT), but RCTs may not always be feasible to conduct. When treatments depend on a threshold however, such as the blood sugar threshold for diabetes diagnosis, we can still sometimes estimate causal effects with regression discontinuities (RDs). RDs are valid when units just above and below the threshold have the same distribution of covariates and thus no confounding in the presence of noise, establishing an as-if randomization. In practice however, implementing RD studies can be difficult as identifying treatment thresholds require considerable domain expertise -- furthermore, the thresholds may differ across subgroups (e.g., the blood sugar threshold for diabetes may differ across demographics), and ignoring these differences can lower statistical power. Finding the thresholds and to whom they apply is an important problem currently solved manually by domain experts, and data-driven approaches are needed when domain expertise is not sufficient. Here, we introduce Regression Discontinuity SubGroup Discovery (RDSGD), a machine-learning method that identifies statistically powerful and interpretable subgroups for RD thresholds. Using a medical claims dataset with over 60 million patients, we apply RDSGD to multiple clinical contexts and identify subgroups with increased compliance to treatment assignment thresholds. As treatment thresholds matter for many diseases and policy decisions, RDSGD can be a powerful tool for discovering new avenues for causal estimation",
    "checked": true,
    "id": "64e1ed0941ed89fcf8a02d5a7d0a3935495c2dfb",
    "semantic_title": "automated detection of causal inference opportunities: regression discontinuity subgroup discovery",
    "citation_count": 0,
    "authors": [
      "Tony Liu",
      "Patrick Lawlor",
      "Lyle Ungar",
      "Konrad Kording",
      "Rahul Ladhania"
    ]
  },
  "https://openreview.net/forum?id=A9yn7KTwsK": {
    "title": "Invariant Structure Learning for Better Generalization and Causal Explainability",
    "volume": "main",
    "abstract": "Learning the causal structure behind data is invaluable for improving generalization and ob- taining high-quality explanations. Towards this end, we propose a novel framework, Invariant Structure Learning (ISL), that is designed to improve causal structure discovery by utilizing generalization as an indication in the process. ISL splits the data into different environments, and learns a structure that is invariant to the target across different environments by imposing a consistency constraint. The proposed aggregation mechanism then selects the classifier based on a graph structure that reflects the causal mechanisms in the data more accurately compared to the structures learnt from individual environments. Furthermore, we extend ISL to a self-supervised learning setting, where accurate causal structure discovery does not rely on any labels. Self-supervised ISL utilizes proposals for invariant causality, by iteratively setting different nodes as targets. On synthetic and real-world datasets, we demonstrate that ISL accurately discovers the causal structure, outperforms alternative methods, and yields superior generalization for datasets with significant distribution shifts",
    "checked": true,
    "id": "edc07047cd45e90aff0556ab10b16740a8110e61",
    "semantic_title": "invariant structure learning for better generalization and causal explainability",
    "citation_count": 2,
    "authors": [
      "Yunhao Ge",
      "Sercan O Arik",
      "Jinsung Yoon",
      "Ao Xu",
      "Laurent Itti",
      "Tomas Pfister"
    ]
  },
  "https://openreview.net/forum?id=iRTL4pDavo": {
    "title": "Data pruning and neural scaling laws: fundamental limitations of score-based algorithms",
    "volume": "main",
    "abstract": "Data pruning algorithms are commonly used to reduce the memory and computational cost of the optimization process. Recent empirical results (Guo, B. Zhao, and Bai, 2022) reveal that random data pruning remains a strong baseline and outperforms most existing data pruning methods in the high compression regime, i.e., where a fraction of 30% or less of the data is kept. This regime has recently attracted a lot of interest as a result of the role of data pruning in improving the so-called neural scaling laws; see (Sorscher et al., 2022), where the authors showed the need for high-quality data pruning algorithms in order to beat the sample power law. In this work, we focus on score-based data pruning algorithms and show theoretically and empirically why such algorithms fail in the high compression regime. We demonstrate \"No Free Lunch\" theorems for data pruning and discuss potential solutions to these limitations",
    "checked": true,
    "id": "bb4721b1a806ac00308bfb174edf3c36b6f0b620",
    "semantic_title": "data pruning and neural scaling laws: fundamental limitations of score-based algorithms",
    "citation_count": 10,
    "authors": [
      "Fadhel Ayed",
      "Soufiane Hayou"
    ]
  },
  "https://openreview.net/forum?id=AfXq3x3X16": {
    "title": "Offline Reinforcement Learning with Additional Covering Distributions",
    "volume": "main",
    "abstract": "We study learning optimal policies from a logged dataset, i.e., offline RL, with function general approximation. Despite the efforts devoted, existing algorithms with theoretic finite-sample guarantees typically assume exploratory data coverage or strong realizable function classes (e.g., Bellman-completeness), which is hard to be satisfied in reality. While there are recent works that successfully tackle these strong assumptions, they either require the gap assumptions that could only be satisfied by part of MDPs or use the behavior regularization that makes the optimality of learned policy even intractable. To solve this challenge, we provide finite-sample guarantees for a simple algorithm based on marginalized importance sampling (MIS), showing that sample-efficient offline RL for general MDPs is possible with only a partial coverage dataset (instead of assuming a dataset covering all possible policies) and weak realizable function classes (assuming function classes containing simply one function) given additional side information of a covering distribution. We demonstrate that the covering distribution trades off prior knowledge of the optimal trajectories against the coverage requirement of the dataset, revealing the effect of this inductive bias in the learning processes. Furthermore, when considering the exploratory dataset, our analysis shows that only realizable function classes are enough for learning near-optimal policies, even with no side information on the additional coverage distributions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenjie Mao"
    ]
  },
  "https://openreview.net/forum?id=23WZFQBUh5": {
    "title": "Online model selection by learning how compositional kernels evolve",
    "volume": "main",
    "abstract": "Motivated by the need for efficient, personalized learning in health, we investigate the problem of online compositional kernel selection for multi-task Gaussian Process regression. Existing composition selection methods do not satisfy our strict criteria in health; selection must occur quickly, and the selected kernels must maintain the appropriate level of complexity, sparsity, and stability as data arrives online. We introduce the Kernel Evolution Model (KEM), a generative process on how to evolve kernel compositions in a way that manages the bias--variance trade-off as we observe more data about a user. Using pilot data, we learn a set of kernel evolutions that can be used to quickly select kernels for new test users. KEM reliably selects high-performing kernels for a range of synthetic and real data sets, including two health data sets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eura Shin",
      "Predrag Klasnja",
      "Susan Murphy",
      "Finale Doshi-Velez"
    ]
  },
  "https://openreview.net/forum?id=EjqopDxLbG": {
    "title": "NOFLITE: Learning to Predict Individual Treatment Effect Distributions",
    "volume": "main",
    "abstract": "Estimating the effect of a treatment on an individual's outcome of interest is an important challenge in various fields, such as healthcare, economics, marketing, and education. Previous work in machine learning has focused on estimating the expected value of the treatment effect. However, effective personalized decision-making requires more than just the treatment expected effect; it requires knowing the entire treatment effect distribution. Knowing this distribution allows analyzing the treatment's expected utility or quantifying the uncertainty regarding a treatment's effect. This information is essential for prescribing optimal treatments. The ability of a model to predict accurate individual treatment effect distributions is captured by its likelihood. In light of this, we propose a novel neural architecture, NOFLITE, that uses normalizing flows to directly optimize this likelihood, while simultaneously learning flexible estimates of the individual treatment effect distribution. Experiments on various semi-synthetic data sets show that NOFLITE outperforms existing methods in terms of loglikelihood. Moreover, we illustrate how the predicted distributions can enable an in-depth analysis of the treatment effect and more accurate decision-making",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Toon Vanderschueren",
      "Jeroen Berrevoets",
      "Wouter Verbeke"
    ]
  },
  "https://openreview.net/forum?id=28bQiPWxHl": {
    "title": "Stochastic Mirror Descent: Convergence Analysis and Adaptive Variants via the Mirror Stochastic Polyak Stepsize",
    "volume": "main",
    "abstract": "We investigate the convergence of stochastic mirror descent (SMD) under interpolation in relatively smooth and smooth convex optimization. In relatively smooth convex optimization we provide new convergence guarantees for SMD with a constant stepsize. For smooth convex optimization we propose a new adaptive stepsize scheme --- the mirror stochastic Polyak stepsize (mSPS). Notably, our convergence results in both settings do not make bounded gradient assumptions or bounded variance assumptions, and we show convergence to a neighborhood that vanishes under interpolation. Consequently, these results correspond to the first convergence guarantees under interpolation for the exponentiated gradient algorithm for fixed or adaptive stepsizes. mSPS generalizes the recently proposed stochastic Polyak stepsize (SPS) (Loizou et al. 2021) to mirror descent and remains both practical and efficient for modern machine learning applications while inheriting the benefits of mirror descent. We complement our results with experiments across various supervised learning tasks and different instances of SMD, demonstrating the effectiveness of mSPS",
    "checked": true,
    "id": "f866b44965117173ced03178fe685fa8d26b2f67",
    "semantic_title": "stochastic mirror descent: convergence analysis and adaptive variants via the mirror stochastic polyak stepsize",
    "citation_count": 31,
    "authors": [
      "Ryan D'Orazio",
      "Nicolas Loizou",
      "Issam H. Laradji",
      "Ioannis Mitliagkas"
    ]
  },
  "https://openreview.net/forum?id=B0uBSSUy0G": {
    "title": "Provably Personalized and Robust Federated Learning",
    "volume": "main",
    "abstract": "Clustering clients with similar objectives and learning a model per cluster is an intuitive and interpretable approach to personalization in federated learning. However, doing so with provable and optimal guarantees has remained an open challenge. In this work, we formalize personalized federated learning as a stochastic optimization problem. We propose simple clustering-based algorithms which iteratively identify and train within clusters, using local client gradients. Our algorithms have optimal convergence rates which asymptotically match those obtained if we knew the true underlying clustering of the clients, and are provably robust in the Byzantine setting where some fraction of the clients are malicious",
    "checked": true,
    "id": "235f66ac870ee8ede8f131fb0da00472545febab",
    "semantic_title": "provably personalized and robust federated learning",
    "citation_count": 11,
    "authors": [
      "Mariel Werner",
      "Lie He",
      "Michael Jordan",
      "Martin Jaggi",
      "Sai Praneeth Karimireddy"
    ]
  },
  "https://openreview.net/forum?id=I5sJ6PU6JN": {
    "title": "Conditional Sampling of Variational Autoencoders via Iterated Approximate Ancestral Sampling",
    "volume": "main",
    "abstract": "Conditional sampling of variational autoencoders (VAEs) is needed in various applications, such as missing data imputation, but is computationally intractable. A principled choice for asymptotically exact conditional sampling is Metropolis-within-Gibbs (MWG). However, we observe that the tendency of VAEs to learn a structured latent space, a commonly desired property, can cause the MWG sampler to get \"stuck\" far from the target distribution. This paper mitigates the limitations of MWG: we systematically outline the pitfalls in the context of VAEs, propose two original methods that address these pitfalls, and demonstrate an improved performance of the proposed methods on a set of sampling tasks",
    "checked": true,
    "id": "eed436b929b954eab3e9aa856dde0a9428f9dbe5",
    "semantic_title": "conditional sampling of variational autoencoders via iterated approximate ancestral sampling",
    "citation_count": 3,
    "authors": [
      "Vaidotas Simkus",
      "Michael U. Gutmann"
    ]
  },
  "https://openreview.net/forum?id=dn3ZkqG2YV": {
    "title": "Rewiring with Positional Encodings for Graph Neural Networks",
    "volume": "main",
    "abstract": "Several recent works use positional encodings to extend the receptive fields of graph neural network (GNN) layers equipped with attention mechanisms. These techniques, however, extend receptive fields to the complete graph, at substantial computational cost and risking a change in the inductive biases of conventional GNNs, or require complex architecture adjustments. As a conservative alternative, we use positional encodings to expand receptive fields to r-hop neighborhoods. More specifically, our method augments the input graph with additional nodes/edges and uses positional encodings as node and/or edge features. We thus modify graphs before inputting them to a downstream GNN model, instead of modifying the model itself. This makes our method model-agnostic, i.e., compatible with any of the existing GNN architectures. We also provide examples of positional encodings that are lossless with a one-to-one map between the original and the modified graphs. We demonstrate that extending receptive fields via positional encodings and a virtual fully- connected node significantly improves GNN performance and alleviates over-squashing using small r. We obtain improvements on a variety of models and datasets and reach competitive performance using traditional GNNs or graph Transformers",
    "checked": true,
    "id": "002390988f7157b425ac7e5dc42f3d06eca6ede7",
    "semantic_title": "rewiring with positional encodings for graph neural networks",
    "citation_count": 33,
    "authors": [
      "Rickard Brüel Gabrielsson",
      "Mikhail Yurochkin",
      "Justin Solomon"
    ]
  },
  "https://openreview.net/forum?id=leqr0vQzeN": {
    "title": "A Robust Backpropagation-Free Framework for Images",
    "volume": "main",
    "abstract": "While current deep learning algorithms have been successful for a wide variety of artificial intelligence (AI) tasks, including those involving structured image data, they present deep neurophysiological conceptual issues due to their reliance on the gradients that are computed by backpropagation of errors (backprop). Gradients are required to obtain synaptic weight adjustments but require knowledge of feed forward activities in order to conduct backward propagation, a biologically implausible process. This is known as the \"weight transport problem''. Therefore, in this work, we present a more biologically plausible approach towards solving the weight transport problem for image data. This approach, which we name the error-kernel driven activation alignment (EKDAA) algorithm, accomplishes through the introduction of locally derived error transmission kernels and error maps. Like standard deep learning networks, EKDAA performs the standard forward process via weights and activation functions; however, its backward error computation involves adaptive error kernels that propagate local error signals through the network. The efficacy of EKDAA is demonstrated by performing visual-recognition tasks on the Fashion MNIST, CIFAR-10 and SVHN benchmarks, along with demonstrating its ability to extract visual features from natural color images. Furthermore, in order to demonstrate its non-reliance on gradient computations, results are presented for an EKDAA-trained CNN that employs a non-differentiable activation function",
    "checked": true,
    "id": "f191c831918b1616f734da071d54de2ddd745b89",
    "semantic_title": "a robust backpropagation-free framework for images",
    "citation_count": 1,
    "authors": [
      "Timothy Zee",
      "Alex Ororbia",
      "Ankur Mali",
      "Ifeoma Nwogu"
    ]
  },
  "https://openreview.net/forum?id=65AzNvY73Q": {
    "title": "Minorization-Maximization for Learning Determinantal Point Processes",
    "volume": "main",
    "abstract": "A determinantal point process (DPP) is a powerful probabilistic model that generates diverse random subsets from a ground set. Since a DPP is characterized by a positive definite kernel, a DPP on a finite ground set can be parameterized by a kernel matrix. Recently, DPPs have gained attention in the machine learning community and have been applied to various practical problems; however, there is still room for further research on the learning of DPPs. In this paper, we propose a simple learning rule for full-rank DPPs based on a minorization-maximization (MM) algorithm, which monotonically increases the likelihood in each iteration. We show that our minorizer of the MM algorithm provides a tighter lower-bound compared to an existing method locally. We also generalize the algorithm for further acceleration. In our experiments on both synthetic and real-world datasets, our method outperforms existing methods in most settings. Our code is available at https://github.com/ISMHinoLab/DPPMMEstimation",
    "checked": true,
    "id": "05272f41369a00407a5547140cb3a45ee53cc025",
    "semantic_title": "minorization-maximization for learning determinantal point processes",
    "citation_count": 2,
    "authors": [
      "Takahiro Kawashima",
      "Hideitsu Hino"
    ]
  },
  "https://openreview.net/forum?id=gKEbBKRUjA": {
    "title": "Understanding Curriculum Learning in Policy Optimization for Online Combinatorial Optimization",
    "volume": "main",
    "abstract": "Over the recent years, reinforcement learning (RL) starts to show promising results in tackling combinatorial optimization (CO) problems, in particular when coupled with curriculum learning to facilitate training. Despite emerging empirical evidence, theoretical study on why RL helps is still at its early stage. This paper presents the first systematic study on policy optimization methods for online CO problems. We show that online CO problems can be naturally formulated as latent Markov Decision Processes (LMDPs), and prove convergence bounds on natural policy gradient (NPG) for solving LMDPs. Furthermore, our theory explains the benefit of curriculum learning: it can find a strong sampling policy and reduce the distribution shift, a critical quantity that governs the convergence rate in our theorem. For a canonical online CO problem, the Best Choice Problem (BCP), we formally prove that distribution shift is reduced exponentially with curriculum learning even if the curriculum is a randomly generated BCP on a smaller scale. Our theory also shows we can simplify the curriculum learning scheme used in prior work from multi-step to single-step. Lastly, we provide extensive experiments on the Best Choice Problem, Online Knapsack, and AdWords to verify our findings",
    "checked": true,
    "id": "396e224a94202c253a6201d670724041f9cb33a3",
    "semantic_title": "understanding curriculum learning in policy optimization for online combinatorial optimization",
    "citation_count": 3,
    "authors": [
      "Runlong Zhou",
      "Zelin He",
      "Yuandong Tian",
      "Yi Wu",
      "Simon Shaolei Du"
    ]
  },
  "https://openreview.net/forum?id=BxjHMPwZIH": {
    "title": "Training DNNs Resilient to Adversarial and Random Bit-Flips by Learning Quantization Ranges",
    "volume": "main",
    "abstract": "Promoting robustness in deep neural networks (DNNs) is crucial for their reliable deployment in uncertain environments, such as low-power settings or in the presence of adversarial attacks. In particular, bit-flip weight perturbations in quantized networks can significantly degrade performance, underscoring the need to improve DNN resilience. In this paper, we introduce a training mechanism to learn the quantization range of different DNN layers to enhance DNN robustness against bit-flip errors on the model parameters. The proposed approach, called weight clipping-aware training (WCAT), minimizes the quantization range while preserving performance, striking a balance between the two. Our experimental results on different models and datasets showcase that DNNs trained with WCAT can tolerate a high amount of noise while keeping the accuracy close to the baseline model. Moreover, we show that our method significantly enhances DNN robustness against adversarial bit-flip attacks. Finally, when considering the energy-reliability trade-off inherent in on-chip SRAM memories, we observe that WCAT consistently improves the Pareto frontier of test accuracy and energy consumption across diverse models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kamran Chitsaz",
      "Goncalo Mordido",
      "Jean-Pierre David",
      "François Leduc-Primeau"
    ]
  },
  "https://openreview.net/forum?id=j4y3gN7VtW": {
    "title": "Feature-Attending Recurrent Modules for Generalization in Reinforcement Learning",
    "volume": "main",
    "abstract": "Many important tasks are defined in terms of object. To generalize across these tasks, a reinforcement learning (RL) agent needs to exploit the structure that the objects induce. Prior work has either hard-coded object-centric features, used complex object-centric generative models, or updated state using local spatial features. However, these approaches have had limited success in enabling general RL agents. Motivated by this, we introduce \"Feature- Attending Recurrent Modules\" (FARM), an architecture for learning state representations that relies on simple, broadly applicable inductive biases for capturing spatial and temporal regularities. FARM learns a state representation that is distributed across multiple modules that each attend to spatiotemporal features with an expressive feature attention mechanism. We show that this improves an RL agent's ability to generalize across object-centric tasks. We study task suites in both 2D and 3D environments and find that FARM better generalizes compared to competing architectures that leverage attention or multiple modules",
    "checked": true,
    "id": "7365d4b97e38ca7fcd8ac3db194854b59def1d42",
    "semantic_title": "feature-attending recurrent modules for generalization in reinforcement learning",
    "citation_count": 7,
    "authors": [
      "Wilka Torrico Carvalho",
      "Andrew Kyle Lampinen",
      "Kyriacos Nikiforou",
      "Felix Hill",
      "Murray Shanahan"
    ]
  },
  "https://openreview.net/forum?id=5Y04GWvoJu": {
    "title": "Achieving Risk Control in Online Learning Settings",
    "volume": "main",
    "abstract": "To provide rigorous uncertainty quantification for online learning models, we develop a framework for constructing uncertainty sets that provably control risk---such as coverage of confidence intervals, false negative rate, or F1 score---in the online setting. This extends conformal prediction to apply to a larger class of online learning problems. Our method guarantees risk control at any user-specified level even when the underlying data distribution shifts drastically, even adversarially, over time in an unknown fashion. The technique we propose is highly flexible as it can be applied with any base online learning algorithm (e.g., a deep neural network trained online), requiring minimal implementation effort and essentially zero additional computational cost. We further extend our approach to control multiple risks simultaneously, so the prediction sets we generate are valid for all given risks. To demonstrate the utility of our method, we conduct experiments on real-world tabular time-series data sets showing that the proposed method rigorously controls various natural risks. Furthermore, we show how to construct valid intervals for an online image-depth estimation problem that previous sequential calibration schemes cannot handle",
    "checked": true,
    "id": "222e7926d07384f104016a7e2264978fedfcdeda",
    "semantic_title": "achieving risk control in online learning settings",
    "citation_count": 30,
    "authors": [
      "Shai Feldman",
      "Liran Ringel",
      "Stephen Bates",
      "Yaniv Romano"
    ]
  },
  "https://openreview.net/forum?id=1kl4YM2Q7P": {
    "title": "Exploring Transformer Backbones for Heterogeneous Treatment Effect Estimation",
    "volume": "main",
    "abstract": "Previous works on Treatment Effect Estimation (TEE) are not in widespread use because they are predominantly theoretical, where strong parametric assumptions are made but untractable for practical application. Recent works use Multilayer Perceptron (MLP) for modeling casual relationships, however, MLPs lag far behind recent advances in ML methodology, which limits their applicability and generalizability. To extend beyond the single domain formulation and towards more realistic learning scenarios, we explore model design spaces beyond MLPs, i.e., transformer backbones, which provide flexibility where attention layers govern interactions among treatments and covariates to exploit structural similarities of potential outcomes for confounding control. Through careful model design, Transformers as Treatment Effect Estimators (TransTEE) is proposed. We show empirically that TransTEE can: (1) serve as a general-purpose treatment effect estimator which significantly outperforms competitive baselines on a variety of challenging TEE problems (e.g., discrete, continuous, structured, or dosage-associated treatments.) and is applicable to both when covariates are tabular and when they consist of structural data (e.g., texts, graphs); (2) yield multiple advantages: compatibility with propensity score modeling, parameter efficiency, robustness to continuous treatment value distribution shifts, explainable in covariate adjustment, and real-world utility in auditing pre-trained language models",
    "checked": true,
    "id": "0109c662c101723aea99e561937e3aca58563537",
    "semantic_title": "exploring transformer backbones for heterogeneous treatment effect estimation",
    "citation_count": 30,
    "authors": [
      "YiFan Zhang",
      "Hanlin Zhang",
      "Zachary Chase Lipton",
      "Li Erran Li",
      "Eric Xing"
    ]
  },
  "https://openreview.net/forum?id=ok18jj7cam": {
    "title": "GraphPNAS: Learning Probabilistic Graph Generators for Neural Architecture Search",
    "volume": "main",
    "abstract": "Neural architectures can be naturally viewed as computational graphs. Motivated by this perspective, we, in this paper, study neural architecture search (NAS) through the lens of learning graph generative models. In contrast to existing NAS methods which largely focus on searching for a single best architecture, i.e, point estimation, we propose GraphPNAS a deep graph generative model that learns a distribution of well-performing architectures. Relying on graph neural networks (GNNs), our GraphPNAS can better capture topologies of good neural architectures and relations between operators therein. Moreover, our graph generator leads to a learnable probabilistic search method that is more flexible and efficient than the commonly used RNN generator and random search methods. Finally, we learn our generator via an efficient reinforcement learning formulation for NAS. To assess the effectiveness of our GraphPNAS, we conduct extensive experiments on four search spaces, including the challenging RandWire on TinyImageNet, ENAS on CIFAR10, and NAS-Bench-101/201. We show that our proposed graph generator consistently outperforms RNN-based one and achieves better or comparable performances than state-of-the-art NAS methods",
    "checked": true,
    "id": "33291bc4488b2a087bf50d8c1d58af3e7c535884",
    "semantic_title": "graphpnas: learning probabilistic graph generators for neural architecture search",
    "citation_count": 2,
    "authors": [
      "Muchen Li",
      "Jeffrey Yunfan Liu",
      "Leonid Sigal",
      "Renjie Liao"
    ]
  },
  "https://openreview.net/forum?id=jLJTqJXAG7": {
    "title": "Federated Learning under Partially Disjoint Data via Manifold Reshaping",
    "volume": "main",
    "abstract": "Statistical heterogeneity severely limits the performance of federated learning (FL), motivating several explorations e.g., FedProx, MOON and FedDyn, to alleviate this problem. Despite effectiveness, their considered scenario generally requires samples from almost all classes during the local training of each client, although some covariate shifts may exist among clients. In fact, the natural case of partially class-disjoint data (PCDD), where each client contributes a few classes (instead of all classes) of samples, is practical yet underexplored. Specifically, the unique collapse and invasion characteristics of PCDD can induce the biased optimization direction in local training, which prevents the efficiency of federated learning. To address this dilemma, we propose a manifold reshaping approach called FedMR to calibrate the feature space of local training. Our FedMR adds two interplaying losses to the vanilla federated learning: one is the intra-class loss to decorrelate feature dimensions for anti-collapse; and the other one is the inter-class loss to guarantee the proper margin among categories in the feature expansion. We conduct extensive experiments on a range of datasets to demonstrate that our FedMR achieves much higher accuracy and better communication efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziqing Fan",
      "Jiangchao Yao",
      "Ruipeng Zhang",
      "Lingjuan Lyu",
      "Yanfeng Wang",
      "Ya Zhang"
    ]
  },
  "https://openreview.net/forum?id=DlRsoxjyPm": {
    "title": "Synthetic Data from Diffusion Models Improves ImageNet Classification",
    "volume": "main",
    "abstract": "Deep generative models are becoming increasingly powerful, now generating diverse, high fidelity, photo-realistic samples given text prompts. Nevertheless, samples from such models have not been shown to significantly improve model training for challenging and well-studied discriminative tasks like ImageNet classification. In this paper we show that augmenting the ImageNet training set with samples from a generative diffusion model can yield substantial improvements in ImageNet classification accuracy over strong ResNet and Vision Transformer baselines. To this end we explore the fine-tuning of large-scale text-to-image diffusion models, yielding class-conditional ImageNet models with state-of-the-art FID score (1.76 at 256×256 resolution) and Inception Score (239 at 256×256). The model also yields a new state-of-the-art in Classification Accuracy Scores, i.e., ImageNet test accuracy for a ResNet-50 architecture trained solely on synthetic data (64.96 top-1 accuracy for 256×256 samples, improving to 69.24 for 1024×1024 samples). Adding up to three times as many synthetic samples as real training samples consistently improves ImageNet classification accuracy across multiple architectures",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shekoofeh Azizi",
      "Simon Kornblith",
      "Chitwan Saharia",
      "Mohammad Norouzi",
      "David J. Fleet"
    ]
  },
  "https://openreview.net/forum?id=f3JLnnZsAm": {
    "title": "ILPO-MP: Mode Priors Prevent Mode Collapse when Imitating Latent Policies from Observations",
    "volume": "main",
    "abstract": "Imitation learning from observations (IfO) constrains the classic imitation learning setting to cases where expert observations are easy to obtain, but no expert actions are available. Most existing IfO methods require access to task-specific cost functions or many interactions with the target environment. Learning a forward dynamics model in combination with a latent policy has been shown to solve these issues. However, the limited supervision in the IfO scenario can lead to mode collapse when learning the generative forward dynamics model and the corresponding latent policy. In this paper, we analyze the mode collapse problem in this setting and show that it is caused by a combination of deterministic expert data and bad initialization of the models. Under the assumption of piecewise continuous system dynamics, we propose ILPO-MP, a method to prevent the mode collapse using clustering of expert transitions to impose a mode prior on the generative model and the latent policy. We show that ILPO-MP prevents mode collapse and improves performance in a variety of environments",
    "checked": true,
    "id": "dc1a8e8f9624c4004ee4968b071db6f5aabd0760",
    "semantic_title": "ilpo-mp: mode priors prevent mode collapse when imitating latent policies from observations",
    "citation_count": 2,
    "authors": [
      "Oliver Struckmeier",
      "Ville Kyrki"
    ]
  },
  "https://openreview.net/forum?id=g1B4qgOw79": {
    "title": "Complementary Sparsity: Accelerating Sparse CNNs with High Accuracy on General-Purpose Computing Platforms",
    "volume": "main",
    "abstract": "Model sparsity is a promising approach to reducing parameters or FLOPs of convolutional neural networks (CNNs). Compared to unstructured or coarse-grained structured sparsity, fine-grained structured sparsity, e.g., N:M sparse pattern, can achieve a better balance between accuracy and efficiency on general computing platforms like CPUs and GPUs. In particular, the 2:4 sparsity can accelerate CNN inference by 2$\\times$ speed and with negligible accuracy drop. However, N:M sparsity needs to be supported by GPU within specific hardware circuits and hardly achieves significant speedups on common GPUs. To accelerate CNNs with general-purposed computing resources and simultaneously retain the model accuracy as much as possible, this paper proposes complementary sparsity (CS). CS denotes that only one weight can be retained for weights spaced at the same distance. On the one hand, CS features high mask flexibility, which is naturally favorable to high model accuracy. Moreover, we propose a CS-specific sparse training method to improve CS-based CNNs' accuracy under high parameter sparsities ($>$75\\%). On the other hand, CS itself is memory-access balanced and robust to pattern hyperparameters, which can be utilized to speedup CS-based convolution computation on CPUs and common GPUs. We thus propose a CS convolution parallel computing algorithm that adapts to common GPUs without sparse tensor cores. Experimental results show that compared to other sparsity patterns, the proposed CS can achieve the optimal trade-off in terms of accuracy and latency for CPUs and common GPUs, respectively. Codes will be available at https://gitee.com/mindspore/models/tree/master/research/cv/CS",
    "checked": true,
    "id": "e6592ed83ee30ba3bb651604b8cbc457834cf1c4",
    "semantic_title": "complementary sparsity: accelerating sparse cnns with high accuracy on general-purpose computing platforms",
    "citation_count": 0,
    "authors": [
      "Kang Zhao",
      "Yijun Tan",
      "Kai Han",
      "Ting Hu",
      "Hanting Chen",
      "Tao Yuan",
      "Yunhe Wang",
      "Jun Yao"
    ]
  },
  "https://openreview.net/forum?id=JYs1R9IMJr": {
    "title": "Finding Neurons in a Haystack: Case Studies with Sparse Probing",
    "volume": "main",
    "abstract": "Despite rapid adoption and deployment of large language models (LLMs), the internal computations of these models remain opaque and poorly understood. In this work, we seek to understand how high-level human-interpretable features are represented within the internal neuron activations of LLMs. We train $k$-sparse linear classifiers (probes) on these internal activations to predict the presence of features in the input; by varying the value of $k$ we study the sparsity of learned representations and how this varies with model scale. With $k=1$, we localize individual neurons that are highly relevant for a particular feature and perform a number of case studies to illustrate general properties of LLMs. In particular, we show that early layers make use of sparse combinations of neurons to represent many features in superposition, that middle layers have seemingly dedicated neurons to represent higher-level contextual features, and that increasing scale causes representational sparsity to increase on average, but there are multiple types of scaling dynamics. In all, we probe for over 100 unique features comprising 10 different categories in 7 different models spanning 70 million to 6.9 billion parameters",
    "checked": true,
    "id": "12910786da7a34c9ee26798fd81b0ed7b0e38789",
    "semantic_title": "finding neurons in a haystack: case studies with sparse probing",
    "citation_count": 218,
    "authors": [
      "Wes Gurnee",
      "Neel Nanda",
      "Matthew Pauly",
      "Katherine Harvey",
      "Dmitrii Troitskii",
      "Dimitris Bertsimas"
    ]
  },
  "https://openreview.net/forum?id=m8U9rSs6gU": {
    "title": "Inducing Meaningful Units from Character Sequences with Dynamic Capacity Slot Attention",
    "volume": "main",
    "abstract": "Characters do not convey meaning, but sequences of characters do. We propose an unsupervised distributional method to learn the abstract meaning-bearing units in a sequence of characters. Rather than segmenting the sequence, our Dynamic Capacity Slot Attention model discovers continuous representations of the objects in the sequence, extending an architecture for object discovery in images. We train our model on different languages and evaluate the quality of the obtained representations with forward and reverse probing classifiers. These experiments show that our model succeeds in discovering units which are similar to those proposed previously in form, content, and level of abstraction, and which show promise for capturing meaningful information at a higher level of abstraction",
    "checked": true,
    "id": "2dce73e4a3e19d71249fc7a53c2a9531daaff839",
    "semantic_title": "inducing meaningful units from character sequences with dynamic capacity slot attention",
    "citation_count": 1,
    "authors": [
      "Melika Behjati",
      "James Henderson"
    ]
  },
  "https://openreview.net/forum?id=YfZ4ZPt8zd": {
    "title": "Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks",
    "volume": "main",
    "abstract": "Recently, there has been significant progress in teaching language models to perform step-by-step reasoning to solve complex numerical reasoning tasks. Chain-of-thoughts prompting (CoT) is the state-of-art method for many of these tasks. CoT uses language models to produce text describing reasoning, and computation, and finally the answer to a question. Here we propose `Program of Thoughts' (PoT), which uses language models (mainly Codex) to generate text and programming language statements, and finally an answer. In PoT, the computation can be delegated to a program interpreter, which is used to execute the generated program, thus decoupling complex computation from reasoning and language understanding. We evaluate PoT on five math word problem datasets and three financial-QA datasets in both few-shot and zero-shot settings. We find that PoT has an average performance gain over CoT of around 12% across all datasets. By combining PoT with self-consistency decoding, we can achieve extremely strong performance on all the math datasets and financial datasets. All of our data and code will be released",
    "checked": true,
    "id": "6c943670dca38bfc7c8b477ae7c2d1fba1ad3691",
    "semantic_title": "program of thoughts prompting: disentangling computation from reasoning for numerical reasoning tasks",
    "citation_count": 829,
    "authors": [
      "Wenhu Chen",
      "Xueguang Ma",
      "Xinyi Wang",
      "William W. Cohen"
    ]
  },
  "https://openreview.net/forum?id=GEcneTl9Mk": {
    "title": "DP-LFlow: Differentially Private Latent Flow for Scalable Sensitive Image Generation",
    "volume": "main",
    "abstract": "Privacy concerns grow with the success of modern deep learning models, especially when the training set contains sensitive data. Differentially private generative model (DPGM) can serve as a solution to circumvent such concerns by generating data that are distributionally similar to the original data yet with differential privacy (DP) guarantees. While GAN has attracted major attention, existing DPGMs based on flow generative models are limited and only developed on low-dimensional tabular datasets. The capability of exact density estimation makes the flow model exceptional when density estimation is of interest. In this work, we will first show that it is challenging (or even infeasible) to train a DP-flow via DP-SGD, i.e. the workhorse algorithm for private deep learning, on high-dimensional image sets with acceptable utility, and then we give an effective solution by reducing the generation from the pixel space to a lower dimensional latent space. We show the effectiveness and scalability of the proposed method via extensive experiments, where the proposed method achieves a significantly better privacy-utility trade-off compared to existing alternatives. Notably, our method is the first DPGM to scale to high-resolution image sets (up to 256 × 256). Our code is available at https://github.com/dihjiang/DP-LFlow",
    "checked": true,
    "id": "ff14a9be477cae258579fc11d0255b99fef81f70",
    "semantic_title": "dp-lflow: differentially private latent flow for scalable sensitive image generation",
    "citation_count": 1,
    "authors": [
      "Dihong Jiang",
      "Sun Sun"
    ]
  },
  "https://openreview.net/forum?id=uKCGOw9bGG": {
    "title": "Binary Classification under Local Label Differential Privacy Using Randomized Response Mechanisms",
    "volume": "main",
    "abstract": "Label differential privacy is a popular branch of $\\epsilon$-differential privacy for protecting labels in training datasets with non-private features. In this paper, we study the generalization performance of a binary classifier trained on a dataset privatized under the label differential privacy achieved by the randomized response mechanism. Particularly, we establish minimax lower bounds for the excess risks of the deep neural network plug-in classifier, theoretically quantifying how privacy guarantee $\\epsilon$ affects its generalization performance. Our theoretical result shows: (1) the randomized response mechanism slows down the convergence of excess risk by lessening the multiplicative constant term compared with the non-private case $(\\epsilon=\\infty)$; (2) as $\\epsilon$ decreases, the optimal structure of the neural network should be smaller for better generalization performance; (3) the convergence of its excess risk is guaranteed even if $\\epsilon$ is adaptive to the size of training sample $n$ at a rate slower than $O(n^{-1/2})$. Our theoretical results are validated by extensive simulated examples and two real applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shirong Xu",
      "Chendi Wang",
      "Will Wei Sun",
      "Guang Cheng"
    ]
  },
  "https://openreview.net/forum?id=Q4aAITDgdP": {
    "title": "Learn the Time to Learn: Replay Scheduling in Continual Learning",
    "volume": "main",
    "abstract": "Replay methods are known to be successful at mitigating catastrophic forgetting in continual learning scenarios despite having limited access to historical data. However, storing historical data is cheap in many real-world settings, yet replaying all historical data is often prohibited due to processing time constraints. In such settings, we propose that continual learning systems should learn the time to learn and schedule which tasks to replay at different time steps. We first demonstrate the benefits of our proposal by using Monte Carlo tree search to find a proper replay schedule, and show that the found replay schedules can outperform fixed scheduling policies when combined with various replay methods in different continual learning settings. Additionally, we propose a framework for learning replay scheduling policies with reinforcement learning. We show that the learned policies can generalize better in new continual learning scenarios compared to equally replaying all seen tasks, without added computational cost. Our study reveals the importance of learning the time to learn in continual learning, which brings current research closer to real-world needs",
    "checked": true,
    "id": "a1953404403f99a3d9c4ee33efecd2eb8f47a0c6",
    "semantic_title": "learn the time to learn: replay scheduling in continual learning",
    "citation_count": 9,
    "authors": [
      "Marcus Klasson",
      "Hedvig Kjellstrom",
      "Cheng Zhang"
    ]
  },
  "https://openreview.net/forum?id=vkiKzK5G3e": {
    "title": "Neighborhood Gradient Mean: An Efficient Decentralized Learning Method for Non-IID Data",
    "volume": "main",
    "abstract": "Decentralized learning algorithms enable the training of deep learning models over large distributed datasets, without the need for a central server. The current state-of-the-art decentralized algorithms mostly assume the data distributions to be Independent and Identically Distributed (IID). In practical scenarios, the distributed datasets can have significantly different data distributions across the agents. This paper focuses on improving decentralized learning on non-IID data with minimal compute and memory overheads. We propose Neighborhood Gradient Mean (NGM), a novel decentralized learning algorithm that modifies the local gradients of each agent using self- and cross-gradient information. In particular, the proposed method averages the local gradients with model-variant or data-variant cross-gradients based on the communication budget. Model-variant cross-gradients are derivatives of the received neighbors' model parameters with respect to the local dataset. Data-variant cross-gradient derivatives of the local model with respect to its neighbors' datasets. The data-variant cross-gradients are aggregated through an additional communication round. We theoretically analyze the convergence characteristics of NGM and demonstrate its efficiency on non-IID data sampled from various vision and language datasets. Our experiments demonstrate that the proposed method either remains competitive or outperforms (by 0-6%) the existing state-of-the-art (SoTA) decentralized learning algorithm on non-IID data with significantly less compute and memory requirements. Further, we show that the model-variant cross-gradient information available locally at each agent can improve the performance on non-IID data by 3-20% without additional communication costs",
    "checked": true,
    "id": "3b8e534931f42ad8d98fc31e00574ccf0be879e1",
    "semantic_title": "neighborhood gradient mean: an efficient decentralized learning method for non-iid data",
    "citation_count": 6,
    "authors": [
      "Sai Aparna Aketi",
      "Sangamesh Kodge",
      "Kaushik Roy"
    ]
  },
  "https://openreview.net/forum?id=140kSqm0uy": {
    "title": "Limitation of Characterizing Implicit Regularization by Data-independent Functions",
    "volume": "main",
    "abstract": "In recent years, understanding the implicit regularization of neural networks (NNs) has become a central task in deep learning theory. However, implicit regularization is itself not completely defined and well understood. In this work, we attempt to mathematically define and study implicit regularization. Importantly, we explore the limitations of a common approach to characterizing implicit regularization using data-independent functions. We propose two dynamical mechanisms, i.e., Two-point and One-point Overlapping mechanisms, based on which we provide two recipes for producing classes of one-hidden-neuron NNs that provably cannot be fully characterized by a type of or all data-independent functions. Following the previous works, our results further emphasize the profound data dependency of implicit regularization in general, inspiring us to study in detail the data dependency of NN implicit regularization in the future",
    "checked": true,
    "id": "e27217173cf86a6786ad5aaf3e48f3eeeee0fe77",
    "semantic_title": "limitation of characterizing implicit regularization by data-independent functions",
    "citation_count": 0,
    "authors": [
      "Leyang Zhang",
      "Zhi-Qin John Xu",
      "Tao Luo",
      "Yaoyu Zhang"
    ]
  },
  "https://openreview.net/forum?id=gQnJ7ODIAx": {
    "title": "Population-based Evaluation in Repeated Rock-Paper-Scissors as a Benchmark for Multiagent Reinforcement Learning",
    "volume": "main",
    "abstract": "Progress in fields of machine learning and adversarial planning has benefited significantly from benchmark domains, from checkers and the classic UCI data sets to Go and Diplomacy. In sequential decision-making, agent evaluation has largely been restricted to few interactions against experts, with the aim to reach some desired level of performance (e.g. beating a human professional player). We propose a benchmark for multiagent learning based on repeated play of the simple game Rock, Paper, Scissors along with a population of forty-three tournament entries, some of which are intentionally sub-optimal. We describe metrics to measure the quality of agents based both on average returns and exploitability. We then show that several RL, online learning, and language model approaches can learn good counter-strategies and generalize well, but ultimately lose to the top-performing bots, creating an opportunity for research in multiagent learning",
    "checked": true,
    "id": "524cdc83813740f4f7f4149896c2feabdf85ff18",
    "semantic_title": "population-based evaluation in repeated rock-paper-scissors as a benchmark for multiagent reinforcement learning",
    "citation_count": 5,
    "authors": [
      "Marc Lanctot",
      "John Schultz",
      "Neil Burch",
      "Max Olan Smith",
      "Daniel Hennes",
      "Thomas Anthony",
      "Julien Perolat"
    ]
  },
  "https://openreview.net/forum?id=aqqfB3p9ZA": {
    "title": "Convergence of SGD for Training Neural Networks with Sliced Wasserstein Losses",
    "volume": "main",
    "abstract": "Optimal Transport has sparked vivid interest in recent years, in particular thanks to the Wasserstein distance, which provides a geometrically sensible and intuitive way of comparing probability measures. For computational reasons, the Sliced Wasserstein (SW) distance was introduced as an alternative to the Wasserstein distance, and has seen uses for training generative Neural Networks (NNs). While convergence of Stochastic Gradient Descent (SGD) has been observed practically in such a setting, there is to our knowledge no theoretical guarantee for this observation. Leveraging recent works on convergence of SGD on non-smooth and non-convex functions by Bianchi et al. (2022), we aim to bridge that knowledge gap, and provide a realistic context under which fixed-step SGD trajectories for the SW loss on NN parameters converge. More precisely, we show that the trajectories approach the set of (sub)-gradient flow equations as the step decreases. Under stricter assumptions, we show a much stronger convergence result for noised and projected SGD schemes, namely that the long-run limits of the trajectories approach a set of generalised critical points of the loss function",
    "checked": true,
    "id": "13908d2c6f60e07a3e5eb7c85e23508ae46e81d1",
    "semantic_title": "convergence of sgd for training neural networks with sliced wasserstein losses",
    "citation_count": 6,
    "authors": [
      "Eloi Tanguy"
    ]
  },
  "https://openreview.net/forum?id=ySWQ6eXAKp": {
    "title": "Not All Causal Inference is the Same",
    "volume": "main",
    "abstract": "Neurally-parameterized Structural Causal Models in the Pearlian notion to causality, referred to as NCM, were recently introduced as a step towards next-generation learning systems. However, said NCM are only concerned with the learning aspect of causal inference and totally miss out on the architecture aspect. That is, actual causal inference within NCM is intractable in that the NCM won't return an answer to a query in polynomial time. This insight follows as corollary to the more general statement on the intractability of arbitrary structural causal model (SCM) parameterizations, which we prove in this work through classical 3-SAT reduction. Since future learning algorithms will be required to deal with both high dimensional data and highly complex mechanisms governing the data, we ultimately believe work on tractable inference for causality to be decisive. We also show that not all \"causal\" models are created equal. More specifically, there are models capable of answering causal queries that are not SCM, which we refer to as partially causal models (PCM). We provide a tabular taxonomy in terms of tractability properties for all of the different model families, namely correlation-based, PCM and SCM. To conclude our work, we also provide some initial ideas on how to overcome parts of the intractability of causal inference with SCM by showing an example of how parameterizing an SCM with SPN modules can at least allow for tractable mechanisms. With this work we hope that our insights can raise awareness for this novel research direction since achieving success with causality in real world downstream tasks will not only depend on learning correct models but also require having the practical ability to gain access to model inferences",
    "checked": true,
    "id": "33baffb40db3c605823944e7074552cff196b8d9",
    "semantic_title": "not all causal inference is the same",
    "citation_count": 1,
    "authors": [
      "Matej Zečević",
      "Devendra Singh Dhami",
      "Kristian Kersting"
    ]
  },
  "https://openreview.net/forum?id=tEKqQgbwbf": {
    "title": "Homomorphic Self-Supervised Learning",
    "volume": "main",
    "abstract": "Many state of the art self-supervised learning approaches fundamentally rely on transformations applied to the input in order to selectively extract task-relevant information. Recently, the field of equivariant deep learning has developed to introduce structure into the feature space of deep neural networks by designing them as homomorphisms with respect to input transformations. In this work, we observe that many existing self-supervised learning algorithms can be both unified and generalized when seen through the lens of equivariant representations. Specifically, we introduce a general framework we call Homomorphic Self-Supervised Learning, and theoretically show how it may subsume the use of input-augmentations provided an augmentation-homomorphic feature extractor. We validate this theory experimentally for simple augmentations, demonstrate the necessity of representational structure for feature-space SSL, and further empirically explore how the parameters of this framework relate to those of traditional augmentation-based self-supervised learning. We conclude with a discussion of the potential benefits afforded by this new perspective on self-supervised learning",
    "checked": true,
    "id": "64b8bf4c805699b084476b5dd7ffa8095be04e6e",
    "semantic_title": "homomorphic self-supervised learning",
    "citation_count": 2,
    "authors": [
      "T. Anderson Keller",
      "Xavier Suau",
      "Luca Zappella"
    ]
  },
  "https://openreview.net/forum?id=cXa6Xdm0v7": {
    "title": "Multimodal Language Learning for Object Retrieval in Low Data Regimes in the Face of Missing Modalities",
    "volume": "main",
    "abstract": "Our study is motivated by robotics, where when dealing with robots or other physical systems, we often need to balance competing concerns of relying on complex, multimodal data coming from a variety of sensors with a general lack of large representative datasets. Despite the complexity of modern robotic platforms and the need for multimodal interaction, there has been little research on integrating more than two modalities in a low data regime with the real-world constraint that sensors fail due to obstructions or adverse conditions. In this work, we consider a case in which natural language is used as a retrieval query against objects, represented across multiple modalities, in a physical environment. We introduce extended multimodal alignment (EMMA), a method that learns to select the appropriate object while jointly refining modality-specific embeddings through a geometric (distance-based) loss. In contrast to prior work, our approach is able to incorporate an arbitrary number of views (modalities) of a particular piece of data. We demonstrate the efficacy of our model on a grounded language object retrieval scenario. We show that our model outperforms state-of-the-art baselines when little training data is available. Our code is available at https://github.com/kasraprime/EMMA",
    "checked": true,
    "id": "1e3d96a9568b7c75f0e79c5069e499bdd3ebbabd",
    "semantic_title": "multimodal language learning for object retrieval in low data regimes in the face of missing modalities",
    "citation_count": 1,
    "authors": [
      "Kasra Darvish",
      "Edward Raff",
      "Francis Ferraro",
      "Cynthia Matuszek"
    ]
  },
  "https://openreview.net/forum?id=czev0exHXT": {
    "title": "Worst-case Feature Risk Minimization for Data-Efficient Learning",
    "volume": "main",
    "abstract": "Deep learning models typically require massive amounts of annotated data to train a strong model for a task of interest. However, data annotation is time-consuming and costly. How to use labeled data from a related but distinct domain, or just a few samples to train a satisfactory model are thus important questions. To achieve this goal, models should resist overfitting to the specifics of the training data in order to generalize well to new data. This paper proposes a novel Worst-case Feature Risk Minimization (WFRM) method that helps improve model generalization. Specifically, we tackle a minimax optimization problem in feature space at each training iteration. Given the input features, we seek the feature perturbation that maximizes the current training loss and then minimizes the training loss of the worst-case features. By incorporating our WFRM during training, we significantly improve model generalization under distributional shift – Domain Generalization (DG) and in the low-data regime – Few-shot Learning (FSL). We theoretically analyze WFRM and find the key reason why it works better than ERM – it induces an empirical risk-based semi-adaptive $L_{2}$ regularization of the classifier weights, enabling a better risk-complexity trade-off. We evaluate WFRM on two data-efficient learning tasks, including three standard DG benchmarks of PACS, VLCS, OfficeHome and the most challenging FSL benchmark Meta-Dataset. Despite the simplicity, our method consistently improves various DG and FSL methods, leading to the new state-of-the-art performances in all settings. Codes & models will be released at https://github.com/jslei/WFRM",
    "checked": true,
    "id": "638f2aaa05512b1cea4cb923b07cacfb2c336a9c",
    "semantic_title": "worst-case feature risk minimization for data-efficient learning",
    "citation_count": 0,
    "authors": [
      "Jingshi Lei",
      "Da Li",
      "Chengming Xu",
      "Liming Fang",
      "Timothy Hospedales",
      "Yanwei Fu"
    ]
  },
  "https://openreview.net/forum?id=CAd6V2qXxc": {
    "title": "Conformal prediction under ambiguous ground truth",
    "volume": "main",
    "abstract": "Conformal Prediction (CP) allows to perform rigorous uncertainty quantification by constructing a prediction set $C(X)$ satisfying $\\mathbb{P}(Y \\in C(X))\\geq 1-\\alpha$ for a user-chosen $\\alpha \\in [0,1]$ by relying on calibration data $(X_1,Y_1),...,(X_n,Y_n)$ from $\\mathbb{P}=\\mathbb{P}^{X} \\otimes \\mathbb{P}^{Y|X}$. It is typically implicitly assumed that $\\mathbb{P}^{Y|X}$ is the ``true'' posterior label distribution. However, in many real-world scenarios, the labels $Y_1,...,Y_n$ are obtained by aggregating expert opinions using a voting procedure, resulting in a one-hot distribution $\\mathbb{P}_{\\textup{vote}}^{Y|X}$. This is the case for most datasets, even well-known ones like ImageNet. For such ``voted'' labels, CP guarantees are thus w.r.t. $\\mathbb{P}_{\\textup{vote}}=\\mathbb{P}^X \\otimes \\mathbb{P}_{\\textup{vote}}^{Y|X}$ rather than the true distribution $\\mathbb{P}$. In cases with unambiguous ground truth labels, the distinction between $\\mathbb{P}_{\\textup{vote}}$ and $\\mathbb{P}$ is irrelevant. However, when experts do not agree because of ambiguous labels, approximating $\\mathbb{P}^{Y|X}$ with a one-hot distribution $\\mathbb{P}_{\\textup{vote}}^{Y|X}$ ignores this uncertainty. In this paper, we propose to leverage expert opinions to approximate $\\mathbb{P}^{Y|X}$ using a non-degenerate distribution $\\mathbb{P}_{\\textup{agg}}^{Y|X}$. We then develop \\emph{Monte Carlo CP} procedures which provide guarantees w.r.t. $\\mathbb{P}_{\\textup{agg}}=\\mathbb{P}^X \\otimes \\mathbb{P}_{\\textup{agg}}^{Y|X}$ by sampling multiple synthetic pseudo-labels from $\\mathbb{P}_{\\textup{agg}}^{Y|X}$ for each calibration example $X_1,...,X_n$. In a case study of skin condition classification with significant disagreement among expert annotators, we show that applying CP w.r.t. $\\mathbb{P}_{\\textup{vote}}$ under-covers expert annotations: calibrated for $72\\%$ coverage, it falls short by on average $10\\%$; our Monte Carlo CP closes this gap both empirically and theoretically. We also extend Monte Carlo CP to multi-label classification and CP with calibration examples enriched through data augmentation",
    "checked": true,
    "id": "7a65e919fa4b2e0d03273935a4ff413a993764f4",
    "semantic_title": "conformal prediction under ambiguous ground truth",
    "citation_count": 20,
    "authors": [
      "David Stutz",
      "Abhijit Guha Roy",
      "Tatiana Matejovicova",
      "Patricia Strachan",
      "Ali Taylan Cemgil",
      "Arnaud Doucet"
    ]
  },
  "https://openreview.net/forum?id=RFfUUtKYOG": {
    "title": "Towards Stability of Autoregressive Neural Operators",
    "volume": "main",
    "abstract": "Neural operators have proven to be a promising approach for modeling spatiotemporal systems in the physical sciences. However, training these models for large systems can be quite challenging as they incur significant computational and memory expense---these systems are often forced to rely on autoregressive time-stepping of the neural network to predict future temporal states. While this is effective in managing costs, it can lead to uncontrolled error growth over time and eventual instability. We analyze the sources of this autoregressive error growth using prototypical neural operator models for physical systems and explore ways to mitigate it. We introduce architectural and application-specific improvements that allow for careful control of instability-inducing operations within these models without inflating the compute/memory expense. We present results on several scientific systems that include Navier-Stokes fluid flow, rotating shallow water, and a high-resolution global weather forecasting system. We demonstrate that applying our design principles to neural operators leads to significantly lower errors for long-term forecasts as well as longer time horizons without qualitative signs of divergence compared to the original models for these systems. We open-source our code for reproducibility",
    "checked": true,
    "id": "b03dfd3500ed198ff5c42ecec3fcd07247baaae5",
    "semantic_title": "towards stability of autoregressive neural operators",
    "citation_count": 20,
    "authors": [
      "Michael McCabe",
      "Peter Harrington",
      "Shashank Subramanian",
      "Jed Brown"
    ]
  },
  "https://openreview.net/forum?id=ZD03VUZmRx": {
    "title": "$f$-MICL: Understanding and Generalizing InfoNCE-based Contrastive Learning",
    "volume": "main",
    "abstract": "In self-supervised contrastive learning, a widely-adopted objective function is InfoNCE, which uses the heuristic cosine similarity for the representation comparison, and is closely related to maximizing the Kullback-Leibler (KL)-based mutual information. In this paper, we aim at answering two intriguing questions: (1) Can we go beyond the KL-based objective? (2) Besides the popular cosine similarity, can we design a better similarity function? We provide answers to both questions by generalizing the KL-based mutual information to the $f$-Mutual Information in Contrastive Learning ($f$-MICL) using the $f$-divergences. To answer the first question, we provide a wide range of $f$-MICL objectives which share the nice properties of InfoNCE (e.g., alignment and uniformity), and meanwhile result in similar or even superior performance. For the second question, assuming that the joint feature distribution is proportional to the Gaussian kernel, we derive an $f$-Gaussian similarity with better interpretability and empirical performance. Finally, we identify close relationships between the $f$-MICL objective and several popular InfoNCE-based objectives. Using benchmark tasks from both vision and natural language, we empirically evaluate $f$-MICL with different $f$-divergences on various architectures (SimCLR, MoCo, and MoCo v3) and datasets. We observe that $f$-MICL generally outperforms the benchmarks and the best-performing $f$-divergence is task and dataset dependent",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiwei Lu",
      "Guojun Zhang",
      "Sun Sun",
      "Hongyu Guo",
      "Yaoliang Yu"
    ]
  },
  "https://openreview.net/forum?id=fWIQ9Oaao0": {
    "title": "Non-Stationary Contextual Pricing with Safety Constraints",
    "volume": "main",
    "abstract": "In a contextual pricing problem, a seller aims at maximizing the revenue over a sequence of sales sessions (described by feature vectors) using binary-censored feedback of \"sold\" or \"not sold\". Existing methods often overlook two practical challenges (1) the best pricing strategy could change over time; (2) the prices and pricing policies must conform to hard constraints due to safety, ethical or legal restrictions. We address both challenges by solving a more general problem of \"universal dynamic regret\" minimization in proper online learning with exp-concave losses --- an open problem posed by Baby & Wang (2021) that we partially resolve in this paper, with attention restricted to loss functions coming from a generalized linear model. Here \"dynamic regret\" measures the performance relative to a non-stationary sequence of policies, and \"proper\" means that the learner must choose feasible strategies within a pre-defined convex set, which we use to model the safety constraints. In this work, we consider a linear noisy valuation model for the customers. In the case of a known strictly log-concave market noise, our algorithm achieves $\\tilde{O}(d^3T^{1/3}C_T^{2/3} \\vee d^3)$ dynamic regret in comparison with the optimal policy series, where $T$, $d$ and $C_T$ stand for the time horizon, the feature dimension and the total variation (characterizing non-stationarity) respectively. This regret is near-optimal with respect to $T$ (within $O(\\log T)$ gaps) and $C_T$, and our algorithm is adaptable to unknown $C_T$ and remains feasible throughout. However, the dependence on $d$ is suboptimal and the minimax rate is still open",
    "checked": true,
    "id": "cc5f481d38da0300b2780e35b9e895651e32894c",
    "semantic_title": "non-stationary contextual pricing with safety constraints",
    "citation_count": 4,
    "authors": [
      "Dheeraj Baby",
      "Jianyu Xu",
      "Yu-Xiang Wang"
    ]
  },
  "https://openreview.net/forum?id=Kt2VJrCKo4": {
    "title": "VoLTA: Vision-Language Transformer with Weakly-Supervised Local-Feature Alignment",
    "volume": "main",
    "abstract": "Vision-language pre-training (VLP) has recently proven highly effective for various uni- and multi-modal downstream applications. However, most existing end-to-end VLP methods use high-resolution image-text-box data to perform well on fine-grained region-level tasks, such as object detection, segmentation, and referring expression comprehension. Unfortunately, such high-resolution images with accurate bounding box annotations are expensive to collect and use for supervision at scale. In this work, we propose VoLTA (Vision Language Transformer with weakly-supervised local-feature Alignment), a new VLP paradigm that only utilizes image-caption data but achieves fine-grained region-level image understanding, eliminating the need for expensive box annotations. VoLTA adopts graph optimal transport-based weakly-supervised alignment on local image patches and text tokens to germinate an explicit, self-normalized, and interpretable low-level matching criterion. In addition, VoLTA pushes multi-modal fusion deep into the uni-modal backbones during pre training and removes fusion-specific transformer layers, further reducing memory requirements. Extensive experiments on a wide range of vision- and vision-language downstream tasks demonstrate the effectiveness of VoLTA on fine-grained applications without compromising the coarse-grained downstream performance, often outperforming methods using significantly more caption and box annotations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shraman Pramanick",
      "Li Jing",
      "Sayan Nag",
      "Jiachen Zhu",
      "Hardik J Shah",
      "Yann LeCun",
      "Rama Chellappa"
    ]
  },
  "https://openreview.net/forum?id=YgeXqrH7gA": {
    "title": "Benefits of Max Pooling in Neural Networks: Theoretical and Experimental Evidence",
    "volume": "main",
    "abstract": "When deep neural networks became state of the art image classifiers, numerous max pooling operations were an important component of the architecture. However, modern computer vision networks typically have few, if any, max pooling operations. To understand whether this trend is justified, we develop a mathematical framework analyzing ReLU based approximations of max pooling, and prove a sense in which max pooling cannot be replicated. We formulate and analyze a novel class of optimal approximations, and find that the residual can be made exponentially small in the kernel size, but only with an exponentially wide approximation. This work gives a theoretical basis for understanding the reduced use of max pooling in newer architectures. It also enables us to establish an empirical observation about natural images: since max pooling does not seem necessary, the inputs on which max pooling is distinct – those with a large difference between the max and other values – are not prevalent",
    "checked": true,
    "id": "4f13e06f319373cb96e10857e1a940db92670e42",
    "semantic_title": "benefits of max pooling in neural networks: theoretical and experimental evidence",
    "citation_count": 0,
    "authors": [
      "Kyle Matoba",
      "Nikolaos Dimitriadis",
      "François Fleuret"
    ]
  },
  "https://openreview.net/forum?id=adpKzWQunW": {
    "title": "Local Advantage Networks for Multi-Agent Reinforcement Learning in Dec-POMDPs",
    "volume": "main",
    "abstract": "Many recent successful off-policy multi-agent reinforcement learning (MARL) algorithms for cooperative partially observable environments focus on finding factorized value functions, leading to convoluted network structures. Building on the structure of independent Q-learners, our LAN algorithm takes a radically different approach, leveraging a dueling architecture to learn for each agent a decentralized best-response policies via individual advantage functions. The learning is stabilized by a centralized critic whose primary objective is to reduce the moving target problem of the individual advantages. The critic, whose network's size is independent of the number of agents, is cast aside after learning. Evaluation on the StarCraft II multi-agent challenge benchmark shows that LAN reaches state-of-the-art performance and is highly scalable with respect to the number of agents, opening up a promising alternative direction for MARL research",
    "checked": true,
    "id": "017cc5edbe203b9cb8b6876124b80ac4e4edcd66",
    "semantic_title": "local advantage networks for multi-agent reinforcement learning in dec-pomdps",
    "citation_count": 6,
    "authors": [
      "Raphaël Avalos",
      "Mathieu Reymond",
      "Ann Nowe",
      "Diederik M Roijers"
    ]
  },
  "https://openreview.net/forum?id=Tkvmt9nDmB": {
    "title": "Beyond Distribution Shift: Spurious Features Through the Lens of Training Dynamics",
    "volume": "main",
    "abstract": "Deep Neural Networks (DNNs) are prone to learning spurious features that correlate with the label during training but are irrelevant to the learning problem. This hurts model generalization and poses problems when deploying them in safety-critical applications. This paper aims to better understand the effects of spurious features through the lens of the learning dynamics of the internal neurons during the training process. We make the following observations: (1) While previous works highlight the harmful effects of spurious features on the generalization ability of DNNs, we emphasize that not all spurious features are harmful. Spurious features can be \"benign\" or \"harmful\" depending on whether they are \"harder\" or \"easier\" to learn than the core features for a given model. This definition is model and dataset dependent. (2) We build upon this premise and use instance difficulty methods (like Prediction Depth) to quantify \"easiness\" for a given model and to identify this behavior during the training phase. (3) We empirically show that the harmful spurious features can be detected by observing the learning dynamics of the DNN's early layers. In other words, easy features learned by the initial layers of a DNN early during the training can (potentially) hurt model generalization. We verify our claims on medical and vision datasets, both simulated and real, and justify the empirical success of our hypothesis by showing the theoretical connections between Prediction Depth and information-theoretic concepts like $\\mathcal{V}$-usable information. Lastly, our experiments show that monitoring only accuracy during training (as is common in machine learning pipelines) is insufficient to detect spurious features. We, therefore, highlight the need for monitoring early training dynamics using suitable instance difficulty metrics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nihal Murali",
      "Aahlad Manas Puli",
      "Ke Yu",
      "Rajesh Ranganath",
      "kayhan Batmanghelich"
    ]
  },
  "https://openreview.net/forum?id=3AzqYa18ah": {
    "title": "Pareto Actor-Critic for Equilibrium Selection in Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Filippos Christianos",
      "Georgios Papoudakis",
      "Stefano V Albrecht"
    ]
  },
  "https://openreview.net/forum?id=lanGfX0M6C": {
    "title": "Bridging Imitation and Online Reinforcement Learning: An Optimistic Tale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Botao Hao",
      "Rahul Jain",
      "Dengwang Tang",
      "Zheng Wen"
    ]
  },
  "https://openreview.net/forum?id=REAyrhRYAo": {
    "title": "Gradient Masked Averaging for Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Irene Tenison",
      "Sai Aravind Sreeramadas",
      "Vaikkunth Mugunthan",
      "Edouard Oyallon",
      "Irina Rish",
      "Eugene Belilovsky"
    ]
  },
  "https://openreview.net/forum?id=xLnbSpozWS": {
    "title": "Training Vision-Language Transformers from Captions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liangke Gui",
      "Yingshan Chang",
      "Qiuyuan Huang",
      "Subhojit Som",
      "Alexander G Hauptmann",
      "Jianfeng Gao",
      "Yonatan Bisk"
    ]
  },
  "https://openreview.net/forum?id=3Ba6Hd3nZt": {
    "title": "Policy Gradient Algorithms Implicitly Optimize by Continuation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adrien Bolland",
      "Gilles Louppe",
      "Damien Ernst"
    ]
  },
  "https://openreview.net/forum?id=3dQCNqqv2d": {
    "title": "Self-Attention in Colors: Another Take on Encoding Graph Structure in Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Romain Menegaux",
      "Emmanuel Jehanno",
      "Margot Selosse",
      "Julien Mairal"
    ]
  },
  "https://openreview.net/forum?id=Q2Gi0TUAdS": {
    "title": "Identifying latent distances with Finslerian geometry",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alison Pouplin",
      "David Eklund",
      "Carl Henrik Ek",
      "Søren Hauberg"
    ]
  },
  "https://openreview.net/forum?id=CpYBAqDgmz": {
    "title": "Discretization Invariant Networks for Learning Maps between Neural Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Clinton Wang",
      "Polina Golland"
    ]
  },
  "https://openreview.net/forum?id=QfyVqvpg7u": {
    "title": "Physics informed neural networks for elliptic equations with oscillatory differential operators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arnav Gangal",
      "Luis Kim",
      "Sean Patrick Carney"
    ]
  },
  "https://openreview.net/forum?id=djD8IbSvgm": {
    "title": "Greedier is Better: Selecting Multiple Neighbors per Iteration for Sparse Subspace Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jwo-Yuh Wu",
      "Liang-Chi Huang",
      "Wen Hsuan Li",
      "Chun-Hung Liu",
      "Rung-Hung Gau"
    ]
  },
  "https://openreview.net/forum?id=NekBTCKJ1H": {
    "title": "Distributed Newton-Type Methods with Communication Compression and Bernoulli Aggregation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rustem Islamov",
      "Xun Qian",
      "Slavomir Hanzely",
      "Mher Safaryan",
      "Peter Richtárik"
    ]
  },
  "https://openreview.net/forum?id=LWotmCKC6Y": {
    "title": "Fourier Features in Reinforcement Learning with Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Brellmann",
      "David Filliat",
      "Goran Frehse"
    ]
  },
  "https://openreview.net/forum?id=Ulf3QZG9DC": {
    "title": "Diagnostic Tool for Out-of-Sample Model Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ludvig Hult",
      "Dave Zachariah",
      "Peter Stoica"
    ]
  },
  "https://openreview.net/forum?id=gxEpUFxIgz": {
    "title": "Straggler-Resilient Personalized Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Isidoros Tziotis",
      "Zebang Shen",
      "Ramtin Pedarsani",
      "Hamed Hassani",
      "Aryan Mokhtari"
    ]
  },
  "https://openreview.net/forum?id=izFnURFG3f": {
    "title": "Self-supervised Learning for Segmentation and Quantification of Dopamine Neurons in Parkinson's Disease",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fatemeh Haghighi",
      "soumitra ghosh",
      "Sarah Chu",
      "Hai Ngu",
      "Mohsen Hejrati",
      "Han Hui Lin",
      "Baris Bingol",
      "Somaye Hashemifar"
    ]
  },
  "https://openreview.net/forum?id=xgYgDEof29": {
    "title": "Analysis of Convolutions, Non-linearity and Depth in Graph Neural Networks using Neural Tangent Kernel",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mahalakshmi Sabanayagam",
      "Pascal Esser",
      "Debarghya Ghoshdastidar"
    ]
  },
  "https://openreview.net/forum?id=8wGXnjRLSy": {
    "title": "Zero-shot Node Classification with Graph Contrastive Embedding Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Ju",
      "Yifang Qin",
      "Siyu Yi",
      "Zhengyang Mao",
      "Kangjie Zheng",
      "Luchen Liu",
      "Xiao Luo",
      "Ming Zhang"
    ]
  },
  "https://openreview.net/forum?id=zKgJ6TWAFE": {
    "title": "Sharper Rates and Flexible Framework for Nonconvex SGD with Client and Data Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Tyurin",
      "Lukang Sun",
      "Konstantin Pavlovich Burlachenko",
      "Peter Richtárik"
    ]
  },
  "https://openreview.net/forum?id=YQWOzzSMPp": {
    "title": "An Analysis of Model-Based Reinforcement Learning From Abstracted Observations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rolf A. N. Starre",
      "Marco Loog",
      "Elena Congeduti",
      "Frans A Oliehoek"
    ]
  },
  "https://openreview.net/forum?id=6OEcDKZj5j": {
    "title": "The Kernel Density Integral Transformation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Calvin McCarter"
    ]
  },
  "https://openreview.net/forum?id=lx1WnkL9fk": {
    "title": "Overcoming Resource Constraints in Federated Learning: Large Models Can Be Trained with only Weak Clients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Niu",
      "Saurav Prakash",
      "Souvik Kundu",
      "Sunwoo Lee",
      "Salman Avestimehr"
    ]
  },
  "https://openreview.net/forum?id=FObkvLwNSo": {
    "title": "Projected Randomized Smoothing for Certified Adversarial Robustness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Pfrommer",
      "Brendon G. Anderson",
      "Somayeh Sojoudi"
    ]
  },
  "https://openreview.net/forum?id=4UXJhNSbwd": {
    "title": "Multi-Domain Long-Tailed Learning by Augmenting Disentangled Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Yang",
      "Huaxiu Yao",
      "Allan Zhou",
      "Chelsea Finn"
    ]
  },
  "https://openreview.net/forum?id=f36LaK7M0F": {
    "title": "CAE v2: Context Autoencoder with CLIP Latent Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Zhang",
      "Jiahui Chen",
      "Junkun Yuan",
      "Qiang Chen",
      "Jian Wang",
      "Xiaodi Wang",
      "Shumin Han",
      "Xiaokang Chen",
      "Jimin Pi",
      "Kun Yao",
      "Junyu Han",
      "Errui Ding",
      "Jingdong Wang"
    ]
  },
  "https://openreview.net/forum?id=VgJhYu7FmQ": {
    "title": "Cross-validation for Geospatial Data: Estimating Generalization Performance in Geostatistical Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Wang",
      "Laurel Hopkins",
      "Tyler Hallman",
      "W. Douglas Robinson",
      "Rebecca Hutchinson"
    ]
  },
  "https://openreview.net/forum?id=LLKI5Lq2YN": {
    "title": "Adaptive Hyperparameter Selection for Differentially Private Gradient Descent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dominik Fay",
      "Sindri Magnússon",
      "Jens Sjölund",
      "Mikael Johansson"
    ]
  },
  "https://openreview.net/forum?id=Ub6XILEF9x": {
    "title": "Multiscale Causal Structure Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gabriele D'Acunto",
      "Paolo Di Lorenzo",
      "Sergio Barbarossa"
    ]
  },
  "https://openreview.net/forum?id=xiQXHvL1eN": {
    "title": "Dynamic Regret Analysis of Safe Distributed Online Optimization for Convex and Non-convex Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ting-Jui Chang",
      "Sapana Chaudhary",
      "Dileep Kalathil",
      "Shahin Shahrampour"
    ]
  },
  "https://openreview.net/forum?id=2tdhQMLg36": {
    "title": "Revisiting Image Classifier Training for Improved Certified Robust Defense against Adversarial Patches",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aniruddha Saha",
      "Shuhua Yu",
      "Mohammad Sadegh Norouzzadeh",
      "Wan-Yi Lin",
      "Chaithanya Kumar Mummadi"
    ]
  },
  "https://openreview.net/forum?id=61TKzU9B96": {
    "title": "An Optical Control Environment for Benchmarking Reinforcement Learning Algorithms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "ABULIKEMU ABUDUWEILI",
      "Changliu Liu"
    ]
  },
  "https://openreview.net/forum?id=0pn3KnbH5F": {
    "title": "Learning-to-defer for sequential medical decision-making under uncertainty",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shalmali Joshi",
      "Sonali Parbhoo",
      "Finale Doshi-Velez"
    ]
  },
  "https://openreview.net/forum?id=JFaZ94tT8M": {
    "title": "Learning domain-specific causal discovery from time series",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyue Wang",
      "Konrad Kording"
    ]
  },
  "https://openreview.net/forum?id=ThJl4d5JRg": {
    "title": "Dynamic Subgoal-based Exploration via Bayesian Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yijia Wang",
      "Matthias Poloczek",
      "Daniel R. Jiang"
    ]
  },
  "https://openreview.net/forum?id=V7BvYJyTmM": {
    "title": "Gated Domain Units for Multi-source Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simon Föll",
      "Alina Dubatovka",
      "Eugen Ernst",
      "Siu Lun Chau",
      "Martin Maritsch",
      "Patrik Okanovic",
      "Gudrun Thaeter",
      "Joachim M. Buhmann",
      "Felix Wortmann",
      "Krikamol Muandet"
    ]
  },
  "https://openreview.net/forum?id=8L7Rh6FIXt": {
    "title": "IBIA: An Incremental Build-Infer-Approximate Framework for Approximate Inference of Partition Function",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shivani Bathla",
      "Vinita Vasudevan"
    ]
  },
  "https://openreview.net/forum?id=iHyhdpsnyi": {
    "title": "Revisiting Sparsity Hunting in Federated Learning: Why does Sparsity Consensus Matter?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sara Babakniya",
      "Souvik Kundu",
      "Saurav Prakash",
      "Yue Niu",
      "Salman Avestimehr"
    ]
  },
  "https://openreview.net/forum?id=Y1eYplvxrE": {
    "title": "Relating graph auto-encoders to linear models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Solveig Klepper",
      "Ulrike von Luxburg"
    ]
  },
  "https://openreview.net/forum?id=zmBFzuT2DN": {
    "title": "Deep Operator Learning Lessens the Curse of Dimensionality for PDEs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ke Chen",
      "Chunmei Wang",
      "Haizhao Yang"
    ]
  },
  "https://openreview.net/forum?id=3taIQG4C7H": {
    "title": "Label Noise-Robust Learning using a Confidence-Based Sieving Strategy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reihaneh Torkzadehmahani",
      "Reza Nasirigerdeh",
      "Daniel Rueckert",
      "Georgios Kaissis"
    ]
  },
  "https://openreview.net/forum?id=igDOV2KBwM": {
    "title": "On Perfect Clustering for Gaussian Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juan Cuesta-Albertos",
      "Subhajit Dutta"
    ]
  },
  "https://openreview.net/forum?id=WJt2Pc3qtI": {
    "title": "How Reliable is Your Regression Model's Uncertainty Under Real-World Distribution Shifts?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fredrik K. Gustafsson",
      "Martin Danelljan",
      "Thomas B. Schön"
    ]
  },
  "https://openreview.net/forum?id=qcCE4mC2jI": {
    "title": "RIGNN: A Rationale Perspective for Semi-supervised Open-world Graph Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Luo",
      "Yusheng Zhao",
      "Zhengyang Mao",
      "Yifang Qin",
      "Wei Ju",
      "Ming Zhang",
      "Yizhou Sun"
    ]
  },
  "https://openreview.net/forum?id=JwGKVpRfVD": {
    "title": "SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giulia Vezzani",
      "Dhruva Tirumala",
      "Markus Wulfmeier",
      "Dushyant Rao",
      "Abbas Abdolmaleki",
      "Ben Moran",
      "Tuomas Haarnoja",
      "Jan Humplik",
      "Roland Hafner",
      "Michael Neunert",
      "Claudio Fantacci",
      "Tim Hertweck",
      "Thomas Lampe",
      "Fereshteh Sadeghi",
      "Nicolas Heess",
      "Martin Riedmiller"
    ]
  },
  "https://openreview.net/forum?id=cJgHzw8Qhq": {
    "title": "Estimating Differential Equations from Temporal Point Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuichi Miyazawa",
      "Daichi Mochihashi"
    ]
  },
  "https://openreview.net/forum?id=XuOE99cmST": {
    "title": "Turning a Curse into a Blessing: Enabling In-Distribution-Data-Free Backdoor Removal via Stabilized Model Inversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Si Chen",
      "Yi Zeng",
      "Won Park",
      "Jiachen T. Wang",
      "Xun Chen",
      "Lingjuan Lyu",
      "Zhuoqing Mao",
      "Ruoxi Jia"
    ]
  },
  "https://openreview.net/forum?id=8koy8QuTZD": {
    "title": "Momentum Tracking: Momentum Acceleration for Decentralized Deep Learning on Heterogeneous Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuki Takezawa",
      "Han Bao",
      "Kenta Niwa",
      "Ryoma Sato",
      "Makoto Yamada"
    ]
  },
  "https://openreview.net/forum?id=KQ5jI19kF3": {
    "title": "Optimistic Optimization of Gaussian Process Samples",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julia Grosse",
      "Cheng Zhang",
      "Philipp Hennig"
    ]
  },
  "https://openreview.net/forum?id=xoLyps2qWc": {
    "title": "Linearized Relative Positional Encoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Qin",
      "Weixuan Sun",
      "Kaiyue Lu",
      "Hui Deng",
      "Dongxu Li",
      "Xiaodong Han",
      "Yuchao Dai",
      "Lingpeng Kong",
      "Yiran Zhong"
    ]
  },
  "https://openreview.net/forum?id=oud7Ny0KQy": {
    "title": "RIFLE: Imputation and Robust Inference from Low Order Marginals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sina Baharlouei",
      "Sze-Chuan Suen",
      "Meisam Razaviyayn"
    ]
  },
  "https://openreview.net/forum?id=zkRCp4RmAF": {
    "title": "Offline Reinforcement Learning with Mixture of Deterministic Policies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Takayuki Osa",
      "Akinobu Hayashi",
      "Pranav Deo",
      "Naoki Morihira",
      "Takahide Yoshiike"
    ]
  },
  "https://openreview.net/forum?id=lvevdX6bxm": {
    "title": "Quantization Robust Federated Learning for Efficient Inference on Heterogeneous Devices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kartik Gupta",
      "Marios Fournarakis",
      "Matthias Reisser",
      "Christos Louizos",
      "Markus Nagel"
    ]
  },
  "https://openreview.net/forum?id=wRepWp1KC7": {
    "title": "Fair and Useful Cohort Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Konstantina Bairaktari",
      "Paul Tsela Langton",
      "Huy Nguyen",
      "Niklas Smedemark-Margulies",
      "Jonathan Ullman"
    ]
  },
  "https://openreview.net/forum?id=vgXnEyeWVY": {
    "title": "Walking Out of the Weisfeiler Leman Hierarchy: Graph Learning Beyond Message Passing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan Tönshoff",
      "Martin Ritzert",
      "Hinrikus Wolf",
      "Martin Grohe"
    ]
  },
  "https://openreview.net/forum?id=xWrtiJwJj5": {
    "title": "Global Contrastive Learning for Long-Tailed Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thong Bach",
      "Anh Tong",
      "Truong Son Hy",
      "Vu Nguyen",
      "Thanh Nguyen-Tang"
    ]
  },
  "https://openreview.net/forum?id=KpElM2S9pw": {
    "title": "Approximating Naive Bayes on Unlabelled Categorical Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cormac Herley"
    ]
  },
  "https://openreview.net/forum?id=uaHyXxyp2r": {
    "title": "Weight-balancing fixes and flows for deep learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lawrence K. Saul"
    ]
  },
  "https://openreview.net/forum?id=lOegPKSu04": {
    "title": "$k$-Mixup Regularization for Deep Learning via Optimal Transport",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kristjan Greenewald",
      "Anming Gu",
      "Mikhail Yurochkin",
      "Justin Solomon",
      "Edward Chien"
    ]
  },
  "https://openreview.net/forum?id=0Xo9giEZWf": {
    "title": "HypUC: Hyperfine Uncertainty Calibration with Gradient- boosted Corrections for Reliable Regression on Imbalanced Electrocardiograms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Uddeshya Upadhyay",
      "Sairam Bade",
      "Arjun Puranik",
      "Shahir Asfahan",
      "Melwin Babu",
      "Francisco Lopez-Jimenez",
      "Samuel Asirvatham",
      "Ashim Prasad",
      "Ajit Rajasekharan",
      "Samir Awasthi",
      "Rakesh Barve"
    ]
  },
  "https://openreview.net/forum?id=wbpxTuXgm0": {
    "title": "TSMixer: An All-MLP Architecture for Time Series Forecast-ing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Si-An Chen",
      "Chun-Liang Li",
      "Sercan O Arik",
      "Nathanael Christian Yoder",
      "Tomas Pfister"
    ]
  },
  "https://openreview.net/forum?id=ScrEUZLxPr": {
    "title": "Revisiting Hidden Representations in Transfer Learning for Medical Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dovile Juodelyte",
      "Amelia Jiménez-Sánchez",
      "Veronika Cheplygina"
    ]
  },
  "https://openreview.net/forum?id=VrvGHDSzZ7": {
    "title": "The Geometry of Mixability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Armando J Cabrera Pacheco",
      "Robert Williamson"
    ]
  },
  "https://openreview.net/forum?id=hjDYJUn9l1": {
    "title": "Evaluating Human-Language Model Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mina Lee",
      "Megha Srivastava",
      "Amelia Hardy",
      "John Thickstun",
      "Esin Durmus",
      "Ashwin Paranjape",
      "Ines Gerard-Ursin",
      "Xiang Lisa Li",
      "Faisal Ladhak",
      "Frieda Rong",
      "Rose E Wang",
      "Minae Kwon",
      "Joon Sung Park",
      "Hancheng Cao",
      "Tony Lee",
      "Rishi Bommasani",
      "Michael S. Bernstein",
      "Percy Liang"
    ]
  },
  "https://openreview.net/forum?id=2uMnAwWnRy": {
    "title": "Benchmarking Continuous Time Models for Predicting Multiple Sclerosis Progression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Luke Ian Norcliffe",
      "Lev Proleev",
      "Diana Mincu",
      "F Lee Hartsell",
      "Katherine A Heller",
      "Subhrajit Roy"
    ]
  },
  "https://openreview.net/forum?id=ZPpQk7FJXF": {
    "title": "Differentially Private Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tim Dockhorn",
      "Tianshi Cao",
      "Arash Vahdat",
      "Karsten Kreis"
    ]
  },
  "https://openreview.net/forum?id=JaNlH6dZYk": {
    "title": "On the special role of class-selective neurons in early training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Omkar Ranadive",
      "Nikhil Thakurdesai",
      "Ari S. Morcos",
      "Matthew L Leavitt",
      "Stephane Deny"
    ]
  },
  "https://openreview.net/forum?id=MgdoxzImlK": {
    "title": "Multi-annotator Deep Learning: A Probabilistic Framework for Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marek Herde",
      "Denis Huseljic",
      "Bernhard Sick"
    ]
  },
  "https://openreview.net/forum?id=Ns2X7Azudy": {
    "title": "Learning to Optimize Quasi-Newton Methods",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Isaac Liao",
      "Rumen Dangovski",
      "Jakob Nicolaus Foerster",
      "Marin Soljacic"
    ]
  },
  "https://openreview.net/forum?id=SSkTBUyJip": {
    "title": "Task Weighting in Meta-learning with Trajectory Optimisation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cuong C. Nguyen",
      "Thanh-Toan Do",
      "Gustavo Carneiro"
    ]
  },
  "https://openreview.net/forum?id=lNB5EHx8uC": {
    "title": "Cyclic and Randomized Stepsizes Invoke Heavier Tails in SGD than Constant Stepsize",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mert Gurbuzbalaban",
      "Yuanhan Hu",
      "Umut Simsekli",
      "Lingjiong Zhu"
    ]
  },
  "https://openreview.net/forum?id=2TneniEIDB": {
    "title": "A probabilistic Taylor expansion with Gaussian processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Toni Karvonen",
      "Jon Cockayne",
      "Filip Tronarp",
      "Simo Särkkä"
    ]
  },
  "https://openreview.net/forum?id=BFvoemrmqX": {
    "title": "Bridging the Gap Between Target Networks and Functional Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexandre Piché",
      "Valentin Thomas",
      "Joseph Marino",
      "Rafael Pardinas",
      "Gian Maria Marconi",
      "Christopher Pal",
      "Mohammad Emtiyaz Khan"
    ]
  },
  "https://openreview.net/forum?id=4ofFo7D5GL": {
    "title": "HERMES: Hybrid Error-corrector Model with inclusion of External Signals for nonstationary fashion time series",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Etienne David",
      "Jean Bellot",
      "Sylvain Le Corff"
    ]
  },
  "https://openreview.net/forum?id=QoRo9QmOAr": {
    "title": "Detecting incidental correlation in multimodal learning via latent variable modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taro Makino",
      "Yixin Wang",
      "Krzysztof J. Geras",
      "Kyunghyun Cho"
    ]
  },
  "https://openreview.net/forum?id=ry2qgRqTOw": {
    "title": "Fast Kernel Methods for Generic Lipschitz Losses via $p$-Sparsified Sketches",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tamim El Ahmad",
      "Pierre Laforgue",
      "Florence d'Alché-Buc"
    ]
  },
  "https://openreview.net/forum?id=244KePn09i": {
    "title": "Single-Pass Contrastive Learning Can Work for Both Homophilic and Heterophilic Graph",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haonan Wang",
      "Jieyu Zhang",
      "Qi Zhu",
      "Wei Huang",
      "Kenji Kawaguchi",
      "Xiaokui Xiao"
    ]
  },
  "https://openreview.net/forum?id=djN3TaqbdA": {
    "title": "Variational Elliptical Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maria Margareta Bånkestad",
      "Jens Sjölund",
      "Jalil Taghia",
      "Thomas B. Schön"
    ]
  },
  "https://openreview.net/forum?id=PRrKOaDQtQ": {
    "title": "Mitigating Confirmation Bias in Semi-supervised Learning via Efficient Bayesian Model Averaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Charlotte Loh",
      "Rumen Dangovski",
      "Shivchander Sudalairaj",
      "Seungwook Han",
      "Ligong Han",
      "Leonid Karlinsky",
      "Marin Soljacic",
      "Akash Srivastava"
    ]
  },
  "https://openreview.net/forum?id=l4Jcxs0fpC": {
    "title": "Individual Privacy Accounting for Differentially Private Stochastic Gradient Descent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Da Yu",
      "Gautam Kamath",
      "Janardhan Kulkarni",
      "Tie-Yan Liu",
      "Jian Yin",
      "Huishuai Zhang"
    ]
  },
  "https://openreview.net/forum?id=VI2JjIfU37": {
    "title": "A DNN Optimizer that Improves over AdaBelief by Suppression of the Adaptive Stepsize Range",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guoqiang Zhang",
      "Kenta Niwa",
      "W. Bastiaan Kleijn"
    ]
  },
  "https://openreview.net/forum?id=f0FSDAy1bU": {
    "title": "Faster Training of Neural ODEs Using Gauß–Legendre Quadrature",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Luke Ian Norcliffe",
      "Marc Peter Deisenroth"
    ]
  },
  "https://openreview.net/forum?id=lAQQx7hlku": {
    "title": "Bridging the Sim2Real gap with CARE: Supervised Detection Adaptation with Conditional Alignment and Reweighting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Viraj Uday Prabhu",
      "David Acuna",
      "Rafid Mahmood",
      "Marc T. Law",
      "Yuan-Hong Liao",
      "Judy Hoffman",
      "Sanja Fidler",
      "James Lucas"
    ]
  },
  "https://openreview.net/forum?id=obB415rg8q": {
    "title": "Efficient Inference With Model Cascades",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luzian Lebovitz",
      "Lukas Cavigelli",
      "Michele Magno",
      "Lorenz K Muller"
    ]
  },
  "https://openreview.net/forum?id=EWPA9TZcUy": {
    "title": "Semantic Representations of Mathematical Expressions in a Continuous Vector Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Neeraj Gangwar",
      "Nickvash Kani"
    ]
  },
  "https://openreview.net/forum?id=oFC2LAqS6Z": {
    "title": "Representations and Computations in Transformers that Support Generalization on Structured Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Li",
      "James McClelland"
    ]
  },
  "https://openreview.net/forum?id=tv46tCzs83": {
    "title": "Causal Parrots: Large Language Models May Talk Causality But Are Not Causal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matej Zečević",
      "Moritz Willig",
      "Devendra Singh Dhami",
      "Kristian Kersting"
    ]
  },
  "https://openreview.net/forum?id=VP9p4u9jAo": {
    "title": "An Option-Dependent Analysis of Regret Minimization Algorithms in Finite-Horizon Semi-MDP",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gianluca Drappo",
      "Alberto Maria Metelli",
      "Marcello Restelli"
    ]
  },
  "https://openreview.net/forum?id=Hf95zFnQ7H": {
    "title": "On Adaptivity in Quantum Testing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Omar Fawzi",
      "Nicolas Flammarion",
      "Aurélien Garivier",
      "Aadil Oufkir"
    ]
  },
  "https://openreview.net/forum?id=d4Vr6E0jjm": {
    "title": "Teaching Smaller Language Models To Generalise To Unseen Compositional Questions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tim Hartill",
      "Neset TAN",
      "Michael Witbrock",
      "Patricia J. Riddle"
    ]
  },
  "https://openreview.net/forum?id=3agxS3aDUs": {
    "title": "Subgraph Permutation Equivariant Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joshua Mitton",
      "Roderick Murray-Smith"
    ]
  },
  "https://openreview.net/forum?id=uq29MIWvIV": {
    "title": "About the Cost of Central Privacy in Density Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Clément Lalanne",
      "Aurélien Garivier",
      "Rémi Gribonval"
    ]
  },
  "https://openreview.net/forum?id=REtKapdkyI": {
    "title": "Some Remarks on Identifiability of Independent Component Analysis in Restricted Function Classes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simon Buchholz"
    ]
  },
  "https://openreview.net/forum?id=Nn71AdKyYH": {
    "title": "You Only Transfer What You Share: Intersection-Induced Graph Transfer Learning for Link Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenqing Zheng",
      "Edward W Huang",
      "Nikhil Rao",
      "Zhangyang Wang",
      "Karthik Subbian"
    ]
  },
  "https://openreview.net/forum?id=7wA65zL3B3": {
    "title": "Logistic-Normal Likelihoods for Heteroscedastic Label Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Erik Englesson",
      "Amir Mehrpanah",
      "Hossein Azizpour"
    ]
  },
  "https://openreview.net/forum?id=Ufc5cWhHko": {
    "title": "RECLIP: Resource-efficient CLIP by Training with Small Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runze Li",
      "Dahun Kim",
      "Bir Bhanu",
      "Weicheng Kuo"
    ]
  },
  "https://openreview.net/forum?id=ubCoTAynPp": {
    "title": "Reinforcement Learning with Delayed, Composite, and Partially Anonymous Reward",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Washim Uddin Mondal",
      "Vaneet Aggarwal"
    ]
  },
  "https://openreview.net/forum?id=dPSTDbGtBY": {
    "title": "Towards Multi-spatiotemporal-scale Generalized PDE Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jayesh K Gupta",
      "Johannes Brandstetter"
    ]
  },
  "https://openreview.net/forum?id=z49eaB8kiH": {
    "title": "The Multiquadric Kernel for Moment-Matching Distributional Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ludvig Killingberg",
      "Helge Langseth"
    ]
  },
  "https://openreview.net/forum?id=EDVIHPZhFo": {
    "title": "Nonconvex-nonconcave min-max optimization on Riemannian manifolds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andi Han",
      "Bamdev Mishra",
      "Pratik Jawanpuria",
      "Junbin Gao"
    ]
  },
  "https://openreview.net/forum?id=moZvOx5cxe": {
    "title": "Learning to Boost Resilience of Complex Networks via Neural Edge Rewiring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shanchao Yang",
      "MA KAILI",
      "Baoxiang Wang",
      "Tianshu Yu",
      "Hongyuan Zha"
    ]
  },
  "https://openreview.net/forum?id=y8RZoPjEUl": {
    "title": "Simulate Time-integrated Coarse-grained Molecular Dynamics with Multi-scale Graph Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Fu",
      "Tian Xie",
      "Nathan J. Rebello",
      "Bradley Olsen",
      "Tommi S. Jaakkola"
    ]
  },
  "https://openreview.net/forum?id=R2hUure38l": {
    "title": "Meta-Calibration: Learning of Model Calibration Using Differentiable Expected Calibration Error",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ondrej Bohdal",
      "Yongxin Yang",
      "Timothy Hospedales"
    ]
  },
  "https://openreview.net/forum?id=v5ew3FPTgb": {
    "title": "Understanding convolution on graphs via energies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesco Di Giovanni",
      "James Rowbottom",
      "Benjamin Paul Chamberlain",
      "Thomas Markovich",
      "Michael M. Bronstein"
    ]
  },
  "https://openreview.net/forum?id=ZPMf53vE1L": {
    "title": "One-Step Distributional Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mastane Achab",
      "Reda ALAMI",
      "YASSER ABDELAZIZ DAHOU DJILALI",
      "Kirill Fedyanin",
      "Eric Moulines"
    ]
  },
  "https://openreview.net/forum?id=PHAr3q49h6": {
    "title": "Dual Representation Learning for Out-of-distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhilin Zhao",
      "Longbing Cao"
    ]
  },
  "https://openreview.net/forum?id=83rgSFPpws": {
    "title": "Cyclophobic Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefan Sylvius Wagner",
      "Peter Arndt",
      "Jan Robine",
      "Stefan Harmeling"
    ]
  },
  "https://openreview.net/forum?id=nYzhlFyjjd": {
    "title": "Rotation-Invariant Random Features Provide a Strong Baseline for Machine Learning on 3D Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Owen Melia",
      "Eric M Jonas",
      "Rebecca Willett"
    ]
  },
  "https://openreview.net/forum?id=pHCdMat0gI": {
    "title": "Graph Neural Networks for Temporal Graphs: State of the Art, Open Challenges, and Opportunities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antonio Longa",
      "Veronica Lachi",
      "Gabriele Santin",
      "Monica Bianchini",
      "Bruno Lepri",
      "Pietro Lio",
      "franco scarselli",
      "Andrea Passerini"
    ]
  },
  "https://openreview.net/forum?id=0f8tU3QwWD": {
    "title": "FairGrad: Fairness Aware Gradient Descent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaurav Maheshwari",
      "Michaël Perrot"
    ]
  },
  "https://openreview.net/forum?id=qHZs2p4ZD4": {
    "title": "V1T: large-scale mouse V1 response prediction using a Vision Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bryan M. Li",
      "Isabel Maria Cornacchia",
      "Nathalie Rochefort",
      "Arno Onken"
    ]
  },
  "https://openreview.net/forum?id=ey5b7kODvK": {
    "title": "Novel Class Discovery for Long-tailed Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuyu Zhang",
      "Ruijie Xu",
      "Xuming He"
    ]
  },
  "https://openreview.net/forum?id=U4XgzRjfF1": {
    "title": "Asymptotic Analysis of Conditioned Stochastic Gradient Descent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rémi Leluc",
      "François Portier"
    ]
  },
  "https://openreview.net/forum?id=WYKTCKpImz": {
    "title": "Learned Thresholds Token Merging and Pruning for Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maxim Bonnaerens",
      "Joni Dambre"
    ]
  },
  "https://openreview.net/forum?id=FqOG4osY7C": {
    "title": "MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weicheng Kuo",
      "AJ Piergiovanni",
      "Dahun Kim",
      "xiyang luo",
      "Benjamin Caine",
      "Wei Li",
      "Abhijit Ogale",
      "Luowei Zhou",
      "Andrew M. Dai",
      "Zhifeng Chen",
      "Claire Cui",
      "Anelia Angelova"
    ]
  },
  "https://openreview.net/forum?id=EwJJks2cSa": {
    "title": "Chasing Better Deep Image Priors between Over- and Under-parameterization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiming Wu",
      "Xiaohan Chen",
      "Yifan Jiang",
      "Zhangyang Wang"
    ]
  },
  "https://openreview.net/forum?id=TjaMO63fc9": {
    "title": "Federated High-Dimensional Online Decision Making",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chi-Hua Wang",
      "Wenjie Li",
      "Guang Lin"
    ]
  },
  "https://openreview.net/forum?id=QnT41ZGNh9": {
    "title": "Regret Bounds for Satisficing in Multi-Armed Bandit Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Michel",
      "Hossein Hajiabolhassan",
      "Ronald Ortner"
    ]
  },
  "https://openreview.net/forum?id=nFWRuJXPkU": {
    "title": "Using Confounded Data in Latent Model-Based Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maxime Gasse",
      "Damien GRASSET",
      "Guillaume Gaudron",
      "Pierre-Yves Oudeyer"
    ]
  },
  "https://openreview.net/forum?id=1irVjE7A3w": {
    "title": "Meta-Learning via Classifier(-free) Diffusion Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elvis Nava",
      "Seijin Kobayashi",
      "Yifei Yin",
      "Robert K. Katzschmann",
      "Benjamin F Grewe"
    ]
  },
  "https://openreview.net/forum?id=nGW2Hotpq3": {
    "title": "Optimizing Learning Rate Schedules for Iterative Pruning of Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiyu Liu",
      "Rohan Ghosh",
      "John Chong Min Tan",
      "Mehul Motani"
    ]
  },
  "https://openreview.net/forum?id=wvLQMHtyLk": {
    "title": "Foiling Explanations in Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Snir Vitrack Tamam",
      "Raz Lapid",
      "Moshe Sipper"
    ]
  },
  "https://openreview.net/forum?id=pCbC3aQB5W": {
    "title": "Long-term Forecasting with TiDE: Time-series Dense Encoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhimanyu Das",
      "Weihao Kong",
      "Andrew Leach",
      "Shaan K Mathur",
      "Rajat Sen",
      "Rose Yu"
    ]
  },
  "https://openreview.net/forum?id=Y3saBb7mCE": {
    "title": "Empirical Limitations of the NTK for Understanding Scaling Laws in Deep Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikhil Vyas",
      "Yamini Bansal",
      "Preetum Nakkiran"
    ]
  },
  "https://openreview.net/forum?id=7KW7zvKd7J": {
    "title": "Transport Score Climbing: Variational Inference Using Forward KL and Adaptive Neural Transport",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liyi Zhang",
      "David Blei",
      "Christian A Naesseth"
    ]
  },
  "https://openreview.net/forum?id=D5Z2E8CNsD": {
    "title": "Distributionally Robust Classification on a Data Budget",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Feuer",
      "Ameya Joshi",
      "Minh Pham",
      "Chinmay Hegde"
    ]
  },
  "https://openreview.net/forum?id=8ykyGbtt2q": {
    "title": "The ConceptARC Benchmark: Evaluating Understanding and Generalization in the ARC Domain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arsenii Kirillovich Moskvichev",
      "Victor Vikram Odouard",
      "Melanie Mitchell"
    ]
  },
  "https://openreview.net/forum?id=Rb6VDOHebB": {
    "title": "Adaptive Compression for Communication-Efficient Distributed Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maksim Makarenko",
      "Elnur Gasanov",
      "Abdurakhmon Sadiev",
      "Rustem Islamov",
      "Peter Richtárik"
    ]
  },
  "https://openreview.net/forum?id=H1SekypXKA": {
    "title": "Expected Worst Case Regret via Stochastic Sequential Covering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changlong Wu",
      "Mohsen Heidari",
      "Ananth Grama",
      "Wojciech Szpankowski"
    ]
  },
  "https://openreview.net/forum?id=HVAeM6sNo8": {
    "title": "Robust Alzheimer's Progression Modeling using Cross-Domain Self-Supervised Deep Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saba Dadsetan",
      "Mohsen Hejrati",
      "Shandong Wu",
      "Somaye Hashemifar"
    ]
  },
  "https://openreview.net/forum?id=LRYtNj8Xw0": {
    "title": "Learning Augmentation Distributions using Transformed Risk Minimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Evangelos Chatzipantazis",
      "Stefanos Pertigkiozoglou",
      "Kostas Daniilidis",
      "Edgar Dobriban"
    ]
  },
  "https://openreview.net/forum?id=qUxBs3Ln41": {
    "title": "Structured Low-Rank Tensors for Generalized Linear Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Batoul Ahmad Taki",
      "Anand Sarwate",
      "Waheed U. Bajwa"
    ]
  },
  "https://openreview.net/forum?id=dXAuvo6CGI": {
    "title": "Scalable Stochastic Gradient Riemannian Langevin Dynamics in Non-Diagonal Metrics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanlin Yu",
      "Marcelo Hartmann",
      "Bernardo Williams",
      "Arto Klami"
    ]
  },
  "https://openreview.net/forum?id=HwcB5elyuG": {
    "title": "Towards a Defense Against Federated Backdoor Attacks Under Continuous Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuaiqi Wang",
      "Jonathan Hayase",
      "Giulia Fanti",
      "Sewoong Oh"
    ]
  },
  "https://openreview.net/forum?id=9jnsPp8DP3": {
    "title": "mL-BFGS: A Momentum-based L-BFGS for Distributed Large-scale Neural Network Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Niu",
      "Zalan Fabian",
      "Sunwoo Lee",
      "Mahdi Soltanolkotabi",
      "Salman Avestimehr"
    ]
  },
  "https://openreview.net/forum?id=lu4oAq55iK": {
    "title": "Mitigating Real-World Distribution Shifts in the Fourier Domain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kiran Krishnamachari",
      "See-Kiong Ng",
      "Chuan-Sheng Foo"
    ]
  },
  "https://openreview.net/forum?id=nOIGfQnFZm": {
    "title": "Learning representations that are closed-form Monge mapping optimal with application to domain adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oliver Struckmeier",
      "Ievgen Redko",
      "Anton Mallasto",
      "Karol Arndt",
      "Markus Heinonen",
      "Ville Kyrki"
    ]
  },
  "https://openreview.net/forum?id=kdfiEu1ul6": {
    "title": "Learning from time-dependent streaming data with online stochastic algorithms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antoine Godichon-Baggioni",
      "Nicklas Werge",
      "Olivier Wintenberger"
    ]
  },
  "https://openreview.net/forum?id=W0ehjkl9x7": {
    "title": "DoCoM: Compressed Decentralized Optimization with Near-Optimal Sample Complexity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chung-Yiu Yau",
      "Hoi To Wai"
    ]
  },
  "https://openreview.net/forum?id=moVEUgJaHO": {
    "title": "GPS++: Reviving the Art of Message Passing for Molecular Property Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dominic Masters",
      "Josef Dean",
      "Kerstin Klaeser",
      "Zhiyi Li",
      "Samuel Maddrell-Mander",
      "Adam Sanders",
      "Hatem Helal",
      "Deniz Beker",
      "Andrew W Fitzgibbon",
      "Shenyang Huang",
      "Ladislav Rampášek",
      "Dominique Beaini"
    ]
  },
  "https://openreview.net/forum?id=3LzgOQ3eOb": {
    "title": "Tackling Provably Hard Representative Selection via Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mehran Kazemi",
      "Anton Tsitsulin",
      "Hossein Esfandiari",
      "Mohammadhossein Bateni",
      "Deepak Ramachandran",
      "Bryan Perozzi",
      "Vahab Mirrokni"
    ]
  },
  "https://openreview.net/forum?id=Qlvgq9eC63": {
    "title": "Improved Group Robustness via Classifier Retraining on Independent Splits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thien Hang Nguyen",
      "Hongyang R. Zhang",
      "Huy Nguyen"
    ]
  },
  "https://openreview.net/forum?id=0XBuaxqEcG": {
    "title": "Execution-based Code Generation using Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Parshin Shojaee",
      "Aneesh Jain",
      "Sindhu Tipirneni",
      "Chandan K. Reddy"
    ]
  },
  "https://openreview.net/forum?id=TIsrnWpjQ0": {
    "title": "TabCBM: Concept-based Interpretable Neural Networks for Tabular Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mateo Espinosa Zarlenga",
      "Zohreh Shams",
      "Michael Edward Nelson",
      "Been Kim",
      "Mateja Jamnik"
    ]
  },
  "https://openreview.net/forum?id=giw2vcAhiH": {
    "title": "Spectral learning of Bernoulli linear dynamical systems models for decision-making",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Iris R Stone",
      "Yotam Sagiv",
      "Il Memming Park",
      "Jonathan W. Pillow"
    ]
  },
  "https://openreview.net/forum?id=bnBeNFB27b": {
    "title": "Self-Supervision is All You Need for Solving Rubik's Cube",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyo Takano"
    ]
  },
  "https://openreview.net/forum?id=EYjfLeJL4l": {
    "title": "Towards Better Generalization with Flexible Representation of Multi-Module Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "HyunGeun Lee",
      "Kijung Yoon"
    ]
  },
  "https://openreview.net/forum?id=5rq8iRzHAQ": {
    "title": "Assisting Human Decisions in Document Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joon Sik Kim",
      "Valerie Chen",
      "Danish Pruthi",
      "Nihar B Shah",
      "Ameet Talwalkar"
    ]
  },
  "https://openreview.net/forum?id=T5sXdAO3EQ": {
    "title": "Bayesian Quadrature for Neural Ensemble Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saad Hamid",
      "Xingchen Wan",
      "Martin Jørgensen",
      "Binxin Ru",
      "Michael A Osborne"
    ]
  },
  "https://openreview.net/forum?id=MMsyqXIJuk": {
    "title": "JiangJun: Mastering Xiangqi by Tackling Non-Transitivity in Two-Player Zero-Sum Games",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Li",
      "Kun Xiong",
      "Yingping Zhang",
      "Jiangcheng Zhu",
      "Stephen Marcus McAleer",
      "Wei Pan",
      "Jun Wang",
      "Zonghong Dai",
      "Yaodong Yang"
    ]
  },
  "https://openreview.net/forum?id=f39UIDkwwc": {
    "title": "Contrastive Attraction and Contrastive Repulsion for Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huangjie Zheng",
      "Xu Chen",
      "Jiangchao Yao",
      "Hongxia Yang",
      "Chunyuan Li",
      "Ya Zhang",
      "Hao Zhang",
      "Ivor Tsang",
      "Jingren Zhou",
      "Mingyuan Zhou"
    ]
  },
  "https://openreview.net/forum?id=HyzCuCV1jH": {
    "title": "Data Augmentation is a Hyperparameter: Cherry-picked Self-Supervision for Unsupervised Anomaly Detection is Creating the Illusion of Success",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaemin Yoo",
      "Tiancheng Zhao",
      "Leman Akoglu"
    ]
  },
  "https://openreview.net/forum?id=Wcui061fxr": {
    "title": "Conditional Generative Models are Provably Robust: Pointwise Guarantees for Bayesian Inverse Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fabian Altekrüger",
      "Paul Hagemann",
      "Gabriele Steidl"
    ]
  },
  "https://openreview.net/forum?id=eLX5XrajXh": {
    "title": "A Characteristic Function for Shapley-Value-Based Attribution of Anomaly Scores",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Naoya Takeishi",
      "Yoshinobu Kawahara"
    ]
  },
  "https://openreview.net/forum?id=QBMyDZsPMd": {
    "title": "The Open MatSci ML Toolkit: A Flexible Framework for Machine Learning in Materials Science",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Santiago Miret",
      "Kin Long Kelvin Lee",
      "Carmelo Gonzales",
      "Marcel Nassar",
      "Matthew Spellings"
    ]
  },
  "https://openreview.net/forum?id=mXfkKtu5JA": {
    "title": "Differentiable Logic Machines",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthieu Zimmer",
      "Xuening Feng",
      "Claire Glanois",
      "Zhaohui JIANG",
      "Jianyi Zhang",
      "Paul Weng",
      "Dong Li",
      "Jianye HAO",
      "Wulong Liu"
    ]
  },
  "https://openreview.net/forum?id=kdPcLdJbt1": {
    "title": "Vulnerability-Aware Instance Reweighting For Adversarial Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Olukorede Fakorede",
      "Ashutosh Kumar Nirala",
      "Modeste Atsague",
      "Jin Tian"
    ]
  },
  "https://openreview.net/forum?id=V7tahqGrOq": {
    "title": "Lifelong Reinforcement Learning with Modulating Masks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eseoghene Ben-Iwhiwhu",
      "Saptarshi Nath",
      "Praveen Kumar Pilly",
      "Soheil Kolouri",
      "Andrea Soltoggio"
    ]
  },
  "https://openreview.net/forum?id=ILNqQhGbLx": {
    "title": "Semantic Self-adaptation: Enhancing Generalization with a Single Sample",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sherwin Bahmani",
      "Oliver Hahn",
      "Eduard Zamfir",
      "Nikita Araslanov",
      "Daniel Cremers",
      "Stefan Roth"
    ]
  },
  "https://openreview.net/forum?id=MyQ1e1VQQ3": {
    "title": "Fair Kernel Regression through Cross-Covariance Operators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adrian Perez-Suay",
      "Paula Gordaliza",
      "Jean-Michel Loubes",
      "Dino Sejdinovic",
      "Gustau Camps-Valls"
    ]
  },
  "https://openreview.net/forum?id=dpGSNLUCzu": {
    "title": "The Score-Difference Flow for Implicit Generative Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Romann M. Weber"
    ]
  },
  "https://openreview.net/forum?id=tLBjsX4tjs": {
    "title": "A Unified Perspective on Natural Gradient Variational Inference with Gaussian Mixture Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oleg Arenz",
      "Philipp Dahlinger",
      "Zihan Ye",
      "Michael Volpp",
      "Gerhard Neumann"
    ]
  },
  "https://openreview.net/forum?id=FbztvhdCX9": {
    "title": "On the Gradient Formula for learning Generative Models with Regularized Optimal Transport Costs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antoine Houdard",
      "Arthur Leclaire",
      "Nicolas Papadakis",
      "Julien Rabin"
    ]
  },
  "https://openreview.net/forum?id=HP7Qpui5YE": {
    "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Zhu",
      "Jiyang Qi",
      "Mingyu Ding",
      "Xiaokang Chen",
      "Ping Luo",
      "Xinggang Wang",
      "Wenyu Liu",
      "Leye Wang",
      "Jingdong Wang"
    ]
  },
  "https://openreview.net/forum?id=SaVEXFuozg": {
    "title": "DSpar: An Embarrassingly Simple Strategy for Efficient GNN training and inference via Degree-based Sparsification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zirui Liu",
      "Kaixiong Zhou",
      "Zhimeng Jiang",
      "Li Li",
      "Rui Chen",
      "Soo-Hyun Choi",
      "Xia Hu"
    ]
  },
  "https://openreview.net/forum?id=1QqIfGZOWu": {
    "title": "Challenges and Opportunities in Offline Reinforcement Learning from Visual Observations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cong Lu",
      "Philip J. Ball",
      "Tim G. J. Rudner",
      "Jack Parker-Holder",
      "Michael A Osborne",
      "Yee Whye Teh"
    ]
  },
  "https://openreview.net/forum?id=LEVbhNrLEL": {
    "title": "Mind the Gap: Mitigating the Distribution Gap in Graph Few-shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chunhui Zhang",
      "Hongfu Liu",
      "Jundong Li",
      "Yanfang Ye",
      "Chuxu Zhang"
    ]
  },
  "https://openreview.net/forum?id=HqIuAzBxbh": {
    "title": "Consistent Collaborative Filtering via Tensor Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiwen Zhao",
      "Guillermo Sapiro"
    ]
  },
  "https://openreview.net/forum?id=jkTqJJOGMS": {
    "title": "Provably Convergent Policy Optimization via Metric-aware Trust Region Methods",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Song",
      "Niao He",
      "Lijun Ding",
      "Chaoyue Zhao"
    ]
  },
  "https://openreview.net/forum?id=P6NcRPb13w": {
    "title": "Adjusting Machine Learning Decisions for Equal Opportunity and Counterfactual Fairness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixin Wang",
      "Dhanya Sridhar",
      "David Blei"
    ]
  },
  "https://openreview.net/forum?id=JJrKbq35l4": {
    "title": "On Average-Case Error Bounds for Kernel-Based Bayesian Quadrature",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xu Cai",
      "Thanh Lam",
      "Jonathan Scarlett"
    ]
  },
  "https://openreview.net/forum?id=ThhMzfrd6r": {
    "title": "Self-Supervised Graph Representation Learning for Neuronal Morphologies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marissa A. Weis",
      "Laura Pede",
      "Timo Lüddecke",
      "Alexander S Ecker"
    ]
  },
  "https://openreview.net/forum?id=VV4zJwLwI7": {
    "title": "Breaking the Spurious Causality of Conditional Generation via Fairness Intervention with Corrective Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junhyun Nam",
      "Sangwoo Mo",
      "Jaeho Lee",
      "Jinwoo Shin"
    ]
  },
  "https://openreview.net/forum?id=VpaXrBFYZ9": {
    "title": "Stochastic Constrained DRO with a Complexity Independent of Sample Size",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Qi",
      "Jiameng Lyu",
      "Kung-Sik Chan",
      "Er-Wei Bai",
      "Tianbao Yang"
    ]
  },
  "https://openreview.net/forum?id=nddEHTSnqg": {
    "title": "Neural Networks beyond explainability: Selective inference for sequence motifs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antoine Villié",
      "Philippe Veber",
      "Yohann De Castro",
      "Laurent Jacob"
    ]
  },
  "https://openreview.net/forum?id=w36pqfaJ4t": {
    "title": "Dynamics Adapted Imitation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixuan Liu",
      "Liu Liu",
      "Bingzhe Wu",
      "Lanqing Li",
      "Xueqian Wang",
      "Bo Yuan",
      "Peilin Zhao"
    ]
  },
  "https://openreview.net/forum?id=CkXOwlhf27": {
    "title": "A Proximal Algorithm for Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaming Liang",
      "Yongxin Chen"
    ]
  },
  "https://openreview.net/forum?id=j3FK00HyfU": {
    "title": "The Meta-Evaluation Problem in Explainable AI: Identifying Reliable Estimators with MetaQuantus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anna Hedström",
      "Philine Lou Bommer",
      "Kristoffer Knutsen Wickstrøm",
      "Wojciech Samek",
      "Sebastian Lapuschkin",
      "Marina MC Höhne"
    ]
  },
  "https://openreview.net/forum?id=LdSP6cvTS4": {
    "title": "Calibrating and Improving Graph Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "MA KAILI",
      "Garry YANG",
      "Han Yang",
      "Yongqiang Chen",
      "James Cheng"
    ]
  },
  "https://openreview.net/forum?id=nfYwRIezvg": {
    "title": "DORA: Exploring Outlier Representations in Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kirill Bykov",
      "Mayukh Deb",
      "Dennis Grinwald",
      "Klaus Robert Muller",
      "Marina MC Höhne"
    ]
  },
  "https://openreview.net/forum?id=g97OHbQyk1": {
    "title": "The Vendi Score: A Diversity Evaluation Metric for Machine Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dan Friedman",
      "Adji Bousso Dieng"
    ]
  },
  "https://openreview.net/forum?id=OqbGu3hdQb": {
    "title": "Contextual Combinatorial Multi-output GP Bandits with Group Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sepehr Elahi",
      "Baran Atalar",
      "Sevda Öğüt",
      "Cem Tekin"
    ]
  },
  "https://openreview.net/forum?id=Gbu1bHQhEL": {
    "title": "Active Acquisition for Multimodal Temporal Data: A Challenging Decision-Making Task",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jannik Kossen",
      "Cătălina Cangea",
      "Eszter Vértes",
      "Andrew Jaegle",
      "Viorica Patraucean",
      "Ira Ktena",
      "Nenad Tomasev",
      "Danielle Belgrave"
    ]
  },
  "https://openreview.net/forum?id=gwRwHUZUgz": {
    "title": "Learning Symbolic Rules for Reasoning in Quasi-Natural Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiyu Yang",
      "Jia Deng"
    ]
  },
  "https://openreview.net/forum?id=XJIg4kQbkv": {
    "title": "CoCoFL: Communication- and Computation-Aware Federated Learning via Partial NN Freezing and Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kilian Pfeiffer",
      "Martin Rapp",
      "Ramin Khalili",
      "Joerg Henkel"
    ]
  },
  "https://openreview.net/forum?id=TdzQtbLeVw": {
    "title": "Online Min-max Problems with Non-convexity and Non-stationarity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Huang",
      "Yuan Cheng",
      "Yingbin Liang",
      "Longbo Huang"
    ]
  },
  "https://openreview.net/forum?id=LKz5SqIXPJ": {
    "title": "On the Robustness of Dataset Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Szyller",
      "Rui Zhang",
      "Jian Liu",
      "N Asokan"
    ]
  },
  "https://openreview.net/forum?id=sY35BAiIf4": {
    "title": "Improving Differentially Private SGD via Randomly Sparsified Gradients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyi Zhu",
      "Matthew B. Blaschko"
    ]
  },
  "https://openreview.net/forum?id=p28wv4G65d": {
    "title": "SC2 Benchmark: Supervised Compression for Split Computing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yoshitomo Matsubara",
      "Ruihan Yang",
      "Marco Levorato",
      "Stephan Mandt"
    ]
  },
  "https://openreview.net/forum?id=izL3B8dPx1": {
    "title": "Inherent Limits on Topology-Based Link Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Justus Isaiah Hibshman",
      "Tim Weninger"
    ]
  },
  "https://openreview.net/forum?id=uv32JOdQuh": {
    "title": "Invariant Feature Coding using Tensor Product Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "YUSUKE Mukuta",
      "Tatsuya Harada"
    ]
  },
  "https://openreview.net/forum?id=wk8oXR0kFA": {
    "title": "Releasing Graph Neural Networks with Differential Privacy Guarantees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Iyiola Emmanuel Olatunji",
      "Thorben Funke",
      "Megha Khosla"
    ]
  },
  "https://openreview.net/forum?id=ERqGqZzSu5": {
    "title": "Sequential Query Encoding for Complex Query Answering on Knowledge Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxin Bai",
      "Tianshi Zheng",
      "Yangqiu Song"
    ]
  },
  "https://openreview.net/forum?id=sFk3aBNb81": {
    "title": "TransFool: An Adversarial Attack against Neural Machine Translation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sahar Sadrizadeh",
      "Ljiljana Dolamic",
      "Pascal Frossard"
    ]
  },
  "https://openreview.net/forum?id=9pWjgQ3y85": {
    "title": "An Explicit Expansion of the Kullback-Leibler Divergence along its Fisher-Rao Gradient Flow",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Carles Domingo-Enrich",
      "Aram-Alexandre Pooladian"
    ]
  },
  "https://openreview.net/forum?id=ZoXi7n54OB": {
    "title": "Training with Mixed-Precision Floating-Point Assignments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wonyeol Lee",
      "Rahul Sharma",
      "Alex Aiken"
    ]
  },
  "https://openreview.net/forum?id=A1N2qp4yAq": {
    "title": "Bandwidth Enables Generalization in Quantum Kernel Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abdulkadir Canatar",
      "Evan Peters",
      "Cengiz Pehlevan",
      "Stefan M. Wild",
      "Ruslan Shaydulin"
    ]
  },
  "https://openreview.net/forum?id=vTsfup5ll6": {
    "title": "Privacy-Preserving Energy-Based Generative Models for Marginal Distribution Protection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Robert E. Tillman",
      "Tucker Balch",
      "Manuela Veloso"
    ]
  },
  "https://openreview.net/forum?id=B7PFZtm8DA": {
    "title": "Unsupervised Discovery and Composition of Object Light Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cameron Omid Smith",
      "Hong-Xing Yu",
      "Sergey Zakharov",
      "Fredo Durand",
      "Joshua B. Tenenbaum",
      "Jiajun Wu",
      "Vincent Sitzmann"
    ]
  },
  "https://openreview.net/forum?id=dXnccpSSYF": {
    "title": "Pareto Optimization for Active Learning under Out-of-Distribution Data Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xueying Zhan",
      "Zeyu Dai",
      "Qingzhong Wang",
      "Qing Li",
      "Haoyi Xiong",
      "Dejing Dou",
      "Antoni B. Chan"
    ]
  },
  "https://openreview.net/forum?id=jYkWdJzTwn": {
    "title": "Predicting Out-of-Domain Generalization with Neighborhood Invariance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nathan Hoyen Ng",
      "Neha Hulkund",
      "Kyunghyun Cho",
      "Marzyeh Ghassemi"
    ]
  },
  "https://openreview.net/forum?id=ipe0IMglFF": {
    "title": "Empirical Study on Optimizer Selection for Out-of-Distribution Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hiroki Naganuma",
      "Kartik Ahuja",
      "Shiro Takagi",
      "Tetsuya Motokawa",
      "Rio Yokota",
      "Kohta Ishikawa",
      "Ikuro Sato",
      "Ioannis Mitliagkas"
    ]
  },
  "https://openreview.net/forum?id=FDbQGCAViI": {
    "title": "The Eigenlearning Framework: A Conservation Law Perspective on Kernel Ridge Regression and Wide Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "James B Simon",
      "Madeline Dickens",
      "Dhruva Karkada",
      "Michael R DeWeese"
    ]
  },
  "https://openreview.net/forum?id=kiPsMct7vL": {
    "title": "Unsupervised Domain Adaptation via Minimized Joint Error",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dexuan Zhang",
      "Thomas Westfechtel",
      "Tatsuya Harada"
    ]
  },
  "https://openreview.net/forum?id=r6oHDYOZ6p": {
    "title": "Undersampling is a Minimax Optimal Robustness Intervention in Nonparametric Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Niladri S. Chatterji",
      "Saminul Haque",
      "Tatsunori Hashimoto"
    ]
  },
  "https://openreview.net/forum?id=K0CAGgjYS1": {
    "title": "On the Convergence and Calibration of Deep Learning with Differential Privacy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiqi Bu",
      "Hua Wang",
      "Zongyu Dai",
      "Qi Long"
    ]
  },
  "https://openreview.net/forum?id=B0WYWvVA2r": {
    "title": "Attentional-Biased Stochastic Gradient Descent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Qi",
      "Yi Xu",
      "Wotao Yin",
      "Rong Jin",
      "Tianbao Yang"
    ]
  },
  "https://openreview.net/forum?id=G2GKiicaJI": {
    "title": "Reinforcement Teaching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Calarina Muslimani",
      "Alex Lewandowski",
      "Dale Schuurmans",
      "Matthew E. Taylor",
      "Jun Luo"
    ]
  },
  "https://openreview.net/forum?id=zshemTAa6U": {
    "title": "Test-Time Adaptation for Visual Document Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sayna Ebrahimi",
      "Sercan O Arik",
      "Tomas Pfister"
    ]
  },
  "https://openreview.net/forum?id=W98AEKQ38Y": {
    "title": "Learning to Incentivize Improvements from Strategic Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yatong Chen",
      "Jialu Wang",
      "Yang Liu"
    ]
  },
  "https://openreview.net/forum?id=TSy0vuwQFN": {
    "title": "Finding Competence Regions in Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jens Müller",
      "Stefan T. Radev",
      "Robert Schmier",
      "Felix Draxler",
      "Carsten Rother",
      "Ullrich Koethe"
    ]
  },
  "https://openreview.net/forum?id=r7imkFEAQb": {
    "title": "Noise-robust Graph Learning by Estimating and Leveraging Pairwise Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuefeng Du",
      "Tian Bian",
      "Yu Rong",
      "Bo Han",
      "Tongliang Liu",
      "Tingyang Xu",
      "Wenbing Huang",
      "Yixuan Li",
      "Junzhou Huang"
    ]
  },
  "https://openreview.net/forum?id=SwlfyDq6B3": {
    "title": "3D-Aware Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sherwin Bahmani",
      "Jeong Joon Park",
      "Despoina Paschalidou",
      "Hao Tang",
      "Gordon Wetzstein",
      "Leonidas Guibas",
      "Luc Van Gool",
      "Radu Timofte"
    ]
  },
  "https://openreview.net/forum?id=sixOD8YVvM": {
    "title": "Bounded Space Differentially Private Quantiles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Alabi",
      "Omri Ben-Eliezer",
      "Anamay Chaturvedi"
    ]
  },
  "https://openreview.net/forum?id=pxpbTdUEpD": {
    "title": "The Stack: 3 TB of permissively licensed source code",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Denis Kocetkov",
      "Raymond Li",
      "Loubna Ben allal",
      "Jia LI",
      "Chenghao Mou",
      "Yacine Jernite",
      "Margaret Mitchell",
      "Carlos Muñoz Ferrandis",
      "Sean Hughes",
      "Thomas Wolf",
      "Dzmitry Bahdanau",
      "Leandro Von Werra",
      "Harm de Vries"
    ]
  },
  "https://openreview.net/forum?id=sWQJfb2GSk": {
    "title": "Exploring the Approximation Capabilities of Multiplicative Neural Networks for Smooth Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ido Ben-Shaul",
      "Tomer Galanti",
      "Shai Dekel"
    ]
  },
  "https://openreview.net/forum?id=na5sHG69rI": {
    "title": "Assuming Locally Equal Calibration Errors for Non-Parametric Multiclass Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaspar Valk",
      "Meelis Kull"
    ]
  },
  "https://openreview.net/forum?id=OILbP0WErR": {
    "title": "Learning Graph Structure from Convolutional Mixtures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Max Wasserman",
      "Saurabh Sihag",
      "Gonzalo Mateos",
      "Alejandro Ribeiro"
    ]
  },
  "https://openreview.net/forum?id=NrfSRtTpN5": {
    "title": "Learning Object-Centric Neural Scattering Functions for Free-viewpoint Relighting and Scene Composition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hong-Xing Yu",
      "Michelle Guo",
      "Alireza Fathi",
      "Yen-Yu Chang",
      "Eric Ryan Chan",
      "Ruohan Gao",
      "Thomas Funkhouser",
      "Jiajun Wu"
    ]
  },
  "https://openreview.net/forum?id=Y42xVBQusn": {
    "title": "Contextualize Me – The Case for Context in Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Carolin Benjamins",
      "Theresa Eimer",
      "Frederik Schubert",
      "Aditya Mohan",
      "Sebastian Döhler",
      "André Biedenkapp",
      "Bodo Rosenhahn",
      "Frank Hutter",
      "Marius Lindauer"
    ]
  },
  "https://openreview.net/forum?id=KxBQPz7HKh": {
    "title": "Multi-dimensional concept discovery (MCD): A unifying framework with completeness guarantees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Johanna Vielhaben",
      "Stefan Bluecher",
      "Nils Strodthoff"
    ]
  },
  "https://openreview.net/forum?id=TyBd56VK7z": {
    "title": "Dr-Fairness: Dynamic Data Ratio Adjustment for Fair Training on Real and Generated Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuji Roh",
      "Weili Nie",
      "De-An Huang",
      "Steven Euijong Whang",
      "Arash Vahdat",
      "Anima Anandkumar"
    ]
  },
  "https://openreview.net/forum?id=N7lCDaeNiS": {
    "title": "Federated Learning under Covariate Shifts with Generalization Guarantees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Ramezani-Kebrya",
      "Fanghui Liu",
      "Thomas Pethick",
      "Grigorios Chrysos",
      "Volkan Cevher"
    ]
  },
  "https://openreview.net/forum?id=pbs22kJmEO": {
    "title": "When Does Uncertainty Matter?: Understanding the Impact of Predictive Uncertainty in ML Assisted Decision Making",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sean McGrath",
      "Parth Mehta",
      "Alexandra Zytek",
      "Isaac Lage",
      "Himabindu Lakkaraju"
    ]
  },
  "https://openreview.net/forum?id=QhHLwn3D0Y": {
    "title": "The Robustness Limits of SoTA Vision Models to Natural Variation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mark Ibrahim",
      "Quentin Garrido",
      "Ari S. Morcos",
      "Diane Bouchacourt"
    ]
  },
  "https://openreview.net/forum?id=CqTkapZ6H9": {
    "title": "Robust Multi-Agent Reinforcement Learning with State Uncertainty",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sihong He",
      "Songyang Han",
      "Sanbao Su",
      "Shuo Han",
      "Shaofeng Zou",
      "Fei Miao"
    ]
  },
  "https://openreview.net/forum?id=ClIcmwdlxn": {
    "title": "Optimum-statistical Collaboration Towards General and Efficient Black-box Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjie Li",
      "Chi-Hua Wang",
      "Guang Cheng",
      "Qifan Song"
    ]
  },
  "https://openreview.net/forum?id=KBhSyBBeeO": {
    "title": "An Adaptive Half-Space Projection Method for Stochastic Optimization Problems with Group Sparse Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yutong Dai",
      "Tianyi Chen",
      "Guanyi Wang",
      "Daniel Robinson"
    ]
  },
  "https://openreview.net/forum?id=iDNMZgjJuJ": {
    "title": "Causally-guided Regularization of Graph Attention Improves Generalizability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander P Wu",
      "Thomas Markovich",
      "Bonnie Berger",
      "Nils Yannick Hammerla",
      "Rohit Singh"
    ]
  },
  "https://openreview.net/forum?id=nEX2q5B2RQ": {
    "title": "Analyzing Deep PAC-Bayesian Learning with Neural Tangent Kernel: Convergence, Analytic Generalization Bound, and Efficient Hyperparameter Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Huang",
      "Chunrui Liu",
      "Yilan Chen",
      "Richard Yi Da Xu",
      "Miao Zhang",
      "Tsui-Wei Weng"
    ]
  },
  "https://openreview.net/forum?id=ttzypy3kT7": {
    "title": "High-Modality Multimodal Transformer: Quantifying Modality & Interaction Heterogeneity for High-Modality Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paul Pu Liang",
      "Yiwei Lyu",
      "Xiang Fan",
      "Jeffrey Tsaw",
      "Yudong Liu",
      "Shentong Mo",
      "Dani Yogatama",
      "Louis-Philippe Morency",
      "Russ Salakhutdinov"
    ]
  },
  "https://openreview.net/forum?id=TH6YrEcbth": {
    "title": "Learning Interpolations between Boltzmann Densities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bálint Máté",
      "François Fleuret"
    ]
  },
  "https://openreview.net/forum?id=LjDFIWWVVa": {
    "title": "Retiring $\\Delta \\text{DP}$: New Distribution-Level Metrics for Demographic Parity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaotian Han",
      "Zhimeng Jiang",
      "Hongye Jin",
      "Zirui Liu",
      "Na Zou",
      "Qifan Wang",
      "Xia Hu"
    ]
  },
  "https://openreview.net/forum?id=2f81Q622ww": {
    "title": "Generating Adversarial Examples with Task Oriented Multi-Objective Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anh Tuan Bui",
      "Trung Le",
      "He Zhao",
      "Quan Hung Tran",
      "Paul Montague",
      "Dinh Phung"
    ]
  },
  "https://openreview.net/forum?id=D45gGvUZp2": {
    "title": "Denise: Deep Robust Principal Component Analysis for Positive Semidefinite Matrices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Calypso Herrera",
      "Florian Krach",
      "Anastasis Kratsios",
      "Pierre Ruyssen",
      "Josef Teichmann"
    ]
  },
  "https://openreview.net/forum?id=ZME2nZMTvY": {
    "title": "Mean-Field Control based Approximation of Multi-Agent Reinforcement Learning in Presence of a Non-decomposable Shared Global State",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Washim Uddin Mondal",
      "Vaneet Aggarwal",
      "Satish Ukkusuri"
    ]
  },
  "https://openreview.net/forum?id=DdZoPUPm0a": {
    "title": "Interpretable Mixture of Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aya Abdelsalam Ismail",
      "Sercan O Arik",
      "Jinsung Yoon",
      "Ankur Taly",
      "Soheil Feizi",
      "Tomas Pfister"
    ]
  },
  "https://openreview.net/forum?id=162TqkUNPO": {
    "title": "Comparative Generalization Bounds for Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tomer Galanti",
      "Liane Galanti",
      "Ido Ben-Shaul"
    ]
  },
  "https://openreview.net/forum?id=wNBARGxoJn": {
    "title": "Learning to correct spectral methods for simulating turbulent flows",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gideon Dresdner",
      "Dmitrii Kochkov",
      "Peter Christian Norgaard",
      "Leonardo Zepeda-Nunez",
      "Jamie Smith",
      "Michael Brenner",
      "Stephan Hoyer"
    ]
  },
  "https://openreview.net/forum?id=xzCDD9i4IZ": {
    "title": "Cox-Hawkes: doubly stochastic spatiotemporal Poisson processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xenia Miscouridou",
      "Samir Bhatt",
      "George Mohler",
      "Seth Flaxman",
      "Swapnil Mishra"
    ]
  },
  "https://openreview.net/forum?id=ilHM31lXC4": {
    "title": "Personalized Federated Learning: A Unified Framework and Universal Optimization Techniques",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Filip Hanzely",
      "Boxin Zhao",
      "mladen kolar"
    ]
  },
  "https://openreview.net/forum?id=l5BzfQhROl": {
    "title": "Generating Teammates for Training Robust Ad Hoc Teamwork Agents via Best-Response Diversity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arrasy Rahman",
      "Elliot Fosong",
      "Ignacio Carlucho",
      "Stefano V Albrecht"
    ]
  },
  "https://openreview.net/forum?id=ZgXfXSz51n": {
    "title": "Guillotine Regularization: Why removing layers is needed to improve generalization in Self-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Florian Bordes",
      "Randall Balestriero",
      "Quentin Garrido",
      "Adrien Bardes",
      "Pascal Vincent"
    ]
  },
  "https://openreview.net/forum?id=MTFf1rDDEI": {
    "title": "Successor Feature Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chris Reinke",
      "Xavier Alameda-Pineda"
    ]
  },
  "https://openreview.net/forum?id=Jjl2c8kWUc": {
    "title": "Lightweight Learner for Shared Knowledge Lifelong Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunhao Ge",
      "Yuecheng Li",
      "Di Wu",
      "Ao Xu",
      "Adam M. Jones",
      "Amanda Sofie Rios",
      "Iordanis Fostiropoulos",
      "shixian wen",
      "Po-Hsuan Huang",
      "Zachary William Murdock",
      "Gozde Sahin",
      "Shuo Ni",
      "Kiran Lekkala",
      "Sumedh Anand Sontakke",
      "Laurent Itti"
    ]
  },
  "https://openreview.net/forum?id=6rbcq0qacA": {
    "title": "Deep Plug-and-Play Clustering with Unknown Number of Clusters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "An Xiao",
      "Hanting Chen",
      "Tianyu Guo",
      "QINGHUA ZHANG",
      "Yunhe Wang"
    ]
  },
  "https://openreview.net/forum?id=v73h3bYE2Z": {
    "title": "When to Trust Aggregated Gradients: Addressing Negative Client Sampling in Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenkai Yang",
      "Yankai Lin",
      "Guangxiang Zhao",
      "Peng Li",
      "Jie Zhou",
      "Xu Sun"
    ]
  },
  "https://openreview.net/forum?id=R8TU3pfzFr": {
    "title": "A Measure of the Complexity of Neural Representations based on Partial Information Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Alexander Ehrlich",
      "Andreas Christian Schneider",
      "Viola Priesemann",
      "Michael Wibral",
      "Abdullah Makkeh"
    ]
  },
  "https://openreview.net/forum?id=MR4glug5GU": {
    "title": "Trip-ROMA: Self-Supervised Learning with Triplets and Random Mappings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenbin Li",
      "Xuesong Yang",
      "Meihao Kong",
      "Lei Wang",
      "Jing Huo",
      "Yang Gao",
      "Jiebo Luo"
    ]
  },
  "https://openreview.net/forum?id=DUsgPi3oCC": {
    "title": "Conditional Permutation Invariant Flows",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Berend Zwartsenberg",
      "Adam Scibior",
      "Matthew Niedoba",
      "Vasileios Lioutas",
      "Justice Sefas",
      "Yunpeng Liu",
      "Setareh Dabiri",
      "Jonathan Wilder Lavington",
      "Trevor Campbell",
      "Frank Wood"
    ]
  },
  "https://openreview.net/forum?id=RLYkyucU6k": {
    "title": "Agent-State Construction with Auxiliary Inputs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruo Yu Tao",
      "Adam White",
      "Marlos C. Machado"
    ]
  },
  "https://openreview.net/forum?id=9KoBOlstTq": {
    "title": "Modelling sequential branching dynamics with a multivariate branching Gaussian process",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elvijs Sarkans",
      "Sumon Ahmed",
      "Magnus Rattray",
      "Alexis Boukouvalas"
    ]
  },
  "https://openreview.net/forum?id=j3oQF9coJd": {
    "title": "U-NO: U-shaped Neural Operators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Ashiqur Rahman",
      "Zachary E Ross",
      "Kamyar Azizzadenesheli"
    ]
  },
  "https://openreview.net/forum?id=nOk4XEB7Ke": {
    "title": "Fast&Fair: Training Acceleration and Bias Mitigation for GNNs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oyku Deniz Kose",
      "Yanning Shen"
    ]
  },
  "https://openreview.net/forum?id=IqJsyulDUX": {
    "title": "Ensembles for Uncertainty Estimation: Benefits of Prior Functions and Bootstrapping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vikranth Dwaracherla",
      "Zheng Wen",
      "Ian Osband",
      "Xiuyuan Lu",
      "Seyed Mohammad Asghari",
      "Benjamin Van Roy"
    ]
  },
  "https://openreview.net/forum?id=W98rebBxlQ": {
    "title": "Soft Diffusion: Score Matching with General Corruptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giannis Daras",
      "Mauricio Delbracio",
      "Hossein Talebi",
      "Alex Dimakis",
      "Peyman Milanfar"
    ]
  },
  "https://openreview.net/forum?id=mySiFHCeAl": {
    "title": "Spectral Regularization Allows Data-frugal Learning over Combinatorial Spaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amirali Aghazadeh",
      "Nived Rajaraman",
      "Tony Tu",
      "Kannan Ramchandran"
    ]
  },
  "https://openreview.net/forum?id=jVMMdg31De": {
    "title": "A Cubic Regularization Approach for Finding Local Minimax Points in Nonconvex Minimax Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyi Chen",
      "Zhengyang Hu",
      "Qunwei Li",
      "Zhe Wang",
      "Yi Zhou"
    ]
  },
  "https://openreview.net/forum?id=SEDWlhcFWA": {
    "title": "Assisted Learning for Organizations with Limited Imbalanced Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Chen",
      "Jiaying Zhou",
      "Jie Ding",
      "Yi Zhou"
    ]
  },
  "https://openreview.net/forum?id=EPPqt3uERT": {
    "title": "Transformer for Partial Differential Equations' Operator Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijie Li",
      "Kazem Meidani",
      "Amir Barati Farimani"
    ]
  },
  "https://openreview.net/forum?id=gvcDSDYUZx": {
    "title": "Efficient Model-Based Multi-Agent Mean-Field Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Barna Pásztor",
      "Andreas Krause",
      "Ilija Bogunovic"
    ]
  },
  "https://openreview.net/forum?id=y4CGF1A8VG": {
    "title": "Machine Explanations and Human Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chacha Chen",
      "Shi Feng",
      "Amit Sharma",
      "Chenhao Tan"
    ]
  },
  "https://openreview.net/forum?id=9aXKUJEKwV": {
    "title": "Learning to Look by Self-Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew Koichi Grimes",
      "Joseph Varughese Modayil",
      "Piotr W Mirowski",
      "Dushyant Rao",
      "Raia Hadsell"
    ]
  },
  "https://openreview.net/forum?id=slsAQHpS7n": {
    "title": "Computationally-efficient initialisation of GPs: The generalised variogram method",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Felipe Tobar",
      "Elsa Cazelles",
      "Taco de Wolff"
    ]
  },
  "https://openreview.net/forum?id=f4VyYhkRvi": {
    "title": "Fairness via In-Processing in the Over-parameterized Regime: A Cautionary Tale with MinDiff Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akshaj Kumar Veldanda",
      "Ivan Brugere",
      "Jiahao Chen",
      "Sanghamitra Dutta",
      "Alan Mishler",
      "Siddharth Garg"
    ]
  },
  "https://openreview.net/forum?id=Oq5XKRVYpQ": {
    "title": "Graph-based Multi-ODE Neural Networks for Spatio-Temporal Traffic Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zibo Liu",
      "Parshin Shojaee",
      "Chandan K. Reddy"
    ]
  },
  "https://openreview.net/forum?id=iMmsCI0JsS": {
    "title": "TimeSeAD: Benchmarking Deep Multivariate Time-Series Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dennis Wagner",
      "Tobias Michels",
      "Florian C.F. Schulz",
      "Arjun Nair",
      "Maja Rudolph",
      "Marius Kloft"
    ]
  },
  "https://openreview.net/forum?id=I4IkGmgFJz": {
    "title": "Data Models for Dataset Drift Controls in Machine Learning With Optical Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luis Oala",
      "Marco Aversa",
      "Gabriel Nobis",
      "Kurt Willis",
      "Yoan Neuenschwander",
      "Michèle Buck",
      "Christian Matek",
      "Jerome Extermann",
      "Enrico Pomarico",
      "Wojciech Samek",
      "Roderick Murray-Smith",
      "Christoph Clausen",
      "Bruno Sanguinetti"
    ]
  },
  "https://openreview.net/forum?id=1nhTDzxxMA": {
    "title": "Multi-Source Transfer Learning for Deep Model-Based Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Remo Sasso",
      "Matthia Sabatelli",
      "Marco A. Wiering"
    ]
  },
  "https://openreview.net/forum?id=YwNrPLjHSL": {
    "title": "Do Vision-Language Pretrained Models Learn Composable Primitive Concepts?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tian Yun",
      "Usha Bhalla",
      "Ellie Pavlick",
      "Chen Sun"
    ]
  },
  "https://openreview.net/forum?id=KSvr8A62MD": {
    "title": "A Simulation Environment and Reinforcement Learning Method for Waste Reduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sami Jullien",
      "Mozhdeh Ariannezhad",
      "Paul Groth",
      "Maarten de Rijke"
    ]
  },
  "https://openreview.net/forum?id=JkIH4MeOc3": {
    "title": "Group Fairness in Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Harsh Satija",
      "Alessandro Lazaric",
      "Matteo Pirotta",
      "Joelle Pineau"
    ]
  },
  "https://openreview.net/forum?id=OarsigVib0": {
    "title": "On the Statistical Complexity of Estimation and Testing under Privacy Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Clément Lalanne",
      "Aurélien Garivier",
      "Rémi Gribonval"
    ]
  },
  "https://openreview.net/forum?id=B4J40x7NjA": {
    "title": "Positive Difference Distribution for Image Outlier Detection using Normalizing Flows and Contrastive Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Robert Schmier",
      "Ullrich Koethe",
      "Christoph-Nikolas Straehle"
    ]
  },
  "https://openreview.net/forum?id=s9efQF3QW1": {
    "title": "Uncovering the Representation of Spiking Neural Networks Trained with Surrogate Gradient",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhang Li",
      "Youngeun Kim",
      "Hyoungseob Park",
      "Priyadarshini Panda"
    ]
  },
  "https://openreview.net/forum?id=qxrwt6F3sf": {
    "title": "PAC-Bayes Generalisation Bounds for Heavy-Tailed Losses through Supermartingales",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maxime Haddouche",
      "Benjamin Guedj"
    ]
  },
  "https://openreview.net/forum?id=Sb6p5mcefw": {
    "title": "Generalization as Dynamical Robustness--The Role of Riemannian Contraction in Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leo Kozachkov",
      "Patrick Wensing",
      "Jean-Jacques Slotine"
    ]
  },
  "https://openreview.net/forum?id=Hnr23knZfY": {
    "title": "POLTER: Policy Trajectory Ensemble Regularization for Unsupervised Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Frederik Schubert",
      "Carolin Benjamins",
      "Sebastian Döhler",
      "Bodo Rosenhahn",
      "Marius Lindauer"
    ]
  },
  "https://openreview.net/forum?id=8WUyeeMxMH": {
    "title": "Proximal Curriculum for Reinforcement Learning Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Georgios Tzannetos",
      "Bárbara Gomes Ribeiro",
      "Parameswaran Kamalaruban",
      "Adish Singla"
    ]
  },
  "https://openreview.net/forum?id=R6W7zkMz0P": {
    "title": "Pre-trained Perceptual Features Improve Differentially Private Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Frederik Harder",
      "Milad Jalali",
      "Danica J. Sutherland",
      "Mijung Park"
    ]
  },
  "https://openreview.net/forum?id=SM1BkjGePI": {
    "title": "Bridging performance gap between minimal and maximal SVM models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ondrej Such",
      "René Fabricius"
    ]
  },
  "https://openreview.net/forum?id=YJDqQSAuB6": {
    "title": "Weisfeiler and Leman Go Infinite: Spectral and Combinatorial Pre-Colorings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Or Feldman",
      "Amit Boyarski",
      "Shai Feldman",
      "Dani Kogan",
      "Avi Mendelson",
      "Chaim Baskin"
    ]
  },
  "https://openreview.net/forum?id=2Yo9xqR6Ab": {
    "title": "Jacobian-based Causal Discovery with Nonlinear ICA",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Patrik Reizinger",
      "Yash Sharma",
      "Matthias Bethge",
      "Bernhard Schölkopf",
      "Ferenc Huszár",
      "Wieland Brendel"
    ]
  },
  "https://openreview.net/forum?id=1IYJfwJtjQ": {
    "title": "FASTRAIN-GNN: Fast and Accurate Self-Training for Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amrit Nagarajan",
      "Anand Raghunathan"
    ]
  },
  "https://openreview.net/forum?id=5nVJlKgmxp": {
    "title": "Online Optimal Tracking of Linear Systems with Adversarial Disturbances",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Farnaz Adib Yaghmaie",
      "Hamidreza Modares"
    ]
  },
  "https://openreview.net/forum?id=T1XtOqrVKn": {
    "title": "Reducing Predictive Feature Suppression in Resource-Constrained Contrastive Image-Caption Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maurits Bleeker",
      "Andrew Yates",
      "Maarten de Rijke"
    ]
  },
  "https://openreview.net/forum?id=fempQstMbV": {
    "title": "Deep Double Descent via Smooth Interpolation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matteo Gamba",
      "Erik Englesson",
      "Mårten Björkman",
      "Hossein Azizpour"
    ]
  },
  "https://openreview.net/forum?id=4zCgjqjzAv": {
    "title": "Bayesian Transformed Gaussian Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinran Zhu",
      "Leo Huang",
      "Eric Hans Lee",
      "Cameron Alexander Ibrahim",
      "David Bindel"
    ]
  },
  "https://openreview.net/forum?id=AZ4GobeSLq": {
    "title": "A Variational Perspective on Generative Flow Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heiko Zimmermann",
      "Fredrik Lindsten",
      "Jan-Willem van de Meent",
      "Christian A Naesseth"
    ]
  },
  "https://openreview.net/forum?id=oq3tx5kinu": {
    "title": "Active Learning of Ordinal Embeddings: A User Study on Football Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christoffer Löffler",
      "Kion Fallah",
      "Stefano Fenu",
      "Dario Zanca",
      "Bjoern Eskofier",
      "Christopher John Rozell",
      "Christopher Mutschler"
    ]
  },
  "https://openreview.net/forum?id=55BcghgicI": {
    "title": "Differentially private partitioned variational inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mikko A. Heikkilä",
      "Matthew Ashman",
      "Siddharth Swaroop",
      "Richard E Turner",
      "Antti Honkela"
    ]
  },
  "https://openreview.net/forum?id=a0T3nOP9sB": {
    "title": "Adaptive patch foraging in deep reinforcement learning agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nathan Wispinski",
      "Andrew Butcher",
      "Kory Wallace Mathewson",
      "Craig S Chapman",
      "Matthew Botvinick",
      "Patrick M. Pilarski"
    ]
  },
  "https://openreview.net/forum?id=onufdyHvqN": {
    "title": "Private Multi-Task Learning: Formulation and Applications to Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengyuan Hu",
      "Steven Wu",
      "Virginia Smith"
    ]
  },
  "https://openreview.net/forum?id=82hRiAbnnm": {
    "title": "Sobolev Spaces, Kernels and Discrepancies over Hyperspheres",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simon Hubbert",
      "Emilio Porcu",
      "Chris J. Oates",
      "Mark Girolami"
    ]
  },
  "https://openreview.net/forum?id=SgTKk6ryPr": {
    "title": "Monotone deep Boltzmann machines",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhili Feng",
      "Ezra Winston",
      "J Zico Kolter"
    ]
  },
  "https://openreview.net/forum?id=4eL6z9ziw7": {
    "title": "NovelCraft: A Dataset for Novelty Detection and Discovery in Open Worlds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Patrick Feeney",
      "Sarah Schneider",
      "Panagiotis Lymperopoulos",
      "Liping Liu",
      "Matthias Scheutz",
      "Michael C Hughes"
    ]
  },
  "https://openreview.net/forum?id=OsKXlWamTQ": {
    "title": "Integrating Bayesian Network Structure into Residual Flows and Variational Autoencoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jacobie Mouton",
      "Rodney Stephen Kroon"
    ]
  },
  "https://openreview.net/forum?id=QTXocpAP9p": {
    "title": "Neural Collapse: A Review on Modelling Principles and Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vignesh Kothapalli"
    ]
  },
  "https://openreview.net/forum?id=ZOAb497iaY": {
    "title": "Unifying physical systems' inductive biases in neural ODE using dynamics constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Heng Lim",
      "Muhammad Firmansyah Kasim"
    ]
  },
  "https://openreview.net/forum?id=tE2NiMGd07": {
    "title": "Bridging Graph Position Encodings for Transformers with Weighted Graph-Walking Automata",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Patrick Soga",
      "David Chiang"
    ]
  },
  "https://openreview.net/forum?id=jdGMBgYvfX": {
    "title": "UncertaINR: Uncertainty Quantification of End-to-End Implicit Neural Representations for Computed Tomography",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francisca Vasconcelos",
      "Bobby He",
      "Nalini M Singh",
      "Yee Whye Teh"
    ]
  },
  "https://openreview.net/forum?id=FdMWtpVT1I": {
    "title": "Training Data Size Induced Double Descent For Denoising Feedforward Neural Networks and the Role of Training Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rishi Sonthalia",
      "Raj Rao Nadakuditi"
    ]
  },
  "https://openreview.net/forum?id=10JdgrzNOk": {
    "title": "Scalable Deep Compressive Sensing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhonghao Zhang",
      "Yipeng Liu",
      "Xingyu Cao",
      "Fei Wen",
      "Ce Zhu"
    ]
  },
  "https://openreview.net/forum?id=MRLHN4MSmA": {
    "title": "A Modulation Layer to Increase Neural Network Robustness Against Data Quality Issues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohamed Abdelhack",
      "Jiaming Zhang",
      "Sandhya Tripathi",
      "Bradley A Fritz",
      "Daniel Felsky",
      "Michael Avidan",
      "Yixin Chen",
      "Christopher Ryan King"
    ]
  },
  "https://openreview.net/forum?id=gR9UVgH8PZ": {
    "title": "Neural Shape Compiler: A Unified Framework for Transforming between Text, Point Cloud, and Program",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tiange Luo",
      "Honglak Lee",
      "Justin Johnson"
    ]
  },
  "https://openreview.net/forum?id=6IFi2soduD": {
    "title": "Can Pruning Improve Certified Robustness of Neural Networks?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhangheng LI",
      "Tianlong Chen",
      "Linyi Li",
      "Bo Li",
      "Zhangyang Wang"
    ]
  },
  "https://openreview.net/forum?id=v5jwDLqfQo": {
    "title": "Extended Agriculture-Vision: An Extension of a Large Aerial Image Dataset for Agricultural Pattern Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Wu",
      "David Pichler",
      "Daniel Marley",
      "Naira Hovakimyan",
      "David A Wilson",
      "Jennifer Hobbs"
    ]
  },
  "https://openreview.net/forum?id=OJtYpdiHNo": {
    "title": "Transframer: Arbitrary Frame Prediction with Generative Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Charlie Nash",
      "Joao Carreira",
      "Jacob C Walker",
      "Iain Barr",
      "Andrew Jaegle",
      "Mateusz Malinowski",
      "Peter Battaglia"
    ]
  },
  "https://openreview.net/forum?id=xqS8k9E75c": {
    "title": "Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dennis Thomas Ulmer",
      "Christian Hardmeier",
      "Jes Frellsen"
    ]
  },
  "https://openreview.net/forum?id=jM8nzUzBWr": {
    "title": "Estimating the Density Ratio between Distributions with High Discrepancy using Multinomial Logistic Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akash Srivastava",
      "Seungwook Han",
      "Kai Xu",
      "Benjamin Rhodes",
      "Michael U. Gutmann"
    ]
  },
  "https://openreview.net/forum?id=QzWr4w8PXx": {
    "title": "A Revenue Function for Comparison-Based Hierarchical Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aishik Mandal",
      "Michaël Perrot",
      "Debarghya Ghoshdastidar"
    ]
  },
  "https://openreview.net/forum?id=C1Xl8dYCBn": {
    "title": "ChemSpacE: Interpretable and Interactive Chemical Space Exploration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanqi Du",
      "Xian Liu",
      "Nilay Mahesh Shah",
      "Shengchao Liu",
      "Jieyu Zhang",
      "Bolei Zhou"
    ]
  },
  "https://openreview.net/forum?id=dQxBRqCjLr": {
    "title": "A Free Lunch with Influence Functions? An Empirical Evaluation of Influence Functions for Average Treatment Effect Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew James Vowels",
      "Sina Akbari",
      "Necati Cihan Camgoz",
      "Richard Bowden"
    ]
  },
  "https://openreview.net/forum?id=TzRXyO3CzX": {
    "title": "Clustering using Approximate Nearest Neighbour Oracles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Enayat Ullah",
      "Harry Lang",
      "Raman Arora",
      "Vladimir Braverman"
    ]
  },
  "https://openreview.net/forum?id=JwgVBv18RG": {
    "title": "Bayesian Optimization with Informative Covariance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Afonso Eduardo",
      "Michael U. Gutmann"
    ]
  },
  "https://openreview.net/forum?id=2UQv8L1Cv9": {
    "title": "Turning Normalizing Flows into Monge Maps with Geodesic Gaussian Preserving Flows",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guillaume Morel",
      "Lucas Drumetz",
      "Simon Benaïchouche",
      "Nicolas Courty",
      "François Rousseau"
    ]
  },
  "https://openreview.net/forum?id=h4BYtZ79uy": {
    "title": "Graph Neural Networks Designed for Different Graph Types: A Survey",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Josephine Thomas",
      "Alice Moallemy-Oureh",
      "Silvia Beddar-Wiesing",
      "Clara Holzhüter"
    ]
  },
  "https://openreview.net/forum?id=KwWKB9Bqam": {
    "title": "Generalization bounds for Kernel Canonical Correlation Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Enayat Ullah",
      "Raman Arora"
    ]
  },
  "https://openreview.net/forum?id=gyhiZYrk5y": {
    "title": "Learning Identity-Preserving Transformations on Data Manifolds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marissa Catherine Connor",
      "Kion Fallah",
      "Christopher John Rozell"
    ]
  },
  "https://openreview.net/forum?id=YtU0nDb5e8": {
    "title": "A Halfspace-Mass Depth-Based Method for Adversarial Attack Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marine Picot",
      "Federica Granese",
      "Guillaume Staerman",
      "Marco Romanelli",
      "Francisco Messina",
      "Pablo Piantanida",
      "Pierre Colombo"
    ]
  },
  "https://openreview.net/forum?id=qdDmxzGuzu": {
    "title": "Reusable Options through Gradient-based Meta Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Kuric",
      "Herke van Hoof"
    ]
  },
  "https://openreview.net/forum?id=qvRWcDXBam": {
    "title": "Containing a spread through sequential learning: to exploit or to explore?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingran Chen",
      "Hesam Nikpey",
      "Jungyeol Kim",
      "Saswati Sarkar",
      "Shirin Saeedi Bidokhti"
    ]
  },
  "https://openreview.net/forum?id=WVwnccBJLz": {
    "title": "Bidirectional View based Consistency Regularization for Semi-Supervised Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuntao Du",
      "娟 江",
      "Hongtao Luo",
      "Haiyang Yang",
      "MingCai Chen",
      "Chongjun Wang"
    ]
  },
  "https://openreview.net/forum?id=UvJBKWaSSH": {
    "title": "FLUID: A Unified Evaluation Framework for Flexible Sequential Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew Wallingford",
      "Aditya Kusupati",
      "Keivan Alizadeh-Vahid",
      "Aaron Walsman",
      "Aniruddha Kembhavi",
      "Ali Farhadi"
    ]
  },
  "https://openreview.net/forum?id=bCiNWDmlY2": {
    "title": "The Low-Rank Simplicity Bias in Deep Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minyoung Huh",
      "Hossein Mobahi",
      "Richard Zhang",
      "Brian Cheung",
      "Pulkit Agrawal",
      "Phillip Isola"
    ]
  },
  "https://openreview.net/forum?id=LIT8tjs6rJ": {
    "title": "Parameter Efficient Node Classification on Homophilic Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucas Prieto",
      "Jeroen Den Boef",
      "Paul Groth",
      "Joran Cornelisse"
    ]
  },
  "https://openreview.net/forum?id=xkrtvHlp3P": {
    "title": "Towards Better Out-of-Distribution Generalization of Neural Algorithmic Reasoning Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sadegh Mahdavi",
      "Kevin Swersky",
      "Thomas Kipf",
      "Milad Hashemi",
      "Christos Thrampoulidis",
      "Renjie Liao"
    ]
  },
  "https://openreview.net/forum?id=9lyqt3rbDc": {
    "title": "L-SVRG and L-Katyusha with Adaptive Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boxin Zhao",
      "Boxiang Lyu",
      "mladen kolar"
    ]
  },
  "https://openreview.net/forum?id=HG11PAmwQ6": {
    "title": "Quantum Policy Iteration via Amplitude Estimation and Grover Search – Towards Quantum Advantage for Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simon Wiedemann",
      "Daniel Hein",
      "Steffen Udluft",
      "Christian B. Mendl"
    ]
  },
  "https://openreview.net/forum?id=tEVpz2xJWX": {
    "title": "Enhancing Diffusion-Based Image Synthesis with Robust Classifier Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bahjat Kawar",
      "Roy Ganz",
      "Michael Elad"
    ]
  },
  "https://openreview.net/forum?id=RjZq6W6FoE": {
    "title": "Improved Overparametrization Bounds for Global Convergence of SGD for Shallow Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bartłomiej Polaczyk",
      "Jacek Cyranka"
    ]
  },
  "https://openreview.net/forum?id=wmGlMhaBe0": {
    "title": "A Unified View of Masked Image Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiliang Peng",
      "Li Dong",
      "Hangbo Bao",
      "Furu Wei",
      "Qixiang Ye"
    ]
  },
  "https://openreview.net/forum?id=11pGlecTz2": {
    "title": "How Robust is Your Fairness? Evaluating and Sustaining Fairness under Unseen Distribution Shifts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haotao Wang",
      "Junyuan Hong",
      "Jiayu Zhou",
      "Zhangyang Wang"
    ]
  },
  "https://openreview.net/forum?id=OzGIu4T4Cz": {
    "title": "Leveraging Demonstrations with Latent Space Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonas Gehring",
      "Deepak Gopinath",
      "Jungdam Won",
      "Andreas Krause",
      "Gabriel Synnaeve",
      "Nicolas Usunier"
    ]
  },
  "https://openreview.net/forum?id=Gp0pHyUyrb": {
    "title": "Solving Nonconvex-Nonconcave Min-Max Problems exhibiting Weak Minty Solutions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Axel Böhm"
    ]
  },
  "https://openreview.net/forum?id=3epEbhdgbv": {
    "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhirong Wu",
      "Zihang Lai",
      "Xiao Sun",
      "Stephen Lin"
    ]
  },
  "https://openreview.net/forum?id=LBA2Jj5Gqn": {
    "title": "Temperature check: theory and practice for training models with softmax-cross-entropy losses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Atish Agarwala",
      "Samuel Stern Schoenholz",
      "Jeffrey Pennington",
      "Yann Dauphin"
    ]
  },
  "https://openreview.net/forum?id=QtrjqVIZna": {
    "title": "Fusion of Global and Local Knowledge for Personalized Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tiansheng Huang",
      "Li Shen",
      "Yan Sun",
      "Weiwei Lin",
      "Dacheng Tao"
    ]
  },
  "https://openreview.net/forum?id=goPsLn3RVo": {
    "title": "Defense Against Reward Poisoning Attacks in Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kiarash Banihashem",
      "Adish Singla",
      "Goran Radanovic"
    ]
  },
  "https://openreview.net/forum?id=DHEZuKStzH": {
    "title": "Learning Energy Conserving Dynamics Efficiently with Hamiltonian Gaussian Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Magnus Ross",
      "Markus Heinonen"
    ]
  },
  "https://openreview.net/forum?id=iDxfGaMYVr": {
    "title": "Continual Learning by Modeling Intra-Class Variation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Longhui Yu",
      "Tianyang Hu",
      "Lanqing HONG",
      "Zhen Liu",
      "Adrian Weller",
      "Weiyang Liu"
    ]
  },
  "https://openreview.net/forum?id=v6anjyEDVW": {
    "title": "Costs and Benefits of Fair Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Zhao"
    ]
  },
  "https://openreview.net/forum?id=kJcwlP7BRs": {
    "title": "Transfer Entropy Bottleneck: Learning Sequence to Sequence Information Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Damjan Kalajdzievski",
      "Ximeng Mao",
      "Pascal Fortier-Poisson",
      "Guillaume Lajoie",
      "Blake Aaron Richards"
    ]
  },
  "https://openreview.net/forum?id=bomdTc9HyL": {
    "title": "Transductive Decoupled Variational Inference for Few-Shot Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anuj Rajeeva Singh",
      "Hadi Jamali-Rad"
    ]
  },
  "https://openreview.net/forum?id=IvsGP7xRvm": {
    "title": "Black-Box Prompt Learning for Pre-trained Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shizhe Diao",
      "Zhichao Huang",
      "Ruijia Xu",
      "Xuechun Li",
      "LIN Yong",
      "Xiao Zhou",
      "Tong Zhang"
    ]
  },
  "https://openreview.net/forum?id=Z2L5d9ay4B": {
    "title": "Image Compression with Product Quantized Masked Image Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alaaeldin El-Nouby",
      "Matthew J. Muckley",
      "Karen Ullrich",
      "Ivan Laptev",
      "Jakob Verbeek",
      "Herve Jegou"
    ]
  },
  "https://openreview.net/forum?id=yhGCKUsKJS": {
    "title": "Action Poisoning Attacks on Linear Contextual Bandits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanlin Liu",
      "Lifeng Lai"
    ]
  },
  "https://openreview.net/forum?id=MKZyHtmfwH": {
    "title": "Mixed effects in machine learning – A flexible mixedML framework to add random effects to supervised machine learning regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pascal Kilian",
      "Sangbeak Ye",
      "Augustin Kelava"
    ]
  },
  "https://openreview.net/forum?id=fTNorIvVXG": {
    "title": "Probing Predictions on OOD Images via Nearest Categories",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yao-Yuan Yang",
      "Cyrus Rashtchian",
      "Ruslan Salakhutdinov",
      "Kamalika Chaudhuri"
    ]
  },
  "https://openreview.net/forum?id=k5m8xXTOrC": {
    "title": "Solving a Special Type of Optimal Transport Problem by a Modified Hungarian Algorithm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiling Xie",
      "Yiling Luo",
      "Xiaoming Huo"
    ]
  },
  "https://openreview.net/forum?id=WoXJFsJ6Zw": {
    "title": "AI-SARAH: Adaptive and Implicit Stochastic Recursive Gradient Methods",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheng Shi",
      "Abdurakhmon Sadiev",
      "Nicolas Loizou",
      "Peter Richtárik",
      "Martin Takáč"
    ]
  },
  "https://openreview.net/forum?id=oXmwAPlbVw": {
    "title": "U-Statistics for Importance-Weighted Variational Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Javier Burroni",
      "Kenta Takatsu",
      "Justin Domke",
      "Daniel Sheldon"
    ]
  },
  "https://openreview.net/forum?id=BVi6MhKO0G": {
    "title": "OADAT: Experimental and Synthetic Clinical Optoacoustic Data for Standardized Image Processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Firat Ozdemir",
      "Berkan Lafci",
      "Xose Luis Dean-Ben",
      "Daniel Razansky",
      "Fernando Perez-Cruz"
    ]
  },
  "https://openreview.net/forum?id=mNEqiC924B": {
    "title": "Stacking Diverse Architectures to Improve Machine Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrea Schioppa",
      "Nal Kalchbrenner"
    ]
  },
  "https://openreview.net/forum?id=GbkWw3jwL9": {
    "title": "Contrastive Search Is What You Need For Neural Text Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixuan Su",
      "Nigel Collier"
    ]
  },
  "https://openreview.net/forum?id=tBl4yBEjKi": {
    "title": "Separable Self-attention for Mobile Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sachin Mehta",
      "Mohammad Rastegari"
    ]
  },
  "https://openreview.net/forum?id=iEq6lhG4O3": {
    "title": "A Flexible Nadaraya-Watson Head Can Offer Explainable and Calibrated Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alan Q. Wang",
      "Mert R. Sabuncu"
    ]
  },
  "https://openreview.net/forum?id=hHiIbk7ApW": {
    "title": "Diffusion-based Time Series Imputation and Forecasting with Structured State Space Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juan Lopez Alcaraz",
      "Nils Strodthoff"
    ]
  },
  "https://openreview.net/forum?id=oe4dl4MCGY": {
    "title": "Robust Hybrid Learning With Expert Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antoine Wehenkel",
      "Jens Behrmann",
      "Hsiang Hsu",
      "Guillermo Sapiro",
      "Gilles Louppe",
      "Joern-Henrik Jacobsen"
    ]
  },
  "https://openreview.net/forum?id=paguBNtqiO": {
    "title": "Improved Differentially Private Riemannian Optimization: Fast Sampling and Variance Reduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saiteja Utpala",
      "Andi Han",
      "Pratik Jawanpuria",
      "Bamdev Mishra"
    ]
  },
  "https://openreview.net/forum?id=lmr2WwlaFc": {
    "title": "Dirichlet Mechanism for Differentially Private KL Divergence Minimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Donlapark Ponnoprat"
    ]
  },
  "https://openreview.net/forum?id=cKsKXR28cG": {
    "title": "Regularized Training of Intermediate Layers for Generative Models for Inverse Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sean Gunn",
      "Jorio Cocola",
      "PAul HAnd"
    ]
  },
  "https://openreview.net/forum?id=6dsvH7pQHH": {
    "title": "Layerwise Bregman Representation Learning of Neural Networks with Applications to Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ehsan Amid",
      "Rohan Anil",
      "Christopher Fifty",
      "Manfred K Warmuth"
    ]
  },
  "https://openreview.net/forum?id=WN1O2MJDST": {
    "title": "Learn, Unlearn and Relearn: An Online Learning Paradigm for Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vijaya Raghavan T Ramkumar",
      "Elahe Arani",
      "Bahram Zonooz"
    ]
  },
  "https://openreview.net/forum?id=5II12ypVQo": {
    "title": "KRADA: Known-region-aware Domain Alignment for Open-set Domain Adaptation in Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenhong Zhou",
      "Feng Liu",
      "Chen Gong",
      "Rongfei Zeng",
      "Tongliang Liu",
      "William Cheung",
      "Bo Han"
    ]
  },
  "https://openreview.net/forum?id=gZna3IiGfl": {
    "title": "Mean-field analysis for heavy ball methods: Dropout-stability, connectivity, and global convergence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Diyuan Wu",
      "Vyacheslav Kungurtsev",
      "Marco Mondelli"
    ]
  },
  "https://openreview.net/forum?id=RZveYHgZbu": {
    "title": "Signed Graph Neural Networks: A Frequency Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rahul Singh",
      "Yongxin Chen"
    ]
  },
  "https://openreview.net/forum?id=TNocbXm5MZ": {
    "title": "Guaranteed Discovery of Control-Endogenous Latent States with Multi-Step Inverse Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alex Lamb",
      "Riashat Islam",
      "Yonathan Efroni",
      "Aniket Rajiv Didolkar",
      "Dipendra Misra",
      "Dylan J Foster",
      "Lekan P Molu",
      "Rajan Chari",
      "Akshay Krishnamurthy",
      "John Langford"
    ]
  },
  "https://openreview.net/forum?id=rm0zIzlhcX": {
    "title": "Beyond Intuition: Rethinking Token Attributions inside Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiamin Chen",
      "Xuhong Li",
      "Lei Yu",
      "Dejing Dou",
      "Haoyi Xiong"
    ]
  },
  "https://openreview.net/forum?id=KQRv0O8iW4": {
    "title": "Finite-Time Analysis of Decentralized Single-Timescale Actor-Critic",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "qijun luo",
      "Xiao Li"
    ]
  },
  "https://openreview.net/forum?id=GcO6ugrLKp": {
    "title": "Supervised Feature Selection with Neuron Evolution in Sparse Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zahra Atashgahi",
      "Xuhao Zhang",
      "Neil Kichler",
      "Shiwei Liu",
      "Lu Yin",
      "Mykola Pechenizkiy",
      "Raymond Veldhuis",
      "Decebal Constantin Mocanu"
    ]
  },
  "https://openreview.net/forum?id=mAx8QqZ14f": {
    "title": "Differentially Private Fréchet Mean on the Manifold of Symmetric Positive Definite (SPD) Matrices with log-Euclidean Metric",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saiteja Utpala",
      "Praneeth Vepakomma",
      "Nina Miolane"
    ]
  },
  "https://openreview.net/forum?id=UntUoeLwwu": {
    "title": "Tailoring to the Tails: Risk Measures for Fine-Grained Tail Sensitivity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christian Fröhlich",
      "Robert Williamson"
    ]
  },
  "https://openreview.net/forum?id=JnsGy9uWtI": {
    "title": "Controlling Neural Network Smoothness for Neural Algorithmic Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David A. Klindt"
    ]
  },
  "https://openreview.net/forum?id=vxyjTUPV24": {
    "title": "Target Propagation via Regularized Inversion for Recurrent Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vincent Roulet",
      "Zaid Harchaoui"
    ]
  },
  "https://openreview.net/forum?id=sMsGv5Kfm3": {
    "title": "Bayesian Causal Bandits with Backdoor Adjustment Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jireh Huang",
      "Qing Zhou"
    ]
  },
  "https://openreview.net/forum?id=znNITCJyTI": {
    "title": "Accelerated Quality-Diversity through Massive Parallelism",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bryan Lim",
      "Maxime Allard",
      "Luca Grillotti",
      "Antoine Cully"
    ]
  },
  "https://openreview.net/forum?id=y7RGNXhGSR": {
    "title": "BIGRoC: Boosting Image Generation via a Robust Classifier",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roy Ganz",
      "Michael Elad"
    ]
  },
  "https://openreview.net/forum?id=CUDdbTT1QC": {
    "title": "Constrained Parameter Inference as a Principle for Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nasir Ahmad",
      "Ellen Schrader",
      "Marcel van Gerven"
    ]
  },
  "https://openreview.net/forum?id=czgMCpvrDM": {
    "title": "SMILE: Sample-to-feature Mixup for Efficient Transfer Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingjian Li",
      "Haoyi Xiong",
      "Cheng-zhong Xu",
      "Dejing Dou"
    ]
  },
  "https://openreview.net/forum?id=Q6ZXm7VBFY": {
    "title": "Optimal Convergence Rates of Deep Convolutional Neural Networks: Additive Ridge Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiying Fang",
      "Guang Cheng"
    ]
  },
  "https://openreview.net/forum?id=myjAVQrRxS": {
    "title": "Dropped Scheduled Task: Mitigating Negative Transfer in Multi-task Learning using Dynamic Task Dropping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aakarsh Malhotra",
      "Mayank Vatsa",
      "Richa Singh"
    ]
  },
  "https://openreview.net/forum?id=wkecshlYxI": {
    "title": "Revisiting adversarial training for the worst-performing class",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Pethick",
      "Grigorios Chrysos",
      "Volkan Cevher"
    ]
  },
  "https://openreview.net/forum?id=JyKNuoZGux": {
    "title": "Calibrate and Debias Layer-wise Sampling for Graph Convolutional Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Chen",
      "Tianning Xu",
      "Dilek Hakkani-Tur",
      "Di Jin",
      "Yun Yang",
      "Ruoqing Zhu"
    ]
  },
  "https://openreview.net/forum?id=nAr9PhyEbQ": {
    "title": "Online Learning for Prediction via Covariance Fitting: Computation, Performance and Robustness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Osama",
      "Dave Zachariah",
      "Peter Stoica",
      "Thomas B. Schön"
    ]
  },
  "https://openreview.net/forum?id=DzJ7JfPXkE": {
    "title": "ViViT: Curvature Access Through The Generalized Gauss-Newton's Low-Rank Structure",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Felix Dangel",
      "Lukas Tatzel",
      "Philipp Hennig"
    ]
  },
  "https://openreview.net/forum?id=EYrRzKPinA": {
    "title": "On a continuous time model of gradient descent dynamics and instability in deep learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mihaela Rosca",
      "Yan Wu",
      "Chongli Qin",
      "Benoit Dherin"
    ]
  },
  "https://openreview.net/forum?id=Lx19EyKX77": {
    "title": "Gradient-adjusted Incremental Target Propagation Provides Effective Credit Assignment in Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sander Dalm",
      "Nasir Ahmad",
      "Luca Ambrogioni",
      "Marcel van Gerven"
    ]
  },
  "https://openreview.net/forum?id=ryUHgEdWCQ": {
    "title": "Proportional Fairness in Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guojun Zhang",
      "Saber Malekmohammadi",
      "Xi Chen",
      "Yaoliang Yu"
    ]
  },
  "https://openreview.net/forum?id=56cTmVrg5w": {
    "title": "On the Role of Fixed Points of Dynamical Systems in Training Physics-Informed Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Franz M. Rohrhofer",
      "Stefan Posch",
      "Clemens Gößnitzer",
      "Bernhard C Geiger"
    ]
  },
  "https://openreview.net/forum?id=EiX2L4sDPG": {
    "title": "VN-Transformer: Rotation-Equivariant Attention for Vector Neurons",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Serge Assaad",
      "Carlton Downey",
      "Rami Al-Rfou'",
      "Nigamaa Nayakanti",
      "Benjamin Sapp"
    ]
  },
  "https://openreview.net/forum?id=1U0aPkBVz0": {
    "title": "lo-fi: distributed fine-tuning without communication",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mitchell Wortsman",
      "Suchin Gururangan",
      "Shen Li",
      "Ali Farhadi",
      "Ludwig Schmidt",
      "Michael Rabbat",
      "Ari S. Morcos"
    ]
  },
  "https://openreview.net/forum?id=mHSAy1n65Z": {
    "title": "Optimal Threshold Labeling for Ordinal Regression Methods",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryoya Yamasaki"
    ]
  },
  "https://openreview.net/forum?id=LTAdaRM29K": {
    "title": "Recognition Models to Learn Dynamics from Partial Observations with Neural ODEs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mona Buisson-Fenet",
      "Valery Morgenthaler",
      "Sebastian Trimpe",
      "Florent Di Meglio"
    ]
  },
  "https://openreview.net/forum?id=GzqdMrFQsE": {
    "title": "Attention Beats Concatenation for Conditioning Neural Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Rebain",
      "Mark J. Matthews",
      "Kwang Moo Yi",
      "Gopal Sharma",
      "Dmitry Lagun",
      "Andrea Tagliasacchi"
    ]
  },
  "https://openreview.net/forum?id=d3rHk4VAf0": {
    "title": "A Ranking Game for Imitation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Harshit Sikchi",
      "Akanksha Saran",
      "Wonjoon Goo",
      "Scott Niekum"
    ]
  },
  "https://openreview.net/forum?id=LfTukxzxTj": {
    "title": "Implicit Ensemble Training for Efficient and Robust Multiagent Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Macheng Shen",
      "JONATHAN P HOW"
    ]
  },
  "https://openreview.net/forum?id=hVT7SHlilx": {
    "title": "Named Tensor Notation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Chiang",
      "Alexander M Rush",
      "Boaz Barak"
    ]
  },
  "https://openreview.net/forum?id=X1pjWMCMB0": {
    "title": "PCPs: Patient Cardiac Prototypes to Probe AI-based Medical Diagnoses, Distill Datasets, and Retrieve Patients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dani Kiyasseh",
      "Tingting Zhu",
      "David A. Clifton"
    ]
  },
  "https://openreview.net/forum?id=RbLsYz1Az9": {
    "title": "On the infinite-depth limit of finite-width neural networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soufiane Hayou"
    ]
  },
  "https://openreview.net/forum?id=85BfDdYMBY": {
    "title": "Intrinsic Dimension for Large-Scale Geometric Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maximilian Stubbemann",
      "Tom Hanika",
      "Friedrich Martin Schneider"
    ]
  },
  "https://openreview.net/forum?id=Xq1sTZTQVm": {
    "title": "Beyond Information Gain: An Empirical Benchmark for Low-Switching-Cost Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shusheng Xu",
      "Yancheng Liang",
      "Yunfei Li",
      "Simon Shaolei Du",
      "Yi Wu"
    ]
  },
  "https://openreview.net/forum?id=EgHnKOLaKW": {
    "title": "DisCo: Improving Compositional Generalization in Visual Reasoning through Distribution Coverage",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joy Hsu",
      "Jiayuan Mao",
      "Jiajun Wu"
    ]
  },
  "https://openreview.net/forum?id=RA0TDqt3hC": {
    "title": "Hidden Heterogeneity: When to Choose Similarity-Based Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kiri L. Wagstaff",
      "Thomas G Dietterich"
    ]
  },
  "https://openreview.net/forum?id=zKnqZeUCLO": {
    "title": "PolyViT: Co-training Vision Transformers on Images, Videos and Audio",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Valerii Likhosherstov",
      "Anurag Arnab",
      "Krzysztof Marcin Choromanski",
      "Mario Lucic",
      "Yi Tay",
      "Mostafa Dehghani"
    ]
  },
  "https://openreview.net/forum?id=lheUXtDNvP": {
    "title": "GSR: A Generalized Symbolic Regression Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tony Tohme",
      "Dehong Liu",
      "KAMAL YOUCEF-TOUMI"
    ]
  },
  "https://openreview.net/forum?id=jbZEUtULft": {
    "title": "Bounding generalization error with input compression: An empirical study with infinite-width networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Angus Galloway",
      "Anna Golubeva",
      "Mahmoud Salem",
      "Mihai Nica",
      "Yani Ioannou",
      "Graham W. Taylor"
    ]
  },
  "https://openreview.net/forum?id=wIXHG8LZ2w": {
    "title": "Learning Representations for Pixel-based Control: What Matters and Why?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manan Tomar",
      "Utkarsh Aashu Mishra",
      "Amy Zhang",
      "Matthew E. Taylor"
    ]
  },
  "https://openreview.net/forum?id=MzWgBjZ6Le": {
    "title": "FedDAG: Federated DAG Structure Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Erdun Gao",
      "Junjia Chen",
      "Li Shen",
      "Tongliang Liu",
      "Mingming Gong",
      "Howard Bondell"
    ]
  },
  "https://openreview.net/forum?id=tnRRHzZPMq": {
    "title": "Communication-Efficient Distributionally Robust Decentralized Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matteo Zecchin",
      "Marios Kountouris",
      "David Gesbert"
    ]
  },
  "https://openreview.net/forum?id=GRBbtkW3Lp": {
    "title": "EdiBERT: a generative model for image editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thibaut Issenhuth",
      "Ugo Tanielian",
      "Jeremie Mary",
      "David Picard"
    ]
  },
  "https://openreview.net/forum?id=2wWJxtpFer": {
    "title": "OpenCon: Open-world Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiyou Sun",
      "Yixuan Li"
    ]
  },
  "https://openreview.net/forum?id=fjkN5Ur2d6": {
    "title": "Linking Neural Collapse and L2 Normalization with Improved Out-of-Distribution Detection in Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jarrod Haas",
      "William Yolland",
      "Bernhard T Rabus"
    ]
  },
  "https://openreview.net/forum?id=Grhi800jVz": {
    "title": "Euclidean-Norm-Induced Schatten-p Quasi-Norm Regularization for Low-Rank Tensor Completion and Tensor Robust Principal Component Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jicong Fan",
      "Lijun Ding",
      "Chengrun Yang",
      "Zhao Zhang",
      "Madeleine Udell"
    ]
  },
  "https://openreview.net/forum?id=TGuXXlbKsn": {
    "title": "Benchmarks and Algorithms for Offline Preference-Based Reward Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Shin",
      "Anca Dragan",
      "Daniel S. Brown"
    ]
  },
  "https://openreview.net/forum?id=mrTXGDZns2": {
    "title": "Fairness and robustness in anti-causal prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maggie Makar",
      "Alexander D'Amour"
    ]
  },
  "https://openreview.net/forum?id=bx24KpJ4Eb": {
    "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stephen Casper",
      "Xander Davies",
      "Claudia Shi",
      "Thomas Krendl Gilbert",
      "Jérémy Scheurer",
      "Javier Rando",
      "Rachel Freedman",
      "Tomek Korbak",
      "David Lindner",
      "Pedro Freire",
      "Tony Tong Wang",
      "Samuel Marks",
      "Charbel-Raphael Segerie",
      "Micah Carroll",
      "Andi Peng",
      "Phillip J.K. Christoffersen",
      "Mehul Damani",
      "Stewart Slocum",
      "Usman Anwar",
      "Anand Siththaranjan",
      "Max Nadeau",
      "Eric J Michaud",
      "Jacob Pfau",
      "Dmitrii Krasheninnikov",
      "Xin Chen",
      "Lauro Langosco",
      "Peter Hase",
      "Erdem Biyik",
      "Anca Dragan",
      "David Krueger",
      "Dorsa Sadigh",
      "Dylan Hadfield-Menell"
    ]
  },
  "https://openreview.net/forum?id=ed8SkMdYFT": {
    "title": "Understanding the robustness difference between stochastic gradient descent and adaptive gradient methods",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Avery Ma",
      "Yangchen Pan",
      "Amir-massoud Farahmand"
    ]
  },
  "https://openreview.net/forum?id=ioFIAQOBOS": {
    "title": "Learning to reconstruct signals from binary measurements alone",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julián Tachella",
      "Laurent Jacques"
    ]
  },
  "https://openreview.net/forum?id=DwgRm72GQF": {
    "title": "Inverse Scaling: When Bigger Isn't Better",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ian R. McKenzie",
      "Alexander Lyzhov",
      "Michael Martin Pieler",
      "Alicia Parrish",
      "Aaron Mueller",
      "Ameya Prabhu",
      "Euan McLean",
      "Xudong Shen",
      "Joe Cavanagh",
      "Andrew George Gritsevskiy",
      "Derik Kauffman",
      "Aaron T. Kirtland",
      "Zhengping Zhou",
      "Yuhui Zhang",
      "Sicong Huang",
      "Daniel Wurgaft",
      "Max Weiss",
      "Alexis Ross",
      "Gabriel Recchia",
      "Alisa Liu",
      "Jiacheng Liu",
      "Tom Tseng",
      "Tomasz Korbak",
      "Najoung Kim",
      "Samuel R. Bowman",
      "Ethan Perez"
    ]
  },
  "https://openreview.net/forum?id=a7nvXxNmdV": {
    "title": "Improved baselines for vision-language pre-training",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Enrico Fini",
      "Pietro Astolfi",
      "Adriana Romero-Soriano",
      "Jakob Verbeek",
      "Michal Drozdzal"
    ]
  },
  "https://openreview.net/forum?id=UIalYAHdBH": {
    "title": "On the Sample Complexity of Lipschitz Constant Estimation",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julien Walden Huang",
      "Stephen J. Roberts",
      "Jan-Peter Calliess"
    ]
  },
  "https://openreview.net/forum?id=XXfEmIMJDm": {
    "title": "Achieving the Pareto Frontier of Regret Minimization and Best Arm Identification in Multi-Armed Bandits",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixin Zhong",
      "Wang Chi Cheung",
      "Vincent Tan"
    ]
  },
  "https://openreview.net/forum?id=ivCd8z8zR2": {
    "title": "High Fidelity Neural Audio Compression",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexandre Défossez",
      "Jade Copet",
      "Gabriel Synnaeve",
      "Yossi Adi"
    ]
  },
  "https://openreview.net/forum?id=EGQSpkUDdD": {
    "title": "AP: Selective Activation for De-sparsifying Pruned Networks",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiyu Liu",
      "Rohan Ghosh",
      "Mehul Motani"
    ]
  },
  "https://openreview.net/forum?id=mvftzofTYQ": {
    "title": "WOODS: Benchmarks for Out-of-Distribution Generalization in Time Series",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jean-Christophe Gagnon-Audet",
      "Kartik Ahuja",
      "Mohammad Javad Darvishi Bayazi",
      "Pooneh Mousavi",
      "Guillaume Dumas",
      "Irina Rish"
    ]
  },
  "https://openreview.net/forum?id=iO4LZibEqW": {
    "title": "Holistic Evaluation of Language Models",
    "volume": "outstanding",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Percy Liang",
      "Rishi Bommasani",
      "Tony Lee",
      "Dimitris Tsipras",
      "Dilara Soylu",
      "Michihiro Yasunaga",
      "Yian Zhang",
      "Deepak Narayanan",
      "Yuhuai Wu",
      "Ananya Kumar",
      "Benjamin Newman",
      "Binhang Yuan",
      "Bobby Yan",
      "Ce Zhang",
      "Christian Cosgrove",
      "Christopher D Manning",
      "Christopher Re",
      "Diana Acosta-Navas",
      "Drew A. Hudson",
      "Eric Zelikman",
      "Esin Durmus",
      "Faisal Ladhak",
      "Frieda Rong",
      "Hongyu Ren",
      "Huaxiu Yao",
      "Jue WANG",
      "Keshav Santhanam",
      "Laurel Orr",
      "Lucia Zheng",
      "Mert Yuksekgonul",
      "Mirac Suzgun",
      "Nathan Kim",
      "Neel Guha",
      "Niladri S. Chatterji",
      "Omar Khattab",
      "Peter Henderson",
      "Qian Huang",
      "Ryan Andrew Chi",
      "Sang Michael Xie",
      "Shibani Santurkar",
      "Surya Ganguli",
      "Tatsunori Hashimoto",
      "Thomas Icard",
      "Tianyi Zhang",
      "Vishrav Chaudhary",
      "William Wang",
      "Xuechen Li",
      "Yifan Mai",
      "Yuhui Zhang",
      "Yuta Koreeda"
    ]
  },
  "https://openreview.net/forum?id=yrkJGne0vN": {
    "title": "Neural Ordinary Differential Equations for Modeling Epidemic Spreading",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chrysoula Kosma",
      "Giannis Nikolentzos",
      "George Panagopoulos",
      "Jean-Marc Steyaert",
      "Michalis Vazirgiannis"
    ]
  },
  "https://openreview.net/forum?id=2mZSlQscj3": {
    "title": "Neural Monge Map estimation and its applications",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaojiao Fan",
      "Shu Liu",
      "Shaojun Ma",
      "Hao-Min Zhou",
      "Yongxin Chen"
    ]
  },
  "https://openreview.net/forum?id=igdWKxK5RZ": {
    "title": "Finding and Only Finding Differential Nash Equilibria by Both Pretending to be a Follower",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuchan Bao",
      "Guodong Zhang"
    ]
  },
  "https://openreview.net/forum?id=25G63lDHV2": {
    "title": "Efficient Reward Poisoning Attacks on Online Deep Reinforcement Learning",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinglun Xu",
      "Qi Zeng",
      "Gagandeep Singh"
    ]
  },
  "https://openreview.net/forum?id=VmyFF5lL3F": {
    "title": "Inversion by Direct Iteration: An Alternative to Denoising Diffusion for Image Restoration",
    "volume": "outstanding",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mauricio Delbracio",
      "Peyman Milanfar"
    ]
  },
  "https://openreview.net/forum?id=vXSsTYs6ZB": {
    "title": "LEAD: Min-Max Optimization from a Physical Perspective",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reyhane Askari Hemmat",
      "Amartya Mitra",
      "Guillaume Lajoie",
      "Ioannis Mitliagkas"
    ]
  },
  "https://openreview.net/forum?id=XNFo3dQiCJ": {
    "title": "Generalizability of Adversarial Robustness Under Distribution Shifts",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kumail Alhamoud",
      "Hasan Abed Al Kader Hammoud",
      "Motasem Alfarra",
      "Bernard Ghanem"
    ]
  },
  "https://openreview.net/forum?id=r9vGSpbbRO": {
    "title": "Attacking Perceptual Similarity Metrics",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhijay Ghildyal",
      "Feng Liu"
    ]
  },
  "https://openreview.net/forum?id=uyTL5Bvosj": {
    "title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aarohi Srivastava",
      "Abhinav Rastogi",
      "Abhishek Rao",
      "Abu Awal Md Shoeb",
      "Abubakar Abid",
      "Adam Fisch",
      "Adam R. Brown",
      "Adam Santoro",
      "Aditya Gupta",
      "Adrià Garriga-Alonso",
      "Agnieszka Kluska",
      "Aitor Lewkowycz",
      "Akshat Agarwal",
      "Alethea Power",
      "Alex Ray",
      "Alex Warstadt",
      "Alexander W. Kocurek",
      "Ali Safaya",
      "Ali Tazarv",
      "Alice Xiang",
      "Alicia Parrish",
      "Allen Nie",
      "Aman Hussain",
      "Amanda Askell",
      "Amanda Dsouza",
      "Ambrose Slone",
      "Ameet Rahane",
      "Anantharaman S. Iyer",
      "Anders Johan Andreassen",
      "Andrea Madotto",
      "Andrea Santilli",
      "Andreas Stuhlmüller",
      "Andrew M. Dai",
      "Andrew La",
      "Andrew Kyle Lampinen",
      "Andy Zou",
      "Angela Jiang",
      "Angelica Chen",
      "Anh Vuong",
      "Animesh Gupta",
      "Anna Gottardi",
      "Antonio Norelli",
      "Anu Venkatesh",
      "Arash Gholamidavoodi",
      "Arfa Tabassum",
      "Arul Menezes",
      "Arun Kirubarajan",
      "Asher Mullokandov",
      "Ashish Sabharwal",
      "Austin Herrick",
      "Avia Efrat",
      "Aykut Erdem",
      "Ayla Karakaş",
      "B. Ryan Roberts",
      "Bao Sheng Loe",
      "Barret Zoph",
      "Bartłomiej Bojanowski",
      "Batuhan Özyurt",
      "Behnam Hedayatnia",
      "Behnam Neyshabur",
      "Benjamin Inden",
      "Benno Stein",
      "Berk Ekmekci",
      "Bill Yuchen Lin",
      "Blake Howald",
      "Bryan Orinion",
      "Cameron Diao",
      "Cameron Dour",
      "Catherine Stinson",
      "Cedrick Argueta",
      "Cesar Ferri",
      "Chandan Singh",
      "Charles Rathkopf",
      "Chenlin Meng",
      "Chitta Baral",
      "Chiyu Wu",
      "Chris Callison-Burch",
      "Christopher Waites",
      "Christian Voigt",
      "Christopher D Manning",
      "Christopher Potts",
      "Cindy Ramirez",
      "Clara E. Rivera",
      "Clemencia Siro",
      "Colin Raffel",
      "Courtney Ashcraft",
      "Cristina Garbacea",
      "Damien Sileo",
      "Dan Garrette",
      "Dan Hendrycks",
      "Dan Kilman",
      "Dan Roth",
      "C. Daniel Freeman",
      "Daniel Khashabi",
      "Daniel Levy",
      "Daniel Moseguí González",
      "Danielle Perszyk",
      "Danny Hernandez",
      "Danqi Chen",
      "Daphne Ippolito",
      "Dar Gilboa",
      "David Dohan",
      "David Drakard",
      "David Jurgens",
      "Debajyoti Datta",
      "Deep Ganguli",
      "Denis Emelin",
      "Denis Kleyko",
      "Deniz Yuret",
      "Derek Chen",
      "Derek Tam",
      "Dieuwke Hupkes",
      "Diganta Misra",
      "Dilyar Buzan",
      "Dimitri Coelho Mollo",
      "Diyi Yang",
      "Dong-Ho Lee",
      "Dylan Schrader",
      "Ekaterina Shutova",
      "Ekin Dogus Cubuk",
      "Elad Segal",
      "Eleanor Hagerman",
      "Elizabeth Barnes",
      "Elizabeth Donoway",
      "Ellie Pavlick",
      "Emanuele Rodolà",
      "Emma Lam",
      "Eric Chu",
      "Eric Tang",
      "Erkut Erdem",
      "Ernie Chang",
      "Ethan A Chi",
      "Ethan Dyer",
      "Ethan Jerzak",
      "Ethan Kim",
      "Eunice Engefu Manyasi",
      "Evgenii Zheltonozhskii",
      "Fanyue Xia",
      "Fatemeh Siar",
      "Fernando Martínez-Plumed",
      "Francesca Happé",
      "Francois Chollet",
      "Frieda Rong",
      "Gaurav Mishra",
      "Genta Indra Winata",
      "Gerard de Melo",
      "Germàn Kruszewski",
      "Giambattista Parascandolo",
      "Giorgio Mariani",
      "Gloria Xinyue Wang",
      "Gonzalo Jaimovitch-Lopez",
      "Gregor Betz",
      "Guy Gur-Ari",
      "Hana Galijasevic",
      "Hannah Kim",
      "Hannah Rashkin",
      "Hannaneh Hajishirzi",
      "Harsh Mehta",
      "Hayden Bogar",
      "Henry Francis Anthony Shevlin",
      "Hinrich Schuetze",
      "Hiromu Yakura",
      "Hongming Zhang",
      "Hugh Mee Wong",
      "Ian Ng",
      "Isaac Noble",
      "Jaap Jumelet",
      "Jack Geissinger",
      "Jackson Kernion",
      "Jacob Hilton",
      "Jaehoon Lee",
      "Jaime Fernández Fisac",
      "James B Simon",
      "James Koppel",
      "James Zheng",
      "James Zou",
      "Jan Kocon",
      "Jana Thompson",
      "Janelle Wingfield",
      "Jared Kaplan",
      "Jarema Radom",
      "Jascha Sohl-Dickstein",
      "Jason Phang",
      "Jason Wei",
      "Jason Yosinski",
      "Jekaterina Novikova",
      "Jelle Bosscher",
      "Jennifer Marsh",
      "Jeremy Kim",
      "Jeroen Taal",
      "Jesse Engel",
      "Jesujoba Alabi",
      "Jiacheng Xu",
      "Jiaming Song",
      "Jillian Tang",
      "Joan Waweru",
      "John Burden",
      "John Miller",
      "John U. Balis",
      "Jonathan Batchelder",
      "Jonathan Berant",
      "Jörg Frohberg",
      "Jos Rozen",
      "Jose Hernandez-Orallo",
      "Joseph Boudeman",
      "Joseph Guerr",
      "Joseph Jones",
      "Joshua B. Tenenbaum",
      "Joshua S. Rule",
      "Joyce Chua",
      "Kamil Kanclerz",
      "Karen Livescu",
      "Karl Krauth",
      "Karthik Gopalakrishnan",
      "Katerina Ignatyeva",
      "Katja Markert",
      "Kaustubh Dhole",
      "Kevin Gimpel",
      "Kevin Omondi",
      "Kory Wallace Mathewson",
      "Kristen Chiafullo",
      "Ksenia Shkaruta",
      "Kumar Shridhar",
      "Kyle McDonell",
      "Kyle Richardson",
      "Laria Reynolds",
      "Leo Gao",
      "Li Zhang",
      "Liam Dugan",
      "Lianhui Qin",
      "Lidia Contreras-Ochando",
      "Louis-Philippe Morency",
      "Luca Moschella",
      "Lucas Lam",
      "Lucy Noble",
      "Ludwig Schmidt",
      "Luheng He",
      "Luis Oliveros-Colón",
      "Luke Metz",
      "Lütfi Kerem Senel",
      "Maarten Bosma",
      "Maarten Sap",
      "Maartje Ter Hoeve",
      "Maheen Farooqi",
      "Manaal Faruqui",
      "Mantas Mazeika",
      "Marco Baturan",
      "Marco Marelli",
      "Marco Maru",
      "Maria Jose Ramirez-Quintana",
      "Marie Tolkiehn",
      "Mario Giulianelli",
      "Martha Lewis",
      "Martin Potthast",
      "Matthew L Leavitt",
      "Matthias Hagen",
      "Mátyás Schubert",
      "Medina Orduna Baitemirova",
      "Melody Arnaud",
      "Melvin McElrath",
      "Michael Andrew Yee",
      "Michael Cohen",
      "Michael Gu",
      "Michael Ivanitskiy",
      "Michael Starritt",
      "Michael Strube",
      "Michał Swędrowski",
      "Michele Bevilacqua",
      "Michihiro Yasunaga",
      "Mihir Kale",
      "Mike Cain",
      "Mimee Xu",
      "Mirac Suzgun",
      "Mitch Walker",
      "Mo Tiwari",
      "Mohit Bansal",
      "Moin Aminnaseri",
      "Mor Geva",
      "Mozhdeh Gheini",
      "Mukund Varma T",
      "Nanyun Peng",
      "Nathan Andrew Chi",
      "Nayeon Lee",
      "Neta Gur-Ari Krakover",
      "Nicholas Cameron",
      "Nicholas Roberts",
      "Nick Doiron",
      "Nicole Martinez",
      "Nikita Nangia",
      "Niklas Deckers",
      "Niklas Muennighoff",
      "Nitish Shirish Keskar",
      "Niveditha S. Iyer",
      "Noah Constant",
      "Noah Fiedel",
      "Nuan Wen",
      "Oliver Zhang",
      "Omar Agha",
      "Omar Elbaghdadi",
      "Omer Levy",
      "Owain Evans",
      "Pablo Antonio Moreno Casares",
      "Parth Doshi",
      "Pascale Fung",
      "Paul Pu Liang",
      "Paul Vicol",
      "Pegah Alipoormolabashi",
      "Peiyuan Liao",
      "Percy Liang",
      "Peter W Chang",
      "Peter Eckersley",
      "Phu Mon Htut",
      "Pinyu Hwang",
      "Piotr Miłkowski",
      "Piyush Patil",
      "Pouya Pezeshkpour",
      "Priti Oli",
      "Qiaozhu Mei",
      "Qing Lyu",
      "Qinlang Chen",
      "Rabin Banjade",
      "Rachel Etta Rudolph",
      "Raefer Gabriel",
      "Rahel Habacker",
      "Ramon Risco",
      "Raphaël Millière",
      "Rhythm Garg",
      "Richard Barnes",
      "Rif A. Saurous",
      "Riku Arakawa",
      "Robbe Raymaekers",
      "Robert Frank",
      "Rohan Sikand",
      "Roman Novak",
      "Roman Sitelew",
      "Ronan Le Bras",
      "Rosanne Liu",
      "Rowan Jacobs",
      "Rui Zhang",
      "Russ Salakhutdinov",
      "Ryan Andrew Chi",
      "Seungjae Ryan Lee",
      "Ryan Stovall",
      "Ryan Teehan",
      "Rylan Yang",
      "Sahib Singh",
      "Saif M. Mohammad",
      "Sajant Anand",
      "Sam Dillavou",
      "Sam Shleifer",
      "Sam Wiseman",
      "Samuel Gruetter",
      "Samuel R. Bowman",
      "Samuel Stern Schoenholz",
      "Sanghyun Han",
      "Sanjeev Kwatra",
      "Sarah A. Rous",
      "Sarik Ghazarian",
      "Sayan Ghosh",
      "Sean Casey",
      "Sebastian Bischoff",
      "Sebastian Gehrmann",
      "Sebastian Schuster",
      "Sepideh Sadeghi",
      "Shadi Hamdan",
      "Sharon Zhou",
      "Shashank Srivastava",
      "Sherry Shi",
      "Shikhar Singh",
      "Shima Asaadi",
      "Shixiang Shane Gu",
      "Shubh Pachchigar",
      "Shubham Toshniwal",
      "Shyam Upadhyay",
      "Shyamolima Shammie Debnath",
      "Siamak Shakeri",
      "Simon Thormeyer",
      "Simone Melzi",
      "Siva Reddy",
      "Sneha Priscilla Makini",
      "Soo-Hwan Lee",
      "Spencer Torene",
      "Sriharsha Hatwar",
      "Stanislas Dehaene",
      "Stefan Divic",
      "Stefano Ermon",
      "Stella Biderman",
      "Stephanie Lin",
      "Stephen Prasad",
      "Steven Piantadosi",
      "Stuart Shieber",
      "Summer Misherghi",
      "Svetlana Kiritchenko",
      "Swaroop Mishra",
      "Tal Linzen",
      "Tal Schuster",
      "Tao Li",
      "Tao Yu",
      "Tariq Ali",
      "Tatsunori Hashimoto",
      "Te-Lin Wu",
      "Théo Desbordes",
      "Theodore Rothschild",
      "Thomas Phan",
      "Tianle Wang",
      "Tiberius Nkinyili",
      "Timo Schick",
      "Timofei Kornev",
      "Titus Tunduny",
      "Tobias Gerstenberg",
      "Trenton Chang",
      "Trishala Neeraj",
      "Tushar Khot",
      "Tyler Shultz",
      "Uri Shaham",
      "Vedant Misra",
      "Vera Demberg",
      "Victoria Nyamai",
      "Vikas Raunak",
      "Vinay Venkatesh Ramasesh",
      "vinay uday prabhu",
      "Vishakh Padmakumar",
      "Vivek Srikumar",
      "William Fedus",
      "William Saunders",
      "William Zhang",
      "Wout Vossen",
      "Xiang Ren",
      "Xiaoyu Tong",
      "Xinran Zhao",
      "Xinyi Wu",
      "Xudong Shen",
      "Yadollah Yaghoobzadeh",
      "Yair Lakretz",
      "Yangqiu Song",
      "Yasaman Bahri",
      "Yejin Choi",
      "Yichi Yang",
      "Sophie Hao",
      "Yifu Chen",
      "Yonatan Belinkov",
      "Yu Hou",
      "Yufang Hou",
      "Yuntao Bai",
      "Zachary Seid",
      "Zhuoye Zhao",
      "Zijian Wang",
      "Zijie J. Wang",
      "Zirui Wang",
      "Ziyi Wu"
    ]
  },
  "https://openreview.net/forum?id=KgfFAI9f3E": {
    "title": "Identification of Negative Transfers in Multitask Learning Using Surrogate Models",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongyue Li",
      "Huy Nguyen",
      "Hongyang Ryan Zhang"
    ]
  },
  "https://openreview.net/forum?id=rAnB7JSMXL": {
    "title": "Patches Are All You Need?",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Asher Trockman",
      "J Zico Kolter"
    ]
  },
  "https://openreview.net/forum?id=11osftjEbF": {
    "title": "Numerical Accounting in the Shuffle Model of Differential Privacy",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antti Koskela",
      "Mikko A. Heikkilä",
      "Antti Honkela"
    ]
  },
  "https://openreview.net/forum?id=L9othQvPks": {
    "title": "Workflow Discovery from Dialogues in the Low Data Regime",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amine El hattami",
      "Issam H. Laradji",
      "Stefania Raimondo",
      "David Vazquez",
      "Pau Rodriguez",
      "Christopher Pal"
    ]
  },
  "https://openreview.net/forum?id=JwDpZSv3yz": {
    "title": "SPADE: Semi-supervised Anomaly Detection under Distribution Mismatch",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinsung Yoon",
      "Kihyuk Sohn",
      "Chun-Liang Li",
      "Sercan O Arik",
      "Tomas Pfister"
    ]
  },
  "https://openreview.net/forum?id=ZR2CDgADRo": {
    "title": "SolidGen: An Autoregressive Model for Direct B-rep Synthesis",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pradeep Kumar Jayaraman",
      "Joseph George Lambourne",
      "Nishkrit Desai",
      "Karl Willis",
      "Aditya Sanghi",
      "Nigel J. W. Morris"
    ]
  },
  "https://openreview.net/forum?id=KoFOg41haE": {
    "title": "StarCoder: may the source be with you!",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raymond Li",
      "Loubna Ben allal",
      "Yangtian Zi",
      "Niklas Muennighoff",
      "Denis Kocetkov",
      "Chenghao Mou",
      "Marc Marone",
      "Christopher Akiki",
      "Jia LI",
      "Jenny Chim",
      "Qian Liu",
      "Evgenii Zheltonozhskii",
      "Terry Yue Zhuo",
      "Thomas Wang",
      "Olivier Dehaene",
      "Joel Lamy-Poirier",
      "Joao Monteiro",
      "Nicolas Gontier",
      "Ming-Ho Yee",
      "Logesh Kumar Umapathi",
      "Jian Zhu",
      "Ben Lipkin",
      "Muhtasham Oblokulov",
      "Zhiruo Wang",
      "Rudra Murthy",
      "Jason T Stillerman",
      "Siva Sankalp Patel",
      "Dmitry Abulkhanov",
      "Marco Zocca",
      "Manan Dey",
      "Zhihan Zhang",
      "Urvashi Bhattacharyya",
      "Wenhao Yu",
      "Sasha Luccioni",
      "Paulo Villegas",
      "Fedor Zhdanov",
      "Tony Lee",
      "Nadav Timor",
      "Jennifer Ding",
      "Claire S Schlesinger",
      "Hailey Schoelkopf",
      "Jan Ebert",
      "Tri Dao",
      "Mayank Mishra",
      "Alex Gu",
      "Carolyn Jane Anderson",
      "Brendan Dolan-Gavitt",
      "Danish Contractor",
      "Siva Reddy",
      "Daniel Fried",
      "Dzmitry Bahdanau",
      "Yacine Jernite",
      "Carlos Muñoz Ferrandis",
      "Sean Hughes",
      "Thomas Wolf",
      "Arjun Guha",
      "Leandro Von Werra",
      "Harm de Vries"
    ]
  },
  "https://openreview.net/forum?id=vlY9GDCCA6": {
    "title": "PAVI: Plate-Amortized Variational Inference",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Louis Rouillard",
      "Alexandre Le Bris",
      "Thomas Moreau",
      "Demian Wassermann"
    ]
  },
  "https://openreview.net/forum?id=1dwXa9vmOI": {
    "title": "Does ‘Deep Learning on a Data Diet' reproduce? Overall yes, but GraNd at Initialization does not",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andreas Kirsch"
    ]
  },
  "https://openreview.net/forum?id=KqR3rgooXb": {
    "title": "Numerical Data Imputation for Multimodal Data Sets: A Probabilistic Nearest-Neighbor Kernel Density Approach",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Florian Lalande",
      "Kenji Doya"
    ]
  },
  "https://openreview.net/forum?id=R9CgBkeZ6Z": {
    "title": "Aux-Drop: Handling Haphazard Inputs in Online Learning Using Auxiliary Dropouts",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rohit Agarwal",
      "Deepak Gupta",
      "Alexander Horsch",
      "Dilip K. Prasad"
    ]
  },
  "https://openreview.net/forum?id=jWr41htaB3": {
    "title": "A Stochastic Proximal Polyak Step Size",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fabian Schaipp",
      "Robert M. Gower",
      "Michael Ulbrich"
    ]
  },
  "https://openreview.net/forum?id=RYeRNwRjNE": {
    "title": "Explaining Visual Counterfactual Explainers",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Diego Velazquez",
      "Pau Rodriguez",
      "Alexandre Lacoste",
      "Issam H. Laradji",
      "Xavier Roca",
      "Jordi Gonzàlez"
    ]
  },
  "https://openreview.net/forum?id=JjbsIYOuNi": {
    "title": "PRUDEX-Compass: Towards Systematic Evaluation of Reinforcement Learning in Financial Markets",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuo Sun",
      "Molei Qin",
      "Xinrun Wang",
      "Bo An"
    ]
  },
  "https://openreview.net/forum?id=AXtFeYjboj": {
    "title": "A Survey on the Possibilities & Impossibilities of AI-generated Text Detection",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soumya Suvra Ghosal",
      "Souradip Chakraborty",
      "Jonas Geiping",
      "Furong Huang",
      "Dinesh Manocha",
      "Amrit Bedi"
    ]
  },
  "https://openreview.net/forum?id=z9EkXfvxta": {
    "title": "Modular Deep Learning",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonas Pfeiffer",
      "Sebastian Ruder",
      "Ivan Vulić",
      "Edoardo Ponti"
    ]
  },
  "https://openreview.net/forum?id=cHroS8VIyN": {
    "title": "Benchmarks for Physical Reasoning AI",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrew Melnik",
      "Robin Schiewer",
      "Moritz Lange",
      "Andrei Ioan Muresanu",
      "mozhgan saeidi",
      "Animesh Garg",
      "Helge Ritter"
    ]
  },
  "https://openreview.net/forum?id=qqnttX9LPo": {
    "title": "Causal Reinforcement Learning: A Survey",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihong Deng",
      "Jing Jiang",
      "Guodong Long",
      "Chengqi Zhang"
    ]
  },
  "https://openreview.net/forum?id=mcN0ezbnzO": {
    "title": "Provably Safe Reinforcement Learning: Conceptual Analysis, Survey, and Benchmarking",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanna Krasowski",
      "Jakob Thumm",
      "Marlon Müller",
      "Lukas Schäfer",
      "Xiao Wang",
      "Matthias Althoff"
    ]
  },
  "https://openreview.net/forum?id=9sVCIngrhP": {
    "title": "Private GANs, Revisited",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alex Bie",
      "Gautam Kamath",
      "Guojun Zhang"
    ]
  },
  "https://openreview.net/forum?id=r30yuDPvf2": {
    "title": "A Survey on Transformers in Reinforcement Learning",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenzhe Li",
      "Hao Luo",
      "Zichuan Lin",
      "Chongjie Zhang",
      "Zongqing Lu",
      "Deheng Ye"
    ]
  },
  "https://openreview.net/forum?id=YdMrdhGx9y": {
    "title": "A Survey on Causal Discovery Methods for I.I.D. and Time Series Data",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Uzma Hasan",
      "Emam Hossain",
      "Md Osman Gani"
    ]
  },
  "https://openreview.net/forum?id=lmXMXP74TO": {
    "title": "Data Distillation: A Survey",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Noveen Sachdeva",
      "Julian McAuley"
    ]
  },
  "https://openreview.net/forum?id=3OSISBQPrM": {
    "title": "On the Predictive Accuracy of Neural Temporal Point Process Models for Continuous-time Event Data",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tanguy Bosser",
      "Souhaib Ben Taieb"
    ]
  },
  "https://openreview.net/forum?id=jh7wH2AzKK": {
    "title": "Augmented Language Models: a Survey",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Grégoire Mialon",
      "Roberto Dessi",
      "Maria Lomeli",
      "Christoforos Nalmpantis",
      "Ramakanth Pasunuru",
      "Roberta Raileanu",
      "Baptiste Roziere",
      "Timo Schick",
      "Jane Dwivedi-Yu",
      "Asli Celikyilmaz",
      "Edouard Grave",
      "Yann LeCun",
      "Thomas Scialom"
    ]
  },
  "https://openreview.net/forum?id=FByH3qL87G": {
    "title": "On Averaging ROC Curves",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jack Hogan",
      "Niall M. Adams"
    ]
  },
  "https://openreview.net/forum?id=VynY6Bk03b": {
    "title": "How to Reuse and Compose Knowledge for a Lifetime of Tasks: A Survey on Continual Learning and Functional Composition",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jorge A Mendez",
      "ERIC EATON"
    ]
  },
  "https://openreview.net/forum?id=Ma25S4ludQ": {
    "title": "Know Your Self-supervised Learning: A Survey on Image-based Generative and Discriminative Training",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Utku Ozbulak",
      "Hyun Jung Lee",
      "Beril Boga",
      "Esla Timothy Anzaku",
      "Ho-min Park",
      "Arnout Van Messem",
      "Wesley De Neve",
      "Joris Vankerschaver"
    ]
  },
  "https://openreview.net/forum?id=A8pqQipwkt": {
    "title": "Forces are not Enough: Benchmark and Critical Evaluation for Machine Learning Force Fields with Molecular Simulations",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Fu",
      "Zhenghao Wu",
      "Wujie Wang",
      "Tian Xie",
      "Sinan Keten",
      "Rafael Gomez-Bombarelli",
      "Tommi Jaakkola"
    ]
  },
  "https://openreview.net/forum?id=e0xaRylNuT": {
    "title": "Partition-Based Active Learning for Graph Neural Networks",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi Ma",
      "Ziqiao Ma",
      "Joyce Chai",
      "Qiaozhu Mei"
    ]
  },
  "https://openreview.net/forum?id=AU4qHN2VkS": {
    "title": "Better Theory for SGD in the Nonconvex World",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahmed Khaled",
      "Peter Richtárik"
    ]
  },
  "https://openreview.net/forum?id=WFtTpQ47A7": {
    "title": "SHAP-XRT: The Shapley Value Meets Conditional Independence Testing",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jacopo Teneggi",
      "Beepul Bharti",
      "Yaniv Romano",
      "Jeremias Sulam"
    ]
  },
  "https://openreview.net/forum?id=rq1SaHQg2k": {
    "title": "Pairwise Learning with Adaptive Online Gradient Descent",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Sun",
      "Qingsong Wang",
      "Yunwen Lei",
      "Dongsheng Li",
      "Bao Wang"
    ]
  },
  "https://openreview.net/forum?id=GlhM6XX1wv": {
    "title": "DPVIm: Differentially Private Variational Inference Improved",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joonas Jälkö",
      "Lukas Prediger",
      "Antti Honkela",
      "Samuel Kaski"
    ]
  },
  "https://openreview.net/forum?id=vcHwQyNBjW": {
    "title": "Stochastic Batch Acquisition: A Simple Baseline for Deep Active Learning",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andreas Kirsch",
      "Sebastian Farquhar",
      "Parmida Atighehchian",
      "Andrew Jesson",
      "Frédéric Branchaud-Charron",
      "Yarin Gal"
    ]
  },
  "https://openreview.net/forum?id=rdHVPPVuXa": {
    "title": "Neural Causal Structure Discovery from Interventions",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nan Rosemary Ke",
      "Olexa Bilaniuk",
      "Anirudh Goyal",
      "Stefan Bauer",
      "Hugo Larochelle",
      "Bernhard Schölkopf",
      "Michael Curtis Mozer",
      "Christopher Pal",
      "Yoshua Bengio"
    ]
  },
  "https://openreview.net/forum?id=xuWTFQ4VGO": {
    "title": "Diffusion Models for Constrained Domains",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nic Fishman",
      "Leo Klarner",
      "Valentin De Bortoli",
      "Emile Mathieu",
      "Michael John Hutchinson"
    ]
  },
  "https://openreview.net/forum?id=10hCbu70Sr": {
    "title": "Catastrophic overfitting can be induced with discriminative non-robust features",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guillermo Ortiz-Jimenez",
      "Pau de Jorge",
      "Amartya Sanyal",
      "Adel Bibi",
      "Puneet K. Dokania",
      "Pascal Frossard",
      "Grégory Rogez",
      "Philip Torr"
    ]
  },
  "https://openreview.net/forum?id=brGgOAXYtr": {
    "title": "POMRL: No-Regret Learning-to-Plan with Increasing Horizons",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Khimya Khetarpal",
      "Claire Vernade",
      "Brendan O'Donoghue",
      "Satinder Singh",
      "Tom Zahavy"
    ]
  },
  "https://openreview.net/forum?id=XnYtGPgG9p": {
    "title": "Off-Policy Evaluation with Out-of-Sample Guarantees",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sofia Ek",
      "Dave Zachariah",
      "Fredrik D. Johansson",
      "Peter Stoica"
    ]
  },
  "https://openreview.net/forum?id=MaDvbLaBiF": {
    "title": "Towards a More Rigorous Science of Blindspot Discovery in Image Classification Models",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gregory Plumb",
      "Nari Johnson",
      "Angel Cabrera",
      "Ameet Talwalkar"
    ]
  },
  "https://openreview.net/forum?id=fvEvDlKko6": {
    "title": "Black-Box Batch Active Learning for Regression",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andreas Kirsch"
    ]
  },
  "https://openreview.net/forum?id=p7UTv2hWgM": {
    "title": "Stochastic gradient updates yield deep equilibrium kernels",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Russell Tsuchida",
      "Cheng Soon Ong"
    ]
  },
  "https://openreview.net/forum?id=akg6kdx0Pk": {
    "title": "Instance-Adaptive Video Compression: Improving Neural Codecs by Training on the Test Set",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ties van Rozendaal",
      "Johann Brehmer",
      "Yunfan Zhang",
      "Reza Pourreza",
      "Auke J. Wiggers",
      "Taco Cohen"
    ]
  },
  "https://openreview.net/forum?id=nHfPXl1ly7": {
    "title": "A Kernel Perspective on Behavioural Metrics for Markov Decision Processes",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pablo Samuel Castro",
      "Tyler Kastner",
      "Prakash Panangaden",
      "Mark Rowland"
    ]
  },
  "https://openreview.net/forum?id=jgMqve6Qhw": {
    "title": "Dual PatchNorm",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manoj Kumar",
      "Mostafa Dehghani",
      "Neil Houlsby"
    ]
  },
  "https://openreview.net/forum?id=NNRIGE8bvF": {
    "title": "Fast Treatment Personalization with Latent Bandits in Fixed-Confidence Pure Exploration",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Newton Mwai Kinyanjui",
      "Emil Carlsson",
      "Fredrik D. Johansson"
    ]
  },
  "https://openreview.net/forum?id=fvyh6mDWFr": {
    "title": "Understanding Noise-Augmented Training for Randomized Smoothing",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ambar Pal",
      "Jeremias Sulam"
    ]
  },
  "https://openreview.net/forum?id=Cj6pLclmwT": {
    "title": "Differentially Private Image Classification from Features",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Harsh Mehta",
      "Walid Krichene",
      "Abhradeep Guha Thakurta",
      "Alexey Kurakin",
      "Ashok Cutkosky"
    ]
  },
  "https://openreview.net/forum?id=eGLdVRvvfQ": {
    "title": "DEUP: Direct Epistemic Uncertainty Prediction",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Salem Lahlou",
      "Moksh Jain",
      "Hadi Nekoei",
      "Victor I Butoi",
      "Paul Bertin",
      "Jarrid Rector-Brooks",
      "Maksym Korablyov",
      "Yoshua Bengio"
    ]
  },
  "https://openreview.net/forum?id=a1meaRy1bN": {
    "title": "Robustness through Data Augmentation Loss Consistency",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianjian Huang",
      "Shaunak Ashish Halbe",
      "Chinnadhurai Sankar",
      "Pooyan Amini",
      "Satwik Kottur",
      "Alborz Geramifard",
      "Meisam Razaviyayn",
      "Ahmad Beirami"
    ]
  },
  "https://openreview.net/forum?id=Uu8WwCFpQv": {
    "title": "Towards Large Scale Transfer Learning for Differentially Private Image Classification",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Harsh Mehta",
      "Abhradeep Guha Thakurta",
      "Alexey Kurakin",
      "Ashok Cutkosky"
    ]
  }
}