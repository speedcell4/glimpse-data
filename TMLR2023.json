{
  "https://openreview.net/forum?id=YQWOzzSMPp": {
    "title": "An Analysis of Model-Based Reinforcement Learning From Abstracted Observations",
    "volume": "main",
    "abstract": "Many methods for Model-based Reinforcement learning (MBRL) in Markov decision processes (MDPs) provide guarantees for both the accuracy of the model they can deliver and the learning efficiency. At the same time, state abstraction techniques allow for a reduction of the size of an MDP while maintaining a bounded loss with respect to the original problem. Therefore, it may come as a surprise that no such guarantees are available when combining both techniques, i.e., where MBRL merely observes abstract states. Our theoretical analysis shows that abstraction can introduce a dependence between samples collected online (e.g., in the real world). That means that, without taking this dependence into account, results for MBRL do not directly extend to this setting. Our result shows that we can use concentration inequalities for martingales to overcome this problem. This result makes it possible to extend the guarantees of existing MBRL algorithms to the setting with abstraction. We illustrate this by combining R-MAX, a prototypical MBRL algorithm, with abstraction, thus producing the first performance guarantees for model-based ‘RL from Abstracted Observations': model-based reinforcement learning with an abstract model",
    "checked": false,
    "id": "c15ba9f3c5892b579ba46fe3aa895b0f41dc09d8",
    "semantic_title": "an analysis of abstracted model-based reinforcement learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6OEcDKZj5j": {
    "title": "The Kernel Density Integral Transformation",
    "volume": "main",
    "abstract": "Feature preprocessing continues to play a critical role when applying machine learning and statistical methods to tabular data. In this paper, we propose the use of the kernel density integral transformation as a feature preprocessing step. Our approach subsumes the two leading feature preprocessing methods as limiting cases: linear min-max scaling and quantile transformation. We demonstrate that, without hyperparameter tuning, the kernel density integral transformation can be used as a simple drop-in replacement for either method, offering robustness to the weaknesses of each. Alternatively, with tuning of a single continuous hyperparameter, we frequently outperform both of these methods. Finally, we show that the kernel density transformation can be profitably applied to statistical data analysis, particularly in correlation analysis and univariate clustering",
    "checked": true,
    "id": "47150b772a11789bed763d4d425ae2539279aa96",
    "semantic_title": "the kernel density integral transformation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lx1WnkL9fk": {
    "title": "Overcoming Resource Constraints in Federated Learning: Large Models Can Be Trained with only Weak Clients",
    "volume": "main",
    "abstract": "Federated Learning (FL) is emerging as a popular, promising decentralized learning framework that enables collaborative training among clients, with no need to share private data between them or to a centralized server. However, considering many edge clients do not have sufficient computing, memory, or communication capabilities, federated learning of large models still faces significant bottlenecks. To keep such weak but crucial clients in the loop, prior works either consider a heterogeneous-client setting where clients train models with different sizes; or offload training to the server. However, the heterogeneous-client setting requires some clients to train full model, which is not aligned with the resource-constrained setting; while the latter ones break privacy promises in FL when sharing intermediate representations or labels with the server. To overcome these limitations, in this work, we formulate a realistic, but much less explored, cross-device FL setting in which no client can train a full large model nor is willing to share any intermediate information with the remote server. Under such a formulation, we develop a principal sub-model (PriSM) training methodology to collaboratively train a full large model, while assigning each client a small sub-model that is a probabilistic low-rank approximation to the full server model. When creating sub-models, PriSM first performs a principal kernel analysis in the orthogonal kernel space to obtain importance of each kernel. Then, PriSM adopts a novel importance-aware sampling process to select a subset of kernels (i.e., a kernel with high importance is assigned with a higher sampling probability). This sampling process ensures each sub-model is still a low-rank approximation to the full model, while all sub-models together achieve nearly full coverage on the principal kernels. To further improve memory efficiency while still preserving accuracy, PriSM also exploits low-rank structure in intermediate representations and allows each sub-model to learn only a subset of them. Our evaluations on various datasets and models (CNNs, LSTMs, Transformers) under different resource-constrained settings demonstrate that PriSM yields an accuracy improvement of up to $10\\%$ compared to existing works. More importantly, PriSM does not incur significant accuracy degradation compared to full-model training (e.g., only $\\sim 2\\%$ accuracy drops for ResNet-18/CIFAR-10 when clients train only $0.2\\times$ sub-models)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FObkvLwNSo": {
    "title": "Projected Randomized Smoothing for Certified Adversarial Robustness",
    "volume": "main",
    "abstract": "Randomized smoothing is the current state-of-the-art method for producing provably robust classifiers. While randomized smoothing typically yields robust $\\ell_2$-ball certificates, recent research has generalized provable robustness to different norm balls as well as anisotropic regions. This work considers a classifier architecture that first projects onto a low-dimensional approximation of the data manifold and then applies a standard classifier. By performing randomized smoothing in the low-dimensional projected space, we characterize the certified region of our smoothed composite classifier back in the high-dimensional input space and prove a tractable lower bound on its volume. We show experimentally on CIFAR-10 and SVHN that classifiers without the initial projection are vulnerable to perturbations that are normal to the data manifold and yet are captured by the certified regions of our method. We compare the volume of our certified regions against various baselines and show that our method improves on the state-of-the-art by many orders of magnitude",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4UXJhNSbwd": {
    "title": "Multi-Domain Long-Tailed Learning by Augmenting Disentangled Representations",
    "volume": "main",
    "abstract": "There is an inescapable long-tailed class-imbalance issue in many real-world classification problems. Current methods for addressing this problem only consider scenarios where all examples come from the same distribution. However, in many cases, there are multiple domains with distinct class imbalance. We study this multi-domain long-tailed learning problem and aim to produce a model that generalizes well across all classes and domains. Towards that goal, we introduce TALLY, a method that addresses this multi-domain long-tailed learning problem. Built upon a proposed selective balanced sampling strategy, TALLY achieves this by mixing the semantic representation of one example with the domain-associated nuisances of another, producing a new representation for use as data augmentation. To improve the disentanglement of semantic representations, TALLY further utilizes a domain-invariant class prototype that averages out domain-specific effects. We evaluate TALLY on several benchmarks and real-world datasets and find that it consistently outperforms other state-of-the-art methods in both subpopulation and domain shift",
    "checked": true,
    "id": "eeec52cc464b50a560b50ef26b566ad243564849",
    "semantic_title": "multi-domain long-tailed learning by augmenting disentangled representations",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=f36LaK7M0F": {
    "title": "CAE v2: Context Autoencoder with CLIP Latent Alignment",
    "volume": "main",
    "abstract": "Masked image modeling (MIM) learns visual representations by predicting the masked patches on a pre-defined target. Inspired by MVP(Wei et al., 2022b) that displays impressive gains with CLIP, in this work, we also employ the semantically rich CLIP latent as target and further tap its potential by introducing a new MIM pipeline, CAE v2, to learn a high-quality encoder and facilitate model convergence on the pre-training task. CAE v2 is an improved variant of CAE (Chen et al., 2023), applying the CLIP latent on two pretraining tasks, i.e., visible latent alignment and masked latent alignment. Visible latent alignment directly mimics the visible latent representations from the encoder to the corresponding CLIP latent, which is beneficial for facilitating model convergence and improving the representative ability of the encoder. Masked latent alignment predicts the representations of masked patches within the feature space of CLIP latent as standard MIM task does, effectively aligning the representations computed from the encoder and the regressor into the same domain. We pretrain CAE v2 on ImageNet-1K images and evaluate on various downstream vision tasks, including image classification, semantic segmentation, object detection and instance segmentation. Experiments show that our CAE v2 achieves competitive performance and even outperforms the CLIP vision encoder, demonstrating the effectiveness of our method. Code is available at https://github.com/Atten4Vis/CAE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VgJhYu7FmQ": {
    "title": "Cross-validation for Geospatial Data: Estimating Generalization Performance in Geostatistical Problems",
    "volume": "main",
    "abstract": "Geostatistical learning problems are frequently characterized by spatial autocorrelation in the input features and/or the potential for covariate shift at test time. These realities violate the classical assumption of independent, identically distributed data, upon which most cross-validation algorithms rely in order to estimate the generalization performance of a model. In this paper, we present a theoretical criterion for unbiased cross-validation estimators in the geospatial setting. We also introduce a new cross-validation algorithm to evaluate models, inspired by the challenges of geospatial problems. We apply a framework for categorizing problems into different types of geospatial scenarios to help practitioners select an appropriate cross-validation strategy. Our empirical analyses compare cross-validation algorithms on both simulated and several real datasets to develop recommendations for a variety of geospatial settings. This paper aims to draw attention to some challenges that arise in model evaluation for geospatial problems and to provide guidance for users",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LLKI5Lq2YN": {
    "title": "Adaptive Hyperparameter Selection for Differentially Private Gradient Descent",
    "volume": "main",
    "abstract": "We present an adaptive mechanism for hyperparameter selection in differentially private optimization that addresses the inherent trade-off between utility and privacy. The mechanism eliminates the often unstructured and time-consuming manual effort of selecting hyperparameters and avoids the additional privacy costs that hyperparameter selection otherwise incurs on top of that of the actual algorithm. We instantiate our mechanism for noisy gradient descent on non-convex, convex and strongly convex loss functions, respectively, to derive schedules for the noise variance and step size. These schedules account for the properties of the loss function and adapt to convergence metrics such as the gradient norm. When using these schedules, we show that noisy gradient descent converges at essentially the same rate as its noise-free counterpart. Numerical experiments show that the schedules consistently perform well across a range of datasets without manual tuning",
    "checked": false,
    "id": "aa7240153f0c92d1058792d0b9634c4843402616",
    "semantic_title": "dpis: an enhanced mechanism for differentially private sgd with importance sampling",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=Ub6XILEF9x": {
    "title": "Multiscale Causal Structure Learning",
    "volume": "main",
    "abstract": "Causal structure learning methods are vital for unveiling causal relationships embedded into observed data. However, the state of the art suffers a major limitation: it assumes that causal interactions occur only at the frequency at which data is observed. To address this limitation, this paper proposes a method that allows structural learning of linear causal relationships occurring at different time scales. Specifically, we explicitly take into account instantaneous and lagged inter-relations between multiple time series, represented at different scales, hinging on wavelet transform. We cast the problem as the learning of a multiscale causal graph having sparse structure and dagness constraints, enforcing causality through directed and acyclic topology. To solve the resulting (non-convex) formulation, we propose an algorithm termed MS-CASTLE, which exhibits consistent performance across different noise distributions and wavelet choices. We also propose a single-scale version of our algorithm, SS-CASTLE, which outperforms existing methods in computational efficiency, performance, and robustness on synthetic data. Finally, we apply the proposed approach to learn the multiscale causal structure of the risk of 15 global equity markets, during covid-19 pandemic, illustrating the importance of multiscale analysis to reveal useful interactions at different time resolutions. Financial investors can leverage our approach to manage risk within equity portfolios from a causal perspective, tailored to their investment horizon",
    "checked": true,
    "id": "6b370dc7e28fb8dcd551153101114d56870e40cb",
    "semantic_title": "multiscale causal structure learning",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=xiQXHvL1eN": {
    "title": "Dynamic Regret Analysis of Safe Distributed Online Optimization for Convex and Non-convex Problems",
    "volume": "main",
    "abstract": "This paper addresses safe distributed online optimization over an unknown set of linear safety constraints. A network of agents aims at jointly minimizing a global, time-varying function, which is only partially observable to each individual agent. Therefore, agents must engage in local communications to generate a safe sequence of actions competitive with the best minimizer sequence in hindsight, and the gap between the two sequences is quantified via dynamic regret. We propose distributed safe online gradient descent (D-Safe-OGD) with an exploration phase, where all agents estimate the constraint parameters collaboratively to build estimated feasible sets, ensuring the action selection safety during the optimization phase. We prove that for convex functions, D-Safe-OGD achieves a dynamic regret bound of $O(T^{2/3} \\sqrt{\\log T} + T^{1/3}C_T^*)$, where $C_T^*$ denotes the path-length of the best minimizer sequence. We further prove a dynamic regret bound of $O(T^{2/3}{\\color{black} \\sqrt{\\log T}} + T^{2/3}C_T^*)$ for certain non-convex problems, which establishes the first dynamic regret bound for a safe distributed algorithm in the non-convex setting",
    "checked": true,
    "id": "e8c5928fe05de0b5f0d76d0ef7b6043b1ff8c388",
    "semantic_title": "dynamic regret analysis of safe distributed online optimization for convex and non-convex problems",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=2tdhQMLg36": {
    "title": "Revisiting Image Classifier Training for Improved Certified Robust Defense against Adversarial Patches",
    "volume": "main",
    "abstract": "Certifiably robust defenses against adversarial patches for image classifiers ensure correct prediction against any changes to a constrained neighborhood of pixels. PatchCleanser, the state-of-the-art certified defense, uses a double-masking strategy for robust classification. The success of this strategy relies heavily on the model's invariance to image pixel masking. In this paper, we take a closer look at model training schemes to improve this invariance. Instead of using Random Cutout augmentations like PatchCleanser, we introduce the notion of worst-case masking, i.e., selecting masked images which maximize classification loss. However, finding worst-case masks requires an exhaustive search, which might be prohibitively expensive to do on-the-fly during training. To solve this problem, we propose a two-round greedy masking strategy (Greedy Cutout) which finds an approximate worst-case mask location with much less compute. We show that the models trained with our Greedy Cutout improves certified robust accuracy over Random Cutout in PatchCleanser across a range of datasets and architectures. Certified robust accuracy on ImageNet with a ViT-B16-224 model increases from 58.1% to 62.3% against a 3% square patch applied anywhere on the image",
    "checked": true,
    "id": "4e4f5f1a72d665ff28e7028f9a483dd1900522d4",
    "semantic_title": "revisiting image classifier training for improved certified robust defense against adversarial patches",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=61TKzU9B96": {
    "title": "An Optical Control Environment for Benchmarking Reinforcement Learning Algorithms",
    "volume": "main",
    "abstract": "Deep reinforcement learning has the potential to address various scientific problems. In this paper, we implement an optics simulation environment for reinforcement learning based controllers. The environment captures the essence of nonconvexity, nonlinearity, and time-dependent noise inherent in optical systems, offering a more realistic setting. Subsequently, we provide the benchmark results of several reinforcement learning algorithms on the proposed simulation environment. The experimental findings demonstrate the superiority of off-policy reinforcement learning approaches over traditional control algorithms in navigating the intricacies of complex optical control environments",
    "checked": true,
    "id": "535722991c93f214446b1bc36421b259b6b76f6e",
    "semantic_title": "an optical control environment for benchmarking reinforcement learning algorithms",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0pn3KnbH5F": {
    "title": "Learning-to-defer for sequential medical decision-making under uncertainty",
    "volume": "main",
    "abstract": "Learning-to-defer is a framework to automatically defer decision-making to a human expert when ML-based decisions are deemed unreliable. Existing learning-to-defer frameworks are not designed for sequential settings. That is, they defer at every instance independently, based on immediate predictions, while ignoring the potential long-term impact of these interventions. As a result, existing frameworks are myopic. Further, they do not defer adaptively, which is crucial when human interventions are costly. In this work, we propose Sequential Learning-to-Defer (SLTD), a framework for learning-to-defer to a domain expert in sequential decision-making settings. Contrary to existing literature, we pose the problem of learning-to-defer as model-based reinforcement learning (RL) to i) account for long-term consequences of ML-based actions using RL and ii) adaptively defer based on the dynamics (model-based). Our proposed framework determines whether to defer (at each time step) by quantifying whether a deferral now will improve the value compared to delaying deferral to the next time step. To quantify the improvement, we account for potential future deferrals. As a result, we learn a pre-emptive deferral policy (i.e. a policy that defers early if using the ML-based policy could worsen long-term outcomes). Our deferral policy is adaptive to the non-stationarity in the dynamics. We demonstrate that adaptive deferral via SLTD provides an improved trade-off between long-term outcomes and deferral frequency on synthetic, semi-synthetic, and real-world data with non-stationary dynamics. Finally, we interpret the deferral decision by decomposing the propagated (long-term) uncertainty around the outcome, to justify the deferral decision",
    "checked": true,
    "id": "793f242c80ab54c43aa7da3b628ad6bbf48c041e",
    "semantic_title": "learning-to-defer for sequential medical decision-making under uncertainty",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JFaZ94tT8M": {
    "title": "Learning domain-specific causal discovery from time series",
    "volume": "main",
    "abstract": "Causal discovery (CD) from time-varying data is important in neuroscience, medicine, and machine learning. Techniques for CD encompass randomized experiments, which are generally unbiased but expensive, and algorithms such as Granger causality, conditional-independence-based, structural-equation-based, and score-based methods that are only accurate under strong assumptions made by human designers. However, as demonstrated in other areas of machine learning, human expertise is often not entirely accurate and tends to be outperformed in domains with abundant data. In this study, we examine whether we can enhance domain-specific causal discovery for time series using a data-driven approach. Our findings indicate that this procedure significantly outperforms human-designed, domain-agnostic causal discovery methods, such as Mutual Information, VAR-LiNGAM, and Granger Causality on the MOS 6502 microprocessor, the NetSim fMRI dataset, and the Dream3 gene dataset. We argue that, when feasible, the causality field should consider a supervised approach in which domain-specific CD procedures are learned from extensive datasets with known causal relationships, rather than being designed by human specialists. Our findings promise a new approach toward improving CD in neural and medical data and for the broader machine learning community",
    "checked": true,
    "id": "78f33165d0a9250fc76edf1a6e831c68657d0a69",
    "semantic_title": "learning domain-specific causal discovery from time series",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ThJl4d5JRg": {
    "title": "Dynamic Subgoal-based Exploration via Bayesian Optimization",
    "volume": "main",
    "abstract": "Reinforcement learning in sparse-reward navigation environments with expensive and limited interactions is challenging and poses a need for effective exploration. Motivated by complex navigation tasks that require real-world training (when cheap simulators are not available), we consider an agent that faces an unknown distribution of environments and must decide on an exploration strategy. It may leverage a series of training environments to improve its policy before it is evaluated in a test environment drawn from the same environment distribution. Most existing approaches focus on fixed exploration strategies, while the few that view exploration as a meta-optimization problem tend to ignore the need for _cost-efficient_ exploration. We propose a cost-aware Bayesian optimization approach that efficiently searches over a class of dynamic subgoal-based exploration strategies. The algorithm adjusts a variety of levers --- the locations of the subgoals, the length of each episode, and the number of replications per trial --- in order to overcome the challenges of sparse rewards, expensive interactions, and noise. An experimental evaluation demonstrates that the new approach outperforms existing baselines across a number of problem domains. We also provide a theoretical foundation and prove that the method asymptotically identifies a near-optimal subgoal design",
    "checked": false,
    "id": "b8dc2c6f9ca7c0d55b0379d5067f67b3e834ee80",
    "semantic_title": "model-based causal bayesian optimization",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=V7BvYJyTmM": {
    "title": "Gated Domain Units for Multi-source Domain Generalization",
    "volume": "main",
    "abstract": "The phenomenon of distribution shift (DS) occurs when a dataset at test time differs from the dataset at training time, which can significantly impair the performance of a machine learning model in practical settings due to a lack of knowledge about the data's distribution at test time. To address this problem, we postulate that real-world distributions are composed of latent Invariant Elementary Distributions (I.E.D) across different domains. This assumption implies an invariant structure in the solution space that enables knowledge transfer to unseen domains. To exploit this property for domain generalization, we introduce a modular neural network layer consisting of Gated Domain Units (GDUs) that learn a representation for each latent elementary distribution. During inference, a weighted ensemble of learning machines can be created by comparing new observations with the representations of each elementary distribution. Our flexible framework also accommodates scenarios where explicit domain information is not present. Extensive experiments on image, text, and graph data show consistent performance improvement on out-of-training target domains. These findings support the practicality of the I.E.D assumption and the effectiveness of GDUs for domain generalisation",
    "checked": true,
    "id": "f26d1e49b28f3214e4307302e66f716c14ef129d",
    "semantic_title": "gated domain units for multi-source domain generalization",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=8L7Rh6FIXt": {
    "title": "IBIA: An Incremental Build-Infer-Approximate Framework for Approximate Inference of Partition Function",
    "volume": "main",
    "abstract": "Exact computation of the partition function is known to be intractable, necessitating approximate inference techniques. Existing methods for approximate inference are slow to converge for many benchmarks. The control of accuracy-complexity trade-off is also non-trivial in many of these methods. We propose a novel incremental build-infer-approximate (IBIA) framework for approximate inference that addresses these issues. In this framework, the probabilistic graphical model is converted into a sequence of clique tree forests (SCTF) with bounded clique sizes. We show that the SCTF can be used to efficiently compute the partition function. We propose two new algorithms which are used to construct the SCTF and prove the correctness of both. The first is an algorithm for incremental construction of CTFs that is guaranteed to give a valid CTF with bounded clique sizes and the second is an approximation algorithm that takes a calibrated CTF as input and yields a valid and calibrated CTF with reduced clique sizes as the output. We have evaluated our method using several benchmark sets from recent UAI competitions and our results show good accuracies with competitive runtimes",
    "checked": true,
    "id": "cd68083d1c3f37e85cdcbaa104c86d92edc43d2f",
    "semantic_title": "ibia: an incremental build-infer-approximate framework for approximate inference of partition function",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=iHyhdpsnyi": {
    "title": "Revisiting Sparsity Hunting in Federated Learning: Why does Sparsity Consensus Matter?",
    "volume": "main",
    "abstract": "Edge devices can benefit remarkably from federated learning due to their distributed nature; however, their limited resource and computing power poses limitations in deployment. A possible solution to this problem is to utilize off-the-shelf sparse learning algorithms at the clients to meet their resource budget. However, such naive deployment in the clients causes significant accuracy degradation, especially for highly resource-constrained clients. In particular, our investigations reveal that the lack of consensus in the sparsity masks among the clients may potentially slow down the convergence of the global model and cause a substantial accuracy drop. With these observations, we present \\textit{federated lottery aware sparsity hunting} (FLASH), a unified sparse learning framework for training a sparse sub-model that maintains the performance under ultra-low parameter density while yielding proportional communication benefits. Moreover, given that different clients may have different resource budgets, we present \\textit{hetero-FLASH} where clients can take different density budgets based on their device resource limitations instead of supporting only one target parameter density. Experimental analysis on diverse models and datasets shows the superiority of FLASH in closing the gap with an unpruned baseline while yielding up to $\\mathord{\\sim}10.1\\%$ improved accuracy with $\\mathord{\\sim}10.26\\times$ fewer communication, compared to existing alternatives, at similar hyperparameter settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y1eYplvxrE": {
    "title": "Relating graph auto-encoders to linear models",
    "volume": "main",
    "abstract": "Graph auto-encoders are widely used to construct graph representations in Euclidean vector spaces. However, it has already been pointed out empirically that linear models on many tasks can outperform graph auto-encoders. In our work, we prove that the solution space induced by graph auto-encoders is a subset of the solution space of a linear map. This demonstrates that linear embedding models have at least the representational power of graph auto-encoders based on graph convolutional networks. So why are we still using nonlinear graph auto-encoders? One reason could be that actively restricting the linear solution space might introduce an inductive bias that helps improve learning and generalization. While many researchers believe that the nonlinearity of the encoder is the critical ingredient towards this end, we instead identify the node features of the graph as a more powerful inductive bias. We give theoretical insights by introducing a corresponding bias in a linear model and analyzing the change in the solution space. Our experiments are aligned with other empirical work on this question and show that the linear encoder can outperform the nonlinear encoder when using feature information",
    "checked": true,
    "id": "1ea30fd85da438eb3f526307330380e0532c84b6",
    "semantic_title": "relating graph auto-encoders to linear models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=zmBFzuT2DN": {
    "title": "Deep Operator Learning Lessens the Curse of Dimensionality for PDEs",
    "volume": "main",
    "abstract": "Deep neural networks (DNNs) have achieved remarkable success in numerous domains, and their application to PDE-related problems has been rapidly advancing. This paper provides an estimate for the generalization error of learning Lipschitz operators over Banach spaces using DNNs with applications to various PDE solution operators. The goal is to specify DNN width, depth, and the number of training samples needed to guarantee a certain testing error. Under mild assumptions on data distributions or operator structures, our analysis shows that deep operator learning can have a relaxed dependence on the discretization resolution of PDEs and, hence, lessen the curse of dimensionality in many PDE-related problems including elliptic equations, parabolic equations, and Burgers equations. Our results are also applied to give insights about discretization-invariant in operator learning",
    "checked": true,
    "id": "c39106c6d3ddcba512fe8f776a32c739e4e75375",
    "semantic_title": "deep operator learning lessens the curse of dimensionality for pdes",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=3taIQG4C7H": {
    "title": "Label Noise-Robust Learning using a Confidence-Based Sieving Strategy",
    "volume": "main",
    "abstract": "In learning tasks with label noise, improving model robustness against overfitting is a pivotal challenge because the model eventually memorizes labels, including the noisy ones. Identifying the samples with noisy labels and preventing the model from learning them is a promising approach to address this challenge. When training with noisy labels, the per-class confidence scores of the model, represented by the class probabilities, can be reliable criteria for assessing whether the input label is the true label or the corrupted one. In this work, we exploit this observation and propose a novel discriminator metric called confidence error and a sieving strategy called CONFES to differentiate between the clean and noisy samples effectively. We provide theoretical guarantees on the probability of error for our proposed metric. Then, we experimentally illustrate the superior performance of our proposed approach compared to recent studies on various settings, such as synthetic and real-world label noise. Moreover, we show CONFES can be combined with other state-of-the-art approaches, such as Co-teaching and DivideMix to further improve model performance",
    "checked": true,
    "id": "ed32c5122d899da367270557cdb363fe5fab838b",
    "semantic_title": "label noise-robust learning using a confidence-based sieving strategy",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=igDOV2KBwM": {
    "title": "On Perfect Clustering for Gaussian Processes",
    "volume": "main",
    "abstract": "In this paper, we propose a data based transformation for infinite-dimensional Gaussian processes and derive its limit theorem. For a clustering problem using mixture models, an appropriate modification of this transformation asymptotically leads to perfect separation of the populations under rather general conditions, except the scenario in which differences between clusters depend only on the locations; in which case our procedure is useless. Theoretical properties related to label consistency are studied for the k-means clustering algorithm when used on this transformed data. Good empirical performance of the proposed methodology is demonstrated using simulated as well as benchmark data sets, when compared with some popular parametric and nonparametric methods for such functional data",
    "checked": false,
    "id": "cbdccd06fe0ce89095470050e6559eac040940e6",
    "semantic_title": "vision based coastline detection using autonomous unmanned aerial systems reu participants",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WJt2Pc3qtI": {
    "title": "How Reliable is Your Regression Model's Uncertainty Under Real-World Distribution Shifts?",
    "volume": "main",
    "abstract": "Many important computer vision applications are naturally formulated as regression problems. Within medical imaging, accurate regression models have the potential to automate various tasks, helping to lower costs and improve patient outcomes. Such safety-critical deployment does however require reliable estimation of model uncertainty, also under the wide variety of distribution shifts that might be encountered in practice. Motivated by this, we set out to investigate the reliability of regression uncertainty estimation methods under various real-world distribution shifts. To that end, we propose an extensive benchmark of 8 image-based regression datasets with different types of challenging distribution shifts. We then employ our benchmark to evaluate many of the most common uncertainty estimation methods, as well as two state-of-the-art uncertainty scores from the task of out-of-distribution detection. We find that while methods are well calibrated when there is no distribution shift, they all become highly overconfident on many of the benchmark datasets. This uncovers important limitations of current uncertainty estimation methods, and the proposed benchmark therefore serves as a challenge to the research community. We hope that our benchmark will spur more work on how to develop truly reliable regression uncertainty estimation methods",
    "checked": true,
    "id": "f01dfb620ae80f8ad56fc1a6f4d84eb52200fbf0",
    "semantic_title": "how reliable is your regression model's uncertainty under real-world distribution shifts?",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=qcCE4mC2jI": {
    "title": "RIGNN: A Rationale Perspective for Semi-supervised Open-world Graph Classification",
    "volume": "main",
    "abstract": "Graph classification has gained growing attention in the graph machine learning community and a variety of semi-supervised methods have been developed to reduce the high cost of annotation. They usually combine graph neural networks (GNNs) and extensive semi-supervised techniques such as knowledge distillation. However, they adhere to the close-set assumption that unlabeled graphs all belong to known classes, limiting their applications in the real world. This paper goes further, investigating a practical problem of semi-supervised open-world graph classification where these unlabeled graph data could come from unseen classes. A novel approach named Rationale-Informed GNN (RIGNN) is proposed, which takes a rationale view to detect components containing the most information related to the label space and classify unlabeled graphs into a known class or an unseen class. In particular, RIGNN contains a relational detector and a feature extractor to produce effective rationale features, which maximize the mutual information with label information and exhibit sufficient disentanglement with non-rationale elements. Furthermore, we construct a graph-of-graph based on geometrical relationships, which gives instructions on enhancing rationale representations. In virtue of effective rationale representations, we can provide accurate and balanced predictions for unlabeled graphs. An extension is also made to accomplish effective open-set graph classification. We verify our proposed methods on four benchmark datasets in various settings and experimental results reveal the effectiveness of our proposed RIGNN compared with state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JwGKVpRfVD": {
    "title": "SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration",
    "volume": "main",
    "abstract": "The ability to effectively reuse prior knowledge is a key requirement when building general and flexible Reinforcement Learning (RL) agents. Skill reuse is one of the most common approaches, but current methods have considerable limitations. For example, fine-tuning an existing policy frequently fails, as the policy can degrade rapidly early in training. In a similar vein, distillation of expert behavior can lead to poor results when given sub-optimal experts. We compare several common approaches for skill transfer on multiple domains including changes in task and system dynamics. We identify how existing methods fail and introduce an alternative approach to mitigate these problems. Our approach learns to sequence temporally-extended skills for exploration but learns the final policy directly from the raw experience. This conceptual split enables rapid adaptation and thus efficient data collection but without constraining the final solution. It significantly outperforms many classical methods across a suite of evaluation tasks and we use a broad set of ablations to highlight the importance of different components of our method",
    "checked": true,
    "id": "6a4c74430e9c9c165ef3321edf6b4e9bdc36dea9",
    "semantic_title": "skills: adaptive skill sequencing for efficient temporally-extended exploration",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=cJgHzw8Qhq": {
    "title": "Estimating Differential Equations from Temporal Point Processes",
    "volume": "main",
    "abstract": "Ordinary differential equations (ODEs) allow interpretation of phenomena in various scientific fields. They have mostly been applied to numerical data observed at regular intervals, but not to irregularly observed discrete events, also known as point processes. In this study, we introduce an ODE modeling of such events by combining ODEs with log-Gaussian Cox processes (Møller et al., 1998). In the experiments with different types of ODEs regarding infectious disease, predator-prey interaction, and competition among participants, our method outperformed existing baseline methods assuming regularly observed continuous data with respect to the accuracy of recovering the latent parameters of ODEs. Through both synthetic and actual examples, we also showed the ability of our method to extrapolate, model latent events that cannot be observed, and offer interpretability of phenomena from the viewpoint of the estimated parameters of ODE",
    "checked": true,
    "id": "65ce2d8059bbff71df6bf402d7b8df9fc8ffa9fa",
    "semantic_title": "estimating differential equations from temporal point processes",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XuOE99cmST": {
    "title": "Turning a Curse into a Blessing: Enabling In-Distribution-Data-Free Backdoor Removal via Stabilized Model Inversion",
    "volume": "main",
    "abstract": "The effectiveness of many existing techniques for removing backdoors from machine learning models relies on access to clean in-distribution data. However, given that these models are often trained on proprietary datasets, it may not be practical to assume that in-distribution samples will always be available. On the other hand, model inversion techniques, which are typically viewed as privacy threats, can reconstruct realistic training samples from a given model, potentially eliminating the need for in-distribution data. To date, the only prior attempt to integrate backdoor removal and model inversion involves a simple combination that produced very limited results. This work represents a first step toward a more thorough understanding of how model inversion techniques could be leveraged for effective backdoor removal. Specifically, we seek to answer several key questions: What properties must reconstructed samples possess to enable successful defense? Is perceptual similarity to clean samples enough, or are additional characteristics necessary? Is it possible for reconstructed samples to contain backdoor triggers? We demonstrate that relying solely on perceptual similarity is insufficient for effective defenses. The stability of model predictions in response to input and parameter perturbations also plays a critical role. To address this, we propose a new bi-level optimization based framework for model inversion that promotes stability in addition to visual quality. Interestingly, we also find that reconstructed samples from a pre-trained generator's latent space do not contain backdoors, even when signals from a backdoored model are utilized for reconstruction. We provide a theoretical analysis to explain this observation. Our evaluation shows that our stabilized model inversion technique achieves state-of-the-art backdoor removal performance without requiring access to clean in-distribution data. Furthermore, its performance is on par with or even better than using the same amount of clean samples",
    "checked": true,
    "id": "f65c2209859b8de0bd7edbfeacbe62e263af6bd7",
    "semantic_title": "turning a curse into a blessing: enabling in-distribution-data-free backdoor removal via stabilized model inversion",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8koy8QuTZD": {
    "title": "Momentum Tracking: Momentum Acceleration for Decentralized Deep Learning on Heterogeneous Data",
    "volume": "main",
    "abstract": "SGD with momentum is one of the key components for improving the performance of neural networks. For decentralized learning, a straightforward approach using momentum is Distributed SGD (DSGD) with momentum (DSGDm). However, DSGDm performs worse than DSGD when the data distributions are statistically heterogeneous. Recently, several studies have addressed this issue and proposed methods with momentum that are more robust to data heterogeneity than DSGDm, although their convergence rates remain dependent on data heterogeneity and deteriorate when the data distributions are heterogeneous. In this study, we propose Momentum Tracking, which is a method with momentum whose convergence rate is proven to be independent of data heterogeneity. More specifically, we analyze the convergence rate of Momentum Tracking in the setting where the objective function is non-convex and the stochastic gradient is used. Then, we identify that it is independent of data heterogeneity for any momentum coefficient $\\beta \\in [0, 1)$. Through experiments, we demonstrate that Momentum Tracking is more robust to data heterogeneity than the existing decentralized learning methods with momentum and can consistently outperform these existing methods when the data distributions are heterogeneous",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KQ5jI19kF3": {
    "title": "Optimistic Optimization of Gaussian Process Samples",
    "volume": "main",
    "abstract": "Bayesian optimization is a popular formalism for global optimization, but its computational costs limit it to expensive-to-evaluate functions. A competing, computationally more effi- cient, global optimization framework is optimistic optimization, which exploits prior knowl- edge about the geometry of the search space in form of a dissimilarity function. We investi- gate to which degree the conceptual advantages of Bayesian Optimization can be combined with the computational efficiency of optimistic optimization. By mapping the kernel to a dissimilarity, we obtain an optimistic optimization algorithm for the Bayesian Optimization setting with a run-time of up to $O(N log N )$. As a high-level take-away we find that, when using stationary kernels on objectives of low evaluation cost, optimistic optimization can be preferable over Bayesian optimization, while for strongly coupled and parametric models, Bayesian optimization can perform much better, even at low evaluation cost. As a concep- tual takeaway, our results demonstrate that balancing exploration and exploitation under Gaussian process assumptions does not require computing a posterior",
    "checked": true,
    "id": "5d3af1d038b1057c319038e896a6fd96c40a764a",
    "semantic_title": "optimistic optimization of gaussian process samples",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xoLyps2qWc": {
    "title": "Linearized Relative Positional Encoding",
    "volume": "main",
    "abstract": "Relative positional encoding is widely used in vanilla and linear transformers to represent positional information. However, existing encoding methods of a vanilla transformer are not always directly applicable to a linear transformer, because the latter requires a decomposition of the query and key representations into separate kernel functions. Nevertheless, principles for designing encoding methods suitable for linear transformers remain understudied. In this work, we put together a variety of existing linear relative positional encoding approaches under a canonical form and further propose a family of linear relative positional encoding algorithms via unitary transformation. Our formulation leads to a principled framework that can be used to develop new relative positional encoding methods that preserve linear space-time complexity. Equipped with different models, the proposed linearized relative positional encoding (LRPE) family derives effective encoding for various applications. Experiments show that compared with existing methods, LRPE achieves state-of-the-art performance in language modeling, text classification, and image classification. Meanwhile, it emphasizes a general paradigm for designing broadly more relative positional encoding methods that are applicable to linear transformers",
    "checked": true,
    "id": "8bc8b9ae855bc0aa19e7223899440ffbdc61f4d8",
    "semantic_title": "linearized relative positional encoding",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=oud7Ny0KQy": {
    "title": "RIFLE: Imputation and Robust Inference from Low Order Marginals",
    "volume": "main",
    "abstract": "The ubiquity of missing values in real-world datasets poses a challenge for statistical inference and can prevent similar datasets from being analyzed in the same study, precluding many existing datasets from being used for new analyses. While an extensive collection of packages and algorithms have been developed for data imputation, the overwhelming majority perform poorly if there are many missing values and low sample sizes, which are unfortunately common characteristics in empirical data. Such low-accuracy estimations adversely affect the performance of downstream statistical models. We develop a statistical inference framework for predicting the target variable in the presence of missing data without imputation. Our framework, RIFLE (Robust InFerence via Low-order moment Estimations), estimates low-order moments of the underlying data distribution with corresponding confidence intervals to learn a distributionally robust model. We specialize our framework to linear regression and normal discriminant analysis, and we provide convergence and performance guarantees. This framework can also be adapted to impute missing data. We compare RIFLE with state-of-the-art approaches (including MICE, Amelia, MissForest, KNN-imputer, MIDA, and Mean Imputer) in numerical experiments. Our experiments demonstrate that RIFLE outperforms other benchmark algorithms when the percentage of missing values is high and/or when the number of data points is relatively small. RIFLE is publicly available",
    "checked": true,
    "id": "d0c854843e362856b46ace44c484310ab33616e2",
    "semantic_title": "rifle: imputation and robust inference from low order marginals",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=zkRCp4RmAF": {
    "title": "Offline Reinforcement Learning with Mixture of Deterministic Policies",
    "volume": "main",
    "abstract": "Offline reinforcement learning (RL) has recently attracted considerable attention as an approach for utilizing past experiences to learn a policy. Recent studies have reported the challenges of offline RL, such as estimating the values of actions that are outside the data distribution. To mitigate offline RL issues, we propose an algorithm that leverages a mixture of deterministic policies. When the data distribution is multimodal, fitting a policy modeled with a unimodal distribution, such as Gaussian distribution, may lead to interpolation between separate modes, thereby resulting in the value estimation of actions that are outside the data distribution. In our framework, the state-action space is divided by learning discrete latent variables, and the sub-policies corresponding to each region are trained. The proposed algorithm was derived by considering the variational lower bound of the offline RL objective function. We show empirically that the use of the proposed mixture policy can reduce the accumulation of the critic loss in offline RL, which was reported in previous studies. Experimental results also indicate that using a mixture of deterministic policies in offline RL improves the performance with the D4RL benchmarking datasets",
    "checked": true,
    "id": "7950a565d8fb5d26d0987a2e3ac336abd9f62581",
    "semantic_title": "offline reinforcement learning with mixture of deterministic policies",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lvevdX6bxm": {
    "title": "Quantization Robust Federated Learning for Efficient Inference on Heterogeneous Devices",
    "volume": "main",
    "abstract": "Federated Learning (FL) is a machine learning paradigm to distributively learn machine learning models from decentralized data that remains on-device. Despite the success of standard Federated optimization methods, such as Federated Averaging (FedAvg) in FL, the energy demands and hardware induced constraints for on-device learning have not been considered sufficiently in the literature. Specifically, an essential demand for on-device learning is to enable trained models to be quantized to various bit-widths based on the energy needs and heterogeneous hardware designs across the federation. In this work, we introduce multiple variants of federated averaging algorithm that train neural networks robust to quantization. Such networks can be quantized to various bit-widths with only limited reduction in full precision model accuracy. We perform extensive experiments on standard FL benchmarks to evaluate our proposed FedAvg variants for quantization robustness and provide a convergence analysis for our Quantization-Aware variants in FL. Our results demonstrate that integrating quantization robustness results in FL models that are significantly more robust to different bit-widths during quantized on-device inference",
    "checked": true,
    "id": "b253022a77ebf30d4a5964d68087d9ff9860b4d4",
    "semantic_title": "quantization robust federated learning for efficient inference on heterogeneous devices",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wRepWp1KC7": {
    "title": "Fair and Useful Cohort Selection",
    "volume": "main",
    "abstract": "A challenge in fair algorithm design is that, while there are compelling notions of individual fairness, these notions typically do not satisfy desirable composition properties, and downstream applications based on fair classifiers might not preserve fairness. To study fairness under composition, Dwork & Ilvento (2019) introduced an archetypal problem called fair-cohort-selection problem, where a single fair classifier is composed with itself to select a group of candidates of a given size, and proposed a solution to this problem. In this work we design algorithms for selecting cohorts that not only preserve fairness, but also maximize the utility of the selected cohort under two notions of utility that we introduce and motivate. We give optimal (or approximately optimal) polynomial-time algorithms for this problem in both an offline setting, and an online setting where candidates arrive one at a time and are classified as they arrive",
    "checked": false,
    "id": "fa0a6e64fe4b44306a26bf25e103ab2e8b716a6f",
    "semantic_title": "fully automated wound tissue segmentation using deep learning on mobile devices: cohort study",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=vgXnEyeWVY": {
    "title": "Walking Out of the Weisfeiler Leman Hierarchy: Graph Learning Beyond Message Passing",
    "volume": "main",
    "abstract": "We propose CRaWl, a novel neural network architecture for graph learning. Like graph neural networks, CRaWl layers update node features on a graph and thus can freely be combined or interleaved with GNN layers. Yet CRaWl operates fundamentally different from message passing graph neural networks. CRaWl layers extract and aggregate information on subgraphs appearing along random walks through a graph using 1D Convolutions. Thereby it detects long range interactions and computes non-local features. As the theoretical basis for our approach, we prove a theorem stating that the expressiveness of CRaWl is incomparable with that of the Weisfeiler Leman algorithm and hence with graph neural networks. That is, there are functions expressible by CRaWl, but not by GNNs and vice versa. This result extends to higher levels of the Weisfeiler Leman hierarchy and thus to higher-order GNNs. Empirically, we show that CRaWl matches state-of-the-art GNN architectures across a multitude of benchmark datasets for classification and regression on graphs",
    "checked": true,
    "id": "bb33180ef8e0ff63947c985e65b13b118443c32c",
    "semantic_title": "walking out of the weisfeiler leman hierarchy: graph learning beyond message passing",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=xWrtiJwJj5": {
    "title": "Global Contrastive Learning for Long-Tailed Classification",
    "volume": "main",
    "abstract": "We consider the long-tailed classification problem in which a few classes in the training data dominate the majority of the other classes. For concreteness, we focus on the visual domain in this paper. Most current methods employ contrastive learning to learn a representation for long-tailed data. In this paper, first, we investigate $k$-positive sampling, a popular baseline method widely used to build contrastive learning models for imbalanced data. Previous works show that $k$-positive learning, which only chooses $k$ positive samples (instead of all positive images) for each query image, suffers from inferior performance in long-tailed data. In this work, we further point out that k-positive learning limits the learning capability of both head and tail classes. Based on this perspective, we propose a novel contrastive learning framework that improves the limitation in k-positive learning by enlarging its positive selection space, so it can help the model learn more semantic discrimination features. Second, we analyze how the temperature (the hyperparameter used for tuning a concentration of samples on feature space) affects the gradients of each class in long-tailed learning, and propose a new method that can mitigate inadequate gradients between classes, which can help model learning easier. We name this framework as CoGloAT. Finally, we go on to introduce a new prototype learning framework namely ProCo based on coreset selection, which creates a global prototype for each cluster while keeping the computation cost within a reasonable time and show that combining CoGloAT with ProCo can further enhance the model learning ability on long-tailed data",
    "checked": true,
    "id": "684514104a254f08e41826edf5788c09a3004dff",
    "semantic_title": "global contrastive learning for long-tailed classification",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KpElM2S9pw": {
    "title": "Approximating Naive Bayes on Unlabelled Categorical Data",
    "volume": "main",
    "abstract": "We address the question of binary classification when no labels are available and the input features are categorical. The lack of labels means supervised approaches can't be used, and the lack of a natural distance measure means that most unsupervised methods do poorly. For such problems, where the alternatives might be a) do nothing or b) heuristic rules-based approaches, we offer a third alternative: a classifier that approximates Naive Bayes. Our primary scenarios are those that involve distinguishing scripted, or bot, web traffic from that of legitimate users. Our main assumption is the existence of some attribute $x_*$ more prevalent in the benign than the scripted traffic; i.e., $P(x_*|\\overline{\\mbox{bot}}) = K \\cdot P(x_*|\\mbox{bot}),$ for $K>1.$ We show that any such disparity yields a lower bound on $P(\\mbox{bot}|x_{j})$ even when we have no prior estimates of $P(x_*|\\overline{\\mbox{bot}}),$ $P(x_*|\\mbox{bot})$ or $K$ (except that $K>1$). We show that when at least one bin of at least one feature receives no attack traffic then we under-estimate the actual conditional probability by a factor of $1-1/K.$ Thus, any attribute with a large disparity between prevalence in benign and abuse traffic (i.e., $K$ is large), allows good approximation of the Naive Bayes classifier without the benefit of labels. The approach is particularly suited to problems where $K$ is high and thus the approximation is very accurate. Example problems (and relevant attributes) might be: password-guessing, if login attempts from legitimate users succeed at a much higher rate than those from password-guessing attackers; Credit Card Verification Value (CVV) guessing, if an attacker exhaustively tries all possible 3 or 4-digit values and fails at a higher rate than legitimate users; account registration, if legitimate users use email addresses from services that do not allow fee anonymous accounts (e.g., {\\tt .edu}) at a much higher rate than attackers; click-fraud if legitimate users visit pages and services that contain no ads at a higher rate than click-fraud bots",
    "checked": true,
    "id": "2efeffd9aa3d78e64c7f9959646b967ed4edf6eb",
    "semantic_title": "approximating naive bayes on unlabelled categorical data",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uaHyXxyp2r": {
    "title": "Weight-balancing fixes and flows for deep learning",
    "volume": "main",
    "abstract": "Feedforward neural networks with homogeneous activation functions possess an internal symmetry: the functions they compute do not change when the incoming and outgoing weights at any hidden unit are rescaled by reciprocal positive values. This paper makes two contributions to our understanding of these networks. The first is to describe a simple procedure, or {\\it fix}, for balancing the weights in these networks: this procedure computes multiplicative rescaling factors---one at each hidden unit---that rebalance the weights of these networks without changing the end-to-end functions that they compute. Specifically, given an initial network with arbitrary weights, the procedure determines the functionally equivalent network whose weight matrix is of minimal $\\ell_{p,q}$-norm; the weights at each hidden unit are said to be balanced when this norm is stationary with respect to rescaling transformations. The optimal rescaling factors are computed in an iterative fashion via simple multiplicative updates, and the updates are notable in that (a) they do not require the tuning of learning rates, (b) they operate in parallel on the rescaling factors at all hidden units, and (c) they converge monotonically to a global minimizer of the $\\ell_{p,q}$-norm. The paper's second contribution is to analyze the optimization landscape for learning in these networks. We suppose that the network's loss function consists of two terms---one that is invariant to rescaling transformations, measuring predictive accuracy, and another (a regularizer) that breaks this invariance, penalizing large weights. We show how to derive a weight-balancing {\\it flow} such that the regularizer remains minimal with respect to rescaling transformations as the weights descend in the loss function. These dynamics reduce to an ordinary gradient flow for $\\ell_2$-norm regularization, but not otherwise. In this way our analysis suggests a canonical pairing of alternative flows and regularizers",
    "checked": true,
    "id": "a78c53259ea22c0b49268dc72c6f5091811aed56",
    "semantic_title": "weight-balancing fixes and flows for deep learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lOegPKSu04": {
    "title": "$k$-Mixup Regularization for Deep Learning via Optimal Transport",
    "volume": "main",
    "abstract": "Mixup is a popular regularization technique for training deep neural networks that improves generalization and increases robustness to certain distribution shifts. It perturbs input training data in the direction of other randomly-chosen instances in the training set. To better leverage the structure of the data, we extend mixup in a simple, broadly applicable way to $k$-mixup, which perturbs $k$-batches of training points in the direction of other $k$-batches. The perturbation is done with displacement interpolation, i.e. interpolation under the Wasserstein metric. We demonstrate theoretically and in simulations that $k$-mixup preserves cluster and manifold structures, and we extend theory studying the efficacy of standard mixup to the $k$-mixup case. Our empirical results show that training with $k$-mixup further improves generalization and robustness across several network architectures and benchmark datasets of differing modalities. For the wide variety of real datasets considered, the performance gains of $k$-mixup over standard mixup are similar to or larger than the gains of mixup itself over standard ERM after hyperparameter optimization. In several instances, in fact, $k$-mixup achieves gains in settings where standard mixup has negligible to zero improvement over ERM",
    "checked": false,
    "id": "3c25c031c93ca1808fcb0923a7b6f89e78ed9a65",
    "semantic_title": "k-mixup regularization for deep learning via optimal transport",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=0Xo9giEZWf": {
    "title": "HypUC: Hyperfine Uncertainty Calibration with Gradient- boosted Corrections for Reliable Regression on Imbalanced Electrocardiograms",
    "volume": "main",
    "abstract": "The automated analysis of medical time series, such as the electrocardiogram (ECG), electroencephalogram (EEG), pulse oximetry, etc, has the potential to serve as a valuable tool for diagnostic decisions, allowing for remote monitoring of patients and more efficient use of expensive and time-consuming medical procedures. Deep neural networks (DNNs) have been demonstrated to process such signals effectively. However, previous research has primarily focused on classifying medical time series rather than attempting to regress the continuous-valued physiological parameters central to diagnosis. One significant challenge in this regard is the imbalanced nature of the dataset, as a low prevalence of abnormal conditions can lead to heavily skewed data that results in inaccurate predictions and a lack of certainty in such predictions when deployed. To address these challenges, we propose HypUC, a framework for imbalanced probabilistic regression in medical time series, making several contributions. (i) We introduce a simple kernel density-based technique to tackle the imbalanced regression problem with medical time series. (ii) Moreover, we employ a probabilistic regression framework that allows uncertainty estimation for the predicted continuous values. (iii) We also present a new approach to calibrate the predicted uncertainty further. (iv) Finally, we demonstrate a technique to use calibrated uncertainty estimates to improve the predicted continuous value and show the efficacy of the calibrated uncertainty estimates to flag unreliable predictions. HypUC is evaluated on a large, diverse, real-world dataset of ECGs collected from millions of patients, outperforming several conventional baselines on various diagnostic tasks, suggesting potential use-case for the reliable clinical deployment of deep learning models and a prospective clinical trial. Consequently, a hyperkalemia diagnosis algorithm based on HypUC is going to be the subject of a real-world clinical prospective study",
    "checked": false,
    "id": "e2b4d10d0b8de66ba072b730b44fc3eb19081311",
    "semantic_title": "hypuc : hyperfine uncertainty calibration with gradient-boosted corrections for reliable regression on imbalanced electrocardiograms",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wbpxTuXgm0": {
    "title": "TSMixer: An All-MLP Architecture for Time Series Forecast-ing",
    "volume": "main",
    "abstract": "Real-world time-series datasets are often multivariate with complex dynamics. To capture this complexity, high capacity architectures like recurrent- or attention-based sequential deep learning models have become popular. However, recent work demonstrates that simple univariate linear models can outperform such deep learning models on several commonly used academic benchmarks. Extending them, in this paper, we investigate the capabilities of linear models for time-series forecasting and present Time-Series Mixer (TSMixer), a novel architecture designed by stacking multi-layer perceptrons (MLPs). TSMixer is based on mixing operations along both the time and feature dimensions to extract information efficiently. On popular academic benchmarks, the simple-to-implement TSMixer is comparable to specialized state-of-the-art models that leverage the inductive biases of specific benchmarks. On the challenging and large scale M5 benchmark, a real-world retail dataset, TSMixer demonstrates superior performance compared to the state-of-the-art alternatives. Our results underline the importance of efficiently utilizing cross-variate and auxiliary information for improving the performance of time series forecasting. We present various analyses to shed light into the capabilities of TSMixer. The design paradigms utilized in TSMixer are expected to open new horizons for deep learning-based time series forecasting",
    "checked": false,
    "id": "59694c8dce4f13db2f486eb8102459a3f7c23da6",
    "semantic_title": "tsmixer: an all-mlp architecture for time series forecasting",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=ScrEUZLxPr": {
    "title": "Revisiting Hidden Representations in Transfer Learning for Medical Imaging",
    "volume": "main",
    "abstract": "While a key component to the success of deep learning is the availability of massive amounts of training data, medical image datasets are often limited in diversity and size. Transfer learning has the potential to bridge the gap between related yet different domains. For medical applications, however, it remains unclear whether it is more beneficial to pre-train on natural or medical images. We aim to shed light on this problem by comparing initialization on ImageNet and RadImageNet on seven medical classification tasks. Our work includes a replication study, which yields results contrary to previously published findings. In our experiments, ResNet50 models pre-trained on ImageNet tend to outperform those trained on RadImageNet. To gain further insights, we investigate the learned representations using Canonical Correlation Analysis (CCA) and compare the predictions of the different models. Our results indicate that, contrary to intuition, ImageNet and RadImageNet may converge to distinct intermediate representations, which appear to diverge further during fine-tuning. Despite these distinct representations, the predictions of the models remain similar. Our findings show that the similarity between networks before and after fine-tuning does not correlate with performance gains, suggesting that the advantages of transfer learning might not solely originate from the reuse of features in the early layers of a convolutional neural network",
    "checked": true,
    "id": "f10a7d49d67d292e03e8f8185310147b4e6fdebc",
    "semantic_title": "revisiting hidden representations in transfer learning for medical imaging",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VrvGHDSzZ7": {
    "title": "The Geometry of Mixability",
    "volume": "main",
    "abstract": "Mixable loss functions are of fundamental importance in the context of prediction with expert advice in the online setting since they characterize fast learning rates. By re-interpreting properness from the point of view of differential geometry, we provide a simple geometric characterization of mixability for the binary and multi-class cases: a proper loss function $\\ell$ is $\\eta$-mixable if and only if the superprediction set $\\textrm{spr}(\\eta \\ell)$ of the scaled loss function $\\eta \\ell$ slides freely inside the superprediction set $\\textrm{spr}(\\ell_{\\log})$ of the log loss $\\ell_{\\log}$, under fairly general assumptions on the differentiability of $\\ell$. Our approach provides a way to treat some concepts concerning loss functions (like properness) in a ''coordinate-free'' manner and reconciles previous results obtained for mixable loss functions for the binary and the multi-class cases",
    "checked": true,
    "id": "5c13e876605fa2e46518ac5ebbd5f2d44d9b425e",
    "semantic_title": "the geometry of mixability",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=hjDYJUn9l1": {
    "title": "Evaluating Human-Language Model Interaction",
    "volume": "main",
    "abstract": "Many real-world applications of language models (LMs), such as writing assistance and code autocomplete, involve human-LM interaction. However, most benchmarks are non-interactive in that a model produces output without human involvement. To evaluate human-LM interaction, we develop a new framework, Human-AI Language-based Interaction Evaluation (HALIE), that defines the components of interactive systems and dimensions to consider when designing evaluation metrics. Compared to standard, non-interactive evaluation, HALIE captures (i) the interactive process, not only the final output; (ii) the first-person subjective experience, not just a third-party assessment; and (iii) notions of preference beyond quality (e.g., enjoyment and ownership). We then design five tasks to cover different forms of interaction: social dialogue, question answering, crossword puzzles, summarization, and metaphor generation. With four state-of-the-art LMs (three variants of OpenAI's GPT-3 and AI21 Labs' Jurassic-1), we find that better non-interactive performance does not always translate to better human-LM interaction. In particular, we highlight three cases where the results from non-interactive and interactive metrics diverge and underscore the importance of human-LM interaction for LM evaluation",
    "checked": true,
    "id": "30fd777f4e25b850236faaaece55978a6bc4f71c",
    "semantic_title": "evaluating human-language model interaction",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=2uMnAwWnRy": {
    "title": "Benchmarking Continuous Time Models for Predicting Multiple Sclerosis Progression",
    "volume": "main",
    "abstract": "Multiple sclerosis is a disease that affects the brain and spinal cord, it can lead to severe disability and has no known cure. The majority of prior work in machine learning for multiple sclerosis has been centered around using Magnetic Resonance Imaging scans or laboratory tests; these modalities are both expensive to acquire and can be unreliable. In a recent paper it was shown that disease progression can be predicted effectively using performance outcome measures and demographic data. In our work we build on this to investigate the modeling side, using continuous time models to predict progression. We benchmark four continuous time models using a publicly available multiple sclerosis dataset. We find that the best continuous model is often able to outperform the best benchmarked discrete time model. We also carry out an extensive ablation to discover the sources of performance gains, we find that standardizing existing features leads to a larger performance increase than interpolating missing features",
    "checked": true,
    "id": "0fa5f75127d08224434d51b1bb5d5f04022a285d",
    "semantic_title": "benchmarking continuous time models for predicting multiple sclerosis progression",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=ZPpQk7FJXF": {
    "title": "Differentially Private Diffusion Models",
    "volume": "main",
    "abstract": "While modern machine learning models rely on increasingly large training datasets, data is often limited in privacy-sensitive domains. Generative models trained with differential privacy (DP) on sensitive data can sidestep this challenge, providing access to synthetic data instead. We build on the recent success of diffusion models (DMs) and introduce Differentially Private Diffusion Models (DPDMs), which enforce privacy using differentially private stochastic gradient descent (DP-SGD). We investigate the DM parameterization and the sampling algorithm, which turn out to be crucial ingredients in DPDMs, and propose noise multiplicity, a powerful modification of DP-SGD tailored to the training of DMs. We validate our novel DPDMs on image generation benchmarks and achieve state-of-the-art performance in all experiments. Moreover, on standard benchmarks, classifiers trained on DPDM-generated synthetic data perform on par with task-specific DP-SGD-trained classifiers, which has not been demonstrated before for DP generative models. Project page and code: https://nv-tlabs.github.io/DPDM",
    "checked": true,
    "id": "9dea2c15a044a3c83d0d66b9c3daa91d457d905f",
    "semantic_title": "differentially private diffusion models",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=JaNlH6dZYk": {
    "title": "On the special role of class-selective neurons in early training",
    "volume": "main",
    "abstract": "It is commonly observed that deep networks trained for classification exhibit class-selective neurons in their early and intermediate layers. Intriguingly, recent studies have shown that these class-selective neurons can be ablated without deteriorating network function. But if class-selective neurons are not necessary, why do they exist? We attempt to answer this question in a series of experiments on ResNet-50s trained on ImageNet. We first show that class-selective neurons emerge during the first few epochs of training, before receding rapidly but not completely; this suggests that class-selective neurons found in trained networks are in fact vestigial remains of early training. With single-neuron ablation experiments, we then show that class-selective neurons are important for network function in this early phase of training. We also observe that the network is close to a linear regime in this early phase; we thus speculate that class-selective neurons appear early in training as quasi-linear shortcut solutions to the classification task. Finally, in causal experiments where we regularize against class selectivity at different points in training, we show that the presence of class-selective neurons early in training is critical to the successful training of the network; in contrast, class-selective neurons can be suppressed later in training with little effect on final accuracy. It remains to be understood by which mechanism the presence of class-selective neurons in the early phase of training contributes to the successful training of networks",
    "checked": true,
    "id": "6fbbe8a5554a3fbc003f288c954c2e66ef4c854d",
    "semantic_title": "on the special role of class-selective neurons in early training",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=MgdoxzImlK": {
    "title": "Multi-annotator Deep Learning: A Probabilistic Framework for Classification",
    "volume": "main",
    "abstract": "Solving complex classification tasks using deep neural networks typically requires large amounts of annotated data. However, corresponding class labels are noisy when provided by error-prone annotators, e.g., crowdworkers. Training standard deep neural networks leads to subpar performances in such multi-annotator supervised learning settings. We address this issue by presenting a probabilistic training framework named multi-annotator deep learning (MaDL). A downstream ground truth and an annotator performance model are jointly trained in an end-to-end learning approach. The ground truth model learns to predict instances' true class labels, while the annotator performance model infers probabilistic estimates of annotators' performances. A modular network architecture enables us to make varying assumptions regarding annotators' performances, e.g., an optional class or instance dependency. Further, we learn annotator embeddings to estimate annotators' densities within a latent space as proxies of their potentially correlated annotations. Together with a weighted loss function, we improve the learning from correlated annotation patterns. In a comprehensive evaluation, we examine three research questions about multi-annotator supervised learning. Our findings show MaDL's state-of-the-art performance and robustness against many correlated, spamming annotators",
    "checked": true,
    "id": "d6c0c06891ce1ae6d5ede5eafa47e8da655d3c80",
    "semantic_title": "multi-annotator deep learning: a probabilistic framework for classification",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ns2X7Azudy": {
    "title": "Learning to Optimize Quasi-Newton Methods",
    "volume": "main",
    "abstract": "Fast gradient-based optimization algorithms have become increasingly essential for the computationally efficient training of machine learning models. One technique is to multiply the gradient by a preconditioner matrix to produce a step, but it is unclear what the best preconditioner matrix is. This paper introduces a novel machine learning optimizer called LODO, which tries to online meta-learn the best preconditioner during optimization. Specifically, our optimizer merges Learning to Optimize (L2O) techniques with quasi-Newton methods to learn preconditioners parameterized as neural networks; they are more flexible than preconditioners in other quasi-Newton methods. Unlike other L2O methods, LODO does not require any meta-training on a training task distribution, and instead learns to optimize on the fly while optimizing on the test task, adapting to the local characteristics of the loss landscape while traversing it. Theoretically, we show that our optimizer approximates the inverse Hessian in noisy loss landscapes and is capable of representing a wide range of inverse Hessians. We experimentally verify that our algorithm can optimize in noisy settings, and show that simpler alternatives for representing the inverse Hessians worsen performance. Lastly, we use our optimizer to train a semi-realistic deep neural network with 95k parameters at speeds comparable to those of standard neural network optimizers",
    "checked": true,
    "id": "17d5196926077fc4343bdb6bc1724ee65f945fe4",
    "semantic_title": "learning to optimize quasi-newton methods",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=SSkTBUyJip": {
    "title": "Task Weighting in Meta-learning with Trajectory Optimisation",
    "volume": "main",
    "abstract": "Developing meta-learning algorithms that are un-biased toward a subset of training tasks often requires hand-designed criteria to weight tasks, potentially resulting in sub-optimal solutions. In this paper, we introduce a new principled and fully-automated task-weighting algorithm for meta-learning methods. By considering the weights of tasks within the same mini-batch as an action, and the meta-parameter of interest as the system state, we cast the task-weighting meta-learning problem to a trajectory optimisation and employ the iterative linear quadratic regulator to determine the optimal action or weights of tasks. We theoretically show that the proposed algorithm converges to an $\\epsilon_{0}$-stationary point, and empirically demonstrate that the proposed approach out-performs common hand-engineering weighting methods in two few-shot learning benchmarks",
    "checked": true,
    "id": "ce854cd64758ec023d34fe129eaf02f17e0d5108",
    "semantic_title": "task weighting in meta-learning with trajectory optimisation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lNB5EHx8uC": {
    "title": "Cyclic and Randomized Stepsizes Invoke Heavier Tails in SGD than Constant Stepsize",
    "volume": "main",
    "abstract": "Cyclic and randomized stepsizes are widely used in the deep learning practice and can often outperform standard stepsize choices such as constant stepsize in SGD. Despite their empirical success, not much is currently known about when and why they can theoretically improve the generalization performance. We consider a general class of Markovian stepsizes for learning, which contain i.i.d. random stepsize, cyclic stepsize as well as the constant stepsize as special cases, and motivated by the literature which shows that heaviness of the tails (measured by the so-called ``tail-index\") in the SGD iterates is correlated with generalization, we study tail-index and provide a number of theoretical results that demonstrate how the tail-index varies on the stepsize scheduling. Our results bring a new understanding of the benefits of cyclic and randomized stepsizes compared to constant stepsize in terms of the tail behavior. We illustrate our theory on linear regression experiments and show through deep learning experiments that Markovian stepsizes can achieve even a heavier tail and be a viable alternative to cyclic and i.i.d. randomized stepsize rules",
    "checked": false,
    "id": "c734607dfd1735d9d2657f37f9f3697117252019",
    "semantic_title": "cyclic and randomized stepsizes invoke heavier tails in sgd",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2TneniEIDB": {
    "title": "A probabilistic Taylor expansion with Gaussian processes",
    "volume": "main",
    "abstract": "We study a class of Gaussian processes for which the posterior mean, for a particular choice of data, replicates a truncated Taylor expansion of any order. The data consist of derivative evaluations at the expansion point and the prior covariance kernel belongs to the class of Taylor kernels, which can be written in a certain power series form. We discuss and prove some results on maximum likelihood estimation of parameters of Taylor kernels. The proposed framework is a special case of Gaussian process regression based on data that is orthogonal in the reproducing kernel Hilbert space of the covariance kernel",
    "checked": true,
    "id": "b8f0221d2bcf1b7aa71b5ecf724d8f63ee391a02",
    "semantic_title": "a probabilistic taylor expansion with gaussian processes",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=BFvoemrmqX": {
    "title": "Bridging the Gap Between Target Networks and Functional Regularization",
    "volume": "main",
    "abstract": "Bootstrapping is behind much of the successes of deep Reinforcement Learning. However, learning the value function via bootstrapping often leads to unstable training due to fast-changing target values. Target Networks are employed to stabilize training by using an additional set of lagging parameters to estimate the target values. Despite the popularity of Target Networks, their effect on the optimization is still misunderstood. In this work, we show that they act as an implicit regularizer which can be beneficial in some cases, but also have disadvantages such as being inflexible and can result in instabilities, even when vanilla TD(0) converges. To overcome these issues, we propose an explicit Functional Regularization alternative that is flexible and a convex regularizer in function space and we theoretically study its convergence. We conducted an experimental study across a range of environments, discount factors, and off-policiness data collections to investigate the effectiveness of the regularization induced by Target Networks and Functional Regularization in terms of performance, accuracy, and stability. Our findings emphasize that Functional Regularization can be used as a drop-in replacement for Target Networks and result in performance improvement. Furthermore, adjusting both the regularization weight and the network update period in Functional Regularization can result in further performance improvements compared to solely adjusting the network update period as typically done with Target Networks. Our approach also enhances the ability to networks to recover accurate $Q$-values",
    "checked": true,
    "id": "187f444fd816f2c613f827464d9c2184288c1c3a",
    "semantic_title": "bridging the gap between target networks and functional regularization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4ofFo7D5GL": {
    "title": "HERMES: Hybrid Error-corrector Model with inclusion of External Signals for nonstationary fashion time series",
    "volume": "main",
    "abstract": "Developing models and algorithms to predict nonstationary time series is a long standing statistical problem. It is crucial for many applications, in particular for fashion or retail industries, to make optimal inventory decisions and avoid massive wastes. By tracking thousands of fashion trends on social media with state-of-the-art computer vision approaches, we propose a new model for fashion time series forecasting. Our contribution is twofold. We first provide publicly a dataset gathering 10000 weekly fashion time series. As influence dynamics are the key of emerging trend detection, we associate with each time series an external weak signal representing behaviours of influencers. Secondly, to leverage such a dataset, we propose a new hybrid forecasting mode. Our approach combines per-time-series parametric models with seasonal components and a global recurrent neural network to include sporadic external signals. This hybrid model provides state-of-the-art results on the proposed fashion dataset, on the weekly time series of the M4 competition, and illustrates the benefit of the contribution of external weak signals",
    "checked": true,
    "id": "44a57c2226a876f1bccf7362d3eb8fd39821c5e6",
    "semantic_title": "hermes: hybrid error-corrector model with inclusion of external signals for nonstationary fashion time series",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QoRo9QmOAr": {
    "title": "Detecting incidental correlation in multimodal learning via latent variable modeling",
    "volume": "main",
    "abstract": "Multimodal neural networks often fail to utilize all modalities. They subsequently generalize worse than their unimodal counterparts, or make predictions that only depend on a subset of modalities. We refer to this problem as \\emph{modality underutilization}. Existing work has addressed this issue by ensuring that there are no systematic biases in dataset creation, or that our neural network architectures and optimization algorithms are capable of learning modality interactions. We demonstrate that even when these favorable conditions are met, modality underutilization can still occur in the small data regime. To explain this phenomenon, we put forth a concept that we call \\emph{incidental correlation}. It is a spurious correlation that emerges in small datasets, despite not being a part of the underlying data generating process (DGP). We develop our argument using a DGP under which multimodal neural networks must utilize all modalities, since all paths between the inputs and target are causal. This represents an idealized scenario that often fails to materialize. Instead, due to incidental correlation, small datasets sampled from this DGP have higher likelihood under an alternative DGP with spurious paths between the inputs and target. Multimodal neural networks that use these spurious paths for prediction fail to utilize all modalities. Given its harmful effects, we propose to detect incidental correlation via latent variable modeling. We specify an identifiable variational autoencoder such that the latent posterior encodes the spurious correlations between the inputs and target. This allows us to interpret the Kullback-Leibler divergence between the latent posterior and prior as the severity of incidental correlation. We use an ablation study to show that identifiability is important in this context, since we derive our conclusions from the latent posterior. Using experiments with synthetic data, as well as with VQA v2.0 and NLVR2, we demonstrate that incidental correlation emerges in the small data regime, and leads to modality underutilization. Practitioners of multimodal learning can use our method to detect whether incidental correlation is present in their datasets, and determine whether they should collect additional data",
    "checked": true,
    "id": "2e5a62dc622f67ff6adf8ebb4aecde812b6e4b28",
    "semantic_title": "detecting incidental correlation in multimodal learning via latent variable modeling",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ry2qgRqTOw": {
    "title": "Fast Kernel Methods for Generic Lipschitz Losses via $p$-Sparsified Sketches",
    "volume": "main",
    "abstract": "Kernel methods are learning algorithms that enjoy solid theoretical foundations while suffering from important computational limitations. Sketching, which consists in looking for solutions among a subspace of reduced dimension, is a well-studied approach to alleviate these computational burdens. However, statistically-accurate sketches, such as the Gaussian one, usually contain few null entries, such that their application to kernel methods and their non-sparse Gram matrices remains slow in practice. In this paper, we show that sparsified Gaussian (and Rademacher) sketches still produce theoretically-valid approximations while allowing for important time and space savings thanks to an efficient \\emph{decomposition trick}. To support our method, we derive excess risk bounds for both single and multiple output kernel problems, with generic Lipschitz losses, hereby providing new guarantees for a wide range of applications, from robust regression to multiple quantile regression. Our theoretical results are complemented with experiments showing the empirical superiority of our approach over state-of-the-art sketching methods",
    "checked": true,
    "id": "012fd55b8ad27f1c9998b977c8286a7ea2a452c5",
    "semantic_title": "fast kernel methods for generic lipschitz losses via $p$-sparsified sketches",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=244KePn09i": {
    "title": "Single-Pass Contrastive Learning Can Work for Both Homophilic and Heterophilic Graph",
    "volume": "main",
    "abstract": "Existing graph contrastive learning (GCL) techniques typically require two forward passes for a single instance to construct the contrastive loss, which is effective for capturing the low-frequency signals of node features. Such a dual-pass design has shown empirical success on homophilic graphs, but its effectiveness on heterophilic graphs, where directly connected nodes typically have different labels, is unknown. In addition, existing GCL approaches fail to provide strong performance guarantees. Coupled with the unpredictability of GCL approaches on heterophilic graphs, their applicability in real-world contexts is limited. Then, a natural question arises: Can we design a GCL method that works for both homophilic and heterophilic graphs with a performance guarantee? To answer this question, we theoretically study the concentration property of features obtained by neighborhood aggregation on homophilic and heterophilic graphs, introduce the single-pass graph contrastive learning loss based on the property, and provide performance guarantees for the minimizer of the loss on downstream tasks. As a direct consequence of our analysis, we implement the Single-Pass Graph Contrastive Learning method (SP-GCL). Empirically, on 14 benchmark datasets with varying degrees of homophily, the features learned by the SP-GCL can match or outperform existing strong baselines with significantly less computational overhead, which demonstrates the usefulness of our findings in real-world cases",
    "checked": true,
    "id": "29bb209777eec7d36111783c9428a213443e711b",
    "semantic_title": "single-pass contrastive learning can work for both homophilic and heterophilic graph",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=djN3TaqbdA": {
    "title": "Variational Elliptical Processes",
    "volume": "main",
    "abstract": "We present elliptical processes—a family of non-parametric probabilistic models that subsumes Gaussian processes and Student's t processes. This generalization includes a range of new heavy-tailed behaviors while retaining computational tractability. Elliptical processes are based on a representation of elliptical distributions as a continuous mixture of Gaussian distributions. We parameterize this mixture distribution as a spline normalizing flow, which we train using variational inference. The proposed form of the variational posterior enables a sparse variational elliptical process applicable to large-scale problems. We highlight advantages compared to Gaussian processes through regression and classification experiments. Elliptical processes can supersede Gaussian processes in several settings, including cases where the likelihood is non-Gaussian or when accurate tail modeling is essential",
    "checked": true,
    "id": "5724cd95dd02b0fde4a0a49f6b00cbcee7f4b151",
    "semantic_title": "variational elliptical processes",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PRrKOaDQtQ": {
    "title": "Mitigating Confirmation Bias in Semi-supervised Learning via Efficient Bayesian Model Averaging",
    "volume": "main",
    "abstract": "State-of-the-art (SOTA) semi-supervised learning (SSL) methods have been highly successful in leveraging a mix of labeled and unlabeled data, often via self-training or pseudo-labeling. During pseudo-labeling, the model's predictions on unlabeled data are used for training and may result in confirmation bias where the model reinforces its own mistakes. In this work, we show that SOTA SSL methods often suffer from confirmation bias and demonstrate that this is often a result of using a poorly calibrated classifier for pseudo labeling. We introduce BaM-SSL, an efficient Bayesian Model averaging technique that improves uncertainty quantification in SSL methods with limited computational or memory overhead. We demonstrate that BaM-SSL mitigates confirmation bias in SOTA SSL methods across standard vision benchmarks of CIFAR-10, CIFAR-100, giving up to 16% improvement in test accuracy on the CIFAR-100 with 400 labels benchmark. Furthermore, we also demonstrate their effectiveness in additional realistic and challenging problems, such as class-imbalanced datasets and in photonics science",
    "checked": true,
    "id": "7a8280a7cc11463dd7f98683018f328b64041a99",
    "semantic_title": "mitigating confirmation bias in semi-supervised learning via efficient bayesian model averaging",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l4Jcxs0fpC": {
    "title": "Individual Privacy Accounting for Differentially Private Stochastic Gradient Descent",
    "volume": "main",
    "abstract": "Differentially private stochastic gradient descent (DP-SGD) is the workhorse algorithm for recent advances in private deep learning. It provides a single privacy guarantee to all datapoints in the dataset. We propose \\emph{output-specific} $(\\varepsilon,\\delta)$-DP to characterize privacy guarantees for individual examples when releasing models trained by DP-SGD. We also design an efficient algorithm to investigate individual privacy across a number of datasets. We find that most examples enjoy stronger privacy guarantees than the worst-case bound. We further discover that the training loss and the privacy parameter of an example are well-correlated. This implies groups that are underserved in terms of model utility simultaneously experience weaker privacy guarantees. For example, on CIFAR-10, the average $\\varepsilon$ of the class with the lowest test accuracy is 44.2\\% higher than that of the class with the highest accuracy",
    "checked": true,
    "id": "750331fa07beb042acb462283e18d05d756824e3",
    "semantic_title": "individual privacy accounting for differentially private stochastic gradient descent",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=VI2JjIfU37": {
    "title": "A DNN Optimizer that Improves over AdaBelief by Suppression of the Adaptive Stepsize Range",
    "volume": "main",
    "abstract": "We make contributions towards improving adaptive-optimizer performance. Our improvements are based on suppression of the range of adaptive stepsizes in the AdaBelief optimizer. Firstly, we show that the particular placement of the parameter $\\epsilon$ within the update expressions of AdaBelief reduces the range of the adaptive stepsizes, making AdaBelief closer to SGD with momentum. Secondly, we extend AdaBelief by further suppressing the range of the adaptive stepsizes. To achieve the above goal, we perform mutual layerwise vector projections between the gradient $\\boldsymbol{g}_t$ and its first momentum $\\boldsymbol{m}_t$ before using them to estimate the second momentum. The new optimization method is referred to as \\emph{Aida}. Thirdly, extensive experimental results show that Aida outperforms nine optimizers when training transformers and LSTMs for NLP, and VGG and ResNet for image classification over CIAF10 and CIFAR100 while matching the best performance of the nine methods when training WGAN-GP models for image generation tasks. Furthermore, Aida produces higher validation accuracies than AdaBelief for training ResNet18 over ImageNet",
    "checked": true,
    "id": "d1a029608df9d652418c776bf3dc6aa836216e7e",
    "semantic_title": "a dnn optimizer that improves over adabelief by suppression of the adaptive stepsize range",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f0FSDAy1bU": {
    "title": "Faster Training of Neural ODEs Using Gauß–Legendre Quadrature",
    "volume": "main",
    "abstract": "Neural ODEs demonstrate strong performance in generative and time-series modelling. However, training them via the adjoint method is slow compared to discrete models due to the requirement of numerically solving ODEs. To speed neural ODEs up, a common approach is to regularise the solutions. However, this approach may affect the expressivity of the model; when the trajectory itself matters, this is particularly important. In this paper, we propose an alternative way to speed up the training of neural ODEs. The key idea is to speed up the adjoint method by using Gauß-Legendre quadrature to solve integrals faster than ODE-based methods while remaining memory efficient. We also extend the idea to training SDEs using the Wong-Zakai theorem, by training a corresponding ODE and transferring the parameters. Our approach leads to faster training of neural ODEs, especially for large models. It also presents a new way to train SDE-based models",
    "checked": false,
    "id": "dde67a6ab628d08f693376a3e7e3332f1795a990",
    "semantic_title": "faster training of neural odes using gauß-legendre quadrature",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lAQQx7hlku": {
    "title": "Bridging the Sim2Real gap with CARE: Supervised Detection Adaptation with Conditional Alignment and Reweighting",
    "volume": "main",
    "abstract": "Sim2Real domain adaptation (DA) research focuses on the constrained setting of adapting from a labeled synthetic source domain to an unlabeled or sparsely labeled real target domain. However, for high-stakes applications (e.g. autonomous driving), it is common to have a modest amount of human-labeled real data in addition to plentiful auto-labeled source data (e.g. from a driving simulator). We study this setting of supervised sim2real DA applied to 2D object detection. We propose Domain Translation via Conditional Alignment and Reweighting (CARE) a novel algorithm that systematically exploits target labels to explicitly close the sim2real appearance and content gaps. We present an analytical justification of our algorithm and demonstrate strong gains over competing methods on standard benchmarks",
    "checked": true,
    "id": "4bfb08bce1f31cd20ee2c3e66ff8078f6501e4fe",
    "semantic_title": "bridging the sim2real gap with care: supervised detection adaptation with conditional alignment and reweighting",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=obB415rg8q": {
    "title": "Efficient Inference With Model Cascades",
    "volume": "main",
    "abstract": "State-of-the-art deep learning models are becoming ever larger. However, many practical applications are constrained by the cost of inference. Cascades of pretrained models with conditional execution address these requirements based on the intuition that some inputs are easy enough that they can be processed correctly by a smaller model allowing for an early exit. If the smaller model is not sufficiently confident in its prediction, the input is passed on to a larger model. The selection of the confidence threshold allows to trade off computational cost against accuracy. In this work we explore the effective design of model cascades, thoroughly evaluate the impact on the accuracy-efficiency trade-off, and provide a reproducible state-of-the-art baseline that is currently missing for related research. We demonstrate that model cascades dominate the ImageNet Pareto front already with 2-model cascades, achieving an average reduction in compute effort at equal accuracy of almost 3.1x above 86% and more than 1.9x between 80% and 86% top-1 accuracy, while 3-model cascades achieve 4.4x above 87% accuracy. We confirm wider applicability and effectiveness of the method on the GLUE benchmark. We release the code to reproduce our experiments in the supplementary material and use only publicly available pretrained models and datasets",
    "checked": true,
    "id": "be6e3a6dcd75f9f8c8ceb3f8ea97eb409aa14cb2",
    "semantic_title": "efficient inference with model cascades",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=EWPA9TZcUy": {
    "title": "Semantic Representations of Mathematical Expressions in a Continuous Vector Space",
    "volume": "main",
    "abstract": "Mathematical notation makes up a large portion of STEM literature, yet finding semantic representations for formulae remains a challenging problem. Because mathematical notation is precise, and its meaning changes significantly with small character shifts, the methods that work for natural text do not necessarily work well for mathematical expressions. This work describes an approach for representing mathematical expressions in a continuous vector space. We use the encoder of a sequence-to-sequence architecture, trained on visually different but mathematically equivalent expressions, to generate vector representations (or embeddings). We compare this approach with a structural approach that considers visual layout to embed an expression and show that our proposed approach is better at capturing mathematical semantics. Finally, to expedite future research, we publish a corpus of equivalent transcendental and algebraic expression pairs",
    "checked": true,
    "id": "ab080117bfdd1cb2ab7d913b45a904f78ee542be",
    "semantic_title": "semantic representations of mathematical expressions in a continuous vector space",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oFC2LAqS6Z": {
    "title": "Representations and Computations in Transformers that Support Generalization on Structured Tasks",
    "volume": "main",
    "abstract": "Transformers have shown remarkable success in natural language processing and computer vision, serving as the foundation of large language and multimodal models. These networks can capture nuanced context sensitivity across high-dimensional language tokens or image pixels, but it remains unclear how highly structured behavior and systematic generalization can arise in these systems. Here, we explore the solution process a causal transformer discovers as it learns to solve a set of algorithmic tasks involving copying, sorting, and hierarchical compositions of these operations. We search for the minimal layer and head configuration sufficient to solve these tasks and unpack the roles of the attention heads, as well as how token representations are reweighted across layers to complement these roles. Our results provide new insights into how attention layers in transformers support structured computation within and across tasks: 1) Replacing fixed position labels with labels sampled from a larger set enables strong length generalization and faster learning. The learnable embeddings of these labels develop different representations, capturing sequence order if necessary, depending on task demand. 2) Two-layer transformers can learn reliable solutions to the multi-level problems we explore. The first layer tends to transform the input representation to allow the second layer to share computation across repeated components within a task or across related tasks. 3) We introduce an analysis pipeline that quantifies how the representation space in a given layer prioritizes different aspects of each item. We show that these representations prioritize information needed to guide attention relative to information that only requires downstream readout",
    "checked": true,
    "id": "79dd9e62f0e3ba35ce54d98b32750933e91a972a",
    "semantic_title": "representations and computations in transformers that support generalization on structured tasks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tv46tCzs83": {
    "title": "Causal Parrots: Large Language Models May Talk Causality But Are Not Causal",
    "volume": "main",
    "abstract": "Some argue scale is all what is needed to achieve AI, covering even causal models. We make it clear that large language models (LLMs) cannot be causal and give reason onto why sometimes we might feel otherwise. To this end, we define and exemplify a new subgroup of Structural Causal Model (SCM) that we call meta SCM which encode causal facts about other SCM within their variables. We conjecture that in the cases where LLM succeed in doing causal inference, underlying was a respective meta SCM that exposed correlations between causal facts in natural language on whose data the LLM was ultimately trained. If our hypothesis holds true, then this would imply that LLMs are like parrots in that they simply recite the causal knowledge embedded in the data. Our empirical analysis provides favoring evidence that current LLMs are even weak `causal parrots",
    "checked": true,
    "id": "b9672ac98913c43fcb996b3def314789d1cc0cf4",
    "semantic_title": "causal parrots: large language models may talk causality but are not causal",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=VP9p4u9jAo": {
    "title": "An Option-Dependent Analysis of Regret Minimization Algorithms in Finite-Horizon Semi-MDP",
    "volume": "main",
    "abstract": "A large variety of real-world Reinforcement Learning (RL) tasks is characterized by a complex and heterogeneous structure that makes end-to-end (or flat) approaches hardly applicable or even infeasible. Hierarchical Reinforcement Learning (HRL) provides general solutions to address these problems thanks to a convenient multi-level decomposition of the tasks, making their solution accessible. Although often used in practice, few works provide theoretical guarantees to justify this outcome effectively. Thus, it is not yet clear when to prefer such approaches compared to standard flat ones. In this work, we provide an option-dependent upper bound to the regret suffered by regret minimization algorithms in finite-horizon problems. We illustrate that the performance improvement derives from the planning horizon reduction induced by the temporal abstraction enforced by the hierarchical structure. Then, focusing on a sub-setting of HRL approaches, the options framework, we highlight how the average duration of the available options affects the planning horizon and, consequently, the regret itself. Finally, we relax the assumption of having pre-trained options to show how, in particular situations, is still preferable a hierarchical approach over a standard one",
    "checked": false,
    "id": "3e45d02b763cb8197369ac0b3ef4a16f4726de85",
    "semantic_title": "an option-dependent analysis of regret minimization algorithms in finite-horizon semi-markov decision processes",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hf95zFnQ7H": {
    "title": "On Adaptivity in Quantum Testing",
    "volume": "main",
    "abstract": "Can adaptive strategies outperform non-adaptive ones for quantum hypothesis selection? We exhibit problems where adaptive strategies provably reduce the number of required samples by a factor four in the worst case, and possibly more when the actual difficulty of the problem makes it possible. In addition, we exhibit specific hypotheses classes for which there is a provable polynomial separation between adaptive and non-adaptive strategies -- a specificity of the quantum framework that does not appear in classical testing",
    "checked": true,
    "id": "329438fe111495e37f45b91641504abee0f05916",
    "semantic_title": "on adaptivity in quantum testing",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=d4Vr6E0jjm": {
    "title": "Teaching Smaller Language Models To Generalise To Unseen Compositional Questions",
    "volume": "main",
    "abstract": "We equip a smaller Language Model to generalise to answering challenging compositional questions that have not been seen in training. To do so we propose a combination of multitask supervised pretraining on up to 93 tasks designed to instill diverse reasoning abilities, and a dense retrieval system that aims to retrieve a set of evidential paragraph fragments. Recent progress in question-answering has been achieved either through prompting methods against very large pretrained Language Models in zero or few-shot fashion, or by fine-tuning smaller models, sometimes in conjunction with information retrieval. We focus on the less explored question of the extent to which zero-shot generalisation can be enabled in smaller models with retrieval against a corpus within which sufficient information to answer a particular question may not exist. We establish strong baselines in this setting for diverse evaluation datasets (StrategyQA, CommonsenseQA, IIRC, DROP, Musique and ARC-DA), and show that performance can be significantly improved by adding retrieval-augmented training datasets which are designed to expose our models to a variety of heuristic reasoning strategies such as weighing partial evidence or ignoring an irrelevant context",
    "checked": true,
    "id": "aea817017e18cc1cb34962ffd399f8a83ab7a076",
    "semantic_title": "teaching smaller language models to generalise to unseen compositional questions",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=3agxS3aDUs": {
    "title": "Subgraph Permutation Equivariant Networks",
    "volume": "main",
    "abstract": "In this work we develop a new method, named Sub-graph Permutation Equivariant Networks (SPEN), which provides a framework for building graph neural networks that operate on sub-graphs, while using a base update function that is permutation equivariant, that are equivariant to a novel choice of automorphism group. Message passing neural networks have been shown to be limited in their expressive power and recent approaches to over come this either lack scalability or require structural information to be encoded into the feature space. The general framework presented here overcomes the scalability issues associated with global permutation equivariance by operating more locally on sub-graphs. In addition, through operating on sub-graphs the expressive power of higher-dimensional global permutation equivariant networks is improved; this is due to fact that two non-distinguishable graphs often contain distinguishable sub-graphs. Furthermore, the proposed framework only requires a choice of $k$-hops for creating ego-network sub-graphs and a choice of representation space to be used for each layer, which makes the method easily applicable across a range of graph based domains. We experimentally validate the method on a range of graph benchmark classification tasks, demonstrating statistically indistinguishable results from the state-of-the-art on six out of seven benchmarks. Further, we demonstrate that the use of local update functions offers a significant improvement in GPU memory over global methods",
    "checked": true,
    "id": "d076a3b0bd4f6d02072a5cd1e97bb9d6e41c1c4d",
    "semantic_title": "subgraph permutation equivariant networks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uq29MIWvIV": {
    "title": "About the Cost of Central Privacy in Density Estimation",
    "volume": "main",
    "abstract": "We study non-parametric density estimation for densities in Lipschitz and Sobolev spaces, and under central privacy. In particular, we investigate regimes where the privacy budget is not supposed to be constant. We consider the classical definition of central differential privacy, but also the more recent notion of central concentrated differential privacy. We recover the result of Barber & Duchi (2014) stating that histogram estimators are optimal against Lipschitz distributions for the L2 risk and, under regular differential privacy, we extend it to other norms and notions of privacy. Then, we investigate higher degrees of smoothness, drawing two conclusions: First, and contrary to what happens with constant privacy budget (Wasserman & Zhou, 2010), there are regimes where imposing privacy degrades the regular minimax risk of estimation on Sobolev densities. Second, so-called projection estimators are near-optimal against the same classes of densities in this new setup with pure differential privacy, but contrary to the constant privacy budget case, it comes at the cost of relaxation. With zero concentrated differential privacy, there is no need for relaxation, and we prove that the estimation is optimal",
    "checked": true,
    "id": "9b0b28f48f828f399cda2801f1e43b6aa384a0b6",
    "semantic_title": "about the cost of central privacy in density estimation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=REtKapdkyI": {
    "title": "Some Remarks on Identifiability of Independent Component Analysis in Restricted Function Classes",
    "volume": "main",
    "abstract": "In this short note, we comment on recent results on identifiability of independent component analysis. We point out an error in earlier works and clarify that this error cannot be fixed as the chosen approach is not sufficiently powerful to prove identifiability results. In addition, we explain the necessary ingredients to prove stronger identifiability results. Finally, we discuss and extend the flow-based technique to construct spurious solutions for independent component analysis problems and provide a counterexample to an earlier identifiability result",
    "checked": true,
    "id": "b5fcb665718a6931ea908d2616fc6e24ae2fcbe9",
    "semantic_title": "some remarks on identifiability of independent component analysis in restricted function classes",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Nn71AdKyYH": {
    "title": "You Only Transfer What You Share: Intersection-Induced Graph Transfer Learning for Link Prediction",
    "volume": "main",
    "abstract": "Link prediction is central to many real-world applications, but its performance may be hampered when the graph of interest is sparse. To alleviate issues caused by sparsity, we investigate a previously overlooked phenomenon: in many cases, a densely connected, complementary graph can be found for the original graph. The denser graph may share nodes with the original graph, which offers a natural bridge for transferring selective, meaningful knowledge. We identify this setting as Graph Intersection-induced Transfer Learning (GITL), which is motivated by practical applications in e-commerce or academic co-authorship predictions. We develop a framework to effectively leverage the structural prior in this setting. We first create an intersection subgraph using the shared nodes between the two graphs, then transfer knowledge from the source-enriched intersection subgraph to the full target graph. In the second step, we consider two approaches: a modified label propagation, and a multi-layer perceptron (MLP) model in a teacher-student regime. Experimental results on proprietary e-commerce datasets and open-source citation graphs show that the proposed workflow outperforms existing transfer learning baselines that do not explicitly utilize the intersection structure",
    "checked": true,
    "id": "19cbae4b5d3a10fd5e5cc27de163a4319c9d4341",
    "semantic_title": "you only transfer what you share: intersection-induced graph transfer learning for link prediction",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7wA65zL3B3": {
    "title": "Logistic-Normal Likelihoods for Heteroscedastic Label Noise",
    "volume": "main",
    "abstract": "A natural way of estimating heteroscedastic label noise in regression is to model the observed (potentially noisy) target as a sample from a normal distribution, whose parameters can be learned by minimizing the negative log-likelihood. This formulation has desirable loss attenuation properties, as it reduces the contribution of high-error examples. Intuitively, this behavior can improve robustness against label noise by reducing overfitting. We propose an extension of this simple and probabilistic approach to classification that has the same desirable loss attenuation properties. Furthermore, we discuss and address some practical challenges of this extension. We evaluate the effectiveness of the method by measuring its robustness against label noise in classification. We perform enlightening experiments exploring the inner workings of the method, including sensitivity to hyperparameters, ablation studies, and other insightful analyses",
    "checked": false,
    "id": "f6ea00c32d30a2705db262032ae3633807f67495",
    "semantic_title": "logistic-normal likelihoods for heteroscedastic label noise in classification",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ufc5cWhHko": {
    "title": "RECLIP: Resource-efficient CLIP by Training with Small Images",
    "volume": "main",
    "abstract": "We present RECLIP (Resource-efficient CLIP), a simple method that minimizes computational resource footprint for CLIP (Contrastive Language Image Pretraining). Inspired by the notion of coarse-to-fine in computer vision, we leverage small images to learn from large-scale language supervision efficiently, and finetune the model with high-resolution data in the end. Since the complexity of the vision transformer heavily depends on input image size, our approach significantly reduces the training resource requirements both in theory and in practice. Using the same batch size and training epoch, RECLIP achieves highly competitive zero-shot classification and image-text retrieval accuracy with 6 to 8× less computational resources and 7 to 9× fewer FLOPs than the base- line. Compared to the state-of-the-art contrastive learning methods, RECLIP demonstrates 5 to 59× training resource savings while maintaining highly competitive zero-shot classification and retrieval performance. Finally, RECLIP matches the state of the art in transfer learning to open-vocabulary detection tasks, achieving 32 APr on LVIS. We hope this work will pave the path for the broader research community to explore language supervised pretraining in resource-friendly settings",
    "checked": true,
    "id": "38a71cff020621b2435ae769b0667d7a4595d0e9",
    "semantic_title": "reclip: resource-efficient clip by training with small images",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=ubCoTAynPp": {
    "title": "Reinforcement Learning with Delayed, Composite, and Partially Anonymous Reward",
    "volume": "main",
    "abstract": "We investigate an infinite-horizon average reward Markov Decision Process (MDP) with delayed, composite, and partially anonymous reward feedback. The delay and compositeness of rewards mean that rewards generated as a result of taking an action at a given state are fragmented into different components, and they are sequentially realized at delayed time instances. The partial anonymity attribute implies that a learner, for each state, only observes the aggregate of past reward components generated as a result of different actions taken at that state, but realized at the observation instance. We propose an algorithm named $\\mathrm{DUCRL2}$ to obtain a near-optimal policy for this setting and show that it achieves a regret bound of $\\tilde{\\mathcal{O}}\\left(DS\\sqrt{AT} + d (SA)^3\\right)$ where $S$ and $A$ are the sizes of the state and action spaces, respectively, $D$ is the diameter of the MDP, $d$ is a parameter upper bounded by the maximum reward delay, and $T$ denotes the time horizon. This demonstrates the optimality of the bound in the order of $T$, and an additive impact of the delay",
    "checked": true,
    "id": "384b658132b3ac7d810b4595c0afff7695935f6f",
    "semantic_title": "reinforcement learning with delayed, composite, and partially anonymous reward",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dPSTDbGtBY": {
    "title": "Towards Multi-spatiotemporal-scale Generalized PDE Modeling",
    "volume": "main",
    "abstract": "Partial differential equations (PDEs) are central to describing complex physical system simulations. Their expensive solution techniques have led to an increased interest in deep neural network based surrogates. However, the practical utility of training such surrogates is contingent on their ability to model complex multi-scale spatio-temporal phenomena. {In recent years}, various neural network architectures have been proposed to target such phenomena, most notably Fourier Neural Operators (FNOs), which give a natural handle over local \\& global spatial information via parameterization of different Fourier modes, and U-Nets which treat local and global information via downsampling and upsampling paths. However, large-scale comparisons between these convolution-based approaches are notoriously sparse. In this work, we make such comprehensive comparisons regarding performance, runtime complexity, memory requirements, and generalization capabilities. Concretely, we stress-test various FNO, (Dilated) ResNet, and U-Net like approaches to fluid mechanics problems in both vorticity-stream and velocity function form. For U-Nets, we transfer recent architectural improvements from computer vision, most notably from object segmentation and generative modeling. Next, we use our insights on design considerations, and introduce U-FNets, i.e., modern U-Nets that are augmented with FNO downsampling layers. Those architectures further improve performance without major degradation of computational cost. Finally, we ablate and discuss various choices for parameter conditioning}, and show promising results on generalization to different PDE parameters and time-scales with a single surrogate model. Source code for our PyTorch benchmark framework is available at https://anonymous.4open.science/r/tmlr-pdemulti-6677/",
    "checked": true,
    "id": "00ffcc997b0bcf0dbf60ff04c29d701919582a62",
    "semantic_title": "towards multi-spatiotemporal-scale generalized pde modeling",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=z49eaB8kiH": {
    "title": "The Multiquadric Kernel for Moment-Matching Distributional Reinforcement Learning",
    "volume": "main",
    "abstract": "Distributional reinforcement learning has gained significant attention in recent years due to its ability to handle uncertainty and variability in the returns an agent will receive for each action it takes. A key challenge in distributional reinforcement learning is finding a measure of the difference between two distributions that is well-suited for use with the distributional Bellman operator, a function that takes in a value distribution and produces a modified distribution based on the agent's current state and action. In this paper, we address this challenge by introducing the multiquadric kernel to moment-matching distributional reinforcement learning. We show that this kernel is both theoretically sound and empirically effective. Our contribution is mainly of a theoretical nature, presenting the first formally sound kernel for moment-matching distributional reinforcement learning with good practical performance. We also provide insights into why the RBF kernel has been shown to provide good practical results despite its theoretical problems. Finally, we evaluate the performance of our kernel on a number of standard benchmarks, obtaining results comparable to the state-of-the-art",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EDVIHPZhFo": {
    "title": "Nonconvex-nonconcave min-max optimization on Riemannian manifolds",
    "volume": "main",
    "abstract": "This work studies nonconvex-nonconcave min-max problems on Riemannian manifolds. We first characterize the local optimality of nonconvex-nonconcave problems on manifolds with a generalized notion of local minimax points. We then define the stability and convergence criteria of dynamical systems on manifolds and provide necessary and sufficient conditions of strictly stable equilibrium points for both continuous and discrete dynamics. Additionally, we propose several novel second-order methods on manifolds that provably converge to local minimax points asymptotically. We validate the empirical benefits of the proposed methods with extensive experiments",
    "checked": true,
    "id": "1ad0256996ab79716fbeee7fbd621b340aae6032",
    "semantic_title": "nonconvex-nonconcave min-max optimization on riemannian manifolds",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=moZvOx5cxe": {
    "title": "Learning to Boost Resilience of Complex Networks via Neural Edge Rewiring",
    "volume": "main",
    "abstract": "The resilience of complex networks refers to their ability to maintain functionality in the face of structural attacks. This ability can be improved by performing minimal modifications to the network structure via degree-preserving edge rewiring-based methods. Existing learning-free edge rewiring methods, although effective, are limited in their ability to generalize to different graphs. Such a limitation cannot be trivially addressed by existing graph neural networks (GNNs)-based learning approaches since there is no rich initial node features for GNNs to learn meaningful representations. In this work, inspired by persistent homology, we specifically design a variant of GNN called FireGNN to learn meaningful node representations solely from graph structures. We then develop an end-to-end inductive method called ResiNet, which aims to discover resilient network topologies while balancing network utility. ResiNet reformulates the optimization of network resilience as a Markov decision process equipped with edge rewiring action space. It learns to sequentially select the appropriate edges to rewire for maximizing resilience. Extensive experiments demonstrate that ResiNet outperforms existing approaches and achieves near-optimal resilience gains on various graphs while balancing network utility",
    "checked": true,
    "id": "320c729de74825d9e5cb8b7db3269e38b192c9d0",
    "semantic_title": "learning to boost resilience of complex networks via neural edge rewiring",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y8RZoPjEUl": {
    "title": "Simulate Time-integrated Coarse-grained Molecular Dynamics with Multi-scale Graph Networks",
    "volume": "main",
    "abstract": "Molecular dynamics (MD) simulation is essential for various scientific domains but computationally expensive. Learning-based force fields have made significant progress in accelerating ab-initio MD simulation but are not fast enough for many real-world applications due to slow inference for large systems and small time steps (femtosecond-level). We aim to address these challenges by learning a multi-scale graph neural network that directly simulates coarse-grained MD with a very large time step (nanosecond-level) and a novel refinement module based on diffusion models to mitigate simulation instability. The effectiveness of our method is demonstrated in two complex systems: single-chain coarse-grained polymers and multi-component Li-ion polymer electrolytes. For evaluation, we simulate trajectories much longer than the training trajectories for systems with different chemical compositions that the model is not trained on. Structural and dynamical properties can be accurately recovered at several orders of magnitude higher speed than classical force fields by getting out of the femtosecond regime",
    "checked": true,
    "id": "3333f169bd4815f92c03c9445761d8267c4e9ef2",
    "semantic_title": "simulate time-integrated coarse-grained molecular dynamics with multi-scale graph networks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R2hUure38l": {
    "title": "Meta-Calibration: Learning of Model Calibration Using Differentiable Expected Calibration Error",
    "volume": "main",
    "abstract": "Calibration of neural networks is a topical problem that is becoming more and more important as neural networks increasingly underpin real-world applications. The problem is especially noticeable when using modern neural networks, for which there is a significant difference between the confidence of the model and the probability of correct prediction. Various strategies have been proposed to improve calibration, yet accurate calibration remains challenging. We propose a novel framework with two contributions: introducing a new differentiable surrogate for expected calibration error (DECE) that allows calibration quality to be directly optimised, and a meta-learning framework that uses DECE to optimise for validation set calibration with respect to model hyper-parameters. The results show that we achieve competitive performance with existing calibration approaches. Our framework opens up a new avenue and toolset for tackling calibration, which we believe will inspire further work on this important challenge",
    "checked": true,
    "id": "bc200b53c46c38dcadfa22ea14645e899c73e99f",
    "semantic_title": "meta-calibration: learning of model calibration using differentiable expected calibration error",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=v5ew3FPTgb": {
    "title": "Understanding convolution on graphs via energies",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) typically operate by message-passing, where the state of a node is updated based on the information received from its neighbours. Most message-passing models act as graph convolutions, where features are mixed by a shared, linear transformation before being propagated over the edges. On node-classification tasks, graph convolutions have been shown to suffer from two limitations: poor performance on heterophilic graphs, and over-smoothing. It is common belief that both phenomena occur because such models behave as low-pass filters, meaning that the Dirichlet energy of the features decreases along the layers incurring a smoothing effect that ultimately makes features no longer distinguishable. In this work, we rigorously prove that simple graph-convolutional models can actually enhance high frequencies and even lead to an asymptotic behaviour we refer to as over-sharpening, opposite to over-smoothing. We do so by showing that linear graph convolutions with symmetric weights minimize a multi-particle energy that generalizes the Dirichlet energy; in this setting, the weight matrices induce edge-wise attraction (repulsion) through their positive (negative) eigenvalues, thereby controlling whether the features are being smoothed or sharpened. We also extend the analysis to non-linear GNNs, and demonstrate that some existing time-continuous GNNs are instead always dominated by the low frequencies. Finally, we validate our theoretical findings through ablations and real-world experiments",
    "checked": true,
    "id": "826d4aa5047f3a9074a11e7ea39e587d7e2d0759",
    "semantic_title": "understanding convolution on graphs via energies",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=ZPMf53vE1L": {
    "title": "One-Step Distributional Reinforcement Learning",
    "volume": "main",
    "abstract": "Reinforcement learning (RL) allows an agent interacting sequentially with an environment to maximize its long-term expected return. In the distributional RL (DistrRL) paradigm, the agent goes beyond the limit of the expected value, to capture the underlying probability distribution of the return across all time steps. The set of DistrRL algorithms has led to improved empirical performance. Nevertheless, the theory of DistrRL is still not fully understood, especially in the control case. In this paper, we present the simpler one-step distributional reinforcement learning (OS-DistrRL) framework encompassing only the randomness induced by the one-step dynamics of the environment. Contrary to DistrRL, we show that our approach comes with a unified theory for both policy evaluation and control. Indeed, we propose two OS-DistrRL algorithms for which we provide an almost sure convergence analysis. The proposed approach compares favorably with categorical DistrRL on various environments",
    "checked": true,
    "id": "40ebcd12c830d411c31b6b6014a43635866807be",
    "semantic_title": "one-step distributional reinforcement learning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=PHAr3q49h6": {
    "title": "Dual Representation Learning for Out-of-distribution Detection",
    "volume": "main",
    "abstract": "To classify in-distribution samples, deep neural networks explore strongly label-related information and discard weakly label-related information according to the information bottleneck. Out-of-distribution samples drawn from distributions differing from that of in-distribution samples could be assigned with unexpected high-confidence predictions because they could obtain minimum strongly label-related information. To distinguish in- and out-of-distribution samples, Dual Representation Learning (DRL) makes out-of-distribution samples harder to have high-confidence predictions by exploring both strongly and weakly label-related information from in-distribution samples. For a pretrained network exploring strongly label-related information to learn label-discriminative representations, DRL trains its auxiliary network exploring the remaining weakly label-related information to learn distribution-discriminative representations. Specifically, for a label-discriminative representation, DRL constructs its complementary distribution-discriminative representation by integrating diverse representations less similar to the label-discriminative representation. Accordingly, DRL combines label- and distribution-discriminative representations to detect out-of-distribution samples. Experiments show that DRL outperforms the state-of-the-art methods for out-of-distribution detection",
    "checked": true,
    "id": "585e3fbf75e1cd3df556f12a1284dc280c782ace",
    "semantic_title": "dual representation learning for out-of-distribution detection",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=83rgSFPpws": {
    "title": "Cyclophobic Reinforcement Learning",
    "volume": "main",
    "abstract": "In environments with sparse rewards, finding a good inductive bias for exploration is crucial to the agent's success. However, there are two competing goals: novelty search and systematic exploration. While existing approaches such as curiosity-driven exploration find novelty, they sometimes do not systematically explore the whole state space, akin to depth-first-search vs breadth-first-search. In this paper, we propose a new intrinsic reward that is cyclophobic, i.e., it does not reward novelty, but punishes redundancy by avoiding cycles. Augmenting the cyclophobic intrinsic reward with a sequence of hierarchical representations based on the agent's cropped observations we are able to achieve excellent results in the MiniGrid and MiniHack environments. Both are particularly hard, as they require complex interactions with different objects in order to be solved. Detailed comparisons with previous approaches and thorough ablation studies show that our newly proposed cyclophobic reinforcement learning is more sample efficient than other state of the art methods in a variety of tasks",
    "checked": true,
    "id": "ccca35498d185ee5b91a6ed4f82861f8bbd59229",
    "semantic_title": "cyclophobic reinforcement learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nYzhlFyjjd": {
    "title": "Rotation-Invariant Random Features Provide a Strong Baseline for Machine Learning on 3D Point Clouds",
    "volume": "main",
    "abstract": "Rotational invariance is a popular inductive bias used by many fields in machine learning, such as computer vision and machine learning for quantum chemistry. Rotation-invariant machine learning methods set the state of the art for many tasks, including molecular property prediction and 3D shape classification. These methods generally either rely on task-specific rotation-invariant features, or they use general-purpose deep neural networks which are complicated to design and train. However, it is unclear whether the success of these methods is primarily due to the rotation invariance or the deep neural networks. To address this question, we suggest a simple and general-purpose method for learning rotation-invariant functions of three-dimensional point cloud data using a random features approach. Specifically, we extend the random features method of Rahimi & Recht (2007) by deriving a version that is invariant to three-dimensional rotations and showing that it is fast to evaluate on point cloud data. We show through experiments that our method matches or outperforms the performance of general-purpose rotation-invariant neural networks on standard molecular property prediction benchmark datasets QM7 and QM9. We also show that our method is general-purpose and provides a rotation-invariant baseline on the ModelNet40 shape classification task. Finally, we show that our method has an order of magnitude smaller prediction latency than competing kernel methods",
    "checked": true,
    "id": "631836cec708152538719af313bf9c1a261f3ec5",
    "semantic_title": "rotation-invariant random features provide a strong baseline for machine learning on 3d point clouds",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pHCdMat0gI": {
    "title": "Graph Neural Networks for Temporal Graphs: State of the Art, Open Challenges, and Opportunities",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) have become the leading paradigm for learning on (static) graph-structured data. However, many real-world systems are dynamic in nature, since the graph and node/edge attributes change over time. In recent years, GNN-based models for temporal graphs have emerged as a promising area of research to extend the capabilities of GNNs. In this work, we provide the first comprehensive overview of the current state-of-the-art of temporal GNN, introducing a rigorous formalization of learning settings and tasks and a novel taxonomy categorizing existing approaches in terms of how the temporal aspect is represented and processed. We conclude the survey with a discussion of the most relevant open challenges for the field, from both research and application perspectives",
    "checked": true,
    "id": "b88f456daaf29860d2b59c621be3bd878a581a59",
    "semantic_title": "graph neural networks for temporal graphs: state of the art, open challenges, and opportunities",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=AXUtAIX0Fn": {
    "title": "A Systematic Approach to Universal Random Features in Graph Neural Networks",
    "volume": "main",
    "abstract": "Universal random features (URF) are state of the art regarding practical graph neural networks that are provably universal. There is great diversity regarding terminology, methodology, benchmarks, and evaluation metrics used among existing URF. Not only does this make it increasingly difficult for practitioners to decide which technique to apply to a given problem, but it also stands in the way of systematic improvements. We propose a new comprehensive framework that captures all previous URF techniques. On the theoretical side, among other results, we formally prove that under natural conditions all instantiations of our framework are universal. The framework thus provides a new simple technique to prove universality results. On the practical side, we develop a method to systematically and automatically train URF. This in turn enables us to impartially and objectively compare all existing URF. New URF naturally emerge from our approach, and our experiments demonstrate that they improve the state of the art",
    "checked": true,
    "id": "0de5aad760c69057e7b1e4cbfe8ec06eae8fb802",
    "semantic_title": "a systematic approach to universal random features in graph neural networks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0f8tU3QwWD": {
    "title": "FairGrad: Fairness Aware Gradient Descent",
    "volume": "main",
    "abstract": "We address the problem of group fairness in classification, where the objective is to learn models that do not unjustly discriminate against subgroups of the population. Most existing approaches are limited to simple binary tasks or involve difficult to implement training mechanisms which reduces their practical applicability. In this paper, we propose FairGrad, a method to enforce fairness based on a re-weighting scheme that iteratively learns group specific weights based on whether they are advantaged or not. FairGrad is easy to implement, accommodates various standard fairness definitions, and comes with minimal overhead. Furthermore, we show that it is competitive with standard baselines over various datasets including ones used in natural language processing and computer vision. FairGrad is available as a PyPI package at - https://pypi.org/project/fairgrad",
    "checked": true,
    "id": "7118e545c3786e0f9a87b25214f1c7d930e068bb",
    "semantic_title": "fairgrad: fairness aware gradient descent",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=qHZs2p4ZD4": {
    "title": "V1T: large-scale mouse V1 response prediction using a Vision Transformer",
    "volume": "main",
    "abstract": "Accurate predictive models of the visual cortex neural response to natural visual stimuli remain a challenge in computational neuroscience. In this work, we introduce $V{\\small 1}T$, a novel Vision Transformer based architecture that learns a shared visual and behavioral representation across animals. We evaluate our model on two large datasets recorded from mouse primary visual cortex and outperform previous convolution-based models by more than 12.7% in prediction performance. Moreover, we show that the self-attention weights learned by the Transformer correlate with the population receptive fields. Our model thus sets a new benchmark for neural response prediction and can be used jointly with behavioral and neural recordings to reveal meaningful characteristic features of the visual cortex",
    "checked": true,
    "id": "370717d2027ae6b8762d8dad518b0b8d706d706a",
    "semantic_title": "v1t: large-scale mouse v1 response prediction using a vision transformer",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=ey5b7kODvK": {
    "title": "Novel Class Discovery for Long-tailed Recognition",
    "volume": "main",
    "abstract": "While the novel class discovery has recently made great progress, existing methods typically focus on improving algorithms on class-balanced benchmarks. However, in real-world recognition tasks, the class distributions of their corresponding datasets are often imbalanced, which leads to serious performance degeneration of those methods. In this paper, we consider a more realistic setting for novel class discovery where the distributions of novel and known classes are long-tailed. One main challenge of this new problem is to discover imbalanced novel classes with the help of long-tailed known classes. To tackle this problem, we propose an adaptive self-labeling strategy based on an equiangular prototype representation of classes. Our method infers high-quality pseudo-labels for the novel classes by solving a relaxed optimal transport problem and effectively mitigates the class biases in learning the known and novel classes. We perform extensive experiments on CIFAR100, ImageNet100, Herbarium19 and large-scale iNaturalist18 datasets, and the results demonstrate the superiority of our method. Our code is available at \\url{https://github.com/kleinzcy/NCDLR}",
    "checked": true,
    "id": "bedf5fa16ae127f599d2e1f562b96f7a12ef7f13",
    "semantic_title": "novel class discovery for long-tailed recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U4XgzRjfF1": {
    "title": "Asymptotic Analysis of Conditioned Stochastic Gradient Descent",
    "volume": "main",
    "abstract": "In this paper, we investigate a general class of stochastic gradient descent (SGD) algorithms, called $\\textit{conditioned}$ SGD, based on a preconditioning of the gradient direction. Using a discrete-time approach with martingale tools, we establish under mild assumptions the weak convergence of the rescaled sequence of iterates for a broad class of conditioning matrices including stochastic first-order and second-order methods. Almost sure convergence results, which may be of independent interest, are also presented. Interestingly, the asymptotic normality result consists in a stochastic equicontinuity property so when the conditioning matrix is an estimate of the inverse Hessian, the algorithm is asymptotically optimal",
    "checked": false,
    "id": "c7b411f10d2d6bbcc4824177fa9bd9942292f61d",
    "semantic_title": "non-asymptotic analysis of online multiplicative stochastic gradient descent",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WYKTCKpImz": {
    "title": "Learned Thresholds Token Merging and Pruning for Vision Transformers",
    "volume": "main",
    "abstract": "Vision transformers have demonstrated remarkable success in a wide range of computer vision tasks over the last years, however, their high computational costs remains a significant barrier to their practical deployment. In particular, the complexity of transformer models is quadratic with respect to the number of input tokens. Therefore techniques that reduce the number of input tokens that need to be processed have been proposed. This paper introduces Learned Thresholds token Merging and Pruning (LTMP), a novel approach that leverages the strengths of both token merging and token pruning. LTMP uses learned threshold masking modules that dynamically determine which tokens to merge and which to prune. We demonstrate our approach with extensive experiments on vision transformers on the ImageNet classification task. Our results demonstrate that LTMP achieves state-of-the-art accuracy across reduction rates while requiring only a single fine-tuning epoch, which is an order of magnitude faster than previous methods",
    "checked": true,
    "id": "80754642beb97b91167efd16b2a265024213857c",
    "semantic_title": "learned thresholds token merging and pruning for vision transformers",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FqOG4osY7C": {
    "title": "MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks",
    "volume": "main",
    "abstract": "The development of language models have moved from encoder-decoder to decoder-only designs. In addition, we observe that the two most popular multimodal tasks, the generative and contrastive tasks, are nontrivial to accommodate in one architecture, and further need adaptations for downstream tasks. We propose a novel paradigm of training with a decoder-only model for multimodal tasks, which is surprisingly effective in jointly learning of these disparate vision-language tasks. This is done with a simple model, called MaMMUT. It consists of a single vision encoder and a text decoder, and is able to accommodate contrastive and generative learning by a novel two-pass approach on the text decoder. We demonstrate that joint learning of these diverse objectives is simple, effective, and maximizes the weight-sharing of the model across these tasks. Furthermore, the same architecture enables straightforward extensions to open-vocabulary object detection and video-language tasks. The model tackles a diverse range of tasks, while being modest in capacity. Our model achieves the state of the art on image-text and text-image retrieval, video question answering and open-vocabulary detection tasks, outperforming much larger and more extensively trained foundational models. It shows very competitive results on VQA and Video Captioning, especially considering its capacity. Ablations confirm the flexibility and advantages of our approach",
    "checked": true,
    "id": "26eb6745006e94317cfb634123fe3015702fb224",
    "semantic_title": "mammut: a simple architecture for joint learning for multimodal tasks",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=EwJJks2cSa": {
    "title": "Chasing Better Deep Image Priors between Over- and Under-parameterization",
    "volume": "main",
    "abstract": "Deep Neural Networks (DNNs) are well-known to act as \\textbf{over-parameterized} deep image priors (DIP) that regularize various image inverse problems. Meanwhile, researchers also proposed extremely compact, \\textbf{under-parameterized} image priors (e.g., deep decoder) that are strikingly competent for image restoration too, despite a loss of accuracy. These two extremes push us to think whether there exists a better solution in the middle: \\textit{between over- and under-parameterized image priors, can one identify ``intermediate\" parameterized image priors that achieve better trade-offs between performance, efficiency, and even preserving strong transferability?} Drawing inspirations from the lottery ticket hypothesis (LTH), we conjecture and study a novel ``lottery image prior\" (\\textbf{LIP}) by exploiting DNN inherent sparsity, stated as: \\textit{given an over-parameterized DNN-based image prior, it will contain a sparse subnetwork that can be trained in isolation, to match the original DNN's performance when being applied as a prior to various image inverse problems}. Our results validate the superiority of LIPs: we can successfully locate the LIP subnetworks from over-parameterized DIPs at substantial sparsity ranges. Those LIP subnetworks significantly outperform deep decoders under comparably compact model sizes (by often fully preserving the effectiveness of their over-parameterized counterparts), and they also possess high transferability across different images as well as restoration task types. Besides, we also extend LIP to compressive sensing image reconstruction, where a \\textit{pre-trained} GAN generator is used as the prior (in contrast to \\textit{untrained} DIP or deep decoder), and confirm its validity in this setting too. To our best knowledge, this is the first time that LTH is demonstrated to be relevant in the context of inverse problems or image priors. Codes are available at https://github.com/VITA-Group/Chasing-Better-DIPs",
    "checked": false,
    "id": "e747dce851ce8f302814eaf1d65ac99aa82963ff",
    "semantic_title": "chasing better deep image priors between over-and under-parameterization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TjaMO63fc9": {
    "title": "Federated High-Dimensional Online Decision Making",
    "volume": "main",
    "abstract": "We resolve the main challenge of federated bandit policy design via exploration-exploitation trade-off delineation under data decentralization with a local privacy protection argument. Such a challenge is practical in domain-specific applications and admits another layer of complexity in applications of medical decision-making and web marketing, where high- dimensional decision contexts are sensitive but important to inform decision-making. Exist- ing (low dimensional) federated bandits suffer super-linear theoretical regret upper bound in high-dimensional scenarios and are at risk of client information leakage due to their in- ability to separate exploration from exploitation. This paper proposes a class of bandit policy design, termed Fedego Lasso, to complete the task of federated high-dimensional online decision-making with sub-linear theoretical regret and local client privacy argument. Fedego Lasso relies on a novel multi-client teamwork-selfish bandit policy design to per- form decentralized collaborative exploration and federated egocentric exploration with log- arithmic communication costs. Experiments demonstrate the effectiveness of the proposed algorithms on both synthetic and real-world datasets",
    "checked": true,
    "id": "8adee03bb063e0e4140ac38a2f35553e65b4c119",
    "semantic_title": "federated high-dimensional online decision making",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QnT41ZGNh9": {
    "title": "Regret Bounds for Satisficing in Multi-Armed Bandit Problems",
    "volume": "main",
    "abstract": "This paper considers the objective of \\textit{satisficing} in multi-armed bandit problems. Instead of aiming to find an optimal arm, the learner is content with an arm whose reward is above a given satisfaction level. We provide algorithms and analysis for the realizable case when such a satisficing arm exists as well as for the general case when this may not be the case. Introducing the notion of \\textit{satisficing regret}, our main result shows that in the general case it is possible to obtain constant satisficing regret when there is a satisficing arm (thereby correcting a contrary claim in the literature), while standard logarithmic regret bounds can be re-established otherwise. Experiments illustrate that our algorithm is not only superior to standard algorithms in the satisficing setting, but also works well in the classic bandit setting",
    "checked": true,
    "id": "ffe948b79308cbe0fa0e75011ba0f0cf828ee65f",
    "semantic_title": "regret bounds for satisficing in multi-armed bandit problems",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nFWRuJXPkU": {
    "title": "Using Confounded Data in Latent Model-Based Reinforcement Learning",
    "volume": "main",
    "abstract": "In the presence of confounding, naively using off-the-shelf offline reinforcement learning (RL) algorithms leads to sub-optimal behaviour. In this work, we propose a safe method to exploit confounded offline data in model-based RL, which improves the sample-efficiency of an interactive agent that also collects online, unconfounded data. First, we import ideas from the well-established framework of $do$-calculus to express model-based RL as a causal inference problem, thus bridging the gap between the fields of RL and causality. Then, we propose a generic method for learning a causal transition model from offline and online data, which captures and corrects the confounding effect using a hidden latent variable. We prove that our method is correct and efficient, in the sense that it attains better generalization guarantees thanks to the confounded offline data (in the asymptotic case), regardless of the confounding effect (the offline expert's behaviour). We showcase our method on a series of synthetic experiments, which demonstrate that a) using confounded offline data naively degrades the sample-efficiency of an RL agent; b) using confounded offline data correctly improves sample-efficiency",
    "checked": true,
    "id": "103881e6eb432286ad86b2a345880e276c60ff50",
    "semantic_title": "using confounded data in latent model-based reinforcement learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1irVjE7A3w": {
    "title": "Meta-Learning via Classifier(-free) Diffusion Guidance",
    "volume": "main",
    "abstract": "We introduce meta-learning algorithms that perform zero-shot weight-space adaptation of neural network models to unseen tasks. Our methods repurpose the popular generative image synthesis techniques of natural language guidance and diffusion models to generate neural network weights adapted for tasks. We first train an unconditional generative hypernetwork model to produce neural network weights; then we train a second \"guidance\" model that, given a natural language task description, traverses the hypernetwork latent space to find high-performance task-adapted weights in a zero-shot manner. We explore two alternative approaches for latent space guidance: \"HyperCLIP\"-based classifier guidance and a conditional Hypernetwork Latent Diffusion Model (\"HyperLDM\"), which we show to benefit from the classifier-free guidance technique common in image generation. Finally, we demonstrate that our approaches outperform existing multi-task and meta-learning methods in a series of zero-shot learning experiments on our Meta-VQA dataset",
    "checked": true,
    "id": "b83c62f3875a649aeafb22ff1c380a7bdd744b43",
    "semantic_title": "meta-learning via classifier(-free) diffusion guidance",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nGW2Hotpq3": {
    "title": "Optimizing Learning Rate Schedules for Iterative Pruning of Deep Neural Networks",
    "volume": "main",
    "abstract": "The importance of learning rate (LR) schedules on network pruning has been observed in a few recent works. As an example, Frankle and Carbin (2019) highlighted that winning tickets (i.e., accuracy preserving subnetworks) can not be found without applying a LR warmup schedule. Renda, Frankle and Carbin (2020) also demonstrated that rewinding the LR to its initial state at the end of each pruning cycle can improve pruning performance. In this paper, we go one step further by first providing a theoretical justification for the surprising effect of LR schedules. Next, we propose a LR schedule for network pruning called SILO, which stands for S-shaped Improved Learning rate Optimization. The advantages of SILO over existing LR schedules are two-fold: (i) SILO has a strong theoretical motivation and dynamically adjusts the LR during pruning to improve generalization. Specifically, SILO increases the LR upper bound (max_lr) in an S-shape. This leads to an improvement of 2% - 4% in extensive experiments with various types of networks (e.g., Vision Transformers, ResNet) on popular datasets such as ImageNet, CIFAR-10/100. (ii) In addition to the strong theoretical motivation, SILO is empirically optimal in the sense of matching an Oracle, which exhaustively searches for the optimal value of max_lr via grid search. We find that SILO is able to precisely adjust the value of max_lr to be within the Oracle optimized interval, resulting in performance competitive with the Oracle with significantly lower complexity",
    "checked": true,
    "id": "2e09a43f15dfe8068283207f37795f2e918bbe15",
    "semantic_title": "optimizing learning rate schedules for iterative pruning of deep neural networks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wvLQMHtyLk": {
    "title": "Foiling Explanations in Deep Neural Networks",
    "volume": "main",
    "abstract": "Deep neural networks (DNNs) have greatly impacted numerous fields over the past decade. Yet despite exhibiting superb performance over many problems, their black-box nature still poses a significant challenge with respect to explainability. Indeed, explainable artificial intelligence (XAI) is crucial in several fields, wherein the answer alone---sans a reasoning of how said answer was derived---is of little value. This paper uncovers a troubling property of explanation methods for image-based DNNs: by making small visual changes to the input image---hardly influencing the network's output---we demonstrate how explanations may be arbitrarily manipulated through the use of evolution strategies. Our novel algorithm, AttaXAI, a model-and-data XAI-agnostic, adversarial attack on XAI algorithms, only requires access to the output logits of a classifier and to the explanation map; these weak assumptions render our approach highly useful where real-world models and data are concerned. We compare our method's performance on two benchmark datasets---CIFAR100 and ImageNet---using four different pretrained deep-learning models: VGG16-CIFAR100, VGG16-ImageNet, MobileNet-CIFAR100, and Inception-v3-ImageNet. We find that the XAI methods can be manipulated without the use of gradients or other model internals. Our novel algorithm is successfully able to manipulate an image in a manner imperceptible to the human eye, such that the XAI method outputs a specific explanation map. To our knowledge, this is the first such method in a black-box setting, and we believe it has significant value where explainability is desired, required, or legally mandatory",
    "checked": true,
    "id": "17ac4d4e9988eeedcf780b81c16b4130f018f3f8",
    "semantic_title": "foiling explanations in deep neural networks",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=pCbC3aQB5W": {
    "title": "Long-term Forecasting with TiDE: Time-series Dense Encoder",
    "volume": "main",
    "abstract": "Recent work has shown that simple linear models can outperform several Transformer based approaches in long term time-series forecasting. Motivated by this, we propose a Multi-layer Perceptron (MLP) based encoder-decoder model, \\underline{Ti}me-series \\underline{D}ense \\underline{E}ncoder (TiDE), for long-term time-series forecasting that enjoys the simplicity and speed of linear models while also being able to handle covariates and non-linear dependencies. Theoretically, we prove that the simplest linear analogue of our model can achieve near optimal error rate for linear dynamical systems (LDS) under some assumptions. Empirically, we show that our method can match or outperform prior approaches on popular long-term time-series forecasting benchmarks while being 5-10x faster than the best Transformer based model",
    "checked": true,
    "id": "2ced138789c8a1c39f0f57c8fbb18b94e6ed8034",
    "semantic_title": "long-term forecasting with tide: time-series dense encoder",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=Y3saBb7mCE": {
    "title": "Empirical Limitations of the NTK for Understanding Scaling Laws in Deep Learning",
    "volume": "main",
    "abstract": "The ``Neural Tangent Kernel'' (NTK) (Jacot et al 2018), and its empirical variants have been proposed as a proxy to capture certain behaviors of real neural networks. In this work, we study NTKs through the lens of scaling laws, and demonstrate that they fall short of explaining important aspects of neural network generalization. In particular, we demonstrate realistic settings where finite-width neural networks have significantly better data scaling exponents as compared to their corresponding empirical and infinite NTKs at initialization. This reveals a more fundamental difference between the real networks and NTKs, beyond just a few percentage points of test accuracy. Further, we show that even if the empirical NTK is allowed to be pre-trained on a constant number of samples, the kernel scaling does not catch up to the neural network scaling. Finally, we show that the empirical NTK continues to evolve throughout most of the training, in contrast with prior work which suggests that it stabilizes after a few epochs of training. Altogether, our work establishes concrete limitations of the NTK approach in understanding scaling laws of real networks on natural datasets",
    "checked": true,
    "id": "3881801d73a223cac1fd317a5c47df26be87af57",
    "semantic_title": "empirical limitations of the ntk for understanding scaling laws in deep learning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=7KW7zvKd7J": {
    "title": "Transport Score Climbing: Variational Inference Using Forward KL and Adaptive Neural Transport",
    "volume": "main",
    "abstract": "Variational inference often minimizes the ``reverse'' Kullbeck-Leibler (KL) $D_{KL}(q||p)$ from the approximate distribution $q$ to the posterior $p$. Recent work studies the ``forward'' KL $D_{KL}(p||q)$, which unlike reverse KL does not lead to variational approximations that underestimate uncertainty. Markov chain Monte Carlo (MCMC) methods were used to evaluate the expectation in computing the forward KL. This paper introduces Transport Score Climbing (TSC), a method that optimizes $D_{KL}(p||q)$ by using Hamiltonian Monte Carlo (HMC) but running the HMC chain on a transformed, or warped, space. A function called the transport map performs the transformation by acting as a change-of-variable from the latent variable space. TSC uses HMC samples to dynamically train the transport map while optimizing $D_{KL}(p||q)$. TSC leverages synergies, where better transport maps lead to better HMC sampling, which then leads to better transport maps. We demonstrate TSC on synthetic and real data, including using TSC to train variational auto-encoders. We find that TSC achieves competitive performance on the experiments",
    "checked": true,
    "id": "5de7813ce5bc9f361f2b6fd09f884b468dcb43bf",
    "semantic_title": "transport score climbing: variational inference using forward kl and adaptive neural transport",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=D5Z2E8CNsD": {
    "title": "Distributionally Robust Classification on a Data Budget",
    "volume": "main",
    "abstract": "Real world uses of deep learning require predictable model behavior under distribution shifts. Models such as CLIP show emergent natural distributional robustness comparable to humans, but may require hundreds of millions of training samples. Can we train robust learners in a domain where data is limited? To rigorously address this question, we introduce JANuS (Joint Annotations and Names Set), a collection of four new training datasets with images, labels, and corresponding captions, and perform a series of carefully controlled investigations of factors contributing to robustness in image classification, then compare those results to findings derived from a large-scale meta-analysis. Using this approach, we show that standard ResNet-50 trained with the cross-entropy loss on 2.4 million image samples can attain comparable robustness to a CLIP ResNet-50 trained on 400 million samples. To our knowledge, this is the first result showing (near) state-of-the-art distributional robustness on limited data budgets",
    "checked": true,
    "id": "6a4e7c54648487cfce1d18344fdadf4a095c055b",
    "semantic_title": "distributionally robust classification on a data budget",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8ykyGbtt2q": {
    "title": "The ConceptARC Benchmark: Evaluating Understanding and Generalization in the ARC Domain",
    "volume": "main",
    "abstract": "The abilities to form and abstract concepts is key to human intelligence, but such abilities remain lacking in state-of-the-art AI systems. There has been substantial research on conceptual abstraction in AI, particularly using idealized domains such as Raven's Progressive Matrices and Bongard problems, but even when AI systems succeed on such problems, the systems are rarely evaluated in depth to see if they have actually grasped the concepts they are meant to capture. In this paper we describe an in-depth evaluation benchmark for the Abstraction and Reasoning Corpus (ARC), a collection of few-shot abstraction and analogy problems developed by Chollet (2019). In particular, we describe ConceptARC, a new, publicly available bench- mark in the ARC domain that systematically assesses abstraction and generalization abilities on a number of basic spatial and semantic concepts. ConceptARC differs from the original ARC dataset in that it is specifically organized around \"concept groups\"—sets of problems that focus on specific concepts and that are vary in complexity and level of abstraction. We report results on testing humans on this benchmark as well as three machine solvers: the top two programs from a 2021 ARC competition and OpenAI's GPT-4. Our results show that humans substantially outperform the machine solvers on this benchmark, showing abilities to abstract and generalize concepts that are not yet captured by AI systems. We believe that this benchmark will spur improvements in the development of AI systems for conceptual abstraction and in the effective evaluation of such systems",
    "checked": true,
    "id": "0febeb46b8216f12337b448ac81ac505c28782c1",
    "semantic_title": "the conceptarc benchmark: evaluating understanding and generalization in the arc domain",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=Rb6VDOHebB": {
    "title": "Adaptive Compression for Communication-Efficient Distributed Training",
    "volume": "main",
    "abstract": "We propose Adaptive Compressed Gradient Descent (AdaCGD) -- a novel optimization algorithm for communication-efficient training of supervised machine learning models with adaptive compression level. Our approach is inspired by the recently proposed three point compressor (3PC) framework of Richtarik et al. (2022) , which includes error feedback (EF21), lazily aggregated gradient (LAG), and their combination as special cases, and offers the current state-of-the-art rates for these methods under weak assumptions. While the above mechanisms offer a fixed compression level or adapt between two extreme compression levels, we propose a much finer adaptation. In particular, we allow users to choose between selected contractive compression mechanisms, such as Top-$K$ sparsification with a user-defined selection of sparsification levels $K$, or quantization with a user-defined selection of quantization levels, or their combination. AdaCGD chooses the appropriate compressor and compression level adaptively during the optimization process. Besides i) proposing a theoretically-grounded multi-adaptive communication compression mechanism, we further ii) extend the 3PC framework to bidirectional compression, i.e., allow the server to compress as well, and iii) provide sharp convergence bounds in the strongly convex, convex, and nonconvex settings. The convex regime results are new even for several key special cases of our general mechanism, including 3PC and EF21. In all regimes, our rates are superior compared to all existing adaptive compression methods",
    "checked": true,
    "id": "e87dea6df52984f4df14edc229515ceb623bfd1a",
    "semantic_title": "adaptive compression for communication-efficient distributed training",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=H1SekypXKA": {
    "title": "Expected Worst Case Regret via Stochastic Sequential Covering",
    "volume": "main",
    "abstract": "We study the problem of sequential prediction and online minimax regret with stochastically generated features under a general loss function. In an online learning setting, Nature selects features and associates a true label with these features. A learner uses features to predict a label, which is compared to the true label, and a loss is incurred. The total loss over $T$ rounds, when compared to a loss incurred by a set of experts, is known as a regret. We introduce the notion of *expected worst case minimax regret* that generalizes and encompasses prior known minimax regrets. For such minimax regrets, we establish tight upper bounds via a novel concept of *stochastic global sequential covering*. We show that for a hypothesis class of VC-dimension $\\mathsf{VC}$ and $i.i.d.$ generated features over $T$ rounds, the cardinality of stochastic global sequential covering can be upper bounded with high probability (w.h.p.) by $e^{O(\\mathsf{VC} \\cdot \\log^2 T)}$. We then improve this bound by introducing a new complexity measure called the *Star-Littlestone* dimension, and show that classes with Star-Littlestone dimension $\\mathsf{SL}$ admit a stochastic global sequential covering of order $e^{O(\\mathsf{SL} \\cdot \\log T)}$. We further establish upper bounds for real valued classes with finite fat-shattering numbers. Finally, by applying information-theoretic tools for the fixed design minimax regrets, we provide lower bounds for expected worst case minimax regret. We demonstrate the effectiveness of our approach by establishing tight bounds on the expected worst case minimax regrets for logarithmic loss and general mixable losses",
    "checked": true,
    "id": "69d9f836fb20625cc3fd5d74410612ff45392165",
    "semantic_title": "expected worst case regret via stochastic sequential covering",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=HVAeM6sNo8": {
    "title": "Robust Alzheimer's Progression Modeling using Cross-Domain Self-Supervised Deep Learning",
    "volume": "main",
    "abstract": "Developing successful artificial intelligence systems in practice depends on both robust deep learning models and large, high-quality data. However, acquiring and labeling data can be prohibitively expensive and time-consuming in many real-world applications, such as clinical disease models. Self-supervised learning has demonstrated great potential in increasing model accuracy and robustness in small data regimes. In addition, many clinical imaging and disease modeling applications rely heavily on regression of continuous quantities. However, the applicability of self-supervised learning for these medical-imaging regression tasks has not been extensively studied. In this study, we develop a cross-domain self-supervised learning approach for disease prognostic modeling as a regression problem using medical images as input. We demonstrate that self-supervised pretraining can improve the prediction of Alzheimer's Disease progression from brain MRI. We also show that pretraining on extended (but not labeled) brain MRI data outperforms pretraining on natural images. We further observe that the highest performance is achieved when both natural images and extended brain-MRI data are used for pretraining",
    "checked": true,
    "id": "c0767c2a97506c762d28b5356d312dab0682da8a",
    "semantic_title": "robust alzheimer's progression modeling using cross-domain self-supervised deep learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LRYtNj8Xw0": {
    "title": "Learning Augmentation Distributions using Transformed Risk Minimization",
    "volume": "main",
    "abstract": "We propose a new \\emph{Transformed Risk Minimization} (TRM) framework as an extension of classical risk minimization. In TRM, we optimize not only over predictive models, but also over data transformations; specifically over distributions thereof. As a key application, we focus on learning augmentations; for instance appropriate rotations of images, to improve classification performance with a given class of predictors. Our TRM method (1) jointly learns transformations and models in a \\emph{single training loop}, (2) works with any training algorithm applicable to standard risk minimization, and (3) handles any transforms, such as discrete and continuous classes of augmentations. To avoid overfitting when implementing empirical transformed risk minimization, we propose a novel regularizer based on PAC-Bayes theory. For learning augmentations of images, we propose a new parametrization of the space of augmentations via a stochastic composition of blocks of geometric transforms. This leads to the new \\emph{Stochastic Compositional Augmentation Learning} (SCALE) algorithm. The performance of TRM with SCALE compares favorably to prior methods on CIFAR10/100. Additionally, we show empirically that SCALE can correctly learn certain symmetries in the data distribution (recovering rotations on rotated MNIST) and can also improve calibration of the learned model",
    "checked": true,
    "id": "dff0c9f18456b38aaefe44daccc0070a7c47b89f",
    "semantic_title": "learning augmentation distributions using transformed risk minimization",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=qUxBs3Ln41": {
    "title": "Structured Low-Rank Tensors for Generalized Linear Models",
    "volume": "main",
    "abstract": "Recent works have shown that imposing tensor structures on the coefficient tensor in regression problems can lead to more reliable parameter estimation and lower sample complexity compared to vector-based methods. This work investigates a new low-rank tensor model, called Low Separation Rank (LSR), in Generalized Linear Model (GLM) problems. The LSR model – which generalizes the well-known Tucker and CANDECOMP/PARAFAC (CP) models, and is a special case of the Block Tensor Decomposition (BTD) model – is imposed onto the coefficient tensor in the GLM model. This work proposes a block coordinate descent algorithm for parameter estimation in LSR-structured tensor GLMs. Most importantly, it derives a minimax lower bound on the error threshold on estimating the coefficient tensor in LSR tensor GLM problems. The minimax bound is proportional to the intrinsic degrees of freedom in the LSR tensor GLM problem, suggesting that its sample complexity may be significantly lower than that of vectorized GLMs. This result can also be specialised to lower bound the estimation error in CP and Tucker-structured GLMs. The derived bounds are comparable to tight bounds in the literature for Tucker linear regression, and the tightness of the minimax lower bound is further assessed numerically. Finally, numerical experiments on synthetic datasets demonstrate the efficacy of the proposed LSR tensor model for three regression types (linear, logistic and Poisson). Experiments on a collection of medical imaging datasets demonstrate the usefulness of the LSR model over other tensor models (Tucker and CP) on real, imbalanced data with limited available samples",
    "checked": true,
    "id": "a50e94dba02e53b032039f29bef46a70e6d63150",
    "semantic_title": "structured low-rank tensors for generalized linear models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dXAuvo6CGI": {
    "title": "Scalable Stochastic Gradient Riemannian Langevin Dynamics in Non-Diagonal Metrics",
    "volume": "main",
    "abstract": "Stochastic-gradient sampling methods are often used to perform Bayesian inference on neural networks. It has been observed that the methods in which notions of differential geometry are included tend to have better performances, with the Riemannian metric improving posterior exploration by accounting for the local curvature. However, the existing methods often resort to simple diagonal metrics to remain computationally efficient. This loses some of the gains. We propose two non-diagonal metrics that can be used in stochastic-gradient samplers to improve convergence and exploration but have only a minor computational overhead over diagonal metrics. We show that for fully connected neural networks (NNs) with sparsity-inducing priors and convolutional NNs with correlated priors, using these metrics can provide improvements. For some other choices the posterior is sufficiently easy also for the simpler metrics",
    "checked": true,
    "id": "5bcc7910b5dd5fe9c3445428c27412c3ef2c4eac",
    "semantic_title": "scalable stochastic gradient riemannian langevin dynamics in non-diagonal metrics",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HwcB5elyuG": {
    "title": "Towards a Defense Against Federated Backdoor Attacks Under Continuous Training",
    "volume": "main",
    "abstract": "Backdoor attacks are dangerous and difficult to prevent in federated learning (FL), where training data is sourced from untrusted clients over long periods of time. These difficulties arise because: (a) defenders in FL do not have access to raw training data, and (b) a phenomenon we identify called backdoor leakage causes models trained continuously to eventually suffer from backdoors due to cumulative errors in defense mechanisms. We propose a framework called shadow learning for defending against backdoor attacks in the FL setting under long-range training. Shadow learning trains two models in parallel: a backbone model and a shadow model. The backbone is trained without any defense mechanism to obtain good performance on the main task. The shadow model combines filtering of malicious clients with early-stopping to control the attack success rate even as the data distribution changes. We theoretically motivate our design and show experimentally that our framework significantly improves upon existing defenses against backdoor attacks",
    "checked": true,
    "id": "fdcd2cb908f32a35b5504467182f7ca631074aaf",
    "semantic_title": "towards a defense against federated backdoor attacks under continuous training",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9jnsPp8DP3": {
    "title": "mL-BFGS: A Momentum-based L-BFGS for Distributed Large-scale Neural Network Optimization",
    "volume": "main",
    "abstract": "Quasi-Newton methods still face significant challenges in training large-scale neural networks due to additional compute costs in the Hessian related computations and instability issues in stochastic training. A well-known method, L-BFGS that efficiently approximates the Hessian using history parameter and gradient changes, suffers convergence instability in stochastic training. So far, attempts that adapt L-BFGS to large-scale stochastic training incur considerable extra overhead, which offsets its convergence benefits in wall-clock time. In this paper, we propose mL-BFGS, a lightweight momentum-based L-BFGS algorithm that paves the way for quasi-Newton (QN) methods in large-scale distributed deep neural network (DNN) optimization. mL-BFGS introduces a nearly cost-free momentum scheme into L-BFGS update and greatly reduces stochastic noise in the Hessian, therefore stabilizing convergence during stochastic optimization. For model training at a large scale, mL-BFGS approximates a block-wise Hessian, thus enabling distributing compute and memory costs across all computing nodes. We provide a supporting convergence analysis for mL-BFGS in stochastic settings. To investigate mL-BFGS's potential in large-scale DNN training, we train benchmark neural models using mL-BFGS and compare performance with baselines (SGD, Adam, and other quasi-Newton methods). Results show that mL-BFGS achieves both noticeable iteration-wise and wall-clock speedup",
    "checked": true,
    "id": "f43c6109e3275fd025c95982653e378f93e5e5b2",
    "semantic_title": "ml-bfgs: a momentum-based l-bfgs for distributed large-scale neural network optimization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lu4oAq55iK": {
    "title": "Mitigating Real-World Distribution Shifts in the Fourier Domain",
    "volume": "main",
    "abstract": "While machine learning systems can be highly accurate in their training environments, their performance in real-world deployments can suffer significantly due to distribution shifts. Real-world distribution shifts involve various input distortions due to noise, weather, device and other variations. Many real-world distribution shifts are not represented in standard domain adaptation datasets and prior empirical work has shown that domain adaptation methods developed using these standard datasets may not generalize well to real-world distribution shifts. Furthermore, motivated by observations of the sensitivity of deep neural networks (DNN) to the spectral statistics of data, which can vary in real-world scenarios, we propose Fourier Moment Matching (FMM), a model-agnostic input transformation that matches the Fourier-amplitude statistics of source to target data using unlabeled samples. We demonstrate through extensive empirical evaluations across time-series, image classification and semantic segmentation tasks that FMM is effective both individually and when combined with a variety of existing methods to overcome real-world distribution shifts",
    "checked": true,
    "id": "48c2dcc475d7e331787665c63a063fb7ec727b98",
    "semantic_title": "mitigating real-world distribution shifts in the fourier domain",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nOIGfQnFZm": {
    "title": "Learning representations that are closed-form Monge mapping optimal with application to domain adaptation",
    "volume": "main",
    "abstract": "Optimal transport (OT) is a powerful geometric tool used to compare and align probability measures following the least effort principle. Despite its widespread use in machine learning (ML), OT problem still bears its computational burden, while at the same time suffering from the curse of dimensionality for measures supported on general high-dimensional spaces. In this paper, we propose to tackle these challenges using representation learning. In particular, we seek to learn an embedding space such that the samples of the two input measures become alignable in it with a simple affine mapping that can be calculated efficiently in closed-form. We then show that such approach leads to results that are comparable to solving the original OT problem when applied to the transfer learning task on which many OT baselines where previously evaluated in both homogeneous and heterogeneous DA settings",
    "checked": true,
    "id": "a627931bd2fcecee236ee12a4074449cdd246020",
    "semantic_title": "learning representations that are closed-form monge mapping optimal with application to domain adaptation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kdfiEu1ul6": {
    "title": "Learning from time-dependent streaming data with online stochastic algorithms",
    "volume": "main",
    "abstract": "This paper addresses stochastic optimization in a streaming setting with time-dependent and biased gradient estimates. We analyze several first-order methods, including Stochastic Gradient Descent (SGD), mini-batch SGD, and time-varying mini-batch SGD, along with their Polyak-Ruppert averages. Our non-asymptotic analysis establishes novel heuristics that link dependence, biases, and convexity levels, enabling accelerated convergence. Specifically, our findings demonstrate that (i) time-varying mini-batch SGD methods have the capability to break long- and short-range dependence structures, (ii) biased SGD methods can achieve comparable performance to their unbiased counterparts, and (iii) incorporating Polyak-Ruppert averaging can accelerate the convergence of the stochastic optimization algorithms. To validate our theoretical findings, we conduct a series of experiments using both simulated and real-life time-dependent data",
    "checked": true,
    "id": "325711632625f4b15c7bf22ac56af69f35be59ab",
    "semantic_title": "learning from time-dependent streaming data with online stochastic algorithms",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=W0ehjkl9x7": {
    "title": "DoCoM: Compressed Decentralized Optimization with Near-Optimal Sample Complexity",
    "volume": "main",
    "abstract": "This paper proposes the Doubly Compressed Momentum-assisted stochastic gradient tracking algorithm (DoCoM) for communication-efficient decentralized optimization. The algorithm features two main ingredients to achieve a near-optimal sample complexity while allowing for communication compression. First, the algorithm tracks both the averaged iterate and stochastic gradient using compressed gossiping consensus. Second, a momentum step is incorporated for adaptive variance reduction with the local gradient estimates. We show that DoCoM finds a near-stationary solution at all participating agents satisfying $\\mathbb{E}[ \\| \\nabla f( \\theta ) \\|^2 ] = {\\cal O}( 1 / T^{2/3} )$ in $T$ iterations, where $f(\\theta)$ is a smooth (possibly non-convex) objective function. Notice that the proof is achieved via analytically designing a new potential function that tightly tracks the one-iteration progress of DoCoM. As a corollary, our analysis also established the linear convergence of DoCoM to a global optimal solution for objective functions with the Polyak-Łojasiewicz condition. Numerical experiments demonstrate that our algorithm outperforms several state-of-the-art algorithms in practice",
    "checked": true,
    "id": "daaf014ab4b0630e0146dd6870a035075dcaaf55",
    "semantic_title": "docom: compressed decentralized optimization with near-optimal sample complexity",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=moVEUgJaHO": {
    "title": "GPS++: Reviving the Art of Message Passing for Molecular Property Prediction",
    "volume": "main",
    "abstract": "We present GPS++, a hybrid Message Passing Neural Network / Graph Transformer model for molecular property prediction. Our model integrates a well-tuned local message passing component and biased global attention with other key ideas from prior literature to achieve state-of-the-art results on large-scale molecular dataset PCQM4Mv2. Through a thorough ablation study we highlight the impact of individual components and find that nearly all of the model's performance can be maintained without any use of global self-attention, showing that message passing is still a competitive approach for 3D molecular property prediction despite the recent dominance of graph transformers. We also find that our approach is significantly more accurate than prior art when 3D positional information is not available",
    "checked": true,
    "id": "db3d0ad5e70f073c9383d199093127e9f70d74db",
    "semantic_title": "gps++: reviving the art of message passing for molecular property prediction",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=3LzgOQ3eOb": {
    "title": "Tackling Provably Hard Representative Selection via Graph Neural Networks",
    "volume": "main",
    "abstract": "Representative Selection (RS) is the problem of finding a small subset of exemplars from a dataset that is representative of the dataset. In this paper, we study RS for attributed graphs, and focus on finding representative nodes that optimize the accuracy of a model trained on the selected representatives. Theoretically, we establish a new hardness result for RS (in the absence of a graph structure) by proving that a particular, highly practical variant of it (RS for Learning) is hard to approximate in polynomial time within any reasonable factor, which implies a significant potential gap between the optimum solution of widely-used surrogate functions and the actual accuracy of the model. We then study the setting where a (homophilous) graph structure is available, or can be constructed, between the data points. We show that with an appropriate modeling approach, the presence of such a structure can turn a hard RS (for learning) problem into one that can be effectively solved. To this end, we develop RS-GNN, a representation learning-based RS model based on Graph Neural Networks. Empirically, we demonstrate the effectiveness of RS-GNN on problems with predefined graph structures as well as problems with graphs induced from node feature similarities, by showing that RS-GNN achieves significant improvements over established baselines on a suite of eight benchmarks",
    "checked": true,
    "id": "ba9e9e6fbc3ed6104a2184004278d825854020aa",
    "semantic_title": "tackling provably hard representative selection via graph neural networks",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=Qlvgq9eC63": {
    "title": "Improved Group Robustness via Classifier Retraining on Independent Splits",
    "volume": "main",
    "abstract": "Deep neural networks trained by minimizing the average risk can achieve strong average performance. Still, their performance for a subgroup may degrade if the subgroup is underrepresented in the overall data population. Group distributionally robust optimization (Sagawa et al., 2020a), or group DRO in short, is a widely used baseline for learning models with strong worst-group performance. We note that this method requires group labels for every example at training time and can overfit to small groups, requiring strong regularization. Given a limited amount of group labels at training time, Just Train Twice (Liu et al., 2021), or JTT in short, is a two-stage method that infers a pseudo group label for every unlabeled example first, then applies group DRO based on the inferred group labels. The inference process is also sensitive to overfitting, sometimes involving additional hyperparameters. This paper designs a simple method based on the idea of classifier retraining on independent splits of the training data. We find that using a novel sample-splitting procedure achieves robust worst-group performance in the fine-tuning step. When evaluated on benchmark image and text classification tasks, our approach consistently performs favorably to group DRO, JTT, and other strong baselines when either group labels are available during training or are only given in validation sets. Importantly, our method only relies on a single hyperparameter, which adjusts the fraction of labels used for training feature extractors vs. training classification layers. We justify the rationale of our splitting scheme with a generalization-bound analysis of the worst-group loss",
    "checked": true,
    "id": "a681cc1250fa84b7af8472b59f99563f816fa303",
    "semantic_title": "improved group robustness via classifier retraining on independent splits",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=0XBuaxqEcG": {
    "title": "Execution-based Code Generation using Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "The utilization of programming language (PL) models, pre-trained on large-scale code corpora, as a means of automating software engineering processes has demonstrated considerable potential in streamlining various code generation tasks such as code completion, code translation, and program synthesis. However, current approaches mainly rely on supervised fine-tuning objectives borrowed from text generation, neglecting unique sequence-level characteristics of code, including but not limited to compilability as well as syntactic and functional correctness. To address this limitation, we propose PPOCoder, a new framework for code generation that synergistically combines pre-trained PL models with Proximal Policy Optimization (PPO) which is a widely used deep reinforcement learning technique. By utilizing non-differentiable feedback from code execution and structure alignment, PPOCoder seamlessly integrates external code-specific knowledge into the model optimization process. It's important to note that PPOCoder is a task-agnostic and model-agnostic framework that can be used across different code generation tasks and PLs. Extensive experiments on three code generation tasks demonstrate the effectiveness of our proposed approach compared to SOTA methods, achieving significant improvements in compilation success rates and functional correctness across different PLs",
    "checked": true,
    "id": "0a6bc37a07a37e3573d36e10cc11669eca0ff903",
    "semantic_title": "execution-based code generation using deep reinforcement learning",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=TIsrnWpjQ0": {
    "title": "TabCBM: Concept-based Interpretable Neural Networks for Tabular Data",
    "volume": "main",
    "abstract": "Concept-based interpretability addresses the opacity of deep neural networks by constructing an explanation for a model's prediction using high-level units of information referred to as concepts. Research in this area, however, has been mainly focused on image and graph-structured data, leaving high-stakes tasks whose data is tabular out of reach of existing methods. In this paper, we address this gap by introducing the first definition of what a high-level concept may entail in tabular data. We use this definition to propose Tabular Concept Bottleneck Models (TabCBMs), a family of interpretable self-explaining neural architectures capable of learning high-level concept explanations for tabular tasks. As our method produces concept-based explanations both when partial concept supervision or no concept supervision is available at training time, it is adaptable to settings where concept annotations are missing. We evaluate our method in both synthetic and real-world tabular tasks and show that TabCBM outperforms or performs competitively compared to state-of-the-art methods, while providing a high level of interpretability as measured by its ability to discover known high-level concepts. Finally, we show that TabCBM can discover important high-level concepts in synthetic datasets inspired by critical tabular tasks (e.g., single-cell RNAseq) and allows for human-in-the-loop concept interventions in which an expert can identify and correct mispredicted concepts to boost the model's performance",
    "checked": true,
    "id": "ed820008a5e3694147c155359387ab0120732acc",
    "semantic_title": "tabcbm: concept-based interpretable neural networks for tabular data",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=giw2vcAhiH": {
    "title": "Spectral learning of Bernoulli linear dynamical systems models for decision-making",
    "volume": "main",
    "abstract": "Latent linear dynamical systems with Bernoulli observations provide a powerful modeling framework for identifying the temporal dynamics underlying binary time series data, which arise in a variety of contexts such as binary decision-making and discrete stochastic processes such as binned neural spike trains. Here we develop a spectral learning method for fast, efficient fitting of probit-Bernoulli latent linear dynamical system (LDS) models. Our approach extends traditional subspace identification methods to the Bernoulli setting via a transformation of the first and second sample moments. This results in a robust, fixed-cost estimator that avoids the hazards of local optima and the long computation time of iterative fitting procedures like the expectation-maximization (EM) algorithm. In regimes where data is limited or assumptions about the statistical structure of the data are not met, we demonstrate that the spectral estimate provides a good initialization for Laplace-EM fitting. Finally, we show that the estimator provides substantial benefits to real world settings by analyzing data from mice performing a sensory decision-making task",
    "checked": true,
    "id": "ca305aaa6504e314e4deb8d8a561119c60590c68",
    "semantic_title": "spectral learning of bernoulli linear dynamical systems models for decision-making",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bnBeNFB27b": {
    "title": "Self-Supervision is All You Need for Solving Rubik's Cube",
    "volume": "main",
    "abstract": "Existing combinatorial search methods are often complex and require some level of expertise. This work introduces a simple and efficient deep learning method for solving combinatorial problems with a predefined goal, represented by Rubik's Cube. We demonstrate that, for such problems, training a deep neural network on random scrambles branching from the goal state is sufficient to achieve near-optimal solutions. When tested on Rubik's Cube, 15 Puzzle, and 7$\\times$7 Lights Out, our method outperformed the previous state-of-the-art method DeepCubeA, improving the trade-off between solution optimality and computational cost, despite significantly less training data. Furthermore, we investigate the scaling law of our Rubik's Cube solver with respect to model size and training data volume",
    "checked": true,
    "id": "28141ca1ce5a2c536358d3934a23045b837e2d0c",
    "semantic_title": "self-supervision is all you need for solving rubik's cube",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EYjfLeJL4l": {
    "title": "Towards Better Generalization with Flexible Representation of Multi-Module Graph Neural Networks",
    "volume": "main",
    "abstract": "Graph neural networks (GNNs) have become compelling models designed to perform learning and inference on graph-structured data. However, little work has been done to understand the fundamental limitations of GNNs for scaling to larger graphs and generalizing to out-of-distribution (OOD) inputs. In this paper, we use a random graph generator to systematically investigate how the graph size and structural properties affect the predictive performance of GNNs. We present specific evidence that the average node degree is a key feature in determining whether GNNs can generalize to unseen graphs, and that the use of multiple node update functions can improve the generalization performance of GNNs when dealing with graphs of multimodal degree distributions. Accordingly, we propose a multi-module GNN framework that allows the network to adapt flexibly to new graphs by generalizing a single canonical nonlinear transformation over aggregated inputs. Our results show that the multi-module GNNs improve the OOD generalization on a variety of inference tasks in the direction of diverse structural features",
    "checked": true,
    "id": "927c9b5afffc7b6b2d4474047e3964c9d5594f0f",
    "semantic_title": "towards better generalization with flexible representation of multi-module graph neural networks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5rq8iRzHAQ": {
    "title": "Assisting Human Decisions in Document Matching",
    "volume": "main",
    "abstract": "Many practical applications, ranging from paper-reviewer assignment in peer review to job-applicant matching for hiring, require human decision makers to identify relevant matches by combining their expertise with predictions from machine learning models. In many such model-assisted document matching tasks, the decision makers have stressed the need for assistive information about the model outputs (or the data) to facilitate their decisions. In this paper, we devise a proxy matching task that allows us to evaluate which kinds of assistive information improve decision makers' performance (in terms of accuracy and time). Through a crowdsourced (N = 271 participants) study, we find that providing black-box model explanations reduces users' accuracy on the matching task, contrary to the commonly-held belief that they can be helpful by allowing better understanding of the model. On the other hand, custom methods that are designed to closely attend to some task-specific desiderata are found to be effective in improving user performance. Surprisingly, we also find that the users' perceived utility of assistive information is misaligned with their objective utility (measured through their task performance)",
    "checked": true,
    "id": "92f9859569cf60f841911960af13efeec095882e",
    "semantic_title": "assisting human decisions in document matching",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=T5sXdAO3EQ": {
    "title": "Bayesian Quadrature for Neural Ensemble Search",
    "volume": "main",
    "abstract": "Ensembling can improve the performance of Neural Networks, but existing approaches struggle when the architecture likelihood surface has dispersed, narrow peaks. Furthermore, existing methods construct equally weighted ensembles, and this is likely to be vulnerable to the failure modes of the weaker architectures. By viewing ensembling as approximately marginalising over architectures we construct ensembles using the tools of Bayesian Quadrature -- tools which are well suited to the exploration of likelihood surfaces with dispersed, narrow peaks. Additionally, the resulting ensembles consist of architectures weighted commensurate with their performance. We show empirically -- in terms of test likelihood, accuracy, and expected calibration error -- that our method outperforms state-of-the-art baselines, and verify via ablation studies that its components do so independently",
    "checked": true,
    "id": "ac698e49c4ff0a56ec6349bfd7ff28a019fbba0c",
    "semantic_title": "bayesian quadrature for neural ensemble search",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MMsyqXIJuk": {
    "title": "JiangJun: Mastering Xiangqi by Tackling Non-Transitivity in Two-Player Zero-Sum Games",
    "volume": "main",
    "abstract": "This paper presents an empirical exploration of non-transitivity in perfect-information games, specifically focusing on Xiangqi, a traditional Chinese board game comparable in game-tree complexity to chess and shogi. By analyzing over 10,000 records of human Xiangqi play, we highlight the existence of both transitive and non-transitive elements within the game's strategic structure. To address non-transitivity, we introduce the JiangJun algorithm, an innovative combination of Monte-Carlo Tree Search (MCTS) and Policy Space Response Oracles (PSRO) designed to approximate a Nash equilibrium. We evaluate the algorithm empirically using a WeChat mini program and achieve a Master level with a 99.41% win rate against human players. The algorithm's effectiveness in overcoming non-transitivity is confirmed by a plethora of metrics, such as relative population performance and visualization results. Our project site is available at https://sites.google.com/view/jiangjun-site/",
    "checked": true,
    "id": "55a71b4f487ee7b44ccf5995be96452f95799ecd",
    "semantic_title": "jiangjun: mastering xiangqi by tackling non-transitivity in two-player zero-sum games",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f39UIDkwwc": {
    "title": "Contrastive Attraction and Contrastive Repulsion for Representation Learning",
    "volume": "main",
    "abstract": "Contrastive learning (CL) methods effectively learn data representations in a self-supervision manner, where the encoder contrasts each positive sample over multiple negative samples via a one-vs-many softmax cross-entropy loss. By leveraging large amounts of unlabeled image data, recent CL methods have achieved promising results when pretrained on large-scale datasets, such as ImageNet. However, most of them consider the augmented views from the same instance are positive pairs, while views from other instances are negative ones. Such binary partition insufficiently considers the relation between samples and tends to yield worse performance when generalized on images in the wild. In this paper, to further improve the performance of CL and enhance its robustness on various datasets, we propose a doubly CL strategy that contrasts positive samples and negative ones within themselves separately. We realize this strategy with contrastive attraction and contrastive repulsion (CACR), which makes the query not only exert a greater force to attract more distant positive samples but also do so to repel closer negative samples. Theoretical analysis reveals that CACR generalizes CL's behavior by positive attraction and negative repulsion. It further considers the intra-contrastive relation within the positive and negative pairs to narrow the gap between the sampled and true distribution, which is important when datasets are less curated. Extensive large-scale experiments on standard vision tasks show that CACR not only consistently outperforms existing CL methods on benchmark datasets, but also shows better robustness when generalized on imbalanced image datasets",
    "checked": true,
    "id": "bc25c37c98b6e0e5424ee173425983228c6e09ab",
    "semantic_title": "contrastive attraction and contrastive repulsion for representation learning",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=HyzCuCV1jH": {
    "title": "Data Augmentation is a Hyperparameter: Cherry-picked Self-Supervision for Unsupervised Anomaly Detection is Creating the Illusion of Success",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL) has emerged as a promising alternative to create supervisory signals to real-world problems, avoiding the extensive cost of manual labeling. SSL is particularly attractive for unsupervised tasks such as anomaly detection (AD), where labeled anomalies are rare or often nonexistent. A large catalog of augmentation functions has been used for SSL-based AD (SSAD) on image data, and recent works have reported that the type of augmentation has a significant impact on accuracy. Motivated by those, this work sets out to put image-based SSAD under a larger lens and investigate the role of data augmentation in SSAD. Through extensive experiments on 3 different detector models and across 420 AD tasks, we provide comprehensive numerical and visual evidences that the alignment between data augmentation and anomaly-generating mechanism is the key to the success of SSAD, and in the lack thereof, SSL may even impair accuracy. To the best of our knowledge, this is the first meta-analysis on the role of data augmentation in SSAD",
    "checked": true,
    "id": "55b54d10679881181433da73d535a7506fca6c17",
    "semantic_title": "data augmentation is a hyperparameter: cherry-picked self-supervision for unsupervised anomaly detection is creating the illusion of success",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Wcui061fxr": {
    "title": "Conditional Generative Models are Provably Robust: Pointwise Guarantees for Bayesian Inverse Problems",
    "volume": "main",
    "abstract": "Conditional generative models became a very powerful tool to sample from Bayesian inverse problem posteriors. It is well-known in classical Bayesian literature that posterior measures are quite robust with respect to perturbations of both the prior measure and the negative log-likelihood, which includes perturbations of the observations. However, to the best of our knowledge, the robustness of conditional generative models with respect to perturbations of the observations has not been investigated yet. In this paper, we prove for the first time that appropriately learned conditional generative models provide robust results for single observations",
    "checked": true,
    "id": "29fcb5ccd0e82cea08cd744d96967be12d3e63ef",
    "semantic_title": "conditional generative models are provably robust: pointwise guarantees for bayesian inverse problems",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=eLX5XrajXh": {
    "title": "A Characteristic Function for Shapley-Value-Based Attribution of Anomaly Scores",
    "volume": "main",
    "abstract": "In anomaly detection, the degree of irregularity is often summarized as a real-valued anomaly score. We address the problem of attributing such anomaly scores to input features for interpreting the results of anomaly detection. We particularly investigate the use of the Shapley value for attributing anomaly scores of semi-supervised detection methods. We propose a characteristic function specifically designed for attributing anomaly scores. The idea is to approximate the absence of some features by locally minimizing the anomaly score with regard to the to-be-absent features. We examine the applicability of the proposed characteristic function and other general approaches for interpreting anomaly scores on multiple datasets and multiple anomaly detection methods. The results indicate the potential utility of the attribution methods including the proposed one",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QBMyDZsPMd": {
    "title": "The Open MatSci ML Toolkit: A Flexible Framework for Machine Learning in Materials Science",
    "volume": "main",
    "abstract": "We present the Open MatSci ML Toolkit: a flexible, self-contained, and scalable Python-based framework to apply deep learning models and methods on scientific data with a specific focus on materials science and the OpenCatalyst Dataset. Our toolkit provides: 1. A scalable machine learning workflow for materials science leveraging PyTorch Lightning, which enables seamless scaling across different computation capabilities (laptop, server, cluster) and hardware platforms (CPU, GPU, XPU). 2. Deep Graph Library (DGL) support for rapid graph neural network prototyping and development. By publishing and sharing this toolkit with the research community via open-source release, we hope to: 1. Lower the entry barrier for new machine learning researchers and practitioners that want to get started with the OpenCatalyst dataset, which presently comprises the largest computational materials science dataset. 2. Enable the scientific community to apply advanced machine learning tools to high-impact scientific challenges, such as modeling of materials behavior for clean energy applications. We demonstrate the capabilities of our framework by enabling three new equivariant neural network models for multiple OpenCatalyst tasks and arrive at promising results for compute scaling and model performance. The code of the framework and experiments presented in this is paper are publicly available at https://github.com/IntelLabs/matsciml",
    "checked": true,
    "id": "21f9ddb6f39b2678bf9266d63895cdf71906784e",
    "semantic_title": "the open matsci ml toolkit: a flexible framework for machine learning in materials science",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=mXfkKtu5JA": {
    "title": "Differentiable Logic Machines",
    "volume": "main",
    "abstract": "The integration of reasoning, learning, and decision-making is key to build more general artificial intelligence systems. As a step in this direction, we propose a novel neural-logic architecture, called differentiable logic machine (DLM), that can solve both inductive logic programming (ILP) and reinforcement learning (RL) problems, where the solution can be interpreted as a first-order logic program. Our proposition includes several innovations. Firstly, our architecture defines a restricted but expressive continuous relaxation of the space of first-order logic programs by assigning weights to predicates instead of rules, in contrast to most previous neural-logic approaches. Secondly, with this differentiable architecture, we propose several (supervised and RL) training procedures, based on gradient descent, which can recover a fully-interpretable solution (i.e., logic formula). Thirdly, to accelerate RL training, we also design a novel critic architecture that enables actor-critic algorithms. Fourthly, to solve hard problems, we propose an incremental training procedure that can learn a logic program progressively. Compared to state-of-the-art (SOTA) differentiable ILP methods, DLM successfully solves all the considered ILP problems with a higher percentage of successful seeds (up to 3.5x). On RL problems, without requiring an interpretable solution, DLM outperforms other non-interpretable neural-logic RL approaches in terms of rewards (up to 3.9%). When enforcing interpretability, DLM can solve harder RL problems (e.g., Sorting, Path) than other interpretable RL methods. Moreover, we show that deep logic programs can be learned via incremental supervised training. In addition to this excellent performance, DLM can scale well in terms of memory and computational time, especially during the testing phase where it can deal with much more constants (>2x) than SOTA",
    "checked": true,
    "id": "411356eb1c5f006c0f6f89d1c59bbcc0acba8d8d",
    "semantic_title": "differentiable logic machines",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=kdPcLdJbt1": {
    "title": "Vulnerability-Aware Instance Reweighting For Adversarial Training",
    "volume": "main",
    "abstract": "Adversarial Training (AT) has been found to substantially improve the robustness of deep learning classifiers against adversarial attacks. AT involves obtaining robustness by including adversarial examples in training a classifier. Most variants of AT algorithms treat every training example equally. However, recent works have shown that better performance is achievable by treating them unequally. In addition, it has been observed that AT exerts an uneven influence on different classes in a training set and unfairly hurts examples corresponding to classes that are inherently harder to classify. Consequently, various reweighting schemes have been proposed that assign unequal weights to robust losses of individual examples in a training set. In this work, we propose a novel instance-wise reweighting scheme. It considers the vulnerability of each natural example and the resulting information loss on its adversarial counterpart occasioned by adversarial attacks. Through extensive experiments, we show that our proposed method significantly improves over existing reweighting schemes, especially against strong white and black-box attacks",
    "checked": true,
    "id": "544a68888a72406b86732d84447b00b769994509",
    "semantic_title": "vulnerability-aware instance reweighting for adversarial training",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V7tahqGrOq": {
    "title": "Lifelong Reinforcement Learning with Modulating Masks",
    "volume": "main",
    "abstract": "Lifelong learning aims to create AI systems that continuously and incrementally learn during a lifetime, similar to biological learning. Attempts so far have met problems, including catastrophic forgetting, interference among tasks, and the inability to exploit previous knowledge. While considerable research has focused on learning multiple supervised classification tasks that involve changes in the input distribution, lifelong reinforcement learning (LRL) must deal with variations in the state and transition distributions, and in the reward functions. Modulating masks with a fixed backbone network, recently developed for classification, are particularly suitable to deal with such a large spectrum of task variations. In this paper, we adapted modulating masks to work with deep LRL, specifically PPO and IMPALA agents. The comparison with LRL baselines in both discrete and continuous RL tasks shows superior performance. We further investigated the use of a linear combination of previously learned masks to exploit previous knowledge when learning new tasks: not only is learning faster, the algorithm solves tasks that we could not otherwise solve from scratch due to extremely sparse rewards. The results suggest that RL with modulating masks is a promising approach to lifelong learning, to the composition of knowledge to learn increasingly complex tasks, and to knowledge reuse for efficient and faster learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ILNqQhGbLx": {
    "title": "Semantic Self-adaptation: Enhancing Generalization with a Single Sample",
    "volume": "main",
    "abstract": "The lack of out-of-domain generalization is a critical weakness of deep networks for semantic segmentation. Previous studies relied on the assumption of a static model, i. e., once the training process is complete, model parameters remain fixed at test time. In this work, we challenge this premise with a self-adaptive approach for semantic segmentation that adjusts the inference process to each input sample. Self-adaptation operates on two levels. First, it fine-tunes the parameters of convolutional layers to the input image using consistency regularization. Second, in Batch Normalization layers, self-adaptation interpolates between the training and the reference distribution derived from a single test sample. Despite both techniques being well known in the literature, their combination sets new state-of-the-art accuracy on synthetic-to-real generalization benchmarks. Our empirical study suggests that self-adaptation may complement the established practice of model regularization at training time for improving deep network generalization to out-of-domain data. Our code and pre-trained models are available at https://github.com/visinf/self-adaptive",
    "checked": true,
    "id": "dbf4423150f45eb3d0cc7e78d6530f95a73cc401",
    "semantic_title": "semantic self-adaptation: enhancing generalization with a single sample",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=MyQ1e1VQQ3": {
    "title": "Fair Kernel Regression through Cross-Covariance Operators",
    "volume": "main",
    "abstract": "Ensuring fairness in machine learning models is a difficult problem from both a formulation and implementation perspective. One sensible criterion for achieving fairness is Equalised Odds, which requires that subjects in protected and unprotected groups have equal true and false positive rates. However, practical implementation is challenging. This work proposes two ways to address this issue through the conditional independence operator. First, given the output values, it is used as a fairness measure of independence between model predictions and sensitive variables. Second, it is used as a regularisation term in the problem formulation, which seeks optimal models that balance performance and fairness concerning the sensitive variables. To illustrate the potential of our approach, we consider different scenarios. First, we use the Gaussian model to provide new insights into the problem formulation and numerical results on its convergence. Second, we present the formulation using the conditional cross-covariance operator. We anticipate that a closed-form solution is possible in the general problem formulation, including in the case of a kernel formulation setting. Third, we introduce a normalised criterion of the conditional independence operator. All formulations are posed under the risk minimisation principle, which leads to theoretical results on the performance. Additionally, insights are provided into using these operators under a Gaussian Process setting. Our methods are compared to state-of-the-art methods in terms of performance and fairness metrics on a representative set of real problems. The results obtained with our proposed methodology show promising performance-fairness curves. Furthermore, we discuss the usefulness of linear weights in the fair model to describe the behaviour of the features when enforcing fairness over a particular set of input features",
    "checked": true,
    "id": "662d30853e87a43be7a21ffa4d4e9983dd5bce4c",
    "semantic_title": "fair kernel regression through cross-covariance operators",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dpGSNLUCzu": {
    "title": "The Score-Difference Flow for Implicit Generative Modeling",
    "volume": "main",
    "abstract": "Implicit generative modeling (IGM) aims to produce samples of synthetic data matching the characteristics of a target data distribution. Recent work (e.g. score-matching networks, diffusion models) has approached the IGM problem from the perspective of pushing synthetic source data toward the target distribution via dynamical perturbations or flows in the ambient space. In this direction, we present the score difference (SD) between arbitrary target and source distributions as a flow that optimally reduces the Kullback-Leibler divergence between them while also solving the Schrödinger bridge problem. We apply the SD flow to convenient proxy distributions, which are aligned if and only if the original distributions are aligned. We demonstrate the formal equivalence of this formulation to denoising diffusion models under certain conditions. We also show that the training of generative adversarial networks includes a hidden data-optimization sub-problem, which induces the SD flow under certain choices of loss function when the discriminator is optimal. As a result, the SD flow provides a theoretical link between model classes that individually address the three challenges of the \"generative modeling trilemma\"—high sample quality, mode coverage, and fast sampling—thereby setting the stage for a unified approach",
    "checked": true,
    "id": "676b97f1fceae606b06f79e4d7396907732de74b",
    "semantic_title": "the score-difference flow for implicit generative modeling",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tLBjsX4tjs": {
    "title": "A Unified Perspective on Natural Gradient Variational Inference with Gaussian Mixture Models",
    "volume": "main",
    "abstract": "Variational inference with Gaussian mixture models (GMMs) enables learning of highly tractable yet multi-modal approximations of intractable target distributions with up to a few hundred dimensions. The two currently most effective methods for GMM-based variational inference, VIPS and iBayes-GMM, both employ independent natural gradient updates for the individual components and their weights. We show for the first time, that their derived updates are equivalent, although their practical implementations and theoretical guarantees differ. We identify several design choices that distinguish both approaches, namely with respect to sample selection, natural gradient estimation, stepsize adaptation, and whether trust regions are enforced or the number of components adapted. We argue that for both approaches, the quality of the learned approximations can heavily suffer from the respective design choices: By updating the individual components using samples from the mixture model, iBayes-GMM often fails to produce meaningful updates to low-weight components, and by using a zero-order method for estimating the natural gradient, VIPS scales badly to higher-dimensional problems. Furthermore, we show that information-geometric trust-regions (used by VIPS) are effective even when using first-order natural gradient estimates, and often outperform the improved Bayesian learning rule (iBLR) update used by iBayes-GMM. We systematically evaluate the effects of design choices and show that a hybrid approach significantly outperforms both prior works. Along with this work, we publish our highly modular and efficient implementation for natural gradient variational inference with Gaussian mixture models, which supports $432$ different combinations of design choices, facilitates the reproduction of all our experiments, and may prove valuable for the practitioner",
    "checked": true,
    "id": "2d5bad54c108443a8532a11febd50c1ce69ea9d5",
    "semantic_title": "a unified perspective on natural gradient variational inference with gaussian mixture models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=FbztvhdCX9": {
    "title": "On the Gradient Formula for learning Generative Models with Regularized Optimal Transport Costs",
    "volume": "main",
    "abstract": "Learning a Wasserstein Generative Adversarial Networks (WGAN) requires the differentiation of the optimal transport cost with respect to the parameters of the generative model. In this work, we provide sufficient conditions for the existence of a gradient formula in two different frameworks: the case of semi-discrete optimal transport (i.e. with a discrete target distribution) and the case of regularized optimal transport (i.e. with an entropic penalty). In both cases the gradient formula involves a solution of the semi-dual formulation of the optimal transport cost. Our study makes a connection between the gradient of the WGAN loss function and the Laguerre diagrams associated to semi-discrete transport maps. The learning problem is addressed with an alternating algorithm, which is in general not convergent. However, in most cases, it stabilizes close to a relevant solution for the generative learning problem. We also show that entropic regularization can improve the convergence speed but noticeably changes the shape of the learned generative model",
    "checked": true,
    "id": "a95b8fa116e60976e8256d124780c6cce17b8acc",
    "semantic_title": "on the gradient formula for learning generative models with regularized optimal transport costs",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HP7Qpui5YE": {
    "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
    "volume": "main",
    "abstract": "In this paper, we are interested in understanding self-supervised pretraining through studying the capability that self-supervised methods learn part-aware representations. The study is mainly motivated by that random views, used in contrastive learning, and random masked (visible) patches, used in masked image modeling, are often about object parts. We explain that contrastive learning is a part-to-whole task: the projection layer hallucinates the whole object representation from the object part representation learned from the encoder, and that masked image modeling is a part-to-part task: the masked patches of the object are hallucinated from the visible patches. The explanation suggests that the self-supervised pretrained encoder leans toward understanding the object part. We empirically compare the off-the-shelf encoders pretrained with several representative methods on object-level recognition and part-level recognition. The results show that the fully-supervised model outperforms self-supervised models for object-level recognition, and most self-supervised contrastive learning and masked image modeling methods outperform the fully-supervised method for part-level recognition. It is observed that the combination of contrastive learning and masked image modeling further improves the performance",
    "checked": true,
    "id": "fb28002574cf7e6f08a10e3fb39b8f05bc4bad7b",
    "semantic_title": "understanding self-supervised pretraining with part-aware representation learning",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=SaVEXFuozg": {
    "title": "DSpar: An Embarrassingly Simple Strategy for Efficient GNN training and inference via Degree-based Sparsification",
    "volume": "main",
    "abstract": "Running Graph Neural Networks (GNNs) on large graphs suffers from notoriously inefficiency. This is attributed to the sparse graph-based operations, which is hard to be accelerated by community hardware, e.g., GPUs and CPUs. One potential solution is to ``sketch'' the original graph by removing unimportant edges, then both the training and inference process are executed on the sparsified graph with improved efficiency. Traditional graph sparsification work calculates the edge importance score, i.e., effective resistance, from graph topology with theoretical guarantee. However, estimating effective resistance is even more expensive than training GNNs itself. Later, learning-based sparsification methods propose to learn the edge importance from data, but with significant overhead due to the extra learning process. Thus, both of them introduce significant ahead-of-training overhead. In this paper, we experimentally and theoretically prove that effective resistance can be approximated using only the node degree information and achieve similar node presentations on graph with/without sparsification. Based on this finding, we propose DSpar, to sparsify the graph once before training based on only the node degree information with negligible ahead-of-training overhead. In practice, for the training phase, DSpar achieves up to $5.9\\times$ faster than baseline with almost no accuracy drop. For the inference phase, DSpar reduces up to $90\\%$ latency",
    "checked": false,
    "id": "809e2aedcbc3703ab6fbf31450b6c115fe00efca",
    "semantic_title": "dspar : an embarrassingly simple strategy for efficient gnn training and inference via degree-based sparsification",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1QqIfGZOWu": {
    "title": "Challenges and Opportunities in Offline Reinforcement Learning from Visual Observations",
    "volume": "main",
    "abstract": "Offline reinforcement learning has shown great promise in leveraging large pre-collected datasets for policy learning, allowing agents to forgo often-expensive online data collection. However, offline reinforcement learning from visual observations with continuous action spaces remains under-explored, with a limited understanding of the key challenges in this complex domain. In this paper, we establish simple baselines for continuous control in the visual domain and introduce a suite of benchmarking tasks for offline reinforcement learning from visual observations designed to better represent the data distributions present in real-world offline RL problems and guided by a set of desiderata for offline RL from visual observations, including robustness to visual distractions and visually identifiable changes in dynamics. Using this suite of benchmarking tasks, we show that simple modifications to two popular vision-based online reinforcement learning algorithms, DreamerV2 and DrQ-v2, suffice to outperform existing offline RL methods and establish competitive baselines for continuous control in the visual domain. We rigorously evaluate these algorithms and perform an empirical evaluation of the differences between state-of-the-art model-based and model-free offline RL methods for continuous control from visual observations. All code and data used in this evaluation are open-sourced to facilitate progress in this domain",
    "checked": true,
    "id": "bbae3200de2d742b2bdcecab51f40a8dccb228cb",
    "semantic_title": "challenges and opportunities in offline reinforcement learning from visual observations",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=LEVbhNrLEL": {
    "title": "Mind the Gap: Mitigating the Distribution Gap in Graph Few-shot Learning",
    "volume": "main",
    "abstract": "Prevailing supervised deep graph learning models often suffer from the issue of label scarcity, leading to performance degradation in the face of limited annotated data. Although numerous graph few-shot learning (GFL) methods have been developed to mitigate this problem, they tend to rely excessively on labeled data. This over-reliance on labeled data can result in impaired generalization ability in the test phase due to the existence of a distribution gap. Moreover, existing GFL methods lack a general purpose as their designs are coupled with task or data-specific characteristics. To address these shortcomings, we propose a novel Self-Distilled Graph Few-shot Learning framework (SDGFL) that is both general and effective. SDGFL leverages a self-distilled contrastive learning procedure to boost GFL. Specifically, our model first pre-trains a graph encoder with contrastive learning using unlabeled data. Later, the trained encoder is frozen as a teacher model to distill a student model with a contrastive loss. The distilled model is then fed to GFL. By learning data representation in a self-supervised manner, SDGFL effectively mitigates the distribution gap and enhances generalization ability. Furthermore, our proposed framework is task and data-independent, making it a versatile tool for general graph mining purposes. To evaluate the effectiveness of our proposed framework, we introduce an information-based measurement that quantifies its capability. Through comprehensive experiments, we demonstrate that SDGFL outperforms state-of-the-art baselines on various graph mining tasks across multiple datasets in the few-shot scenario. We also provide a quantitative measurement of SDGFL's superior performance in comparison to existing methods",
    "checked": true,
    "id": "8c945d9ebce699687fe10ed9d6f2a3c4721842ba",
    "semantic_title": "mind the gap: mitigating the distribution gap in graph few-shot learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HqIuAzBxbh": {
    "title": "Consistent Collaborative Filtering via Tensor Decomposition",
    "volume": "main",
    "abstract": "Collaborative filtering is the de facto standard for analyzing users' activities and building recommendation systems for items. In this work we develop Sliced Anti-symmetric Decomposition (SAD), a new model for collaborative filtering based on implicit feedback. In contrast to traditional techniques where a latent representation of users (user vectors) and items (item vectors) are estimated, SAD introduces one additional latent vector to each item, using a novel three-way tensor view of user-item interactions. This new vector extends user-item preferences calculated by standard dot products to general inner products, producing interactions between items when evaluating their relative preferences. SAD reduces to state-of-the-art (SOTA) collaborative filtering models when the vector collapses to 1, while in this paper we allow its value to be estimated from data. Allowing the values of the new item vector to be different from 1 has profound implications. It suggests users may have nonlinear mental models when evaluating items, allowing the existence of cycles in pairwise comparisons. We demonstrate the efficiency of SAD in both simulated and real world datasets containing over 1M user-item interactions. By comparing with seven SOTA collaborative filtering models with implicit feedbacks, SAD produces the most consistent personalized preferences, in the meanwhile maintaining top-level of accuracy in personalized recommendations. We release the model and inference algorithms in a Python library https://github.com/apple/ml-sad",
    "checked": true,
    "id": "0dccc06f3f05b7174be9756142a795579f6c6eb5",
    "semantic_title": "consistent collaborative filtering via tensor decomposition",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jkTqJJOGMS": {
    "title": "Provably Convergent Policy Optimization via Metric-aware Trust Region Methods",
    "volume": "main",
    "abstract": "Trust-region methods based on Kullback-Leibler divergence are pervasively used to stabilize policy optimization in reinforcement learning. In this paper, we exploit more flexible metrics and examine two natural extensions of policy optimization with Wasserstein and Sinkhorn trust regions, namely Wasserstein policy optimization (WPO) and Sinkhorn policy optimization (SPO). Instead of restricting the policy to a parametric distribution class, we directly optimize the policy distribution and derive their close-form policy updates based on the Lagrangian duality. Theoretically, we show that WPO guarantees a monotonic performance improvement, and SPO provably converges to WPO as the entropic regularizer diminishes. Moreover, we prove that with a decaying Lagrangian multiplier to the trust region constraint, both methods converge to global optimality. Experiments across tabular domains, robotic locomotion, and continuous control tasks further demonstrate the performance improvement of both approaches, more robustness of WPO to sample insufficiency, and faster convergence of SPO, over state-of-art policy gradient methods",
    "checked": true,
    "id": "2732028656ffa7c240932d38a7e134747c2aefeb",
    "semantic_title": "provably convergent policy optimization via metric-aware trust region methods",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P6NcRPb13w": {
    "title": "Adjusting Machine Learning Decisions for Equal Opportunity and Counterfactual Fairness",
    "volume": "main",
    "abstract": "Machine learning (ML) methods have the potential to automate high-stakes decisions, such as bail admissions or credit lending, by analyzing and learning from historical data. But these algorithmic decisions may be unfair: in learning from historical data, they may replicate discriminatory practices from the past. In this paper, we propose two algorithms that adjust fitted ML predictors to produce decisions that are fair. Our methods provide post-hoc adjustments to the predictors, without requiring that they be retrained. We consider a causal model of the ML decisions, define fairness through counterfactual decisions within the model, and then form algorithmic decisions that capture the historical data as well as possible but are provably fair. In particular, we consider two definitions of fairness. The first is ``equal counterfactual opportunity,'' where the counterfactual distribution of the decision is the same regardless of the protected attribute; the second is counterfactual fairness. We evaluate the algorithms, and the trade-off between accuracy and fairness, on datasets about admissions, income, credit, and recidivism",
    "checked": true,
    "id": "cb39129d23d427b397d930bca1079a7b8d0176c6",
    "semantic_title": "adjusting machine learning decisions for equal opportunity and counterfactual fairness",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JJrKbq35l4": {
    "title": "On Average-Case Error Bounds for Kernel-Based Bayesian Quadrature",
    "volume": "main",
    "abstract": "In this paper, we study error bounds for Bayesian quadrature (BQ), with an emphasis on noisy settings, randomized algorithms, and average-case performance measures. We seek to approximate the integral of functions in a Reproducing Kernel Hilbert Space (RKHS), particularly focusing on the Mat\\'ern-$\\nu$ and squared exponential (SE) kernels, with samples from the function potentially being corrupted by Gaussian noise. We provide a two-step meta-algorithm that serves as a general tool for relating the average-case quadrature error with the $L^2$-function approximation error. When specialized to the Mat\\'ern kernel, we recover an existing near-optimal error rate while avoiding the existing method of repeatedly sampling points. When specialized to other settings, we obtain new average-case results for settings including the SE kernel with noise and the Mat\\'ern kernel with misspecification. Finally, we present algorithm-independent lower bounds that have greater generality and/or give distinct proofs compared to existing ones",
    "checked": true,
    "id": "013e187dbf20d5153198b74cef2382b8d47b5413",
    "semantic_title": "on average-case error bounds for kernel-based bayesian quadrature",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ThhMzfrd6r": {
    "title": "Self-Supervised Graph Representation Learning for Neuronal Morphologies",
    "volume": "main",
    "abstract": "Unsupervised graph representation learning has recently gained interest in several application domains such as neuroscience, where modeling the diverse morphology of cell types in the brain is one of the key challenges. It is currently unknown how many excitatory cortical cell types exist and what their defining morphological features are. Here we present GraphDINO, a purely data-driven approach to learn low-dimensional representations of 3D neuronal morphologies from unlabeled large-scale datasets. GraphDINO is a novel transformer-based representation learning method for spatially-embedded graphs. To enable self-supervised learning on transformers, we (1) developed data augmentation strategies for spatially-embedded graphs, (2) adapted the positional encoding and (3) introduced a novel attention mechanism, AC-Attention, which combines attention-based global interaction between nodes and classic graph convolutional processing. We show, in two different species and across multiple brain areas, that this method yields morphological cell type clusterings that are on par with manual feature-based classification by experts, but without using prior knowledge about the structural features of neurons. Moreover, it outperforms previous approaches on quantitative benchmarks predicting expert labels. Our method could potentially enable data-driven discovery of novel morphological features and cell types in large-scale datasets. It is applicable beyond neuroscience in settings where samples in a dataset are graphs and graph-level embeddings are desired",
    "checked": true,
    "id": "804f1bff8f76488c13657f492b5c9d2523ad3a45",
    "semantic_title": "self-supervised graph representation learning for neuronal morphologies",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=VV4zJwLwI7": {
    "title": "Breaking the Spurious Causality of Conditional Generation via Fairness Intervention with Corrective Sampling",
    "volume": "main",
    "abstract": "Trying to capture the sample-label relationship, conditional generative models often end up inheriting the spurious correlation in the training dataset, giving label-conditional distributions that are severely imbalanced in another latent attribute. To mitigate such undesirable correlations engraved into generative models, which we call spurious causality, we propose a general two-step strategy. (a) Fairness Intervention (FI): Emphasize the minority samples that are hard to be generated due to the spurious correlation in the training dataset. (b) Corrective Sampling (CS): Filter the generated samples explicitly to follow the desired label-conditional latent attribute distribution. We design the fairness intervention for various degrees of supervision on the spurious attribute, including unsupervised, weakly-supervised, and semi-supervised scenarios. Our experimental results show that the proposed FICS can successfully resolve the spurious correlation in generated samples on various datasets",
    "checked": true,
    "id": "a7cf8f52c7797f2015ab9600cfb4f2993379acb9",
    "semantic_title": "breaking the spurious causality of conditional generation via fairness intervention with corrective sampling",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=VpaXrBFYZ9": {
    "title": "Stochastic Constrained DRO with a Complexity Independent of Sample Size",
    "volume": "main",
    "abstract": "Distributionally Robust Optimization (DRO), as a popular method to train robust models against distribution shift between training and test sets, has received tremendous attention in recent years. In this paper, we propose and analyze stochastic algorithms that apply to both non-convex and convex losses for solving Kullback–Leibler divergence constrained DRO problem. Compared with existing methods solving this problem, our stochastic algorithms not only enjoy competitive if not better complexity independent of sample size but also just require a constant batch size at every iteration, which is more practical for broad applications. We establish a nearly optimal complexity bound for finding an $\\epsilon$-stationary solution for non-convex losses and an optimal complexity for finding an $\\epsilon$-optimal solution for convex losses. Empirical studies demonstrate the effectiveness of the proposed algorithms for solving non-convex and convex constrained DRO problems",
    "checked": true,
    "id": "a1382290160a39a7211d8291faa21f3833818061",
    "semantic_title": "stochastic constrained dro with a complexity independent of sample size",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=nddEHTSnqg": {
    "title": "Neural Networks beyond explainability: Selective inference for sequence motifs",
    "volume": "main",
    "abstract": "Over the past decade, neural networks have been successful at making predictions from biological sequences, especially in the context of regulatory genomics. As in other fields of deep learning, tools have been devised to extract features such as sequence motifs that can explain the predictions made by a trained network. Here we intend to go beyond explainable machine learning and introduce SEISM, a selective inference procedure to test the association between these extracted features and the predicted phenotype. In particular, we discuss how training a one-layer convolutional network is formally equivalent to selecting motifs maximizing some association score. We adapt existing sampling-based selective inference procedures by quantizing this selection over an infinite set to a large but finite grid. Finally,we show that sampling under a specific choice of parameters is sufficient to characterize the composite null hypothesis typically used for selective inference - a result that goes well beyond our particular framework. We illustrate the behavior of our method in terms of calibration, power and speed and discuss its power/speed trade-off with a simpler data-split strategy. SEISM paves the way to an easier analysis of neural networks used in regulatory genomics, and to more powerful methods for genome wide association studies (GWAS)",
    "checked": true,
    "id": "206b19e38788b5bb0ea8bedac2a1a8fd43f6375f",
    "semantic_title": "neural networks beyond explainability: selective inference for sequence motifs",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w36pqfaJ4t": {
    "title": "Dynamics Adapted Imitation Learning",
    "volume": "main",
    "abstract": "We consider Imitation Learning with dynamics variation between the expert demonstration (source domain) and the environment (target domain). Based on the popular framework of Adversarial Imitation Learning, we propose a novel algorithm – Dynamics Adapted Imitation Learning (DYNAIL), which incorporates the dynamics variation into the state-action occupancy measure matching as a regularization term. The dynamics variation is modeled by a pair of classifiers to distinguish between source dynamics and target dynamics. Theoretically, we provide an upper bound on the divergence between the learned policy and expert demonstrations in the source domain. Our error bound only depends on the expectation of the discrepancy between the source and target dynamics for the optimal policy in the target domain. The experiment evaluation validates that our method achieves superior results on high dimensional continuous control tasks, compared to existing imitation learning methods",
    "checked": true,
    "id": "e436df13213f288a3f398482b2a540f1c0602eb9",
    "semantic_title": "dynamics adapted imitation learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CkXOwlhf27": {
    "title": "A Proximal Algorithm for Sampling",
    "volume": "main",
    "abstract": "We study sampling problems associated with potentials that lack smoothness. The potentials can be either convex or non-convex. Departing from the standard smooth setting, the potentials are only assumed to be weakly smooth or non-smooth, or the summation of multiple such functions. We develop a sampling algorithm that resembles proximal algorithms in optimization for this challenging sampling task. Our algorithm is based on a special case of Gibbs sampling known as the alternating sampling framework (ASF). The key contribution of this work is a practical realization of the ASF based on rejection sampling for both non-convex and convex potentials that are not necessarily smooth. In almost all the cases of sampling considered in this work, our proximal sampling algorithm achieves a better complexity than all existing methods",
    "checked": true,
    "id": "be5a49e6dc47608a36cf8ac8f4ec80088e133023",
    "semantic_title": "a proximal algorithm for sampling",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=j3FK00HyfU": {
    "title": "The Meta-Evaluation Problem in Explainable AI: Identifying Reliable Estimators with MetaQuantus",
    "volume": "main",
    "abstract": "One of the unsolved challenges in the field of Explainable AI (XAI) is determining how to most reliably estimate the quality of an explanation method in the absence of ground truth explanation labels. Resolving this issue is of utmost importance as the evaluation outcomes generated by competing evaluation methods (or ``quality estimators''), which aim at measuring the same property of an explanation method, frequently present conflicting rankings. Such disagreements can be challenging for practitioners to interpret, thereby complicating their ability to select the best-performing explanation method. We address this problem through a meta-evaluation of different quality estimators in XAI, which we define as ``the process of evaluating the evaluation method''. Our novel framework, MetaQuantus, analyses two complementary performance characteristics of a quality estimator: its resilience to noise and reactivity to randomness, thus circumventing the need for ground truth labels. We demonstrate the effectiveness of our framework through a series of experiments, targeting various open questions in XAI such as the selection and hyperparameter optimisation of quality estimators. Our work is released under an open-source license (https://github.com/annahedstroem/MetaQuantus) to serve as a development tool for XAI- and Machine Learning (ML) practitioners to verify and benchmark newly constructed quality estimators in a given explainability context. With this work, we provide the community with clear and theoretically-grounded guidance for identifying reliable evaluation methods, thus facilitating reproducibility in the field",
    "checked": true,
    "id": "498cc16f23413c66b17b4bffc8475a4079cb312c",
    "semantic_title": "the meta-evaluation problem in explainable ai: identifying reliable estimators with metaquantus",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=LdSP6cvTS4": {
    "title": "Calibrating and Improving Graph Contrastive Learning",
    "volume": "main",
    "abstract": "Graph contrastive learning algorithms have demonstrated remarkable success in various applications such as node classification, link prediction, and graph clustering. However, in unsupervised graph contrastive learning, some contrastive pairs may contradict the truths in downstream tasks and thus the decrease of losses on these pairs undesirably harms the performance in the downstream tasks. To assess the discrepancy between the prediction and the ground-truth in the downstream tasks for these contrastive pairs, we adapt expected calibration error (ECE) to graph contrastive learning. The analysis of ECE motivates us to propose a novel regularization method, Contrast-Reg, to ensure that decreasing the contrastive loss leads to better performance in the downstream tasks. As a plug-in regularizer, Contrast-Reg effectively improves the performance of existing graph contrastive learning algorithms. We provide both theoretical and empirical results to demonstrate the effectiveness of Contrast-Reg in enhancing the generalizability of the Graph Neural Network (GNN) model and improving the performance of graph contrastive algorithms with different similarity definitions and encoder backbones across various downstream tasks",
    "checked": true,
    "id": "34cce045b2106decb208e25197619628859fa3c0",
    "semantic_title": "calibrating and improving graph contrastive learning",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=nfYwRIezvg": {
    "title": "DORA: Exploring Outlier Representations in Deep Neural Networks",
    "volume": "main",
    "abstract": "Deep Neural Networks (DNNs) excel at learning complex abstractions within their internal representations. However, the concepts they learn remain opaque, a problem that becomes particularly acute when models unintentionally learn spurious correlations. In this work, we present DORA (Data-agnOstic Representation Analysis), the first data-agnostic framework for analyzing the representational space of DNNs. Central to our framework is the proposed Extreme-Activation (EA) distance measure, which assesses similarities between representations by analyzing their activation patterns on data points that cause the highest level of activation. As spurious correlations often manifest in features of data that are anomalous to the desired task, such as watermarks or artifacts, we demonstrate that internal representations capable of detecting such artifactual concepts can be found by analyzing relationships within neural representations. We validate the EA metric quantitatively, demonstrating its effectiveness both in controlled scenarios and real-world applications. Finally, we provide practical examples from popular Computer Vision models to illustrate that representations identified as outliers using the EA metric often correspond to undesired and spurious concepts",
    "checked": true,
    "id": "93f691020a216dc3a6079ef04b422d8f3a215da0",
    "semantic_title": "dora: exploring outlier representations in deep neural networks",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=g97OHbQyk1": {
    "title": "The Vendi Score: A Diversity Evaluation Metric for Machine Learning",
    "volume": "main",
    "abstract": "Diversity is an important criterion for many areas of machine learning (ML), including generative modeling and dataset curation. However, existing metrics for measuring diversity are often domain-specific and limited in flexibility. In this paper we address the diversity evaluation problem by proposing the Vendi Score, which connects and extends ideas from ecology and quantum statistical mechanics to ml. The Vendi Score is defined as the exponential of the Shannon entropy of the eigenvalues of a similarity matrix. This matrix is induced by a user-defined similarity function applied to the sample to be evaluated for diversity. In taking a similarity function as input, the Vendi Score enables its user to specify any desired form of diversity. Importantly, unlike many existing metrics in ML, the Vendi Score does not require a reference dataset or distribution over samples or labels, it is therefore general and applicable to any generative model, decoding algorithm, and dataset from any domain where similarity can be defined. We showcase the Vendi Score on molecular generative modeling where we found it addresses shortcomings of the current diversity metric of choice in that domain. We also applied the Vendi Score to generative models of images and decoding algorithms of text where we found it confirms known results about diversity in those domains. Furthermore, we used the Vendi Score to measure mode collapse, a known shortcoming of generative adversarial networks (GANs). In particular, the Vendi Score revealed that even GANs that capture all the modes of a labelled dataset can be less diverse than the original dataset. Finally, the interpretability of the Vendi Score allowed us to diagnose several benchmark ML datasets for diversity, opening the door for diversity-informed data augmentation",
    "checked": true,
    "id": "b03c078303326ff022f525fccdf028b73ccb1cb4",
    "semantic_title": "the vendi score: a diversity evaluation metric for machine learning",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=OqbGu3hdQb": {
    "title": "Contextual Combinatorial Multi-output GP Bandits with Group Constraints",
    "volume": "main",
    "abstract": "In federated multi-armed bandit problems, maximizing global reward while satisfying minimum privacy requirements to protect clients is the main goal. To formulate such problems, we consider a combinatorial contextual bandit setting with groups and changing action sets, where similar base arms arrive in groups and a set of base arms, called a super arm, must be chosen in each round to maximize super arm reward while satisfying the constraints of the rewards of groups from which base arms were chosen. To allow for greater flexibility, we let each base arm have two outcomes, modeled as the output of a two-output Gaussian process (GP), where one outcome is used to compute super arm reward and the other for group reward. We then propose a novel double-UCB GP-bandit algorithm, called Thresholded Combinatorial Gaussian Process Upper Confidence Bounds (TCGP-UCB), which balances between maximizing cumulative super arm reward and satisfying group reward constraints and can be tuned to prefer one over the other. We also define a new notion of regret that combines super arm regret with group reward constraint satisfaction and prove that TCGP-UCB incurs $\\tilde{O}(\\sqrt{KT\\overline{\\gamma}_{T}} )$ regret with high probability, where $\\overline{\\gamma}_{T}$ is the maximum information gain associated with the set of base arm contexts that appeared in the first $T$ rounds and $K$ is the maximum super arm cardinality over all rounds. We lastly show in experiments using synthetic and real-world data and based on a federated learning setup as well as a content-recommendation one that our algorithm performs better then the current non-GP state-of-the-art combinatorial bandit algorithm, while satisfying group constraints",
    "checked": true,
    "id": "e514ef53e5e44559cb403bf3cb6a7cad57e0938e",
    "semantic_title": "contextual combinatorial multi-output gp bandits with group constraints",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Gbu1bHQhEL": {
    "title": "Active Acquisition for Multimodal Temporal Data: A Challenging Decision-Making Task",
    "volume": "main",
    "abstract": "We introduce a challenging decision-making task that we call active acquisition for multimodal temporal data (A2MT). In many real-world scenarios, input features are not readily available at test time and must instead be acquired at significant cost. With A2MT, we aim to learn agents that actively select which modalities of an input to acquire, trading off acquisition cost and predictive performance. A2MT extends a previous task called active feature acquisition to temporal decision making about high-dimensional inputs. We propose a method based on the Perceiver IO architecture to address A2MT in practice. Our agents are able to solve a novel synthetic scenario requiring practically relevant cross-modal reasoning skills. On two large-scale, real-world datasets, Kinetics-700 and AudioSet, our agents successfully learn cost-reactive acquisition behavior. However, an ablation reveals they are unable to learn adaptive acquisition strategies, emphasizing the difficulty of the task even for state-of-the-art models. Applications of A2MT may be impactful in domains like medicine, robotics, or finance, where modalities differ in acquisition cost and informativeness",
    "checked": true,
    "id": "56b35f7bca65566543469e79b04497e3d7436637",
    "semantic_title": "active acquisition for multimodal temporal data: a challenging decision-making task",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=gwRwHUZUgz": {
    "title": "Learning Symbolic Rules for Reasoning in Quasi-Natural Language",
    "volume": "main",
    "abstract": "Symbolic reasoning, rule-based symbol manipulation, is a hallmark of human intelligence. However, rule-based systems have had limited success competing with learning-based systems outside formalized domains such as automated theorem proving. We hypothesize that this is due to the manual construction of rules in past attempts. In this work, we take initial steps towards rule-based systems that can reason with natural language but without manually constructing rules. We propose MetaQNL, a \"Quasi-Natural Language\" that can express both formal logic and natural language sentences, and MetaInduce, a learning algorithm that induces MetaQNL rules from training data consisting of questions and answers, with or without intermediate reasoning steps. In addition, we introduce soft matching—a flexible mechanism for applying rules without rigid matching, overcoming a typical source of brittleness in symbolic reasoning. Our approach achieves state-of-the-art accuracies on multiple reasoning benchmarks; it learns compact models with much less data and produces not only answers but also checkable proofs. Further, experiments on two simple real-world datasets demonstrate the possibility for our method to handle noise and ambiguity",
    "checked": true,
    "id": "2cffec40f1d7cfc81d498b8939493243bbcadebe",
    "semantic_title": "learning symbolic rules for reasoning in quasi-natural language",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=XJIg4kQbkv": {
    "title": "CoCoFL: Communication- and Computation-Aware Federated Learning via Partial NN Freezing and Quantization",
    "volume": "main",
    "abstract": "Devices participating in federated learning (FL) typically have heterogeneous communication, computation, and memory resources. However, in synchronous FL, all devices need to finish training by the same deadline dictated by the server. Our results show that training a smaller subset of the neural network (NN) at constrained devices, i.e., dropping neurons/filters as proposed by state of the art, is inefficient, preventing these devices to make an effective contribution to the model. This causes unfairness w.r.t the achievable accuracies of constrained devices, especially in cases with a skewed distribution of class labels across devices. We present a novel FL technique, CoCoFL, which maintains the full NN structure on all devices. To adapt to the devices' heterogeneous resources, CoCoFL freezes and quantizes selected layers, reducing communication, computation, and memory requirements, whereas other layers are still trained in full precision, enabling to reach a high accuracy. Thereby, CoCoFL efficiently utilizes the available resources on devices and allows constrained devices to make a significant contribution to the FL system, preserving fairness among participants (accuracy parity) and significantly improving final accuracy",
    "checked": false,
    "id": "518747257702484d80d0d106185eec67c39a57ab",
    "semantic_title": "coco-fl: communication- and computation-aware federated learning via partial nn freezing and quantization",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=TdzQtbLeVw": {
    "title": "Online Min-max Problems with Non-convexity and Non-stationarity",
    "volume": "main",
    "abstract": "Online min-max optimization has recently gained considerable interest due to its rich applications to game theory, multi-agent reinforcement learning, online robust learning, etc. Theoretical understanding in this field has been mainly focused on convex-concave settings. Online min-max optimization with nonconvex geometries, which captures various online deep learning problems, has yet been studied so far. In this paper, we make the first effort and investigate online nonconvex-strongly-concave min-max optimization in the nonstationary environment. We first introduce a natural notion of local Nash equilibrium (NE)-regret, and then propose a novel algorithm coined TSODA to achieve the optimal regret. We further generalize our study to the setting with stochastic first-order feedback, and show that a variation of TSODA can also achieve the same optimal regret in expectation. Our theoretical results and the superior performance of the proposed method are further validated by empirical experiments. To our best knowledge, this is the first exploration of efficient online nonconvex min-max optimization",
    "checked": false,
    "id": "0cc96b88549fb81e161e446da8a067cedeb814f4",
    "semantic_title": "online min-max problems with non-convexity and nonstationarity",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LKz5SqIXPJ": {
    "title": "On the Robustness of Dataset Inference",
    "volume": "main",
    "abstract": "Machine learning (ML) models are costly to train as they can require a significant amount of data, computational resources and technical expertise. Thus, they constitute valuable intellectual property that needs protection from adversaries wanting to steal them. Ownership verification techniques allow the victims of model stealing attacks to demonstrate that a suspect model was in fact stolen from theirs. Although a number of ownership verification techniques based on watermarking or fingerprinting have been proposed, most of them fall short either in terms of security guarantees (well-equipped adversaries can evade verification) or computational cost. A fingerprinting technique, Dataset Inference (DI) has been shown to offer better robustness and efficiency than prior methods. The authors of DI provided a correctness proof for linear (suspect) models. However, in a subspace of the same setting, we prove that DI suffers from high false positives (FPs) -- it can incorrectly identify an independent model trained with non-overlapping data from the same distribution as stolen. We further prove that DI also triggers FPs in realistic, non-linear suspect models. We then confirm empirically that DI in the black-box setting leads to FPs, with high confidence. Second, we show that DI also suffers from false negatives (FNs) -- an adversary can fool DI by regularising a stolen model's decision boundaries using adversarial training, thereby leading to an FN. To this end, we demonstrate that black-box DI fails to identify a model adversarially trained from a stolen dataset -- the setting where DI is the hardest to evade. Finally, we discuss the implications of our findings, the viability of fingerprinting-based ownership verification in general, and suggest directions for future work",
    "checked": true,
    "id": "a46d1f8e5ed6182df8c864e72160372cf2a14b13",
    "semantic_title": "on the robustness of dataset inference",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=sY35BAiIf4": {
    "title": "Improving Differentially Private SGD via Randomly Sparsified Gradients",
    "volume": "main",
    "abstract": "Differentially private stochastic gradient descent (DP-SGD) has been widely adopted in deep learning to provide rigorously defined privacy, which requires gradient clipping to bound the maximum norm of individual gradients and additive isotropic Gaussian noise. With analysis of the convergence rate of DP-SGD in a non-convex setting, we identify that randomly sparsifying gradients before clipping and noisification adjusts a trade-off between internal components of the convergence bound and leads to a smaller upper bound when the noise is dominant. Additionally, our theoretical analysis and empirical evaluations show that the trade-off is not trivial but possibly a unique property of DP-SGD, as either canceling noisification or gradient clipping eliminates the trade-off in the bound. This observation is indicative, as it implies DP-SGD has special inherent room for (even simply random) gradient compression. To verify the observation an utilize it, we propose an efficient and lightweight extension using random sparsification (RS) to strengthen DP-SGD. Experiments with various DP-SGD frameworks show that RS can improve performance. Additionally, the produced sparse gradients of RS exhibit advantages in reducing communication cost and strengthening privacy against reconstruction attacks, which are also key problems in private machine learning",
    "checked": true,
    "id": "ba5b71313ab921d4c79626e537563c6e0614446f",
    "semantic_title": "improving differentially private sgd via randomly sparsified gradients",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=p28wv4G65d": {
    "title": "SC2 Benchmark: Supervised Compression for Split Computing",
    "volume": "main",
    "abstract": "With the increasing demand for deep learning models on mobile devices, splitting neural network computation between the device and a more powerful edge server has become an attractive solution. However, existing split computing approaches often underperform compared to a naive baseline of remote computation on compressed data. Recent studies propose learning compressed representations that contain more relevant information for supervised downstream tasks, showing improved tradeoffs between compressed data size and supervised performance. However, existing evaluation metrics only provide an incomplete picture of split computing. This study introduces supervised compression for split computing (SC2) and proposes new evaluation criteria: minimizing computation on the mobile device, minimizing transmitted data size, and maximizing model accuracy. We conduct a comprehensive benchmark study using 10 baseline methods, three computer vision tasks, and over 180 trained models, and discuss various aspects of SC2. We also release our code and sc2bench, a Python package for future research on SC2. Our proposed metrics and package will help researchers better understand the tradeoffs of supervised compression in split computing",
    "checked": true,
    "id": "5592106cb23e8958920a10b211921a96afe65199",
    "semantic_title": "sc2 benchmark: supervised compression for split computing",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=izL3B8dPx1": {
    "title": "Inherent Limits on Topology-Based Link Prediction",
    "volume": "main",
    "abstract": "Link prediction systems (e.g. recommender systems) typically use graph topology as one of their main sources of information. However, automorphisms and related properties of graphs beget inherent limits in predictability. We calculate hard upper bounds on how well graph topology alone enables link prediction for a wide variety of real-world graphs. We find that in the sparsest of these graphs the upper bounds are surprisingly low, thereby demonstrating that prediction systems on sparse graph data are inherently limited and require information in addition to the graph topology",
    "checked": true,
    "id": "8cebabc99da5ba488dadeae98626b38347ba6b49",
    "semantic_title": "inherent limits on topology-based link prediction",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uv32JOdQuh": {
    "title": "Invariant Feature Coding using Tensor Product Representation",
    "volume": "main",
    "abstract": "In this study, a novel feature coding method that exploits invariance for transformations represented by a finite group of orthogonal matrices is proposed. We prove that the group-invariant feature vector contains sufficient discriminative information when learning a linear classifier using convex loss minimization. Based on this result, a novel feature model that explicitly considers group action is proposed for principal component analysis and k-means clustering, which are commonly used in most feature coding methods, and global feature functions. Although the global feature functions are in general complex nonlinear functions, the group action on this space can be easily calculated by constructing these functions as tensor-product representations of basic representations, resulting in an explicit form of invariant feature functions. The effectiveness of our method is demonstrated on several image datasets",
    "checked": false,
    "id": "a42f518428bc91d100f323611ddc1179e063feea",
    "semantic_title": "quantum state preparation using tensor networks",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=wk8oXR0kFA": {
    "title": "Releasing Graph Neural Networks with Differential Privacy Guarantees",
    "volume": "main",
    "abstract": "With the increasing popularity of graph neural networks (GNNs) in several sensitive applications like healthcare and medicine, concerns have been raised over the privacy aspects of trained GNNs. More notably, GNNs are vulnerable to privacy attacks, such as membership inference attacks, even if only black-box access to the trained model is granted. We propose PRIVGNN, a privacy-preserving framework for releasing GNN models in a centralized setting. Assuming an access to a public unlabeled graph, PRIVGNN provides a framework to release GNN models trained explicitly on public data along with knowledge obtained from the private data in a privacy preserving manner. PRIVGNN combines the knowledge-distillation framework with the two noise mechanisms, random subsampling, and noisy labeling, to ensure rigorous privacy guarantees. We theoretically analyze our approach in the Rènyi differential privacy framework. Besides, we show the solid experimental performance of our method compared to several baselines adapted for graph-structured data. Our code is available at https://github.com/iyempissy/privGnn",
    "checked": true,
    "id": "44fb93ce9016fbc1deb3fe0c5098e8707c0ca77d",
    "semantic_title": "releasing graph neural networks with differential privacy guarantees",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=ERqGqZzSu5": {
    "title": "Sequential Query Encoding for Complex Query Answering on Knowledge Graphs",
    "volume": "main",
    "abstract": "Complex Query Answering (CQA) is an important and fundamental task for knowledge graph (KG) reasoning. Query encoding (QE) is proposed as a fast and robust solution to CQA. In the encoding process, most existing QE methods first parse the logical query into an executable computational direct-acyclic graph (DAG), then use neural networks to parameterize the operators, and finally, recursively execute these neuralized operators. However, the parameterization-and-execution paradigm may be potentially over-complicated, as it can be structurally simplified by a single neural network encoder. Meanwhile, sequence encoders, like LSTM and Transformer, proved to be effective for encoding semantic graphs in related tasks. Motivated by this, we propose sequential query encoding (SQE) as an alternative to encode queries for CQA. Instead of parameterizing and executing the computational graph, SQE first uses a search-based algorithm to linearize the computational graph to a sequence of tokens and then uses a sequence encoder to compute its vector representation. Then this vector representation is used as a query embedding to retrieve answers from the embedding space according to similarity scores. Despite its simplicity, SQE demonstrates state-of-the-art neural query encoding performance on FB15k, FB15k-237, and NELL on an extended benchmark including twenty-nine types of in-distribution queries. Further experiment shows that SQE also demonstrates comparable knowledge inference capability on out-of-distribution queries, whose query types are not observed during the training process",
    "checked": true,
    "id": "b45ea6ddc9d964116addaf1dafd0641d78a6228e",
    "semantic_title": "sequential query encoding for complex query answering on knowledge graphs",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=sFk3aBNb81": {
    "title": "TransFool: An Adversarial Attack against Neural Machine Translation Models",
    "volume": "main",
    "abstract": "Deep neural networks have been shown to be vulnerable to small perturbations of their inputs, known as adversarial attacks. In this paper, we investigate the vulnerability of Neural Machine Translation (NMT) models to adversarial attacks and propose a new attack algorithm called TransFool. To fool NMT models, TransFool builds on a multi-term optimization problem and a gradient projection step. By integrating the embedding representation of a language model, we generate fluent adversarial examples in the source language that maintain a high level of semantic similarity with the clean samples. Experimental results demonstrate that, for different translation tasks and NMT architectures, our white-box attack can severely degrade the translation quality while the semantic similarity between the original and the adversarial sentences stays high. Moreover, we show that TransFool is transferable to unknown target models. Finally, based on automatic and human evaluations, TransFool leads to improvement in terms of success rate, semantic similarity, and fluency compared to the existing attacks both in white-box and black-box settings. Thus, TransFool permits us to better characterize the vulnerability of NMT models and outlines the necessity to design strong defense mechanisms and more robust NMT systems for real-life applications",
    "checked": true,
    "id": "c5e9fdb1b81edf470141843c44264a5eb1ff0cc1",
    "semantic_title": "transfool: an adversarial attack against neural machine translation models",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=9pWjgQ3y85": {
    "title": "An Explicit Expansion of the Kullback-Leibler Divergence along its Fisher-Rao Gradient Flow",
    "volume": "main",
    "abstract": "Let $V_* : \\mathbb{R}^d \\to \\mathbb{R}$ be some (possibly non-convex) potential function, and consider the probability measure $\\pi \\propto e^{-V_*}$. When $\\pi$ exhibits multiple modes, it is known that sampling techniques based on Wasserstein gradient flows of the Kullback-Leibler (KL) divergence (e.g. Langevin Monte Carlo) suffer poorly in the rate of convergence, where the dynamics are unable to easily traverse between modes. In stark contrast, the work of Lu et al. (2019; 2022) has shown that the gradient flow of the KL with respect to the Fisher-Rao (FR) geometry exhibits a convergence rate to $\\pi$ is that \\textit{independent} of the potential function. In this short note, we complement these existing results in the literature by providing an explicit expansion of $\\text{KL}(\\rho_t^{\\text{FR}}\\|\\pi)$ in terms of $e^{-t}$, where $(\\rho_t^{\\text{FR}})_{t\\geq 0}$ is the FR gradient flow of the KL divergence. In turn, we are able to provide a clean asymptotic convergence rate, where the burn-in time is guaranteed to be finite. Our proof is based on observing a similarity between FR gradient flows and simulated annealing with linear scaling, and facts about cumulant generating functions. We conclude with simple synthetic experiments that demonstrate our theoretical findings are indeed tight. Based on our numerical findings, we conjecture that the asymptotic rates of convergence for Wasserstein-Fisher-Rao gradient flows are possibly related to this expansion in some cases",
    "checked": true,
    "id": "8570695c0ee3574f1a0829f0fe4672de634acb1b",
    "semantic_title": "an explicit expansion of the kullback-leibler divergence along its fisher-rao gradient flow",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=ZoXi7n54OB": {
    "title": "Training with Mixed-Precision Floating-Point Assignments",
    "volume": "main",
    "abstract": "When training deep neural networks, keeping all tensors in high precision (e.g., 32-bit or even 16-bit floats) is often wasteful. However, keeping all tensors in low precision (e.g., 8-bit floats) can lead to unacceptable accuracy loss. Hence, it is important to use a precision assignment—a mapping from all tensors (arising in training) to precision levels (high or low)—that keeps most of the tensors in low precision and leads to sufficiently accurate models. We provide a technique that explores this memory-accuracy tradeoff by generating precision assignments for convolutional neural networks that (i) use less memory and (ii) lead to more accurate convolutional networks at the same time, compared to the precision assignments considered by prior work in low-precision floating-point training. We evaluate our technique on image classiﬁcation tasks by training convolutional networks on CIFAR-10, CIFAR-100, and ImageNet. Our method typically provides > 2× memory reduction over a baseline precision assignment while preserving training accuracy, and gives further reductions by trading off accuracy. Compared to other baselines which sometimes cause training to diverge, our method provides similar or better memory reduction while avoiding divergence",
    "checked": true,
    "id": "6d06cd10665360739317c288ca25c68699611926",
    "semantic_title": "training with mixed-precision floating-point assignments",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A1N2qp4yAq": {
    "title": "Bandwidth Enables Generalization in Quantum Kernel Models",
    "volume": "main",
    "abstract": "Quantum computers are known to provide speedups over classical state-of-the-art machine learning methods in some specialized settings. For example, quantum kernel methods have been shown to provide an exponential speedup on a learning version of the discrete logarithm problem. Understanding the generalization of quantum models is essential to realizing similar speedups on problems of practical interest. Recent results demonstrate that generalization is hindered by the exponential size of the quantum feature space. Although these results suggest that quantum models cannot generalize when the number of qubits is large, in this paper we show that these results rely on overly restrictive assumptions. We consider a wider class of models by varying a hyperparameter that we call quantum kernel bandwidth. We analyze the large-qubit limit and provide explicit formulas for the generalization of a quantum model that can be solved in closed form. Specifically, we show that changing the value of the bandwidth can take a model from provably not being able to generalize to any target function to good generalization for well-aligned targets. Our analysis shows how the bandwidth controls the spectrum of the kernel integral operator and thereby the inductive bias of the model. We demonstrate empirically that our theory correctly predicts how varying the bandwidth affects generalization of quantum models on challenging datasets, including those far outside our theoretical assumptions. We discuss the implications of our results for quantum advantage in machine learning",
    "checked": true,
    "id": "67557d52497b2805a840083564ee2596ef609042",
    "semantic_title": "bandwidth enables generalization in quantum kernel models",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=vTsfup5ll6": {
    "title": "Privacy-Preserving Energy-Based Generative Models for Marginal Distribution Protection",
    "volume": "main",
    "abstract": "We consider learning generative models for sensitive financial and healthcare data. While previous work incorporates Differential Privacy (DP) into GAN training to protect the privacy of individual training instances, we consider a different privacy context where the primary objective is protecting the privacy of sensitive marginal distributions of the true generative process. We propose and motivate a new notion of privacy: \\emph{$\\alpha$-Level Marginal Distribution Privacy} ($\\alpha$-LMDP), which provides a statistical guarantee that the sensitive generative marginal distributions are different from the observed real data. We then propose \\emph{Privacy-Preserving Energy Models (PPEMs)}, a novel energy-based generative model formulation where the representations for these attributes are isolated from other attributes. This structured formulation motivates a learning procedure where a penalty based on a statistical goodness of fit test, the \\emph{Kernel Stein Discrepancy}, can be applied to only the attributes requiring privacy so that $\\alpha$-LMDP may be satisfied without affecting the other attributes. We evaluate this approach using financial and healthcare datasets and demonstrate that the resulting learnt generative models produce high fidelity synthetic data while preserving privacy. We also show that PPEMs can incorporate both $\\alpha$-LMDP \\emph{and} DP in contexts where both forms of privacy are required",
    "checked": true,
    "id": "c27a01ff464bcc72124b6901ab3b5e2c0f517d98",
    "semantic_title": "privacy-preserving energy-based generative models for marginal distribution protection",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=B7PFZtm8DA": {
    "title": "Unsupervised Discovery and Composition of Object Light Fields",
    "volume": "main",
    "abstract": "Neural scene representations, both continuous and discrete, have recently emerged as a powerful new paradigm for 3D scene understanding. Recent efforts have tackled unsupervised discovery of object-centric neural scene representations. However, the high cost of ray-marching, exacerbated by the fact that each object representation has to be ray-marched separately, leads to insufficiently sampled radiance fields and thus, noisy renderings, poor framerates, and high memory and time complexity during training and rendering. Here, we propose to represent objects in an object-centric, compositional scene representation as light fields. We propose a novel light field compositor module that enables reconstructing the global light field from a set of object-centric light fields. Dubbed Compositional Object Light Fields (COLF), our method enables unsupervised learning of object-centric neural scene representations, state-of-the-art reconstruction and novel view synthesis performance on standard datasets, and rendering and training speeds at orders of magnitude faster than existing 3D approaches",
    "checked": true,
    "id": "4fa77b26bffc65a1baad55248fd7df06e46bc58e",
    "semantic_title": "unsupervised discovery and composition of object light fields",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=dXnccpSSYF": {
    "title": "Pareto Optimization for Active Learning under Out-of-Distribution Data Scenarios",
    "volume": "main",
    "abstract": "Pool-based Active Learning (AL) has proven successful in minimizing labeling costs by sequentially selecting the most informative unlabeled data from large pool and querying their labels from an oracle or annotators. However, existing AL sampling schemes may not perform well in out-of-distribution (OOD) data scenarios, where the unlabeled data pool contains samples that do not belong to the pre-defined categories of the target task. Achieving strong AL performance under OOD data scenarios presents a challenge due to the inherent conflict between AL sampling strategies and OOD data detection. For instance, both more informative in-distribution (ID) data and OOD data in an unlabeled data pool would be assigned high informativeness scores (e.g., high entropy) during AL processes. To address this dilemma, we propose a Monte-Carlo Pareto Optimization for Active Learning (POAL) sampling scheme, which selects optimal subsets of unlabeled samples with fixed batch size from the unlabeled data pool. We formulate the AL sampling task as a multi-objective optimization problem and employ Pareto optimization based on two conflicting objectives: (1) the conventional AL sampling scheme (e.g., maximum entropy) and (2) the confidence of excluding OOD data samples. Experimental results demonstrate the effectiveness of our POAL approach on classical Machine Learning (ML) and Deep Learning (DL) tasks",
    "checked": true,
    "id": "e702416f7a9f62bf275c35a58a08839d65211733",
    "semantic_title": "pareto optimization for active learning under out-of-distribution data scenarios",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=jYkWdJzTwn": {
    "title": "Predicting Out-of-Domain Generalization with Neighborhood Invariance",
    "volume": "main",
    "abstract": "Developing and deploying machine learning models safely depends on the ability to char- acterize and compare their abilities to generalize to new environments. Although recent work has proposed a variety of methods that can directly predict or theoretically bound the generalization capacity of a model, they rely on strong assumptions such as matching train/test distributions and access to model gradients. In order to characterize generalization when these assumptions are not satisfied, we propose neighborhood invariance, a measure of a classifier's output invariance in a local transformation neighborhood. Specifically, we sample a set of transformations and given an input test point, calculate the invariance as the largest fraction of transformed points classified into the same class. Crucially, our measure is simple to calculate, does not depend on the test point's true label, makes no assumptions about the data distribution or model, and can be applied even in out-of-domain (OOD) settings where existing methods cannot, requiring only selecting a set of appropriate data transformations. In experiments on robustness benchmarks in image classification, sentiment analysis, and natural language inference, we demonstrate a strong and robust correlation between our neighborhood invariance measure and actual OOD generalization on over 4,600 models evaluated on over 100 train/test domain pairs",
    "checked": true,
    "id": "1faa156ed77ecf0204d6a83d4dd7114943cc57ae",
    "semantic_title": "predicting out-of-domain generalization with neighborhood invariance",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=ipe0IMglFF": {
    "title": "Empirical Study on Optimizer Selection for Out-of-Distribution Generalization",
    "volume": "main",
    "abstract": "Modern deep learning systems do not generalize well when the test data distribution is slightly different to the training data distribution. While much promising work has been accomplished to address this fragility, a systematic study of the role of optimizers and their out-of-distribution generalization performance has not been undertaken. In this study, we examine the performance of popular first-order optimizers for different classes of distributional shift under empirical risk minimization and invariant risk minimization. We address this question for image and text classification using DomainBed, WILDS, and Backgrounds Challenge as testbeds for studying different types of shifts---namely correlation and diversity shift. We search over a wide range of hyperparameters and examine classification accuracy (in-distribution and out-of-distribution) for over 20,000 models. We arrive at the following findings, which we expect to be helpful for practitioners: i) adaptive optimizers (e.g., Adam) perform worse than non-adaptive optimizers (e.g., SGD, momentum SGD) on out-of-distribution performance. In particular, even though there is no significant difference in in-distribution performance, we show a measurable difference in out-of-distribution performance. ii) in-distribution performance and out-of-distribution performance exhibit three types of behavior depending on the dataset---linear returns, increasing returns, and diminishing returns. For example, in the training of natural language data using Adam, fine-tuning the performance of in-distribution performance does not significantly contribute to the out-of-distribution generalization performance",
    "checked": true,
    "id": "9d99e651212de629568de2f2dcaa0856fe4c0bbe",
    "semantic_title": "empirical study on optimizer selection for out-of-distribution generalization",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=FDbQGCAViI": {
    "title": "The Eigenlearning Framework: A Conservation Law Perspective on Kernel Ridge Regression and Wide Neural Networks",
    "volume": "main",
    "abstract": "We derive simple closed-form estimates for the test risk and other generalization metrics of kernel ridge regression (KRR). Relative to prior work, our derivations are greatly simplified and our final expressions are more readily interpreted. In particular, we show that KRR can be interpreted as an explicit competition among kernel eigenmodes for a fixed supply of a quantity we term \"learnability.'' These improvements are enabled by a sharp conservation law which limits the ability of KRR to learn any orthonormal basis of functions. Test risk and other objects of interest are expressed transparently in terms of our conserved quantity evaluated in the kernel eigenbasis. We use our improved framework to: i) provide a theoretical explanation for the \"deep bootstrap\" of Nakkiran et al (2020), ii) generalize a previous result regarding the hardness of the classic parity problem, iii) fashion a theoretical tool for the study of adversarial robustness, and iv) draw a tight analogy between KRR and a well-studied system in statistical physics",
    "checked": false,
    "id": "79f95d13d8430b2e31cb8a8104c0455c6995a259",
    "semantic_title": "the eigenlearning framework: a conservation law perspective on kernel regression and wide neural networks",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=kiPsMct7vL": {
    "title": "Unsupervised Domain Adaptation via Minimized Joint Error",
    "volume": "main",
    "abstract": "Unsupervised domain adaptation transfers knowledge from a fully labeled source domain to a different target domain, where no labeled data are available. Some researchers have proposed upper bounds for the target error when transferring knowledge. For example, Ben-David et al. (2010) established a theory based on minimizing the source error and distance between marginal distributions simultaneously. However, in most research, the joint error is ignored because of its intractability. In this research, we argue that joint errors are essential for domain adaptation problems, particularly when the domain gap is large. To address this problem, we propose a novel objective related to the upper bound of the joint error. Moreover, we adopt a source/pseudo-target label-induced hypothesis space that can reduce the search space to further tighten this bound. To measure the dissimilarity between hypotheses, we define a novel cross-margin discrepancy to alleviate instability during adversarial learning. In addition, we present extensive empirical evidence showing that the proposed method boosts the performance of image classification accuracy on standard domain adaptation benchmarks",
    "checked": true,
    "id": "d8fcfb51844125476ebcaf2ffe53352e06d5dd89",
    "semantic_title": "unsupervised domain adaptation via minimized joint error",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r6oHDYOZ6p": {
    "title": "Undersampling is a Minimax Optimal Robustness Intervention in Nonparametric Classification",
    "volume": "main",
    "abstract": "While a broad range of techniques have been proposed to tackle distribution shift, the simple baseline of training on an undersampled balanced dataset often achieves close to state-of-the-art-accuracy across several popular benchmarks. This is rather surprising, since undersampling algorithms discard excess majority group data. To understand this phenomenon, we ask if learning is fundamentally constrained by a lack of minority group samples. We prove that this is indeed the case in the setting of nonparametric binary classification. Our results show that in the worst case, an algorithm cannot outperform undersampling unless there is a high degree of overlap between the train and test distributions (which is unlikely to be the case in real-world datasets), or if the algorithm leverages additional structure about the distribution shift. In particular, in the case of label shift we show that there is always an undersampling algorithm that is minimax optimal. In the case of group-covariate shift we show that there is an undersampling algorithm that is minimax optimal when the overlap between the group distributions is small. We also perform an experimental case study on a label shift dataset and find that in line with our theory, the test accuracy of robust neural network classifiers is constrained by the number of minority samples",
    "checked": true,
    "id": "598fe230576ff15e637be2dc5bf420ecba5d8688",
    "semantic_title": "undersampling is a minimax optimal robustness intervention in nonparametric classification",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=K0CAGgjYS1": {
    "title": "On the Convergence and Calibration of Deep Learning with Differential Privacy",
    "volume": "main",
    "abstract": "Differentially private (DP) training preserves the data privacy usually at the cost of slower convergence (and thus lower accuracy), as well as more severe mis-calibration than its non-private counterpart. To analyze the convergence of DP training, we formulate a continuous time analysis through the lens of neural tangent kernel (NTK), which characterizes the per-sample gradient clipping and the noise addition in DP training, for arbitrary network architectures and loss functions. Interestingly, we show that the noise addition only affects the privacy risk but not the convergence or calibration, whereas the per-sample gradient clipping (under both flat and layerwise clipping styles) only affects the convergence and calibration. Furthermore, we observe that while DP models trained with small clipping norm usually achieve the best accurate, but are poorly calibrated and thus unreliable. In sharp contrast, DP models trained with large clipping norm enjoy the same privacy guarantee and similar accuracy, but are significantly more \\textit{calibrated}. Our code can be found at https://github.com/woodyx218/opacus_global_clipping",
    "checked": true,
    "id": "a7d7db2320436a90dac4fb10a50dc0500952e7a6",
    "semantic_title": "on the convergence and calibration of deep learning with differential privacy",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=B0WYWvVA2r": {
    "title": "Attentional-Biased Stochastic Gradient Descent",
    "volume": "main",
    "abstract": "In this paper, we present a simple yet effective provable method (named ABSGD) for addressing the data imbalance or label noise problem in deep learning. Our method is a simple modification to momentum SGD where we assign an individual importance weight to each sample in the mini-batch. The individual-level weight of a sampled data is systematically proportional to the exponential of a scaled loss value of the data, where the scaling factor is interpreted as the regularization parameter in the framework of distributionally robust optimization (DRO). Depending on whether the scaling factor is positive or negative, ABSGD is guaranteed to converge to a stationary point of an information-regularized min-max or min-min DRO problem, respectively. Compared with existing class-level weighting schemes, our method can capture the diversity between individual examples within each class. Compared with existing individual-level weighting methods using meta-learning that require three backward propagations for computing mini-batch stochastic gradients, our method is more efficient with only one backward propagation at each iteration as in standard deep learning methods. ABSGD is flexible enough to combine with other robust losses without any additional cost. Our empirical studies on several benchmark datasets demonstrate the effectiveness of the proposed method",
    "checked": false,
    "id": "02bde59697f537813276dc86ee9d963c3bdd2452",
    "semantic_title": "stability and generalization of decentralized stochastic gradient descent",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=G2GKiicaJI": {
    "title": "Reinforcement Teaching",
    "volume": "main",
    "abstract": "Machine learning algorithms learn to solve a task, but are unable to improve their ability to learn. Meta-learning methods learn about machine learning algorithms and improve them so that they learn more quickly. However, existing meta-learning methods are either hand-crafted to improve one specific component of an algorithm or only work with differentiable algorithms. We develop a unifying meta-learning framework, called \\textit{Reinforcement Teaching}, to improve the learning process of \\emph{any} algorithm. Under Reinforcement Teaching, a teaching policy is learned, through reinforcement, to improve a student's learning algorithm. To learn an effective teaching policy, we introduce the \\textit{parametric-behavior embedder} that learns a representation of the student's learnable parameters from its input/output behavior. We further use \\textit{learning progress} to shape the teacher's reward, allowing it to more quickly maximize the student's performance. To demonstrate the generality of Reinforcement Teaching, we conduct experiments in which a teacher learns to significantly improve both reinforcement and supervised learning algorithms. Reinforcement Teaching outperforms previous work using heuristic reward functions and state representations, as well as other parameter representations",
    "checked": true,
    "id": "fa71320d3ef00075f4ddd6df0d328d00016377b0",
    "semantic_title": "reinforcement teaching",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zshemTAa6U": {
    "title": "Test-Time Adaptation for Visual Document Understanding",
    "volume": "main",
    "abstract": "For visual document understanding (VDU), self-supervised pretraining has been shown to successfully generate transferable representations, yet, effective adaptation of such representations to distribution shifts at test-time remains to be an unexplored area. We propose DocTTA, a novel test-time adaptation method for documents, that does source-free domain adaptation using unlabeled target document data. DocTTA leverages cross-modality self-supervised learning via masked visual language modeling, as well as pseudo labeling to adapt models learned on a \\textit{source} domain to an unlabeled \\textit{target} domain at test time. We introduce new benchmarks using existing public datasets for various VDU tasks, including entity recognition, key-value extraction, and document visual question answering. DocTTA shows significant improvements on these compared to the source model performance, up to 1.89\\% in (F1 score), 3.43\\% (F1 score), and 17.68\\% (ANLS score), respectively",
    "checked": true,
    "id": "2ab8d3a547d6a806d75332bae0915d4f37a41d1e",
    "semantic_title": "test-time adaptation for visual document understanding",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=W98AEKQ38Y": {
    "title": "Learning to Incentivize Improvements from Strategic Agents",
    "volume": "main",
    "abstract": "Machine learning systems are often used in settings where individuals adapt their features to obtain a desired outcome. In such settings, strategic behavior leads to a sharp loss in model performance in deployment. In this work, we aim to address this problem by learning classifiers that encourage decision subjects to change their features in a way that leads to improvement in both predicted and true outcome. We frame the dynamics of prediction and adaptation as a two-stage game, and characterize optimal strategies for the model designer and its decision subjects. In benchmarks on simulated and real-world datasets, we find that classifiers trained using our method maintain the accuracy of existing approaches while inducing higher levels of improvement and less manipulation",
    "checked": true,
    "id": "a2a0667a08b3c043b2e480786321b8f6039f1084",
    "semantic_title": "learning to incentivize improvements from strategic agents",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TSy0vuwQFN": {
    "title": "Finding Competence Regions in Domain Generalization",
    "volume": "main",
    "abstract": "We investigate a \"learning to reject\" framework to address the problem of silent failures in Domain Generalization (DG), where the test distribution differs from the training distribution. Assuming a mild distribution shift, we wish to accept out-of-distribution (OOD) data from a new domain whenever a model's estimated competence foresees trustworthy responses, instead of rejecting OOD data outright. Trustworthiness is then predicted via a proxy incompetence score that is tightly linked to the performance of a classifier. We present a comprehensive experimental evaluation of existing proxy scores as incompetence scores for classification and highlight the resulting trade-offs between rejection rate and accuracy gain. For comparability with prior work, we focus on standard DG benchmarks and consider the effect of measuring incompetence via different learned representations in a closed versus an open world setting. Our results suggest that increasing incompetence scores are indeed predictive of reduced accuracy, leading to significant improvements of the average accuracy below a suitable incompetence threshold. However, the scores are not yet good enough to allow for a favorable accuracy/rejection trade-off in all tested domains. Surprisingly, our results also indicate that classifiers optimized for DG robustness do not outperform a naive Empirical Risk Minimization (ERM) baseline in the competence region, that is, where test samples elicit low incompetence scores",
    "checked": true,
    "id": "376fd16949012d585fa4482540cbcdae8cf6ad5d",
    "semantic_title": "finding competence regions in domain generalization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r7imkFEAQb": {
    "title": "Noise-robust Graph Learning by Estimating and Leveraging Pairwise Interactions",
    "volume": "main",
    "abstract": "Teaching Graph Neural Networks (GNNs) to accurately classify nodes under severely noisy labels is an important problem in real-world graph learning applications, but is currently underexplored. Although pairwise training methods have demonstrated promise in supervised metric learning and unsupervised contrastive learning, they remain less studied on noisy graphs, where the structural pairwise interactions (PI) between nodes are abundant and thus might benefit label noise learning rather than the pointwise methods. This paper bridges the gap by proposing a pairwise framework for noisy node classification on graphs, which relies on the PI as a primary learning proxy in addition to the pointwise learning from the noisy node class labels. Our proposed framework PI-GNN contributes two novel components: (1) a confidence-aware PI estimation model that adaptively estimates the PI labels, which are defined as whether the two nodes share the same node labels, and (2) a decoupled training approach that leverages the estimated PI labels to regularize a node classification model for robust node classification. Extensive experiments on different datasets and GNN architectures demonstrate the effectiveness of PI-GNN, yielding a promising improvement over the state-of-the-art methods. Code is publicly available at https://github.com/TianBian95/pi-gnn",
    "checked": true,
    "id": "564bd41471fdfd371d11bbc83b7c4b97fccaf429",
    "semantic_title": "noise-robust graph learning by estimating and leveraging pairwise interactions",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SwlfyDq6B3": {
    "title": "3D-Aware Video Generation",
    "volume": "main",
    "abstract": "Generative models have emerged as an essential building block for many image synthesis and editing tasks. Recent advances in this field have also enabled high-quality 3D or video content to be generated that exhibits either multi-view or temporal consistency. With our work, we explore 4D generative adversarial networks (GANs) that learn unconditional generation of 3D-aware videos. By combining neural implicit representations with time-aware discriminator, we develop a GAN framework that synthesizes 3D video supervised only with monocular videos. We show that our method learns a rich embedding of decomposable 3D structures and motions that enables new visual effects of spatio-temporal renderings while producing imagery with quality comparable to that of existing 3D or video GANs",
    "checked": true,
    "id": "17771572e21e3f92fa7c1495341848c19fe2b62e",
    "semantic_title": "3d-aware video generation",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=sixOD8YVvM": {
    "title": "Bounded Space Differentially Private Quantiles",
    "volume": "main",
    "abstract": "Estimating the quantiles of a large dataset is a fundamental problem in both the streaming algorithms literature and the differential privacy literature. However, all existing private mechanisms for distribution-independent quantile computation require space at least linear in the input size $n$. In this work, we devise a differentially private algorithm for the quantile estimation problem, with strongly sublinear space complexity, in the one-shot and continual observation settings. Our basic mechanism estimates any $\\alpha$-approximate quantile of a length-$n$ stream over a data universe $\\mathcal{X}$ with probability $1-\\beta$ using $O\\left( \\frac{\\log (|\\mathcal{X}|/\\beta) \\log (\\alpha \\epsilon n)}{\\alpha \\epsilon} \\right)$ space while satisfying $\\epsilon$-differential privacy at a single time point. Our approach builds upon deterministic streaming algorithms for non-private quantile estimation instantiating the exponential mechanism using a utility function defined on sketch items, while (privately) sampling from intervals defined by the sketch. We also present another algorithm based on histograms that is especially well-suited to the multiple quantiles case. We implement our algorithms and experimentally evaluate them on synthetic and real-world datasets",
    "checked": true,
    "id": "3ef96d9ee132ca2edc5fdb5e76f4c4cf538e4ece",
    "semantic_title": "bounded space differentially private quantiles",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=pxpbTdUEpD": {
    "title": "The Stack: 3 TB of permissively licensed source code",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) play an ever-increasing role in the field of Artificial Intelligence (AI)--not only for natural language processing but also for code understanding and generation. To stimulate open and responsible research on LLMs for code, we introduce The Stack, a 3.1 TB dataset consisting of permissively licensed source code in 30 programming languages. We describe how we collect the full dataset, construct a permissively licensed subset, present a data governance plan, discuss limitations, and show promising results on text2code benchmarks by training 350M-parameter decoders on different Python subsets. We find that (1) near-deduplicating the data significantly boosts performance across all experiments, and (2) it is possible to match previously reported HumanEval and MBPP performance using only permissively licensed data. We make the dataset available at https://hf.co/BigCode, provide a tool called \"Am I in The Stack\" for developers to search The Stack for copies of their code (https://hf.co/spaces/bigcode/in-the-stack), and provide a process for code to be removed from the dataset",
    "checked": true,
    "id": "f3a6115e5fb2237df938976e005468f0b18da797",
    "semantic_title": "the stack: 3 tb of permissively licensed source code",
    "citation_count": 53,
    "authors": []
  },
  "https://openreview.net/forum?id=sWQJfb2GSk": {
    "title": "Exploring the Approximation Capabilities of Multiplicative Neural Networks for Smooth Functions",
    "volume": "main",
    "abstract": "Multiplication layers are a key component in various influential neural network modules, including self-attention and hypernetwork layers. In this paper, we investigate the approximation capabilities of deep neural networks with intermediate neurons connected by simple multiplication operations. We consider two classes of target functions: generalized bandlimited functions, which are frequently used to model real-world signals with finite bandwidth, and Sobolev-Type balls, which are embedded in the Sobolev Space $\\mathcal{W}^{r,2}$. Our results demonstrate that multiplicative neural networks can approximate these functions with significantly fewer layers and neurons compared to standard ReLU neural networks, with respect to both input dimension and approximation error. These findings suggest that multiplicative gates can outperform standard feed-forward layers and have potential for improving neural network design",
    "checked": true,
    "id": "c27bbdd8968c11513a68383145f7935293a57c25",
    "semantic_title": "exploring the approximation capabilities of multiplicative neural networks for smooth functions",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=na5sHG69rI": {
    "title": "Assuming Locally Equal Calibration Errors for Non-Parametric Multiclass Calibration",
    "volume": "main",
    "abstract": "A probabilistic classifier is considered calibrated if it outputs probabilities equal to the expected class distribution given the classifier's output. Calibration is essential in safety-critical tasks where small deviations between the predicted probabilities and the actually observed class proportions can incur high costs. A common approach to improve the calibration of a classifier is to use a hold-out data set and a post-hoc calibration method to learn a correcting transformation for the classifier's output. This work explores the field of post-hoc calibration methods for multi-class classifiers and formulates two assumptions about the probability simplex which have been used by many existing non-parametric calibration methods, but despite this, have never been explicitly stated: assuming locally equal label distributions or assuming locally equal calibration errors. Based on the latter assumption, an intuitive non-parametric post-hoc calibration method is proposed, which is shown to offer improvements to the state-of-the-art according to the expected calibration error metric on CIFAR-10 and CIFAR-100 data sets",
    "checked": true,
    "id": "98a59cf71e7f2623625cfae4772b12388a723751",
    "semantic_title": "assuming locally equal calibration errors for non-parametric multiclass calibration",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OILbP0WErR": {
    "title": "Learning Graph Structure from Convolutional Mixtures",
    "volume": "main",
    "abstract": "Machine learning frameworks such as graph neural networks typically rely on a given, fixed graph to exploit relational inductive biases and thus effectively learn from network data. However, when said graphs are (partially) unobserved, noisy, or dynamic, the problem of inferring graph structure from data becomes relevant. In this paper, we postulate a graph convolutional relationship between the observed and latent graphs, and formulate the graph structure learning task as a network inverse (deconvolution) problem. In lieu of eigendecomposition-based spectral methods or iterative optimization solutions, we unroll and truncate proximal gradient iterations to arrive at a parameterized neural network architecture that we call a Graph Deconvolution Network (GDN). GDNs can learn a distribution of graphs in a supervised fashion, perform link prediction or edge-weight regression tasks by adapting the loss function, and they are inherently inductive as well as node permutation equivariant. We corroborate GDN's superior graph learning performance and its generalization to larger graphs using synthetic data in supervised settings. Moreover, we demonstrate the robustness and representation power of GDNs on real world neuroimaging and social network datasets",
    "checked": true,
    "id": "064a66238196f14b5815dc5bed72a5499bef2e5b",
    "semantic_title": "learning graph structure from convolutional mixtures",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=NrfSRtTpN5": {
    "title": "Learning Object-Centric Neural Scattering Functions for Free-viewpoint Relighting and Scene Composition",
    "volume": "main",
    "abstract": "Photorealistic object appearance modeling from 2D images is a constant topic in vision and graphics. While neural implicit methods (such as Neural Radiance Fields) have shown high-fidelity view synthesis results, they cannot relight the captured objects. More recent neural inverse rendering approaches have enabled object relighting, but they represent surface properties as simple BRDFs, and therefore cannot handle translucent objects. We propose Object-Centric Neural Scattering Functions (OSFs) for learning to reconstruct object appearance from only images. OSFs not only support free-viewpoint object relighting, but also can model both opaque and translucent objects. While accurately modeling subsurface light transport for translucent objects can be highly complex and even intractable for neural methods, OSFs learn to approximate the radiance transfer from a distant light to an outgoing direction at any spatial location. This approximation avoids explicitly modeling complex subsurface scattering, making learning a neural implicit model tractable. Experiments on real and synthetic data show that OSFs accurately reconstruct appearances for both opaque and translucent objects, allowing faithful free-viewpoint relighting as well as scene composition. In our supplementary material, we include a video for an overview. Project website with video results: https://kovenyu.com/OSF/",
    "checked": true,
    "id": "2622704ad6d5585d9ba4cdced8174454e37ed864",
    "semantic_title": "learning object-centric neural scattering functions for free-viewpoint relighting and scene composition",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=Y42xVBQusn": {
    "title": "Contextualize Me – The Case for Context in Reinforcement Learning",
    "volume": "main",
    "abstract": "While Reinforcement Learning ( RL) has made great strides towards solving increasingly complicated problems, many algorithms are still brittle to even slight environmental changes. Contextual Reinforcement Learning (cRL) provides a framework to model such changes in a principled manner, thereby enabling flexible, precise and interpretable task specification and generation. Our goal is to show how the framework of cRL contributes to improving zero-shot generalization in RL through meaningful benchmarks and structured reasoning about generalization tasks. We confirm the insight that optimal behavior in cRL requires context information, as in other related areas of partial observability. To empirically validate this in the cRL framework, we provide various context-extended versions of common RL environments. They are part of the first benchmark library, CARL, designed for generalization based on cRL extensions of popular benchmarks, which we propose as a testbed to further study general agents. We show that in the contextual setting, even simple RL environments become challenging - and that naive solutions are not enough to generalize across complex context spaces",
    "checked": false,
    "id": "df8e5f2e19b696fc5ed4bec9b61835943c8e8a8f",
    "semantic_title": "contextualize me - the case for context in reinforcement learning",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=KxBQPz7HKh": {
    "title": "Multi-dimensional concept discovery (MCD): A unifying framework with completeness guarantees",
    "volume": "main",
    "abstract": "The completeness axiom renders the explanation of a post-hoc eXplainable AI (XAI) method only locally faithful to the model, i.e. for a single decision. For the trustworthy application of XAI, in particular for high-stake decisions, a more global model understanding is required. To this end, concept-based methods have been proposed, which are however not guaranteed to be bound to the actual model reasoning. To circumvent this problem, we propose Multi-dimensional Concept Discovery (MCD) as an extension of previous approaches that fulfills a completeness relation on the level of concepts. Our method starts from general linear subspaces as concepts and does neither require reinforcing concept interpretability nor re-training of model parts. We propose sparse subspace clustering to discover improved concepts and fully leverage the potential of multi-dimensional subspaces. MCD offers two complementary analysis tools for concepts in input space: (1) concept activation maps, that show where a concept is expressed within a sample, allowing for concept characterization through prototypical samples, and (2) concept relevance heatmaps, that decompose the model decision into concept contributions. Both tools together enable a detailed global understanding of the model reasoning, which is guaranteed to relate to the model via a completeness relation. Thus, MCD paves the way towards more trustworthy concept-based XAI. We empirically demonstrate the superiority of MCD against more constrained concept definitions",
    "checked": true,
    "id": "e695a6a6d8a677f528add0118effc7736da35709",
    "semantic_title": "multi-dimensional concept discovery (mcd): a unifying framework with completeness guarantees",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=TyBd56VK7z": {
    "title": "Dr-Fairness: Dynamic Data Ratio Adjustment for Fair Training on Real and Generated Data",
    "volume": "main",
    "abstract": "Fair visual recognition has become critical for preventing demographic disparity. A major cause of model unfairness is the imbalanced representation of different groups in training data. Recently, several works aim to alleviate this issue using generated data. However, these approaches often use generated data to obtain similar amounts of data across groups, which is not optimal for achieving high fairness due to different learning difficulties and generated data qualities across groups. To address this issue, we propose a novel adaptive sampling approach that leverages both real and generated data for fairness. We design a bilevel optimization that finds the optimal data sampling ratios among groups and between real and generated data while training a model. The ratios are dynamically adjusted considering both the model's accuracy as well as its fairness. To efficiently solve our non-convex bilevel optimization, we propose a simple approximation to the solution given by the implicit function theorem. Extensive experiments show that our framework achieves state-of-the-art fairness and accuracy on the CelebA and ImageNet People Subtree datasets. We also observe that our method adaptively relies less on the generated data when it has poor quality. Our work shows the importance of using generated data together with real data for improving model fairness",
    "checked": true,
    "id": "37aa9f75ac8b983470e89989a533e277e00c0ef8",
    "semantic_title": "dr-fairness: dynamic data ratio adjustment for fair training on real and generated data",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N7lCDaeNiS": {
    "title": "Federated Learning under Covariate Shifts with Generalization Guarantees",
    "volume": "main",
    "abstract": "This paper addresses intra-client and inter-client covariate shifts in federated learning (FL) with a focus on the overall generalization performance. To handle covariate shifts, we formulate a new global model training paradigm and propose Federated Importance-Weighted Empirical Risk Minimization (FTW-ERM) along with improving density ratio matching methods without requiring perfect knowledge of the supremum over true ratios. We also propose the communication-efficient variant FITW-ERM with the same level of privacy guarantees as those of classical ERM in FL. We theoretically show that FTW-ERM achieves smaller generalization error than classical ERM under certain settings. Experimental results demonstrate the superiority of FTW-ERM over existing FL baselines in challenging imbalanced federated settings in terms of data distribution shifts across clients",
    "checked": true,
    "id": "537ede99556e763aaeb1ccaa1a34801d276daee4",
    "semantic_title": "federated learning under covariate shifts with generalization guarantees",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pbs22kJmEO": {
    "title": "When Does Uncertainty Matter?: Understanding the Impact of Predictive Uncertainty in ML Assisted Decision Making",
    "volume": "main",
    "abstract": "As machine learning (ML) models are increasingly being employed to assist human decision makers, it becomes critical to provide these decision makers with relevant inputs which can help them decide if and how to incorporate model predictions into their decision making. For instance, communicating the uncertainty associated with model predictions could potentially be helpful in this regard. In this work, we carry out user studies (1,330 responses from 190 participants) to systematically assess how people with differing levels of expertise respond to different types of predictive uncertainty (i.e., posterior predictive distributions with different shapes and variances) in the context of ML assisted decision making for predicting apartment rental prices. We found that showing posterior predictive distributions led to smaller disagreements with the ML model's predictions, regardless of the shapes and variances of the posterior predictive distributions we considered, and that these effects may be sensitive to expertise in both ML and the domain. This suggests that posterior predictive distributions can potentially serve as useful decision aids which should be used with caution and take into account the type of distribution and the expertise of the human",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QhHLwn3D0Y": {
    "title": "The Robustness Limits of SoTA Vision Models to Natural Variation",
    "volume": "main",
    "abstract": "Recent state-of-the-art vision models have introduced new architectures, learning paradigms, and larger pretraining data, leading to impressive performance on tasks such as classification. While previous generations of vision models were shown to lack robustness to factors such as pose, the extent to which this next generation of models are more robust remains unclear. To study this question, we develop a dataset of more than 7 million images with controlled changes in pose, position background, lighting color, and size. We study not only how robust recent state-of- the-art models are, but also the extent to which models can generalize to variation in each of these factors. We consider a catalog of recent vision models, including vision transformers (ViT), self-supervised models such as masked autoencoders (MAE), and models trained on larger datasets such as CLIP. We find that even today's best models are not robust to common changes in pose, size, and background. When some samples varied during training, we found models required a significant portion of instances seen varying to generalize—though eventually robustness did improve. When variability is only witnessed for some classes however, we found that models did not generalize to other classes unless the classes were very similar to those seen varying during training. We hope our work will shed further light on the blind spots of SoTA models and spur the development of more robust vision models",
    "checked": true,
    "id": "8b1aa3a20638363be3a6d941e907df704c738dfc",
    "semantic_title": "the robustness limits of sota vision models to natural variation",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=CqTkapZ6H9": {
    "title": "Robust Multi-Agent Reinforcement Learning with State Uncertainty",
    "volume": "main",
    "abstract": "In real-world multi-agent reinforcement learning (MARL) applications, agents may not have perfect state information (e.g., due to inaccurate measurement or malicious attacks), which challenges the robustness of agents' policies. Though robustness is getting important in MARL deployment, little prior work has studied state uncertainties in MARL, neither in problem formulation nor algorithm design. Motivated by this robustness issue and the lack of corresponding studies, we study the problem of MARL with state uncertainty in this work. We provide the first attempt to the theoretical and empirical analysis of this challenging problem. We first model the problem as a Markov Game with state perturbation adversaries (MG-SPA) by introducing a set of state perturbation adversaries into a Markov Game. We then introduce robust equilibrium (RE) as the solution concept of an MG-SPA. We conduct a fundamental analysis regarding MG-SPA such as giving conditions under which such a robust equilibrium exists. Then we propose a robust multi-agent Q-learning (RMAQ) algorithm to find such an equilibrium, with convergence guarantees. To handle high-dimensional state-action space, we design a robust multi-agent actor-critic (RMAAC) algorithm based on an analytical expression of the policy gradient derived in the paper. Our experiments show that the proposed RMAQ algorithm converges to the optimal value function; our RMAAC algorithm outperforms several MARL and robust MARL methods in multiple multi-agent environments when state uncertainty is present. The source code is public on https://github.com/sihongho/robust_marl_with_state_uncertainty",
    "checked": true,
    "id": "b1dd351bab053ea1acac9c1822034480fa3a201e",
    "semantic_title": "robust multi-agent reinforcement learning with state uncertainty",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=ClIcmwdlxn": {
    "title": "Optimum-statistical Collaboration Towards General and Efficient Black-box Optimization",
    "volume": "main",
    "abstract": "In this paper, we make the key delineation on the roles of resolution and statistical uncertainty in hierarchical bandits-based black-box optimization algorithms, guiding a more general analysis and a more efficient algorithm design. We introduce the optimum-statistical collaboration, an algorithm framework of managing the interaction between optimization error flux and statistical error flux evolving in the optimization process. We provide a general analysis of this framework without specifying the forms of statistical error and uncertainty quantifier. Our framework and its analysis, due to their generality, can be applied to a large family of functions and partitions that satisfy different local smoothness assumptions and have different numbers of local optimums, which is much richer than the class of functions studied in prior works. Our framework also inspires us to propose a better measure of the statistical uncertainty and consequently a variance-adaptive algorithm VHCT. In theory, we prove the algorithm enjoys rate-optimal regret bounds under different local smoothness assumptions; in experiments, we show the algorithm outperforms prior efforts in different settings",
    "checked": true,
    "id": "c3a62d14244c5c7fea01c161e32685c0c6e2f043",
    "semantic_title": "optimum-statistical collaboration towards general and efficient black-box optimization",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=KBhSyBBeeO": {
    "title": "An Adaptive Half-Space Projection Method for Stochastic Optimization Problems with Group Sparse Regularization",
    "volume": "main",
    "abstract": "Optimization problems with group sparse regularization are ubiquitous in various popular downstream applications, such as feature selection and compression for Deep Neural Networks (DNNs). Nonetheless, the existing methods in the literature do not perform particularly well when such regularization is used in combination with a stochastic loss function. In particular, it is challenging to design a computationally efficient algorithm with a convergence guarantee and can compute group-sparse solutions. Recently, a half-space stochastic projected gradient (HSPG) method was proposed that partly addressed these challenges. This paper presents a substantially enhanced version of HSPG that we call AdaHSPG+ that makes two noticeable advances. First, AdaHSPG+ is shown to have a stronger convergence result under significantly looser assumptions than those required by HSPG. This improvement in convergence is achieved by integrating variance reduction techniques with a new adaptive strategy for iteratively predicting the support of a solution. Second, AdaHSPG+ requires significantly less parameter tuning compared to HSPG, thus making it more practical and user-friendly. This advance is achieved by designing automatic and adaptive strategies for choosing the type of step employed at each iteration and for updating key hyperparameters. The numerical effectiveness of our proposed AdaHSPG+ algorithm is demonstrated on both convex and non-convex benchmark problems. The source code is available at https://github.com/tianyic/adahspg",
    "checked": true,
    "id": "6826fe9e5d593cb63129039d35e19d7c0f482790",
    "semantic_title": "an adaptive half-space projection method for stochastic optimization problems with group sparse regularization",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=iDNMZgjJuJ": {
    "title": "Causally-guided Regularization of Graph Attention Improves Generalizability",
    "volume": "main",
    "abstract": "Graph attention networks estimate the relational importance of node neighbors to aggregate relevant information over local neighborhoods for a prediction task. However, the inferred attentions are vulnerable to spurious correlations and connectivity in the training data, hampering the generalizability of models. We introduce CAR, a general-purpose regularization framework for graph attention networks. Embodying a causal inference approach based on invariance prediction, CAR aligns the attention mechanism with the causal effects of active interventions on graph connectivity in a scalable manner. CAR is compatible with a variety of graph attention architectures, and we show that it systematically improves generalizability on various node classification tasks. Our ablation studies indicate that CAR hones in on the aspects of graph structure most pertinent to the prediction (e.g., homophily), and does so more effectively than alternative approaches. Finally, we also show that \\methodname enhances interpretability of attention coefficients by accentuating node-neighbor relations that point to causal hypotheses",
    "checked": true,
    "id": "5a7c11c952e51497d92bec310e15b1947bf460bf",
    "semantic_title": "causally-guided regularization of graph attention improves generalizability",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=nEX2q5B2RQ": {
    "title": "Analyzing Deep PAC-Bayesian Learning with Neural Tangent Kernel: Convergence, Analytic Generalization Bound, and Efficient Hyperparameter Selection",
    "volume": "main",
    "abstract": "PAC-Bayes is a well-established framework for analyzing generalization performance in machine learning models. This framework provides a bound on the expected population error by considering the sum of training error and the divergence between posterior and prior distributions. In addition to being a successful generalization bound analysis tool, the PAC-Bayesian bound can also be incorporated into an objective function for training probabilistic neural networks, which we refer to simply as {\\it Deep PAC-Bayesian Learning}. Deep PAC-Bayesian learning has been shown to achieve competitive expected test set error and provide a tight generalization bound in practice at the same time through gradient descent training. Despite its empirical success, theoretical analysis of deep PAC-Bayesian learning for neural networks is rarely explored. To this end, this paper proposes a theoretical convergence and generalization analysis for Deep PAC-Bayesian learning. For a deep and wide probabilistic neural network, our analysis shows that PAC-Bayesian learning corresponds to solving a kernel ridge regression when the probabilistic neural tangent kernel (PNTK) is used as the kernel. We utilize this outcome in conjunction with the PAC-Bayes $\\mathcal{C}$-bound, enabling us to derive an analytical and guaranteed PAC-Bayesian generalization bound for the first time. Finally, drawing insight from our theoretical results, we propose a proxy measure for efficient hyperparameter selection, which is proven to be time-saving on various benchmarks. Our work not only provides a better understanding of the theoretical underpinnings of Deep PAC-Bayesian learning, but also offers practical tools for improving the training and generalization performance of these models",
    "checked": true,
    "id": "8dab53b0cc7358b36a0fc7e5ff09f5c9a492b1c8",
    "semantic_title": "analyzing deep pac-bayesian learning with neural tangent kernel: convergence, analytic generalization bound, and efficient hyperparameter selection",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=ttzypy3kT7": {
    "title": "High-Modality Multimodal Transformer: Quantifying Modality & Interaction Heterogeneity for High-Modality Representation Learning",
    "volume": "main",
    "abstract": "Many real-world problems are inherently multimodal, from the communicative modalities humans use to express social and emotional states such as spoken language, gestures, and paralinguistics to the force, proprioception, and visual sensors ubiquitous on robots. While there has been an explosion of interest in multimodal representation learning, these methods are still largely focused on a small set of modalities, primarily in the language, vision, and audio space. In order to accelerate generalization towards diverse and understudied modalities, this paper studies efficient representation learning for high-modality scenarios involving a large set of diverse modalities. Since adding new models for every new modality or task becomes prohibitively expensive, a critical technical challenge is heterogeneity quantification: how can we measure which modalities encode similar information and interactions in order to permit parameter sharing with previous modalities? This paper proposes two new information theoretic metrics for heterogeneity quantification: (1) modality heterogeneity studies how similar $2$ modalities $\\{X_1,X_2\\}$ are by measuring how much information can be transferred from $X_1$ to $X_2$, while (2) interaction heterogeneity studies how similarly pairs of modalities $\\{X_1,X_2\\}, \\{X_3,X_4\\}$ interact by measuring how much interaction information can be transferred from $\\{X_1,X_2\\}$ to $\\{X_3,X_4\\}$. We show the importance of these $2$ proposed metrics in high-modality scenarios as a way to automatically prioritize the fusion of modalities that contain unique information or unique interactions. The result is a single model, HighMMT, that scales up to $10$ modalities (text, image, audio, video, sensors, proprioception, speech, time-series, sets, and tables) and $15$ tasks from $5$ different research areas. Not only does HighMMT outperform prior methods on the tradeoff between performance and efficiency, it also demonstrates a crucial scaling behavior: performance continues to improve with each modality added, and it transfers to entirely new modalities and tasks during fine-tuning. We release our code and benchmarks, which we hope will present a unified platform for subsequent theoretical and empirical analysis",
    "checked": false,
    "id": "0651e9cbe0b8c1b4465c80d2309af62e5e4da574",
    "semantic_title": "high-modality multimodal transformer: quantifying modality&interaction heterogeneity for high-modality representation learning",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=TH6YrEcbth": {
    "title": "Learning Interpolations between Boltzmann Densities",
    "volume": "main",
    "abstract": "We introduce a training objective for continuous normalizing flows that can be used in the absence of samples but in the presence of an energy function. Our method relies on either a prescribed or a learnt interpolation $f_t$ of energy functions between the target energy $f_1$ and the energy function of a generalized Gaussian $f_0(x) = ||x/\\sigma||_p^p$. The interpolation of energy functions induces an interpolation of Boltzmann densities $p_t \\propto e^{-f_t}$ and we aim to find a time-dependent vector field $V_t$ that transports samples along the family $p_t$ of densities. The condition of transporting samples along the family $p_t$ is equivalent to satisfying the continuity equation with $V_t$ and $p_t = Z_t^{-1}e^{-f_t}$. Consequently, we optimize $V_t$ and $f_t$ to satisfy this partial differential equation. We experimentally compare the proposed training objective to the reverse KL-divergence on Gaussian mixtures and on the Boltzmann density of a quantum mechanical particle in a double-well potential",
    "checked": true,
    "id": "6404636ef49ceef284c09ed6aaa32641b6723cde",
    "semantic_title": "learning interpolations between boltzmann densities",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LjDFIWWVVa": {
    "title": "Retiring $\\Delta \\text{DP}$: New Distribution-Level Metrics for Demographic Parity",
    "volume": "main",
    "abstract": "Demographic parity is the most widely recognized measure of group fairness in machine learning, which ensures equal treatment of different demographic groups. Numerous works aim to achieve demographic parity by pursuing the commonly used metric $\\Delta DP$. Unfortunately, in this paper, we reveal that the fairness metric $\\Delta DP$ can not precisely measure the violation of demographic parity, because it inherently has the following drawbacks: i) zero-value $\\Delta DP$ does not guarantee zero violation of demographic parity, ii) $\\Delta DP$ values can vary with different classification thresholds. To this end, we propose two new fairness metrics, Area Between Probability density function Curves (ABPC) and Area Between Cumulative density function Curves (ABCC), to precisely measure the violation of demographic parity at the distribution level. The new fairness metrics directly measure the difference between the distributions of the prediction probability for different demographic groups. Thus our proposed new metrics enjoy: i) zero-value ABCC/ABPC guarantees zero violation of demographic parity; ii) ABCC/ABPC guarantees demographic parity while the classification thresholds are adjusted. We further re-evaluate the existing fair models with our proposed fairness metrics and observe different fairness behaviors of those models under the new metrics",
    "checked": false,
    "id": "8dc4f76559e3d603412d4ba7b3af118233942b05",
    "semantic_title": "retiring $\\delta$dp: new distribution-level metrics for demographic parity",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=2f81Q622ww": {
    "title": "Generating Adversarial Examples with Task Oriented Multi-Objective Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D45gGvUZp2": {
    "title": "Denise: Deep Robust Principal Component Analysis for Positive Semidefinite Matrices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZME2nZMTvY": {
    "title": "Mean-Field Control based Approximation of Multi-Agent Reinforcement Learning in Presence of a Non-decomposable Shared Global State",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DdZoPUPm0a": {
    "title": "Interpretable Mixture of Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=162TqkUNPO": {
    "title": "Comparative Generalization Bounds for Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wNBARGxoJn": {
    "title": "Learning to correct spectral methods for simulating turbulent flows",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xzCDD9i4IZ": {
    "title": "Cox-Hawkes: doubly stochastic spatiotemporal Poisson processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ilHM31lXC4": {
    "title": "Personalized Federated Learning: A Unified Framework and Universal Optimization Techniques",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l5BzfQhROl": {
    "title": "Generating Teammates for Training Robust Ad Hoc Teamwork Agents via Best-Response Diversity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZgXfXSz51n": {
    "title": "Guillotine Regularization: Why removing layers is needed to improve generalization in Self-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MTFf1rDDEI": {
    "title": "Successor Feature Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Jjl2c8kWUc": {
    "title": "Lightweight Learner for Shared Knowledge Lifelong Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6rbcq0qacA": {
    "title": "Deep Plug-and-Play Clustering with Unknown Number of Clusters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v73h3bYE2Z": {
    "title": "When to Trust Aggregated Gradients: Addressing Negative Client Sampling in Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R8TU3pfzFr": {
    "title": "A Measure of the Complexity of Neural Representations based on Partial Information Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MR4glug5GU": {
    "title": "Trip-ROMA: Self-Supervised Learning with Triplets and Random Mappings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DUsgPi3oCC": {
    "title": "Conditional Permutation Invariant Flows",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RLYkyucU6k": {
    "title": "Agent-State Construction with Auxiliary Inputs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9KoBOlstTq": {
    "title": "Modelling sequential branching dynamics with a multivariate branching Gaussian process",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j3oQF9coJd": {
    "title": "U-NO: U-shaped Neural Operators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uyTL5Bvosj": {
    "title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nOk4XEB7Ke": {
    "title": "Fast&Fair: Training Acceleration and Bias Mitigation for GNNs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IqJsyulDUX": {
    "title": "Ensembles for Uncertainty Estimation: Benefits of Prior Functions and Bootstrapping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W98rebBxlQ": {
    "title": "Soft Diffusion: Score Matching with General Corruptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mySiFHCeAl": {
    "title": "Spectral Regularization Allows Data-frugal Learning over Combinatorial Spaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jVMMdg31De": {
    "title": "A Cubic Regularization Approach for Finding Local Minimax Points in Nonconvex Minimax Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SEDWlhcFWA": {
    "title": "Assisted Learning for Organizations with Limited Imbalanced Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EPPqt3uERT": {
    "title": "Transformer for Partial Differential Equations' Operator Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gvcDSDYUZx": {
    "title": "Efficient Model-Based Multi-Agent Mean-Field Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y4CGF1A8VG": {
    "title": "Machine Explanations and Human Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9aXKUJEKwV": {
    "title": "Learning to Look by Self-Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=slsAQHpS7n": {
    "title": "Computationally-efficient initialisation of GPs: The generalised variogram method",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f4VyYhkRvi": {
    "title": "Fairness via In-Processing in the Over-parameterized Regime: A Cautionary Tale with MinDiff Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Oq5XKRVYpQ": {
    "title": "Graph-based Multi-ODE Neural Networks for Spatio-Temporal Traffic Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iMmsCI0JsS": {
    "title": "TimeSeAD: Benchmarking Deep Multivariate Time-Series Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I4IkGmgFJz": {
    "title": "Data Models for Dataset Drift Controls in Machine Learning With Optical Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1nhTDzxxMA": {
    "title": "Multi-Source Transfer Learning for Deep Model-Based Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YwNrPLjHSL": {
    "title": "Do Vision-Language Pretrained Models Learn Composable Primitive Concepts?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KSvr8A62MD": {
    "title": "A Simulation Environment and Reinforcement Learning Method for Waste Reduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JkIH4MeOc3": {
    "title": "Group Fairness in Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OarsigVib0": {
    "title": "On the Statistical Complexity of Estimation and Testing under Privacy Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B4J40x7NjA": {
    "title": "Positive Difference Distribution for Image Outlier Detection using Normalizing Flows and Contrastive Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s9efQF3QW1": {
    "title": "Uncovering the Representation of Spiking Neural Networks Trained with Surrogate Gradient",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qxrwt6F3sf": {
    "title": "PAC-Bayes Generalisation Bounds for Heavy-Tailed Losses through Supermartingales",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sb6p5mcefw": {
    "title": "Generalization as Dynamical Robustness--The Role of Riemannian Contraction in Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hnr23knZfY": {
    "title": "POLTER: Policy Trajectory Ensemble Regularization for Unsupervised Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8WUyeeMxMH": {
    "title": "Proximal Curriculum for Reinforcement Learning Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R6W7zkMz0P": {
    "title": "Pre-trained Perceptual Features Improve Differentially Private Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SM1BkjGePI": {
    "title": "Bridging performance gap between minimal and maximal SVM models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YJDqQSAuB6": {
    "title": "Weisfeiler and Leman Go Infinite: Spectral and Combinatorial Pre-Colorings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2Yo9xqR6Ab": {
    "title": "Jacobian-based Causal Discovery with Nonlinear ICA",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1IYJfwJtjQ": {
    "title": "FASTRAIN-GNN: Fast and Accurate Self-Training for Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5nVJlKgmxp": {
    "title": "Online Optimal Tracking of Linear Systems with Adversarial Disturbances",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T1XtOqrVKn": {
    "title": "Reducing Predictive Feature Suppression in Resource-Constrained Contrastive Image-Caption Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fempQstMbV": {
    "title": "Deep Double Descent via Smooth Interpolation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4zCgjqjzAv": {
    "title": "Bayesian Transformed Gaussian Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AZ4GobeSLq": {
    "title": "A Variational Perspective on Generative Flow Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oq3tx5kinu": {
    "title": "Active Learning of Ordinal Embeddings: A User Study on Football Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=55BcghgicI": {
    "title": "Differentially private partitioned variational inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a0T3nOP9sB": {
    "title": "Adaptive patch foraging in deep reinforcement learning agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=onufdyHvqN": {
    "title": "Private Multi-Task Learning: Formulation and Applications to Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=82hRiAbnnm": {
    "title": "Sobolev Spaces, Kernels and Discrepancies over Hyperspheres",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SgTKk6ryPr": {
    "title": "Monotone deep Boltzmann machines",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4eL6z9ziw7": {
    "title": "NovelCraft: A Dataset for Novelty Detection and Discovery in Open Worlds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OsKXlWamTQ": {
    "title": "Integrating Bayesian Network Structure into Residual Flows and Variational Autoencoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QTXocpAP9p": {
    "title": "Neural Collapse: A Review on Modelling Principles and Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZOAb497iaY": {
    "title": "Unifying physical systems' inductive biases in neural ODE using dynamics constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tE2NiMGd07": {
    "title": "Bridging Graph Position Encodings for Transformers with Weighted Graph-Walking Automata",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jdGMBgYvfX": {
    "title": "UncertaINR: Uncertainty Quantification of End-to-End Implicit Neural Representations for Computed Tomography",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FdMWtpVT1I": {
    "title": "Training Data Size Induced Double Descent For Denoising Feedforward Neural Networks and the Role of Training Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=10JdgrzNOk": {
    "title": "Scalable Deep Compressive Sensing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MRLHN4MSmA": {
    "title": "A Modulation Layer to Increase Neural Network Robustness Against Data Quality Issues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gR9UVgH8PZ": {
    "title": "Neural Shape Compiler: A Unified Framework for Transforming between Text, Point Cloud, and Program",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6IFi2soduD": {
    "title": "Can Pruning Improve Certified Robustness of Neural Networks?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v5jwDLqfQo": {
    "title": "Extended Agriculture-Vision: An Extension of a Large Aerial Image Dataset for Agricultural Pattern Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OJtYpdiHNo": {
    "title": "Transframer: Arbitrary Frame Prediction with Generative Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xqS8k9E75c": {
    "title": "Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jM8nzUzBWr": {
    "title": "Estimating the Density Ratio between Distributions with High Discrepancy using Multinomial Logistic Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QzWr4w8PXx": {
    "title": "A Revenue Function for Comparison-Based Hierarchical Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C1Xl8dYCBn": {
    "title": "ChemSpacE: Interpretable and Interactive Chemical Space Exploration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dQxBRqCjLr": {
    "title": "A Free Lunch with Influence Functions? An Empirical Evaluation of Influence Functions for Average Treatment Effect Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TzRXyO3CzX": {
    "title": "Clustering using Approximate Nearest Neighbour Oracles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JwgVBv18RG": {
    "title": "Bayesian Optimization with Informative Covariance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2UQv8L1Cv9": {
    "title": "Turning Normalizing Flows into Monge Maps with Geodesic Gaussian Preserving Flows",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h4BYtZ79uy": {
    "title": "Graph Neural Networks Designed for Different Graph Types: A Survey",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KwWKB9Bqam": {
    "title": "Generalization bounds for Kernel Canonical Correlation Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gyhiZYrk5y": {
    "title": "Learning Identity-Preserving Transformations on Data Manifolds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YtU0nDb5e8": {
    "title": "A Halfspace-Mass Depth-Based Method for Adversarial Attack Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qdDmxzGuzu": {
    "title": "Reusable Options through Gradient-based Meta Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qvRWcDXBam": {
    "title": "Containing a spread through sequential learning: to exploit or to explore?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WVwnccBJLz": {
    "title": "Bidirectional View based Consistency Regularization for Semi-Supervised Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UvJBKWaSSH": {
    "title": "FLUID: A Unified Evaluation Framework for Flexible Sequential Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bCiNWDmlY2": {
    "title": "The Low-Rank Simplicity Bias in Deep Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LIT8tjs6rJ": {
    "title": "Parameter Efficient Node Classification on Homophilic Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xkrtvHlp3P": {
    "title": "Towards Better Out-of-Distribution Generalization of Neural Algorithmic Reasoning Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9lyqt3rbDc": {
    "title": "L-SVRG and L-Katyusha with Adaptive Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HG11PAmwQ6": {
    "title": "Quantum Policy Iteration via Amplitude Estimation and Grover Search – Towards Quantum Advantage for Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tEVpz2xJWX": {
    "title": "Enhancing Diffusion-Based Image Synthesis with Robust Classifier Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RjZq6W6FoE": {
    "title": "Improved Overparametrization Bounds for Global Convergence of SGD for Shallow Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wmGlMhaBe0": {
    "title": "A Unified View of Masked Image Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=11pGlecTz2": {
    "title": "How Robust is Your Fairness? Evaluating and Sustaining Fairness under Unseen Distribution Shifts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OzGIu4T4Cz": {
    "title": "Leveraging Demonstrations with Latent Space Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Gp0pHyUyrb": {
    "title": "Solving Nonconvex-Nonconcave Min-Max Problems exhibiting Weak Minty Solutions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3epEbhdgbv": {
    "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LBA2Jj5Gqn": {
    "title": "Temperature check: theory and practice for training models with softmax-cross-entropy losses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QtrjqVIZna": {
    "title": "Fusion of Global and Local Knowledge for Personalized Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=goPsLn3RVo": {
    "title": "Defense Against Reward Poisoning Attacks in Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DHEZuKStzH": {
    "title": "Learning Energy Conserving Dynamics Efficiently with Hamiltonian Gaussian Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iDxfGaMYVr": {
    "title": "Continual Learning by Modeling Intra-Class Variation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v6anjyEDVW": {
    "title": "Costs and Benefits of Fair Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kJcwlP7BRs": {
    "title": "Transfer Entropy Bottleneck: Learning Sequence to Sequence Information Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bomdTc9HyL": {
    "title": "Transductive Decoupled Variational Inference for Few-Shot Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IvsGP7xRvm": {
    "title": "Black-Box Prompt Learning for Pre-trained Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z2L5d9ay4B": {
    "title": "Image Compression with Product Quantized Masked Image Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yhGCKUsKJS": {
    "title": "Action Poisoning Attacks on Linear Contextual Bandits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MKZyHtmfwH": {
    "title": "Mixed effects in machine learning – A flexible mixedML framework to add random effects to supervised machine learning regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fTNorIvVXG": {
    "title": "Probing Predictions on OOD Images via Nearest Categories",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dqgdBy4Uv5": {
    "title": "Cheap and Deterministic Inference for Deep State-Space Models of Interacting Dynamical Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k5m8xXTOrC": {
    "title": "Solving a Special Type of Optimal Transport Problem by a Modified Hungarian Algorithm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WoXJFsJ6Zw": {
    "title": "AI-SARAH: Adaptive and Implicit Stochastic Recursive Gradient Methods",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oXmwAPlbVw": {
    "title": "U-Statistics for Importance-Weighted Variational Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BVi6MhKO0G": {
    "title": "OADAT: Experimental and Synthetic Clinical Optoacoustic Data for Standardized Image Processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mNEqiC924B": {
    "title": "Stacking Diverse Architectures to Improve Machine Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GbkWw3jwL9": {
    "title": "Contrastive Search Is What You Need For Neural Text Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tBl4yBEjKi": {
    "title": "Separable Self-attention for Mobile Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iEq6lhG4O3": {
    "title": "A Flexible Nadaraya-Watson Head Can Offer Explainable and Calibrated Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hHiIbk7ApW": {
    "title": "Diffusion-based Time Series Imputation and Forecasting with Structured State Space Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oe4dl4MCGY": {
    "title": "Robust Hybrid Learning With Expert Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=paguBNtqiO": {
    "title": "Improved Differentially Private Riemannian Optimization: Fast Sampling and Variance Reduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lmr2WwlaFc": {
    "title": "Dirichlet Mechanism for Differentially Private KL Divergence Minimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cKsKXR28cG": {
    "title": "Regularized Training of Intermediate Layers for Generative Models for Inverse Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6dsvH7pQHH": {
    "title": "Layerwise Bregman Representation Learning of Neural Networks with Applications to Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WN1O2MJDST": {
    "title": "Learn, Unlearn and Relearn: An Online Learning Paradigm for Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5II12ypVQo": {
    "title": "KRADA: Known-region-aware Domain Alignment for Open-set Domain Adaptation in Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gZna3IiGfl": {
    "title": "Mean-field analysis for heavy ball methods: Dropout-stability, connectivity, and global convergence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RZveYHgZbu": {
    "title": "Signed Graph Neural Networks: A Frequency Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TNocbXm5MZ": {
    "title": "Guaranteed Discovery of Control-Endogenous Latent States with Multi-Step Inverse Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rm0zIzlhcX": {
    "title": "Beyond Intuition: Rethinking Token Attributions inside Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KQRv0O8iW4": {
    "title": "Finite-Time Analysis of Decentralized Single-Timescale Actor-Critic",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GcO6ugrLKp": {
    "title": "Supervised Feature Selection with Neuron Evolution in Sparse Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mAx8QqZ14f": {
    "title": "Differentially Private Fréchet Mean on the Manifold of Symmetric Positive Definite (SPD) Matrices with log-Euclidean Metric",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UntUoeLwwu": {
    "title": "Tailoring to the Tails: Risk Measures for Fine-Grained Tail Sensitivity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JnsGy9uWtI": {
    "title": "Controlling Neural Network Smoothness for Neural Algorithmic Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vxyjTUPV24": {
    "title": "Target Propagation via Regularized Inversion for Recurrent Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sMsGv5Kfm3": {
    "title": "Bayesian Causal Bandits with Backdoor Adjustment Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=znNITCJyTI": {
    "title": "Accelerated Quality-Diversity through Massive Parallelism",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oKlEOT83gI": {
    "title": "Risk Sensitive Dead-end Identification in Safety-Critical Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y7RGNXhGSR": {
    "title": "BIGRoC: Boosting Image Generation via a Robust Classifier",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CUDdbTT1QC": {
    "title": "Constrained Parameter Inference as a Principle for Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=czgMCpvrDM": {
    "title": "SMILE: Sample-to-feature Mixup for Efficient Transfer Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q6ZXm7VBFY": {
    "title": "Optimal Convergence Rates of Deep Convolutional Neural Networks: Additive Ridge Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=myjAVQrRxS": {
    "title": "Dropped Scheduled Task: Mitigating Negative Transfer in Multi-task Learning using Dynamic Task Dropping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wkecshlYxI": {
    "title": "Revisiting adversarial training for the worst-performing class",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JyKNuoZGux": {
    "title": "Calibrate and Debias Layer-wise Sampling for Graph Convolutional Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nAr9PhyEbQ": {
    "title": "Online Learning for Prediction via Covariance Fitting: Computation, Performance and Robustness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DzJ7JfPXkE": {
    "title": "ViViT: Curvature Access Through The Generalized Gauss-Newton's Low-Rank Structure",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EYrRzKPinA": {
    "title": "On a continuous time model of gradient descent dynamics and instability in deep learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Lx19EyKX77": {
    "title": "Gradient-adjusted Incremental Target Propagation Provides Effective Credit Assignment in Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ryUHgEdWCQ": {
    "title": "Proportional Fairness in Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=56cTmVrg5w": {
    "title": "On the Role of Fixed Points of Dynamical Systems in Training Physics-Informed Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EiX2L4sDPG": {
    "title": "VN-Transformer: Rotation-Equivariant Attention for Vector Neurons",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1U0aPkBVz0": {
    "title": "lo-fi: distributed fine-tuning without communication",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mHSAy1n65Z": {
    "title": "Optimal Threshold Labeling for Ordinal Regression Methods",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LTAdaRM29K": {
    "title": "Recognition Models to Learn Dynamics from Partial Observations with Neural ODEs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GzqdMrFQsE": {
    "title": "Attention Beats Concatenation for Conditioning Neural Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d3rHk4VAf0": {
    "title": "A Ranking Game for Imitation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LfTukxzxTj": {
    "title": "Implicit Ensemble Training for Efficient and Robust Multiagent Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hVT7SHlilx": {
    "title": "Named Tensor Notation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X1pjWMCMB0": {
    "title": "PCPs: Patient Cardiac Prototypes to Probe AI-based Medical Diagnoses, Distill Datasets, and Retrieve Patients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RbLsYz1Az9": {
    "title": "On the infinite-depth limit of finite-width neural networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=85BfDdYMBY": {
    "title": "Intrinsic Dimension for Large-Scale Geometric Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Xq1sTZTQVm": {
    "title": "Beyond Information Gain: An Empirical Benchmark for Low-Switching-Cost Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EgHnKOLaKW": {
    "title": "DisCo: Improving Compositional Generalization in Visual Reasoning through Distribution Coverage",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RA0TDqt3hC": {
    "title": "Hidden Heterogeneity: When to Choose Similarity-Based Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zKnqZeUCLO": {
    "title": "PolyViT: Co-training Vision Transformers on Images, Videos and Audio",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lheUXtDNvP": {
    "title": "GSR: A Generalized Symbolic Regression Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jbZEUtULft": {
    "title": "Bounding generalization error with input compression: An empirical study with infinite-width networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wIXHG8LZ2w": {
    "title": "Learning Representations for Pixel-based Control: What Matters and Why?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MzWgBjZ6Le": {
    "title": "FedDAG: Federated DAG Structure Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tnRRHzZPMq": {
    "title": "Communication-Efficient Distributionally Robust Decentralized Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GRBbtkW3Lp": {
    "title": "EdiBERT: a generative model for image editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2wWJxtpFer": {
    "title": "OpenCon: Open-world Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fjkN5Ur2d6": {
    "title": "Linking Neural Collapse and L2 Normalization with Improved Out-of-Distribution Detection in Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Grhi800jVz": {
    "title": "Euclidean-Norm-Induced Schatten-p Quasi-Norm Regularization for Low-Rank Tensor Completion and Tensor Robust Principal Component Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TGuXXlbKsn": {
    "title": "Benchmarks and Algorithms for Offline Preference-Based Reward Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mrTXGDZns2": {
    "title": "Fairness and robustness in anti-causal prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a7nvXxNmdV": {
    "title": "Improved baselines for vision-language pre-training",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UIalYAHdBH": {
    "title": "On the Sample Complexity of Lipschitz Constant Estimation",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XXfEmIMJDm": {
    "title": "Achieving the Pareto Frontier of Regret Minimization and Best Arm Identification in Multi-Armed Bandits",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ivCd8z8zR2": {
    "title": "High Fidelity Neural Audio Compression",
    "volume": "reproducibility",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EGQSpkUDdD": {
    "title": "AP: Selective Activation for De-sparsifying Pruned Networks",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mvftzofTYQ": {
    "title": "WOODS: Benchmarks for Out-of-Distribution Generalization in Time Series",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iO4LZibEqW": {
    "title": "Holistic Evaluation of Language Models",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yrkJGne0vN": {
    "title": "Neural Ordinary Differential Equations for Modeling Epidemic Spreading",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2mZSlQscj3": {
    "title": "Neural Monge Map estimation and its applications",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=igdWKxK5RZ": {
    "title": "Finding and Only Finding Differential Nash Equilibria by Both Pretending to be a Follower",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=25G63lDHV2": {
    "title": "Efficient Reward Poisoning Attacks on Online Deep Reinforcement Learning",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VmyFF5lL3F": {
    "title": "Inversion by Direct Iteration: An Alternative to Denoising Diffusion for Image Restoration",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vXSsTYs6ZB": {
    "title": "LEAD: Min-Max Optimization from a Physical Perspective",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XNFo3dQiCJ": {
    "title": "Generalizability of Adversarial Robustness Under Distribution Shifts",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r9vGSpbbRO": {
    "title": "Attacking Perceptual Similarity Metrics",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KgfFAI9f3E": {
    "title": "Identification of Negative Transfers in Multitask Learning Using Surrogate Models",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rAnB7JSMXL": {
    "title": "Patches Are All You Need?",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=11osftjEbF": {
    "title": "Numerical Accounting in the Shuffle Model of Differential Privacy",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L9othQvPks": {
    "title": "Workflow Discovery from Dialogues in the Low Data Regime",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JwDpZSv3yz": {
    "title": "SPADE: Semi-supervised Anomaly Detection under Distribution Mismatch",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZR2CDgADRo": {
    "title": "SolidGen: An Autoregressive Model for Direct B-rep Synthesis",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KqR3rgooXb": {
    "title": "Numerical Data Imputation for Multimodal Data Sets: A Probabilistic Nearest-Neighbor Kernel Density Approach",
    "volume": "reproducibility",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R9CgBkeZ6Z": {
    "title": "Aux-Drop: Handling Haphazard Inputs in Online Learning Using Auxiliary Dropouts",
    "volume": "reproducibility",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jWr41htaB3": {
    "title": "A Stochastic Proximal Polyak Step Size",
    "volume": "reproducibility",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RYeRNwRjNE": {
    "title": "Explaining Visual Counterfactual Explainers",
    "volume": "reproducibility",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JjbsIYOuNi": {
    "title": "PRUDEX-Compass: Towards Systematic Evaluation of Reinforcement Learning in Financial Markets",
    "volume": "reproducibility",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r30yuDPvf2": {
    "title": "A Survey on Transformers in Reinforcement Learning",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YdMrdhGx9y": {
    "title": "A Survey on Causal Discovery Methods for I.I.D. and Time Series Data",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lmXMXP74TO": {
    "title": "Data Distillation: A Survey",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3OSISBQPrM": {
    "title": "On the Predictive Accuracy of Neural Temporal Point Process Models for Continuous-time Event Data",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jh7wH2AzKK": {
    "title": "Augmented Language Models: a Survey",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FByH3qL87G": {
    "title": "On Averaging ROC Curves",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VynY6Bk03b": {
    "title": "How to Reuse and Compose Knowledge for a Lifetime of Tasks: A Survey on Continual Learning and Functional Composition",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ma25S4ludQ": {
    "title": "Know Your Self-supervised Learning: A Survey on Image-based Generative and Discriminative Training",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A8pqQipwkt": {
    "title": "Forces are not Enough: Benchmark and Critical Evaluation for Machine Learning Force Fields with Molecular Simulations",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e0xaRylNuT": {
    "title": "Partition-Based Active Learning for Graph Neural Networks",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AU4qHN2VkS": {
    "title": "Better Theory for SGD in the Nonconvex World",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  }
}