{
  "https://openreview.net/forum?id=YVPb6tyRJu": {
    "title": "FREED++: Improving RL Agents for Fragment-Based Molecule Generation by Thorough Reproduction",
    "volume": "main",
    "abstract": "A rational design of new therapeutic drugs aims to find a molecular structure with desired biological functionality, e.g., an ability to activate or suppress a specific protein via binding to it. Molecular docking is a common technique for evaluating protein-molecule interactions. Recently, Reinforcement Learning (RL) has emerged as a promising approach to generating molecules with the docking score (DS) as a reward. In this work, we reproduce, scrutinize and improve the recent RL model for molecule generation called FREED (Yang et al., 2021). Extensive evaluation of the proposed method reveals several limitations and challenges despite the outstanding results reported for three target proteins. Our contributions include fixing numerous implementation bugs and simplifying the model while increasing its quality, significantly extending experiments, and conducting an accurate comparison with current state-of-the-art methods for protein-conditioned molecule generation. We show that the resulting fixed model is capable of producing molecules with superior docking scores compared to alternative approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Telepov",
      "Artem Tsypin",
      "Kuzma Khrabrov",
      "Sergey Yakukhnov",
      "Pavel Strashnov",
      "Petr Zhilyaev",
      "Egor Rumiantsev",
      "Daniel Ezhov",
      "Manvel Avetisian",
      "Olga Popova",
      "Artur Kadurin"
    ]
  },
  "https://openreview.net/forum?id=Ls1E16bTj8": {
    "title": "In search of projectively equivariant networks",
    "volume": "main",
    "abstract": "Equivariance of linear neural network layers is well studied. In this work, we relax the equivariance condition to only be true in a projective sense. Hereby, we introduce the topic of projective equivariance to the machine learning audience. We theoretically study the relation of projectively and linearly equivariant linear layers. We find that in some important cases, surprisingly, the two types of layers coincide. We also propose a way to construct a projectively equivariant neural network, which boils down to building a standard equivariant network where the linear group representations acting on each intermediate feature space are lifts of projective group representations. Projective equivariance is showcased in two simple experiments. Code for the experiments is provided in the supplementary material",
    "checked": true,
    "id": "87e4e0723dea55ad3779ffe9a047f495990ddd1c",
    "semantic_title": "in search of projectively equivariant networks",
    "citation_count": 0,
    "authors": [
      "Georg BÃ¶kman",
      "Axel Flinth",
      "Fredrik Kahl"
    ]
  },
  "https://openreview.net/forum?id=2wecNCpZ7Y": {
    "title": "Improving Native CNN Robustness with Filter Frequency Regularization",
    "volume": "main",
    "abstract": "Neural networks tend to overfit the training distribution and perform poorly on out-of-distribution data. A conceptually simple solution lies in adversarial training, which introduces worst-case perturbations into the training data and thus improves model generalization to some extent. However, it is only one ingredient towards generally more robust models and requires knowledge about the potential attacks or inference time data corruptions during model training. This paper focuses on the native robustness of models that can learn robust behavior directly from conventional training data without out-of-distribution examples. To this end, we study the frequencies in learned convolution filters. Clean-trained models often prioritize high-frequency information, whereas adversarial training enforces models to shift the focus to low-frequency details during training. By mimicking this behavior through frequency regularization in learned convolution weights, we achieve improved native robustness to adversarial attacks, common corruptions, and other out-of-distribution tests. Additionally, this method leads to more favorable shifts in decision-making towards low-frequency information, such as shapes, which inherently aligns more closely with human vision",
    "checked": true,
    "id": "f2b15de1db12bfaeab8365202e03f364caa1dee2",
    "semantic_title": "improving native cnn robustness with filter frequency regularization",
    "citation_count": 9,
    "authors": [
      "Jovita Lukasik",
      "Paul Gavrikov",
      "Janis Keuper",
      "Margret Keuper"
    ]
  },
  "https://openreview.net/forum?id=wzzrs5QH5k": {
    "title": "Resmax: An Alternative Soft-Greedy Operator for Reinforcement Learning",
    "volume": "main",
    "abstract": "Soft-greedy operators, namely $\\varepsilon$-greedy and softmax, remain a common choice to induce a basic level of exploration for action-value methods in reinforcement learning. These operators, however, have a few critical limitations. In this work, we investigate a simple soft-greedy operator, which we call resmax, that takes actions proportionally to their max action gap: the residual to the estimated maximal value. It is simple to use and ensures coverage of the state-space like $\\varepsilon$-greedy, but focuses exploration more on potentially promising actions like softmax. Further, it does not concentrate probability as quickly as softmax, and so better avoids overemphasizing sub-optimal actions that appear high-valued during learning. Additionally, we prove it is a non-expansion for any fixed exploration hyperparameter, unlike the softmax policy which requires a state-action specific temperature to obtain a non-expansion (called mellowmax). We empirically validate that resmax is comparable to or outperforms $\\varepsilon$-greedy and softmax across a variety of environments in tabular and deep RL",
    "checked": true,
    "id": "8321641f955713efa53caba33b6d7cbf87964f38",
    "semantic_title": "resmax: an alternative soft-greedy operator for reinforcement learning",
    "citation_count": 2,
    "authors": [
      "Erfan Miahi",
      "Revan MacQueen",
      "Alex Ayoub",
      "Abbas Masoumzadeh",
      "Martha White"
    ]
  },
  "https://openreview.net/forum?id=SnPEhMyuYX": {
    "title": "Privacy Budget Tailoring in Private Data Analysis",
    "volume": "main",
    "abstract": "We consider the problem of learning differentially private linear and logistic regression models that do not exhibit disparate performance for minority groups in the data. Small-sized datasets pose a challenging regime for differential privacy; that is, satisfying differential privacy while learning models from data can lead to models with worse accuracy for minority---in size---subgroups. To address this challenge, inspired by Abowd & Schmutte (2018), we propose: (i) to systematically tailor the privacy budget to the different groups, (ii) use linear optimization oracles in a grid to optimize Lagrangian objectives that correspond to fair learning and optimization. We present efficient differentially private algorithms for linear and logistic regression subject to fairness constraints (e.g., bounded group loss) that allocate the privacy budget based on the private standard error of each subgroup in the data. Consequently, the formulation reduces the amount of noise added to these groups, which leads to more accurate models for such groups. We validate the proposed, group-aware budget allocation, method on synthetic and real-world datasets where we show significant reductions in prediction error for the smallest groups, while still preserving sufficient privacy to protect the minority group from re-identification attacks. In addition, we provide sample complexity lower bounds for our problem formulation",
    "checked": true,
    "id": "90934943ff80ffa3e36efaddd23b222f523bf318",
    "semantic_title": "privacy budget tailoring in private data analysis",
    "citation_count": 1,
    "authors": [
      "Daniel Alabi",
      "Chris Wiggins"
    ]
  },
  "https://openreview.net/forum?id=CviCLt44Em": {
    "title": "Smoothed Differential Privacy",
    "volume": "main",
    "abstract": "Differential privacy (DP) is a widely-accepted and widely-applied notion of privacy based on worst-case analysis. Often, DP classifies most mechanisms without additive noise as non-private (Dwork et al., 2014). Thus, additive noises are added to improve privacy (to achieve DP). However, in many real-world applications, adding additive noise is undesirable (Bagdasaryan et al., 2019) and sometimes prohibited (Liu et al., 2020). In this paper, we propose a natural extension of DP following the worst average-case idea behind the celebrated smoothed analysis (Spielman & Teng, May 2004). Our notion, smoothed DP, can effectively measure the privacy leakage of mechanisms without additive noises under realistic settings. We prove that any discrete mechanism with sampling procedures is more private than what DP predicts, while many continuous mechanisms with sampling procedures are still non-private under smoothed DP. In addition, we prove several desirable properties of smoothed DP, including composition, robustness to post-processing, and distribution reduction. Based on those properties, we propose an efficient algorithm to calculate the privacy parameters for smoothed DP. Experimentally, we verify that, according to smoothed DP, the discrete sampling mechanisms are private in real-world elections, and some discrete neural networks can be private without adding any additive noise. We believe that these results contribute to the theoretical foundation of realistic privacy measures beyond worst-case analysis",
    "checked": true,
    "id": "a2beb939daba8c6a835accb98afe1d79990e7aae",
    "semantic_title": "smoothed differential privacy",
    "citation_count": 0,
    "authors": [
      "Ao Liu",
      "Yu-Xiang Wang",
      "Lirong Xia"
    ]
  },
  "https://openreview.net/forum?id=sY75NqDRk1": {
    "title": "Distributed Architecture Search Over Heterogeneous Distributions",
    "volume": "main",
    "abstract": "Federated learning (FL) is an efficient learning framework that assists distributed machine learning when data cannot be shared with a centralized server. Recent advancements in FL use predefined architecture-based learning for all clients. However, given that clients' data are invisible to the server and data distributions are non-identical across clients, a predefined architecture discovered in a centralized setting may not be an optimal solution for all the clients in FL. Motivated by this challenge, we introduce SPIDER, an algorithmic framework that aims to Search PersonalIzed neural architecture for feDERated learning. SPIDER is designed based on two unique features: (1) alternately optimizing one architecture-homogeneous global model in a generic FL manner and architecture-heterogeneous local models that are connected to the global model by weight-sharing-based regularization, (2) achieving architecture-heterogeneous local models by a perturbation-based neural architecture search method. Experimental results demonstrate superior prediction performance compared with other state-of-the-art personalization methods",
    "checked": true,
    "id": "e3d4c7af9fe350ec99e77aff9c1e729da6363c04",
    "semantic_title": "distributed architecture search over heterogeneous distributions",
    "citation_count": 0,
    "authors": [
      "Erum Mushtaq",
      "Chaoyang He",
      "Jie Ding",
      "Salman Avestimehr"
    ]
  },
  "https://openreview.net/forum?id=P9haooN9v2": {
    "title": "DreamEdit: Subject-driven Image Editing",
    "volume": "main",
    "abstract": "Subject-driven image generation aims at generating images containing customized subjects, which has recently drawn enormous attention from the research community. Nevertheless, the previous works cannot precisely control the background and position of the target subject. In this work, we aspire to fill the void of the existing subject-driven generation tasks. To this end, we propose two novel subject-driven editing sub-tasks, i.e., Subject Replacement and Subject Addition. The new tasks are challenging in multiple aspects: replacing a subject with a customized one can totally change its shape, texture, and color, while adding a target subject to a designated position in a provided scene necessitates a rational context-aware posture of the subject. To conquer these two novel tasks, we first manually curate a new dataset called DreamEditBench containing 22 different types of subjects, and 440 source images, which cover diverse scenarios with different difficulty levels. We plan to host DreamEditBench as a platform and hire trained evaluators for standardized human evaluation. We also devise an innovative method DreamEditor to resolve these tasks by performing iterative generation, which enables a smooth adaptation to the customized subject. In this project, we conduct automatic and human evaluations to understand the performance of our DreamEditor and baselines on DreamEditBench. We found that the new tasks are challenging for the existing models. For Subject Replacement, we found that the existing models are particularly sensitive to the shape and color of the original subject. When the original subject and the customized subject are highly different, the model failure rate will dramatically increase. For Subject Addition, we found that the existing models cannot easily blend the customized subjects into the background smoothly, which causes noticeable artifacts in the generated image. We hope that DreamEditBench can become a standardized platform to enable future investigations towards building more controllable subject-driven image editing. Our project and benchmark homepage is https://dreameditbenchteam.github.io/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianle Li",
      "Max Ku",
      "Cong Wei",
      "Wenhu Chen"
    ]
  },
  "https://openreview.net/forum?id=4uflhObpcp": {
    "title": "UnIVAL: Unified Model for Image, Video, Audio and Language Tasks",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have made the ambitious quest for generalist agents significantly far from being a fantasy. A key hurdle for building such general models is the diversity and heterogeneity of tasks and modalities. A promising solution is unification, allowing the support of a myriad of tasks and modalities within one unified framework. While few large models (e.g., Flamingo (Alayrac et al. 2022)), trained on massive datasets, can support more than two modalities, current small to mid-scale unified models are still limited to 2 modalities, usually image-text or video-text. The question that we ask is: is it possible to build efficiently a unified model that can support all modalities? To answer this, we propose UnIVAL, a step further towards this ambitious goal. Without relying on fancy datasets sizes or models with billions of parameters, the ~ 0.25B parameter UnIVAL model goes beyond two modalities and unifies text, images, video, and audio into a single model. Our model is efficiently pretrained on many tasks, based on task balancing and multimodal curriculum learning. UnIVAL shows competitive performance to existing state-of-the-art approaches, across image and video-text tasks. The feature representations learned from image and video-text modalities, allows the model to achieve competitive performance when finetuned on audio-text tasks, despite not being pretrained on audio. Thanks to the unified model, we propose a novel study on multimodal model merging via weight interpolation of models trained on different multimodal tasks, showing their benefits in particular for out-of-distribution generalization. Finally, we motivate unification by showing the synergy between tasks. The model weights and code are available at: https://github.com/mshukor/UnIVAL",
    "checked": false,
    "id": "0fe88452660cb8a0e37f54bcd44f3cd6504354b5",
    "semantic_title": "unified model for image, video, audio and language tasks",
    "citation_count": 46,
    "authors": [
      "Mustafa Shukor",
      "Corentin Dancette",
      "Alexandre Rame",
      "Matthieu Cord"
    ]
  },
  "https://openreview.net/forum?id=vfT4YuzAYA": {
    "title": "IndicTrans2: Towards High-Quality and Accessible Machine Translation Models for all 22 Scheduled Indian Languages",
    "volume": "main",
    "abstract": "India has a rich linguistic landscape, with languages from 4 major language families spoken by over a billion people. 22 of these languages listed in the Constitution of India (referred to as scheduled languages) are the focus of this work. Given the linguistic diversity, high-quality and accessible Machine Translation (MT) systems are essential in a country like India. Before this work, there was (i) no parallel training data spanning all 22 languages, (ii) no robust benchmarks covering all these languages and containing content relevant to India, and (iii) no existing translation models that support all 22 scheduled languages of India. In this work, we aim to address this gap by focusing on the missing pieces required for enabling wide, easy, and open access to good machine translation systems for all 22 scheduled Indian languages. We identify four key areas of improvement: curating and creating larger training datasets, creating diverse and high-quality benchmarks, training multilingual models, and releasing models with open access. Our first contribution is the release of the Bharat Parallel Corpus Collection (BPCC), the largest publicly available parallel corpora for Indic languages. BPCC contains a total of 230M bitext pairs, of which a total of 126M were newly added, including 644K manually translated sentence pairs created as part of this work. Our second contribution is the release of the first $n$-way parallel benchmark covering all 22 Indian languages, featuring diverse domains, Indian-origin content, and conversational test sets. Next, we present IndicTrans2, the first translation model to support all 22 languages, surpassing existing models in performance on multiple existing and new benchmarks created as a part of this work. Lastly, to promote accessibility and collaboration, we release our models and associated data with permissive licenses at https://github.com/AI4Bharat/IndicTrans2",
    "checked": true,
    "id": "59919e80ede70e82a70e8f76f533360c22f37275",
    "semantic_title": "indictrans2: towards high-quality and accessible machine translation models for all 22 scheduled indian languages",
    "citation_count": 138,
    "authors": [
      "Jay Gala",
      "Pranjal A Chitale",
      "A K Raghavan",
      "Varun Gumma",
      "Sumanth Doddapaneni",
      "Aswanth Kumar M",
      "Janki Atul Nawale",
      "Anupama Sujatha",
      "Ratish Puduppully",
      "Vivek Raghavan",
      "Pratyush Kumar",
      "Mitesh M Khapra",
      "Raj Dabre",
      "Anoop Kunchukuttan"
    ]
  },
  "https://openreview.net/forum?id=4Hq816XDDG": {
    "title": "Towards Optimization-Friendly Binary Neural Network",
    "volume": "main",
    "abstract": "Binary neural networks (BNNs) are a promising approach for compressing and accelerating deep learning models, especially in resource-constrained environments. However, the optimization gap between BNNs and their full-precision counterparts has long been an open problem limiting their performance. In this work, we propose a novel optimization pipeline to enhance the performance of BNNs. The main approach includes three key components: (1) BNext, a strong binary baseline based on an optimization-friendly basic block design, (2) knowledge complexity, a simple yet effective teacher-selection metric taking the capacity gap between teachers and binary students under consideration, (3) consecutive knowledge distillation (CKD), a novel multi-round optimization technique to transfer high-confidence knowledge from strong teachers to low-capacity BNNs. We empirically validate the superiority of the method on several vision classification tasks CIFAR-10/100 & ImageNet. For instance, the BNext family outperforms previous BNNs under different capacity levels and contributes the first binary neural network to reach the state-of-the-art 80.57\\% Top-1 accuracy on ImageNet with 0.82 GOPS, which verifies the potential of BNNs and already contributes a strong baseline for future research on high-accuracy BNNs. The code will be publicly available at (blind URL, see supplementary material)",
    "checked": true,
    "id": "7f9e7c1497e87e6b584046bb94009da92fe0d90a",
    "semantic_title": "towards optimization-friendly binary neural network",
    "citation_count": 2,
    "authors": [
      "Nianhui Guo",
      "Joseph Bethge",
      "Hong Guo",
      "Christoph Meinel",
      "Haojin Yang"
    ]
  },
  "https://openreview.net/forum?id=ExbGarTbLE": {
    "title": "Equivariant MuZero",
    "volume": "main",
    "abstract": "Deep reinforcement learning has shown lots of success in closed, well-defined domains such as games (Chess, Go, StarCraft). The next frontier is real-world scenarios, where setups are numerous and varied. For this, agents need to learn the underlying environment dynamics, so as to robustly generalise to conditions that differ from those they were trained on. Model-based reinforcement learning algorithms, such as MuZero or Dreamer, aim to accomplish this by learning a world model. However, leveraging a world model has not yet consistently shown greater generalisation capabilities compared to model-free alternatives. In this work, we propose improving the data efficiency and generalisation capabilities of MuZero by explicitly incorporating the \\emph{symmetries} of the environment in its world-model architecture. We prove that, so long as the neural networks used by MuZero are equivariant to a particular symmetry group acting on the environment, the entirety of MuZero's action-selection algorithm will also be equivariant to that group. As such, Equivariant MuZero is guaranteed to behave symmetrically in symmetrically-transformed states, and will hence be more data-efficient when learning its world models. We evaluate Equivariant MuZero on procedurally-generated MiniPacman and on Chaser from the ProcGen suite: training on a set of mazes, and then testing on unseen rotated versions, demonstrating the benefits of equivariance. We verify that our improvements hold even when only some of the components of Equivariant MuZero obey strict equivariance, which highlights the robustness of our construction",
    "checked": true,
    "id": "2e8355d05976a96018757af12177a78f1911e912",
    "semantic_title": "equivariant muzero",
    "citation_count": 4,
    "authors": [
      "Andreea Deac",
      "Theophane Weber",
      "George Papamakarios"
    ]
  },
  "https://openreview.net/forum?id=hFsr59Imzm": {
    "title": "On the Efficacy of Differentially Private Few-shot Image Classification",
    "volume": "main",
    "abstract": "There has been significant recent progress in training differentially private (DP) models which achieve accuracy that approaches the best non-private models. These DP models are typically pretrained on large public datasets and then fine-tuned on private downstream datasets that are relatively large and similar in distribution to the pretraining data. However, in many applications including personalization and federated learning, it is crucial to perform well (i) in the few-shot setting, as obtaining large amounts of labeled data may be problematic; and (ii) on datasets from a wide variety of domains for use in various specialist settings. To understand under which conditions few-shot DP can be effective, we perform an exhaustive set of experiments that reveals how the accuracy and vulnerability to attack of few-shot DP image classification models are affected as the number of shots per class, privacy level, model architecture, downstream dataset, and subset of learnable parameters in the model vary. We show that to achieve DP accuracy on par with non-private models, the shots per class must be increased as the privacy level increases. We also show that learning parameter-efficient FiLM adapters under DP is competitive with learning just the final classifier layer or learning all of the network parameters. Finally, we evaluate DP federated learning systems and establish state-of-the-art performance on the challenging FLAIR benchmark",
    "checked": true,
    "id": "947cf0610c58fb87e9b4d09d3e4e99311d3a7a05",
    "semantic_title": "on the efficacy of differentially private few-shot image classification",
    "citation_count": 12,
    "authors": [
      "Marlon Tobaben",
      "Aliaksandra Shysheya",
      "John F Bronskill",
      "Andrew Paverd",
      "Shruti Tople",
      "Santiago Zanella-Beguelin",
      "Richard E Turner",
      "Antti Honkela"
    ]
  },
  "https://openreview.net/forum?id=sbkZKBVC31": {
    "title": "Mixture of Dynamical Variational Autoencoders for Multi-Source Trajectory Modeling and Separation",
    "volume": "main",
    "abstract": "In this paper, we propose a latent-variable generative model called mixture of dynamical variational autoencoders (MixDVAE) to model the dynamics of a system composed of multiple moving sources. A DVAE model is pre-trained on a single-source dataset to capture the source dynamics. Then, multiple instances of the pre-trained DVAE model are integrated into a multi-source mixture model with a discrete observation-to-source assignment latent variable. The posterior distributions of both the discrete observation-to-source assignment variable and the continuous DVAE variables representing the sources content/position are estimated using the variational expectation-maximization algorithm, leading to multi-source trajectories estimation. We illustrate the versatility of the proposed MixDVAE model on two tasks: a computer vision task, namely multi-object tracking, and an audio processing task, namely single-channel audio source separation. Experimental results show that the proposed method works well on these two tasks, and outperforms several baseline methods",
    "checked": true,
    "id": "b0218ce7d6f38137ba8523b1d268e688052519f2",
    "semantic_title": "mixture of dynamical variational autoencoders for multi-source trajectory modeling and separation",
    "citation_count": 2,
    "authors": [
      "Xiaoyu Lin",
      "Laurent Girin",
      "Xavier Alameda-Pineda"
    ]
  },
  "https://openreview.net/forum?id=f7a8XCRtUu": {
    "title": "Fast Slate Policy Optimization: Going Beyond Plackett-Luce",
    "volume": "main",
    "abstract": "An increasingly important building block of large scale machine learning systems is based on returning slates; an ordered lists of items given a query. Applications of this technology include: search, information retrieval and recommender systems. When the action space is large, decision systems are restricted to a particular structure to complete online queries quickly. This paper addresses the optimization of these large scale decision systems given an arbitrary reward function. We cast this learning problem in a policy optimization framework and propose a new class of policies, born from a novel relaxation of decision functions. This results in a simple, yet efficient learning algorithm that scales to massive action spaces. We compare our method to the commonly adopted Plackett-Luce policy class and demonstrate the effectiveness of our approach on problems with action space sizes in the order of millions",
    "checked": true,
    "id": "583eec43ce04e3b7c58247832d766c4e257fb98a",
    "semantic_title": "fast slate policy optimization: going beyond plackett-luce",
    "citation_count": 4,
    "authors": [
      "Otmane Sakhi",
      "David Rohde",
      "Nicolas Chopin"
    ]
  },
  "https://openreview.net/forum?id=QCjMJfSnYk": {
    "title": "Error bounds and dynamics of bootstrapping in actor-critic reinforcement learning",
    "volume": "main",
    "abstract": "Actor-critic algorithms such as DDPG, TD3, and SAC, which are built on Silver's deterministic policy gradient theorem, are among the most successful reinforcement-learning methods, but their mathematical basis is not entirely clear. In particular, the critic networks in these algorithms learn to estimate action-value functions by a \"bootstrapping\" technique based on Bellman error, and it is unclear why this approach works so well in practice, given that Bellman error is only very loosely related to value error, i.e. to the inaccuracy of the action-value estimate. Here we show that policy training in this class of actor-critic methods depends not on the accuracy of the critic's action-value estimate but on how well the critic estimates the gradient of the action-value, which is better assessed using what we call difference error. We show that this difference error is closely related to the Bellman error â a finding which helps to explain why Bellman-based bootstrapping leads to good policies. Further, we show that value error and difference error show different dynamics along on-policy trajectories through state-action space: value error is a low-pass anticausal (i.e., backward-in-time) filter of Bellman error, and therefore accumulates along trajectories, whereas difference error is a high-pass filter of Bellman error. It follows that techniques which reduce the high-frequency Fourier components of the Bellman error may improve policy training even if they increase the actual size of the Bellman errors. These findings help to explain certain aspects of actor-critic methods that are otherwise theoretically puzzling, such as the use of policy (as distinct from exploratory) noise, and they suggest other measures that may improve these methods",
    "checked": true,
    "id": "d4c92347bff3a51245f2c9f5e2af861cf264d94b",
    "semantic_title": "error bounds and dynamics of bootstrapping in actor-critic reinforcement learning",
    "citation_count": 0,
    "authors": [
      "Ahmed J Zerouali",
      "Douglas Blair Tweed"
    ]
  },
  "https://openreview.net/forum?id=NnUmg1chLL": {
    "title": "Federated Minimax Optimization with Client Heterogeneity",
    "volume": "main",
    "abstract": "Minimax optimization has seen a surge in interest with the advent of modern applications such as GANs, and it is inherently more challenging than simple minimization. The difficulty is exacerbated by the training data residing at multiple edge devices or \\textit{clients}, especially when these clients can have heterogeneous datasets and heterogeneous local computation capabilities. We propose a general federated minimax optimization framework that subsumes such settings and several existing methods like Local SGDA. We show that naive aggregation of model updates made by clients running unequal number of local steps can result in optimizing a mismatched objective function -- a phenomenon previously observed in standard federated minimization. To fix this problem, we propose normalizing the client updates by the number of local steps. We analyze the convergence of the proposed algorithm for classes of nonconvex-concave and nonconvex-nonconcave functions and characterize the impact of heterogeneous client data, partial client participation, and heterogeneous local computations. For all the function classes considered, we significantly improve the existing computation and communication complexity results. Experimental results support our theoretical claims",
    "checked": true,
    "id": "410dc1db47548691bb5a1de2db247bae81324098",
    "semantic_title": "federated minimax optimization with client heterogeneity",
    "citation_count": 9,
    "authors": [
      "Pranay Sharma",
      "Rohan Panda",
      "Gauri Joshi"
    ]
  },
  "https://openreview.net/forum?id=Uj6MRfR1P5": {
    "title": "Towards Fair Video Summarization",
    "volume": "main",
    "abstract": "Automated video summarization is a vision task that aims to generate concise summaries of lengthy videos. Recent advancements in deep learning have led to highly performant video summarization models; however, there has been a lack of attention given to fairness and unbiased representation in the generated summaries. To bridge this gap, we introduce and analytically define the fair video summarization problem, and demonstrate its connections to the well-established problem of fair clustering. To facilitate fair model development, we also introduce the FairVidSum dataset, which is similar in design to state-of-the-art video summarization datasets such as TVSum and SumMe, but also includes annotations for sensitive attributes and individuals alongside frame importance scores. Finally, we propose the SumBal metric for quantifying the fairness of an outputted video summary. We conduct extensive experiments to benchmark the fairness of various state-of-the-art video summarization models. Our results highlight the need for better models that balance accuracy and fairness to ensure equitable representation and inclusion in summaries. For completeness, we also provide a novel fair-only baseline, FVS-LP, to showcase the fairness-utility gap models can improve upon",
    "checked": true,
    "id": "a0786692ef0768176bf1f59489d95dab69f43d28",
    "semantic_title": "towards fair video summarization",
    "citation_count": 3,
    "authors": [
      "Anshuman Chhabra",
      "Kartik Patwari",
      "Chandana Kuntala",
      "Sristi",
      "Deepak Kumar Sharma",
      "Prasant Mohapatra"
    ]
  },
  "https://openreview.net/forum?id=FWyabz82fH": {
    "title": "Uncertainty Estimation for Computed Tomography with a Linearised Deep Image Prior",
    "volume": "main",
    "abstract": "Existing deep-learning based tomographic image reconstruction methods do not provide accurate uncertainty estimates of their reconstructions, hindering their real-world deployment. This paper develops a method, termed as linearised deep image prior (DIP), to estimate the uncertainty associated with reconstructions produced by the DIP with total variation (TV) regularisation. We endow the DIP with conjugate Gaussian-linear model type error-bars computed from a local linearisation of the neural network around its optimised parameters. To preserve conjugacy, we approximate the TV regulariser with a Gaussian surrogate. This approach provides pixel-wise uncertainty estimates and a marginal likelihood objective for hyperparameter optimisation. We demonstrate the method on synthetic data and real-measured high-resolution 2D $\\mu$CT data, and show that it provides superior calibration of uncertainty estimates relative to previous probabilistic formulations of the~DIP. Our code is available at https://github.com/educating-dip/bayes_dip",
    "checked": true,
    "id": "2eec71f84d06d4f554a4243874fe3d1780ba90c2",
    "semantic_title": "uncertainty estimation for computed tomography with a linearised deep image prior",
    "citation_count": 10,
    "authors": [
      "Javier Antoran",
      "Riccardo Barbano",
      "Johannes Leuschner",
      "JosÃ© Miguel HernÃ¡ndez-Lobato",
      "Bangti Jin"
    ]
  },
  "https://openreview.net/forum?id=231ZzrLC8X": {
    "title": "Early Stopping for Deep Image Prior",
    "volume": "main",
    "abstract": "Deep image prior (DIP) and its variants have shown remarkable potential to solve inverse problems in computational imaging (CI), needing no separate training data. Practical DIP models are often substantially overparameterized. During the learning process, these models first learn the desired visual content and then pick up potential modeling and observational noise, i.e., performing early learning then overfitting. Thus, the practicality of DIP hinges on early stopping (ES) that can capture the transition period. In this regard, most previous DIP works for CI tasks only demonstrate the potential of the models, reporting the peak performance against the ground truth but providing no clue about how to operationally obtain near-peak performance without access to the ground truth. In this paper, we set to break this practicality barrier of DIP, and propose an effective ES strategy that consistently detects near-peak performance across several CI tasks and DIP variants. Simply based on the running variance of DIP intermediate reconstructions, our ES method not only outpaces the existing ones---which only work in very narrow regimes, but also remains effective when combined with methods that try to mitigate overfitting",
    "checked": true,
    "id": "1f82c4a2962a3e6fb0800661ae7239092037bd08",
    "semantic_title": "early stopping for deep image prior",
    "citation_count": 64,
    "authors": [
      "Hengkang Wang",
      "Taihui Li",
      "Zhong Zhuang",
      "Tiancong Chen",
      "Hengyue Liang",
      "Ju Sun"
    ]
  },
  "https://openreview.net/forum?id=xflYdGZMpv": {
    "title": "Image retrieval outperforms diffusion models on data augmentation",
    "volume": "main",
    "abstract": "Many approaches have been proposed to use diffusion models to augment training datasets for downstream tasks, such as classification. However, diffusion models are themselves trained on large datasets, often with noisy annotations, and it remains an open question to which extent these models contribute to downstream classification performance. In particular, it remains unclear if they generalize enough to improve over directly using the additional data of their pre-training process for augmentation. We systematically evaluate a range of existing methods to generate images from diffusion models and study new extensions to assess their benefit for data augmentation. Personalizing diffusion models towards the target data outperforms simpler prompting strategies. However, using the pre-training data of the diffusion model alone, via a simple nearest-neighbor retrieval procedure, leads to even stronger downstream performance. Our study explores the potential of diffusion models in generating new training data, and surprisingly finds that these sophisticated models are not yet able to beat a simple and strong image retrieval baseline on simple downstream vision tasks",
    "checked": true,
    "id": "c22dfd7dd7364d96e03e47998819cc188a53d3f4",
    "semantic_title": "image retrieval outperforms diffusion models on data augmentation",
    "citation_count": 17,
    "authors": [
      "Max F Burg",
      "Florian Wenzel",
      "Dominik Zietlow",
      "Max Horn",
      "Osama Makansi",
      "Francesco Locatello",
      "Chris Russell"
    ]
  },
  "https://openreview.net/forum?id=Mbc58EzF5q": {
    "title": "Transport with Support: Data-Conditional Diffusion Bridges",
    "volume": "main",
    "abstract": "The dynamic SchrÃ¶dinger bridge problem provides an appealing setting for solving constrained time-series data generation tasks posed as optimal transport problems. It consists of learning non-linear diffusion processes using efficient iterative solvers. Recent works have demonstrated state-of-the-art results (eg., in modelling single-cell embryo RNA sequences or sampling from complex posteriors) but are limited to learning bridges with only initial and terminal constraints. Our work extends this paradigm by proposing the Iterative Smoothing Bridge (ISB). We integrate Bayesian filtering and optimal control into learning the diffusion process, enabling the generation of constrained stochastic processes governed by sparse observations at intermediate stages and terminal constraints. We assess the effectiveness of our method on synthetic and real-world data generation tasks and we show that the ISB generalises well to high-dimensional data, is computationally efficient, and provides accurate estimates of the marginals at intermediate and terminal times",
    "checked": true,
    "id": "8d61874776bf54ced20fffe3b2258da85090d7b8",
    "semantic_title": "transport with support: data-conditional diffusion bridges",
    "citation_count": 7,
    "authors": [
      "Ella Tamir",
      "Martin Trapp",
      "Arno Solin"
    ]
  },
  "https://openreview.net/forum?id=w4MoQ39zmc": {
    "title": "Local Function Complexity for Active Learning via Mixture of Gaussian Processes",
    "volume": "main",
    "abstract": "Inhomogeneities in real-world data, e.g., due to changes in the observation noise level or variations in the structural complexity of the source function, pose a unique set of challenges for statistical inference. Accounting for them can greatly improve predictive power when physical resources or computation time is limited. In this paper, we draw on recent theoretical results on the estimation of local function complexity (LFC), derived from the domain of local polynomial smoothing (LPS), to establish a notion of local structural complexity, which is used to develop a model-agnostic active learning (AL) framework. Due to its reliance on pointwise estimates, the LPS model class is not robust and scalable concerning large input space dimensions that typically come along with real-world problems. Here, we derive and estimate the Gaussian process regression (GPR)-based analog of the LPS-based LFC and use it as a substitute in the above framework to make it robust and scalable. We assess the effectiveness of our LFC estimate in an AL application on a prototypical low-dimensional synthetic dataset, before taking on the challenging real-world task of reconstructing a quantum chemical force field for a small organic molecule and demonstrating state-of-the-art performance with a significantly reduced training demand",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Danny Panknin",
      "Stefan Chmiela",
      "Klaus Robert Muller",
      "Shinichi Nakajima"
    ]
  },
  "https://openreview.net/forum?id=vJcTm2v9Ku": {
    "title": "Towards a General Transfer Approach for Policy-Value Networks",
    "volume": "main",
    "abstract": "Transferring trained policies and value functions from one task to another, such as one game to another with a different board size, board shape, or more substantial rule changes, is a challenging problem. Popular benchmarks for reinforcement learning (RL), such as Atari games and ProcGen, have limited variety especially in terms of action spaces. Due to a focus on such benchmarks, the development of transfer methods that can also handle changes in action spaces has received relatively little attention. Furthermore, we argue that progress towards more general methods should include benchmarks where new problem instances can be described by domain experts, rather than machine learning experts, using convenient, high-level domain specific languages (DSLs). In addition to enabling end users to more easily describe their problems, user-friendly DSLs also contain relevant task information which can be leveraged to make effective zero-shot transfer plausibly achievable. As an example, we use the Ludii general game system, which includes a highly varied set of over 1000 distinct games described in such a language. We propose a simple baseline approach for transferring fully convolutional policy-value networks, which are used to guide search agents similar to AlphaZero, between any pair of games modelled in this system. Extensive results---including various cases of highly successful zero-shot transfer---are provided for a wide variety of source and target games",
    "checked": true,
    "id": "80453413e567db49cb355520d9c7aa063138b85b",
    "semantic_title": "towards a general transfer approach for policy-value networks",
    "citation_count": 3,
    "authors": [
      "Dennis J. N. J. Soemers",
      "Vegard Mella",
      "Eric Piette",
      "Matthew Stephenson",
      "Cameron Browne",
      "Olivier Teytaud"
    ]
  },
  "https://openreview.net/forum?id=Id10mlBjcx": {
    "title": "ProtoCaps: A Fast and Non-Iterative Capsule Network Routing Method",
    "volume": "main",
    "abstract": "Capsule Networks have emerged as a powerful class of deep learning architectures, known for robust performance with relatively few parameters compared to Convolutional Neural Networks (CNNs). However, their inherent efficiency is often overshadowed by their slow, iterative routing mechanisms which establish connections between Capsule layers, posing computational challenges resulting in an inability to scale. In this paper, we introduce a novel, non-iterative routing mechanism, inspired by trainable prototype clustering. This innovative approach aims to mitigate computational complexity, while retaining, if not enhancing, performance efficacy. Furthermore, we harness a shared Capsule subspace, negating the need to project each lower-level Capsule to each higher-level Capsule, thereby significantly reducing memory requisites during training. Our approach demonstrates superior results compared to the current best non-iterative Capsule Network and tests on the Imagewoof dataset, which is too computationally demanding to handle efficiently by iterative approaches. Our findings underscore the potential of our proposed methodology in enhancing the operational efficiency and performance of Capsule Networks, paving the way for their application in increasingly complex computational scenarios. Code is available at https://github.com/mileseverett/ProtoCaps",
    "checked": true,
    "id": "3f870b61566940acdb565e88bd4f5c0183c03827",
    "semantic_title": "protocaps: a fast and non-iterative capsule network routing method",
    "citation_count": 4,
    "authors": [
      "Miles Everett",
      "Mingjun Zhong",
      "Georgios Leontidis"
    ]
  },
  "https://openreview.net/forum?id=t4p612DftO": {
    "title": "Detecting danger in gridworlds using Gromov's Link Condition",
    "volume": "main",
    "abstract": "Gridworlds have been long-utilised in AI research, particularly in reinforcement learning, as they provide simple yet scalable models for many real-world applications such as robot navigation, emergent behaviour, and operations research. We initiate a study of gridworlds using the mathematical framework of reconfigurable systems and state complexes due to Abrams, Ghrist & Peterson. State complexes, a higher-dimensional analogue of state graphs, represent all possible configurations of a system as a single geometric space, thus making them conducive to study using geometric, topological, or combinatorial methods. The main contribution of this work is a modification to the original Abrams, Ghrist & Peterson setup which we introduce to capture agent braiding and thereby more naturally represent the topology of gridworlds. With this modification, the state complexes may exhibit geometric defects (failure of Gromov's Link Condition). Serendipitously, we discover these failures for agent-only cases occur exactly where undesirable or dangerous states appear in the gridworld. Our results therefore provide a novel method for seeking guaranteed safety limitations in discrete task environments with single or multiple agents, and offer useful safety information (in geometric and topological forms) for incorporation in or analysis of machine learning systems. More broadly, our work introduces tools from geometric group theory and combinatorics to the AI community and demonstrates a proof-of-concept for this geometric viewpoint of the task domain through the example of simple environments",
    "checked": true,
    "id": "7d2d1e91f66790aaeefa123304be153297c8a82b",
    "semantic_title": "detecting danger in gridworlds using gromov's link condition",
    "citation_count": 2,
    "authors": [
      "Thomas F Burns",
      "Robert Tang"
    ]
  },
  "https://openreview.net/forum?id=75CcopPxIr": {
    "title": "Partial Optimal Transport for Support Subset Selection",
    "volume": "main",
    "abstract": "In probabilistic terms, optimal transport aims to find a joint distribution that couples two distributions and minimizes the cost of transforming one distribution to another. Any feasible coupling necessarily maintains the support of both distributions. However, maintaining the entire support is not ideal when only a subset of one of the distributions, namely the source, is assumed to align with the other target distribution. For these cases, which are common in machine learning applications, we study the semi-relaxed partial optimal transport problem that relaxes the constraints on the joint distribution allowing it to under-represent a subset of the source by over-representing other subsets of the source by a constant factor. In the discrete distribution case, such as in the case of two samples from continuous random variables, optimal transport with the relaxed constraints is a linear program. When sufficiently relaxed, the solution has a source marginal with only a subset of its original support. We investigate the scaling path of solutions, specifically the relaxed marginal distribution for the source, across different relaxations and show that it is distinct from the solutions from penalty-based semi-relaxed unbalanced optimal transport problems and fully-relaxed partial optimal transport, which have previously been explored. We demonstrate the usefulness of this support subset selection in applications such as color transfer, partial point cloud alignment, and semi-supervised machine learning, where a part of data is curated to have reliable labels and another part is unlabeled or has unreliable labels. Our experiments show that optimal transport under the relaxed constraint can improve the performance of these applications by allowing for more flexible alignment between distributions",
    "checked": true,
    "id": "7d2880a2eea9196780d33e7173699d2b9d80e5b6",
    "semantic_title": "partial optimal transport for support subset selection",
    "citation_count": 2,
    "authors": [
      "Bilal Riaz",
      "Yuksel Karahan",
      "Austin J. Brockmeier"
    ]
  },
  "https://openreview.net/forum?id=KrequDpWzt": {
    "title": "Wrapped $\\beta$-Gaussians with compact support for exact probabilistic modeling on manifolds",
    "volume": "main",
    "abstract": "We introduce wrapped $\\beta$-Gaussians, a family of wrapped distributions on Riemannian manifolds, supporting efficient reparametrized sampling, as well as exact density estimation, effortlessly supporting high dimensions and anisotropic scale parameters. We extend Fenchel-Young losses for geometry-aware learning with wrapped $\\beta$-Gaussians, and demonstrate the efficacy of our proposed family in a suite of experiments on hypersphere and rotation manifolds: data fitting, hierarchy encoding, generative modeling with variational autoencoders, and multilingual word embedding alignment",
    "checked": false,
    "id": "1ece7fca5488633dda8fd9b16305caabc82f6810",
    "semantic_title": "wrapped Î²-gaussians with compact support for exact probabilistic modeling on manifolds",
    "citation_count": 1,
    "authors": [
      "Sergey Troshin",
      "Vlad Niculae"
    ]
  },
  "https://openreview.net/forum?id=0WKTmrVkd2": {
    "title": "GIT-Net: Generalized Integral Transform for Operator Learning",
    "volume": "main",
    "abstract": "This article introduces GIT-Net, a deep neural network architecture for approximating Partial Differential Equation (PDE) operators, inspired by integral transform operators. GIT-NET harnesses the fact that common differential operators commonly used for defining PDEs can often be represented parsimoniously when expressed in specialized functional bases (e.g., Fourier basis). Unlike rigid integral transforms, GIT-Net parametrizes adaptive generalized integral transforms with deep neural networks. When compared to several recently proposed alternatives, GIT-Net's computational and memory requirements scale gracefully with mesh discretizations, facilitating its application to PDE problems on complex geometries. Numerical experiments demonstrate that GIT-Net is a competitive neural network operator, exhibiting small test errors and low evaluations across a range of PDE problems. This stands in contrast to existing neural network operators, which typically excel in just one of these areas",
    "checked": true,
    "id": "ef45194c431ebddf869fc8191ab91c105389e01b",
    "semantic_title": "git-net: generalized integral transform for operator learning",
    "citation_count": 0,
    "authors": [
      "Chao Wang",
      "Alexandre H. Thiery"
    ]
  },
  "https://openreview.net/forum?id=sUlbRfLijj": {
    "title": "Semi-Supervised Single Domain Generalization with Label-Free Adversarial Data Augmentation",
    "volume": "main",
    "abstract": "Domain generalization (DG) has attracted increasing attention recently, as it seeks to improve the generalization ability of visual recognition models to unseen target domains. DG leverages multiple source domains for model training, while single domain generalization (SDG) further restricts such setting by exploiting only a single source domain. Nevertheless, both DG and SDG assume that the source domains are fully labeled, which might not be practical in many real world scenarios. In this paper, we present a new problem, i.e., semi-supervised single domain generalization (SS-SDG), which aims to train a model with a partially labeled single source domain to generalize to multiple unseen testing domains. We propose an effective framework to address this problem. In particular, we design a label-free adversarial data augmentation strategy to diversify the source domain, and propose a novel multi-pair FixMatch loss to generalize classifiers to unseen testing domains. Extensive experiments on OfficeHome, PACS and DomainNet20 datasets show that our method surpasses the latest SDG and semi-supervised methods. Moreover, on PACS and DomainNet20, our method approaches the fully supervised ERM upper bound within $5\\%$ gap, but only uses less than $8\\%$ of the labels",
    "checked": true,
    "id": "8fb4eda80a0fe08b32e3c016ad6ed96131568ac4",
    "semantic_title": "semi-supervised single domain generalization with label-free adversarial data augmentation",
    "citation_count": 2,
    "authors": [
      "Ronghang Zhu",
      "Xiang Yu",
      "Sheng Li"
    ]
  },
  "https://openreview.net/forum?id=jpZmhiIys1": {
    "title": "Beyond Boundaries: A Novel Data-Augmentation Discourse for Open Domain Generalization",
    "volume": "main",
    "abstract": "The problem of Open Domain Generalization (ODG) is multifaceted, encompassing shifts in domains and labels across all source and target domains. Existing approaches have encountered challenges such as style bias towards training domains, insufficient feature-space disentanglement to highlight semantic features, and discriminativeness of the latent space. Additionally, they rely on a confidence-based target outlier detection approach, which can lead to misclassifications when target open samples visually align with the source domain data. In response to these challenges, we present a solution named \\textsc{ODG-Net}. We aim to create a direct open-set classifier within a \\textit{discriminative}, \\textit{unbiased}, and \\textit{disentangled} semantic embedding space. To enrich data density and diversity, we introduce a generative augmentation framework that produces \\textit{style-interpolated} novel domains for closed-set images and novel pseudo-open images by \\textit{interpolating the contents of paired training images}. Our augmentation strategy skillfully utilizes \\textit{disentangled style and content information} to synthesize images effectively. Furthermore, we tackle the issue of style bias by representing all images in relation to all source domain properties, which effectively accentuates complementary visual features. Consequently, we train a multi-class semantic object classifier, incorporating both closed and open class classification capabilities, along with a style classifier to identify style primitives. The joint use of style and semantic classifiers facilitates the disentanglement of the latent space, thereby enhancing the generalization performance of the semantic classifier. To ensure discriminativeness in both closed and open spaces, we optimize the semantic feature space using novel metric losses. The experimental results on six benchmark datasets convincingly demonstrate that \\textsc{ODG-Net} surpasses the state-of-the-art by an impressive margin of $1-4\\%$ in both open and closed-set DG scenarios",
    "checked": true,
    "id": "af5dea08a858c96c2311e5de6a41e8fbf2ad0dcc",
    "semantic_title": "beyond boundaries: a novel data-augmentation discourse for open domain generalization",
    "citation_count": 5,
    "authors": [
      "Shirsha Bose",
      "Ankit Jha",
      "Hitesh Kandala",
      "Biplab Banerjee"
    ]
  },
  "https://openreview.net/forum?id=T55dLSgsEf": {
    "title": "Accelerating Batch Active Learning Using Continual Learning Techniques",
    "volume": "main",
    "abstract": "A major problem with Active Learning (AL) is high training costs since models are typically retrained from scratch after every query round. We start by demonstrating that standard AL on neural networks with warm starting fails, both to accelerate training and to avoid catastrophic forgetting when using fine-tuning over AL query rounds. We then develop a new class of techniques, circumventing this problem, by biasing further training towards previously labeled sets. We accomplish this by employing existing, and developing novel, replay-based Continual Learning (CL) algorithms that are effective at quickly learning the new without forgetting the old, especially when data comes from an evolving distribution. We call this paradigm \\textit{\"Continual Active Learning\" (CAL)}. We show CAL achieves significant speedups using a plethora of replay schemes that use model distillation and that select diverse/uncertain points from the history. We conduct experiments across many data domains, including natural language, vision, medical imaging, and computational biology, each with different neural architectures and dataset sizes. CAL consistently provides a $\\sim$3x reduction in training time, while retaining performance and out-of-distribution robustness, showing its wide applicability",
    "checked": true,
    "id": "ef120748f38e47adfa7f4f95dc161f414389f498",
    "semantic_title": "accelerating batch active learning using continual learning techniques",
    "citation_count": 10,
    "authors": [
      "Arnav Mohanty Das",
      "Gantavya Bhatt",
      "Megh Manoj Bhalerao",
      "Vianne R. Gao",
      "Rui Yang",
      "Jeff Bilmes"
    ]
  },
  "https://openreview.net/forum?id=lXBEwFfxpA": {
    "title": "Revisiting Topic-Guided Language Models",
    "volume": "main",
    "abstract": "A recent line of work in natural language processing has aimed to combine language models and topic models. These \\textit{topic-guided language models} augment neural language models with topic models, unsupervised learning methods that can discover document-level patterns of word use. This paper compares the effectiveness of these methods in a standardized setting. We study four topic-guided language models and two baselines, evaluating the held-out predictive performance of each model on four corpora. Surprisingly, we find that \\textit{none of these methods outperform a standard LSTM language model baseline}, and most fail to learn good topics. Further, we train a probe of the neural language model that shows that the baseline's hidden states already encode topic information. We make public all code used for this study",
    "checked": true,
    "id": "2ccb7e10e1f5bebe7e203a2e99a23c4998287e4d",
    "semantic_title": "revisiting topic-guided language models",
    "citation_count": 1,
    "authors": [
      "Carolina Zheng",
      "Keyon Vafa",
      "David Blei"
    ]
  },
  "https://openreview.net/forum?id=LfQ6uAVAEo": {
    "title": "Two-Level Actor-Critic Using Multiple Teachers",
    "volume": "main",
    "abstract": "Deep reinforcement learning has successfully allowed agents to learn complex behaviors for many tasks. However, a key limitation of current learning approaches is the sample-inefficiency problem, which limits performance of the learning agent. This paper considers how agents can benefit from improved learning via teachers' advice. In particular, we consider the setting with multiple sub-optimal teachers, as opposed to having a single near-optimal teacher. We propose a flexible two-level actor-critic algorithm where the high-level network learns to choose the best teacher in the current situation while the low-level network learns the control policy",
    "checked": true,
    "id": "ce7be018cc30bd2bb195e7f97849ef004fdc33e9",
    "semantic_title": "two-level actor-critic using multiple teachers",
    "citation_count": 0,
    "authors": [
      "Su Zhang",
      "Srijita Das",
      "Sriram Ganapathi Subramanian",
      "Matthew E. Taylor"
    ]
  },
  "https://openreview.net/forum?id=UxmvCwuTMG": {
    "title": "ECG Representation Learning with Multi-Modal EHR Data",
    "volume": "main",
    "abstract": "Electronic Health Records (EHRs) provide a rich source of medical information across different modalities such as electrocardiograms (ECG), structured EHRs (sEHR), and unstructured EHRs (text). Inspired by the fact that many cardiac and non-cardiac diseases influence the behavior of the ECG, we leverage structured EHRs and unstructured EHRs from multiple sources by pairing with ECGs and propose a set of three new multi-modal contrastive learning models that combine ECG, sEHR, and text modalities. The performance of these models is compared against different baseline models such as supervised learning models trained from scratch with random weights initialization, and self-supervised learning models trained only on ECGs. We pre-train the models on a large proprietary dataset of about 9 $million$ ECGs from around 2.4 $million$ patients and evaluate the pre-trained models on various downstream tasks such as classification, zero-shot retrieval, and out-of-distribution detection involving the prediction of various heart conditions using ECG waveforms as input, and demonstrate that the models presented in this work show significant improvements compared to all baseline modes",
    "checked": true,
    "id": "15a7b048c5ad3f3b37844426a94fb66c3074e568",
    "semantic_title": "ecg representation learning with multi-modal ehr data",
    "citation_count": 10,
    "authors": [
      "Sravan Kumar Lalam",
      "Hari Krishna Kunderu",
      "Shayan Ghosh",
      "Harish Kumar A",
      "Samir Awasthi",
      "Ashim Prasad",
      "Francisco Lopez-Jimenez",
      "Zachi I Attia",
      "Samuel Asirvatham",
      "Paul Friedman",
      "Rakesh Barve",
      "Melwin Babu"
    ]
  },
  "https://openreview.net/forum?id=V9tQKYYNK1": {
    "title": "Variational Causal Dynamics: Discovering Modular World Models from Interventions",
    "volume": "main",
    "abstract": "Latent world models allow agents to reason about complex environments with high-dimensional observations. However, adapting to new environments and effectively leveraging previous knowledge remain significant challenges. We present Variational Causal Dynamics (VCD), a structured world model that exploits the invariance of causal mechanisms across environments to achieve fast and modular adaptation. By causally factorising a transition model, VCD is able to identify reusable components across different environments. This is achieved by combining causal discovery and variational inference to learn a latent representation and transition model jointly in an unsupervised manner. Specifically, we optimise the evidence lower bound jointly over a representation model and a transition model structured as a causal graphical model. In evaluations on simulated environments with state and image observations, we show that VCD is able to successfully identify causal variables, and to discover consistent causal structures across different environments. Moreover, given a small number of observations in a previously unseen, intervened environment, VCD is able to identify the sparse changes in the dynamics and to adapt efficiently. In doing so, VCD significantly extends the capabilities of the current state-of-the-art in latent world models while also comparing favourably in terms of prediction accuracy",
    "checked": true,
    "id": "73f959402b39929dbad93610436ca078262c0fbf",
    "semantic_title": "variational causal dynamics: discovering modular world models from interventions",
    "citation_count": 9,
    "authors": [
      "Anson Lei",
      "Bernhard SchÃ¶lkopf",
      "Ingmar Posner"
    ]
  },
  "https://openreview.net/forum?id=F74ZZk5hPa": {
    "title": "RCT Rejection Sampling for Causal Estimation Evaluation",
    "volume": "main",
    "abstract": "Confounding is a significant obstacle to unbiased estimation of causal effects from observational data. For settings with high-dimensional covariates---such as text data, genomics, or the behavioral social sciences---researchers have proposed methods to adjust for confounding by adapting machine learning methods to the goal of causal estimation. However, empirical evaluation of these adjustment methods has been challenging and limited. In this work, we build on a promising empirical evaluation strategy that simplifies evaluation design and uses real data: subsampling randomized controlled trials (RCTs) to create confounded observational datasets while using the average causal effects from the RCTs as ground-truth. We contribute a new sampling algorithm, which we call RCT rejection sampling, and provide theoretical guarantees that causal identification holds in the observational data to allow for valid comparisons to the ground-truth RCT. Using synthetic data, we show our algorithm indeed results in low bias when oracle estimators are evaluated on the confounded samples, which is not always the case for a previously proposed algorithm. In addition to this identification result, we highlight several finite data considerations for evaluation designers who plan to use RCT rejection sampling on their own datasets. As a proof of concept, we implement an example evaluation pipeline and walk through these finite data considerations with a novel, real-world RCT---which we release publicly---consisting of approximately 70k observations and text data as high-dimensional covariates. Together, these contributions build towards a broader agenda of improved empirical evaluation for causal estimation",
    "checked": true,
    "id": "f2410b3da60ba080031d50b6e2217003173c0eb0",
    "semantic_title": "rct rejection sampling for causal estimation evaluation",
    "citation_count": 7,
    "authors": [
      "Katherine A. Keith",
      "Sergey Feldman",
      "David Jurgens",
      "Jonathan Bragg",
      "Rohit Bhattacharya"
    ]
  },
  "https://openreview.net/forum?id=qM7JPBYROr": {
    "title": "Tight conditions for when the NTK approximation is valid",
    "volume": "main",
    "abstract": "We study when the neural tangent kernel (NTK) approximation is valid for training a model with the square loss. In the lazy training setting of Chizat et al. 2019, we show that rescaling the model by a factor of $\\alpha = O(T)$ suffices for the NTK approximation to be valid until training time $T$. Our bound is tight and improves on the previous bound of Chizat et al. 2019, which required a larger rescaling factor of $\\alpha = O(T^2)$",
    "checked": true,
    "id": "3a9185c60a36d48fc301188d32df361d9f0d865f",
    "semantic_title": "tight conditions for when the ntk approximation is valid",
    "citation_count": 0,
    "authors": [
      "Enric Boix-AdserÃ ",
      "Etai Littwin"
    ]
  },
  "https://openreview.net/forum?id=ORMlg4g3mG": {
    "title": "Data-Free Diversity-Based Ensemble Selection for One-Shot Federated Learning",
    "volume": "main",
    "abstract": "The emerging availability of various machine learning models creates a great demand to harness the collective intelligence of many independently well-trained models to improve overall performance. Considering the privacy concern and non-negligible communication costs, one-shot federated learning and ensemble learning in a data-free manner attract significant attention. However, conventional ensemble selection approaches are neither training efficient nor applicable to federated learning due to the risk of privacy leakage from local clients; meanwhile, the \"many could be better than all\" principle under data-free constraints makes it even more challenging. Therefore, it becomes crucial to design an effective ensemble selection strategy to find a good subset of the base models as the ensemble team for the federated learning scenario. In this paper, we propose a novel data-free diversity-based framework, DeDES, to address the ensemble selection problem with diversity consideration for models under the one-shot federated learning setting. Experimental results show that our method can achieve both better performance and higher efficiency over 5 datasets, 4 different model structures, and both homogeneous and heterogeneous model groups under four different data-partition strategies",
    "checked": true,
    "id": "23234175cbdb8664bfbca42a28e37bf39ccff4ec",
    "semantic_title": "data-free diversity-based ensemble selection for one-shot federated learning",
    "citation_count": 5,
    "authors": [
      "Naibo Wang",
      "Wenjie Feng",
      "yuchen deng",
      "Moming Duan",
      "Fusheng Liu",
      "See-Kiong Ng"
    ]
  },
  "https://openreview.net/forum?id=wzRE5kTnl3": {
    "title": "Universal Graph Continual Learning",
    "volume": "main",
    "abstract": "We address catastrophic forgetting issues in graph learning as the arrival of new data from diverse task distributions often leads graph models to prioritize the current task, causing them to forget valuable insights from previous tasks. Whereas prior studies primarily tackle one setting of graph continual learning such as incremental node classification, we focus on a universal approach wherein each data point in a task can be a node or a graph, and the task varies from node to graph classification. We refer to this setting as Universal Graph Continual Learning (UGCL), which includes node-unit node classification (NUNC), graph-unit node classification (GUNC), and graph-unit graph classification (GUGC). Our novel method maintains a replay memory of nodes and neighbours to remind the model of past graph structures through distillation. Emphasizing the importance of preserving distinctive graph structures across tasks, we enforce that coarse-to-grain graph representations stay close to previous ones by minimizing our proposed global and local structure losses. We benchmark our method against various continual learning baselines in 8 real-world graph datasets and achieve significant improvement in average performance and forgetting across tasks",
    "checked": true,
    "id": "a38346d888a632b9fca2dd0c2947d5620b7b4117",
    "semantic_title": "universal graph continual learning",
    "citation_count": 4,
    "authors": [
      "Thanh Duc Hoang",
      "Do Viet Tung",
      "Duy-Hung Nguyen",
      "Bao-Sinh Nguyen",
      "Huy Hoang Nguyen",
      "Hung Le"
    ]
  },
  "https://openreview.net/forum?id=gY04GX8R5k": {
    "title": "Cross-client Label Propagation for Transductive and Semi-Supervised Federated Learning",
    "volume": "main",
    "abstract": "We present Cross-Client Label Propagation (XCLP), a new method for transductive and semi-supervised federated learning. XCLP estimates a data graph jointly from the data of multiple clients and computes labels for the unlabeled data by propagating label information across the graph. To avoid clients having to share their data with anyone, XCLP employs two cryptographically secure protocols: secure Hamming distance computation and secure summation. We demonstrate two distinct applications of XCLP within federated learning. In the first, we use it in a one-shot way to predict labels for unseen test points. In the second, we use it to repeatedly pseudo-label unlabeled training data in a federated semi-supervised setting. Experiments on both real federated and standard benchmark datasets show that in both applications XCLP achieves higher classification accuracy than alternative approaches",
    "checked": true,
    "id": "cbf044732c521a8ee3b2fcf3abc8d98d587f49ec",
    "semantic_title": "cross-client label propagation for transductive and semi-supervised federated learning",
    "citation_count": 0,
    "authors": [
      "Jonathan Scott",
      "Michelle Yeo",
      "Christoph H Lampert"
    ]
  },
  "https://openreview.net/forum?id=H5VRvCXCzf": {
    "title": "MERMAIDE: Learning to Align Learners using Model-Based Meta-Learning",
    "volume": "main",
    "abstract": "We study how a principal can efficiently and effectively intervene on the rewards of a previously unseen learning agent in order to induce desirable outcomes. This is relevant to many real-world settings like auctions or taxation, where the principal may not know the learning behavior nor the rewards of real people. Moreover, the principal should be few-shot adaptable and minimize the number of interventions, because interventions are often costly. We introduce MERMAIDE, a model-based meta-learning framework to train a principal that can quickly adapt to out-of-distribution agents with different learning strategies and reward functions. We validate this approach step-by-step. First, in a Stackelberg setting with a best-response agent, we show that meta-learning enables quick convergence to the theoretically known Stackelberg equilibrium at test time, although noisy observations severely increase the sample complexity. We then show that our model-based meta-learning approach is cost-effective in intervening on bandit agents with unseen explore-exploit strategies. Finally, we outperform baselines that use either meta-learning or agent behavior modeling, in both $0$-shot and $1$-shot settings with partial agent information",
    "checked": true,
    "id": "2226ab21e8059a2e154655e0c8d19779f0fc30fb",
    "semantic_title": "mermaide: learning to align learners using model-based meta-learning",
    "citation_count": 1,
    "authors": [
      "Arundhati Banerjee",
      "Soham Rajesh Phade",
      "Stefano Ermon",
      "Stephan Zheng"
    ]
  },
  "https://openreview.net/forum?id=8tnrh56P5W": {
    "title": "Meta Continual Learning on Graphs with Experience Replay",
    "volume": "main",
    "abstract": "Continual learning is a machine learning approach where the challenge is that a constructed learning model executes incoming tasks while maintaining its performance over the earlier tasks. In order to address this issue, we devise a technique that combines two uniquely important concepts in machine learning, namely \"replay buffer\" and \"meta learning\", aiming to exploit the best of two worlds. In this method, the model weights are initially computed by using the current task dataset. Next, the dataset of the current task is merged with the stored samples from the earlier tasks and the model weights are updated using the combined dataset. This aids in preventing the model weights converging to the optimal parameters of the current task and enables the preservation of information from earlier tasks. We choose to adapt our technique to graph data structure and the task of node classification on graphs. We introduce MetaCLGraph, which outperforms the baseline methods over various graph datasets including Citeseer, Corafull, Arxiv, and Reddit. This method illustrates the potential of combining replay buffer and meta learning in the field of continual learning on graphs",
    "checked": true,
    "id": "65894534b1fb06a4bfa80d409d29048572ca7a73",
    "semantic_title": "meta continual learning on graphs with experience replay",
    "citation_count": 3,
    "authors": [
      "Altay Unal",
      "Abdullah AkgÃ¼l",
      "Melih Kandemir",
      "Gozde Unal"
    ]
  },
  "https://openreview.net/forum?id=0ck7hJ8EVC": {
    "title": "Improved identification accuracy in equation learning via comprehensive $\\boldsymbol{R^2}$-elimination and Bayesian model selection",
    "volume": "main",
    "abstract": "In the field of equation learning, exhaustively considering all possible combinations derived from a basis function dictionary is infeasible. Sparse regression and greedy algorithms have emerged as popular approaches to tackle this challenge. However, the presence of strong collinearities poses difficulties for sparse regression techniques, and greedy steps may inadvertently exclude important components of the true equation, leading to reduced identification accuracy. In this article, we present a novel algorithm that strikes a balance between comprehensiveness and efficiency in equation learning. Inspired by stepwise regression, our approach combines the coefficient of determination, $R^2$, and the Bayesian model evidence, $p(y|\\mathcal{M})$, in a novel way. Through three extensive numerical experiments involving random polynomials and dynamical systems, we compare our method against two standard approaches, four state-of-the-art methods, and bidirectional stepwise regression incorporating $p(y|\\mathcal{M})$. The results demonstrate that our less greedy algorithm surpasses all other methods in terms of identification accuracy. Furthermore, we discover a heuristic approach to mitigate the overfitting penalty associated with $R^2$ and propose an equation learning procedure solely based on $R^2$, which achieves high rates of exact equation recovery",
    "checked": false,
    "id": "0684e51641821819f73f47679d4d67a0725186f4",
    "semantic_title": "improved identification accuracy in equation learning via comprehensive r2-elimination and bayesian model selection",
    "citation_count": 0,
    "authors": [
      "Daniel Nickelsen",
      "Bubacarr Bah"
    ]
  },
  "https://openreview.net/forum?id=dN9YICB6hN": {
    "title": "Reliable Active Learning via Influence Functions",
    "volume": "main",
    "abstract": "Due to the high cost and time-consuming nature of collecting labeled data, having insufficient labeled data is a common challenge that can negatively impact the performance of deep learning models when applied to real-world applications. Active learning (AL) aims to reduce the cost and time required for obtaining labeled data by selecting valuable samples during model training. However, recent works have pointed out the performance unreliability of existing AL algorithms for deep learning (DL) architectures under different scenarios, which manifests as their performance being comparable (or worse) to that of basic random selection. This behavior compromises the applicability of these approaches. We address this problem by proposing a theoretically motivated AL framework for DL architectures. We demonstrate that the most valuable samples for the model are those that, unsurprisingly, improve its performance on the entire dataset, most of which is unlabeled, and present a framework to efficiently estimate such performance (or loss) via influence functions, pseudo labels and diversity selection. Experimental results show that the proposed reliable active learning via influence functions (RALIF) can consistently outperform the random selection baseline as well as other existing and state-of-the art active learning approaches",
    "checked": true,
    "id": "55736f19cdd0a6bd0946041f806335995190893c",
    "semantic_title": "reliable active learning via influence functions",
    "citation_count": 0,
    "authors": [
      "Meng Xia",
      "Ricardo Henao"
    ]
  },
  "https://openreview.net/forum?id=dZugyhbNFY": {
    "title": "Personalized Federated Learning with Communication Compression",
    "volume": "main",
    "abstract": "In contrast to training traditional machine learning~(ML) models in data centers, federated learning~(FL) trains ML models over local datasets contained on resource-constrained heterogeneous edge devices. Existing FL algorithms aim to learn a single global model for all participating devices, which may not be helpful to all devices participating in the training due to the heterogeneity of the data across the devices. Recently, Hanzely and Richt\\'{a}rik (2020) proposed a new formulation for training personalized FL models aimed at balancing the trade-off between the traditional global model and the local models that could be trained by individual devices using their private data only. They derived a new algorithm, called {\\em loopless gradient descent}~(L2GD), to solve it and showed that this algorithms leads to improved communication complexity guarantees in regimes when more personalization is required. In this paper, we equip their L2GD algorithm with a {\\em bidirectional} compression mechanism to further reduce the communication bottleneck between the local devices and the server. Unlike other compression-based algorithms used in the FL-setting, our compressed L2GD algorithm operates on a probabilistic communication protocol, where communication does not happen on a fixed schedule. Moreover, our compressed L2GD algorithm maintains a similar convergence rate as vanilla SGD without compression. To empirically validate the efficiency of our algorithm, we perform diverse numerical experiments on both convex and non-convex problems and using various compression techniques",
    "checked": true,
    "id": "9849705c3afaa6a147cc0c4d95d97130132e3199",
    "semantic_title": "personalized federated learning with communication compression",
    "citation_count": 10,
    "authors": [
      "El houcine Bergou",
      "Konstantin Pavlovich Burlachenko",
      "Aritra Dutta",
      "Peter RichtÃ¡rik"
    ]
  },
  "https://openreview.net/forum?id=LT4DXqUJTD": {
    "title": "Uncovering Unique Concept Vectors through Latent Space Decomposition",
    "volume": "main",
    "abstract": "Interpreting the inner workings of deep learning models is crucial for establishing trust and ensuring model safety. Concept-based explanations have emerged as a superior approach that is more interpretable than feature attribution estimates such as pixel saliency. However, defining the concepts for the interpretability analysis biases the explanations by the user's expectations on the concepts. To address this, we propose a novel post-hoc unsupervised method that automatically uncovers the concepts learned by deep models during training. By decomposing the latent space of a layer in singular vectors and refining them by unsupervised clustering, we uncover concept vectors aligned with directions of high variance that are relevant to the model prediction, and that point to semantically distinct concepts. Our extensive experiments reveal that the majority of our concepts are readily understandable to humans, exhibit coherency, and bear relevance to the task at hand. Moreover, we showcase the practical utility of our method in dataset exploration, where our concept vectors successfully identify outlier training samples affected by various confounding factors. This novel exploration technique has remarkable versatility to data types and model architectures and it will facilitate the identification of biases and the discovery of sources of error within training data",
    "checked": true,
    "id": "a0f7ebe4c978c8c9e42e4e5004b968d58a9e901e",
    "semantic_title": "uncovering unique concept vectors through latent space decomposition",
    "citation_count": 5,
    "authors": [
      "Mara Graziani",
      "Laura O'Mahony",
      "An-phi Nguyen",
      "Henning MÃ¼ller",
      "Vincent Andrearczyk"
    ]
  },
  "https://openreview.net/forum?id=V7guVYzvE4": {
    "title": "SANTA: Source Anchoring Network and Target Alignment for Continual Test Time Adaptation",
    "volume": "main",
    "abstract": "Adapting a trained model to perform satisfactorily on continually changing test environments is an important and challenging task. In this work, we propose a novel framework, SANTA, which aims to satisfy the following characteristics required for online adaptation: 1) can work effectively for different (even small) batch sizes; 2) should continue to work well on the source domain; 3) should have minimal tunable hyperparameters and storage requirements. Given a pre-trained network trained on source domain data, the proposed framework modifies the affine parameters of the batch normalization layers using source anchoring based self-distillation. This ensures that the model incorporates knowledge from the newly encountered domains, without catastrophically forgetting the previously seen domains. We also propose a source-prototype driven contrastive alignment to ensure natural grouping of the target samples, while maintaining the already learnt semantic information. Extensive evaluation on three benchmark datasets under challenging settings justify the effectiveness of SANTA for real-world applications. Code here: https://github.com/goirik-chakrabarty/SANTA",
    "checked": true,
    "id": "a08f8b9355eff9135970c378bddc3252ecf61236",
    "semantic_title": "santa: source anchoring network and target alignment for continual test time adaptation",
    "citation_count": 8,
    "authors": [
      "Goirik Chakrabarty",
      "Manogna Sreenivas",
      "Soma Biswas"
    ]
  },
  "https://openreview.net/forum?id=gvqzvUVPiQ": {
    "title": "The Analysis of the Expected Change in the Classification Probability of the Predicted Label",
    "volume": "main",
    "abstract": "We present a formalism for estimating the expected change in the probability distribution of the predicted label of an object, with respect to all small perturbations to the object. We first derive analytically an estimate of the expected probability change as a function of the input noise. We then conduct three empirical studies: in the first study, experimental results on image classification show that the proposed measure can be used to distinguish the not-robust label predictions from those that are robust, even when they are all predicted with high confidence. The second study shows that the proposed robustness measure is almost always higher for the predictions on the corrupted images, compared to the predictions on the original versions of them. The final study shows that the proposed measure is lower for models when they are trained using adversarial training approaches",
    "checked": true,
    "id": "17f3ca0e7489faf6b3f53cfa327c7bca2d936e30",
    "semantic_title": "the analysis of the expected change in the classification probability of the predicted label",
    "citation_count": 0,
    "authors": [
      "Ruo Yang",
      "Ping Liu",
      "Mustafa Bilgic"
    ]
  },
  "https://openreview.net/forum?id=NE2xXWo0LF": {
    "title": "Latent State Models of Training Dynamics",
    "volume": "main",
    "abstract": "The impact of randomness on model training is poorly understood. How do differences in data order and initialization actually manifest in the model, such that some training runs outperform others or converge faster? Furthermore, how can we interpret the resulting training dynamics and the phase transitions that characterize different trajectories? To understand the effect of randomness on the dynamics and outcomes of neural network training, we train models multiple times with different random seeds and compute a variety of metrics throughout training, such as the $L_2$ norm, mean, and variance of the neural network's weights. We then fit a hidden Markov model (HMM) over the resulting sequences of metrics. The HMM represents training as a stochastic process of transitions between latent states, providing an intuitive overview of significant changes during training. Using our method, we produce a low-dimensional, discrete representation of training dynamics on grokking tasks, image classification, and masked language modeling. We use the HMM representation to study phase transitions and identify latent \"detour\" states that slow down convergence",
    "checked": true,
    "id": "01bc697a530bf5b15ec0f20b6419947d66af4d83",
    "semantic_title": "latent state models of training dynamics",
    "citation_count": 8,
    "authors": [
      "Michael Y. Hu",
      "Angelica Chen",
      "Naomi Saphra",
      "Kyunghyun Cho"
    ]
  },
  "https://openreview.net/forum?id=o8VgRNYh6n": {
    "title": "Differentially Private Optimizers Can Learn Adversarially Robust Models",
    "volume": "main",
    "abstract": "Machine learning models have shone in a variety of domains and attracted increasing attention from both the security and the privacy communities. One important yet worrying question is: Will training models under the differential privacy (DP) constraint have an unfavorable impact on their adversarial robustness? While previous works have postulated that privacy comes at the cost of worse robustness, we give the first theoretical analysis to show that DP models can indeed be robust and accurate, even sometimes more robust than their naturally-trained non-private counterparts. We observe three key factors that influence the privacy-robustness-accuracy tradeoff: (1) hyper-parameters for DP optimizers are critical; (2) pre-training on public data significantly mitigates the accuracy and robustness drop; (3) choice of DP optimizers makes a difference. With these factors set properly, we achieve 90\\% natural accuracy, 72\\% robust accuracy ($+9\\%$ than the non-private model) under $l_2(0.5)$ attack, and 69\\% robust accuracy ($+16\\%$ than the non-private model) with pre-trained SimCLRv2 model under $l_\\infty(4/255)$ attack on CIFAR10 with $\\epsilon=2$. In fact, we show both theoretically and empirically that DP models are Pareto optimal on the accuracy-robustness tradeoff. Empirically, the robustness of DP models is consistently observed across various datasets and models. We believe our encouraging results are a significant step towards training models that are private as well as robust",
    "checked": true,
    "id": "19c8fee47f4a88fae01042f907d8030bf73f9bb0",
    "semantic_title": "differentially private optimizers can learn adversarially robust models",
    "citation_count": 3,
    "authors": [
      "Zhiqi Bu",
      "Yuan Zhang"
    ]
  },
  "https://openreview.net/forum?id=oyfRWeoUJY": {
    "title": "Addressing caveats of neural persistence with deep graph persistence",
    "volume": "main",
    "abstract": "Neural Persistence is a prominent measure for quantifying neural network complexity, proposed in the emerging field of topological data analysis in deep learning. In this work, however, we find both theoretically and empirically that the variance of network weights and spatial concentration of large weights are the main factors that impact neural persistence. Whilst this captures useful information for linear classifiers, we find that no relevant spatial structure is present in later layers of deep neural networks, making neural persistence roughly equivalent to the variance of weights. Additionally, the proposed averaging procedure across layers for deep neural networks does not consider interaction between layers. Based on our analysis, we propose an extension of the filtration underlying neural persistence to the whole neural network instead of single layers, which is equivalent to calculating neural persistence on one particular matrix. This yields our deep graph persistence measure, which implicitly incorporates persistent paths through the network and alleviates variance-related issues through standardisation. Code is available at https://github.com/ExplainableML/Deep-Graph-Persistence",
    "checked": true,
    "id": "926070acfff1ba430944f0773dd41a3d56e15cb3",
    "semantic_title": "addressing caveats of neural persistence with deep graph persistence",
    "citation_count": 1,
    "authors": [
      "Leander Girrbach",
      "Anders Christensen",
      "Ole Winther",
      "Zeynep Akata",
      "A. Sophia Koepke"
    ]
  },
  "https://openreview.net/forum?id=91hfMEUukm": {
    "title": "Replay-enhanced Continual Reinforcement Learning",
    "volume": "main",
    "abstract": "Replaying past experiences has proven to be a highly effective approach for averting catastrophic forgetting in supervised continual learning. However, some crucial factors are still largely ignored, making it vulnerable to serious failure, when used as a solution to forgetting in continual reinforcement learning, even in the context of perfect memory where all data of previous tasks are accessible in the current task. On the one hand, since most reinforcement learning algorithms are not invariant to the reward scale, the previously well-learned tasks (with high rewards) may appear to be more salient to the current learning process than the current task (with small initial rewards). This causes the agent to concentrate on those salient tasks at the expense of generality on the current task. On the other hand, offline learning on replayed tasks while learning a new task may induce a distributional shift between the dataset and the learned policy on old tasks, resulting in forgetting. In this paper, we introduce RECALL, a replay-enhanced method that greatly improves the plasticity of existing replay-based methods on new tasks while effectively avoiding the recurrence of catastrophic forgetting in continual reinforcement learning. RECALL leverages adaptive normalization on approximate targets and policy distillation on old tasks to enhance generality and stability, respectively. Extensive experiments on the Continual World benchmark show that RECALL performs significantly better than purely perfect memory replay, and achieves comparable or better overall performance against state-of-the-art continual learning methods",
    "checked": true,
    "id": "f5bbd862df67bc681957e8c1bc64159c5ac38020",
    "semantic_title": "replay-enhanced continual reinforcement learning",
    "citation_count": 7,
    "authors": [
      "Tiantian Zhang",
      "Kevin Zehua Shen",
      "Zichuan Lin",
      "Bo Yuan",
      "Xueqian Wang",
      "Xiu Li",
      "Deheng Ye"
    ]
  },
  "https://openreview.net/forum?id=JllRdycmLk": {
    "title": "The (Un)Scalability of Informed Heuristic Function Estimation in NP-Hard Search Problems",
    "volume": "main",
    "abstract": "The A* algorithm is commonly used to solve NP-hard combinatorial optimization problems. When provided with a completely informed heuristic function, A* can solve such problems in time complexity that is polynomial in the solution cost and branching factor. In light of this fact, we examine a line of recent publications that propose fitting deep neural networks to the completely informed heuristic function. We assert that these works suffer from inherent scalability limitations since --- under the assumption of NP $\\not \\subseteq$ P/poly --- such approaches result in either (a) network sizes that scale super-polynomially in the instance sizes or (b) the accuracy of the fitted deep neural networks scales inversely with the instance sizes. Complementing our theoretical claims, we provide experimental results for three representative NP-hard search problems. The results suggest that fitting deep neural networks to informed heuristic functions requires network sizes that grow quickly with the problem instance size. We conclude by suggesting that the research community should focus on scalable methods for integrating heuristic search with machine learning, as opposed to methods relying on informed heuristic estimation",
    "checked": true,
    "id": "394bcb1ed8b7ad8d4580ab7e78cfc40ea528d78a",
    "semantic_title": "the (un)scalability of informed heuristic function estimation in np-hard search problems",
    "citation_count": 1,
    "authors": [
      "Sumedh Pendurkar",
      "Taoan Huang",
      "Brendan Juba",
      "Jiapeng Zhang",
      "Sven Koenig",
      "Guni Sharon"
    ]
  },
  "https://openreview.net/forum?id=ndw90pkNM9": {
    "title": "A Combinatorial Semi-Bandit Approach to Charging Station Selection for Electric Vehicles",
    "volume": "main",
    "abstract": "In this work, we address the problem of long-distance navigation for battery electric vehicles (BEVs), where one or more charging sessions are required to reach the intended destination. We consider the availability and performance of the charging stations to be unknown and stochastic, and develop a combinatorial semi-bandit framework for exploring the road network to learn the parameters of the queue time and charging power distributions. Within this framework, we first outline a method for transforming the road network graph into a graph of feasible paths between charging stations to handle the constrained combinatorial optimization problem in an efficient way. Then, for the feasibility graph, we use a Bayesian approach to model the stochastic edge weights, utilizing conjugate priors for the one-parameter exponential and two-parameter gamma distributions, the latter of which is novel to multi-armed bandit literature. Finally, we apply combinatorial versions of Thompson Sampling, BayesUCB and Epsilon-greedy to the problem. We demonstrate the performance of our framework on long-distance navigation problem instances in large-scale country-sized road networks, with simulation experiments in Norway, Sweden and Finland",
    "checked": true,
    "id": "969676d0f6002d57b487d220ba400ee6d90b8e5b",
    "semantic_title": "a combinatorial semi-bandit approach to charging station selection for electric vehicles",
    "citation_count": 0,
    "authors": [
      "Niklas Ãkerblom",
      "Morteza Haghir Chehreghani"
    ]
  },
  "https://openreview.net/forum?id=4rkKN4tM63": {
    "title": "Invertible Hierarchical Generative Model for Images",
    "volume": "main",
    "abstract": "Normalizing flows (NFs) as generative models enjoy desirable properties such as exact invertibility and exact likelihood evaluation, while being efficient to sample from. These properties, however, come at the cost of heavy restrictions on the architecture. Due to these limitations, modeling multi-modal probability distributions can yield poor results even with low-dimensional data. Additionally, typical flow architectures employed on real image datasets produce samples with visible aliasing artifacts and limited variation. The latent decomposition of flow-models also falls short on that of competing methods, with uneven contribution to a decoded image. In this work we build an invertible generative model using conditional normalizing flows in a hierarchical fashion to circumvent the aforementioned limitations. We show that we can achieve superior sample quality among flow-based models with fewer parameters compared to the state of the art. We demonstrate ability to control individual levels of detail via the latent decomposition of our model",
    "checked": true,
    "id": "781fde26e007ba48b2759169fb2321a8671344f1",
    "semantic_title": "invertible hierarchical generative model for images",
    "citation_count": 0,
    "authors": [
      "Heikki Timonen",
      "Miika Aittala",
      "Jaakko Lehtinen"
    ]
  },
  "https://openreview.net/forum?id=BxdrpnRHNh": {
    "title": "Using Representation Expressiveness and Learnability to Evaluate Self-Supervised Learning Methods",
    "volume": "main",
    "abstract": "We address the problem of evaluating the quality of self-supervised learning (SSL) models without access to supervised labels, while being agnostic to the architecture, learning algorithm or data manipulation used during training. We argue that representations can be evaluated through the lens of expressiveness and learnability. We propose to use the Intrinsic Dimension (ID) to assess expressiveness and introduce Cluster Learnability (CL) to assess learnability. CL is measured in terms of the performance of a KNN classifier trained to predict labels obtained by clustering the representations with K-means. We thus combine CL and ID into a single predictor â CLID. Through a large-scale empirical study with a diverse family of SSL algorithms, we find that CLID better correlates with in-distribution model performance than other competing recent evaluation schemes. We also benchmark CLID on out-of-domain generalization, where CLID serves as a predictor of the transfer performance of SSL models on several visual classification tasks, yielding improvements with respect to the competing baselines",
    "checked": true,
    "id": "f3f3cacc5e36ab98e719b896c99ca757fee592a8",
    "semantic_title": "using representation expressiveness and learnability to evaluate self-supervised learning methods",
    "citation_count": 0,
    "authors": [
      "Yuchen Lu",
      "Zhen Liu",
      "Aristide Baratin",
      "Romain Laroche",
      "Aaron Courville",
      "Alessandro Sordoni"
    ]
  },
  "https://openreview.net/forum?id=SQnPE63jtA": {
    "title": "Learning Multiscale Non-stationary Causal Structures",
    "volume": "main",
    "abstract": "This paper addresses a gap in the current state of the art by providing a solution for modeling causal relationships that evolve over time and occur at different time scales. Specifically, we introduce the multiscale non-stationary directed acyclic graph (MN-DAG), a framework for modeling multivariate time series data. Our contribution is twofold. Firstly, we expose a probabilistic generative model by leveraging results from spectral and causality theories. Our model allows sampling an MN-DAG according to user-specified priors on the time-dependence and multiscale properties of the causal graph. Secondly, we devise a Bayesian method named Multiscale Non-stationary Causal Structure Learner (MN-CASTLE) that uses stochastic variational inference to estimate MN-DAGs. The method also exploits information from the local partial correlation between time series over different time resolutions. The data generated from an MN-DAG reproduces well-known features of time series in different domains, such as volatility clustering and serial correlation. Additionally, we show the superior performance of MN-CASTLE on synthetic data with different multiscale and non-stationary properties compared to baseline models. Finally, we apply MN-CASTLE to identify the drivers of the natural gas prices in the US market. Causal relationships have strengthened during the COVID-19 outbreak and the Russian invasion of Ukraine, a fact that baseline methods fail to capture. MN-CASTLE identifies the causal impact of critical economic drivers on natural gas prices, such as seasonal factors, economic uncertainty, oil prices, and gas storage deviations",
    "checked": true,
    "id": "7706ef7e213cbb099dfeb1861dae82971de50780",
    "semantic_title": "learning multiscale non-stationary causal structures",
    "citation_count": 4,
    "authors": [
      "Gabriele D'Acunto",
      "Gianmarco De Francisci Morales",
      "Paolo Bajardi",
      "Francesco Bonchi"
    ]
  },
  "https://openreview.net/forum?id=r06xREo3QG": {
    "title": "Bag of Image Patch Embedding Behind the Success of Self-Supervised Learning",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL) has recently achieved tremendous empirical advancements in learning image representation. However, our understanding of the principle behind learning such a representation is still limited. This work shows that joint-embedding SSL approaches learn a representation of image patches, which reflects their co-occurrence. Such a connection to co-occurrence modeling can be established formally, and it supplements the prevailing invariance perspective. We empirically show that learning a representation for fixed-scale patches and aggregating local patch representations as the image representation achieves similar or even better results than the baseline methods. We denote this process as {\\it BagSSL}. Even with $32\\times 32$ patch representation, BagSSL achieves $62\\%$ top-1 linear probing accuracy on ImageNet. On the other hand, with a multi-scale pretrained model, we show that the whole image embedding is approximately the average of local patch embeddings. While the SSL representation is relatively invariant at the global scale, we show that locality is preserved when we zoom into local patch-level representation. Further, we show that patch representation aggregation can improve various SOTA baseline methods by a large margin. The patch representation is considerably easier to understand, and this work makes a step to demystify self-supervised representation learning",
    "checked": true,
    "id": "0bbb60fd0fe2e0ffefacb16fc2c527cb2f01a71e",
    "semantic_title": "bag of image patch embedding behind the success of self-supervised learning",
    "citation_count": 7,
    "authors": [
      "Yubei Chen",
      "Adrien Bardes",
      "ZENGYI LI",
      "Yann LeCun"
    ]
  },
  "https://openreview.net/forum?id=8HQCOMRa7g": {
    "title": "One-Round Active Learning through Data Utility Learning and Proxy Models",
    "volume": "main",
    "abstract": "While active learning (AL) techniques have demonstrated the potential to produce high-performance models with fewer labeled data, their application remains limited due to the necessity for multiple rounds of interaction with annotators. This paper studies the problem of one-round AL, which aims at selecting a subset of unlabeled points and querying their labels \\emph{all at once}. A fundamental challenge is how to measure the utility of different choices of labeling queries for learning a target model. Our key idea is to learn such a utility metric from a small initial labeled set. We demonstrate that our approach leads to state-of-the-art performance on various AL benchmarks and is more robust to the lack of initial labeled data. In addition to algorithmic development and evaluation, we introduce a novel metric for quantifying `\\emph{utility transferability}' -- the degree of correlation between the performance changes of two learning algorithms due to variations in training data selection. Previous studies have often observed a notable utility transferability between models, even those with differing complexities. Such transferability enabled our approach, as well as other techniques such as coresets, hyperparameter tuning, and data valuation, to scale up to more sophisticated target models by substituting them with smaller proxy models. Nevertheless, utility transferability has not yet been rigorously defined within a formal mathematical framework, a gap that our work addresses innovatively. We further propose two Monte Carlo-based methods for efficiently comparing utility transferability for different proxy models, thereby facilitating a more informed selection of proxy models",
    "checked": true,
    "id": "89ffeb90dcdf7823844fbc0dd3a8bb3a60d48b0f",
    "semantic_title": "one-round active learning through data utility learning and proxy models",
    "citation_count": 1,
    "authors": [
      "Jiachen T. Wang",
      "Si Chen",
      "Ruoxi Jia"
    ]
  },
  "https://openreview.net/forum?id=J3veZdVpts": {
    "title": "Bridging the Gap Between Offline and Online Reinforcement Learning Evaluation Methodologies",
    "volume": "main",
    "abstract": "Reinforcement learning (RL) has shown great promise with algorithms learning in environments with large state and action spaces purely from scalar reward signals. A crucial challenge for current deep RL algorithms is that they require a tremendous amount of environment interactions for learning. This can be infeasible in situations where such interactions are expensive, such as in robotics. Offline RL algorithms try to address this issue by bootstrapping the learning process from existing logged data without needing to interact with the environment from the very beginning. While online RL algorithms are typically evaluated as a function of the number of environment interactions, there isn't a single established protocol for evaluating offline RL methods. In this paper, we propose a sequential approach to evaluate offline RL algorithms as a function of the training set size and thus by their data efficiency. Sequential evaluation provides valuable insights into the data efficiency of the learning process and the robustness of algorithms to distribution changes in the dataset while also harmonizing the visualization of the offline and online learning phases. Our approach is generally applicable and easy to implement. We compare several existing offline RL algorithms using this approach and present insights from a variety of tasks and offline datasets",
    "checked": true,
    "id": "0f1947fe9ae67eccd2c09057ab00553ebf8c318b",
    "semantic_title": "bridging the gap between offline and online reinforcement learning evaluation methodologies",
    "citation_count": 1,
    "authors": [
      "Shivakanth Sujit",
      "Pedro Braga",
      "Jorg Bornschein",
      "Samira Ebrahimi Kahou"
    ]
  },
  "https://openreview.net/forum?id=hjYmsV6nXZ": {
    "title": "RLTF: Reinforcement Learning from Unit Test Feedback",
    "volume": "main",
    "abstract": "The goal of program synthesis, or code generation, is to generate executable code based on given descriptions. Recently, there has been an increasing number of studies employing reinforcement learning (RL) to improve the performance of large language models (LLMs) for code. However, some of the current representative RL methods have only used offline frameworks, limiting the exploration of new sample spaces. Additionally, the utilization of unit test signals is limited, not accounting for specific error locations within the code. To address these issues, we proposed RLTF, i.e., Reinforcement Learning from Unit Test Feedback, a novel online RL framework with unit test feedback of multi-granularity for refining code LLMs. Our approach generates data in real-time during training and simultaneously utilizes fine-grained feedback signals to guide the model towards producing higher-quality code. Extensive experiments show that RLTF achieves state-of-the-art performance on the APPS and the MBPP benchmarks. Our code is available at: \\url{https://github.com/Zyq-scut/RLTF}",
    "checked": true,
    "id": "a669ea57529f4db630043c8c75d8f840c485d24d",
    "semantic_title": "rltf: reinforcement learning from unit test feedback",
    "citation_count": 62,
    "authors": [
      "Jiate Liu",
      "Yiqin Zhu",
      "Kaiwen Xiao",
      "QIANG FU",
      "Xiao Han",
      "Yang Wei",
      "Deheng Ye"
    ]
  },
  "https://openreview.net/forum?id=ZSxvyWrX6k": {
    "title": "Visualizing the Diversity of Representations Learned by Bayesian Neural Networks",
    "volume": "main",
    "abstract": "Explainable Artificial Intelligence (XAI) aims to make learning machines less opaque, and offers researchers and practitioners various tools to reveal the decision-making strategies of neural networks. In this work, we investigate how XAI methods can be used for exploring and visualizing the diversity of feature representations learned by Bayesian Neural Networks (BNNs). Our goal is to provide a global understanding of BNNs by making their decision-making strategies a) visible and tangible through feature visualizations and b) quantitatively measurable with a distance measure learned by contrastive learning. Our work provides new insights into the posterior distribution in terms of human-understandable feature information with regard to the underlying decision-making strategies. The main findings of our work are the following: 1) global XAI methods can be applied to explain the diversity of decision-making strategies of BNN instances, 2) Monte Carlo dropout with commonly used Dropout rates exhibit increased diversity in feature representations compared to the multimodal posterior approximation of MultiSWAG, 3) the diversity of learned feature representations highly correlates with the uncertainty estimate for the output and 4) the inter-mode diversity of the multimodal posterior decreases as the network width increases, while the intra-mode diversity increases. These findings are consistent with the recent Deep Neural Networks theory, providing additional intuitions about what the theory implies in terms of humanly understandable concepts",
    "checked": true,
    "id": "bd1486c78d5a16b544e6d9d014d96681fc7dd56b",
    "semantic_title": "visualizing the diversity of representations learned by bayesian neural networks",
    "citation_count": 5,
    "authors": [
      "Dennis Grinwald",
      "Kirill Bykov",
      "Shinichi Nakajima",
      "Marina MC HÃ¶hne"
    ]
  },
  "https://openreview.net/forum?id=cdRYoTyHZh": {
    "title": "Automated Detection of Causal Inference Opportunities: Regression Discontinuity Subgroup Discovery",
    "volume": "main",
    "abstract": "The gold standard for the identification of causal effects are randomized controlled trials (RCT), but RCTs may not always be feasible to conduct. When treatments depend on a threshold however, such as the blood sugar threshold for diabetes diagnosis, we can still sometimes estimate causal effects with regression discontinuities (RDs). RDs are valid when units just above and below the threshold have the same distribution of covariates and thus no confounding in the presence of noise, establishing an as-if randomization. In practice however, implementing RD studies can be difficult as identifying treatment thresholds require considerable domain expertise -- furthermore, the thresholds may differ across subgroups (e.g., the blood sugar threshold for diabetes may differ across demographics), and ignoring these differences can lower statistical power. Finding the thresholds and to whom they apply is an important problem currently solved manually by domain experts, and data-driven approaches are needed when domain expertise is not sufficient. Here, we introduce Regression Discontinuity SubGroup Discovery (RDSGD), a machine-learning method that identifies statistically powerful and interpretable subgroups for RD thresholds. Using a medical claims dataset with over 60 million patients, we apply RDSGD to multiple clinical contexts and identify subgroups with increased compliance to treatment assignment thresholds. As treatment thresholds matter for many diseases and policy decisions, RDSGD can be a powerful tool for discovering new avenues for causal estimation",
    "checked": true,
    "id": "64e1ed0941ed89fcf8a02d5a7d0a3935495c2dfb",
    "semantic_title": "automated detection of causal inference opportunities: regression discontinuity subgroup discovery",
    "citation_count": 0,
    "authors": [
      "Tony Liu",
      "Patrick Lawlor",
      "Lyle Ungar",
      "Konrad Kording",
      "Rahul Ladhania"
    ]
  },
  "https://openreview.net/forum?id=A9yn7KTwsK": {
    "title": "Invariant Structure Learning for Better Generalization and Causal Explainability",
    "volume": "main",
    "abstract": "Learning the causal structure behind data is invaluable for improving generalization and ob- taining high-quality explanations. Towards this end, we propose a novel framework, Invariant Structure Learning (ISL), that is designed to improve causal structure discovery by utilizing generalization as an indication in the process. ISL splits the data into different environments, and learns a structure that is invariant to the target across different environments by imposing a consistency constraint. The proposed aggregation mechanism then selects the classifier based on a graph structure that reflects the causal mechanisms in the data more accurately compared to the structures learnt from individual environments. Furthermore, we extend ISL to a self-supervised learning setting, where accurate causal structure discovery does not rely on any labels. Self-supervised ISL utilizes proposals for invariant causality, by iteratively setting different nodes as targets. On synthetic and real-world datasets, we demonstrate that ISL accurately discovers the causal structure, outperforms alternative methods, and yields superior generalization for datasets with significant distribution shifts",
    "checked": true,
    "id": "edc07047cd45e90aff0556ab10b16740a8110e61",
    "semantic_title": "invariant structure learning for better generalization and causal explainability",
    "citation_count": 2,
    "authors": [
      "Yunhao Ge",
      "Sercan O Arik",
      "Jinsung Yoon",
      "Ao Xu",
      "Laurent Itti",
      "Tomas Pfister"
    ]
  },
  "https://openreview.net/forum?id=iRTL4pDavo": {
    "title": "Data pruning and neural scaling laws: fundamental limitations of score-based algorithms",
    "volume": "main",
    "abstract": "Data pruning algorithms are commonly used to reduce the memory and computational cost of the optimization process. Recent empirical results (Guo, B. Zhao, and Bai, 2022) reveal that random data pruning remains a strong baseline and outperforms most existing data pruning methods in the high compression regime, i.e., where a fraction of 30% or less of the data is kept. This regime has recently attracted a lot of interest as a result of the role of data pruning in improving the so-called neural scaling laws; see (Sorscher et al., 2022), where the authors showed the need for high-quality data pruning algorithms in order to beat the sample power law. In this work, we focus on score-based data pruning algorithms and show theoretically and empirically why such algorithms fail in the high compression regime. We demonstrate \"No Free Lunch\" theorems for data pruning and discuss potential solutions to these limitations",
    "checked": true,
    "id": "bb4721b1a806ac00308bfb174edf3c36b6f0b620",
    "semantic_title": "data pruning and neural scaling laws: fundamental limitations of score-based algorithms",
    "citation_count": 10,
    "authors": [
      "Fadhel Ayed",
      "Soufiane Hayou"
    ]
  },
  "https://openreview.net/forum?id=AfXq3x3X16": {
    "title": "Offline Reinforcement Learning with Additional Covering Distributions",
    "volume": "main",
    "abstract": "We study learning optimal policies from a logged dataset, i.e., offline RL, with function general approximation. Despite the efforts devoted, existing algorithms with theoretic finite-sample guarantees typically assume exploratory data coverage or strong realizable function classes (e.g., Bellman-completeness), which is hard to be satisfied in reality. While there are recent works that successfully tackle these strong assumptions, they either require the gap assumptions that could only be satisfied by part of MDPs or use the behavior regularization that makes the optimality of learned policy even intractable. To solve this challenge, we provide finite-sample guarantees for a simple algorithm based on marginalized importance sampling (MIS), showing that sample-efficient offline RL for general MDPs is possible with only a partial coverage dataset (instead of assuming a dataset covering all possible policies) and weak realizable function classes (assuming function classes containing simply one function) given additional side information of a covering distribution. We demonstrate that the covering distribution trades off prior knowledge of the optimal trajectories against the coverage requirement of the dataset, revealing the effect of this inductive bias in the learning processes. Furthermore, when considering the exploratory dataset, our analysis shows that only realizable function classes are enough for learning near-optimal policies, even with no side information on the additional coverage distributions",
    "checked": true,
    "id": "3f1302446ae40cceea8a804efc9fc2dba1d056d0",
    "semantic_title": "offline reinforcement learning with additional covering distributions",
    "citation_count": 0,
    "authors": [
      "Chenjie Mao"
    ]
  },
  "https://openreview.net/forum?id=23WZFQBUh5": {
    "title": "Online model selection by learning how compositional kernels evolve",
    "volume": "main",
    "abstract": "Motivated by the need for efficient, personalized learning in health, we investigate the problem of online compositional kernel selection for multi-task Gaussian Process regression. Existing composition selection methods do not satisfy our strict criteria in health; selection must occur quickly, and the selected kernels must maintain the appropriate level of complexity, sparsity, and stability as data arrives online. We introduce the Kernel Evolution Model (KEM), a generative process on how to evolve kernel compositions in a way that manages the bias--variance trade-off as we observe more data about a user. Using pilot data, we learn a set of kernel evolutions that can be used to quickly select kernels for new test users. KEM reliably selects high-performing kernels for a range of synthetic and real data sets, including two health data sets",
    "checked": true,
    "id": "cf03f313f861c567d5f6076702835cbc3691f771",
    "semantic_title": "online model selection by learning how compositional kernels evolve",
    "citation_count": 3,
    "authors": [
      "Eura Shin",
      "Predrag Klasnja",
      "Susan Murphy",
      "Finale Doshi-Velez"
    ]
  },
  "https://openreview.net/forum?id=EjqopDxLbG": {
    "title": "NOFLITE: Learning to Predict Individual Treatment Effect Distributions",
    "volume": "main",
    "abstract": "Estimating the effect of a treatment on an individual's outcome of interest is an important challenge in various fields, such as healthcare, economics, marketing, and education. Previous work in machine learning has focused on estimating the expected value of the treatment effect. However, effective personalized decision-making requires more than just the treatment expected effect; it requires knowing the entire treatment effect distribution. Knowing this distribution allows analyzing the treatment's expected utility or quantifying the uncertainty regarding a treatment's effect. This information is essential for prescribing optimal treatments. The ability of a model to predict accurate individual treatment effect distributions is captured by its likelihood. In light of this, we propose a novel neural architecture, NOFLITE, that uses normalizing flows to directly optimize this likelihood, while simultaneously learning flexible estimates of the individual treatment effect distribution. Experiments on various semi-synthetic data sets show that NOFLITE outperforms existing methods in terms of loglikelihood. Moreover, we illustrate how the predicted distributions can enable an in-depth analysis of the treatment effect and more accurate decision-making",
    "checked": true,
    "id": "3e5b2797577be2bcb1d7af3ff848703727b22efe",
    "semantic_title": "noflite: learning to predict individual treatment effect distributions",
    "citation_count": 4,
    "authors": [
      "Toon Vanderschueren",
      "Jeroen Berrevoets",
      "Wouter Verbeke"
    ]
  },
  "https://openreview.net/forum?id=28bQiPWxHl": {
    "title": "Stochastic Mirror Descent: Convergence Analysis and Adaptive Variants via the Mirror Stochastic Polyak Stepsize",
    "volume": "main",
    "abstract": "We investigate the convergence of stochastic mirror descent (SMD) under interpolation in relatively smooth and smooth convex optimization. In relatively smooth convex optimization we provide new convergence guarantees for SMD with a constant stepsize. For smooth convex optimization we propose a new adaptive stepsize scheme --- the mirror stochastic Polyak stepsize (mSPS). Notably, our convergence results in both settings do not make bounded gradient assumptions or bounded variance assumptions, and we show convergence to a neighborhood that vanishes under interpolation. Consequently, these results correspond to the first convergence guarantees under interpolation for the exponentiated gradient algorithm for fixed or adaptive stepsizes. mSPS generalizes the recently proposed stochastic Polyak stepsize (SPS) (Loizou et al. 2021) to mirror descent and remains both practical and efficient for modern machine learning applications while inheriting the benefits of mirror descent. We complement our results with experiments across various supervised learning tasks and different instances of SMD, demonstrating the effectiveness of mSPS",
    "checked": true,
    "id": "f866b44965117173ced03178fe685fa8d26b2f67",
    "semantic_title": "stochastic mirror descent: convergence analysis and adaptive variants via the mirror stochastic polyak stepsize",
    "citation_count": 31,
    "authors": [
      "Ryan D'Orazio",
      "Nicolas Loizou",
      "Issam H. Laradji",
      "Ioannis Mitliagkas"
    ]
  },
  "https://openreview.net/forum?id=B0uBSSUy0G": {
    "title": "Provably Personalized and Robust Federated Learning",
    "volume": "main",
    "abstract": "Clustering clients with similar objectives and learning a model per cluster is an intuitive and interpretable approach to personalization in federated learning. However, doing so with provable and optimal guarantees has remained an open challenge. In this work, we formalize personalized federated learning as a stochastic optimization problem. We propose simple clustering-based algorithms which iteratively identify and train within clusters, using local client gradients. Our algorithms have optimal convergence rates which asymptotically match those obtained if we knew the true underlying clustering of the clients, and are provably robust in the Byzantine setting where some fraction of the clients are malicious",
    "checked": true,
    "id": "235f66ac870ee8ede8f131fb0da00472545febab",
    "semantic_title": "provably personalized and robust federated learning",
    "citation_count": 11,
    "authors": [
      "Mariel Werner",
      "Lie He",
      "Michael Jordan",
      "Martin Jaggi",
      "Sai Praneeth Karimireddy"
    ]
  },
  "https://openreview.net/forum?id=I5sJ6PU6JN": {
    "title": "Conditional Sampling of Variational Autoencoders via Iterated Approximate Ancestral Sampling",
    "volume": "main",
    "abstract": "Conditional sampling of variational autoencoders (VAEs) is needed in various applications, such as missing data imputation, but is computationally intractable. A principled choice for asymptotically exact conditional sampling is Metropolis-within-Gibbs (MWG). However, we observe that the tendency of VAEs to learn a structured latent space, a commonly desired property, can cause the MWG sampler to get \"stuck\" far from the target distribution. This paper mitigates the limitations of MWG: we systematically outline the pitfalls in the context of VAEs, propose two original methods that address these pitfalls, and demonstrate an improved performance of the proposed methods on a set of sampling tasks",
    "checked": true,
    "id": "eed436b929b954eab3e9aa856dde0a9428f9dbe5",
    "semantic_title": "conditional sampling of variational autoencoders via iterated approximate ancestral sampling",
    "citation_count": 3,
    "authors": [
      "Vaidotas Simkus",
      "Michael U. Gutmann"
    ]
  },
  "https://openreview.net/forum?id=dn3ZkqG2YV": {
    "title": "Rewiring with Positional Encodings for Graph Neural Networks",
    "volume": "main",
    "abstract": "Several recent works use positional encodings to extend the receptive fields of graph neural network (GNN) layers equipped with attention mechanisms. These techniques, however, extend receptive fields to the complete graph, at substantial computational cost and risking a change in the inductive biases of conventional GNNs, or require complex architecture adjustments. As a conservative alternative, we use positional encodings to expand receptive fields to r-hop neighborhoods. More specifically, our method augments the input graph with additional nodes/edges and uses positional encodings as node and/or edge features. We thus modify graphs before inputting them to a downstream GNN model, instead of modifying the model itself. This makes our method model-agnostic, i.e., compatible with any of the existing GNN architectures. We also provide examples of positional encodings that are lossless with a one-to-one map between the original and the modified graphs. We demonstrate that extending receptive fields via positional encodings and a virtual fully- connected node significantly improves GNN performance and alleviates over-squashing using small r. We obtain improvements on a variety of models and datasets and reach competitive performance using traditional GNNs or graph Transformers",
    "checked": true,
    "id": "002390988f7157b425ac7e5dc42f3d06eca6ede7",
    "semantic_title": "rewiring with positional encodings for graph neural networks",
    "citation_count": 33,
    "authors": [
      "Rickard BrÃ¼el Gabrielsson",
      "Mikhail Yurochkin",
      "Justin Solomon"
    ]
  },
  "https://openreview.net/forum?id=leqr0vQzeN": {
    "title": "A Robust Backpropagation-Free Framework for Images",
    "volume": "main",
    "abstract": "While current deep learning algorithms have been successful for a wide variety of artificial intelligence (AI) tasks, including those involving structured image data, they present deep neurophysiological conceptual issues due to their reliance on the gradients that are computed by backpropagation of errors (backprop). Gradients are required to obtain synaptic weight adjustments but require knowledge of feed forward activities in order to conduct backward propagation, a biologically implausible process. This is known as the \"weight transport problem''. Therefore, in this work, we present a more biologically plausible approach towards solving the weight transport problem for image data. This approach, which we name the error-kernel driven activation alignment (EKDAA) algorithm, accomplishes through the introduction of locally derived error transmission kernels and error maps. Like standard deep learning networks, EKDAA performs the standard forward process via weights and activation functions; however, its backward error computation involves adaptive error kernels that propagate local error signals through the network. The efficacy of EKDAA is demonstrated by performing visual-recognition tasks on the Fashion MNIST, CIFAR-10 and SVHN benchmarks, along with demonstrating its ability to extract visual features from natural color images. Furthermore, in order to demonstrate its non-reliance on gradient computations, results are presented for an EKDAA-trained CNN that employs a non-differentiable activation function",
    "checked": true,
    "id": "f191c831918b1616f734da071d54de2ddd745b89",
    "semantic_title": "a robust backpropagation-free framework for images",
    "citation_count": 1,
    "authors": [
      "Timothy Zee",
      "Alex Ororbia",
      "Ankur Mali",
      "Ifeoma Nwogu"
    ]
  },
  "https://openreview.net/forum?id=65AzNvY73Q": {
    "title": "Minorization-Maximization for Learning Determinantal Point Processes",
    "volume": "main",
    "abstract": "A determinantal point process (DPP) is a powerful probabilistic model that generates diverse random subsets from a ground set. Since a DPP is characterized by a positive definite kernel, a DPP on a finite ground set can be parameterized by a kernel matrix. Recently, DPPs have gained attention in the machine learning community and have been applied to various practical problems; however, there is still room for further research on the learning of DPPs. In this paper, we propose a simple learning rule for full-rank DPPs based on a minorization-maximization (MM) algorithm, which monotonically increases the likelihood in each iteration. We show that our minorizer of the MM algorithm provides a tighter lower-bound compared to an existing method locally. We also generalize the algorithm for further acceleration. In our experiments on both synthetic and real-world datasets, our method outperforms existing methods in most settings. Our code is available at https://github.com/ISMHinoLab/DPPMMEstimation",
    "checked": true,
    "id": "05272f41369a00407a5547140cb3a45ee53cc025",
    "semantic_title": "minorization-maximization for learning determinantal point processes",
    "citation_count": 2,
    "authors": [
      "Takahiro Kawashima",
      "Hideitsu Hino"
    ]
  },
  "https://openreview.net/forum?id=gKEbBKRUjA": {
    "title": "Understanding Curriculum Learning in Policy Optimization for Online Combinatorial Optimization",
    "volume": "main",
    "abstract": "Over the recent years, reinforcement learning (RL) starts to show promising results in tackling combinatorial optimization (CO) problems, in particular when coupled with curriculum learning to facilitate training. Despite emerging empirical evidence, theoretical study on why RL helps is still at its early stage. This paper presents the first systematic study on policy optimization methods for online CO problems. We show that online CO problems can be naturally formulated as latent Markov Decision Processes (LMDPs), and prove convergence bounds on natural policy gradient (NPG) for solving LMDPs. Furthermore, our theory explains the benefit of curriculum learning: it can find a strong sampling policy and reduce the distribution shift, a critical quantity that governs the convergence rate in our theorem. For a canonical online CO problem, the Best Choice Problem (BCP), we formally prove that distribution shift is reduced exponentially with curriculum learning even if the curriculum is a randomly generated BCP on a smaller scale. Our theory also shows we can simplify the curriculum learning scheme used in prior work from multi-step to single-step. Lastly, we provide extensive experiments on the Best Choice Problem, Online Knapsack, and AdWords to verify our findings",
    "checked": true,
    "id": "396e224a94202c253a6201d670724041f9cb33a3",
    "semantic_title": "understanding curriculum learning in policy optimization for online combinatorial optimization",
    "citation_count": 3,
    "authors": [
      "Runlong Zhou",
      "Zelin He",
      "Yuandong Tian",
      "Yi Wu",
      "Simon Shaolei Du"
    ]
  },
  "https://openreview.net/forum?id=BxjHMPwZIH": {
    "title": "Training DNNs Resilient to Adversarial and Random Bit-Flips by Learning Quantization Ranges",
    "volume": "main",
    "abstract": "Promoting robustness in deep neural networks (DNNs) is crucial for their reliable deployment in uncertain environments, such as low-power settings or in the presence of adversarial attacks. In particular, bit-flip weight perturbations in quantized networks can significantly degrade performance, underscoring the need to improve DNN resilience. In this paper, we introduce a training mechanism to learn the quantization range of different DNN layers to enhance DNN robustness against bit-flip errors on the model parameters. The proposed approach, called weight clipping-aware training (WCAT), minimizes the quantization range while preserving performance, striking a balance between the two. Our experimental results on different models and datasets showcase that DNNs trained with WCAT can tolerate a high amount of noise while keeping the accuracy close to the baseline model. Moreover, we show that our method significantly enhances DNN robustness against adversarial bit-flip attacks. Finally, when considering the energy-reliability trade-off inherent in on-chip SRAM memories, we observe that WCAT consistently improves the Pareto frontier of test accuracy and energy consumption across diverse models",
    "checked": true,
    "id": "17b6068b5552a5d8c15d3cb8f6cd6452aee29c29",
    "semantic_title": "training dnns resilient to adversarial and random bit-flips by learning quantization ranges",
    "citation_count": 3,
    "authors": [
      "Kamran Chitsaz",
      "Goncalo Mordido",
      "Jean-Pierre David",
      "FranÃ§ois Leduc-Primeau"
    ]
  },
  "https://openreview.net/forum?id=j4y3gN7VtW": {
    "title": "Feature-Attending Recurrent Modules for Generalization in Reinforcement Learning",
    "volume": "main",
    "abstract": "Many important tasks are defined in terms of object. To generalize across these tasks, a reinforcement learning (RL) agent needs to exploit the structure that the objects induce. Prior work has either hard-coded object-centric features, used complex object-centric generative models, or updated state using local spatial features. However, these approaches have had limited success in enabling general RL agents. Motivated by this, we introduce \"Feature- Attending Recurrent Modules\" (FARM), an architecture for learning state representations that relies on simple, broadly applicable inductive biases for capturing spatial and temporal regularities. FARM learns a state representation that is distributed across multiple modules that each attend to spatiotemporal features with an expressive feature attention mechanism. We show that this improves an RL agent's ability to generalize across object-centric tasks. We study task suites in both 2D and 3D environments and find that FARM better generalizes compared to competing architectures that leverage attention or multiple modules",
    "checked": true,
    "id": "7365d4b97e38ca7fcd8ac3db194854b59def1d42",
    "semantic_title": "feature-attending recurrent modules for generalization in reinforcement learning",
    "citation_count": 7,
    "authors": [
      "Wilka Torrico Carvalho",
      "Andrew Kyle Lampinen",
      "Kyriacos Nikiforou",
      "Felix Hill",
      "Murray Shanahan"
    ]
  },
  "https://openreview.net/forum?id=5Y04GWvoJu": {
    "title": "Achieving Risk Control in Online Learning Settings",
    "volume": "main",
    "abstract": "To provide rigorous uncertainty quantification for online learning models, we develop a framework for constructing uncertainty sets that provably control risk---such as coverage of confidence intervals, false negative rate, or F1 score---in the online setting. This extends conformal prediction to apply to a larger class of online learning problems. Our method guarantees risk control at any user-specified level even when the underlying data distribution shifts drastically, even adversarially, over time in an unknown fashion. The technique we propose is highly flexible as it can be applied with any base online learning algorithm (e.g., a deep neural network trained online), requiring minimal implementation effort and essentially zero additional computational cost. We further extend our approach to control multiple risks simultaneously, so the prediction sets we generate are valid for all given risks. To demonstrate the utility of our method, we conduct experiments on real-world tabular time-series data sets showing that the proposed method rigorously controls various natural risks. Furthermore, we show how to construct valid intervals for an online image-depth estimation problem that previous sequential calibration schemes cannot handle",
    "checked": true,
    "id": "222e7926d07384f104016a7e2264978fedfcdeda",
    "semantic_title": "achieving risk control in online learning settings",
    "citation_count": 30,
    "authors": [
      "Shai Feldman",
      "Liran Ringel",
      "Stephen Bates",
      "Yaniv Romano"
    ]
  },
  "https://openreview.net/forum?id=1kl4YM2Q7P": {
    "title": "Exploring Transformer Backbones for Heterogeneous Treatment Effect Estimation",
    "volume": "main",
    "abstract": "Previous works on Treatment Effect Estimation (TEE) are not in widespread use because they are predominantly theoretical, where strong parametric assumptions are made but untractable for practical application. Recent works use Multilayer Perceptron (MLP) for modeling casual relationships, however, MLPs lag far behind recent advances in ML methodology, which limits their applicability and generalizability. To extend beyond the single domain formulation and towards more realistic learning scenarios, we explore model design spaces beyond MLPs, i.e., transformer backbones, which provide flexibility where attention layers govern interactions among treatments and covariates to exploit structural similarities of potential outcomes for confounding control. Through careful model design, Transformers as Treatment Effect Estimators (TransTEE) is proposed. We show empirically that TransTEE can: (1) serve as a general-purpose treatment effect estimator which significantly outperforms competitive baselines on a variety of challenging TEE problems (e.g., discrete, continuous, structured, or dosage-associated treatments.) and is applicable to both when covariates are tabular and when they consist of structural data (e.g., texts, graphs); (2) yield multiple advantages: compatibility with propensity score modeling, parameter efficiency, robustness to continuous treatment value distribution shifts, explainable in covariate adjustment, and real-world utility in auditing pre-trained language models",
    "checked": true,
    "id": "0109c662c101723aea99e561937e3aca58563537",
    "semantic_title": "exploring transformer backbones for heterogeneous treatment effect estimation",
    "citation_count": 30,
    "authors": [
      "YiFan Zhang",
      "Hanlin Zhang",
      "Zachary Chase Lipton",
      "Li Erran Li",
      "Eric Xing"
    ]
  },
  "https://openreview.net/forum?id=ok18jj7cam": {
    "title": "GraphPNAS: Learning Probabilistic Graph Generators for Neural Architecture Search",
    "volume": "main",
    "abstract": "Neural architectures can be naturally viewed as computational graphs. Motivated by this perspective, we, in this paper, study neural architecture search (NAS) through the lens of learning graph generative models. In contrast to existing NAS methods which largely focus on searching for a single best architecture, i.e, point estimation, we propose GraphPNAS a deep graph generative model that learns a distribution of well-performing architectures. Relying on graph neural networks (GNNs), our GraphPNAS can better capture topologies of good neural architectures and relations between operators therein. Moreover, our graph generator leads to a learnable probabilistic search method that is more flexible and efficient than the commonly used RNN generator and random search methods. Finally, we learn our generator via an efficient reinforcement learning formulation for NAS. To assess the effectiveness of our GraphPNAS, we conduct extensive experiments on four search spaces, including the challenging RandWire on TinyImageNet, ENAS on CIFAR10, and NAS-Bench-101/201. We show that our proposed graph generator consistently outperforms RNN-based one and achieves better or comparable performances than state-of-the-art NAS methods",
    "checked": true,
    "id": "33291bc4488b2a087bf50d8c1d58af3e7c535884",
    "semantic_title": "graphpnas: learning probabilistic graph generators for neural architecture search",
    "citation_count": 2,
    "authors": [
      "Muchen Li",
      "Jeffrey Yunfan Liu",
      "Leonid Sigal",
      "Renjie Liao"
    ]
  },
  "https://openreview.net/forum?id=jLJTqJXAG7": {
    "title": "Federated Learning under Partially Disjoint Data via Manifold Reshaping",
    "volume": "main",
    "abstract": "Statistical heterogeneity severely limits the performance of federated learning (FL), motivating several explorations e.g., FedProx, MOON and FedDyn, to alleviate this problem. Despite effectiveness, their considered scenario generally requires samples from almost all classes during the local training of each client, although some covariate shifts may exist among clients. In fact, the natural case of partially class-disjoint data (PCDD), where each client contributes a few classes (instead of all classes) of samples, is practical yet underexplored. Specifically, the unique collapse and invasion characteristics of PCDD can induce the biased optimization direction in local training, which prevents the efficiency of federated learning. To address this dilemma, we propose a manifold reshaping approach called FedMR to calibrate the feature space of local training. Our FedMR adds two interplaying losses to the vanilla federated learning: one is the intra-class loss to decorrelate feature dimensions for anti-collapse; and the other one is the inter-class loss to guarantee the proper margin among categories in the feature expansion. We conduct extensive experiments on a range of datasets to demonstrate that our FedMR achieves much higher accuracy and better communication efficiency",
    "checked": true,
    "id": "521bcc5bc7da0a52976551c1a2b8630d01e660b5",
    "semantic_title": "federated learning under partially disjoint data via manifold reshaping",
    "citation_count": 5,
    "authors": [
      "Ziqing Fan",
      "Jiangchao Yao",
      "Ruipeng Zhang",
      "Lingjuan Lyu",
      "Yanfeng Wang",
      "Ya Zhang"
    ]
  },
  "https://openreview.net/forum?id=DlRsoxjyPm": {
    "title": "Synthetic Data from Diffusion Models Improves ImageNet Classification",
    "volume": "main",
    "abstract": "Deep generative models are becoming increasingly powerful, now generating diverse, high fidelity, photo-realistic samples given text prompts. Nevertheless, samples from such models have not been shown to significantly improve model training for challenging and well-studied discriminative tasks like ImageNet classification. In this paper we show that augmenting the ImageNet training set with samples from a generative diffusion model can yield substantial improvements in ImageNet classification accuracy over strong ResNet and Vision Transformer baselines. To this end we explore the fine-tuning of large-scale text-to-image diffusion models, yielding class-conditional ImageNet models with state-of-the-art FID score (1.76 at 256Ã256 resolution) and Inception Score (239 at 256Ã256). The model also yields a new state-of-the-art in Classification Accuracy Scores, i.e., ImageNet test accuracy for a ResNet-50 architecture trained solely on synthetic data (64.96 top-1 accuracy for 256Ã256 samples, improving to 69.24 for 1024Ã1024 samples). Adding up to three times as many synthetic samples as real training samples consistently improves ImageNet classification accuracy across multiple architectures",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shekoofeh Azizi",
      "Simon Kornblith",
      "Chitwan Saharia",
      "Mohammad Norouzi",
      "David J. Fleet"
    ]
  },
  "https://openreview.net/forum?id=f3JLnnZsAm": {
    "title": "ILPO-MP: Mode Priors Prevent Mode Collapse when Imitating Latent Policies from Observations",
    "volume": "main",
    "abstract": "Imitation learning from observations (IfO) constrains the classic imitation learning setting to cases where expert observations are easy to obtain, but no expert actions are available. Most existing IfO methods require access to task-specific cost functions or many interactions with the target environment. Learning a forward dynamics model in combination with a latent policy has been shown to solve these issues. However, the limited supervision in the IfO scenario can lead to mode collapse when learning the generative forward dynamics model and the corresponding latent policy. In this paper, we analyze the mode collapse problem in this setting and show that it is caused by a combination of deterministic expert data and bad initialization of the models. Under the assumption of piecewise continuous system dynamics, we propose ILPO-MP, a method to prevent the mode collapse using clustering of expert transitions to impose a mode prior on the generative model and the latent policy. We show that ILPO-MP prevents mode collapse and improves performance in a variety of environments",
    "checked": true,
    "id": "dc1a8e8f9624c4004ee4968b071db6f5aabd0760",
    "semantic_title": "ilpo-mp: mode priors prevent mode collapse when imitating latent policies from observations",
    "citation_count": 2,
    "authors": [
      "Oliver Struckmeier",
      "Ville Kyrki"
    ]
  },
  "https://openreview.net/forum?id=g1B4qgOw79": {
    "title": "Complementary Sparsity: Accelerating Sparse CNNs with High Accuracy on General-Purpose Computing Platforms",
    "volume": "main",
    "abstract": "Model sparsity is a promising approach to reducing parameters or FLOPs of convolutional neural networks (CNNs). Compared to unstructured or coarse-grained structured sparsity, fine-grained structured sparsity, e.g., N:M sparse pattern, can achieve a better balance between accuracy and efficiency on general computing platforms like CPUs and GPUs. In particular, the 2:4 sparsity can accelerate CNN inference by 2$\\times$ speed and with negligible accuracy drop. However, N:M sparsity needs to be supported by GPU within specific hardware circuits and hardly achieves significant speedups on common GPUs. To accelerate CNNs with general-purposed computing resources and simultaneously retain the model accuracy as much as possible, this paper proposes complementary sparsity (CS). CS denotes that only one weight can be retained for weights spaced at the same distance. On the one hand, CS features high mask flexibility, which is naturally favorable to high model accuracy. Moreover, we propose a CS-specific sparse training method to improve CS-based CNNs' accuracy under high parameter sparsities ($>$75\\%). On the other hand, CS itself is memory-access balanced and robust to pattern hyperparameters, which can be utilized to speedup CS-based convolution computation on CPUs and common GPUs. We thus propose a CS convolution parallel computing algorithm that adapts to common GPUs without sparse tensor cores. Experimental results show that compared to other sparsity patterns, the proposed CS can achieve the optimal trade-off in terms of accuracy and latency for CPUs and common GPUs, respectively. Codes will be available at https://gitee.com/mindspore/models/tree/master/research/cv/CS",
    "checked": true,
    "id": "e6592ed83ee30ba3bb651604b8cbc457834cf1c4",
    "semantic_title": "complementary sparsity: accelerating sparse cnns with high accuracy on general-purpose computing platforms",
    "citation_count": 0,
    "authors": [
      "Kang Zhao",
      "Yijun Tan",
      "Kai Han",
      "Ting Hu",
      "Hanting Chen",
      "Tao Yuan",
      "Yunhe Wang",
      "Jun Yao"
    ]
  },
  "https://openreview.net/forum?id=JYs1R9IMJr": {
    "title": "Finding Neurons in a Haystack: Case Studies with Sparse Probing",
    "volume": "main",
    "abstract": "Despite rapid adoption and deployment of large language models (LLMs), the internal computations of these models remain opaque and poorly understood. In this work, we seek to understand how high-level human-interpretable features are represented within the internal neuron activations of LLMs. We train $k$-sparse linear classifiers (probes) on these internal activations to predict the presence of features in the input; by varying the value of $k$ we study the sparsity of learned representations and how this varies with model scale. With $k=1$, we localize individual neurons that are highly relevant for a particular feature and perform a number of case studies to illustrate general properties of LLMs. In particular, we show that early layers make use of sparse combinations of neurons to represent many features in superposition, that middle layers have seemingly dedicated neurons to represent higher-level contextual features, and that increasing scale causes representational sparsity to increase on average, but there are multiple types of scaling dynamics. In all, we probe for over 100 unique features comprising 10 different categories in 7 different models spanning 70 million to 6.9 billion parameters",
    "checked": true,
    "id": "12910786da7a34c9ee26798fd81b0ed7b0e38789",
    "semantic_title": "finding neurons in a haystack: case studies with sparse probing",
    "citation_count": 218,
    "authors": [
      "Wes Gurnee",
      "Neel Nanda",
      "Matthew Pauly",
      "Katherine Harvey",
      "Dmitrii Troitskii",
      "Dimitris Bertsimas"
    ]
  },
  "https://openreview.net/forum?id=m8U9rSs6gU": {
    "title": "Inducing Meaningful Units from Character Sequences with Dynamic Capacity Slot Attention",
    "volume": "main",
    "abstract": "Characters do not convey meaning, but sequences of characters do. We propose an unsupervised distributional method to learn the abstract meaning-bearing units in a sequence of characters. Rather than segmenting the sequence, our Dynamic Capacity Slot Attention model discovers continuous representations of the objects in the sequence, extending an architecture for object discovery in images. We train our model on different languages and evaluate the quality of the obtained representations with forward and reverse probing classifiers. These experiments show that our model succeeds in discovering units which are similar to those proposed previously in form, content, and level of abstraction, and which show promise for capturing meaningful information at a higher level of abstraction",
    "checked": true,
    "id": "2dce73e4a3e19d71249fc7a53c2a9531daaff839",
    "semantic_title": "inducing meaningful units from character sequences with dynamic capacity slot attention",
    "citation_count": 1,
    "authors": [
      "Melika Behjati",
      "James Henderson"
    ]
  },
  "https://openreview.net/forum?id=YfZ4ZPt8zd": {
    "title": "Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks",
    "volume": "main",
    "abstract": "Recently, there has been significant progress in teaching language models to perform step-by-step reasoning to solve complex numerical reasoning tasks. Chain-of-thoughts prompting (CoT) is the state-of-art method for many of these tasks. CoT uses language models to produce text describing reasoning, and computation, and finally the answer to a question. Here we propose `Program of Thoughts' (PoT), which uses language models (mainly Codex) to generate text and programming language statements, and finally an answer. In PoT, the computation can be delegated to a program interpreter, which is used to execute the generated program, thus decoupling complex computation from reasoning and language understanding. We evaluate PoT on five math word problem datasets and three financial-QA datasets in both few-shot and zero-shot settings. We find that PoT has an average performance gain over CoT of around 12% across all datasets. By combining PoT with self-consistency decoding, we can achieve extremely strong performance on all the math datasets and financial datasets. All of our data and code will be released",
    "checked": true,
    "id": "6c943670dca38bfc7c8b477ae7c2d1fba1ad3691",
    "semantic_title": "program of thoughts prompting: disentangling computation from reasoning for numerical reasoning tasks",
    "citation_count": 829,
    "authors": [
      "Wenhu Chen",
      "Xueguang Ma",
      "Xinyi Wang",
      "William W. Cohen"
    ]
  },
  "https://openreview.net/forum?id=GEcneTl9Mk": {
    "title": "DP-LFlow: Differentially Private Latent Flow for Scalable Sensitive Image Generation",
    "volume": "main",
    "abstract": "Privacy concerns grow with the success of modern deep learning models, especially when the training set contains sensitive data. Differentially private generative model (DPGM) can serve as a solution to circumvent such concerns by generating data that are distributionally similar to the original data yet with differential privacy (DP) guarantees. While GAN has attracted major attention, existing DPGMs based on flow generative models are limited and only developed on low-dimensional tabular datasets. The capability of exact density estimation makes the flow model exceptional when density estimation is of interest. In this work, we will first show that it is challenging (or even infeasible) to train a DP-flow via DP-SGD, i.e. the workhorse algorithm for private deep learning, on high-dimensional image sets with acceptable utility, and then we give an effective solution by reducing the generation from the pixel space to a lower dimensional latent space. We show the effectiveness and scalability of the proposed method via extensive experiments, where the proposed method achieves a significantly better privacy-utility trade-off compared to existing alternatives. Notably, our method is the first DPGM to scale to high-resolution image sets (up to 256 Ã 256). Our code is available at https://github.com/dihjiang/DP-LFlow",
    "checked": true,
    "id": "ff14a9be477cae258579fc11d0255b99fef81f70",
    "semantic_title": "dp-lflow: differentially private latent flow for scalable sensitive image generation",
    "citation_count": 1,
    "authors": [
      "Dihong Jiang",
      "Sun Sun"
    ]
  },
  "https://openreview.net/forum?id=uKCGOw9bGG": {
    "title": "Binary Classification under Local Label Differential Privacy Using Randomized Response Mechanisms",
    "volume": "main",
    "abstract": "Label differential privacy is a popular branch of $\\epsilon$-differential privacy for protecting labels in training datasets with non-private features. In this paper, we study the generalization performance of a binary classifier trained on a dataset privatized under the label differential privacy achieved by the randomized response mechanism. Particularly, we establish minimax lower bounds for the excess risks of the deep neural network plug-in classifier, theoretically quantifying how privacy guarantee $\\epsilon$ affects its generalization performance. Our theoretical result shows: (1) the randomized response mechanism slows down the convergence of excess risk by lessening the multiplicative constant term compared with the non-private case $(\\epsilon=\\infty)$; (2) as $\\epsilon$ decreases, the optimal structure of the neural network should be smaller for better generalization performance; (3) the convergence of its excess risk is guaranteed even if $\\epsilon$ is adaptive to the size of training sample $n$ at a rate slower than $O(n^{-1/2})$. Our theoretical results are validated by extensive simulated examples and two real applications",
    "checked": true,
    "id": "7d7e82810d60101189b1fbc2c0b2804b07bb667b",
    "semantic_title": "binary classification under local label differential privacy using randomized response mechanisms",
    "citation_count": 7,
    "authors": [
      "Shirong Xu",
      "Chendi Wang",
      "Will Wei Sun",
      "Guang Cheng"
    ]
  },
  "https://openreview.net/forum?id=Q4aAITDgdP": {
    "title": "Learn the Time to Learn: Replay Scheduling in Continual Learning",
    "volume": "main",
    "abstract": "Replay methods are known to be successful at mitigating catastrophic forgetting in continual learning scenarios despite having limited access to historical data. However, storing historical data is cheap in many real-world settings, yet replaying all historical data is often prohibited due to processing time constraints. In such settings, we propose that continual learning systems should learn the time to learn and schedule which tasks to replay at different time steps. We first demonstrate the benefits of our proposal by using Monte Carlo tree search to find a proper replay schedule, and show that the found replay schedules can outperform fixed scheduling policies when combined with various replay methods in different continual learning settings. Additionally, we propose a framework for learning replay scheduling policies with reinforcement learning. We show that the learned policies can generalize better in new continual learning scenarios compared to equally replaying all seen tasks, without added computational cost. Our study reveals the importance of learning the time to learn in continual learning, which brings current research closer to real-world needs",
    "checked": true,
    "id": "a1953404403f99a3d9c4ee33efecd2eb8f47a0c6",
    "semantic_title": "learn the time to learn: replay scheduling in continual learning",
    "citation_count": 9,
    "authors": [
      "Marcus Klasson",
      "Hedvig Kjellstrom",
      "Cheng Zhang"
    ]
  },
  "https://openreview.net/forum?id=vkiKzK5G3e": {
    "title": "Neighborhood Gradient Mean: An Efficient Decentralized Learning Method for Non-IID Data",
    "volume": "main",
    "abstract": "Decentralized learning algorithms enable the training of deep learning models over large distributed datasets, without the need for a central server. The current state-of-the-art decentralized algorithms mostly assume the data distributions to be Independent and Identically Distributed (IID). In practical scenarios, the distributed datasets can have significantly different data distributions across the agents. This paper focuses on improving decentralized learning on non-IID data with minimal compute and memory overheads. We propose Neighborhood Gradient Mean (NGM), a novel decentralized learning algorithm that modifies the local gradients of each agent using self- and cross-gradient information. In particular, the proposed method averages the local gradients with model-variant or data-variant cross-gradients based on the communication budget. Model-variant cross-gradients are derivatives of the received neighbors' model parameters with respect to the local dataset. Data-variant cross-gradient derivatives of the local model with respect to its neighbors' datasets. The data-variant cross-gradients are aggregated through an additional communication round. We theoretically analyze the convergence characteristics of NGM and demonstrate its efficiency on non-IID data sampled from various vision and language datasets. Our experiments demonstrate that the proposed method either remains competitive or outperforms (by 0-6%) the existing state-of-the-art (SoTA) decentralized learning algorithm on non-IID data with significantly less compute and memory requirements. Further, we show that the model-variant cross-gradient information available locally at each agent can improve the performance on non-IID data by 3-20% without additional communication costs",
    "checked": true,
    "id": "3b8e534931f42ad8d98fc31e00574ccf0be879e1",
    "semantic_title": "neighborhood gradient mean: an efficient decentralized learning method for non-iid data",
    "citation_count": 6,
    "authors": [
      "Sai Aparna Aketi",
      "Sangamesh Kodge",
      "Kaushik Roy"
    ]
  },
  "https://openreview.net/forum?id=140kSqm0uy": {
    "title": "Limitation of Characterizing Implicit Regularization by Data-independent Functions",
    "volume": "main",
    "abstract": "In recent years, understanding the implicit regularization of neural networks (NNs) has become a central task in deep learning theory. However, implicit regularization is itself not completely defined and well understood. In this work, we attempt to mathematically define and study implicit regularization. Importantly, we explore the limitations of a common approach to characterizing implicit regularization using data-independent functions. We propose two dynamical mechanisms, i.e., Two-point and One-point Overlapping mechanisms, based on which we provide two recipes for producing classes of one-hidden-neuron NNs that provably cannot be fully characterized by a type of or all data-independent functions. Following the previous works, our results further emphasize the profound data dependency of implicit regularization in general, inspiring us to study in detail the data dependency of NN implicit regularization in the future",
    "checked": true,
    "id": "e27217173cf86a6786ad5aaf3e48f3eeeee0fe77",
    "semantic_title": "limitation of characterizing implicit regularization by data-independent functions",
    "citation_count": 0,
    "authors": [
      "Leyang Zhang",
      "Zhi-Qin John Xu",
      "Tao Luo",
      "Yaoyu Zhang"
    ]
  },
  "https://openreview.net/forum?id=gQnJ7ODIAx": {
    "title": "Population-based Evaluation in Repeated Rock-Paper-Scissors as a Benchmark for Multiagent Reinforcement Learning",
    "volume": "main",
    "abstract": "Progress in fields of machine learning and adversarial planning has benefited significantly from benchmark domains, from checkers and the classic UCI data sets to Go and Diplomacy. In sequential decision-making, agent evaluation has largely been restricted to few interactions against experts, with the aim to reach some desired level of performance (e.g. beating a human professional player). We propose a benchmark for multiagent learning based on repeated play of the simple game Rock, Paper, Scissors along with a population of forty-three tournament entries, some of which are intentionally sub-optimal. We describe metrics to measure the quality of agents based both on average returns and exploitability. We then show that several RL, online learning, and language model approaches can learn good counter-strategies and generalize well, but ultimately lose to the top-performing bots, creating an opportunity for research in multiagent learning",
    "checked": true,
    "id": "524cdc83813740f4f7f4149896c2feabdf85ff18",
    "semantic_title": "population-based evaluation in repeated rock-paper-scissors as a benchmark for multiagent reinforcement learning",
    "citation_count": 5,
    "authors": [
      "Marc Lanctot",
      "John Schultz",
      "Neil Burch",
      "Max Olan Smith",
      "Daniel Hennes",
      "Thomas Anthony",
      "Julien Perolat"
    ]
  },
  "https://openreview.net/forum?id=aqqfB3p9ZA": {
    "title": "Convergence of SGD for Training Neural Networks with Sliced Wasserstein Losses",
    "volume": "main",
    "abstract": "Optimal Transport has sparked vivid interest in recent years, in particular thanks to the Wasserstein distance, which provides a geometrically sensible and intuitive way of comparing probability measures. For computational reasons, the Sliced Wasserstein (SW) distance was introduced as an alternative to the Wasserstein distance, and has seen uses for training generative Neural Networks (NNs). While convergence of Stochastic Gradient Descent (SGD) has been observed practically in such a setting, there is to our knowledge no theoretical guarantee for this observation. Leveraging recent works on convergence of SGD on non-smooth and non-convex functions by Bianchi et al. (2022), we aim to bridge that knowledge gap, and provide a realistic context under which fixed-step SGD trajectories for the SW loss on NN parameters converge. More precisely, we show that the trajectories approach the set of (sub)-gradient flow equations as the step decreases. Under stricter assumptions, we show a much stronger convergence result for noised and projected SGD schemes, namely that the long-run limits of the trajectories approach a set of generalised critical points of the loss function",
    "checked": true,
    "id": "13908d2c6f60e07a3e5eb7c85e23508ae46e81d1",
    "semantic_title": "convergence of sgd for training neural networks with sliced wasserstein losses",
    "citation_count": 6,
    "authors": [
      "Eloi Tanguy"
    ]
  },
  "https://openreview.net/forum?id=ySWQ6eXAKp": {
    "title": "Not All Causal Inference is the Same",
    "volume": "main",
    "abstract": "Neurally-parameterized Structural Causal Models in the Pearlian notion to causality, referred to as NCM, were recently introduced as a step towards next-generation learning systems. However, said NCM are only concerned with the learning aspect of causal inference and totally miss out on the architecture aspect. That is, actual causal inference within NCM is intractable in that the NCM won't return an answer to a query in polynomial time. This insight follows as corollary to the more general statement on the intractability of arbitrary structural causal model (SCM) parameterizations, which we prove in this work through classical 3-SAT reduction. Since future learning algorithms will be required to deal with both high dimensional data and highly complex mechanisms governing the data, we ultimately believe work on tractable inference for causality to be decisive. We also show that not all \"causal\" models are created equal. More specifically, there are models capable of answering causal queries that are not SCM, which we refer to as partially causal models (PCM). We provide a tabular taxonomy in terms of tractability properties for all of the different model families, namely correlation-based, PCM and SCM. To conclude our work, we also provide some initial ideas on how to overcome parts of the intractability of causal inference with SCM by showing an example of how parameterizing an SCM with SPN modules can at least allow for tractable mechanisms. With this work we hope that our insights can raise awareness for this novel research direction since achieving success with causality in real world downstream tasks will not only depend on learning correct models but also require having the practical ability to gain access to model inferences",
    "checked": true,
    "id": "33baffb40db3c605823944e7074552cff196b8d9",
    "semantic_title": "not all causal inference is the same",
    "citation_count": 1,
    "authors": [
      "Matej ZeÄeviÄ",
      "Devendra Singh Dhami",
      "Kristian Kersting"
    ]
  },
  "https://openreview.net/forum?id=tEKqQgbwbf": {
    "title": "Homomorphic Self-Supervised Learning",
    "volume": "main",
    "abstract": "Many state of the art self-supervised learning approaches fundamentally rely on transformations applied to the input in order to selectively extract task-relevant information. Recently, the field of equivariant deep learning has developed to introduce structure into the feature space of deep neural networks by designing them as homomorphisms with respect to input transformations. In this work, we observe that many existing self-supervised learning algorithms can be both unified and generalized when seen through the lens of equivariant representations. Specifically, we introduce a general framework we call Homomorphic Self-Supervised Learning, and theoretically show how it may subsume the use of input-augmentations provided an augmentation-homomorphic feature extractor. We validate this theory experimentally for simple augmentations, demonstrate the necessity of representational structure for feature-space SSL, and further empirically explore how the parameters of this framework relate to those of traditional augmentation-based self-supervised learning. We conclude with a discussion of the potential benefits afforded by this new perspective on self-supervised learning",
    "checked": true,
    "id": "64b8bf4c805699b084476b5dd7ffa8095be04e6e",
    "semantic_title": "homomorphic self-supervised learning",
    "citation_count": 2,
    "authors": [
      "T. Anderson Keller",
      "Xavier Suau",
      "Luca Zappella"
    ]
  },
  "https://openreview.net/forum?id=cXa6Xdm0v7": {
    "title": "Multimodal Language Learning for Object Retrieval in Low Data Regimes in the Face of Missing Modalities",
    "volume": "main",
    "abstract": "Our study is motivated by robotics, where when dealing with robots or other physical systems, we often need to balance competing concerns of relying on complex, multimodal data coming from a variety of sensors with a general lack of large representative datasets. Despite the complexity of modern robotic platforms and the need for multimodal interaction, there has been little research on integrating more than two modalities in a low data regime with the real-world constraint that sensors fail due to obstructions or adverse conditions. In this work, we consider a case in which natural language is used as a retrieval query against objects, represented across multiple modalities, in a physical environment. We introduce extended multimodal alignment (EMMA), a method that learns to select the appropriate object while jointly refining modality-specific embeddings through a geometric (distance-based) loss. In contrast to prior work, our approach is able to incorporate an arbitrary number of views (modalities) of a particular piece of data. We demonstrate the efficacy of our model on a grounded language object retrieval scenario. We show that our model outperforms state-of-the-art baselines when little training data is available. Our code is available at https://github.com/kasraprime/EMMA",
    "checked": true,
    "id": "1e3d96a9568b7c75f0e79c5069e499bdd3ebbabd",
    "semantic_title": "multimodal language learning for object retrieval in low data regimes in the face of missing modalities",
    "citation_count": 1,
    "authors": [
      "Kasra Darvish",
      "Edward Raff",
      "Francis Ferraro",
      "Cynthia Matuszek"
    ]
  },
  "https://openreview.net/forum?id=czev0exHXT": {
    "title": "Worst-case Feature Risk Minimization for Data-Efficient Learning",
    "volume": "main",
    "abstract": "Deep learning models typically require massive amounts of annotated data to train a strong model for a task of interest. However, data annotation is time-consuming and costly. How to use labeled data from a related but distinct domain, or just a few samples to train a satisfactory model are thus important questions. To achieve this goal, models should resist overfitting to the specifics of the training data in order to generalize well to new data. This paper proposes a novel Worst-case Feature Risk Minimization (WFRM) method that helps improve model generalization. Specifically, we tackle a minimax optimization problem in feature space at each training iteration. Given the input features, we seek the feature perturbation that maximizes the current training loss and then minimizes the training loss of the worst-case features. By incorporating our WFRM during training, we significantly improve model generalization under distributional shift â Domain Generalization (DG) and in the low-data regime â Few-shot Learning (FSL). We theoretically analyze WFRM and find the key reason why it works better than ERM â it induces an empirical risk-based semi-adaptive $L_{2}$ regularization of the classifier weights, enabling a better risk-complexity trade-off. We evaluate WFRM on two data-efficient learning tasks, including three standard DG benchmarks of PACS, VLCS, OfficeHome and the most challenging FSL benchmark Meta-Dataset. Despite the simplicity, our method consistently improves various DG and FSL methods, leading to the new state-of-the-art performances in all settings. Codes & models will be released at https://github.com/jslei/WFRM",
    "checked": true,
    "id": "638f2aaa05512b1cea4cb923b07cacfb2c336a9c",
    "semantic_title": "worst-case feature risk minimization for data-efficient learning",
    "citation_count": 0,
    "authors": [
      "Jingshi Lei",
      "Da Li",
      "Chengming Xu",
      "Liming Fang",
      "Timothy Hospedales",
      "Yanwei Fu"
    ]
  },
  "https://openreview.net/forum?id=CAd6V2qXxc": {
    "title": "Conformal prediction under ambiguous ground truth",
    "volume": "main",
    "abstract": "Conformal Prediction (CP) allows to perform rigorous uncertainty quantification by constructing a prediction set $C(X)$ satisfying $\\mathbb{P}(Y \\in C(X))\\geq 1-\\alpha$ for a user-chosen $\\alpha \\in [0,1]$ by relying on calibration data $(X_1,Y_1),...,(X_n,Y_n)$ from $\\mathbb{P}=\\mathbb{P}^{X} \\otimes \\mathbb{P}^{Y|X}$. It is typically implicitly assumed that $\\mathbb{P}^{Y|X}$ is the ``true'' posterior label distribution. However, in many real-world scenarios, the labels $Y_1,...,Y_n$ are obtained by aggregating expert opinions using a voting procedure, resulting in a one-hot distribution $\\mathbb{P}_{\\textup{vote}}^{Y|X}$. This is the case for most datasets, even well-known ones like ImageNet. For such ``voted'' labels, CP guarantees are thus w.r.t. $\\mathbb{P}_{\\textup{vote}}=\\mathbb{P}^X \\otimes \\mathbb{P}_{\\textup{vote}}^{Y|X}$ rather than the true distribution $\\mathbb{P}$. In cases with unambiguous ground truth labels, the distinction between $\\mathbb{P}_{\\textup{vote}}$ and $\\mathbb{P}$ is irrelevant. However, when experts do not agree because of ambiguous labels, approximating $\\mathbb{P}^{Y|X}$ with a one-hot distribution $\\mathbb{P}_{\\textup{vote}}^{Y|X}$ ignores this uncertainty. In this paper, we propose to leverage expert opinions to approximate $\\mathbb{P}^{Y|X}$ using a non-degenerate distribution $\\mathbb{P}_{\\textup{agg}}^{Y|X}$. We then develop \\emph{Monte Carlo CP} procedures which provide guarantees w.r.t. $\\mathbb{P}_{\\textup{agg}}=\\mathbb{P}^X \\otimes \\mathbb{P}_{\\textup{agg}}^{Y|X}$ by sampling multiple synthetic pseudo-labels from $\\mathbb{P}_{\\textup{agg}}^{Y|X}$ for each calibration example $X_1,...,X_n$. In a case study of skin condition classification with significant disagreement among expert annotators, we show that applying CP w.r.t. $\\mathbb{P}_{\\textup{vote}}$ under-covers expert annotations: calibrated for $72\\%$ coverage, it falls short by on average $10\\%$; our Monte Carlo CP closes this gap both empirically and theoretically. We also extend Monte Carlo CP to multi-label classification and CP with calibration examples enriched through data augmentation",
    "checked": true,
    "id": "7a65e919fa4b2e0d03273935a4ff413a993764f4",
    "semantic_title": "conformal prediction under ambiguous ground truth",
    "citation_count": 20,
    "authors": [
      "David Stutz",
      "Abhijit Guha Roy",
      "Tatiana Matejovicova",
      "Patricia Strachan",
      "Ali Taylan Cemgil",
      "Arnaud Doucet"
    ]
  },
  "https://openreview.net/forum?id=RFfUUtKYOG": {
    "title": "Towards Stability of Autoregressive Neural Operators",
    "volume": "main",
    "abstract": "Neural operators have proven to be a promising approach for modeling spatiotemporal systems in the physical sciences. However, training these models for large systems can be quite challenging as they incur significant computational and memory expense---these systems are often forced to rely on autoregressive time-stepping of the neural network to predict future temporal states. While this is effective in managing costs, it can lead to uncontrolled error growth over time and eventual instability. We analyze the sources of this autoregressive error growth using prototypical neural operator models for physical systems and explore ways to mitigate it. We introduce architectural and application-specific improvements that allow for careful control of instability-inducing operations within these models without inflating the compute/memory expense. We present results on several scientific systems that include Navier-Stokes fluid flow, rotating shallow water, and a high-resolution global weather forecasting system. We demonstrate that applying our design principles to neural operators leads to significantly lower errors for long-term forecasts as well as longer time horizons without qualitative signs of divergence compared to the original models for these systems. We open-source our code for reproducibility",
    "checked": true,
    "id": "b03dfd3500ed198ff5c42ecec3fcd07247baaae5",
    "semantic_title": "towards stability of autoregressive neural operators",
    "citation_count": 20,
    "authors": [
      "Michael McCabe",
      "Peter Harrington",
      "Shashank Subramanian",
      "Jed Brown"
    ]
  },
  "https://openreview.net/forum?id=ZD03VUZmRx": {
    "title": "$f$-MICL: Understanding and Generalizing InfoNCE-based Contrastive Learning",
    "volume": "main",
    "abstract": "In self-supervised contrastive learning, a widely-adopted objective function is InfoNCE, which uses the heuristic cosine similarity for the representation comparison, and is closely related to maximizing the Kullback-Leibler (KL)-based mutual information. In this paper, we aim at answering two intriguing questions: (1) Can we go beyond the KL-based objective? (2) Besides the popular cosine similarity, can we design a better similarity function? We provide answers to both questions by generalizing the KL-based mutual information to the $f$-Mutual Information in Contrastive Learning ($f$-MICL) using the $f$-divergences. To answer the first question, we provide a wide range of $f$-MICL objectives which share the nice properties of InfoNCE (e.g., alignment and uniformity), and meanwhile result in similar or even superior performance. For the second question, assuming that the joint feature distribution is proportional to the Gaussian kernel, we derive an $f$-Gaussian similarity with better interpretability and empirical performance. Finally, we identify close relationships between the $f$-MICL objective and several popular InfoNCE-based objectives. Using benchmark tasks from both vision and natural language, we empirically evaluate $f$-MICL with different $f$-divergences on various architectures (SimCLR, MoCo, and MoCo v3) and datasets. We observe that $f$-MICL generally outperforms the benchmarks and the best-performing $f$-divergence is task and dataset dependent",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiwei Lu",
      "Guojun Zhang",
      "Sun Sun",
      "Hongyu Guo",
      "Yaoliang Yu"
    ]
  },
  "https://openreview.net/forum?id=fWIQ9Oaao0": {
    "title": "Non-Stationary Contextual Pricing with Safety Constraints",
    "volume": "main",
    "abstract": "In a contextual pricing problem, a seller aims at maximizing the revenue over a sequence of sales sessions (described by feature vectors) using binary-censored feedback of \"sold\" or \"not sold\". Existing methods often overlook two practical challenges (1) the best pricing strategy could change over time; (2) the prices and pricing policies must conform to hard constraints due to safety, ethical or legal restrictions. We address both challenges by solving a more general problem of \"universal dynamic regret\" minimization in proper online learning with exp-concave losses --- an open problem posed by Baby & Wang (2021) that we partially resolve in this paper, with attention restricted to loss functions coming from a generalized linear model. Here \"dynamic regret\" measures the performance relative to a non-stationary sequence of policies, and \"proper\" means that the learner must choose feasible strategies within a pre-defined convex set, which we use to model the safety constraints. In this work, we consider a linear noisy valuation model for the customers. In the case of a known strictly log-concave market noise, our algorithm achieves $\\tilde{O}(d^3T^{1/3}C_T^{2/3} \\vee d^3)$ dynamic regret in comparison with the optimal policy series, where $T$, $d$ and $C_T$ stand for the time horizon, the feature dimension and the total variation (characterizing non-stationarity) respectively. This regret is near-optimal with respect to $T$ (within $O(\\log T)$ gaps) and $C_T$, and our algorithm is adaptable to unknown $C_T$ and remains feasible throughout. However, the dependence on $d$ is suboptimal and the minimax rate is still open",
    "checked": true,
    "id": "cc5f481d38da0300b2780e35b9e895651e32894c",
    "semantic_title": "non-stationary contextual pricing with safety constraints",
    "citation_count": 4,
    "authors": [
      "Dheeraj Baby",
      "Jianyu Xu",
      "Yu-Xiang Wang"
    ]
  },
  "https://openreview.net/forum?id=Kt2VJrCKo4": {
    "title": "VoLTA: Vision-Language Transformer with Weakly-Supervised Local-Feature Alignment",
    "volume": "main",
    "abstract": "Vision-language pre-training (VLP) has recently proven highly effective for various uni- and multi-modal downstream applications. However, most existing end-to-end VLP methods use high-resolution image-text-box data to perform well on fine-grained region-level tasks, such as object detection, segmentation, and referring expression comprehension. Unfortunately, such high-resolution images with accurate bounding box annotations are expensive to collect and use for supervision at scale. In this work, we propose VoLTA (Vision Language Transformer with weakly-supervised local-feature Alignment), a new VLP paradigm that only utilizes image-caption data but achieves fine-grained region-level image understanding, eliminating the need for expensive box annotations. VoLTA adopts graph optimal transport-based weakly-supervised alignment on local image patches and text tokens to germinate an explicit, self-normalized, and interpretable low-level matching criterion. In addition, VoLTA pushes multi-modal fusion deep into the uni-modal backbones during pre training and removes fusion-specific transformer layers, further reducing memory requirements. Extensive experiments on a wide range of vision- and vision-language downstream tasks demonstrate the effectiveness of VoLTA on fine-grained applications without compromising the coarse-grained downstream performance, often outperforming methods using significantly more caption and box annotations",
    "checked": true,
    "id": "0ee11b28a9ce49d3030cab11f1178fa5abae9c3b",
    "semantic_title": "volta: vision-language transformer with weakly-supervised local-feature alignment",
    "citation_count": 22,
    "authors": [
      "Shraman Pramanick",
      "Li Jing",
      "Sayan Nag",
      "Jiachen Zhu",
      "Hardik J Shah",
      "Yann LeCun",
      "Rama Chellappa"
    ]
  },
  "https://openreview.net/forum?id=YgeXqrH7gA": {
    "title": "Benefits of Max Pooling in Neural Networks: Theoretical and Experimental Evidence",
    "volume": "main",
    "abstract": "When deep neural networks became state of the art image classifiers, numerous max pooling operations were an important component of the architecture. However, modern computer vision networks typically have few, if any, max pooling operations. To understand whether this trend is justified, we develop a mathematical framework analyzing ReLU based approximations of max pooling, and prove a sense in which max pooling cannot be replicated. We formulate and analyze a novel class of optimal approximations, and find that the residual can be made exponentially small in the kernel size, but only with an exponentially wide approximation. This work gives a theoretical basis for understanding the reduced use of max pooling in newer architectures. It also enables us to establish an empirical observation about natural images: since max pooling does not seem necessary, the inputs on which max pooling is distinct â those with a large difference between the max and other values â are not prevalent",
    "checked": true,
    "id": "4f13e06f319373cb96e10857e1a940db92670e42",
    "semantic_title": "benefits of max pooling in neural networks: theoretical and experimental evidence",
    "citation_count": 0,
    "authors": [
      "Kyle Matoba",
      "Nikolaos Dimitriadis",
      "FranÃ§ois Fleuret"
    ]
  },
  "https://openreview.net/forum?id=adpKzWQunW": {
    "title": "Local Advantage Networks for Multi-Agent Reinforcement Learning in Dec-POMDPs",
    "volume": "main",
    "abstract": "Many recent successful off-policy multi-agent reinforcement learning (MARL) algorithms for cooperative partially observable environments focus on finding factorized value functions, leading to convoluted network structures. Building on the structure of independent Q-learners, our LAN algorithm takes a radically different approach, leveraging a dueling architecture to learn for each agent a decentralized best-response policies via individual advantage functions. The learning is stabilized by a centralized critic whose primary objective is to reduce the moving target problem of the individual advantages. The critic, whose network's size is independent of the number of agents, is cast aside after learning. Evaluation on the StarCraft II multi-agent challenge benchmark shows that LAN reaches state-of-the-art performance and is highly scalable with respect to the number of agents, opening up a promising alternative direction for MARL research",
    "checked": true,
    "id": "017cc5edbe203b9cb8b6876124b80ac4e4edcd66",
    "semantic_title": "local advantage networks for multi-agent reinforcement learning in dec-pomdps",
    "citation_count": 6,
    "authors": [
      "RaphaÃ«l Avalos",
      "Mathieu Reymond",
      "Ann Nowe",
      "Diederik M Roijers"
    ]
  },
  "https://openreview.net/forum?id=Tkvmt9nDmB": {
    "title": "Beyond Distribution Shift: Spurious Features Through the Lens of Training Dynamics",
    "volume": "main",
    "abstract": "Deep Neural Networks (DNNs) are prone to learning spurious features that correlate with the label during training but are irrelevant to the learning problem. This hurts model generalization and poses problems when deploying them in safety-critical applications. This paper aims to better understand the effects of spurious features through the lens of the learning dynamics of the internal neurons during the training process. We make the following observations: (1) While previous works highlight the harmful effects of spurious features on the generalization ability of DNNs, we emphasize that not all spurious features are harmful. Spurious features can be \"benign\" or \"harmful\" depending on whether they are \"harder\" or \"easier\" to learn than the core features for a given model. This definition is model and dataset dependent. (2) We build upon this premise and use instance difficulty methods (like Prediction Depth) to quantify \"easiness\" for a given model and to identify this behavior during the training phase. (3) We empirically show that the harmful spurious features can be detected by observing the learning dynamics of the DNN's early layers. In other words, easy features learned by the initial layers of a DNN early during the training can (potentially) hurt model generalization. We verify our claims on medical and vision datasets, both simulated and real, and justify the empirical success of our hypothesis by showing the theoretical connections between Prediction Depth and information-theoretic concepts like $\\mathcal{V}$-usable information. Lastly, our experiments show that monitoring only accuracy during training (as is common in machine learning pipelines) is insufficient to detect spurious features. We, therefore, highlight the need for monitoring early training dynamics using suitable instance difficulty metrics",
    "checked": true,
    "id": "8c3ed2f05511ececa8cbe28bdd5dd5071ab934b5",
    "semantic_title": "beyond distribution shift: spurious features through the lens of training dynamics",
    "citation_count": 10,
    "authors": [
      "Nihal Murali",
      "Aahlad Manas Puli",
      "Ke Yu",
      "Rajesh Ranganath",
      "kayhan Batmanghelich"
    ]
  },
  "https://openreview.net/forum?id=3AzqYa18ah": {
    "title": "Pareto Actor-Critic for Equilibrium Selection in Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "This work focuses on equilibrium selection in no-conflict multi-agent games, where we specifically study the problem of selecting a Pareto-optimal Nash equilibrium among several existing equilibria. It has been shown that many state-of-the-art multi-agent reinforcement learning (MARL) algorithms are prone to converging to Pareto-dominated equilibria due to the uncertainty each agent has about the policy of the other agents during training. To address sub-optimal equilibrium selection, we propose Pareto Actor-Critic (Pareto-AC), which is an actor-critic algorithm that utilises a simple property of no-conflict games (a superset of cooperative games): the Pareto-optimal equilibrium in a no-conflict game maximises the returns of all agents and, therefore, is the preferred outcome for all agents. We evaluate Pareto-AC in a diverse set of multi-agent games and show that it converges to higher episodic returns compared to seven state-of-the-art MARL algorithms and that it successfully converges to a Pareto-optimal equilibrium in a range of matrix games. Finally, we propose PACDCG, a graph neural network extension of Pareto-AC, which is shown to efficiently scale in games with a large number of agents",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Filippos Christianos",
      "Georgios Papoudakis",
      "Stefano V Albrecht"
    ]
  },
  "https://openreview.net/forum?id=lanGfX0M6C": {
    "title": "Bridging Imitation and Online Reinforcement Learning: An Optimistic Tale",
    "volume": "main",
    "abstract": "In this paper, we address the following problem: Given an offline demonstration dataset from an imperfect expert, what is the best way to leverage it to bootstrap online learning performance in MDPs. We first propose an Informed Posterior Sampling-based RL (iPSRL) algorithm that uses the offline dataset, and information about the expert's behavioral policy used to generate the offline dataset. Its cumulative Bayesian regret goes down to zero exponentially fast in $N$, the offline dataset size if the expert is competent enough. Since this algorithm is computationally impractical, we then propose the iRLSVI algorithm that can be seen as a combination of the RLSVI algorithm for online RL, and imitation learning. Our empirical results show that the proposed iRLSVI algorithm is able to achieve significant reduction in regret as compared to two baselines: no offline data, and offline dataset but used without suitably modeling the generative policy. Our algorithm can be seen as bridging online RL and imitation learning",
    "checked": true,
    "id": "5aeef5fc2533f8deeefb73688040279acad67e96",
    "semantic_title": "bridging imitation and online reinforcement learning: an optimistic tale",
    "citation_count": 3,
    "authors": [
      "Botao Hao",
      "Rahul Jain",
      "Dengwang Tang",
      "Zheng Wen"
    ]
  },
  "https://openreview.net/forum?id=REAyrhRYAo": {
    "title": "Gradient Masked Averaging for Federated Learning",
    "volume": "main",
    "abstract": "Federated learning (FL) is an emerging paradigm that permits a large number of clients with heterogeneous data to coordinate learning of a unified global model without the need to share data amongst each other. A major challenge in federated learning is the heterogeneity of data across client, which can degrade the performance of standard FL algorithms. Standard FL algorithms involve averaging of model parameters or gradient updates to approximate the global model at the server. However, we argue that in heterogeneous settings, averaging can result in information loss and lead to poor generalization due to the bias induced by dominant client gradients. We hypothesize that to generalize better across non-i.i.d datasets, the algorithms should focus on learning the invariant mechanism that is constant while ignoring spurious mechanisms that differ across clients. Inspired from recent works in Out-of-Distribution generalization, we propose a gradient masked averaging approach for FL as an alternative to the standard averaging of client updates. This aggregation technique for client updates can be adapted as a drop-in replacement in most existing federated algorithms. We perform extensive experiments on multiple FL algorithms with in-distribution, real-world, feature-skewed out-of-distribution, and quantity imbalanced datasets and show that it provides consistent improvements, particularly in the case of heterogeneous clients",
    "checked": true,
    "id": "3e583ea1e45c7772904131850add04d4ac8b8c91",
    "semantic_title": "gradient masked averaging for federated learning",
    "citation_count": 25,
    "authors": [
      "Irene Tenison",
      "Sai Aravind Sreeramadas",
      "Vaikkunth Mugunthan",
      "Edouard Oyallon",
      "Irina Rish",
      "Eugene Belilovsky"
    ]
  },
  "https://openreview.net/forum?id=xLnbSpozWS": {
    "title": "Training Vision-Language Transformers from Captions",
    "volume": "main",
    "abstract": "Vision-Language Transformers can be learned without low-level human labels (e.g. class labels, bounding boxes, etc). Existing work, whether explicitly utilizing bounding boxes (Chen et al., 2020b; Tan & Bansal, 2019; Lu et al., 2019) or patches (Kim et al., 2021), assumes that the visual backbone must first be trained on ImageNet (Russakovsky et al., 2015) class prediction before being integrated into a multimodal linguistic pipeline. We show that this is not necessary and introduce a new model Vision-Language from Captions (VLC) built on top of Masked Auto-Encoders (He et al., 2022) that does not require this supervision. We seek to provide general advice on multimodal pretraining by examining the roles of (a) unimodal initialization, (b) unimodal architectural components and (c) data annotation in the pretraining corpus. Our extensive and carefully controlled studies suggest that none of the above factors is absolutely important in achieving versatile vision-language representations. We conclude our analysis with suggestions on the choices of initialization, architectural components, and annotation formats targeting a better balance between data efficiency and representation quality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liangke Gui",
      "Yingshan Chang",
      "Qiuyuan Huang",
      "Subhojit Som",
      "Alexander G Hauptmann",
      "Jianfeng Gao",
      "Yonatan Bisk"
    ]
  },
  "https://openreview.net/forum?id=3Ba6Hd3nZt": {
    "title": "Policy Gradient Algorithms Implicitly Optimize by Continuation",
    "volume": "main",
    "abstract": "Direct policy optimization in reinforcement learning is usually solved with policy-gradient algorithms, which optimize policy parameters via stochastic gradient ascent. This paper provides a new theoretical interpretation and justification of these algorithms. First, we formulate direct policy optimization in the optimization by continuation framework. The latter is a framework for optimizing nonconvex functions where a sequence of surrogate objective functions, called continuations, are locally optimized. Second, we show that optimizing affine Gaussian policies and performing entropy regularization can be interpreted as implicitly optimizing deterministic policies by continuation. Based on these theoretical results, we argue that exploration in policy-gradient algorithms consists in computing a continuation of the return of the policy at hand, and that the variance of policies should be history-dependent functions adapted to avoid local extrema rather than to maximize the return of the policy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adrien Bolland",
      "Gilles Louppe",
      "Damien Ernst"
    ]
  },
  "https://openreview.net/forum?id=3dQCNqqv2d": {
    "title": "Self-Attention in Colors: Another Take on Encoding Graph Structure in Transformers",
    "volume": "main",
    "abstract": "We introduce a novel self-attention mechanism, which we call CSA (Chromatic Self-Attention), which extends the notion of attention scores to attention _filters_, independently modulating the feature channels. We showcase CSA in a fully-attentional graph Transformer CGT (Chromatic Graph Transformer) which integrates both graph structural information and edge features, completely bypassing the need for local message-passing components. Our method flexibly encodes graph structure through node-node interactions, by enriching the original edge features with a relative positional encoding scheme. We propose a new scheme based on random walks that encodes both structural and positional information, and show how to incorporate higher-order topological information, such as rings in molecular graphs. Our approach achieves state-of-the-art results on the ZINC benchmark dataset, while providing a flexible framework for encoding graph structure and incorporating higher-order topology",
    "checked": true,
    "id": "458c8a10ec37466a6a554202c472e8faea0834ad",
    "semantic_title": "self-attention in colors: another take on encoding graph structure in transformers",
    "citation_count": 7,
    "authors": [
      "Romain Menegaux",
      "Emmanuel Jehanno",
      "Margot Selosse",
      "Julien Mairal"
    ]
  },
  "https://openreview.net/forum?id=Q2Gi0TUAdS": {
    "title": "Identifying latent distances with Finslerian geometry",
    "volume": "main",
    "abstract": "Riemannian geometry provides us with powerful tools to explore the latent space of generative models while preserving the underlying structure of the data. The latent space can be equipped it with a Riemannian metric, pulled back from the data manifold. With this metric, we can systematically navigate the space relying on geodesics defined as the shortest curves between two points. Generative models are often stochastic, causing the data space, the Riemannian metric, and the geodesics, to be stochastic as well. Stochastic objects are at best impractical, and at worst impossible, to manipulate. A common solution is to approximate the stochastic pullback metric by its expectation. But the geodesics derived from this expected Riemannian metric do not correspond to the expected length-minimising curves. In this work, we propose another metric whose geodesics explicitly minimise the expected length of the pullback metric. We show this metric defines a Finsler metric, and we compare it with the expected Riemannian metric. In high dimensions, we prove that both metrics converge to each other at a rate of $\\mathcal{O}\\left(\\frac{1}{D}\\right)$. This convergence implies that the established expected Riemannian metric is an accurate approximation of the theoretically more grounded Finsler metric. This provides justification for using the expected Riemannian metric for practical implementations",
    "checked": true,
    "id": "0b1e512ac57a9ecd11635980605bae84888f65d9",
    "semantic_title": "identifying latent distances with finslerian geometry",
    "citation_count": 1,
    "authors": [
      "Alison Pouplin",
      "David Eklund",
      "Carl Henrik Ek",
      "SÃ¸ren Hauberg"
    ]
  },
  "https://openreview.net/forum?id=CpYBAqDgmz": {
    "title": "Discretization Invariant Networks for Learning Maps between Neural Fields",
    "volume": "main",
    "abstract": "With the emergence of powerful representations of continuous data in the form of neural fields, there is a need for discretization invariant learning: an approach for learning maps between functions on continuous domains without being sensitive to how the function is sampled. We present a new framework for understanding and designing discretization invariant neural networks (DI-Nets), which generalizes many discrete networks such as convolutional neural networks as well as continuous networks such as neural operators. Our analysis establishes upper bounds on the deviation in model outputs under different finite discretizations, and highlights the central role of point set discrepancy in characterizing such bounds. This insight leads to the design of a family of neural networks driven by numerical integration via quasi-Monte Carlo sampling with discretizations of low discrepancy. We prove by construction that DI-Nets universally approximate a large class of maps between integrable function spaces, and show that discretization invariance also describes backpropagation through such models. Applied to neural fields, convolutional DI-Nets can learn to classify and segment visual data under various discretizations, and sometimes generalize to new types of discretizations at test time",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Clinton Wang",
      "Polina Golland"
    ]
  },
  "https://openreview.net/forum?id=QfyVqvpg7u": {
    "title": "Physics informed neural networks for elliptic equations with oscillatory differential operators",
    "volume": "main",
    "abstract": "Physics informed neural network (PINN) based solution methods for differential equations have recently shown success in a variety of scientific computing applications. Several authors have reported difficulties, however, when using PINNs to solve equations with multiscale features. The objective of the present work is to illustrate and explain the difficulty of using standard PINNs for the particular case of divergence-form elliptic partial differential equations (PDEs) with oscillatory coefficients present in the differential operator. We show that if the coefficient in the elliptic operator $a^{\\epsilon}(x)$ is of the form $a(x/\\epsilon)$ for a 1-periodic coercive function $a(\\cdot)$, then the Frobenius norm of the neural tangent kernel (NTK) matrix associated to the loss function grows as $1/\\epsilon^2$. This implies that as the separation of scales in the problem increases, training the neural network with gradient descent based methods to achieve an accurate approximation of the solution to the PDE becomes increasingly difficult. Numerical examples illustrate the stiffness of the optimization problem",
    "checked": true,
    "id": "8fbb03cb338f27f99f768d2f1d170fe8234b4391",
    "semantic_title": "physics informed neural networks for elliptic equations with oscillatory differential operators",
    "citation_count": 1,
    "authors": [
      "Arnav Gangal",
      "Luis Kim",
      "Sean Patrick Carney"
    ]
  },
  "https://openreview.net/forum?id=djD8IbSvgm": {
    "title": "Greedier is Better: Selecting Multiple Neighbors per Iteration for Sparse Subspace Clustering",
    "volume": "main",
    "abstract": "Sparse subspace clustering (SSC) using greedy-based neighbor selection, such as orthogonal matching pursuit (OMP), has been known as a popular computationally-efficient alternative to the standard $\\ell_1$-minimization based methods. However, existing stopping rules of OMP to halt neighbor search needs additional offline work to estimate some ground truths, e.g., subspace dimension and/or noise strength. This paper proposes a new SSC scheme using generalized OMP (GOMP), a soup-up of OMP whereby multiple, say $p(\\geq1)$, neighbors are identified per iteration to further speed up neighbor acquisition, along with a new stopping rule requiring nothing more than a knowledge of the ambient signal dimension and the number $p$ of identified neighbors in each iteration. Compared to conventional OMP (i.e., $p=1$), the proposed GOMP method involves fewer iterations, thereby enjoying lower algorithmic complexity. Under the semi-random model, analytic performance guarantees are provided. It is shown that, with a high probability, (i) GOMP can retrieve more true neighbors than OMP, consequently yielding higher data clustering accuracy, and (ii) the proposed stopping rule terminates neighbor search once the number of recovered neighbors is close to the subspace dimension. Issues about selecting $p$ for practical implementation are also discussed. Computer simulations using both synthetic and real data are provided to demonstrate the effectiveness of the proposed approach and validate our analytic study",
    "checked": true,
    "id": "ae114bf870d7d110892f8bd9d89bd4a62aeb3216",
    "semantic_title": "greedier is better: selecting multiple neighbors per iteration for sparse subspace clustering",
    "citation_count": 0,
    "authors": [
      "Jwo-Yuh Wu",
      "Liang-Chi Huang",
      "Wen Hsuan Li",
      "Chun-Hung Liu",
      "Rung-Hung Gau"
    ]
  },
  "https://openreview.net/forum?id=NekBTCKJ1H": {
    "title": "Distributed Newton-Type Methods with Communication Compression and Bernoulli Aggregation",
    "volume": "main",
    "abstract": "Despite their high computation and communication costs, Newton-type methods remain an appealing option for distributed training due to their robustness against ill-conditioned convex problems. In this work, we study communication compression and aggregation mechanisms for curvature information in order to reduce these costs while preserving theoretically superior local convergence guarantees. We prove that the recently developed class of three point compressors (3PC) of [Richtarik et al., 2022] for gradient communication can be generalized to Hessian communication as well. This result opens up a wide variety of communication strategies, such as contractive compression and lazy aggregation, available to our disposal to compress prohibitively costly curvature information. Moreover, we discovered several new 3PC mechanisms, such as adaptive thresholding and Bernoulli aggregation, which require reduced communication and occasional Hessian computations. Furthermore, we extend and analyze our approach to bidirectional communication compression and partial device participation setups to cater to the practical considerations of applications in federated learning. For all our methods, we derive fast condition-number-independent local linear and/or superlinear convergence rates. Finally, with extensive numerical evaluations on convex optimization problems, we illustrate that our designed schemes achieve state-of-the-art communication complexity compared to several key baselines using second-order information",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rustem Islamov",
      "Xun Qian",
      "Slavomir Hanzely",
      "Mher Safaryan",
      "Peter RichtÃ¡rik"
    ]
  },
  "https://openreview.net/forum?id=LWotmCKC6Y": {
    "title": "Fourier Features in Reinforcement Learning with Neural Networks",
    "volume": "main",
    "abstract": "In classic Reinforcement Learning (RL), the performance of algorithms depends critically on data representation, i.e., the way the states of the system are represented as features. Choosing appropriate features for a task is an important way of adding prior domain knowledge since cleverly distributing information into states facilitates appropriate generalization. For linear function approximations, the representation is usually hand-designed according to the task at hand and projected into a higher-dimensional space to facilitate linear separation. Among the feature encodings used in RL for linear function approximation, we can mention in a non-exhaustive way Polynomial Features or Tile Coding. However, the main bottleneck of such feature encodings is that they do not scale to high-dimensional inputs as they grow exponentially in size with the input dimension",
    "checked": true,
    "id": "fddebe1b1ed229db667b2406041e7859148e71d3",
    "semantic_title": "fourier features in reinforcement learning with neural networks",
    "citation_count": 5,
    "authors": [
      "David Brellmann",
      "David Filliat",
      "Goran Frehse"
    ]
  },
  "https://openreview.net/forum?id=Ulf3QZG9DC": {
    "title": "Diagnostic Tool for Out-of-Sample Model Evaluation",
    "volume": "main",
    "abstract": "Assessment of model fitness is a key part of machine learning. The standard paradigm of model evaluation is analysis of the average loss over future data. This is often explicit in model fitting, where we select models that minimize the average loss over training data as a surrogate, but comes with limited theoretical guarantees. In this paper, we consider the problem of characterizing a batch of out-of-sample losses of a model using a calibration data set. We provide finite-sample limits on the out-of-sample losses that are statistically valid under quite general conditions and propose a diagonistic tool that is simple to compute and interpret. Several numerical experiments are presented to show how the proposed method quantifies the impact of distribution shifts, aids the analysis of regression, and enables model selection as well as hyperparameter tuning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ludvig Hult",
      "Dave Zachariah",
      "Peter Stoica"
    ]
  },
  "https://openreview.net/forum?id=gxEpUFxIgz": {
    "title": "Straggler-Resilient Personalized Federated Learning",
    "volume": "main",
    "abstract": "Federated Learning is an emerging learning paradigm that allows training models from samples distributed across a large network of clients while respecting privacy and communication restrictions. Despite its success, federated learning faces several challenges related to its decentralized nature. In this work, we develop a novel algorithmic procedure with theoretical speedup guarantees that simultaneously handles two of these hurdles, namely (i) data heterogeneity, i.e., data distributions can vary substantially across clients, and (ii) system heterogeneity, i.e., the computational power of the clients could differ significantly. By leveraging previous works in the realm of representation learning (Collins et al., 2021; Liang et al., 2020), our method constructs a global common representation utilizing the data from all clients. Additionally, it learns a user-specific set of parameters resulting in a personalized solution for each individual client. Furthermore, it mitigates the effects of stragglers by adaptively selecting clients based on their computational characteristics, thus achieving for the first time near optimal sample complexity and provable logarithmic speedup. Experimental results support our theoretical findings showing the superiority of our method over alternative personalized federated schemes in system and data heterogeneous environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Isidoros Tziotis",
      "Zebang Shen",
      "Ramtin Pedarsani",
      "Hamed Hassani",
      "Aryan Mokhtari"
    ]
  },
  "https://openreview.net/forum?id=izFnURFG3f": {
    "title": "Self-supervised Learning for Segmentation and Quantification of Dopamine Neurons in Parkinson's Disease",
    "volume": "main",
    "abstract": "Parkinson's Disease (PD) is the second most common neurodegenerative disease in humans. PD is characterized by the gradual loss of dopaminergic neurons in the Substantia Nigra (SN, a part of the mid-brain). Counting the number of dopaminergic neurons in the SN is one of the most important indexes in evaluating drug efficacy in PD animal models. Currently, analyzing and quantifying dopaminergic neurons is conducted manually by experts through analysis of digital pathology images which is laborious, time-consuming, and highly subjective. As such, a reliable and unbiased automated system is demanded for the quantification of dopaminergic neurons in digital pathology images. Recent years have seen a surge in adopting deep learning solutions in medical image processing. However, developing high-performing deep learning models hinges on the availability of large-scale, high-quality annotated data, which can be expensive to acquire, especially in applications like digital pathology image analysis. To this end, we propose an end-to-end deep learning framework based on self-supervised learning for the segmentation and quantification of dopaminergic neurons in PD animal models. To the best of our knowledge, this is the first deep learning model that detects the cell body of dopaminergic neurons, counts the number of dopaminergic neurons, and provides characteristics of individual dopaminergic neurons as a numerical output. Extensive experiments demonstrate the effectiveness of our model in quantifying neurons with high precision, which can provide a faster turnaround for drug efficacy studies,better understanding of dopaminergic neuronal health status, and unbiased results in PD pre-clinical research. As part of our contributions, we also provide the first publicly available dataset of histology digital images along with expert annotations for the segmentation of TH-positive DA neuronal soma",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fatemeh Haghighi",
      "soumitra ghosh",
      "Sarah Chu",
      "Hai Ngu",
      "Mohsen Hejrati",
      "Han Hui Lin",
      "Baris Bingol",
      "Somaye Hashemifar"
    ]
  },
  "https://openreview.net/forum?id=xgYgDEof29": {
    "title": "Analysis of Convolutions, Non-linearity and Depth in Graph Neural Networks using Neural Tangent Kernel",
    "volume": "main",
    "abstract": "The fundamental principle of Graph Neural Networks (GNNs) is to exploit the structural information of the data by aggregating the neighboring nodes using a `graph convolution' in conjunction with a suitable choice for the network architecture, such as depth and activation functions. Therefore, understanding the influence of each of the design choice on the network performance is crucial. Convolutions based on graph Laplacian have emerged as the dominant choice with the symmetric normalization of the adjacency matrix as the most widely adopted one. However, some empirical studies show that row normalization of the adjacency matrix outperforms it in node classification. Despite the widespread use of GNNs, there is no rigorous theoretical study on the representation power of these convolutions, that could explain this behavior. Similarly, the empirical observation of the linear GNNs performance being on par with non-linear ReLU GNNs lacks rigorous theory. In this work, we theoretically analyze the influence of different aspects of the GNN architecture using the Graph Neural Tangent Kernel in a semi-supervised node classification setting. Under the population Degree Corrected Stochastic Block Model, we prove that: (i) linear networks capture the class information as good as ReLU networks; (ii) row normalization preserves the underlying class structure better than other convolutions; (iii) performance degrades with network depth due to over-smoothing, but the loss in class information is the slowest in row normalization; (iv) skip connections retain the class information even at infinite depth, thereby eliminating over-smoothing. We finally validate our theoretical findings numerically and on real datasets such as Cora and Citeseer",
    "checked": true,
    "id": "8df003baa77649a80cc44ba548e1351909bcf116",
    "semantic_title": "analysis of convolutions, non-linearity and depth in graph neural networks using neural tangent kernel",
    "citation_count": 2,
    "authors": [
      "Mahalakshmi Sabanayagam",
      "Pascal Esser",
      "Debarghya Ghoshdastidar"
    ]
  },
  "https://openreview.net/forum?id=8wGXnjRLSy": {
    "title": "Zero-shot Node Classification with Graph Contrastive Embedding Network",
    "volume": "main",
    "abstract": "This paper studies zero-shot node classification, which aims to predict new classes (i.e., unseen classes) of nodes in a graph. This problem is challenging yet promising in a variety of real-world applications such as social analysis and bioinformatics. The key of zero-shot node classification is to enable the knowledge transfer of nodes from training classes to unseen classes. However, existing methods typically ignore the dependencies between nodes and classes, and fail to be organically integrated in a united way. In this paper, we present a novel framework called the Graph Contrastive Embedding Network (GraphCEN) for zero-shot node classification. Specifically, GraphCEN first constructs an affinity graph to model the relations between the classes. Then the node- and class-level contrastive learning (CL) are proposed to jointly learn node embeddings and class assignments in an end-to-end manner. The two-level CL can be optimized to mutually enhance each other. Extensive experiments indicate that our GraphCEN significantly outperforms the state-of-the-art approaches on multiple challenging benchmark datasets",
    "checked": true,
    "id": "4b31bd0ebbf4fcba2a538b17250277dedf3cf5e8",
    "semantic_title": "zero-shot node classification with graph contrastive embedding network",
    "citation_count": 21,
    "authors": [
      "Wei Ju",
      "Yifang Qin",
      "Siyu Yi",
      "Zhengyang Mao",
      "Kangjie Zheng",
      "Luchen Liu",
      "Xiao Luo",
      "Ming Zhang"
    ]
  },
  "https://openreview.net/forum?id=zKgJ6TWAFE": {
    "title": "Sharper Rates and Flexible Framework for Nonconvex SGD with Client and Data Sampling",
    "volume": "main",
    "abstract": "We revisit the classical problem of finding an approximately stationary point of the average of $n$ smooth and possibly nonconvex functions. The optimal complexity of stochastic first-order methods in terms of the number of gradient evaluations of individual functions is $\\mathcal{O}\\left(n + n^{1/2}\\varepsilon^{-1}\\right)$, attained by the optimal SGD methods SPIDER (Fang et al., 2018) and PAGE (Li et al., 2021), for example, where $\\varepsilon$ is the error tolerance. However, i) the big-$\\mathcal{O}$ notation hides crucial dependencies on the smoothness constants associated with the functions, and ii) the rates and theory in these methods assume simplistic sampling mechanisms that do not offer any flexibility. In this work we remedy the situation. First, we generalize the PAGE (Li et al., 2021) algorithm so that it can provably work with virtually any (unbiased) sampling mechanism. This is particularly useful in federated learning, as it allows us to construct and better understand the impact of various combinations of client and data sampling strategies. Second, our analysis is sharper as we make explicit use of certain novel inequalities that capture the intricate interplay between the smoothness constants and the sampling procedure. Indeed, our analysis is better even for the simple sampling procedure analyzed in the PAGE (Li et al., 2021) paper. However, this already improved bound can be further sharpened by a different sampling scheme which we propose. In summary, we provide the most general and most accurate analysis of optimal SGD in the smooth nonconvex regime. Finally, our theoretical findings are supposed with carefully designed experiments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Tyurin",
      "Lukang Sun",
      "Konstantin Pavlovich Burlachenko",
      "Peter RichtÃ¡rik"
    ]
  },
  "https://openreview.net/forum?id=YQWOzzSMPp": {
    "title": "An Analysis of Model-Based Reinforcement Learning From Abstracted Observations",
    "volume": "main",
    "abstract": "Many methods for Model-based Reinforcement learning (MBRL) in Markov decision processes (MDPs) provide guarantees for both the accuracy of the model they can deliver and the learning efficiency. At the same time, state abstraction techniques allow for a reduction of the size of an MDP while maintaining a bounded loss with respect to the original problem. Therefore, it may come as a surprise that no such guarantees are available when combining both techniques, i.e., where MBRL merely observes abstract states. Our theoretical analysis shows that abstraction can introduce a dependence between samples collected online (e.g., in the real world). That means that, without taking this dependence into account, results for MBRL do not directly extend to this setting. Our result shows that we can use concentration inequalities for martingales to overcome this problem. This result makes it possible to extend the guarantees of existing MBRL algorithms to the setting with abstraction. We illustrate this by combining R-MAX, a prototypical MBRL algorithm, with abstraction, thus producing the first performance guarantees for model-based âRL from Abstracted Observations': model-based reinforcement learning with an abstract model",
    "checked": true,
    "id": "40bda2294008695ffa48b0ef24d5d0952cb83065",
    "semantic_title": "an analysis of model-based reinforcement learning from abstracted observations",
    "citation_count": 1,
    "authors": [
      "Rolf A. N. Starre",
      "Marco Loog",
      "Elena Congeduti",
      "Frans A Oliehoek"
    ]
  },
  "https://openreview.net/forum?id=6OEcDKZj5j": {
    "title": "The Kernel Density Integral Transformation",
    "volume": "main",
    "abstract": "Feature preprocessing continues to play a critical role when applying machine learning and statistical methods to tabular data. In this paper, we propose the use of the kernel density integral transformation as a feature preprocessing step. Our approach subsumes the two leading feature preprocessing methods as limiting cases: linear min-max scaling and quantile transformation. We demonstrate that, without hyperparameter tuning, the kernel density integral transformation can be used as a simple drop-in replacement for either method, offering robustness to the weaknesses of each. Alternatively, with tuning of a single continuous hyperparameter, we frequently outperform both of these methods. Finally, we show that the kernel density transformation can be profitably applied to statistical data analysis, particularly in correlation analysis and univariate clustering",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Calvin McCarter"
    ]
  },
  "https://openreview.net/forum?id=lx1WnkL9fk": {
    "title": "Overcoming Resource Constraints in Federated Learning: Large Models Can Be Trained with only Weak Clients",
    "volume": "main",
    "abstract": "Federated Learning (FL) is emerging as a popular, promising decentralized learning framework that enables collaborative training among clients, with no need to share private data between them or to a centralized server. However, considering many edge clients do not have sufficient computing, memory, or communication capabilities, federated learning of large models still faces significant bottlenecks. To keep such weak but crucial clients in the loop, prior works either consider a heterogeneous-client setting where clients train models with different sizes; or offload training to the server. However, the heterogeneous-client setting requires some clients to train full model, which is not aligned with the resource-constrained setting; while the latter ones break privacy promises in FL when sharing intermediate representations or labels with the server. To overcome these limitations, in this work, we formulate a realistic, but much less explored, cross-device FL setting in which no client can train a full large model nor is willing to share any intermediate information with the remote server. Under such a formulation, we develop a principal sub-model (PriSM) training methodology to collaboratively train a full large model, while assigning each client a small sub-model that is a probabilistic low-rank approximation to the full server model. When creating sub-models, PriSM first performs a principal kernel analysis in the orthogonal kernel space to obtain importance of each kernel. Then, PriSM adopts a novel importance-aware sampling process to select a subset of kernels (i.e., a kernel with high importance is assigned with a higher sampling probability). This sampling process ensures each sub-model is still a low-rank approximation to the full model, while all sub-models together achieve nearly full coverage on the principal kernels. To further improve memory efficiency while still preserving accuracy, PriSM also exploits low-rank structure in intermediate representations and allows each sub-model to learn only a subset of them. Our evaluations on various datasets and models (CNNs, LSTMs, Transformers) under different resource-constrained settings demonstrate that PriSM yields an accuracy improvement of up to $10\\%$ compared to existing works. More importantly, PriSM does not incur significant accuracy degradation compared to full-model training (e.g., only $\\sim 2\\%$ accuracy drops for ResNet-18/CIFAR-10 when clients train only $0.2\\times$ sub-models)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Niu",
      "Saurav Prakash",
      "Souvik Kundu",
      "Sunwoo Lee",
      "Salman Avestimehr"
    ]
  },
  "https://openreview.net/forum?id=FObkvLwNSo": {
    "title": "Projected Randomized Smoothing for Certified Adversarial Robustness",
    "volume": "main",
    "abstract": "Randomized smoothing is the current state-of-the-art method for producing provably robust classifiers. While randomized smoothing typically yields robust $\\ell_2$-ball certificates, recent research has generalized provable robustness to different norm balls as well as anisotropic regions. This work considers a classifier architecture that first projects onto a low-dimensional approximation of the data manifold and then applies a standard classifier. By performing randomized smoothing in the low-dimensional projected space, we characterize the certified region of our smoothed composite classifier back in the high-dimensional input space and prove a tractable lower bound on its volume. We show experimentally on CIFAR-10 and SVHN that classifiers without the initial projection are vulnerable to perturbations that are normal to the data manifold and yet are captured by the certified regions of our method. We compare the volume of our certified regions against various baselines and show that our method improves on the state-of-the-art by many orders of magnitude",
    "checked": true,
    "id": "efb6333deaf551f642f6f76af896c0a6adf26cd5",
    "semantic_title": "projected randomized smoothing for certified adversarial robustness",
    "citation_count": 16,
    "authors": [
      "Samuel Pfrommer",
      "Brendon G. Anderson",
      "Somayeh Sojoudi"
    ]
  },
  "https://openreview.net/forum?id=4UXJhNSbwd": {
    "title": "Multi-Domain Long-Tailed Learning by Augmenting Disentangled Representations",
    "volume": "main",
    "abstract": "There is an inescapable long-tailed class-imbalance issue in many real-world classification problems. Current methods for addressing this problem only consider scenarios where all examples come from the same distribution. However, in many cases, there are multiple domains with distinct class imbalance. We study this multi-domain long-tailed learning problem and aim to produce a model that generalizes well across all classes and domains. Towards that goal, we introduce TALLY, a method that addresses this multi-domain long-tailed learning problem. Built upon a proposed selective balanced sampling strategy, TALLY achieves this by mixing the semantic representation of one example with the domain-associated nuisances of another, producing a new representation for use as data augmentation. To improve the disentanglement of semantic representations, TALLY further utilizes a domain-invariant class prototype that averages out domain-specific effects. We evaluate TALLY on several benchmarks and real-world datasets and find that it consistently outperforms other state-of-the-art methods in both subpopulation and domain shift",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Yang",
      "Huaxiu Yao",
      "Allan Zhou",
      "Chelsea Finn"
    ]
  },
  "https://openreview.net/forum?id=f36LaK7M0F": {
    "title": "CAE v2: Context Autoencoder with CLIP Latent Alignment",
    "volume": "main",
    "abstract": "Masked image modeling (MIM) learns visual representations by predicting the masked patches on a pre-defined target. Inspired by MVP(Wei et al., 2022b) that displays impressive gains with CLIP, in this work, we also employ the semantically rich CLIP latent as target and further tap its potential by introducing a new MIM pipeline, CAE v2, to learn a high-quality encoder and facilitate model convergence on the pre-training task. CAE v2 is an improved variant of CAE (Chen et al., 2023), applying the CLIP latent on two pretraining tasks, i.e., visible latent alignment and masked latent alignment. Visible latent alignment directly mimics the visible latent representations from the encoder to the corresponding CLIP latent, which is beneficial for facilitating model convergence and improving the representative ability of the encoder. Masked latent alignment predicts the representations of masked patches within the feature space of CLIP latent as standard MIM task does, effectively aligning the representations computed from the encoder and the regressor into the same domain. We pretrain CAE v2 on ImageNet-1K images and evaluate on various downstream vision tasks, including image classification, semantic segmentation, object detection and instance segmentation. Experiments show that our CAE v2 achieves competitive performance and even outperforms the CLIP vision encoder, demonstrating the effectiveness of our method. Code is available at https://github.com/Atten4Vis/CAE",
    "checked": true,
    "id": "6e966e5d3c15fbb162f19ba2e1cf448ff079c345",
    "semantic_title": "cae v2: context autoencoder with clip latent alignment",
    "citation_count": 7,
    "authors": [
      "Xinyu Zhang",
      "Jiahui Chen",
      "Junkun Yuan",
      "Qiang Chen",
      "Jian Wang",
      "Xiaodi Wang",
      "Shumin Han",
      "Xiaokang Chen",
      "Jimin Pi",
      "Kun Yao",
      "Junyu Han",
      "Errui Ding",
      "Jingdong Wang"
    ]
  },
  "https://openreview.net/forum?id=VgJhYu7FmQ": {
    "title": "Cross-validation for Geospatial Data: Estimating Generalization Performance in Geostatistical Problems",
    "volume": "main",
    "abstract": "Geostatistical learning problems are frequently characterized by spatial autocorrelation in the input features and/or the potential for covariate shift at test time. These realities violate the classical assumption of independent, identically distributed data, upon which most cross-validation algorithms rely in order to estimate the generalization performance of a model. In this paper, we present a theoretical criterion for unbiased cross-validation estimators in the geospatial setting. We also introduce a new cross-validation algorithm to evaluate models, inspired by the challenges of geospatial problems. We apply a framework for categorizing problems into different types of geospatial scenarios to help practitioners select an appropriate cross-validation strategy. Our empirical analyses compare cross-validation algorithms on both simulated and several real datasets to develop recommendations for a variety of geospatial settings. This paper aims to draw attention to some challenges that arise in model evaluation for geospatial problems and to provide guidance for users",
    "checked": true,
    "id": "4afd99f5d36a6687bf43724168a834d683c2ef67",
    "semantic_title": "cross-validation for geospatial data: estimating generalization performance in geostatistical problems",
    "citation_count": 3,
    "authors": [
      "Jing Wang",
      "Laurel Hopkins",
      "Tyler Hallman",
      "W. Douglas Robinson",
      "Rebecca Hutchinson"
    ]
  },
  "https://openreview.net/forum?id=LLKI5Lq2YN": {
    "title": "Adaptive Hyperparameter Selection for Differentially Private Gradient Descent",
    "volume": "main",
    "abstract": "We present an adaptive mechanism for hyperparameter selection in differentially private optimization that addresses the inherent trade-off between utility and privacy. The mechanism eliminates the often unstructured and time-consuming manual effort of selecting hyperparameters and avoids the additional privacy costs that hyperparameter selection otherwise incurs on top of that of the actual algorithm. We instantiate our mechanism for noisy gradient descent on non-convex, convex and strongly convex loss functions, respectively, to derive schedules for the noise variance and step size. These schedules account for the properties of the loss function and adapt to convergence metrics such as the gradient norm. When using these schedules, we show that noisy gradient descent converges at essentially the same rate as its noise-free counterpart. Numerical experiments show that the schedules consistently perform well across a range of datasets without manual tuning",
    "checked": true,
    "id": "883bdc74ba6df56bbee38f573170cce882900bd9",
    "semantic_title": "adaptive hyperparameter selection for differentially private gradient descent",
    "citation_count": 2,
    "authors": [
      "Dominik Fay",
      "Sindri MagnÃºsson",
      "Jens SjÃ¶lund",
      "Mikael Johansson"
    ]
  },
  "https://openreview.net/forum?id=Ub6XILEF9x": {
    "title": "Multiscale Causal Structure Learning",
    "volume": "main",
    "abstract": "Causal structure learning methods are vital for unveiling causal relationships embedded into observed data. However, the state of the art suffers a major limitation: it assumes that causal interactions occur only at the frequency at which data is observed. To address this limitation, this paper proposes a method that allows structural learning of linear causal relationships occurring at different time scales. Specifically, we explicitly take into account instantaneous and lagged inter-relations between multiple time series, represented at different scales, hinging on wavelet transform. We cast the problem as the learning of a multiscale causal graph having sparse structure and dagness constraints, enforcing causality through directed and acyclic topology. To solve the resulting (non-convex) formulation, we propose an algorithm termed MS-CASTLE, which exhibits consistent performance across different noise distributions and wavelet choices. We also propose a single-scale version of our algorithm, SS-CASTLE, which outperforms existing methods in computational efficiency, performance, and robustness on synthetic data. Finally, we apply the proposed approach to learn the multiscale causal structure of the risk of 15 global equity markets, during covid-19 pandemic, illustrating the importance of multiscale analysis to reveal useful interactions at different time resolutions. Financial investors can leverage our approach to manage risk within equity portfolios from a causal perspective, tailored to their investment horizon",
    "checked": true,
    "id": "6b370dc7e28fb8dcd551153101114d56870e40cb",
    "semantic_title": "multiscale causal structure learning",
    "citation_count": 7,
    "authors": [
      "Gabriele D'Acunto",
      "Paolo Di Lorenzo",
      "Sergio Barbarossa"
    ]
  },
  "https://openreview.net/forum?id=xiQXHvL1eN": {
    "title": "Dynamic Regret Analysis of Safe Distributed Online Optimization for Convex and Non-convex Problems",
    "volume": "main",
    "abstract": "This paper addresses safe distributed online optimization over an unknown set of linear safety constraints. A network of agents aims at jointly minimizing a global, time-varying function, which is only partially observable to each individual agent. Therefore, agents must engage in local communications to generate a safe sequence of actions competitive with the best minimizer sequence in hindsight, and the gap between the two sequences is quantified via dynamic regret. We propose distributed safe online gradient descent (D-Safe-OGD) with an exploration phase, where all agents estimate the constraint parameters collaboratively to build estimated feasible sets, ensuring the action selection safety during the optimization phase. We prove that for convex functions, D-Safe-OGD achieves a dynamic regret bound of $O(T^{2/3} \\sqrt{\\log T} + T^{1/3}C_T^*)$, where $C_T^*$ denotes the path-length of the best minimizer sequence. We further prove a dynamic regret bound of $O(T^{2/3}{\\color{black} \\sqrt{\\log T}} + T^{2/3}C_T^*)$ for certain non-convex problems, which establishes the first dynamic regret bound for a safe distributed algorithm in the non-convex setting",
    "checked": true,
    "id": "e8c5928fe05de0b5f0d76d0ef7b6043b1ff8c388",
    "semantic_title": "dynamic regret analysis of safe distributed online optimization for convex and non-convex problems",
    "citation_count": 6,
    "authors": [
      "Ting-Jui Chang",
      "Sapana Chaudhary",
      "Dileep Kalathil",
      "Shahin Shahrampour"
    ]
  },
  "https://openreview.net/forum?id=2tdhQMLg36": {
    "title": "Revisiting Image Classifier Training for Improved Certified Robust Defense against Adversarial Patches",
    "volume": "main",
    "abstract": "Certifiably robust defenses against adversarial patches for image classifiers ensure correct prediction against any changes to a constrained neighborhood of pixels. PatchCleanser, the state-of-the-art certified defense, uses a double-masking strategy for robust classification. The success of this strategy relies heavily on the model's invariance to image pixel masking. In this paper, we take a closer look at model training schemes to improve this invariance. Instead of using Random Cutout augmentations like PatchCleanser, we introduce the notion of worst-case masking, i.e., selecting masked images which maximize classification loss. However, finding worst-case masks requires an exhaustive search, which might be prohibitively expensive to do on-the-fly during training. To solve this problem, we propose a two-round greedy masking strategy (Greedy Cutout) which finds an approximate worst-case mask location with much less compute. We show that the models trained with our Greedy Cutout improves certified robust accuracy over Random Cutout in PatchCleanser across a range of datasets and architectures. Certified robust accuracy on ImageNet with a ViT-B16-224 model increases from 58.1% to 62.3% against a 3% square patch applied anywhere on the image",
    "checked": true,
    "id": "4e4f5f1a72d665ff28e7028f9a483dd1900522d4",
    "semantic_title": "revisiting image classifier training for improved certified robust defense against adversarial patches",
    "citation_count": 3,
    "authors": [
      "Aniruddha Saha",
      "Shuhua Yu",
      "Mohammad Sadegh Norouzzadeh",
      "Wan-Yi Lin",
      "Chaithanya Kumar Mummadi"
    ]
  },
  "https://openreview.net/forum?id=61TKzU9B96": {
    "title": "An Optical Control Environment for Benchmarking Reinforcement Learning Algorithms",
    "volume": "main",
    "abstract": "Deep reinforcement learning has the potential to address various scientific problems. In this paper, we implement an optics simulation environment for reinforcement learning based controllers. The environment captures the essence of nonconvexity, nonlinearity, and time-dependent noise inherent in optical systems, offering a more realistic setting. Subsequently, we provide the benchmark results of several reinforcement learning algorithms on the proposed simulation environment. The experimental findings demonstrate the superiority of off-policy reinforcement learning approaches over traditional control algorithms in navigating the intricacies of complex optical control environments",
    "checked": true,
    "id": "535722991c93f214446b1bc36421b259b6b76f6e",
    "semantic_title": "an optical control environment for benchmarking reinforcement learning algorithms",
    "citation_count": 1,
    "authors": [
      "ABULIKEMU ABUDUWEILI",
      "Changliu Liu"
    ]
  },
  "https://openreview.net/forum?id=0pn3KnbH5F": {
    "title": "Learning-to-defer for sequential medical decision-making under uncertainty",
    "volume": "main",
    "abstract": "Learning-to-defer is a framework to automatically defer decision-making to a human expert when ML-based decisions are deemed unreliable. Existing learning-to-defer frameworks are not designed for sequential settings. That is, they defer at every instance independently, based on immediate predictions, while ignoring the potential long-term impact of these interventions. As a result, existing frameworks are myopic. Further, they do not defer adaptively, which is crucial when human interventions are costly. In this work, we propose Sequential Learning-to-Defer (SLTD), a framework for learning-to-defer to a domain expert in sequential decision-making settings. Contrary to existing literature, we pose the problem of learning-to-defer as model-based reinforcement learning (RL) to i) account for long-term consequences of ML-based actions using RL and ii) adaptively defer based on the dynamics (model-based). Our proposed framework determines whether to defer (at each time step) by quantifying whether a deferral now will improve the value compared to delaying deferral to the next time step. To quantify the improvement, we account for potential future deferrals. As a result, we learn a pre-emptive deferral policy (i.e. a policy that defers early if using the ML-based policy could worsen long-term outcomes). Our deferral policy is adaptive to the non-stationarity in the dynamics. We demonstrate that adaptive deferral via SLTD provides an improved trade-off between long-term outcomes and deferral frequency on synthetic, semi-synthetic, and real-world data with non-stationary dynamics. Finally, we interpret the deferral decision by decomposing the propagated (long-term) uncertainty around the outcome, to justify the deferral decision",
    "checked": true,
    "id": "793f242c80ab54c43aa7da3b628ad6bbf48c041e",
    "semantic_title": "learning-to-defer for sequential medical decision-making under uncertainty",
    "citation_count": 7,
    "authors": [
      "Shalmali Joshi",
      "Sonali Parbhoo",
      "Finale Doshi-Velez"
    ]
  },
  "https://openreview.net/forum?id=JFaZ94tT8M": {
    "title": "Learning domain-specific causal discovery from time series",
    "volume": "main",
    "abstract": "Causal discovery (CD) from time-varying data is important in neuroscience, medicine, and machine learning. Techniques for CD encompass randomized experiments, which are generally unbiased but expensive, and algorithms such as Granger causality, conditional-independence-based, structural-equation-based, and score-based methods that are only accurate under strong assumptions made by human designers. However, as demonstrated in other areas of machine learning, human expertise is often not entirely accurate and tends to be outperformed in domains with abundant data. In this study, we examine whether we can enhance domain-specific causal discovery for time series using a data-driven approach. Our findings indicate that this procedure significantly outperforms human-designed, domain-agnostic causal discovery methods, such as Mutual Information, VAR-LiNGAM, and Granger Causality on the MOS 6502 microprocessor, the NetSim fMRI dataset, and the Dream3 gene dataset. We argue that, when feasible, the causality field should consider a supervised approach in which domain-specific CD procedures are learned from extensive datasets with known causal relationships, rather than being designed by human specialists. Our findings promise a new approach toward improving CD in neural and medical data and for the broader machine learning community",
    "checked": true,
    "id": "78f33165d0a9250fc76edf1a6e831c68657d0a69",
    "semantic_title": "learning domain-specific causal discovery from time series",
    "citation_count": 1,
    "authors": [
      "Xinyue Wang",
      "Konrad Kording"
    ]
  },
  "https://openreview.net/forum?id=ThJl4d5JRg": {
    "title": "Dynamic Subgoal-based Exploration via Bayesian Optimization",
    "volume": "main",
    "abstract": "Reinforcement learning in sparse-reward navigation environments with expensive and limited interactions is challenging and poses a need for effective exploration. Motivated by complex navigation tasks that require real-world training (when cheap simulators are not available), we consider an agent that faces an unknown distribution of environments and must decide on an exploration strategy. It may leverage a series of training environments to improve its policy before it is evaluated in a test environment drawn from the same environment distribution. Most existing approaches focus on fixed exploration strategies, while the few that view exploration as a meta-optimization problem tend to ignore the need for _cost-efficient_ exploration. We propose a cost-aware Bayesian optimization approach that efficiently searches over a class of dynamic subgoal-based exploration strategies. The algorithm adjusts a variety of levers --- the locations of the subgoals, the length of each episode, and the number of replications per trial --- in order to overcome the challenges of sparse rewards, expensive interactions, and noise. An experimental evaluation demonstrates that the new approach outperforms existing baselines across a number of problem domains. We also provide a theoretical foundation and prove that the method asymptotically identifies a near-optimal subgoal design",
    "checked": false,
    "id": "b8dc2c6f9ca7c0d55b0379d5067f67b3e834ee80",
    "semantic_title": "model-based causal bayesian optimization",
    "citation_count": 25,
    "authors": [
      "Yijia Wang",
      "Matthias Poloczek",
      "Daniel R. Jiang"
    ]
  },
  "https://openreview.net/forum?id=V7BvYJyTmM": {
    "title": "Gated Domain Units for Multi-source Domain Generalization",
    "volume": "main",
    "abstract": "The phenomenon of distribution shift (DS) occurs when a dataset at test time differs from the dataset at training time, which can significantly impair the performance of a machine learning model in practical settings due to a lack of knowledge about the data's distribution at test time. To address this problem, we postulate that real-world distributions are composed of latent Invariant Elementary Distributions (I.E.D) across different domains. This assumption implies an invariant structure in the solution space that enables knowledge transfer to unseen domains. To exploit this property for domain generalization, we introduce a modular neural network layer consisting of Gated Domain Units (GDUs) that learn a representation for each latent elementary distribution. During inference, a weighted ensemble of learning machines can be created by comparing new observations with the representations of each elementary distribution. Our flexible framework also accommodates scenarios where explicit domain information is not present. Extensive experiments on image, text, and graph data show consistent performance improvement on out-of-training target domains. These findings support the practicality of the I.E.D assumption and the effectiveness of GDUs for domain generalisation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simon FÃ¶ll",
      "Alina Dubatovka",
      "Eugen Ernst",
      "Siu Lun Chau",
      "Martin Maritsch",
      "Patrik Okanovic",
      "Gudrun Thaeter",
      "Joachim M. Buhmann",
      "Felix Wortmann",
      "Krikamol Muandet"
    ]
  },
  "https://openreview.net/forum?id=8L7Rh6FIXt": {
    "title": "IBIA: An Incremental Build-Infer-Approximate Framework for Approximate Inference of Partition Function",
    "volume": "main",
    "abstract": "Exact computation of the partition function is known to be intractable, necessitating approximate inference techniques. Existing methods for approximate inference are slow to converge for many benchmarks. The control of accuracy-complexity trade-off is also non-trivial in many of these methods. We propose a novel incremental build-infer-approximate (IBIA) framework for approximate inference that addresses these issues. In this framework, the probabilistic graphical model is converted into a sequence of clique tree forests (SCTF) with bounded clique sizes. We show that the SCTF can be used to efficiently compute the partition function. We propose two new algorithms which are used to construct the SCTF and prove the correctness of both. The first is an algorithm for incremental construction of CTFs that is guaranteed to give a valid CTF with bounded clique sizes and the second is an approximation algorithm that takes a calibrated CTF as input and yields a valid and calibrated CTF with reduced clique sizes as the output. We have evaluated our method using several benchmark sets from recent UAI competitions and our results show good accuracies with competitive runtimes",
    "checked": true,
    "id": "cd68083d1c3f37e85cdcbaa104c86d92edc43d2f",
    "semantic_title": "ibia: an incremental build-infer-approximate framework for approximate inference of partition function",
    "citation_count": 1,
    "authors": [
      "Shivani Bathla",
      "Vinita Vasudevan"
    ]
  },
  "https://openreview.net/forum?id=iHyhdpsnyi": {
    "title": "Revisiting Sparsity Hunting in Federated Learning: Why does Sparsity Consensus Matter?",
    "volume": "main",
    "abstract": "Edge devices can benefit remarkably from federated learning due to their distributed nature; however, their limited resource and computing power poses limitations in deployment. A possible solution to this problem is to utilize off-the-shelf sparse learning algorithms at the clients to meet their resource budget. However, such naive deployment in the clients causes significant accuracy degradation, especially for highly resource-constrained clients. In particular, our investigations reveal that the lack of consensus in the sparsity masks among the clients may potentially slow down the convergence of the global model and cause a substantial accuracy drop. With these observations, we present \\textit{federated lottery aware sparsity hunting} (FLASH), a unified sparse learning framework for training a sparse sub-model that maintains the performance under ultra-low parameter density while yielding proportional communication benefits. Moreover, given that different clients may have different resource budgets, we present \\textit{hetero-FLASH} where clients can take different density budgets based on their device resource limitations instead of supporting only one target parameter density. Experimental analysis on diverse models and datasets shows the superiority of FLASH in closing the gap with an unpruned baseline while yielding up to $\\mathord{\\sim}10.1\\%$ improved accuracy with $\\mathord{\\sim}10.26\\times$ fewer communication, compared to existing alternatives, at similar hyperparameter settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sara Babakniya",
      "Souvik Kundu",
      "Saurav Prakash",
      "Yue Niu",
      "Salman Avestimehr"
    ]
  },
  "https://openreview.net/forum?id=Y1eYplvxrE": {
    "title": "Relating graph auto-encoders to linear models",
    "volume": "main",
    "abstract": "Graph auto-encoders are widely used to construct graph representations in Euclidean vector spaces. However, it has already been pointed out empirically that linear models on many tasks can outperform graph auto-encoders. In our work, we prove that the solution space induced by graph auto-encoders is a subset of the solution space of a linear map. This demonstrates that linear embedding models have at least the representational power of graph auto-encoders based on graph convolutional networks. So why are we still using nonlinear graph auto-encoders? One reason could be that actively restricting the linear solution space might introduce an inductive bias that helps improve learning and generalization. While many researchers believe that the nonlinearity of the encoder is the critical ingredient towards this end, we instead identify the node features of the graph as a more powerful inductive bias. We give theoretical insights by introducing a corresponding bias in a linear model and analyzing the change in the solution space. Our experiments are aligned with other empirical work on this question and show that the linear encoder can outperform the nonlinear encoder when using feature information",
    "checked": true,
    "id": "1ea30fd85da438eb3f526307330380e0532c84b6",
    "semantic_title": "relating graph auto-encoders to linear models",
    "citation_count": 1,
    "authors": [
      "Solveig Klepper",
      "Ulrike von Luxburg"
    ]
  },
  "https://openreview.net/forum?id=zmBFzuT2DN": {
    "title": "Deep Operator Learning Lessens the Curse of Dimensionality for PDEs",
    "volume": "main",
    "abstract": "Deep neural networks (DNNs) have achieved remarkable success in numerous domains, and their application to PDE-related problems has been rapidly advancing. This paper provides an estimate for the generalization error of learning Lipschitz operators over Banach spaces using DNNs with applications to various PDE solution operators. The goal is to specify DNN width, depth, and the number of training samples needed to guarantee a certain testing error. Under mild assumptions on data distributions or operator structures, our analysis shows that deep operator learning can have a relaxed dependence on the discretization resolution of PDEs and, hence, lessen the curse of dimensionality in many PDE-related problems including elliptic equations, parabolic equations, and Burgers equations. Our results are also applied to give insights about discretization-invariant in operator learning",
    "checked": true,
    "id": "c39106c6d3ddcba512fe8f776a32c739e4e75375",
    "semantic_title": "deep operator learning lessens the curse of dimensionality for pdes",
    "citation_count": 14,
    "authors": [
      "Ke Chen",
      "Chunmei Wang",
      "Haizhao Yang"
    ]
  },
  "https://openreview.net/forum?id=3taIQG4C7H": {
    "title": "Label Noise-Robust Learning using a Confidence-Based Sieving Strategy",
    "volume": "main",
    "abstract": "In learning tasks with label noise, improving model robustness against overfitting is a pivotal challenge because the model eventually memorizes labels, including the noisy ones. Identifying the samples with noisy labels and preventing the model from learning them is a promising approach to address this challenge. When training with noisy labels, the per-class confidence scores of the model, represented by the class probabilities, can be reliable criteria for assessing whether the input label is the true label or the corrupted one. In this work, we exploit this observation and propose a novel discriminator metric called confidence error and a sieving strategy called CONFES to differentiate between the clean and noisy samples effectively. We provide theoretical guarantees on the probability of error for our proposed metric. Then, we experimentally illustrate the superior performance of our proposed approach compared to recent studies on various settings, such as synthetic and real-world label noise. Moreover, we show CONFES can be combined with other state-of-the-art approaches, such as Co-teaching and DivideMix to further improve model performance",
    "checked": true,
    "id": "ed32c5122d899da367270557cdb363fe5fab838b",
    "semantic_title": "label noise-robust learning using a confidence-based sieving strategy",
    "citation_count": 7,
    "authors": [
      "Reihaneh Torkzadehmahani",
      "Reza Nasirigerdeh",
      "Daniel Rueckert",
      "Georgios Kaissis"
    ]
  },
  "https://openreview.net/forum?id=igDOV2KBwM": {
    "title": "On Perfect Clustering for Gaussian Processes",
    "volume": "main",
    "abstract": "In this paper, we propose a data based transformation for infinite-dimensional Gaussian processes and derive its limit theorem. For a clustering problem using mixture models, an appropriate modification of this transformation asymptotically leads to perfect separation of the populations under rather general conditions, except the scenario in which differences between clusters depend only on the locations; in which case our procedure is useless. Theoretical properties related to label consistency are studied for the k-means clustering algorithm when used on this transformed data. Good empirical performance of the proposed methodology is demonstrated using simulated as well as benchmark data sets, when compared with some popular parametric and nonparametric methods for such functional data",
    "checked": true,
    "id": "74dde6d496a4a4ec9b8e38f30079c719ae49c962",
    "semantic_title": "on perfect clustering for gaussian processes",
    "citation_count": 2,
    "authors": [
      "Juan Cuesta-Albertos",
      "Subhajit Dutta"
    ]
  },
  "https://openreview.net/forum?id=WJt2Pc3qtI": {
    "title": "How Reliable is Your Regression Model's Uncertainty Under Real-World Distribution Shifts?",
    "volume": "main",
    "abstract": "Many important computer vision applications are naturally formulated as regression problems. Within medical imaging, accurate regression models have the potential to automate various tasks, helping to lower costs and improve patient outcomes. Such safety-critical deployment does however require reliable estimation of model uncertainty, also under the wide variety of distribution shifts that might be encountered in practice. Motivated by this, we set out to investigate the reliability of regression uncertainty estimation methods under various real-world distribution shifts. To that end, we propose an extensive benchmark of 8 image-based regression datasets with different types of challenging distribution shifts. We then employ our benchmark to evaluate many of the most common uncertainty estimation methods, as well as two state-of-the-art uncertainty scores from the task of out-of-distribution detection. We find that while methods are well calibrated when there is no distribution shift, they all become highly overconfident on many of the benchmark datasets. This uncovers important limitations of current uncertainty estimation methods, and the proposed benchmark therefore serves as a challenge to the research community. We hope that our benchmark will spur more work on how to develop truly reliable regression uncertainty estimation methods",
    "checked": true,
    "id": "f01dfb620ae80f8ad56fc1a6f4d84eb52200fbf0",
    "semantic_title": "how reliable is your regression model's uncertainty under real-world distribution shifts?",
    "citation_count": 12,
    "authors": [
      "Fredrik K. Gustafsson",
      "Martin Danelljan",
      "Thomas B. SchÃ¶n"
    ]
  },
  "https://openreview.net/forum?id=qcCE4mC2jI": {
    "title": "RIGNN: A Rationale Perspective for Semi-supervised Open-world Graph Classification",
    "volume": "main",
    "abstract": "Graph classification has gained growing attention in the graph machine learning community and a variety of semi-supervised methods have been developed to reduce the high cost of annotation. They usually combine graph neural networks (GNNs) and extensive semi-supervised techniques such as knowledge distillation. However, they adhere to the close-set assumption that unlabeled graphs all belong to known classes, limiting their applications in the real world. This paper goes further, investigating a practical problem of semi-supervised open-world graph classification where these unlabeled graph data could come from unseen classes. A novel approach named Rationale-Informed GNN (RIGNN) is proposed, which takes a rationale view to detect components containing the most information related to the label space and classify unlabeled graphs into a known class or an unseen class. In particular, RIGNN contains a relational detector and a feature extractor to produce effective rationale features, which maximize the mutual information with label information and exhibit sufficient disentanglement with non-rationale elements. Furthermore, we construct a graph-of-graph based on geometrical relationships, which gives instructions on enhancing rationale representations. In virtue of effective rationale representations, we can provide accurate and balanced predictions for unlabeled graphs. An extension is also made to accomplish effective open-set graph classification. We verify our proposed methods on four benchmark datasets in various settings and experimental results reveal the effectiveness of our proposed RIGNN compared with state-of-the-art methods",
    "checked": true,
    "id": "76b9d2931374d060ab55b220af51bffc10479432",
    "semantic_title": "rignn: a rationale perspective for semi-supervised open-world graph classification",
    "citation_count": 10,
    "authors": [
      "Xiao Luo",
      "Yusheng Zhao",
      "Zhengyang Mao",
      "Yifang Qin",
      "Wei Ju",
      "Ming Zhang",
      "Yizhou Sun"
    ]
  },
  "https://openreview.net/forum?id=JwGKVpRfVD": {
    "title": "SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration",
    "volume": "main",
    "abstract": "The ability to effectively reuse prior knowledge is a key requirement when building general and flexible Reinforcement Learning (RL) agents. Skill reuse is one of the most common approaches, but current methods have considerable limitations. For example, fine-tuning an existing policy frequently fails, as the policy can degrade rapidly early in training. In a similar vein, distillation of expert behavior can lead to poor results when given sub-optimal experts. We compare several common approaches for skill transfer on multiple domains including changes in task and system dynamics. We identify how existing methods fail and introduce an alternative approach to mitigate these problems. Our approach learns to sequence temporally-extended skills for exploration but learns the final policy directly from the raw experience. This conceptual split enables rapid adaptation and thus efficient data collection but without constraining the final solution. It significantly outperforms many classical methods across a suite of evaluation tasks and we use a broad set of ablations to highlight the importance of different components of our method",
    "checked": true,
    "id": "6a4c74430e9c9c165ef3321edf6b4e9bdc36dea9",
    "semantic_title": "skills: adaptive skill sequencing for efficient temporally-extended exploration",
    "citation_count": 6,
    "authors": [
      "Giulia Vezzani",
      "Dhruva Tirumala",
      "Markus Wulfmeier",
      "Dushyant Rao",
      "Abbas Abdolmaleki",
      "Ben Moran",
      "Tuomas Haarnoja",
      "Jan Humplik",
      "Roland Hafner",
      "Michael Neunert",
      "Claudio Fantacci",
      "Tim Hertweck",
      "Thomas Lampe",
      "Fereshteh Sadeghi",
      "Nicolas Heess",
      "Martin Riedmiller"
    ]
  },
  "https://openreview.net/forum?id=cJgHzw8Qhq": {
    "title": "Estimating Differential Equations from Temporal Point Processes",
    "volume": "main",
    "abstract": "Ordinary differential equations (ODEs) allow interpretation of phenomena in various scientific fields. They have mostly been applied to numerical data observed at regular intervals, but not to irregularly observed discrete events, also known as point processes. In this study, we introduce an ODE modeling of such events by combining ODEs with log-Gaussian Cox processes (MÃ¸ller et al., 1998). In the experiments with different types of ODEs regarding infectious disease, predator-prey interaction, and competition among participants, our method outperformed existing baseline methods assuming regularly observed continuous data with respect to the accuracy of recovering the latent parameters of ODEs. Through both synthetic and actual examples, we also showed the ability of our method to extrapolate, model latent events that cannot be observed, and offer interpretability of phenomena from the viewpoint of the estimated parameters of ODE",
    "checked": true,
    "id": "65ce2d8059bbff71df6bf402d7b8df9fc8ffa9fa",
    "semantic_title": "estimating differential equations from temporal point processes",
    "citation_count": 0,
    "authors": [
      "Shuichi Miyazawa",
      "Daichi Mochihashi"
    ]
  },
  "https://openreview.net/forum?id=XuOE99cmST": {
    "title": "Turning a Curse into a Blessing: Enabling In-Distribution-Data-Free Backdoor Removal via Stabilized Model Inversion",
    "volume": "main",
    "abstract": "The effectiveness of many existing techniques for removing backdoors from machine learning models relies on access to clean in-distribution data. However, given that these models are often trained on proprietary datasets, it may not be practical to assume that in-distribution samples will always be available. On the other hand, model inversion techniques, which are typically viewed as privacy threats, can reconstruct realistic training samples from a given model, potentially eliminating the need for in-distribution data. To date, the only prior attempt to integrate backdoor removal and model inversion involves a simple combination that produced very limited results. This work represents a first step toward a more thorough understanding of how model inversion techniques could be leveraged for effective backdoor removal. Specifically, we seek to answer several key questions: What properties must reconstructed samples possess to enable successful defense? Is perceptual similarity to clean samples enough, or are additional characteristics necessary? Is it possible for reconstructed samples to contain backdoor triggers? We demonstrate that relying solely on perceptual similarity is insufficient for effective defenses. The stability of model predictions in response to input and parameter perturbations also plays a critical role. To address this, we propose a new bi-level optimization based framework for model inversion that promotes stability in addition to visual quality. Interestingly, we also find that reconstructed samples from a pre-trained generator's latent space do not contain backdoors, even when signals from a backdoored model are utilized for reconstruction. We provide a theoretical analysis to explain this observation. Our evaluation shows that our stabilized model inversion technique achieves state-of-the-art backdoor removal performance without requiring access to clean in-distribution data. Furthermore, its performance is on par with or even better than using the same amount of clean samples",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Si Chen",
      "Yi Zeng",
      "Won Park",
      "Jiachen T. Wang",
      "Xun Chen",
      "Lingjuan Lyu",
      "Zhuoqing Mao",
      "Ruoxi Jia"
    ]
  },
  "https://openreview.net/forum?id=8koy8QuTZD": {
    "title": "Momentum Tracking: Momentum Acceleration for Decentralized Deep Learning on Heterogeneous Data",
    "volume": "main",
    "abstract": "SGD with momentum is one of the key components for improving the performance of neural networks. For decentralized learning, a straightforward approach using momentum is Distributed SGD (DSGD) with momentum (DSGDm). However, DSGDm performs worse than DSGD when the data distributions are statistically heterogeneous. Recently, several studies have addressed this issue and proposed methods with momentum that are more robust to data heterogeneity than DSGDm, although their convergence rates remain dependent on data heterogeneity and deteriorate when the data distributions are heterogeneous. In this study, we propose Momentum Tracking, which is a method with momentum whose convergence rate is proven to be independent of data heterogeneity. More specifically, we analyze the convergence rate of Momentum Tracking in the setting where the objective function is non-convex and the stochastic gradient is used. Then, we identify that it is independent of data heterogeneity for any momentum coefficient $\\beta \\in [0, 1)$. Through experiments, we demonstrate that Momentum Tracking is more robust to data heterogeneity than the existing decentralized learning methods with momentum and can consistently outperform these existing methods when the data distributions are heterogeneous",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuki Takezawa",
      "Han Bao",
      "Kenta Niwa",
      "Ryoma Sato",
      "Makoto Yamada"
    ]
  },
  "https://openreview.net/forum?id=KQ5jI19kF3": {
    "title": "Optimistic Optimization of Gaussian Process Samples",
    "volume": "main",
    "abstract": "Bayesian optimization is a popular formalism for global optimization, but its computational costs limit it to expensive-to-evaluate functions. A competing, computationally more effi- cient, global optimization framework is optimistic optimization, which exploits prior knowl- edge about the geometry of the search space in form of a dissimilarity function. We investi- gate to which degree the conceptual advantages of Bayesian Optimization can be combined with the computational efficiency of optimistic optimization. By mapping the kernel to a dissimilarity, we obtain an optimistic optimization algorithm for the Bayesian Optimization setting with a run-time of up to $O(N log N )$. As a high-level take-away we find that, when using stationary kernels on objectives of low evaluation cost, optimistic optimization can be preferable over Bayesian optimization, while for strongly coupled and parametric models, Bayesian optimization can perform much better, even at low evaluation cost. As a concep- tual takeaway, our results demonstrate that balancing exploration and exploitation under Gaussian process assumptions does not require computing a posterior",
    "checked": true,
    "id": "5d3af1d038b1057c319038e896a6fd96c40a764a",
    "semantic_title": "optimistic optimization of gaussian process samples",
    "citation_count": 0,
    "authors": [
      "Julia Grosse",
      "Cheng Zhang",
      "Philipp Hennig"
    ]
  },
  "https://openreview.net/forum?id=xoLyps2qWc": {
    "title": "Linearized Relative Positional Encoding",
    "volume": "main",
    "abstract": "Relative positional encoding is widely used in vanilla and linear transformers to represent positional information. However, existing encoding methods of a vanilla transformer are not always directly applicable to a linear transformer, because the latter requires a decomposition of the query and key representations into separate kernel functions. Nevertheless, principles for designing encoding methods suitable for linear transformers remain understudied. In this work, we put together a variety of existing linear relative positional encoding approaches under a canonical form and further propose a family of linear relative positional encoding algorithms via unitary transformation. Our formulation leads to a principled framework that can be used to develop new relative positional encoding methods that preserve linear space-time complexity. Equipped with different models, the proposed linearized relative positional encoding (LRPE) family derives effective encoding for various applications. Experiments show that compared with existing methods, LRPE achieves state-of-the-art performance in language modeling, text classification, and image classification. Meanwhile, it emphasizes a general paradigm for designing broadly more relative positional encoding methods that are applicable to linear transformers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Qin",
      "Weixuan Sun",
      "Kaiyue Lu",
      "Hui Deng",
      "Dongxu Li",
      "Xiaodong Han",
      "Yuchao Dai",
      "Lingpeng Kong",
      "Yiran Zhong"
    ]
  },
  "https://openreview.net/forum?id=oud7Ny0KQy": {
    "title": "RIFLE: Imputation and Robust Inference from Low Order Marginals",
    "volume": "main",
    "abstract": "The ubiquity of missing values in real-world datasets poses a challenge for statistical inference and can prevent similar datasets from being analyzed in the same study, precluding many existing datasets from being used for new analyses. While an extensive collection of packages and algorithms have been developed for data imputation, the overwhelming majority perform poorly if there are many missing values and low sample sizes, which are unfortunately common characteristics in empirical data. Such low-accuracy estimations adversely affect the performance of downstream statistical models. We develop a statistical inference framework for predicting the target variable in the presence of missing data without imputation. Our framework, RIFLE (Robust InFerence via Low-order moment Estimations), estimates low-order moments of the underlying data distribution with corresponding confidence intervals to learn a distributionally robust model. We specialize our framework to linear regression and normal discriminant analysis, and we provide convergence and performance guarantees. This framework can also be adapted to impute missing data. We compare RIFLE with state-of-the-art approaches (including MICE, Amelia, MissForest, KNN-imputer, MIDA, and Mean Imputer) in numerical experiments. Our experiments demonstrate that RIFLE outperforms other benchmark algorithms when the percentage of missing values is high and/or when the number of data points is relatively small. RIFLE is publicly available",
    "checked": true,
    "id": "d0c854843e362856b46ace44c484310ab33616e2",
    "semantic_title": "rifle: imputation and robust inference from low order marginals",
    "citation_count": 5,
    "authors": [
      "Sina Baharlouei",
      "Sze-Chuan Suen",
      "Meisam Razaviyayn"
    ]
  },
  "https://openreview.net/forum?id=zkRCp4RmAF": {
    "title": "Offline Reinforcement Learning with Mixture of Deterministic Policies",
    "volume": "main",
    "abstract": "Offline reinforcement learning (RL) has recently attracted considerable attention as an approach for utilizing past experiences to learn a policy. Recent studies have reported the challenges of offline RL, such as estimating the values of actions that are outside the data distribution. To mitigate offline RL issues, we propose an algorithm that leverages a mixture of deterministic policies. When the data distribution is multimodal, fitting a policy modeled with a unimodal distribution, such as Gaussian distribution, may lead to interpolation between separate modes, thereby resulting in the value estimation of actions that are outside the data distribution. In our framework, the state-action space is divided by learning discrete latent variables, and the sub-policies corresponding to each region are trained. The proposed algorithm was derived by considering the variational lower bound of the offline RL objective function. We show empirically that the use of the proposed mixture policy can reduce the accumulation of the critic loss in offline RL, which was reported in previous studies. Experimental results also indicate that using a mixture of deterministic policies in offline RL improves the performance with the D4RL benchmarking datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Takayuki Osa",
      "Akinobu Hayashi",
      "Pranav Deo",
      "Naoki Morihira",
      "Takahide Yoshiike"
    ]
  },
  "https://openreview.net/forum?id=lvevdX6bxm": {
    "title": "Quantization Robust Federated Learning for Efficient Inference on Heterogeneous Devices",
    "volume": "main",
    "abstract": "Federated Learning (FL) is a machine learning paradigm to distributively learn machine learning models from decentralized data that remains on-device. Despite the success of standard Federated optimization methods, such as Federated Averaging (FedAvg) in FL, the energy demands and hardware induced constraints for on-device learning have not been considered sufficiently in the literature. Specifically, an essential demand for on-device learning is to enable trained models to be quantized to various bit-widths based on the energy needs and heterogeneous hardware designs across the federation. In this work, we introduce multiple variants of federated averaging algorithm that train neural networks robust to quantization. Such networks can be quantized to various bit-widths with only limited reduction in full precision model accuracy. We perform extensive experiments on standard FL benchmarks to evaluate our proposed FedAvg variants for quantization robustness and provide a convergence analysis for our Quantization-Aware variants in FL. Our results demonstrate that integrating quantization robustness results in FL models that are significantly more robust to different bit-widths during quantized on-device inference",
    "checked": true,
    "id": "b253022a77ebf30d4a5964d68087d9ff9860b4d4",
    "semantic_title": "quantization robust federated learning for efficient inference on heterogeneous devices",
    "citation_count": 16,
    "authors": [
      "Kartik Gupta",
      "Marios Fournarakis",
      "Matthias Reisser",
      "Christos Louizos",
      "Markus Nagel"
    ]
  },
  "https://openreview.net/forum?id=wRepWp1KC7": {
    "title": "Fair and Useful Cohort Selection",
    "volume": "main",
    "abstract": "A challenge in fair algorithm design is that, while there are compelling notions of individual fairness, these notions typically do not satisfy desirable composition properties, and downstream applications based on fair classifiers might not preserve fairness. To study fairness under composition, Dwork & Ilvento (2019) introduced an archetypal problem called fair-cohort-selection problem, where a single fair classifier is composed with itself to select a group of candidates of a given size, and proposed a solution to this problem. In this work we design algorithms for selecting cohorts that not only preserve fairness, but also maximize the utility of the selected cohort under two notions of utility that we introduce and motivate. We give optimal (or approximately optimal) polynomial-time algorithms for this problem in both an offline setting, and an online setting where candidates arrive one at a time and are classified as they arrive",
    "checked": false,
    "id": "a46b28076aee58e200cb7421b6f8af9991e293a5",
    "semantic_title": "enhancing students' engagement and learning through peer assessment in group projects",
    "citation_count": 2,
    "authors": [
      "Konstantina Bairaktari",
      "Paul Tsela Langton",
      "Huy Nguyen",
      "Niklas Smedemark-Margulies",
      "Jonathan Ullman"
    ]
  },
  "https://openreview.net/forum?id=vgXnEyeWVY": {
    "title": "Walking Out of the Weisfeiler Leman Hierarchy: Graph Learning Beyond Message Passing",
    "volume": "main",
    "abstract": "We propose CRaWl, a novel neural network architecture for graph learning. Like graph neural networks, CRaWl layers update node features on a graph and thus can freely be combined or interleaved with GNN layers. Yet CRaWl operates fundamentally different from message passing graph neural networks. CRaWl layers extract and aggregate information on subgraphs appearing along random walks through a graph using 1D Convolutions. Thereby it detects long range interactions and computes non-local features. As the theoretical basis for our approach, we prove a theorem stating that the expressiveness of CRaWl is incomparable with that of the Weisfeiler Leman algorithm and hence with graph neural networks. That is, there are functions expressible by CRaWl, but not by GNNs and vice versa. This result extends to higher levels of the Weisfeiler Leman hierarchy and thus to higher-order GNNs. Empirically, we show that CRaWl matches state-of-the-art GNN architectures across a multitude of benchmark datasets for classification and regression on graphs",
    "checked": true,
    "id": "ceb26d1f1a888a58190f8d2d33b2c2d0539a1523",
    "semantic_title": "walking out of the weisfeiler leman hierarchy: graph learning beyond message passing",
    "citation_count": 33,
    "authors": [
      "Jan TÃ¶nshoff",
      "Martin Ritzert",
      "Hinrikus Wolf",
      "Martin Grohe"
    ]
  },
  "https://openreview.net/forum?id=xWrtiJwJj5": {
    "title": "Global Contrastive Learning for Long-Tailed Classification",
    "volume": "main",
    "abstract": "We consider the long-tailed classification problem in which a few classes in the training data dominate the majority of the other classes. For concreteness, we focus on the visual domain in this paper. Most current methods employ contrastive learning to learn a representation for long-tailed data. In this paper, first, we investigate $k$-positive sampling, a popular baseline method widely used to build contrastive learning models for imbalanced data. Previous works show that $k$-positive learning, which only chooses $k$ positive samples (instead of all positive images) for each query image, suffers from inferior performance in long-tailed data. In this work, we further point out that k-positive learning limits the learning capability of both head and tail classes. Based on this perspective, we propose a novel contrastive learning framework that improves the limitation in k-positive learning by enlarging its positive selection space, so it can help the model learn more semantic discrimination features. Second, we analyze how the temperature (the hyperparameter used for tuning a concentration of samples on feature space) affects the gradients of each class in long-tailed learning, and propose a new method that can mitigate inadequate gradients between classes, which can help model learning easier. We name this framework as CoGloAT. Finally, we go on to introduce a new prototype learning framework namely ProCo based on coreset selection, which creates a global prototype for each cluster while keeping the computation cost within a reasonable time and show that combining CoGloAT with ProCo can further enhance the model learning ability on long-tailed data",
    "checked": true,
    "id": "684514104a254f08e41826edf5788c09a3004dff",
    "semantic_title": "global contrastive learning for long-tailed classification",
    "citation_count": 0,
    "authors": [
      "Thong Bach",
      "Anh Tong",
      "Truong Son Hy",
      "Vu Nguyen",
      "Thanh Nguyen-Tang"
    ]
  },
  "https://openreview.net/forum?id=KpElM2S9pw": {
    "title": "Approximating Naive Bayes on Unlabelled Categorical Data",
    "volume": "main",
    "abstract": "We address the question of binary classification when no labels are available and the input features are categorical. The lack of labels means supervised approaches can't be used, and the lack of a natural distance measure means that most unsupervised methods do poorly. For such problems, where the alternatives might be a) do nothing or b) heuristic rules-based approaches, we offer a third alternative: a classifier that approximates Naive Bayes. Our primary scenarios are those that involve distinguishing scripted, or bot, web traffic from that of legitimate users. Our main assumption is the existence of some attribute $x_*$ more prevalent in the benign than the scripted traffic; i.e., $P(x_*|\\overline{\\mbox{bot}}) = K \\cdot P(x_*|\\mbox{bot}),$ for $K>1.$ We show that any such disparity yields a lower bound on $P(\\mbox{bot}|x_{j})$ even when we have no prior estimates of $P(x_*|\\overline{\\mbox{bot}}),$ $P(x_*|\\mbox{bot})$ or $K$ (except that $K>1$). We show that when at least one bin of at least one feature receives no attack traffic then we under-estimate the actual conditional probability by a factor of $1-1/K.$ Thus, any attribute with a large disparity between prevalence in benign and abuse traffic (i.e., $K$ is large), allows good approximation of the Naive Bayes classifier without the benefit of labels. The approach is particularly suited to problems where $K$ is high and thus the approximation is very accurate. Example problems (and relevant attributes) might be: password-guessing, if login attempts from legitimate users succeed at a much higher rate than those from password-guessing attackers; Credit Card Verification Value (CVV) guessing, if an attacker exhaustively tries all possible 3 or 4-digit values and fails at a higher rate than legitimate users; account registration, if legitimate users use email addresses from services that do not allow fee anonymous accounts (e.g., {\\tt .edu}) at a much higher rate than attackers; click-fraud if legitimate users visit pages and services that contain no ads at a higher rate than click-fraud bots",
    "checked": true,
    "id": "2efeffd9aa3d78e64c7f9959646b967ed4edf6eb",
    "semantic_title": "approximating naive bayes on unlabelled categorical data",
    "citation_count": 0,
    "authors": [
      "Cormac Herley"
    ]
  },
  "https://openreview.net/forum?id=uaHyXxyp2r": {
    "title": "Weight-balancing fixes and flows for deep learning",
    "volume": "main",
    "abstract": "Feedforward neural networks with homogeneous activation functions possess an internal symmetry: the functions they compute do not change when the incoming and outgoing weights at any hidden unit are rescaled by reciprocal positive values. This paper makes two contributions to our understanding of these networks. The first is to describe a simple procedure, or {\\it fix}, for balancing the weights in these networks: this procedure computes multiplicative rescaling factors---one at each hidden unit---that rebalance the weights of these networks without changing the end-to-end functions that they compute. Specifically, given an initial network with arbitrary weights, the procedure determines the functionally equivalent network whose weight matrix is of minimal $\\ell_{p,q}$-norm; the weights at each hidden unit are said to be balanced when this norm is stationary with respect to rescaling transformations. The optimal rescaling factors are computed in an iterative fashion via simple multiplicative updates, and the updates are notable in that (a) they do not require the tuning of learning rates, (b) they operate in parallel on the rescaling factors at all hidden units, and (c) they converge monotonically to a global minimizer of the $\\ell_{p,q}$-norm. The paper's second contribution is to analyze the optimization landscape for learning in these networks. We suppose that the network's loss function consists of two terms---one that is invariant to rescaling transformations, measuring predictive accuracy, and another (a regularizer) that breaks this invariance, penalizing large weights. We show how to derive a weight-balancing {\\it flow} such that the regularizer remains minimal with respect to rescaling transformations as the weights descend in the loss function. These dynamics reduce to an ordinary gradient flow for $\\ell_2$-norm regularization, but not otherwise. In this way our analysis suggests a canonical pairing of alternative flows and regularizers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lawrence K. Saul"
    ]
  },
  "https://openreview.net/forum?id=lOegPKSu04": {
    "title": "$k$-Mixup Regularization for Deep Learning via Optimal Transport",
    "volume": "main",
    "abstract": "Mixup is a popular regularization technique for training deep neural networks that improves generalization and increases robustness to certain distribution shifts. It perturbs input training data in the direction of other randomly-chosen instances in the training set. To better leverage the structure of the data, we extend mixup in a simple, broadly applicable way to $k$-mixup, which perturbs $k$-batches of training points in the direction of other $k$-batches. The perturbation is done with displacement interpolation, i.e. interpolation under the Wasserstein metric. We demonstrate theoretically and in simulations that $k$-mixup preserves cluster and manifold structures, and we extend theory studying the efficacy of standard mixup to the $k$-mixup case. Our empirical results show that training with $k$-mixup further improves generalization and robustness across several network architectures and benchmark datasets of differing modalities. For the wide variety of real datasets considered, the performance gains of $k$-mixup over standard mixup are similar to or larger than the gains of mixup itself over standard ERM after hyperparameter optimization. In several instances, in fact, $k$-mixup achieves gains in settings where standard mixup has negligible to zero improvement over ERM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kristjan Greenewald",
      "Anming Gu",
      "Mikhail Yurochkin",
      "Justin Solomon",
      "Edward Chien"
    ]
  },
  "https://openreview.net/forum?id=0Xo9giEZWf": {
    "title": "HypUC: Hyperfine Uncertainty Calibration with Gradient- boosted Corrections for Reliable Regression on Imbalanced Electrocardiograms",
    "volume": "main",
    "abstract": "The automated analysis of medical time series, such as the electrocardiogram (ECG), electroencephalogram (EEG), pulse oximetry, etc, has the potential to serve as a valuable tool for diagnostic decisions, allowing for remote monitoring of patients and more efficient use of expensive and time-consuming medical procedures. Deep neural networks (DNNs) have been demonstrated to process such signals effectively. However, previous research has primarily focused on classifying medical time series rather than attempting to regress the continuous-valued physiological parameters central to diagnosis. One significant challenge in this regard is the imbalanced nature of the dataset, as a low prevalence of abnormal conditions can lead to heavily skewed data that results in inaccurate predictions and a lack of certainty in such predictions when deployed. To address these challenges, we propose HypUC, a framework for imbalanced probabilistic regression in medical time series, making several contributions. (i) We introduce a simple kernel density-based technique to tackle the imbalanced regression problem with medical time series. (ii) Moreover, we employ a probabilistic regression framework that allows uncertainty estimation for the predicted continuous values. (iii) We also present a new approach to calibrate the predicted uncertainty further. (iv) Finally, we demonstrate a technique to use calibrated uncertainty estimates to improve the predicted continuous value and show the efficacy of the calibrated uncertainty estimates to flag unreliable predictions. HypUC is evaluated on a large, diverse, real-world dataset of ECGs collected from millions of patients, outperforming several conventional baselines on various diagnostic tasks, suggesting potential use-case for the reliable clinical deployment of deep learning models and a prospective clinical trial. Consequently, a hyperkalemia diagnosis algorithm based on HypUC is going to be the subject of a real-world clinical prospective study",
    "checked": false,
    "id": "e2b4d10d0b8de66ba072b730b44fc3eb19081311",
    "semantic_title": "hypuc: hyperfine uncertainty calibration with gradient-boosted corrections for reliable regression on imbalanced electrocardiograms",
    "citation_count": 3,
    "authors": [
      "Uddeshya Upadhyay",
      "Sairam Bade",
      "Arjun Puranik",
      "Shahir Asfahan",
      "Melwin Babu",
      "Francisco Lopez-Jimenez",
      "Samuel Asirvatham",
      "Ashim Prasad",
      "Ajit Rajasekharan",
      "Samir Awasthi",
      "Rakesh Barve"
    ]
  },
  "https://openreview.net/forum?id=wbpxTuXgm0": {
    "title": "TSMixer: An All-MLP Architecture for Time Series Forecast-ing",
    "volume": "main",
    "abstract": "Real-world time-series datasets are often multivariate with complex dynamics. To capture this complexity, high capacity architectures like recurrent- or attention-based sequential deep learning models have become popular. However, recent work demonstrates that simple univariate linear models can outperform such deep learning models on several commonly used academic benchmarks. Extending them, in this paper, we investigate the capabilities of linear models for time-series forecasting and present Time-Series Mixer (TSMixer), a novel architecture designed by stacking multi-layer perceptrons (MLPs). TSMixer is based on mixing operations along both the time and feature dimensions to extract information efficiently. On popular academic benchmarks, the simple-to-implement TSMixer is comparable to specialized state-of-the-art models that leverage the inductive biases of specific benchmarks. On the challenging and large scale M5 benchmark, a real-world retail dataset, TSMixer demonstrates superior performance compared to the state-of-the-art alternatives. Our results underline the importance of efficiently utilizing cross-variate and auxiliary information for improving the performance of time series forecasting. We present various analyses to shed light into the capabilities of TSMixer. The design paradigms utilized in TSMixer are expected to open new horizons for deep learning-based time series forecasting",
    "checked": false,
    "id": "59694c8dce4f13db2f486eb8102459a3f7c23da6",
    "semantic_title": "tsmixer: an all-mlp architecture for time series forecasting",
    "citation_count": 190,
    "authors": [
      "Si-An Chen",
      "Chun-Liang Li",
      "Sercan O Arik",
      "Nathanael Christian Yoder",
      "Tomas Pfister"
    ]
  },
  "https://openreview.net/forum?id=ScrEUZLxPr": {
    "title": "Revisiting Hidden Representations in Transfer Learning for Medical Imaging",
    "volume": "main",
    "abstract": "While a key component to the success of deep learning is the availability of massive amounts of training data, medical image datasets are often limited in diversity and size. Transfer learning has the potential to bridge the gap between related yet different domains. For medical applications, however, it remains unclear whether it is more beneficial to pre-train on natural or medical images. We aim to shed light on this problem by comparing initialization on ImageNet and RadImageNet on seven medical classification tasks. Our work includes a replication study, which yields results contrary to previously published findings. In our experiments, ResNet50 models pre-trained on ImageNet tend to outperform those trained on RadImageNet. To gain further insights, we investigate the learned representations using Canonical Correlation Analysis (CCA) and compare the predictions of the different models. Our results indicate that, contrary to intuition, ImageNet and RadImageNet may converge to distinct intermediate representations, which appear to diverge further during fine-tuning. Despite these distinct representations, the predictions of the models remain similar. Our findings show that the similarity between networks before and after fine-tuning does not correlate with performance gains, suggesting that the advantages of transfer learning might not solely originate from the reuse of features in the early layers of a convolutional neural network",
    "checked": true,
    "id": "f10a7d49d67d292e03e8f8185310147b4e6fdebc",
    "semantic_title": "revisiting hidden representations in transfer learning for medical imaging",
    "citation_count": 1,
    "authors": [
      "Dovile Juodelyte",
      "Amelia JimÃ©nez-SÃ¡nchez",
      "Veronika Cheplygina"
    ]
  },
  "https://openreview.net/forum?id=VrvGHDSzZ7": {
    "title": "The Geometry of Mixability",
    "volume": "main",
    "abstract": "Mixable loss functions are of fundamental importance in the context of prediction with expert advice in the online setting since they characterize fast learning rates. By re-interpreting properness from the point of view of differential geometry, we provide a simple geometric characterization of mixability for the binary and multi-class cases: a proper loss function $\\ell$ is $\\eta$-mixable if and only if the superprediction set $\\textrm{spr}(\\eta \\ell)$ of the scaled loss function $\\eta \\ell$ slides freely inside the superprediction set $\\textrm{spr}(\\ell_{\\log})$ of the log loss $\\ell_{\\log}$, under fairly general assumptions on the differentiability of $\\ell$. Our approach provides a way to treat some concepts concerning loss functions (like properness) in a ''coordinate-free'' manner and reconciles previous results obtained for mixable loss functions for the binary and the multi-class cases",
    "checked": true,
    "id": "5c13e876605fa2e46518ac5ebbd5f2d44d9b425e",
    "semantic_title": "the geometry of mixability",
    "citation_count": 3,
    "authors": [
      "Armando J Cabrera Pacheco",
      "Robert Williamson"
    ]
  },
  "https://openreview.net/forum?id=hjDYJUn9l1": {
    "title": "Evaluating Human-Language Model Interaction",
    "volume": "main",
    "abstract": "Many real-world applications of language models (LMs), such as writing assistance and code autocomplete, involve human-LM interaction. However, most benchmarks are non-interactive in that a model produces output without human involvement. To evaluate human-LM interaction, we develop a new framework, Human-AI Language-based Interaction Evaluation (HALIE), that defines the components of interactive systems and dimensions to consider when designing evaluation metrics. Compared to standard, non-interactive evaluation, HALIE captures (i) the interactive process, not only the final output; (ii) the first-person subjective experience, not just a third-party assessment; and (iii) notions of preference beyond quality (e.g., enjoyment and ownership). We then design five tasks to cover different forms of interaction: social dialogue, question answering, crossword puzzles, summarization, and metaphor generation. With four state-of-the-art LMs (three variants of OpenAI's GPT-3 and AI21 Labs' Jurassic-1), we find that better non-interactive performance does not always translate to better human-LM interaction. In particular, we highlight three cases where the results from non-interactive and interactive metrics diverge and underscore the importance of human-LM interaction for LM evaluation",
    "checked": true,
    "id": "a640cdafc10181517b7694ab589db515595b3490",
    "semantic_title": "evaluating human-language model interaction",
    "citation_count": 102,
    "authors": [
      "Mina Lee",
      "Megha Srivastava",
      "Amelia Hardy",
      "John Thickstun",
      "Esin Durmus",
      "Ashwin Paranjape",
      "Ines Gerard-Ursin",
      "Xiang Lisa Li",
      "Faisal Ladhak",
      "Frieda Rong",
      "Rose E Wang",
      "Minae Kwon",
      "Joon Sung Park",
      "Hancheng Cao",
      "Tony Lee",
      "Rishi Bommasani",
      "Michael S. Bernstein",
      "Percy Liang"
    ]
  },
  "https://openreview.net/forum?id=2uMnAwWnRy": {
    "title": "Benchmarking Continuous Time Models for Predicting Multiple Sclerosis Progression",
    "volume": "main",
    "abstract": "Multiple sclerosis is a disease that affects the brain and spinal cord, it can lead to severe disability and has no known cure. The majority of prior work in machine learning for multiple sclerosis has been centered around using Magnetic Resonance Imaging scans or laboratory tests; these modalities are both expensive to acquire and can be unreliable. In a recent paper it was shown that disease progression can be predicted effectively using performance outcome measures and demographic data. In our work we build on this to investigate the modeling side, using continuous time models to predict progression. We benchmark four continuous time models using a publicly available multiple sclerosis dataset. We find that the best continuous model is often able to outperform the best benchmarked discrete time model. We also carry out an extensive ablation to discover the sources of performance gains, we find that standardizing existing features leads to a larger performance increase than interpolating missing features",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Luke Ian Norcliffe",
      "Lev Proleev",
      "Diana Mincu",
      "F Lee Hartsell",
      "Katherine A Heller",
      "Subhrajit Roy"
    ]
  },
  "https://openreview.net/forum?id=ZPpQk7FJXF": {
    "title": "Differentially Private Diffusion Models",
    "volume": "main",
    "abstract": "While modern machine learning models rely on increasingly large training datasets, data is often limited in privacy-sensitive domains. Generative models trained with differential privacy (DP) on sensitive data can sidestep this challenge, providing access to synthetic data instead. We build on the recent success of diffusion models (DMs) and introduce Differentially Private Diffusion Models (DPDMs), which enforce privacy using differentially private stochastic gradient descent (DP-SGD). We investigate the DM parameterization and the sampling algorithm, which turn out to be crucial ingredients in DPDMs, and propose noise multiplicity, a powerful modification of DP-SGD tailored to the training of DMs. We validate our novel DPDMs on image generation benchmarks and achieve state-of-the-art performance in all experiments. Moreover, on standard benchmarks, classifiers trained on DPDM-generated synthetic data perform on par with task-specific DP-SGD-trained classifiers, which has not been demonstrated before for DP generative models. Project page and code: https://nv-tlabs.github.io/DPDM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tim Dockhorn",
      "Tianshi Cao",
      "Arash Vahdat",
      "Karsten Kreis"
    ]
  },
  "https://openreview.net/forum?id=JaNlH6dZYk": {
    "title": "On the special role of class-selective neurons in early training",
    "volume": "main",
    "abstract": "It is commonly observed that deep networks trained for classification exhibit class-selective neurons in their early and intermediate layers. Intriguingly, recent studies have shown that these class-selective neurons can be ablated without deteriorating network function. But if class-selective neurons are not necessary, why do they exist? We attempt to answer this question in a series of experiments on ResNet-50s trained on ImageNet. We first show that class-selective neurons emerge during the first few epochs of training, before receding rapidly but not completely; this suggests that class-selective neurons found in trained networks are in fact vestigial remains of early training. With single-neuron ablation experiments, we then show that class-selective neurons are important for network function in this early phase of training. We also observe that the network is close to a linear regime in this early phase; we thus speculate that class-selective neurons appear early in training as quasi-linear shortcut solutions to the classification task. Finally, in causal experiments where we regularize against class selectivity at different points in training, we show that the presence of class-selective neurons early in training is critical to the successful training of the network; in contrast, class-selective neurons can be suppressed later in training with little effect on final accuracy. It remains to be understood by which mechanism the presence of class-selective neurons in the early phase of training contributes to the successful training of networks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Omkar Ranadive",
      "Nikhil Thakurdesai",
      "Ari S. Morcos",
      "Matthew L Leavitt",
      "Stephane Deny"
    ]
  },
  "https://openreview.net/forum?id=MgdoxzImlK": {
    "title": "Multi-annotator Deep Learning: A Probabilistic Framework for Classification",
    "volume": "main",
    "abstract": "Solving complex classification tasks using deep neural networks typically requires large amounts of annotated data. However, corresponding class labels are noisy when provided by error-prone annotators, e.g., crowdworkers. Training standard deep neural networks leads to subpar performances in such multi-annotator supervised learning settings. We address this issue by presenting a probabilistic training framework named multi-annotator deep learning (MaDL). A downstream ground truth and an annotator performance model are jointly trained in an end-to-end learning approach. The ground truth model learns to predict instances' true class labels, while the annotator performance model infers probabilistic estimates of annotators' performances. A modular network architecture enables us to make varying assumptions regarding annotators' performances, e.g., an optional class or instance dependency. Further, we learn annotator embeddings to estimate annotators' densities within a latent space as proxies of their potentially correlated annotations. Together with a weighted loss function, we improve the learning from correlated annotation patterns. In a comprehensive evaluation, we examine three research questions about multi-annotator supervised learning. Our findings show MaDL's state-of-the-art performance and robustness against many correlated, spamming annotators",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marek Herde",
      "Denis Huseljic",
      "Bernhard Sick"
    ]
  },
  "https://openreview.net/forum?id=Ns2X7Azudy": {
    "title": "Learning to Optimize Quasi-Newton Methods",
    "volume": "main",
    "abstract": "Fast gradient-based optimization algorithms have become increasingly essential for the computationally efficient training of machine learning models. One technique is to multiply the gradient by a preconditioner matrix to produce a step, but it is unclear what the best preconditioner matrix is. This paper introduces a novel machine learning optimizer called LODO, which tries to online meta-learn the best preconditioner during optimization. Specifically, our optimizer merges Learning to Optimize (L2O) techniques with quasi-Newton methods to learn preconditioners parameterized as neural networks; they are more flexible than preconditioners in other quasi-Newton methods. Unlike other L2O methods, LODO does not require any meta-training on a training task distribution, and instead learns to optimize on the fly while optimizing on the test task, adapting to the local characteristics of the loss landscape while traversing it. Theoretically, we show that our optimizer approximates the inverse Hessian in noisy loss landscapes and is capable of representing a wide range of inverse Hessians. We experimentally verify that our algorithm can optimize in noisy settings, and show that simpler alternatives for representing the inverse Hessians worsen performance. Lastly, we use our optimizer to train a semi-realistic deep neural network with 95k parameters at speeds comparable to those of standard neural network optimizers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Isaac Liao",
      "Rumen Dangovski",
      "Jakob Nicolaus Foerster",
      "Marin Soljacic"
    ]
  },
  "https://openreview.net/forum?id=SSkTBUyJip": {
    "title": "Task Weighting in Meta-learning with Trajectory Optimisation",
    "volume": "main",
    "abstract": "Developing meta-learning algorithms that are un-biased toward a subset of training tasks often requires hand-designed criteria to weight tasks, potentially resulting in sub-optimal solutions. In this paper, we introduce a new principled and fully-automated task-weighting algorithm for meta-learning methods. By considering the weights of tasks within the same mini-batch as an action, and the meta-parameter of interest as the system state, we cast the task-weighting meta-learning problem to a trajectory optimisation and employ the iterative linear quadratic regulator to determine the optimal action or weights of tasks. We theoretically show that the proposed algorithm converges to an $\\epsilon_{0}$-stationary point, and empirically demonstrate that the proposed approach out-performs common hand-engineering weighting methods in two few-shot learning benchmarks",
    "checked": true,
    "id": "ce854cd64758ec023d34fe129eaf02f17e0d5108",
    "semantic_title": "task weighting in meta-learning with trajectory optimisation",
    "citation_count": 3,
    "authors": [
      "Cuong C. Nguyen",
      "Thanh-Toan Do",
      "Gustavo Carneiro"
    ]
  },
  "https://openreview.net/forum?id=lNB5EHx8uC": {
    "title": "Cyclic and Randomized Stepsizes Invoke Heavier Tails in SGD than Constant Stepsize",
    "volume": "main",
    "abstract": "Cyclic and randomized stepsizes are widely used in the deep learning practice and can often outperform standard stepsize choices such as constant stepsize in SGD. Despite their empirical success, not much is currently known about when and why they can theoretically improve the generalization performance. We consider a general class of Markovian stepsizes for learning, which contain i.i.d. random stepsize, cyclic stepsize as well as the constant stepsize as special cases, and motivated by the literature which shows that heaviness of the tails (measured by the so-called ``tail-index\") in the SGD iterates is correlated with generalization, we study tail-index and provide a number of theoretical results that demonstrate how the tail-index varies on the stepsize scheduling. Our results bring a new understanding of the benefits of cyclic and randomized stepsizes compared to constant stepsize in terms of the tail behavior. We illustrate our theory on linear regression experiments and show through deep learning experiments that Markovian stepsizes can achieve even a heavier tail and be a viable alternative to cyclic and i.i.d. randomized stepsize rules",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mert Gurbuzbalaban",
      "Yuanhan Hu",
      "Umut Simsekli",
      "Lingjiong Zhu"
    ]
  },
  "https://openreview.net/forum?id=2TneniEIDB": {
    "title": "A probabilistic Taylor expansion with Gaussian processes",
    "volume": "main",
    "abstract": "We study a class of Gaussian processes for which the posterior mean, for a particular choice of data, replicates a truncated Taylor expansion of any order. The data consist of derivative evaluations at the expansion point and the prior covariance kernel belongs to the class of Taylor kernels, which can be written in a certain power series form. We discuss and prove some results on maximum likelihood estimation of parameters of Taylor kernels. The proposed framework is a special case of Gaussian process regression based on data that is orthogonal in the reproducing kernel Hilbert space of the covariance kernel",
    "checked": true,
    "id": "b8f0221d2bcf1b7aa71b5ecf724d8f63ee391a02",
    "semantic_title": "a probabilistic taylor expansion with gaussian processes",
    "citation_count": 0,
    "authors": [
      "Toni Karvonen",
      "Jon Cockayne",
      "Filip Tronarp",
      "Simo SÃ¤rkkÃ¤"
    ]
  },
  "https://openreview.net/forum?id=BFvoemrmqX": {
    "title": "Bridging the Gap Between Target Networks and Functional Regularization",
    "volume": "main",
    "abstract": "Bootstrapping is behind much of the successes of deep Reinforcement Learning. However, learning the value function via bootstrapping often leads to unstable training due to fast-changing target values. Target Networks are employed to stabilize training by using an additional set of lagging parameters to estimate the target values. Despite the popularity of Target Networks, their effect on the optimization is still misunderstood. In this work, we show that they act as an implicit regularizer which can be beneficial in some cases, but also have disadvantages such as being inflexible and can result in instabilities, even when vanilla TD(0) converges. To overcome these issues, we propose an explicit Functional Regularization alternative that is flexible and a convex regularizer in function space and we theoretically study its convergence. We conducted an experimental study across a range of environments, discount factors, and off-policiness data collections to investigate the effectiveness of the regularization induced by Target Networks and Functional Regularization in terms of performance, accuracy, and stability. Our findings emphasize that Functional Regularization can be used as a drop-in replacement for Target Networks and result in performance improvement. Furthermore, adjusting both the regularization weight and the network update period in Functional Regularization can result in further performance improvements compared to solely adjusting the network update period as typically done with Target Networks. Our approach also enhances the ability to networks to recover accurate $Q$-values",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexandre PichÃ©",
      "Valentin Thomas",
      "Joseph Marino",
      "Rafael Pardinas",
      "Gian Maria Marconi",
      "Christopher Pal",
      "Mohammad Emtiyaz Khan"
    ]
  },
  "https://openreview.net/forum?id=4ofFo7D5GL": {
    "title": "HERMES: Hybrid Error-corrector Model with inclusion of External Signals for nonstationary fashion time series",
    "volume": "main",
    "abstract": "Developing models and algorithms to predict nonstationary time series is a long standing statistical problem. It is crucial for many applications, in particular for fashion or retail industries, to make optimal inventory decisions and avoid massive wastes. By tracking thousands of fashion trends on social media with state-of-the-art computer vision approaches, we propose a new model for fashion time series forecasting. Our contribution is twofold. We first provide publicly a dataset gathering 10000 weekly fashion time series. As influence dynamics are the key of emerging trend detection, we associate with each time series an external weak signal representing behaviours of influencers. Secondly, to leverage such a dataset, we propose a new hybrid forecasting mode. Our approach combines per-time-series parametric models with seasonal components and a global recurrent neural network to include sporadic external signals. This hybrid model provides state-of-the-art results on the proposed fashion dataset, on the weekly time series of the M4 competition, and illustrates the benefit of the contribution of external weak signals",
    "checked": true,
    "id": "44a57c2226a876f1bccf7362d3eb8fd39821c5e6",
    "semantic_title": "hermes: hybrid error-corrector model with inclusion of external signals for nonstationary fashion time series",
    "citation_count": 2,
    "authors": [
      "Etienne David",
      "Jean Bellot",
      "Sylvain Le Corff"
    ]
  },
  "https://openreview.net/forum?id=QoRo9QmOAr": {
    "title": "Detecting incidental correlation in multimodal learning via latent variable modeling",
    "volume": "main",
    "abstract": "Multimodal neural networks often fail to utilize all modalities. They subsequently generalize worse than their unimodal counterparts, or make predictions that only depend on a subset of modalities. We refer to this problem as \\emph{modality underutilization}. Existing work has addressed this issue by ensuring that there are no systematic biases in dataset creation, or that our neural network architectures and optimization algorithms are capable of learning modality interactions. We demonstrate that even when these favorable conditions are met, modality underutilization can still occur in the small data regime. To explain this phenomenon, we put forth a concept that we call \\emph{incidental correlation}. It is a spurious correlation that emerges in small datasets, despite not being a part of the underlying data generating process (DGP). We develop our argument using a DGP under which multimodal neural networks must utilize all modalities, since all paths between the inputs and target are causal. This represents an idealized scenario that often fails to materialize. Instead, due to incidental correlation, small datasets sampled from this DGP have higher likelihood under an alternative DGP with spurious paths between the inputs and target. Multimodal neural networks that use these spurious paths for prediction fail to utilize all modalities. Given its harmful effects, we propose to detect incidental correlation via latent variable modeling. We specify an identifiable variational autoencoder such that the latent posterior encodes the spurious correlations between the inputs and target. This allows us to interpret the Kullback-Leibler divergence between the latent posterior and prior as the severity of incidental correlation. We use an ablation study to show that identifiability is important in this context, since we derive our conclusions from the latent posterior. Using experiments with synthetic data, as well as with VQA v2.0 and NLVR2, we demonstrate that incidental correlation emerges in the small data regime, and leads to modality underutilization. Practitioners of multimodal learning can use our method to detect whether incidental correlation is present in their datasets, and determine whether they should collect additional data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taro Makino",
      "Yixin Wang",
      "Krzysztof J. Geras",
      "Kyunghyun Cho"
    ]
  },
  "https://openreview.net/forum?id=ry2qgRqTOw": {
    "title": "Fast Kernel Methods for Generic Lipschitz Losses via $p$-Sparsified Sketches",
    "volume": "main",
    "abstract": "Kernel methods are learning algorithms that enjoy solid theoretical foundations while suffering from important computational limitations. Sketching, which consists in looking for solutions among a subspace of reduced dimension, is a well-studied approach to alleviate these computational burdens. However, statistically-accurate sketches, such as the Gaussian one, usually contain few null entries, such that their application to kernel methods and their non-sparse Gram matrices remains slow in practice. In this paper, we show that sparsified Gaussian (and Rademacher) sketches still produce theoretically-valid approximations while allowing for important time and space savings thanks to an efficient \\emph{decomposition trick}. To support our method, we derive excess risk bounds for both single and multiple output kernel problems, with generic Lipschitz losses, hereby providing new guarantees for a wide range of applications, from robust regression to multiple quantile regression. Our theoretical results are complemented with experiments showing the empirical superiority of our approach over state-of-the-art sketching methods",
    "checked": false,
    "id": "012fd55b8ad27f1c9998b977c8286a7ea2a452c5",
    "semantic_title": "fast kernel methods for generic lipschitz losses via p-sparsified sketches",
    "citation_count": 5,
    "authors": [
      "Tamim El Ahmad",
      "Pierre Laforgue",
      "Florence d'AlchÃ©-Buc"
    ]
  },
  "https://openreview.net/forum?id=244KePn09i": {
    "title": "Single-Pass Contrastive Learning Can Work for Both Homophilic and Heterophilic Graph",
    "volume": "main",
    "abstract": "Existing graph contrastive learning (GCL) techniques typically require two forward passes for a single instance to construct the contrastive loss, which is effective for capturing the low-frequency signals of node features. Such a dual-pass design has shown empirical success on homophilic graphs, but its effectiveness on heterophilic graphs, where directly connected nodes typically have different labels, is unknown. In addition, existing GCL approaches fail to provide strong performance guarantees. Coupled with the unpredictability of GCL approaches on heterophilic graphs, their applicability in real-world contexts is limited. Then, a natural question arises: Can we design a GCL method that works for both homophilic and heterophilic graphs with a performance guarantee? To answer this question, we theoretically study the concentration property of features obtained by neighborhood aggregation on homophilic and heterophilic graphs, introduce the single-pass graph contrastive learning loss based on the property, and provide performance guarantees for the minimizer of the loss on downstream tasks. As a direct consequence of our analysis, we implement the Single-Pass Graph Contrastive Learning method (SP-GCL). Empirically, on 14 benchmark datasets with varying degrees of homophily, the features learned by the SP-GCL can match or outperform existing strong baselines with significantly less computational overhead, which demonstrates the usefulness of our findings in real-world cases",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haonan Wang",
      "Jieyu Zhang",
      "Qi Zhu",
      "Wei Huang",
      "Kenji Kawaguchi",
      "Xiaokui Xiao"
    ]
  },
  "https://openreview.net/forum?id=djN3TaqbdA": {
    "title": "Variational Elliptical Processes",
    "volume": "main",
    "abstract": "We present elliptical processesâa family of non-parametric probabilistic models that subsumes Gaussian processes and Student's t processes. This generalization includes a range of new heavy-tailed behaviors while retaining computational tractability. Elliptical processes are based on a representation of elliptical distributions as a continuous mixture of Gaussian distributions. We parameterize this mixture distribution as a spline normalizing flow, which we train using variational inference. The proposed form of the variational posterior enables a sparse variational elliptical process applicable to large-scale problems. We highlight advantages compared to Gaussian processes through regression and classification experiments. Elliptical processes can supersede Gaussian processes in several settings, including cases where the likelihood is non-Gaussian or when accurate tail modeling is essential",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maria Margareta BÃ¥nkestad",
      "Jens SjÃ¶lund",
      "Jalil Taghia",
      "Thomas B. SchÃ¶n"
    ]
  },
  "https://openreview.net/forum?id=PRrKOaDQtQ": {
    "title": "Mitigating Confirmation Bias in Semi-supervised Learning via Efficient Bayesian Model Averaging",
    "volume": "main",
    "abstract": "State-of-the-art (SOTA) semi-supervised learning (SSL) methods have been highly successful in leveraging a mix of labeled and unlabeled data, often via self-training or pseudo-labeling. During pseudo-labeling, the model's predictions on unlabeled data are used for training and may result in confirmation bias where the model reinforces its own mistakes. In this work, we show that SOTA SSL methods often suffer from confirmation bias and demonstrate that this is often a result of using a poorly calibrated classifier for pseudo labeling. We introduce BaM-SSL, an efficient Bayesian Model averaging technique that improves uncertainty quantification in SSL methods with limited computational or memory overhead. We demonstrate that BaM-SSL mitigates confirmation bias in SOTA SSL methods across standard vision benchmarks of CIFAR-10, CIFAR-100, giving up to 16% improvement in test accuracy on the CIFAR-100 with 400 labels benchmark. Furthermore, we also demonstrate their effectiveness in additional realistic and challenging problems, such as class-imbalanced datasets and in photonics science",
    "checked": true,
    "id": "7a8280a7cc11463dd7f98683018f328b64041a99",
    "semantic_title": "mitigating confirmation bias in semi-supervised learning via efficient bayesian model averaging",
    "citation_count": 2,
    "authors": [
      "Charlotte Loh",
      "Rumen Dangovski",
      "Shivchander Sudalairaj",
      "Seungwook Han",
      "Ligong Han",
      "Leonid Karlinsky",
      "Marin Soljacic",
      "Akash Srivastava"
    ]
  },
  "https://openreview.net/forum?id=l4Jcxs0fpC": {
    "title": "Individual Privacy Accounting for Differentially Private Stochastic Gradient Descent",
    "volume": "main",
    "abstract": "Differentially private stochastic gradient descent (DP-SGD) is the workhorse algorithm for recent advances in private deep learning. It provides a single privacy guarantee to all datapoints in the dataset. We propose \\emph{output-specific} $(\\varepsilon,\\delta)$-DP to characterize privacy guarantees for individual examples when releasing models trained by DP-SGD. We also design an efficient algorithm to investigate individual privacy across a number of datasets. We find that most examples enjoy stronger privacy guarantees than the worst-case bound. We further discover that the training loss and the privacy parameter of an example are well-correlated. This implies groups that are underserved in terms of model utility simultaneously experience weaker privacy guarantees. For example, on CIFAR-10, the average $\\varepsilon$ of the class with the lowest test accuracy is 44.2\\% higher than that of the class with the highest accuracy",
    "checked": true,
    "id": "750331fa07beb042acb462283e18d05d756824e3",
    "semantic_title": "individual privacy accounting for differentially private stochastic gradient descent",
    "citation_count": 22,
    "authors": [
      "Da Yu",
      "Gautam Kamath",
      "Janardhan Kulkarni",
      "Tie-Yan Liu",
      "Jian Yin",
      "Huishuai Zhang"
    ]
  },
  "https://openreview.net/forum?id=VI2JjIfU37": {
    "title": "A DNN Optimizer that Improves over AdaBelief by Suppression of the Adaptive Stepsize Range",
    "volume": "main",
    "abstract": "We make contributions towards improving adaptive-optimizer performance. Our improvements are based on suppression of the range of adaptive stepsizes in the AdaBelief optimizer. Firstly, we show that the particular placement of the parameter $\\epsilon$ within the update expressions of AdaBelief reduces the range of the adaptive stepsizes, making AdaBelief closer to SGD with momentum. Secondly, we extend AdaBelief by further suppressing the range of the adaptive stepsizes. To achieve the above goal, we perform mutual layerwise vector projections between the gradient $\\boldsymbol{g}_t$ and its first momentum $\\boldsymbol{m}_t$ before using them to estimate the second momentum. The new optimization method is referred to as \\emph{Aida}. Thirdly, extensive experimental results show that Aida outperforms nine optimizers when training transformers and LSTMs for NLP, and VGG and ResNet for image classification over CIAF10 and CIFAR100 while matching the best performance of the nine methods when training WGAN-GP models for image generation tasks. Furthermore, Aida produces higher validation accuracies than AdaBelief for training ResNet18 over ImageNet",
    "checked": true,
    "id": "d1a029608df9d652418c776bf3dc6aa836216e7e",
    "semantic_title": "a dnn optimizer that improves over adabelief by suppression of the adaptive stepsize range",
    "citation_count": 2,
    "authors": [
      "Guoqiang Zhang",
      "Kenta Niwa",
      "W. Bastiaan Kleijn"
    ]
  },
  "https://openreview.net/forum?id=f0FSDAy1bU": {
    "title": "Faster Training of Neural ODEs Using GauÃâLegendre Quadrature",
    "volume": "main",
    "abstract": "Neural ODEs demonstrate strong performance in generative and time-series modelling. However, training them via the adjoint method is slow compared to discrete models due to the requirement of numerically solving ODEs. To speed neural ODEs up, a common approach is to regularise the solutions. However, this approach may affect the expressivity of the model; when the trajectory itself matters, this is particularly important. In this paper, we propose an alternative way to speed up the training of neural ODEs. The key idea is to speed up the adjoint method by using GauÃ-Legendre quadrature to solve integrals faster than ODE-based methods while remaining memory efficient. We also extend the idea to training SDEs using the Wong-Zakai theorem, by training a corresponding ODE and transferring the parameters. Our approach leads to faster training of neural ODEs, especially for large models. It also presents a new way to train SDE-based models",
    "checked": false,
    "id": "dde67a6ab628d08f693376a3e7e3332f1795a990",
    "semantic_title": "faster training of neural odes using gauÃ-legendre quadrature",
    "citation_count": 3,
    "authors": [
      "Alexander Luke Ian Norcliffe",
      "Marc Peter Deisenroth"
    ]
  },
  "https://openreview.net/forum?id=lAQQx7hlku": {
    "title": "Bridging the Sim2Real gap with CARE: Supervised Detection Adaptation with Conditional Alignment and Reweighting",
    "volume": "main",
    "abstract": "Sim2Real domain adaptation (DA) research focuses on the constrained setting of adapting from a labeled synthetic source domain to an unlabeled or sparsely labeled real target domain. However, for high-stakes applications (e.g. autonomous driving), it is common to have a modest amount of human-labeled real data in addition to plentiful auto-labeled source data (e.g. from a driving simulator). We study this setting of supervised sim2real DA applied to 2D object detection. We propose Domain Translation via Conditional Alignment and Reweighting (CARE) a novel algorithm that systematically exploits target labels to explicitly close the sim2real appearance and content gaps. We present an analytical justification of our algorithm and demonstrate strong gains over competing methods on standard benchmarks",
    "checked": true,
    "id": "4bfb08bce1f31cd20ee2c3e66ff8078f6501e4fe",
    "semantic_title": "bridging the sim2real gap with care: supervised detection adaptation with conditional alignment and reweighting",
    "citation_count": 12,
    "authors": [
      "Viraj Uday Prabhu",
      "David Acuna",
      "Rafid Mahmood",
      "Marc T. Law",
      "Yuan-Hong Liao",
      "Judy Hoffman",
      "Sanja Fidler",
      "James Lucas"
    ]
  },
  "https://openreview.net/forum?id=obB415rg8q": {
    "title": "Efficient Inference With Model Cascades",
    "volume": "main",
    "abstract": "State-of-the-art deep learning models are becoming ever larger. However, many practical applications are constrained by the cost of inference. Cascades of pretrained models with conditional execution address these requirements based on the intuition that some inputs are easy enough that they can be processed correctly by a smaller model allowing for an early exit. If the smaller model is not sufficiently confident in its prediction, the input is passed on to a larger model. The selection of the confidence threshold allows to trade off computational cost against accuracy. In this work we explore the effective design of model cascades, thoroughly evaluate the impact on the accuracy-efficiency trade-off, and provide a reproducible state-of-the-art baseline that is currently missing for related research. We demonstrate that model cascades dominate the ImageNet Pareto front already with 2-model cascades, achieving an average reduction in compute effort at equal accuracy of almost 3.1x above 86% and more than 1.9x between 80% and 86% top-1 accuracy, while 3-model cascades achieve 4.4x above 87% accuracy. We confirm wider applicability and effectiveness of the method on the GLUE benchmark. We release the code to reproduce our experiments in the supplementary material and use only publicly available pretrained models and datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luzian Lebovitz",
      "Lukas Cavigelli",
      "Michele Magno",
      "Lorenz K Muller"
    ]
  },
  "https://openreview.net/forum?id=EWPA9TZcUy": {
    "title": "Semantic Representations of Mathematical Expressions in a Continuous Vector Space",
    "volume": "main",
    "abstract": "Mathematical notation makes up a large portion of STEM literature, yet finding semantic representations for formulae remains a challenging problem. Because mathematical notation is precise, and its meaning changes significantly with small character shifts, the methods that work for natural text do not necessarily work well for mathematical expressions. This work describes an approach for representing mathematical expressions in a continuous vector space. We use the encoder of a sequence-to-sequence architecture, trained on visually different but mathematically equivalent expressions, to generate vector representations (or embeddings). We compare this approach with a structural approach that considers visual layout to embed an expression and show that our proposed approach is better at capturing mathematical semantics. Finally, to expedite future research, we publish a corpus of equivalent transcendental and algebraic expression pairs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Neeraj Gangwar",
      "Nickvash Kani"
    ]
  },
  "https://openreview.net/forum?id=oFC2LAqS6Z": {
    "title": "Representations and Computations in Transformers that Support Generalization on Structured Tasks",
    "volume": "main",
    "abstract": "Transformers have shown remarkable success in natural language processing and computer vision, serving as the foundation of large language and multimodal models. These networks can capture nuanced context sensitivity across high-dimensional language tokens or image pixels, but it remains unclear how highly structured behavior and systematic generalization can arise in these systems. Here, we explore the solution process a causal transformer discovers as it learns to solve a set of algorithmic tasks involving copying, sorting, and hierarchical compositions of these operations. We search for the minimal layer and head configuration sufficient to solve these tasks and unpack the roles of the attention heads, as well as how token representations are reweighted across layers to complement these roles. Our results provide new insights into how attention layers in transformers support structured computation within and across tasks: 1) Replacing fixed position labels with labels sampled from a larger set enables strong length generalization and faster learning. The learnable embeddings of these labels develop different representations, capturing sequence order if necessary, depending on task demand. 2) Two-layer transformers can learn reliable solutions to the multi-level problems we explore. The first layer tends to transform the input representation to allow the second layer to share computation across repeated components within a task or across related tasks. 3) We introduce an analysis pipeline that quantifies how the representation space in a given layer prioritizes different aspects of each item. We show that these representations prioritize information needed to guide attention relative to information that only requires downstream readout",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Li",
      "James McClelland"
    ]
  },
  "https://openreview.net/forum?id=tv46tCzs83": {
    "title": "Causal Parrots: Large Language Models May Talk Causality But Are Not Causal",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b9672ac98913c43fcb996b3def314789d1cc0cf4",
    "semantic_title": "causal parrots: large language models may talk causality but are not causal",
    "citation_count": 121,
    "authors": [
      "Matej ZeÄeviÄ",
      "Moritz Willig",
      "Devendra Singh Dhami",
      "Kristian Kersting"
    ]
  },
  "https://openreview.net/forum?id=VP9p4u9jAo": {
    "title": "An Option-Dependent Analysis of Regret Minimization Algorithms in Finite-Horizon Semi-MDP",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": "3e45d02b763cb8197369ac0b3ef4a16f4726de85",
    "semantic_title": "an option-dependent analysis of regret minimization algorithms in finite-horizon semi-markov decision processes",
    "citation_count": 0,
    "authors": [
      "Gianluca Drappo",
      "Alberto Maria Metelli",
      "Marcello Restelli"
    ]
  },
  "https://openreview.net/forum?id=Hf95zFnQ7H": {
    "title": "On Adaptivity in Quantum Testing",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "329438fe111495e37f45b91641504abee0f05916",
    "semantic_title": "on adaptivity in quantum testing",
    "citation_count": 3,
    "authors": [
      "Omar Fawzi",
      "Nicolas Flammarion",
      "AurÃ©lien Garivier",
      "Aadil Oufkir"
    ]
  },
  "https://openreview.net/forum?id=d4Vr6E0jjm": {
    "title": "Teaching Smaller Language Models To Generalise To Unseen Compositional Questions",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "aea817017e18cc1cb34962ffd399f8a83ab7a076",
    "semantic_title": "teaching smaller language models to generalise to unseen compositional questions",
    "citation_count": 2,
    "authors": [
      "Tim Hartill",
      "Neset TAN",
      "Michael Witbrock",
      "Patricia J. Riddle"
    ]
  },
  "https://openreview.net/forum?id=3agxS3aDUs": {
    "title": "Subgraph Permutation Equivariant Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joshua Mitton",
      "Roderick Murray-Smith"
    ]
  },
  "https://openreview.net/forum?id=uq29MIWvIV": {
    "title": "About the Cost of Central Privacy in Density Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "ClÃ©ment Lalanne",
      "AurÃ©lien Garivier",
      "RÃ©mi Gribonval"
    ]
  },
  "https://openreview.net/forum?id=REtKapdkyI": {
    "title": "Some Remarks on Identifiability of Independent Component Analysis in Restricted Function Classes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simon Buchholz"
    ]
  },
  "https://openreview.net/forum?id=Nn71AdKyYH": {
    "title": "You Only Transfer What You Share: Intersection-Induced Graph Transfer Learning for Link Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenqing Zheng",
      "Edward W Huang",
      "Nikhil Rao",
      "Zhangyang Wang",
      "Karthik Subbian"
    ]
  },
  "https://openreview.net/forum?id=7wA65zL3B3": {
    "title": "Logistic-Normal Likelihoods for Heteroscedastic Label Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Erik Englesson",
      "Amir Mehrpanah",
      "Hossein Azizpour"
    ]
  },
  "https://openreview.net/forum?id=Ufc5cWhHko": {
    "title": "RECLIP: Resource-efficient CLIP by Training with Small Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runze Li",
      "Dahun Kim",
      "Bir Bhanu",
      "Weicheng Kuo"
    ]
  },
  "https://openreview.net/forum?id=ubCoTAynPp": {
    "title": "Reinforcement Learning with Delayed, Composite, and Partially Anonymous Reward",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Washim Uddin Mondal",
      "Vaneet Aggarwal"
    ]
  },
  "https://openreview.net/forum?id=dPSTDbGtBY": {
    "title": "Towards Multi-spatiotemporal-scale Generalized PDE Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jayesh K Gupta",
      "Johannes Brandstetter"
    ]
  },
  "https://openreview.net/forum?id=z49eaB8kiH": {
    "title": "The Multiquadric Kernel for Moment-Matching Distributional Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ludvig Killingberg",
      "Helge Langseth"
    ]
  },
  "https://openreview.net/forum?id=EDVIHPZhFo": {
    "title": "Nonconvex-nonconcave min-max optimization on Riemannian manifolds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andi Han",
      "Bamdev Mishra",
      "Pratik Jawanpuria",
      "Junbin Gao"
    ]
  },
  "https://openreview.net/forum?id=moZvOx5cxe": {
    "title": "Learning to Boost Resilience of Complex Networks via Neural Edge Rewiring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shanchao Yang",
      "MA KAILI",
      "Baoxiang Wang",
      "Tianshu Yu",
      "Hongyuan Zha"
    ]
  },
  "https://openreview.net/forum?id=y8RZoPjEUl": {
    "title": "Simulate Time-integrated Coarse-grained Molecular Dynamics with Multi-scale Graph Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Fu",
      "Tian Xie",
      "Nathan J. Rebello",
      "Bradley Olsen",
      "Tommi S. Jaakkola"
    ]
  },
  "https://openreview.net/forum?id=R2hUure38l": {
    "title": "Meta-Calibration: Learning of Model Calibration Using Differentiable Expected Calibration Error",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ondrej Bohdal",
      "Yongxin Yang",
      "Timothy Hospedales"
    ]
  },
  "https://openreview.net/forum?id=v5ew3FPTgb": {
    "title": "Understanding convolution on graphs via energies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesco Di Giovanni",
      "James Rowbottom",
      "Benjamin Paul Chamberlain",
      "Thomas Markovich",
      "Michael M. Bronstein"
    ]
  },
  "https://openreview.net/forum?id=ZPMf53vE1L": {
    "title": "One-Step Distributional Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mastane Achab",
      "Reda ALAMI",
      "YASSER ABDELAZIZ DAHOU DJILALI",
      "Kirill Fedyanin",
      "Eric Moulines"
    ]
  },
  "https://openreview.net/forum?id=PHAr3q49h6": {
    "title": "Dual Representation Learning for Out-of-distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhilin Zhao",
      "Longbing Cao"
    ]
  },
  "https://openreview.net/forum?id=83rgSFPpws": {
    "title": "Cyclophobic Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefan Sylvius Wagner",
      "Peter Arndt",
      "Jan Robine",
      "Stefan Harmeling"
    ]
  },
  "https://openreview.net/forum?id=nYzhlFyjjd": {
    "title": "Rotation-Invariant Random Features Provide a Strong Baseline for Machine Learning on 3D Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Owen Melia",
      "Eric M Jonas",
      "Rebecca Willett"
    ]
  },
  "https://openreview.net/forum?id=pHCdMat0gI": {
    "title": "Graph Neural Networks for Temporal Graphs: State of the Art, Open Challenges, and Opportunities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antonio Longa",
      "Veronica Lachi",
      "Gabriele Santin",
      "Monica Bianchini",
      "Bruno Lepri",
      "Pietro Lio",
      "franco scarselli",
      "Andrea Passerini"
    ]
  },
  "https://openreview.net/forum?id=0f8tU3QwWD": {
    "title": "FairGrad: Fairness Aware Gradient Descent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaurav Maheshwari",
      "MichaÃ«l Perrot"
    ]
  },
  "https://openreview.net/forum?id=qHZs2p4ZD4": {
    "title": "V1T: large-scale mouse V1 response prediction using a Vision Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bryan M. Li",
      "Isabel Maria Cornacchia",
      "Nathalie Rochefort",
      "Arno Onken"
    ]
  },
  "https://openreview.net/forum?id=ey5b7kODvK": {
    "title": "Novel Class Discovery for Long-tailed Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuyu Zhang",
      "Ruijie Xu",
      "Xuming He"
    ]
  },
  "https://openreview.net/forum?id=U4XgzRjfF1": {
    "title": "Asymptotic Analysis of Conditioned Stochastic Gradient Descent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "RÃ©mi Leluc",
      "FranÃ§ois Portier"
    ]
  },
  "https://openreview.net/forum?id=WYKTCKpImz": {
    "title": "Learned Thresholds Token Merging and Pruning for Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maxim Bonnaerens",
      "Joni Dambre"
    ]
  },
  "https://openreview.net/forum?id=FqOG4osY7C": {
    "title": "MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weicheng Kuo",
      "AJ Piergiovanni",
      "Dahun Kim",
      "xiyang luo",
      "Benjamin Caine",
      "Wei Li",
      "Abhijit Ogale",
      "Luowei Zhou",
      "Andrew M. Dai",
      "Zhifeng Chen",
      "Claire Cui",
      "Anelia Angelova"
    ]
  },
  "https://openreview.net/forum?id=EwJJks2cSa": {
    "title": "Chasing Better Deep Image Priors between Over- and Under-parameterization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiming Wu",
      "Xiaohan Chen",
      "Yifan Jiang",
      "Zhangyang Wang"
    ]
  },
  "https://openreview.net/forum?id=TjaMO63fc9": {
    "title": "Federated High-Dimensional Online Decision Making",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chi-Hua Wang",
      "Wenjie Li",
      "Guang Lin"
    ]
  },
  "https://openreview.net/forum?id=QnT41ZGNh9": {
    "title": "Regret Bounds for Satisficing in Multi-Armed Bandit Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Michel",
      "Hossein Hajiabolhassan",
      "Ronald Ortner"
    ]
  },
  "https://openreview.net/forum?id=nFWRuJXPkU": {
    "title": "Using Confounded Data in Latent Model-Based Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maxime Gasse",
      "Damien GRASSET",
      "Guillaume Gaudron",
      "Pierre-Yves Oudeyer"
    ]
  },
  "https://openreview.net/forum?id=1irVjE7A3w": {
    "title": "Meta-Learning via Classifier(-free) Diffusion Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elvis Nava",
      "Seijin Kobayashi",
      "Yifei Yin",
      "Robert K. Katzschmann",
      "Benjamin F Grewe"
    ]
  },
  "https://openreview.net/forum?id=nGW2Hotpq3": {
    "title": "Optimizing Learning Rate Schedules for Iterative Pruning of Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiyu Liu",
      "Rohan Ghosh",
      "John Chong Min Tan",
      "Mehul Motani"
    ]
  },
  "https://openreview.net/forum?id=wvLQMHtyLk": {
    "title": "Foiling Explanations in Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Snir Vitrack Tamam",
      "Raz Lapid",
      "Moshe Sipper"
    ]
  },
  "https://openreview.net/forum?id=pCbC3aQB5W": {
    "title": "Long-term Forecasting with TiDE: Time-series Dense Encoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhimanyu Das",
      "Weihao Kong",
      "Andrew Leach",
      "Shaan K Mathur",
      "Rajat Sen",
      "Rose Yu"
    ]
  },
  "https://openreview.net/forum?id=Y3saBb7mCE": {
    "title": "Empirical Limitations of the NTK for Understanding Scaling Laws in Deep Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikhil Vyas",
      "Yamini Bansal",
      "Preetum Nakkiran"
    ]
  },
  "https://openreview.net/forum?id=7KW7zvKd7J": {
    "title": "Transport Score Climbing: Variational Inference Using Forward KL and Adaptive Neural Transport",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liyi Zhang",
      "David Blei",
      "Christian A Naesseth"
    ]
  },
  "https://openreview.net/forum?id=D5Z2E8CNsD": {
    "title": "Distributionally Robust Classification on a Data Budget",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Feuer",
      "Ameya Joshi",
      "Minh Pham",
      "Chinmay Hegde"
    ]
  },
  "https://openreview.net/forum?id=8ykyGbtt2q": {
    "title": "The ConceptARC Benchmark: Evaluating Understanding and Generalization in the ARC Domain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arsenii Kirillovich Moskvichev",
      "Victor Vikram Odouard",
      "Melanie Mitchell"
    ]
  },
  "https://openreview.net/forum?id=Rb6VDOHebB": {
    "title": "Adaptive Compression for Communication-Efficient Distributed Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maksim Makarenko",
      "Elnur Gasanov",
      "Abdurakhmon Sadiev",
      "Rustem Islamov",
      "Peter RichtÃ¡rik"
    ]
  },
  "https://openreview.net/forum?id=H1SekypXKA": {
    "title": "Expected Worst Case Regret via Stochastic Sequential Covering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changlong Wu",
      "Mohsen Heidari",
      "Ananth Grama",
      "Wojciech Szpankowski"
    ]
  },
  "https://openreview.net/forum?id=HVAeM6sNo8": {
    "title": "Robust Alzheimer's Progression Modeling using Cross-Domain Self-Supervised Deep Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saba Dadsetan",
      "Mohsen Hejrati",
      "Shandong Wu",
      "Somaye Hashemifar"
    ]
  },
  "https://openreview.net/forum?id=LRYtNj8Xw0": {
    "title": "Learning Augmentation Distributions using Transformed Risk Minimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Evangelos Chatzipantazis",
      "Stefanos Pertigkiozoglou",
      "Kostas Daniilidis",
      "Edgar Dobriban"
    ]
  },
  "https://openreview.net/forum?id=qUxBs3Ln41": {
    "title": "Structured Low-Rank Tensors for Generalized Linear Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Batoul Ahmad Taki",
      "Anand Sarwate",
      "Waheed U. Bajwa"
    ]
  },
  "https://openreview.net/forum?id=dXAuvo6CGI": {
    "title": "Scalable Stochastic Gradient Riemannian Langevin Dynamics in Non-Diagonal Metrics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanlin Yu",
      "Marcelo Hartmann",
      "Bernardo Williams",
      "Arto Klami"
    ]
  },
  "https://openreview.net/forum?id=HwcB5elyuG": {
    "title": "Towards a Defense Against Federated Backdoor Attacks Under Continuous Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuaiqi Wang",
      "Jonathan Hayase",
      "Giulia Fanti",
      "Sewoong Oh"
    ]
  },
  "https://openreview.net/forum?id=9jnsPp8DP3": {
    "title": "mL-BFGS: A Momentum-based L-BFGS for Distributed Large-scale Neural Network Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Niu",
      "Zalan Fabian",
      "Sunwoo Lee",
      "Mahdi Soltanolkotabi",
      "Salman Avestimehr"
    ]
  },
  "https://openreview.net/forum?id=lu4oAq55iK": {
    "title": "Mitigating Real-World Distribution Shifts in the Fourier Domain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kiran Krishnamachari",
      "See-Kiong Ng",
      "Chuan-Sheng Foo"
    ]
  },
  "https://openreview.net/forum?id=nOIGfQnFZm": {
    "title": "Learning representations that are closed-form Monge mapping optimal with application to domain adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oliver Struckmeier",
      "Ievgen Redko",
      "Anton Mallasto",
      "Karol Arndt",
      "Markus Heinonen",
      "Ville Kyrki"
    ]
  },
  "https://openreview.net/forum?id=kdfiEu1ul6": {
    "title": "Learning from time-dependent streaming data with online stochastic algorithms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antoine Godichon-Baggioni",
      "Nicklas Werge",
      "Olivier Wintenberger"
    ]
  },
  "https://openreview.net/forum?id=W0ehjkl9x7": {
    "title": "DoCoM: Compressed Decentralized Optimization with Near-Optimal Sample Complexity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chung-Yiu Yau",
      "Hoi To Wai"
    ]
  },
  "https://openreview.net/forum?id=moVEUgJaHO": {
    "title": "GPS++: Reviving the Art of Message Passing for Molecular Property Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dominic Masters",
      "Josef Dean",
      "Kerstin Klaeser",
      "Zhiyi Li",
      "Samuel Maddrell-Mander",
      "Adam Sanders",
      "Hatem Helal",
      "Deniz Beker",
      "Andrew W Fitzgibbon",
      "Shenyang Huang",
      "Ladislav RampÃ¡Å¡ek",
      "Dominique Beaini"
    ]
  },
  "https://openreview.net/forum?id=3LzgOQ3eOb": {
    "title": "Tackling Provably Hard Representative Selection via Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mehran Kazemi",
      "Anton Tsitsulin",
      "Hossein Esfandiari",
      "Mohammadhossein Bateni",
      "Deepak Ramachandran",
      "Bryan Perozzi",
      "Vahab Mirrokni"
    ]
  },
  "https://openreview.net/forum?id=Qlvgq9eC63": {
    "title": "Improved Group Robustness via Classifier Retraining on Independent Splits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thien Hang Nguyen",
      "Hongyang R. Zhang",
      "Huy Nguyen"
    ]
  },
  "https://openreview.net/forum?id=0XBuaxqEcG": {
    "title": "Execution-based Code Generation using Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Parshin Shojaee",
      "Aneesh Jain",
      "Sindhu Tipirneni",
      "Chandan K. Reddy"
    ]
  },
  "https://openreview.net/forum?id=TIsrnWpjQ0": {
    "title": "TabCBM: Concept-based Interpretable Neural Networks for Tabular Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mateo Espinosa Zarlenga",
      "Zohreh Shams",
      "Michael Edward Nelson",
      "Been Kim",
      "Mateja Jamnik"
    ]
  },
  "https://openreview.net/forum?id=giw2vcAhiH": {
    "title": "Spectral learning of Bernoulli linear dynamical systems models for decision-making",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Iris R Stone",
      "Yotam Sagiv",
      "Il Memming Park",
      "Jonathan W. Pillow"
    ]
  },
  "https://openreview.net/forum?id=bnBeNFB27b": {
    "title": "Self-Supervision is All You Need for Solving Rubik's Cube",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyo Takano"
    ]
  },
  "https://openreview.net/forum?id=EYjfLeJL4l": {
    "title": "Towards Better Generalization with Flexible Representation of Multi-Module Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "HyunGeun Lee",
      "Kijung Yoon"
    ]
  },
  "https://openreview.net/forum?id=5rq8iRzHAQ": {
    "title": "Assisting Human Decisions in Document Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joon Sik Kim",
      "Valerie Chen",
      "Danish Pruthi",
      "Nihar B Shah",
      "Ameet Talwalkar"
    ]
  },
  "https://openreview.net/forum?id=T5sXdAO3EQ": {
    "title": "Bayesian Quadrature for Neural Ensemble Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saad Hamid",
      "Xingchen Wan",
      "Martin JÃ¸rgensen",
      "Binxin Ru",
      "Michael A Osborne"
    ]
  },
  "https://openreview.net/forum?id=MMsyqXIJuk": {
    "title": "JiangJun: Mastering Xiangqi by Tackling Non-Transitivity in Two-Player Zero-Sum Games",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Li",
      "Kun Xiong",
      "Yingping Zhang",
      "Jiangcheng Zhu",
      "Stephen Marcus McAleer",
      "Wei Pan",
      "Jun Wang",
      "Zonghong Dai",
      "Yaodong Yang"
    ]
  },
  "https://openreview.net/forum?id=f39UIDkwwc": {
    "title": "Contrastive Attraction and Contrastive Repulsion for Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huangjie Zheng",
      "Xu Chen",
      "Jiangchao Yao",
      "Hongxia Yang",
      "Chunyuan Li",
      "Ya Zhang",
      "Hao Zhang",
      "Ivor Tsang",
      "Jingren Zhou",
      "Mingyuan Zhou"
    ]
  },
  "https://openreview.net/forum?id=HyzCuCV1jH": {
    "title": "Data Augmentation is a Hyperparameter: Cherry-picked Self-Supervision for Unsupervised Anomaly Detection is Creating the Illusion of Success",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaemin Yoo",
      "Tiancheng Zhao",
      "Leman Akoglu"
    ]
  },
  "https://openreview.net/forum?id=Wcui061fxr": {
    "title": "Conditional Generative Models are Provably Robust: Pointwise Guarantees for Bayesian Inverse Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fabian AltekrÃ¼ger",
      "Paul Hagemann",
      "Gabriele Steidl"
    ]
  },
  "https://openreview.net/forum?id=eLX5XrajXh": {
    "title": "A Characteristic Function for Shapley-Value-Based Attribution of Anomaly Scores",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Naoya Takeishi",
      "Yoshinobu Kawahara"
    ]
  },
  "https://openreview.net/forum?id=QBMyDZsPMd": {
    "title": "The Open MatSci ML Toolkit: A Flexible Framework for Machine Learning in Materials Science",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Santiago Miret",
      "Kin Long Kelvin Lee",
      "Carmelo Gonzales",
      "Marcel Nassar",
      "Matthew Spellings"
    ]
  },
  "https://openreview.net/forum?id=mXfkKtu5JA": {
    "title": "Differentiable Logic Machines",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthieu Zimmer",
      "Xuening Feng",
      "Claire Glanois",
      "Zhaohui JIANG",
      "Jianyi Zhang",
      "Paul Weng",
      "Dong Li",
      "Jianye HAO",
      "Wulong Liu"
    ]
  },
  "https://openreview.net/forum?id=kdPcLdJbt1": {
    "title": "Vulnerability-Aware Instance Reweighting For Adversarial Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Olukorede Fakorede",
      "Ashutosh Kumar Nirala",
      "Modeste Atsague",
      "Jin Tian"
    ]
  },
  "https://openreview.net/forum?id=V7tahqGrOq": {
    "title": "Lifelong Reinforcement Learning with Modulating Masks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eseoghene Ben-Iwhiwhu",
      "Saptarshi Nath",
      "Praveen Kumar Pilly",
      "Soheil Kolouri",
      "Andrea Soltoggio"
    ]
  },
  "https://openreview.net/forum?id=ILNqQhGbLx": {
    "title": "Semantic Self-adaptation: Enhancing Generalization with a Single Sample",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sherwin Bahmani",
      "Oliver Hahn",
      "Eduard Zamfir",
      "Nikita Araslanov",
      "Daniel Cremers",
      "Stefan Roth"
    ]
  },
  "https://openreview.net/forum?id=MyQ1e1VQQ3": {
    "title": "Fair Kernel Regression through Cross-Covariance Operators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adrian Perez-Suay",
      "Paula Gordaliza",
      "Jean-Michel Loubes",
      "Dino Sejdinovic",
      "Gustau Camps-Valls"
    ]
  },
  "https://openreview.net/forum?id=dpGSNLUCzu": {
    "title": "The Score-Difference Flow for Implicit Generative Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Romann M. Weber"
    ]
  },
  "https://openreview.net/forum?id=tLBjsX4tjs": {
    "title": "A Unified Perspective on Natural Gradient Variational Inference with Gaussian Mixture Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oleg Arenz",
      "Philipp Dahlinger",
      "Zihan Ye",
      "Michael Volpp",
      "Gerhard Neumann"
    ]
  },
  "https://openreview.net/forum?id=FbztvhdCX9": {
    "title": "On the Gradient Formula for learning Generative Models with Regularized Optimal Transport Costs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antoine Houdard",
      "Arthur Leclaire",
      "Nicolas Papadakis",
      "Julien Rabin"
    ]
  },
  "https://openreview.net/forum?id=HP7Qpui5YE": {
    "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Zhu",
      "Jiyang Qi",
      "Mingyu Ding",
      "Xiaokang Chen",
      "Ping Luo",
      "Xinggang Wang",
      "Wenyu Liu",
      "Leye Wang",
      "Jingdong Wang"
    ]
  },
  "https://openreview.net/forum?id=SaVEXFuozg": {
    "title": "DSpar: An Embarrassingly Simple Strategy for Efficient GNN training and inference via Degree-based Sparsification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zirui Liu",
      "Kaixiong Zhou",
      "Zhimeng Jiang",
      "Li Li",
      "Rui Chen",
      "Soo-Hyun Choi",
      "Xia Hu"
    ]
  },
  "https://openreview.net/forum?id=1QqIfGZOWu": {
    "title": "Challenges and Opportunities in Offline Reinforcement Learning from Visual Observations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cong Lu",
      "Philip J. Ball",
      "Tim G. J. Rudner",
      "Jack Parker-Holder",
      "Michael A Osborne",
      "Yee Whye Teh"
    ]
  },
  "https://openreview.net/forum?id=LEVbhNrLEL": {
    "title": "Mind the Gap: Mitigating the Distribution Gap in Graph Few-shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chunhui Zhang",
      "Hongfu Liu",
      "Jundong Li",
      "Yanfang Ye",
      "Chuxu Zhang"
    ]
  },
  "https://openreview.net/forum?id=HqIuAzBxbh": {
    "title": "Consistent Collaborative Filtering via Tensor Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiwen Zhao",
      "Guillermo Sapiro"
    ]
  },
  "https://openreview.net/forum?id=jkTqJJOGMS": {
    "title": "Provably Convergent Policy Optimization via Metric-aware Trust Region Methods",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Song",
      "Niao He",
      "Lijun Ding",
      "Chaoyue Zhao"
    ]
  },
  "https://openreview.net/forum?id=P6NcRPb13w": {
    "title": "Adjusting Machine Learning Decisions for Equal Opportunity and Counterfactual Fairness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixin Wang",
      "Dhanya Sridhar",
      "David Blei"
    ]
  },
  "https://openreview.net/forum?id=JJrKbq35l4": {
    "title": "On Average-Case Error Bounds for Kernel-Based Bayesian Quadrature",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xu Cai",
      "Thanh Lam",
      "Jonathan Scarlett"
    ]
  },
  "https://openreview.net/forum?id=ThhMzfrd6r": {
    "title": "Self-Supervised Graph Representation Learning for Neuronal Morphologies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marissa A. Weis",
      "Laura Pede",
      "Timo LÃ¼ddecke",
      "Alexander S Ecker"
    ]
  },
  "https://openreview.net/forum?id=VV4zJwLwI7": {
    "title": "Breaking the Spurious Causality of Conditional Generation via Fairness Intervention with Corrective Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junhyun Nam",
      "Sangwoo Mo",
      "Jaeho Lee",
      "Jinwoo Shin"
    ]
  },
  "https://openreview.net/forum?id=VpaXrBFYZ9": {
    "title": "Stochastic Constrained DRO with a Complexity Independent of Sample Size",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Qi",
      "Jiameng Lyu",
      "Kung-Sik Chan",
      "Er-Wei Bai",
      "Tianbao Yang"
    ]
  },
  "https://openreview.net/forum?id=nddEHTSnqg": {
    "title": "Neural Networks beyond explainability: Selective inference for sequence motifs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antoine VilliÃ©",
      "Philippe Veber",
      "Yohann De Castro",
      "Laurent Jacob"
    ]
  },
  "https://openreview.net/forum?id=w36pqfaJ4t": {
    "title": "Dynamics Adapted Imitation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixuan Liu",
      "Liu Liu",
      "Bingzhe Wu",
      "Lanqing Li",
      "Xueqian Wang",
      "Bo Yuan",
      "Peilin Zhao"
    ]
  },
  "https://openreview.net/forum?id=CkXOwlhf27": {
    "title": "A Proximal Algorithm for Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaming Liang",
      "Yongxin Chen"
    ]
  },
  "https://openreview.net/forum?id=j3FK00HyfU": {
    "title": "The Meta-Evaluation Problem in Explainable AI: Identifying Reliable Estimators with MetaQuantus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anna HedstrÃ¶m",
      "Philine Lou Bommer",
      "Kristoffer Knutsen WickstrÃ¸m",
      "Wojciech Samek",
      "Sebastian Lapuschkin",
      "Marina MC HÃ¶hne"
    ]
  },
  "https://openreview.net/forum?id=LdSP6cvTS4": {
    "title": "Calibrating and Improving Graph Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "MA KAILI",
      "Garry YANG",
      "Han Yang",
      "Yongqiang Chen",
      "James Cheng"
    ]
  },
  "https://openreview.net/forum?id=nfYwRIezvg": {
    "title": "DORA: Exploring Outlier Representations in Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kirill Bykov",
      "Mayukh Deb",
      "Dennis Grinwald",
      "Klaus Robert Muller",
      "Marina MC HÃ¶hne"
    ]
  },
  "https://openreview.net/forum?id=g97OHbQyk1": {
    "title": "The Vendi Score: A Diversity Evaluation Metric for Machine Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dan Friedman",
      "Adji Bousso Dieng"
    ]
  },
  "https://openreview.net/forum?id=OqbGu3hdQb": {
    "title": "Contextual Combinatorial Multi-output GP Bandits with Group Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sepehr Elahi",
      "Baran Atalar",
      "Sevda ÃÄÃ¼t",
      "Cem Tekin"
    ]
  },
  "https://openreview.net/forum?id=Gbu1bHQhEL": {
    "title": "Active Acquisition for Multimodal Temporal Data: A Challenging Decision-Making Task",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jannik Kossen",
      "CÄtÄlina Cangea",
      "Eszter VÃ©rtes",
      "Andrew Jaegle",
      "Viorica Patraucean",
      "Ira Ktena",
      "Nenad Tomasev",
      "Danielle Belgrave"
    ]
  },
  "https://openreview.net/forum?id=gwRwHUZUgz": {
    "title": "Learning Symbolic Rules for Reasoning in Quasi-Natural Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiyu Yang",
      "Jia Deng"
    ]
  },
  "https://openreview.net/forum?id=XJIg4kQbkv": {
    "title": "CoCoFL: Communication- and Computation-Aware Federated Learning via Partial NN Freezing and Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kilian Pfeiffer",
      "Martin Rapp",
      "Ramin Khalili",
      "Joerg Henkel"
    ]
  },
  "https://openreview.net/forum?id=TdzQtbLeVw": {
    "title": "Online Min-max Problems with Non-convexity and Non-stationarity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Huang",
      "Yuan Cheng",
      "Yingbin Liang",
      "Longbo Huang"
    ]
  },
  "https://openreview.net/forum?id=LKz5SqIXPJ": {
    "title": "On the Robustness of Dataset Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Szyller",
      "Rui Zhang",
      "Jian Liu",
      "N Asokan"
    ]
  },
  "https://openreview.net/forum?id=sY35BAiIf4": {
    "title": "Improving Differentially Private SGD via Randomly Sparsified Gradients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyi Zhu",
      "Matthew B. Blaschko"
    ]
  },
  "https://openreview.net/forum?id=p28wv4G65d": {
    "title": "SC2 Benchmark: Supervised Compression for Split Computing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yoshitomo Matsubara",
      "Ruihan Yang",
      "Marco Levorato",
      "Stephan Mandt"
    ]
  },
  "https://openreview.net/forum?id=izL3B8dPx1": {
    "title": "Inherent Limits on Topology-Based Link Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Justus Isaiah Hibshman",
      "Tim Weninger"
    ]
  },
  "https://openreview.net/forum?id=uv32JOdQuh": {
    "title": "Invariant Feature Coding using Tensor Product Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "YUSUKE Mukuta",
      "Tatsuya Harada"
    ]
  },
  "https://openreview.net/forum?id=wk8oXR0kFA": {
    "title": "Releasing Graph Neural Networks with Differential Privacy Guarantees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Iyiola Emmanuel Olatunji",
      "Thorben Funke",
      "Megha Khosla"
    ]
  },
  "https://openreview.net/forum?id=ERqGqZzSu5": {
    "title": "Sequential Query Encoding for Complex Query Answering on Knowledge Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxin Bai",
      "Tianshi Zheng",
      "Yangqiu Song"
    ]
  },
  "https://openreview.net/forum?id=sFk3aBNb81": {
    "title": "TransFool: An Adversarial Attack against Neural Machine Translation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sahar Sadrizadeh",
      "Ljiljana Dolamic",
      "Pascal Frossard"
    ]
  },
  "https://openreview.net/forum?id=9pWjgQ3y85": {
    "title": "An Explicit Expansion of the Kullback-Leibler Divergence along its Fisher-Rao Gradient Flow",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Carles Domingo-Enrich",
      "Aram-Alexandre Pooladian"
    ]
  },
  "https://openreview.net/forum?id=ZoXi7n54OB": {
    "title": "Training with Mixed-Precision Floating-Point Assignments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wonyeol Lee",
      "Rahul Sharma",
      "Alex Aiken"
    ]
  },
  "https://openreview.net/forum?id=A1N2qp4yAq": {
    "title": "Bandwidth Enables Generalization in Quantum Kernel Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abdulkadir Canatar",
      "Evan Peters",
      "Cengiz Pehlevan",
      "Stefan M. Wild",
      "Ruslan Shaydulin"
    ]
  },
  "https://openreview.net/forum?id=vTsfup5ll6": {
    "title": "Privacy-Preserving Energy-Based Generative Models for Marginal Distribution Protection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Robert E. Tillman",
      "Tucker Balch",
      "Manuela Veloso"
    ]
  },
  "https://openreview.net/forum?id=B7PFZtm8DA": {
    "title": "Unsupervised Discovery and Composition of Object Light Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cameron Omid Smith",
      "Hong-Xing Yu",
      "Sergey Zakharov",
      "Fredo Durand",
      "Joshua B. Tenenbaum",
      "Jiajun Wu",
      "Vincent Sitzmann"
    ]
  },
  "https://openreview.net/forum?id=dXnccpSSYF": {
    "title": "Pareto Optimization for Active Learning under Out-of-Distribution Data Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xueying Zhan",
      "Zeyu Dai",
      "Qingzhong Wang",
      "Qing Li",
      "Haoyi Xiong",
      "Dejing Dou",
      "Antoni B. Chan"
    ]
  },
  "https://openreview.net/forum?id=jYkWdJzTwn": {
    "title": "Predicting Out-of-Domain Generalization with Neighborhood Invariance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nathan Hoyen Ng",
      "Neha Hulkund",
      "Kyunghyun Cho",
      "Marzyeh Ghassemi"
    ]
  },
  "https://openreview.net/forum?id=ipe0IMglFF": {
    "title": "Empirical Study on Optimizer Selection for Out-of-Distribution Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hiroki Naganuma",
      "Kartik Ahuja",
      "Shiro Takagi",
      "Tetsuya Motokawa",
      "Rio Yokota",
      "Kohta Ishikawa",
      "Ikuro Sato",
      "Ioannis Mitliagkas"
    ]
  },
  "https://openreview.net/forum?id=FDbQGCAViI": {
    "title": "The Eigenlearning Framework: A Conservation Law Perspective on Kernel Ridge Regression and Wide Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "James B Simon",
      "Madeline Dickens",
      "Dhruva Karkada",
      "Michael R DeWeese"
    ]
  },
  "https://openreview.net/forum?id=kiPsMct7vL": {
    "title": "Unsupervised Domain Adaptation via Minimized Joint Error",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dexuan Zhang",
      "Thomas Westfechtel",
      "Tatsuya Harada"
    ]
  },
  "https://openreview.net/forum?id=r6oHDYOZ6p": {
    "title": "Undersampling is a Minimax Optimal Robustness Intervention in Nonparametric Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Niladri S. Chatterji",
      "Saminul Haque",
      "Tatsunori Hashimoto"
    ]
  },
  "https://openreview.net/forum?id=K0CAGgjYS1": {
    "title": "On the Convergence and Calibration of Deep Learning with Differential Privacy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiqi Bu",
      "Hua Wang",
      "Zongyu Dai",
      "Qi Long"
    ]
  },
  "https://openreview.net/forum?id=B0WYWvVA2r": {
    "title": "Attentional-Biased Stochastic Gradient Descent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Qi",
      "Yi Xu",
      "Wotao Yin",
      "Rong Jin",
      "Tianbao Yang"
    ]
  },
  "https://openreview.net/forum?id=G2GKiicaJI": {
    "title": "Reinforcement Teaching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Calarina Muslimani",
      "Alex Lewandowski",
      "Dale Schuurmans",
      "Matthew E. Taylor",
      "Jun Luo"
    ]
  },
  "https://openreview.net/forum?id=zshemTAa6U": {
    "title": "Test-Time Adaptation for Visual Document Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sayna Ebrahimi",
      "Sercan O Arik",
      "Tomas Pfister"
    ]
  },
  "https://openreview.net/forum?id=W98AEKQ38Y": {
    "title": "Learning to Incentivize Improvements from Strategic Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yatong Chen",
      "Jialu Wang",
      "Yang Liu"
    ]
  },
  "https://openreview.net/forum?id=TSy0vuwQFN": {
    "title": "Finding Competence Regions in Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jens MÃ¼ller",
      "Stefan T. Radev",
      "Robert Schmier",
      "Felix Draxler",
      "Carsten Rother",
      "Ullrich Koethe"
    ]
  },
  "https://openreview.net/forum?id=r7imkFEAQb": {
    "title": "Noise-robust Graph Learning by Estimating and Leveraging Pairwise Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuefeng Du",
      "Tian Bian",
      "Yu Rong",
      "Bo Han",
      "Tongliang Liu",
      "Tingyang Xu",
      "Wenbing Huang",
      "Yixuan Li",
      "Junzhou Huang"
    ]
  },
  "https://openreview.net/forum?id=SwlfyDq6B3": {
    "title": "3D-Aware Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sherwin Bahmani",
      "Jeong Joon Park",
      "Despoina Paschalidou",
      "Hao Tang",
      "Gordon Wetzstein",
      "Leonidas Guibas",
      "Luc Van Gool",
      "Radu Timofte"
    ]
  },
  "https://openreview.net/forum?id=sixOD8YVvM": {
    "title": "Bounded Space Differentially Private Quantiles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Alabi",
      "Omri Ben-Eliezer",
      "Anamay Chaturvedi"
    ]
  },
  "https://openreview.net/forum?id=pxpbTdUEpD": {
    "title": "The Stack: 3 TB of permissively licensed source code",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Denis Kocetkov",
      "Raymond Li",
      "Loubna Ben allal",
      "Jia LI",
      "Chenghao Mou",
      "Yacine Jernite",
      "Margaret Mitchell",
      "Carlos MuÃ±oz Ferrandis",
      "Sean Hughes",
      "Thomas Wolf",
      "Dzmitry Bahdanau",
      "Leandro Von Werra",
      "Harm de Vries"
    ]
  },
  "https://openreview.net/forum?id=sWQJfb2GSk": {
    "title": "Exploring the Approximation Capabilities of Multiplicative Neural Networks for Smooth Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ido Ben-Shaul",
      "Tomer Galanti",
      "Shai Dekel"
    ]
  },
  "https://openreview.net/forum?id=na5sHG69rI": {
    "title": "Assuming Locally Equal Calibration Errors for Non-Parametric Multiclass Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaspar Valk",
      "Meelis Kull"
    ]
  },
  "https://openreview.net/forum?id=OILbP0WErR": {
    "title": "Learning Graph Structure from Convolutional Mixtures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Max Wasserman",
      "Saurabh Sihag",
      "Gonzalo Mateos",
      "Alejandro Ribeiro"
    ]
  },
  "https://openreview.net/forum?id=NrfSRtTpN5": {
    "title": "Learning Object-Centric Neural Scattering Functions for Free-viewpoint Relighting and Scene Composition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hong-Xing Yu",
      "Michelle Guo",
      "Alireza Fathi",
      "Yen-Yu Chang",
      "Eric Ryan Chan",
      "Ruohan Gao",
      "Thomas Funkhouser",
      "Jiajun Wu"
    ]
  },
  "https://openreview.net/forum?id=Y42xVBQusn": {
    "title": "Contextualize Me â The Case for Context in Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Carolin Benjamins",
      "Theresa Eimer",
      "Frederik Schubert",
      "Aditya Mohan",
      "Sebastian DÃ¶hler",
      "AndrÃ© Biedenkapp",
      "Bodo Rosenhahn",
      "Frank Hutter",
      "Marius Lindauer"
    ]
  },
  "https://openreview.net/forum?id=KxBQPz7HKh": {
    "title": "Multi-dimensional concept discovery (MCD): A unifying framework with completeness guarantees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Johanna Vielhaben",
      "Stefan Bluecher",
      "Nils Strodthoff"
    ]
  },
  "https://openreview.net/forum?id=TyBd56VK7z": {
    "title": "Dr-Fairness: Dynamic Data Ratio Adjustment for Fair Training on Real and Generated Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuji Roh",
      "Weili Nie",
      "De-An Huang",
      "Steven Euijong Whang",
      "Arash Vahdat",
      "Anima Anandkumar"
    ]
  },
  "https://openreview.net/forum?id=N7lCDaeNiS": {
    "title": "Federated Learning under Covariate Shifts with Generalization Guarantees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Ramezani-Kebrya",
      "Fanghui Liu",
      "Thomas Pethick",
      "Grigorios Chrysos",
      "Volkan Cevher"
    ]
  },
  "https://openreview.net/forum?id=pbs22kJmEO": {
    "title": "When Does Uncertainty Matter?: Understanding the Impact of Predictive Uncertainty in ML Assisted Decision Making",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sean McGrath",
      "Parth Mehta",
      "Alexandra Zytek",
      "Isaac Lage",
      "Himabindu Lakkaraju"
    ]
  },
  "https://openreview.net/forum?id=QhHLwn3D0Y": {
    "title": "The Robustness Limits of SoTA Vision Models to Natural Variation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mark Ibrahim",
      "Quentin Garrido",
      "Ari S. Morcos",
      "Diane Bouchacourt"
    ]
  },
  "https://openreview.net/forum?id=CqTkapZ6H9": {
    "title": "Robust Multi-Agent Reinforcement Learning with State Uncertainty",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sihong He",
      "Songyang Han",
      "Sanbao Su",
      "Shuo Han",
      "Shaofeng Zou",
      "Fei Miao"
    ]
  },
  "https://openreview.net/forum?id=ClIcmwdlxn": {
    "title": "Optimum-statistical Collaboration Towards General and Efficient Black-box Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjie Li",
      "Chi-Hua Wang",
      "Guang Cheng",
      "Qifan Song"
    ]
  },
  "https://openreview.net/forum?id=KBhSyBBeeO": {
    "title": "An Adaptive Half-Space Projection Method for Stochastic Optimization Problems with Group Sparse Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yutong Dai",
      "Tianyi Chen",
      "Guanyi Wang",
      "Daniel Robinson"
    ]
  },
  "https://openreview.net/forum?id=iDNMZgjJuJ": {
    "title": "Causally-guided Regularization of Graph Attention Improves Generalizability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander P Wu",
      "Thomas Markovich",
      "Bonnie Berger",
      "Nils Yannick Hammerla",
      "Rohit Singh"
    ]
  },
  "https://openreview.net/forum?id=nEX2q5B2RQ": {
    "title": "Analyzing Deep PAC-Bayesian Learning with Neural Tangent Kernel: Convergence, Analytic Generalization Bound, and Efficient Hyperparameter Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Huang",
      "Chunrui Liu",
      "Yilan Chen",
      "Richard Yi Da Xu",
      "Miao Zhang",
      "Tsui-Wei Weng"
    ]
  },
  "https://openreview.net/forum?id=ttzypy3kT7": {
    "title": "High-Modality Multimodal Transformer: Quantifying Modality & Interaction Heterogeneity for High-Modality Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paul Pu Liang",
      "Yiwei Lyu",
      "Xiang Fan",
      "Jeffrey Tsaw",
      "Yudong Liu",
      "Shentong Mo",
      "Dani Yogatama",
      "Louis-Philippe Morency",
      "Russ Salakhutdinov"
    ]
  },
  "https://openreview.net/forum?id=TH6YrEcbth": {
    "title": "Learning Interpolations between Boltzmann Densities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "BÃ¡lint MÃ¡tÃ©",
      "FranÃ§ois Fleuret"
    ]
  },
  "https://openreview.net/forum?id=LjDFIWWVVa": {
    "title": "Retiring $\\Delta \\text{DP}$: New Distribution-Level Metrics for Demographic Parity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaotian Han",
      "Zhimeng Jiang",
      "Hongye Jin",
      "Zirui Liu",
      "Na Zou",
      "Qifan Wang",
      "Xia Hu"
    ]
  },
  "https://openreview.net/forum?id=2f81Q622ww": {
    "title": "Generating Adversarial Examples with Task Oriented Multi-Objective Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anh Tuan Bui",
      "Trung Le",
      "He Zhao",
      "Quan Hung Tran",
      "Paul Montague",
      "Dinh Phung"
    ]
  },
  "https://openreview.net/forum?id=D45gGvUZp2": {
    "title": "Denise: Deep Robust Principal Component Analysis for Positive Semidefinite Matrices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Calypso Herrera",
      "Florian Krach",
      "Anastasis Kratsios",
      "Pierre Ruyssen",
      "Josef Teichmann"
    ]
  },
  "https://openreview.net/forum?id=ZME2nZMTvY": {
    "title": "Mean-Field Control based Approximation of Multi-Agent Reinforcement Learning in Presence of a Non-decomposable Shared Global State",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Washim Uddin Mondal",
      "Vaneet Aggarwal",
      "Satish Ukkusuri"
    ]
  },
  "https://openreview.net/forum?id=DdZoPUPm0a": {
    "title": "Interpretable Mixture of Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aya Abdelsalam Ismail",
      "Sercan O Arik",
      "Jinsung Yoon",
      "Ankur Taly",
      "Soheil Feizi",
      "Tomas Pfister"
    ]
  },
  "https://openreview.net/forum?id=162TqkUNPO": {
    "title": "Comparative Generalization Bounds for Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tomer Galanti",
      "Liane Galanti",
      "Ido Ben-Shaul"
    ]
  },
  "https://openreview.net/forum?id=wNBARGxoJn": {
    "title": "Learning to correct spectral methods for simulating turbulent flows",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gideon Dresdner",
      "Dmitrii Kochkov",
      "Peter Christian Norgaard",
      "Leonardo Zepeda-Nunez",
      "Jamie Smith",
      "Michael Brenner",
      "Stephan Hoyer"
    ]
  },
  "https://openreview.net/forum?id=xzCDD9i4IZ": {
    "title": "Cox-Hawkes: doubly stochastic spatiotemporal Poisson processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xenia Miscouridou",
      "Samir Bhatt",
      "George Mohler",
      "Seth Flaxman",
      "Swapnil Mishra"
    ]
  },
  "https://openreview.net/forum?id=ilHM31lXC4": {
    "title": "Personalized Federated Learning: A Unified Framework and Universal Optimization Techniques",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Filip Hanzely",
      "Boxin Zhao",
      "mladen kolar"
    ]
  },
  "https://openreview.net/forum?id=l5BzfQhROl": {
    "title": "Generating Teammates for Training Robust Ad Hoc Teamwork Agents via Best-Response Diversity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arrasy Rahman",
      "Elliot Fosong",
      "Ignacio Carlucho",
      "Stefano V Albrecht"
    ]
  },
  "https://openreview.net/forum?id=ZgXfXSz51n": {
    "title": "Guillotine Regularization: Why removing layers is needed to improve generalization in Self-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Florian Bordes",
      "Randall Balestriero",
      "Quentin Garrido",
      "Adrien Bardes",
      "Pascal Vincent"
    ]
  },
  "https://openreview.net/forum?id=MTFf1rDDEI": {
    "title": "Successor Feature Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chris Reinke",
      "Xavier Alameda-Pineda"
    ]
  },
  "https://openreview.net/forum?id=Jjl2c8kWUc": {
    "title": "Lightweight Learner for Shared Knowledge Lifelong Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunhao Ge",
      "Yuecheng Li",
      "Di Wu",
      "Ao Xu",
      "Adam M. Jones",
      "Amanda Sofie Rios",
      "Iordanis Fostiropoulos",
      "shixian wen",
      "Po-Hsuan Huang",
      "Zachary William Murdock",
      "Gozde Sahin",
      "Shuo Ni",
      "Kiran Lekkala",
      "Sumedh Anand Sontakke",
      "Laurent Itti"
    ]
  },
  "https://openreview.net/forum?id=6rbcq0qacA": {
    "title": "Deep Plug-and-Play Clustering with Unknown Number of Clusters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "An Xiao",
      "Hanting Chen",
      "Tianyu Guo",
      "QINGHUA ZHANG",
      "Yunhe Wang"
    ]
  },
  "https://openreview.net/forum?id=v73h3bYE2Z": {
    "title": "When to Trust Aggregated Gradients: Addressing Negative Client Sampling in Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenkai Yang",
      "Yankai Lin",
      "Guangxiang Zhao",
      "Peng Li",
      "Jie Zhou",
      "Xu Sun"
    ]
  },
  "https://openreview.net/forum?id=R8TU3pfzFr": {
    "title": "A Measure of the Complexity of Neural Representations based on Partial Information Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Alexander Ehrlich",
      "Andreas Christian Schneider",
      "Viola Priesemann",
      "Michael Wibral",
      "Abdullah Makkeh"
    ]
  },
  "https://openreview.net/forum?id=MR4glug5GU": {
    "title": "Trip-ROMA: Self-Supervised Learning with Triplets and Random Mappings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenbin Li",
      "Xuesong Yang",
      "Meihao Kong",
      "Lei Wang",
      "Jing Huo",
      "Yang Gao",
      "Jiebo Luo"
    ]
  },
  "https://openreview.net/forum?id=DUsgPi3oCC": {
    "title": "Conditional Permutation Invariant Flows",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Berend Zwartsenberg",
      "Adam Scibior",
      "Matthew Niedoba",
      "Vasileios Lioutas",
      "Justice Sefas",
      "Yunpeng Liu",
      "Setareh Dabiri",
      "Jonathan Wilder Lavington",
      "Trevor Campbell",
      "Frank Wood"
    ]
  },
  "https://openreview.net/forum?id=RLYkyucU6k": {
    "title": "Agent-State Construction with Auxiliary Inputs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruo Yu Tao",
      "Adam White",
      "Marlos C. Machado"
    ]
  },
  "https://openreview.net/forum?id=9KoBOlstTq": {
    "title": "Modelling sequential branching dynamics with a multivariate branching Gaussian process",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elvijs Sarkans",
      "Sumon Ahmed",
      "Magnus Rattray",
      "Alexis Boukouvalas"
    ]
  },
  "https://openreview.net/forum?id=j3oQF9coJd": {
    "title": "U-NO: U-shaped Neural Operators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Ashiqur Rahman",
      "Zachary E Ross",
      "Kamyar Azizzadenesheli"
    ]
  },
  "https://openreview.net/forum?id=nOk4XEB7Ke": {
    "title": "Fast&Fair: Training Acceleration and Bias Mitigation for GNNs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oyku Deniz Kose",
      "Yanning Shen"
    ]
  },
  "https://openreview.net/forum?id=IqJsyulDUX": {
    "title": "Ensembles for Uncertainty Estimation: Benefits of Prior Functions and Bootstrapping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vikranth Dwaracherla",
      "Zheng Wen",
      "Ian Osband",
      "Xiuyuan Lu",
      "Seyed Mohammad Asghari",
      "Benjamin Van Roy"
    ]
  },
  "https://openreview.net/forum?id=W98rebBxlQ": {
    "title": "Soft Diffusion: Score Matching with General Corruptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giannis Daras",
      "Mauricio Delbracio",
      "Hossein Talebi",
      "Alex Dimakis",
      "Peyman Milanfar"
    ]
  },
  "https://openreview.net/forum?id=mySiFHCeAl": {
    "title": "Spectral Regularization Allows Data-frugal Learning over Combinatorial Spaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amirali Aghazadeh",
      "Nived Rajaraman",
      "Tony Tu",
      "Kannan Ramchandran"
    ]
  },
  "https://openreview.net/forum?id=jVMMdg31De": {
    "title": "A Cubic Regularization Approach for Finding Local Minimax Points in Nonconvex Minimax Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyi Chen",
      "Zhengyang Hu",
      "Qunwei Li",
      "Zhe Wang",
      "Yi Zhou"
    ]
  },
  "https://openreview.net/forum?id=SEDWlhcFWA": {
    "title": "Assisted Learning for Organizations with Limited Imbalanced Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Chen",
      "Jiaying Zhou",
      "Jie Ding",
      "Yi Zhou"
    ]
  },
  "https://openreview.net/forum?id=EPPqt3uERT": {
    "title": "Transformer for Partial Differential Equations' Operator Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijie Li",
      "Kazem Meidani",
      "Amir Barati Farimani"
    ]
  },
  "https://openreview.net/forum?id=gvcDSDYUZx": {
    "title": "Efficient Model-Based Multi-Agent Mean-Field Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Barna PÃ¡sztor",
      "Andreas Krause",
      "Ilija Bogunovic"
    ]
  },
  "https://openreview.net/forum?id=y4CGF1A8VG": {
    "title": "Machine Explanations and Human Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chacha Chen",
      "Shi Feng",
      "Amit Sharma",
      "Chenhao Tan"
    ]
  },
  "https://openreview.net/forum?id=9aXKUJEKwV": {
    "title": "Learning to Look by Self-Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew Koichi Grimes",
      "Joseph Varughese Modayil",
      "Piotr W Mirowski",
      "Dushyant Rao",
      "Raia Hadsell"
    ]
  },
  "https://openreview.net/forum?id=slsAQHpS7n": {
    "title": "Computationally-efficient initialisation of GPs: The generalised variogram method",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Felipe Tobar",
      "Elsa Cazelles",
      "Taco de Wolff"
    ]
  },
  "https://openreview.net/forum?id=f4VyYhkRvi": {
    "title": "Fairness via In-Processing in the Over-parameterized Regime: A Cautionary Tale with MinDiff Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akshaj Kumar Veldanda",
      "Ivan Brugere",
      "Jiahao Chen",
      "Sanghamitra Dutta",
      "Alan Mishler",
      "Siddharth Garg"
    ]
  },
  "https://openreview.net/forum?id=Oq5XKRVYpQ": {
    "title": "Graph-based Multi-ODE Neural Networks for Spatio-Temporal Traffic Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zibo Liu",
      "Parshin Shojaee",
      "Chandan K. Reddy"
    ]
  },
  "https://openreview.net/forum?id=iMmsCI0JsS": {
    "title": "TimeSeAD: Benchmarking Deep Multivariate Time-Series Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dennis Wagner",
      "Tobias Michels",
      "Florian C.F. Schulz",
      "Arjun Nair",
      "Maja Rudolph",
      "Marius Kloft"
    ]
  },
  "https://openreview.net/forum?id=I4IkGmgFJz": {
    "title": "Data Models for Dataset Drift Controls in Machine Learning With Optical Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luis Oala",
      "Marco Aversa",
      "Gabriel Nobis",
      "Kurt Willis",
      "Yoan Neuenschwander",
      "MichÃ¨le Buck",
      "Christian Matek",
      "Jerome Extermann",
      "Enrico Pomarico",
      "Wojciech Samek",
      "Roderick Murray-Smith",
      "Christoph Clausen",
      "Bruno Sanguinetti"
    ]
  },
  "https://openreview.net/forum?id=1nhTDzxxMA": {
    "title": "Multi-Source Transfer Learning for Deep Model-Based Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Remo Sasso",
      "Matthia Sabatelli",
      "Marco A. Wiering"
    ]
  },
  "https://openreview.net/forum?id=YwNrPLjHSL": {
    "title": "Do Vision-Language Pretrained Models Learn Composable Primitive Concepts?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tian Yun",
      "Usha Bhalla",
      "Ellie Pavlick",
      "Chen Sun"
    ]
  },
  "https://openreview.net/forum?id=KSvr8A62MD": {
    "title": "A Simulation Environment and Reinforcement Learning Method for Waste Reduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sami Jullien",
      "Mozhdeh Ariannezhad",
      "Paul Groth",
      "Maarten de Rijke"
    ]
  },
  "https://openreview.net/forum?id=JkIH4MeOc3": {
    "title": "Group Fairness in Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Harsh Satija",
      "Alessandro Lazaric",
      "Matteo Pirotta",
      "Joelle Pineau"
    ]
  },
  "https://openreview.net/forum?id=OarsigVib0": {
    "title": "On the Statistical Complexity of Estimation and Testing under Privacy Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "ClÃ©ment Lalanne",
      "AurÃ©lien Garivier",
      "RÃ©mi Gribonval"
    ]
  },
  "https://openreview.net/forum?id=B4J40x7NjA": {
    "title": "Positive Difference Distribution for Image Outlier Detection using Normalizing Flows and Contrastive Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Robert Schmier",
      "Ullrich Koethe",
      "Christoph-Nikolas Straehle"
    ]
  },
  "https://openreview.net/forum?id=s9efQF3QW1": {
    "title": "Uncovering the Representation of Spiking Neural Networks Trained with Surrogate Gradient",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhang Li",
      "Youngeun Kim",
      "Hyoungseob Park",
      "Priyadarshini Panda"
    ]
  },
  "https://openreview.net/forum?id=qxrwt6F3sf": {
    "title": "PAC-Bayes Generalisation Bounds for Heavy-Tailed Losses through Supermartingales",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maxime Haddouche",
      "Benjamin Guedj"
    ]
  },
  "https://openreview.net/forum?id=Sb6p5mcefw": {
    "title": "Generalization as Dynamical Robustness--The Role of Riemannian Contraction in Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leo Kozachkov",
      "Patrick Wensing",
      "Jean-Jacques Slotine"
    ]
  },
  "https://openreview.net/forum?id=Hnr23knZfY": {
    "title": "POLTER: Policy Trajectory Ensemble Regularization for Unsupervised Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Frederik Schubert",
      "Carolin Benjamins",
      "Sebastian DÃ¶hler",
      "Bodo Rosenhahn",
      "Marius Lindauer"
    ]
  },
  "https://openreview.net/forum?id=8WUyeeMxMH": {
    "title": "Proximal Curriculum for Reinforcement Learning Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Georgios Tzannetos",
      "BÃ¡rbara Gomes Ribeiro",
      "Parameswaran Kamalaruban",
      "Adish Singla"
    ]
  },
  "https://openreview.net/forum?id=R6W7zkMz0P": {
    "title": "Pre-trained Perceptual Features Improve Differentially Private Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Frederik Harder",
      "Milad Jalali",
      "Danica J. Sutherland",
      "Mijung Park"
    ]
  },
  "https://openreview.net/forum?id=SM1BkjGePI": {
    "title": "Bridging performance gap between minimal and maximal SVM models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ondrej Such",
      "RenÃ© Fabricius"
    ]
  },
  "https://openreview.net/forum?id=YJDqQSAuB6": {
    "title": "Weisfeiler and Leman Go Infinite: Spectral and Combinatorial Pre-Colorings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Or Feldman",
      "Amit Boyarski",
      "Shai Feldman",
      "Dani Kogan",
      "Avi Mendelson",
      "Chaim Baskin"
    ]
  },
  "https://openreview.net/forum?id=2Yo9xqR6Ab": {
    "title": "Jacobian-based Causal Discovery with Nonlinear ICA",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Patrik Reizinger",
      "Yash Sharma",
      "Matthias Bethge",
      "Bernhard SchÃ¶lkopf",
      "Ferenc HuszÃ¡r",
      "Wieland Brendel"
    ]
  },
  "https://openreview.net/forum?id=1IYJfwJtjQ": {
    "title": "FASTRAIN-GNN: Fast and Accurate Self-Training for Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amrit Nagarajan",
      "Anand Raghunathan"
    ]
  },
  "https://openreview.net/forum?id=5nVJlKgmxp": {
    "title": "Online Optimal Tracking of Linear Systems with Adversarial Disturbances",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Farnaz Adib Yaghmaie",
      "Hamidreza Modares"
    ]
  },
  "https://openreview.net/forum?id=T1XtOqrVKn": {
    "title": "Reducing Predictive Feature Suppression in Resource-Constrained Contrastive Image-Caption Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maurits Bleeker",
      "Andrew Yates",
      "Maarten de Rijke"
    ]
  },
  "https://openreview.net/forum?id=fempQstMbV": {
    "title": "Deep Double Descent via Smooth Interpolation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matteo Gamba",
      "Erik Englesson",
      "MÃ¥rten BjÃ¶rkman",
      "Hossein Azizpour"
    ]
  },
  "https://openreview.net/forum?id=4zCgjqjzAv": {
    "title": "Bayesian Transformed Gaussian Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinran Zhu",
      "Leo Huang",
      "Eric Hans Lee",
      "Cameron Alexander Ibrahim",
      "David Bindel"
    ]
  },
  "https://openreview.net/forum?id=AZ4GobeSLq": {
    "title": "A Variational Perspective on Generative Flow Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heiko Zimmermann",
      "Fredrik Lindsten",
      "Jan-Willem van de Meent",
      "Christian A Naesseth"
    ]
  },
  "https://openreview.net/forum?id=oq3tx5kinu": {
    "title": "Active Learning of Ordinal Embeddings: A User Study on Football Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christoffer LÃ¶ffler",
      "Kion Fallah",
      "Stefano Fenu",
      "Dario Zanca",
      "Bjoern Eskofier",
      "Christopher John Rozell",
      "Christopher Mutschler"
    ]
  },
  "https://openreview.net/forum?id=55BcghgicI": {
    "title": "Differentially private partitioned variational inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mikko A. HeikkilÃ¤",
      "Matthew Ashman",
      "Siddharth Swaroop",
      "Richard E Turner",
      "Antti Honkela"
    ]
  },
  "https://openreview.net/forum?id=a0T3nOP9sB": {
    "title": "Adaptive patch foraging in deep reinforcement learning agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nathan Wispinski",
      "Andrew Butcher",
      "Kory Wallace Mathewson",
      "Craig S Chapman",
      "Matthew Botvinick",
      "Patrick M. Pilarski"
    ]
  },
  "https://openreview.net/forum?id=onufdyHvqN": {
    "title": "Private Multi-Task Learning: Formulation and Applications to Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengyuan Hu",
      "Steven Wu",
      "Virginia Smith"
    ]
  },
  "https://openreview.net/forum?id=82hRiAbnnm": {
    "title": "Sobolev Spaces, Kernels and Discrepancies over Hyperspheres",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simon Hubbert",
      "Emilio Porcu",
      "Chris J. Oates",
      "Mark Girolami"
    ]
  },
  "https://openreview.net/forum?id=SgTKk6ryPr": {
    "title": "Monotone deep Boltzmann machines",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhili Feng",
      "Ezra Winston",
      "J Zico Kolter"
    ]
  },
  "https://openreview.net/forum?id=4eL6z9ziw7": {
    "title": "NovelCraft: A Dataset for Novelty Detection and Discovery in Open Worlds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Patrick Feeney",
      "Sarah Schneider",
      "Panagiotis Lymperopoulos",
      "Liping Liu",
      "Matthias Scheutz",
      "Michael C Hughes"
    ]
  },
  "https://openreview.net/forum?id=OsKXlWamTQ": {
    "title": "Integrating Bayesian Network Structure into Residual Flows and Variational Autoencoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jacobie Mouton",
      "Rodney Stephen Kroon"
    ]
  },
  "https://openreview.net/forum?id=QTXocpAP9p": {
    "title": "Neural Collapse: A Review on Modelling Principles and Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vignesh Kothapalli"
    ]
  },
  "https://openreview.net/forum?id=ZOAb497iaY": {
    "title": "Unifying physical systems' inductive biases in neural ODE using dynamics constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Heng Lim",
      "Muhammad Firmansyah Kasim"
    ]
  },
  "https://openreview.net/forum?id=tE2NiMGd07": {
    "title": "Bridging Graph Position Encodings for Transformers with Weighted Graph-Walking Automata",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Patrick Soga",
      "David Chiang"
    ]
  },
  "https://openreview.net/forum?id=jdGMBgYvfX": {
    "title": "UncertaINR: Uncertainty Quantification of End-to-End Implicit Neural Representations for Computed Tomography",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francisca Vasconcelos",
      "Bobby He",
      "Nalini M Singh",
      "Yee Whye Teh"
    ]
  },
  "https://openreview.net/forum?id=FdMWtpVT1I": {
    "title": "Training Data Size Induced Double Descent For Denoising Feedforward Neural Networks and the Role of Training Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rishi Sonthalia",
      "Raj Rao Nadakuditi"
    ]
  },
  "https://openreview.net/forum?id=10JdgrzNOk": {
    "title": "Scalable Deep Compressive Sensing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhonghao Zhang",
      "Yipeng Liu",
      "Xingyu Cao",
      "Fei Wen",
      "Ce Zhu"
    ]
  },
  "https://openreview.net/forum?id=MRLHN4MSmA": {
    "title": "A Modulation Layer to Increase Neural Network Robustness Against Data Quality Issues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohamed Abdelhack",
      "Jiaming Zhang",
      "Sandhya Tripathi",
      "Bradley A Fritz",
      "Daniel Felsky",
      "Michael Avidan",
      "Yixin Chen",
      "Christopher Ryan King"
    ]
  },
  "https://openreview.net/forum?id=gR9UVgH8PZ": {
    "title": "Neural Shape Compiler: A Unified Framework for Transforming between Text, Point Cloud, and Program",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tiange Luo",
      "Honglak Lee",
      "Justin Johnson"
    ]
  },
  "https://openreview.net/forum?id=6IFi2soduD": {
    "title": "Can Pruning Improve Certified Robustness of Neural Networks?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhangheng LI",
      "Tianlong Chen",
      "Linyi Li",
      "Bo Li",
      "Zhangyang Wang"
    ]
  },
  "https://openreview.net/forum?id=v5jwDLqfQo": {
    "title": "Extended Agriculture-Vision: An Extension of a Large Aerial Image Dataset for Agricultural Pattern Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Wu",
      "David Pichler",
      "Daniel Marley",
      "Naira Hovakimyan",
      "David A Wilson",
      "Jennifer Hobbs"
    ]
  },
  "https://openreview.net/forum?id=OJtYpdiHNo": {
    "title": "Transframer: Arbitrary Frame Prediction with Generative Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Charlie Nash",
      "Joao Carreira",
      "Jacob C Walker",
      "Iain Barr",
      "Andrew Jaegle",
      "Mateusz Malinowski",
      "Peter Battaglia"
    ]
  },
  "https://openreview.net/forum?id=xqS8k9E75c": {
    "title": "Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dennis Thomas Ulmer",
      "Christian Hardmeier",
      "Jes Frellsen"
    ]
  },
  "https://openreview.net/forum?id=jM8nzUzBWr": {
    "title": "Estimating the Density Ratio between Distributions with High Discrepancy using Multinomial Logistic Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akash Srivastava",
      "Seungwook Han",
      "Kai Xu",
      "Benjamin Rhodes",
      "Michael U. Gutmann"
    ]
  },
  "https://openreview.net/forum?id=QzWr4w8PXx": {
    "title": "A Revenue Function for Comparison-Based Hierarchical Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aishik Mandal",
      "MichaÃ«l Perrot",
      "Debarghya Ghoshdastidar"
    ]
  },
  "https://openreview.net/forum?id=C1Xl8dYCBn": {
    "title": "ChemSpacE: Interpretable and Interactive Chemical Space Exploration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanqi Du",
      "Xian Liu",
      "Nilay Mahesh Shah",
      "Shengchao Liu",
      "Jieyu Zhang",
      "Bolei Zhou"
    ]
  },
  "https://openreview.net/forum?id=dQxBRqCjLr": {
    "title": "A Free Lunch with Influence Functions? An Empirical Evaluation of Influence Functions for Average Treatment Effect Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew James Vowels",
      "Sina Akbari",
      "Necati Cihan Camgoz",
      "Richard Bowden"
    ]
  },
  "https://openreview.net/forum?id=TzRXyO3CzX": {
    "title": "Clustering using Approximate Nearest Neighbour Oracles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Enayat Ullah",
      "Harry Lang",
      "Raman Arora",
      "Vladimir Braverman"
    ]
  },
  "https://openreview.net/forum?id=JwgVBv18RG": {
    "title": "Bayesian Optimization with Informative Covariance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Afonso Eduardo",
      "Michael U. Gutmann"
    ]
  },
  "https://openreview.net/forum?id=2UQv8L1Cv9": {
    "title": "Turning Normalizing Flows into Monge Maps with Geodesic Gaussian Preserving Flows",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guillaume Morel",
      "Lucas Drumetz",
      "Simon BenaÃ¯chouche",
      "Nicolas Courty",
      "FranÃ§ois Rousseau"
    ]
  },
  "https://openreview.net/forum?id=h4BYtZ79uy": {
    "title": "Graph Neural Networks Designed for Different Graph Types: A Survey",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Josephine Thomas",
      "Alice Moallemy-Oureh",
      "Silvia Beddar-Wiesing",
      "Clara HolzhÃ¼ter"
    ]
  },
  "https://openreview.net/forum?id=KwWKB9Bqam": {
    "title": "Generalization bounds for Kernel Canonical Correlation Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Enayat Ullah",
      "Raman Arora"
    ]
  },
  "https://openreview.net/forum?id=gyhiZYrk5y": {
    "title": "Learning Identity-Preserving Transformations on Data Manifolds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marissa Catherine Connor",
      "Kion Fallah",
      "Christopher John Rozell"
    ]
  },
  "https://openreview.net/forum?id=YtU0nDb5e8": {
    "title": "A Halfspace-Mass Depth-Based Method for Adversarial Attack Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marine Picot",
      "Federica Granese",
      "Guillaume Staerman",
      "Marco Romanelli",
      "Francisco Messina",
      "Pablo Piantanida",
      "Pierre Colombo"
    ]
  },
  "https://openreview.net/forum?id=qdDmxzGuzu": {
    "title": "Reusable Options through Gradient-based Meta Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Kuric",
      "Herke van Hoof"
    ]
  },
  "https://openreview.net/forum?id=qvRWcDXBam": {
    "title": "Containing a spread through sequential learning: to exploit or to explore?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingran Chen",
      "Hesam Nikpey",
      "Jungyeol Kim",
      "Saswati Sarkar",
      "Shirin Saeedi Bidokhti"
    ]
  },
  "https://openreview.net/forum?id=WVwnccBJLz": {
    "title": "Bidirectional View based Consistency Regularization for Semi-Supervised Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuntao Du",
      "å¨ æ±",
      "Hongtao Luo",
      "Haiyang Yang",
      "MingCai Chen",
      "Chongjun Wang"
    ]
  },
  "https://openreview.net/forum?id=UvJBKWaSSH": {
    "title": "FLUID: A Unified Evaluation Framework for Flexible Sequential Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew Wallingford",
      "Aditya Kusupati",
      "Keivan Alizadeh-Vahid",
      "Aaron Walsman",
      "Aniruddha Kembhavi",
      "Ali Farhadi"
    ]
  },
  "https://openreview.net/forum?id=bCiNWDmlY2": {
    "title": "The Low-Rank Simplicity Bias in Deep Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minyoung Huh",
      "Hossein Mobahi",
      "Richard Zhang",
      "Brian Cheung",
      "Pulkit Agrawal",
      "Phillip Isola"
    ]
  },
  "https://openreview.net/forum?id=LIT8tjs6rJ": {
    "title": "Parameter Efficient Node Classification on Homophilic Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucas Prieto",
      "Jeroen Den Boef",
      "Paul Groth",
      "Joran Cornelisse"
    ]
  },
  "https://openreview.net/forum?id=xkrtvHlp3P": {
    "title": "Towards Better Out-of-Distribution Generalization of Neural Algorithmic Reasoning Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sadegh Mahdavi",
      "Kevin Swersky",
      "Thomas Kipf",
      "Milad Hashemi",
      "Christos Thrampoulidis",
      "Renjie Liao"
    ]
  },
  "https://openreview.net/forum?id=9lyqt3rbDc": {
    "title": "L-SVRG and L-Katyusha with Adaptive Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boxin Zhao",
      "Boxiang Lyu",
      "mladen kolar"
    ]
  },
  "https://openreview.net/forum?id=HG11PAmwQ6": {
    "title": "Quantum Policy Iteration via Amplitude Estimation and Grover Search â Towards Quantum Advantage for Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simon Wiedemann",
      "Daniel Hein",
      "Steffen Udluft",
      "Christian B. Mendl"
    ]
  },
  "https://openreview.net/forum?id=tEVpz2xJWX": {
    "title": "Enhancing Diffusion-Based Image Synthesis with Robust Classifier Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bahjat Kawar",
      "Roy Ganz",
      "Michael Elad"
    ]
  },
  "https://openreview.net/forum?id=RjZq6W6FoE": {
    "title": "Improved Overparametrization Bounds for Global Convergence of SGD for Shallow Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "BartÅomiej Polaczyk",
      "Jacek Cyranka"
    ]
  },
  "https://openreview.net/forum?id=wmGlMhaBe0": {
    "title": "A Unified View of Masked Image Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiliang Peng",
      "Li Dong",
      "Hangbo Bao",
      "Furu Wei",
      "Qixiang Ye"
    ]
  },
  "https://openreview.net/forum?id=11pGlecTz2": {
    "title": "How Robust is Your Fairness? Evaluating and Sustaining Fairness under Unseen Distribution Shifts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haotao Wang",
      "Junyuan Hong",
      "Jiayu Zhou",
      "Zhangyang Wang"
    ]
  },
  "https://openreview.net/forum?id=OzGIu4T4Cz": {
    "title": "Leveraging Demonstrations with Latent Space Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonas Gehring",
      "Deepak Gopinath",
      "Jungdam Won",
      "Andreas Krause",
      "Gabriel Synnaeve",
      "Nicolas Usunier"
    ]
  },
  "https://openreview.net/forum?id=Gp0pHyUyrb": {
    "title": "Solving Nonconvex-Nonconcave Min-Max Problems exhibiting Weak Minty Solutions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Axel BÃ¶hm"
    ]
  },
  "https://openreview.net/forum?id=3epEbhdgbv": {
    "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhirong Wu",
      "Zihang Lai",
      "Xiao Sun",
      "Stephen Lin"
    ]
  },
  "https://openreview.net/forum?id=LBA2Jj5Gqn": {
    "title": "Temperature check: theory and practice for training models with softmax-cross-entropy losses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Atish Agarwala",
      "Samuel Stern Schoenholz",
      "Jeffrey Pennington",
      "Yann Dauphin"
    ]
  },
  "https://openreview.net/forum?id=QtrjqVIZna": {
    "title": "Fusion of Global and Local Knowledge for Personalized Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tiansheng Huang",
      "Li Shen",
      "Yan Sun",
      "Weiwei Lin",
      "Dacheng Tao"
    ]
  },
  "https://openreview.net/forum?id=goPsLn3RVo": {
    "title": "Defense Against Reward Poisoning Attacks in Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kiarash Banihashem",
      "Adish Singla",
      "Goran Radanovic"
    ]
  },
  "https://openreview.net/forum?id=DHEZuKStzH": {
    "title": "Learning Energy Conserving Dynamics Efficiently with Hamiltonian Gaussian Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Magnus Ross",
      "Markus Heinonen"
    ]
  },
  "https://openreview.net/forum?id=iDxfGaMYVr": {
    "title": "Continual Learning by Modeling Intra-Class Variation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Longhui Yu",
      "Tianyang Hu",
      "Lanqing HONG",
      "Zhen Liu",
      "Adrian Weller",
      "Weiyang Liu"
    ]
  },
  "https://openreview.net/forum?id=v6anjyEDVW": {
    "title": "Costs and Benefits of Fair Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Zhao"
    ]
  },
  "https://openreview.net/forum?id=kJcwlP7BRs": {
    "title": "Transfer Entropy Bottleneck: Learning Sequence to Sequence Information Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Damjan Kalajdzievski",
      "Ximeng Mao",
      "Pascal Fortier-Poisson",
      "Guillaume Lajoie",
      "Blake Aaron Richards"
    ]
  },
  "https://openreview.net/forum?id=bomdTc9HyL": {
    "title": "Transductive Decoupled Variational Inference for Few-Shot Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anuj Rajeeva Singh",
      "Hadi Jamali-Rad"
    ]
  },
  "https://openreview.net/forum?id=IvsGP7xRvm": {
    "title": "Black-Box Prompt Learning for Pre-trained Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shizhe Diao",
      "Zhichao Huang",
      "Ruijia Xu",
      "Xuechun Li",
      "LIN Yong",
      "Xiao Zhou",
      "Tong Zhang"
    ]
  },
  "https://openreview.net/forum?id=Z2L5d9ay4B": {
    "title": "Image Compression with Product Quantized Masked Image Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alaaeldin El-Nouby",
      "Matthew J. Muckley",
      "Karen Ullrich",
      "Ivan Laptev",
      "Jakob Verbeek",
      "Herve Jegou"
    ]
  },
  "https://openreview.net/forum?id=yhGCKUsKJS": {
    "title": "Action Poisoning Attacks on Linear Contextual Bandits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanlin Liu",
      "Lifeng Lai"
    ]
  },
  "https://openreview.net/forum?id=MKZyHtmfwH": {
    "title": "Mixed effects in machine learning â A flexible mixedML framework to add random effects to supervised machine learning regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pascal Kilian",
      "Sangbeak Ye",
      "Augustin Kelava"
    ]
  },
  "https://openreview.net/forum?id=fTNorIvVXG": {
    "title": "Probing Predictions on OOD Images via Nearest Categories",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yao-Yuan Yang",
      "Cyrus Rashtchian",
      "Ruslan Salakhutdinov",
      "Kamalika Chaudhuri"
    ]
  },
  "https://openreview.net/forum?id=k5m8xXTOrC": {
    "title": "Solving a Special Type of Optimal Transport Problem by a Modified Hungarian Algorithm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiling Xie",
      "Yiling Luo",
      "Xiaoming Huo"
    ]
  },
  "https://openreview.net/forum?id=WoXJFsJ6Zw": {
    "title": "AI-SARAH: Adaptive and Implicit Stochastic Recursive Gradient Methods",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheng Shi",
      "Abdurakhmon Sadiev",
      "Nicolas Loizou",
      "Peter RichtÃ¡rik",
      "Martin TakÃ¡Ä"
    ]
  },
  "https://openreview.net/forum?id=oXmwAPlbVw": {
    "title": "U-Statistics for Importance-Weighted Variational Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Javier Burroni",
      "Kenta Takatsu",
      "Justin Domke",
      "Daniel Sheldon"
    ]
  },
  "https://openreview.net/forum?id=BVi6MhKO0G": {
    "title": "OADAT: Experimental and Synthetic Clinical Optoacoustic Data for Standardized Image Processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Firat Ozdemir",
      "Berkan Lafci",
      "Xose Luis Dean-Ben",
      "Daniel Razansky",
      "Fernando Perez-Cruz"
    ]
  },
  "https://openreview.net/forum?id=mNEqiC924B": {
    "title": "Stacking Diverse Architectures to Improve Machine Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrea Schioppa",
      "Nal Kalchbrenner"
    ]
  },
  "https://openreview.net/forum?id=GbkWw3jwL9": {
    "title": "Contrastive Search Is What You Need For Neural Text Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixuan Su",
      "Nigel Collier"
    ]
  },
  "https://openreview.net/forum?id=tBl4yBEjKi": {
    "title": "Separable Self-attention for Mobile Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sachin Mehta",
      "Mohammad Rastegari"
    ]
  },
  "https://openreview.net/forum?id=iEq6lhG4O3": {
    "title": "A Flexible Nadaraya-Watson Head Can Offer Explainable and Calibrated Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alan Q. Wang",
      "Mert R. Sabuncu"
    ]
  },
  "https://openreview.net/forum?id=hHiIbk7ApW": {
    "title": "Diffusion-based Time Series Imputation and Forecasting with Structured State Space Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juan Lopez Alcaraz",
      "Nils Strodthoff"
    ]
  },
  "https://openreview.net/forum?id=oe4dl4MCGY": {
    "title": "Robust Hybrid Learning With Expert Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antoine Wehenkel",
      "Jens Behrmann",
      "Hsiang Hsu",
      "Guillermo Sapiro",
      "Gilles Louppe",
      "Joern-Henrik Jacobsen"
    ]
  },
  "https://openreview.net/forum?id=paguBNtqiO": {
    "title": "Improved Differentially Private Riemannian Optimization: Fast Sampling and Variance Reduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saiteja Utpala",
      "Andi Han",
      "Pratik Jawanpuria",
      "Bamdev Mishra"
    ]
  },
  "https://openreview.net/forum?id=lmr2WwlaFc": {
    "title": "Dirichlet Mechanism for Differentially Private KL Divergence Minimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Donlapark Ponnoprat"
    ]
  },
  "https://openreview.net/forum?id=cKsKXR28cG": {
    "title": "Regularized Training of Intermediate Layers for Generative Models for Inverse Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sean Gunn",
      "Jorio Cocola",
      "PAul HAnd"
    ]
  },
  "https://openreview.net/forum?id=6dsvH7pQHH": {
    "title": "Layerwise Bregman Representation Learning of Neural Networks with Applications to Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ehsan Amid",
      "Rohan Anil",
      "Christopher Fifty",
      "Manfred K Warmuth"
    ]
  },
  "https://openreview.net/forum?id=WN1O2MJDST": {
    "title": "Learn, Unlearn and Relearn: An Online Learning Paradigm for Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vijaya Raghavan T Ramkumar",
      "Elahe Arani",
      "Bahram Zonooz"
    ]
  },
  "https://openreview.net/forum?id=5II12ypVQo": {
    "title": "KRADA: Known-region-aware Domain Alignment for Open-set Domain Adaptation in Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenhong Zhou",
      "Feng Liu",
      "Chen Gong",
      "Rongfei Zeng",
      "Tongliang Liu",
      "William Cheung",
      "Bo Han"
    ]
  },
  "https://openreview.net/forum?id=gZna3IiGfl": {
    "title": "Mean-field analysis for heavy ball methods: Dropout-stability, connectivity, and global convergence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Diyuan Wu",
      "Vyacheslav Kungurtsev",
      "Marco Mondelli"
    ]
  },
  "https://openreview.net/forum?id=RZveYHgZbu": {
    "title": "Signed Graph Neural Networks: A Frequency Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rahul Singh",
      "Yongxin Chen"
    ]
  },
  "https://openreview.net/forum?id=TNocbXm5MZ": {
    "title": "Guaranteed Discovery of Control-Endogenous Latent States with Multi-Step Inverse Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alex Lamb",
      "Riashat Islam",
      "Yonathan Efroni",
      "Aniket Rajiv Didolkar",
      "Dipendra Misra",
      "Dylan J Foster",
      "Lekan P Molu",
      "Rajan Chari",
      "Akshay Krishnamurthy",
      "John Langford"
    ]
  },
  "https://openreview.net/forum?id=rm0zIzlhcX": {
    "title": "Beyond Intuition: Rethinking Token Attributions inside Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiamin Chen",
      "Xuhong Li",
      "Lei Yu",
      "Dejing Dou",
      "Haoyi Xiong"
    ]
  },
  "https://openreview.net/forum?id=KQRv0O8iW4": {
    "title": "Finite-Time Analysis of Decentralized Single-Timescale Actor-Critic",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "qijun luo",
      "Xiao Li"
    ]
  },
  "https://openreview.net/forum?id=GcO6ugrLKp": {
    "title": "Supervised Feature Selection with Neuron Evolution in Sparse Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zahra Atashgahi",
      "Xuhao Zhang",
      "Neil Kichler",
      "Shiwei Liu",
      "Lu Yin",
      "Mykola Pechenizkiy",
      "Raymond Veldhuis",
      "Decebal Constantin Mocanu"
    ]
  },
  "https://openreview.net/forum?id=mAx8QqZ14f": {
    "title": "Differentially Private FrÃ©chet Mean on the Manifold of Symmetric Positive Definite (SPD) Matrices with log-Euclidean Metric",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saiteja Utpala",
      "Praneeth Vepakomma",
      "Nina Miolane"
    ]
  },
  "https://openreview.net/forum?id=UntUoeLwwu": {
    "title": "Tailoring to the Tails: Risk Measures for Fine-Grained Tail Sensitivity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christian FrÃ¶hlich",
      "Robert Williamson"
    ]
  },
  "https://openreview.net/forum?id=JnsGy9uWtI": {
    "title": "Controlling Neural Network Smoothness for Neural Algorithmic Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David A. Klindt"
    ]
  },
  "https://openreview.net/forum?id=vxyjTUPV24": {
    "title": "Target Propagation via Regularized Inversion for Recurrent Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vincent Roulet",
      "Zaid Harchaoui"
    ]
  },
  "https://openreview.net/forum?id=sMsGv5Kfm3": {
    "title": "Bayesian Causal Bandits with Backdoor Adjustment Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jireh Huang",
      "Qing Zhou"
    ]
  },
  "https://openreview.net/forum?id=znNITCJyTI": {
    "title": "Accelerated Quality-Diversity through Massive Parallelism",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bryan Lim",
      "Maxime Allard",
      "Luca Grillotti",
      "Antoine Cully"
    ]
  },
  "https://openreview.net/forum?id=y7RGNXhGSR": {
    "title": "BIGRoC: Boosting Image Generation via a Robust Classifier",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roy Ganz",
      "Michael Elad"
    ]
  },
  "https://openreview.net/forum?id=CUDdbTT1QC": {
    "title": "Constrained Parameter Inference as a Principle for Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nasir Ahmad",
      "Ellen Schrader",
      "Marcel van Gerven"
    ]
  },
  "https://openreview.net/forum?id=czgMCpvrDM": {
    "title": "SMILE: Sample-to-feature Mixup for Efficient Transfer Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingjian Li",
      "Haoyi Xiong",
      "Cheng-zhong Xu",
      "Dejing Dou"
    ]
  },
  "https://openreview.net/forum?id=Q6ZXm7VBFY": {
    "title": "Optimal Convergence Rates of Deep Convolutional Neural Networks: Additive Ridge Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiying Fang",
      "Guang Cheng"
    ]
  },
  "https://openreview.net/forum?id=myjAVQrRxS": {
    "title": "Dropped Scheduled Task: Mitigating Negative Transfer in Multi-task Learning using Dynamic Task Dropping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aakarsh Malhotra",
      "Mayank Vatsa",
      "Richa Singh"
    ]
  },
  "https://openreview.net/forum?id=wkecshlYxI": {
    "title": "Revisiting adversarial training for the worst-performing class",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Pethick",
      "Grigorios Chrysos",
      "Volkan Cevher"
    ]
  },
  "https://openreview.net/forum?id=JyKNuoZGux": {
    "title": "Calibrate and Debias Layer-wise Sampling for Graph Convolutional Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Chen",
      "Tianning Xu",
      "Dilek Hakkani-Tur",
      "Di Jin",
      "Yun Yang",
      "Ruoqing Zhu"
    ]
  },
  "https://openreview.net/forum?id=nAr9PhyEbQ": {
    "title": "Online Learning for Prediction via Covariance Fitting: Computation, Performance and Robustness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Osama",
      "Dave Zachariah",
      "Peter Stoica",
      "Thomas B. SchÃ¶n"
    ]
  },
  "https://openreview.net/forum?id=DzJ7JfPXkE": {
    "title": "ViViT: Curvature Access Through The Generalized Gauss-Newton's Low-Rank Structure",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Felix Dangel",
      "Lukas Tatzel",
      "Philipp Hennig"
    ]
  },
  "https://openreview.net/forum?id=EYrRzKPinA": {
    "title": "On a continuous time model of gradient descent dynamics and instability in deep learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mihaela Rosca",
      "Yan Wu",
      "Chongli Qin",
      "Benoit Dherin"
    ]
  },
  "https://openreview.net/forum?id=Lx19EyKX77": {
    "title": "Gradient-adjusted Incremental Target Propagation Provides Effective Credit Assignment in Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sander Dalm",
      "Nasir Ahmad",
      "Luca Ambrogioni",
      "Marcel van Gerven"
    ]
  },
  "https://openreview.net/forum?id=ryUHgEdWCQ": {
    "title": "Proportional Fairness in Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guojun Zhang",
      "Saber Malekmohammadi",
      "Xi Chen",
      "Yaoliang Yu"
    ]
  },
  "https://openreview.net/forum?id=56cTmVrg5w": {
    "title": "On the Role of Fixed Points of Dynamical Systems in Training Physics-Informed Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Franz M. Rohrhofer",
      "Stefan Posch",
      "Clemens GÃ¶Ãnitzer",
      "Bernhard C Geiger"
    ]
  },
  "https://openreview.net/forum?id=EiX2L4sDPG": {
    "title": "VN-Transformer: Rotation-Equivariant Attention for Vector Neurons",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Serge Assaad",
      "Carlton Downey",
      "Rami Al-Rfou'",
      "Nigamaa Nayakanti",
      "Benjamin Sapp"
    ]
  },
  "https://openreview.net/forum?id=1U0aPkBVz0": {
    "title": "lo-fi: distributed fine-tuning without communication",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mitchell Wortsman",
      "Suchin Gururangan",
      "Shen Li",
      "Ali Farhadi",
      "Ludwig Schmidt",
      "Michael Rabbat",
      "Ari S. Morcos"
    ]
  },
  "https://openreview.net/forum?id=mHSAy1n65Z": {
    "title": "Optimal Threshold Labeling for Ordinal Regression Methods",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryoya Yamasaki"
    ]
  },
  "https://openreview.net/forum?id=LTAdaRM29K": {
    "title": "Recognition Models to Learn Dynamics from Partial Observations with Neural ODEs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mona Buisson-Fenet",
      "Valery Morgenthaler",
      "Sebastian Trimpe",
      "Florent Di Meglio"
    ]
  },
  "https://openreview.net/forum?id=GzqdMrFQsE": {
    "title": "Attention Beats Concatenation for Conditioning Neural Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Rebain",
      "Mark J. Matthews",
      "Kwang Moo Yi",
      "Gopal Sharma",
      "Dmitry Lagun",
      "Andrea Tagliasacchi"
    ]
  },
  "https://openreview.net/forum?id=d3rHk4VAf0": {
    "title": "A Ranking Game for Imitation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Harshit Sikchi",
      "Akanksha Saran",
      "Wonjoon Goo",
      "Scott Niekum"
    ]
  },
  "https://openreview.net/forum?id=LfTukxzxTj": {
    "title": "Implicit Ensemble Training for Efficient and Robust Multiagent Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Macheng Shen",
      "JONATHAN P HOW"
    ]
  },
  "https://openreview.net/forum?id=hVT7SHlilx": {
    "title": "Named Tensor Notation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Chiang",
      "Alexander M Rush",
      "Boaz Barak"
    ]
  },
  "https://openreview.net/forum?id=X1pjWMCMB0": {
    "title": "PCPs: Patient Cardiac Prototypes to Probe AI-based Medical Diagnoses, Distill Datasets, and Retrieve Patients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dani Kiyasseh",
      "Tingting Zhu",
      "David A. Clifton"
    ]
  },
  "https://openreview.net/forum?id=RbLsYz1Az9": {
    "title": "On the infinite-depth limit of finite-width neural networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soufiane Hayou"
    ]
  },
  "https://openreview.net/forum?id=85BfDdYMBY": {
    "title": "Intrinsic Dimension for Large-Scale Geometric Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maximilian Stubbemann",
      "Tom Hanika",
      "Friedrich Martin Schneider"
    ]
  },
  "https://openreview.net/forum?id=Xq1sTZTQVm": {
    "title": "Beyond Information Gain: An Empirical Benchmark for Low-Switching-Cost Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shusheng Xu",
      "Yancheng Liang",
      "Yunfei Li",
      "Simon Shaolei Du",
      "Yi Wu"
    ]
  },
  "https://openreview.net/forum?id=EgHnKOLaKW": {
    "title": "DisCo: Improving Compositional Generalization in Visual Reasoning through Distribution Coverage",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joy Hsu",
      "Jiayuan Mao",
      "Jiajun Wu"
    ]
  },
  "https://openreview.net/forum?id=RA0TDqt3hC": {
    "title": "Hidden Heterogeneity: When to Choose Similarity-Based Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kiri L. Wagstaff",
      "Thomas G Dietterich"
    ]
  },
  "https://openreview.net/forum?id=zKnqZeUCLO": {
    "title": "PolyViT: Co-training Vision Transformers on Images, Videos and Audio",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Valerii Likhosherstov",
      "Anurag Arnab",
      "Krzysztof Marcin Choromanski",
      "Mario Lucic",
      "Yi Tay",
      "Mostafa Dehghani"
    ]
  },
  "https://openreview.net/forum?id=lheUXtDNvP": {
    "title": "GSR: A Generalized Symbolic Regression Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tony Tohme",
      "Dehong Liu",
      "KAMAL YOUCEF-TOUMI"
    ]
  },
  "https://openreview.net/forum?id=jbZEUtULft": {
    "title": "Bounding generalization error with input compression: An empirical study with infinite-width networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Angus Galloway",
      "Anna Golubeva",
      "Mahmoud Salem",
      "Mihai Nica",
      "Yani Ioannou",
      "Graham W. Taylor"
    ]
  },
  "https://openreview.net/forum?id=wIXHG8LZ2w": {
    "title": "Learning Representations for Pixel-based Control: What Matters and Why?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manan Tomar",
      "Utkarsh Aashu Mishra",
      "Amy Zhang",
      "Matthew E. Taylor"
    ]
  },
  "https://openreview.net/forum?id=MzWgBjZ6Le": {
    "title": "FedDAG: Federated DAG Structure Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Erdun Gao",
      "Junjia Chen",
      "Li Shen",
      "Tongliang Liu",
      "Mingming Gong",
      "Howard Bondell"
    ]
  },
  "https://openreview.net/forum?id=tnRRHzZPMq": {
    "title": "Communication-Efficient Distributionally Robust Decentralized Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matteo Zecchin",
      "Marios Kountouris",
      "David Gesbert"
    ]
  },
  "https://openreview.net/forum?id=GRBbtkW3Lp": {
    "title": "EdiBERT: a generative model for image editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thibaut Issenhuth",
      "Ugo Tanielian",
      "Jeremie Mary",
      "David Picard"
    ]
  },
  "https://openreview.net/forum?id=2wWJxtpFer": {
    "title": "OpenCon: Open-world Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiyou Sun",
      "Yixuan Li"
    ]
  },
  "https://openreview.net/forum?id=fjkN5Ur2d6": {
    "title": "Linking Neural Collapse and L2 Normalization with Improved Out-of-Distribution Detection in Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jarrod Haas",
      "William Yolland",
      "Bernhard T Rabus"
    ]
  },
  "https://openreview.net/forum?id=Grhi800jVz": {
    "title": "Euclidean-Norm-Induced Schatten-p Quasi-Norm Regularization for Low-Rank Tensor Completion and Tensor Robust Principal Component Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jicong Fan",
      "Lijun Ding",
      "Chengrun Yang",
      "Zhao Zhang",
      "Madeleine Udell"
    ]
  },
  "https://openreview.net/forum?id=TGuXXlbKsn": {
    "title": "Benchmarks and Algorithms for Offline Preference-Based Reward Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Shin",
      "Anca Dragan",
      "Daniel S. Brown"
    ]
  },
  "https://openreview.net/forum?id=mrTXGDZns2": {
    "title": "Fairness and robustness in anti-causal prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maggie Makar",
      "Alexander D'Amour"
    ]
  },
  "https://openreview.net/forum?id=bx24KpJ4Eb": {
    "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stephen Casper",
      "Xander Davies",
      "Claudia Shi",
      "Thomas Krendl Gilbert",
      "JÃ©rÃ©my Scheurer",
      "Javier Rando",
      "Rachel Freedman",
      "Tomek Korbak",
      "David Lindner",
      "Pedro Freire",
      "Tony Tong Wang",
      "Samuel Marks",
      "Charbel-Raphael Segerie",
      "Micah Carroll",
      "Andi Peng",
      "Phillip J.K. Christoffersen",
      "Mehul Damani",
      "Stewart Slocum",
      "Usman Anwar",
      "Anand Siththaranjan",
      "Max Nadeau",
      "Eric J Michaud",
      "Jacob Pfau",
      "Dmitrii Krasheninnikov",
      "Xin Chen",
      "Lauro Langosco",
      "Peter Hase",
      "Erdem Biyik",
      "Anca Dragan",
      "David Krueger",
      "Dorsa Sadigh",
      "Dylan Hadfield-Menell"
    ]
  },
  "https://openreview.net/forum?id=ed8SkMdYFT": {
    "title": "Understanding the robustness difference between stochastic gradient descent and adaptive gradient methods",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Avery Ma",
      "Yangchen Pan",
      "Amir-massoud Farahmand"
    ]
  },
  "https://openreview.net/forum?id=ioFIAQOBOS": {
    "title": "Learning to reconstruct signals from binary measurements alone",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "JuliÃ¡n Tachella",
      "Laurent Jacques"
    ]
  },
  "https://openreview.net/forum?id=DwgRm72GQF": {
    "title": "Inverse Scaling: When Bigger Isn't Better",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ian R. McKenzie",
      "Alexander Lyzhov",
      "Michael Martin Pieler",
      "Alicia Parrish",
      "Aaron Mueller",
      "Ameya Prabhu",
      "Euan McLean",
      "Xudong Shen",
      "Joe Cavanagh",
      "Andrew George Gritsevskiy",
      "Derik Kauffman",
      "Aaron T. Kirtland",
      "Zhengping Zhou",
      "Yuhui Zhang",
      "Sicong Huang",
      "Daniel Wurgaft",
      "Max Weiss",
      "Alexis Ross",
      "Gabriel Recchia",
      "Alisa Liu",
      "Jiacheng Liu",
      "Tom Tseng",
      "Tomasz Korbak",
      "Najoung Kim",
      "Samuel R. Bowman",
      "Ethan Perez"
    ]
  },
  "https://openreview.net/forum?id=a7nvXxNmdV": {
    "title": "Improved baselines for vision-language pre-training",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Enrico Fini",
      "Pietro Astolfi",
      "Adriana Romero-Soriano",
      "Jakob Verbeek",
      "Michal Drozdzal"
    ]
  },
  "https://openreview.net/forum?id=UIalYAHdBH": {
    "title": "On the Sample Complexity of Lipschitz Constant Estimation",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julien Walden Huang",
      "Stephen J. Roberts",
      "Jan-Peter Calliess"
    ]
  },
  "https://openreview.net/forum?id=XXfEmIMJDm": {
    "title": "Achieving the Pareto Frontier of Regret Minimization and Best Arm Identification in Multi-Armed Bandits",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixin Zhong",
      "Wang Chi Cheung",
      "Vincent Tan"
    ]
  },
  "https://openreview.net/forum?id=ivCd8z8zR2": {
    "title": "High Fidelity Neural Audio Compression",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexandre DÃ©fossez",
      "Jade Copet",
      "Gabriel Synnaeve",
      "Yossi Adi"
    ]
  },
  "https://openreview.net/forum?id=EGQSpkUDdD": {
    "title": "AP: Selective Activation for De-sparsifying Pruned Networks",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiyu Liu",
      "Rohan Ghosh",
      "Mehul Motani"
    ]
  },
  "https://openreview.net/forum?id=mvftzofTYQ": {
    "title": "WOODS: Benchmarks for Out-of-Distribution Generalization in Time Series",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jean-Christophe Gagnon-Audet",
      "Kartik Ahuja",
      "Mohammad Javad Darvishi Bayazi",
      "Pooneh Mousavi",
      "Guillaume Dumas",
      "Irina Rish"
    ]
  },
  "https://openreview.net/forum?id=iO4LZibEqW": {
    "title": "Holistic Evaluation of Language Models",
    "volume": "outstanding",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Percy Liang",
      "Rishi Bommasani",
      "Tony Lee",
      "Dimitris Tsipras",
      "Dilara Soylu",
      "Michihiro Yasunaga",
      "Yian Zhang",
      "Deepak Narayanan",
      "Yuhuai Wu",
      "Ananya Kumar",
      "Benjamin Newman",
      "Binhang Yuan",
      "Bobby Yan",
      "Ce Zhang",
      "Christian Cosgrove",
      "Christopher D Manning",
      "Christopher Re",
      "Diana Acosta-Navas",
      "Drew A. Hudson",
      "Eric Zelikman",
      "Esin Durmus",
      "Faisal Ladhak",
      "Frieda Rong",
      "Hongyu Ren",
      "Huaxiu Yao",
      "Jue WANG",
      "Keshav Santhanam",
      "Laurel Orr",
      "Lucia Zheng",
      "Mert Yuksekgonul",
      "Mirac Suzgun",
      "Nathan Kim",
      "Neel Guha",
      "Niladri S. Chatterji",
      "Omar Khattab",
      "Peter Henderson",
      "Qian Huang",
      "Ryan Andrew Chi",
      "Sang Michael Xie",
      "Shibani Santurkar",
      "Surya Ganguli",
      "Tatsunori Hashimoto",
      "Thomas Icard",
      "Tianyi Zhang",
      "Vishrav Chaudhary",
      "William Wang",
      "Xuechen Li",
      "Yifan Mai",
      "Yuhui Zhang",
      "Yuta Koreeda"
    ]
  },
  "https://openreview.net/forum?id=yrkJGne0vN": {
    "title": "Neural Ordinary Differential Equations for Modeling Epidemic Spreading",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chrysoula Kosma",
      "Giannis Nikolentzos",
      "George Panagopoulos",
      "Jean-Marc Steyaert",
      "Michalis Vazirgiannis"
    ]
  },
  "https://openreview.net/forum?id=2mZSlQscj3": {
    "title": "Neural Monge Map estimation and its applications",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaojiao Fan",
      "Shu Liu",
      "Shaojun Ma",
      "Hao-Min Zhou",
      "Yongxin Chen"
    ]
  },
  "https://openreview.net/forum?id=igdWKxK5RZ": {
    "title": "Finding and Only Finding Differential Nash Equilibria by Both Pretending to be a Follower",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuchan Bao",
      "Guodong Zhang"
    ]
  },
  "https://openreview.net/forum?id=25G63lDHV2": {
    "title": "Efficient Reward Poisoning Attacks on Online Deep Reinforcement Learning",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinglun Xu",
      "Qi Zeng",
      "Gagandeep Singh"
    ]
  },
  "https://openreview.net/forum?id=VmyFF5lL3F": {
    "title": "Inversion by Direct Iteration: An Alternative to Denoising Diffusion for Image Restoration",
    "volume": "outstanding",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mauricio Delbracio",
      "Peyman Milanfar"
    ]
  },
  "https://openreview.net/forum?id=vXSsTYs6ZB": {
    "title": "LEAD: Min-Max Optimization from a Physical Perspective",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reyhane Askari Hemmat",
      "Amartya Mitra",
      "Guillaume Lajoie",
      "Ioannis Mitliagkas"
    ]
  },
  "https://openreview.net/forum?id=XNFo3dQiCJ": {
    "title": "Generalizability of Adversarial Robustness Under Distribution Shifts",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kumail Alhamoud",
      "Hasan Abed Al Kader Hammoud",
      "Motasem Alfarra",
      "Bernard Ghanem"
    ]
  },
  "https://openreview.net/forum?id=r9vGSpbbRO": {
    "title": "Attacking Perceptual Similarity Metrics",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhijay Ghildyal",
      "Feng Liu"
    ]
  },
  "https://openreview.net/forum?id=uyTL5Bvosj": {
    "title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aarohi Srivastava",
      "Abhinav Rastogi",
      "Abhishek Rao",
      "Abu Awal Md Shoeb",
      "Abubakar Abid",
      "Adam Fisch",
      "Adam R. Brown",
      "Adam Santoro",
      "Aditya Gupta",
      "AdriÃ  Garriga-Alonso",
      "Agnieszka Kluska",
      "Aitor Lewkowycz",
      "Akshat Agarwal",
      "Alethea Power",
      "Alex Ray",
      "Alex Warstadt",
      "Alexander W. Kocurek",
      "Ali Safaya",
      "Ali Tazarv",
      "Alice Xiang",
      "Alicia Parrish",
      "Allen Nie",
      "Aman Hussain",
      "Amanda Askell",
      "Amanda Dsouza",
      "Ambrose Slone",
      "Ameet Rahane",
      "Anantharaman S. Iyer",
      "Anders Johan Andreassen",
      "Andrea Madotto",
      "Andrea Santilli",
      "Andreas StuhlmÃ¼ller",
      "Andrew M. Dai",
      "Andrew La",
      "Andrew Kyle Lampinen",
      "Andy Zou",
      "Angela Jiang",
      "Angelica Chen",
      "Anh Vuong",
      "Animesh Gupta",
      "Anna Gottardi",
      "Antonio Norelli",
      "Anu Venkatesh",
      "Arash Gholamidavoodi",
      "Arfa Tabassum",
      "Arul Menezes",
      "Arun Kirubarajan",
      "Asher Mullokandov",
      "Ashish Sabharwal",
      "Austin Herrick",
      "Avia Efrat",
      "Aykut Erdem",
      "Ayla KarakaÅ",
      "B. Ryan Roberts",
      "Bao Sheng Loe",
      "Barret Zoph",
      "BartÅomiej Bojanowski",
      "Batuhan Ãzyurt",
      "Behnam Hedayatnia",
      "Behnam Neyshabur",
      "Benjamin Inden",
      "Benno Stein",
      "Berk Ekmekci",
      "Bill Yuchen Lin",
      "Blake Howald",
      "Bryan Orinion",
      "Cameron Diao",
      "Cameron Dour",
      "Catherine Stinson",
      "Cedrick Argueta",
      "Cesar Ferri",
      "Chandan Singh",
      "Charles Rathkopf",
      "Chenlin Meng",
      "Chitta Baral",
      "Chiyu Wu",
      "Chris Callison-Burch",
      "Christopher Waites",
      "Christian Voigt",
      "Christopher D Manning",
      "Christopher Potts",
      "Cindy Ramirez",
      "Clara E. Rivera",
      "Clemencia Siro",
      "Colin Raffel",
      "Courtney Ashcraft",
      "Cristina Garbacea",
      "Damien Sileo",
      "Dan Garrette",
      "Dan Hendrycks",
      "Dan Kilman",
      "Dan Roth",
      "C. Daniel Freeman",
      "Daniel Khashabi",
      "Daniel Levy",
      "Daniel MoseguÃ­ GonzÃ¡lez",
      "Danielle Perszyk",
      "Danny Hernandez",
      "Danqi Chen",
      "Daphne Ippolito",
      "Dar Gilboa",
      "David Dohan",
      "David Drakard",
      "David Jurgens",
      "Debajyoti Datta",
      "Deep Ganguli",
      "Denis Emelin",
      "Denis Kleyko",
      "Deniz Yuret",
      "Derek Chen",
      "Derek Tam",
      "Dieuwke Hupkes",
      "Diganta Misra",
      "Dilyar Buzan",
      "Dimitri Coelho Mollo",
      "Diyi Yang",
      "Dong-Ho Lee",
      "Dylan Schrader",
      "Ekaterina Shutova",
      "Ekin Dogus Cubuk",
      "Elad Segal",
      "Eleanor Hagerman",
      "Elizabeth Barnes",
      "Elizabeth Donoway",
      "Ellie Pavlick",
      "Emanuele RodolÃ ",
      "Emma Lam",
      "Eric Chu",
      "Eric Tang",
      "Erkut Erdem",
      "Ernie Chang",
      "Ethan A Chi",
      "Ethan Dyer",
      "Ethan Jerzak",
      "Ethan Kim",
      "Eunice Engefu Manyasi",
      "Evgenii Zheltonozhskii",
      "Fanyue Xia",
      "Fatemeh Siar",
      "Fernando MartÃ­nez-Plumed",
      "Francesca HappÃ©",
      "Francois Chollet",
      "Frieda Rong",
      "Gaurav Mishra",
      "Genta Indra Winata",
      "Gerard de Melo",
      "GermÃ n Kruszewski",
      "Giambattista Parascandolo",
      "Giorgio Mariani",
      "Gloria Xinyue Wang",
      "Gonzalo Jaimovitch-Lopez",
      "Gregor Betz",
      "Guy Gur-Ari",
      "Hana Galijasevic",
      "Hannah Kim",
      "Hannah Rashkin",
      "Hannaneh Hajishirzi",
      "Harsh Mehta",
      "Hayden Bogar",
      "Henry Francis Anthony Shevlin",
      "Hinrich Schuetze",
      "Hiromu Yakura",
      "Hongming Zhang",
      "Hugh Mee Wong",
      "Ian Ng",
      "Isaac Noble",
      "Jaap Jumelet",
      "Jack Geissinger",
      "Jackson Kernion",
      "Jacob Hilton",
      "Jaehoon Lee",
      "Jaime FernÃ¡ndez Fisac",
      "James B Simon",
      "James Koppel",
      "James Zheng",
      "James Zou",
      "Jan Kocon",
      "Jana Thompson",
      "Janelle Wingfield",
      "Jared Kaplan",
      "Jarema Radom",
      "Jascha Sohl-Dickstein",
      "Jason Phang",
      "Jason Wei",
      "Jason Yosinski",
      "Jekaterina Novikova",
      "Jelle Bosscher",
      "Jennifer Marsh",
      "Jeremy Kim",
      "Jeroen Taal",
      "Jesse Engel",
      "Jesujoba Alabi",
      "Jiacheng Xu",
      "Jiaming Song",
      "Jillian Tang",
      "Joan Waweru",
      "John Burden",
      "John Miller",
      "John U. Balis",
      "Jonathan Batchelder",
      "Jonathan Berant",
      "JÃ¶rg Frohberg",
      "Jos Rozen",
      "Jose Hernandez-Orallo",
      "Joseph Boudeman",
      "Joseph Guerr",
      "Joseph Jones",
      "Joshua B. Tenenbaum",
      "Joshua S. Rule",
      "Joyce Chua",
      "Kamil Kanclerz",
      "Karen Livescu",
      "Karl Krauth",
      "Karthik Gopalakrishnan",
      "Katerina Ignatyeva",
      "Katja Markert",
      "Kaustubh Dhole",
      "Kevin Gimpel",
      "Kevin Omondi",
      "Kory Wallace Mathewson",
      "Kristen Chiafullo",
      "Ksenia Shkaruta",
      "Kumar Shridhar",
      "Kyle McDonell",
      "Kyle Richardson",
      "Laria Reynolds",
      "Leo Gao",
      "Li Zhang",
      "Liam Dugan",
      "Lianhui Qin",
      "Lidia Contreras-Ochando",
      "Louis-Philippe Morency",
      "Luca Moschella",
      "Lucas Lam",
      "Lucy Noble",
      "Ludwig Schmidt",
      "Luheng He",
      "Luis Oliveros-ColÃ³n",
      "Luke Metz",
      "LÃ¼tfi Kerem Senel",
      "Maarten Bosma",
      "Maarten Sap",
      "Maartje Ter Hoeve",
      "Maheen Farooqi",
      "Manaal Faruqui",
      "Mantas Mazeika",
      "Marco Baturan",
      "Marco Marelli",
      "Marco Maru",
      "Maria Jose Ramirez-Quintana",
      "Marie Tolkiehn",
      "Mario Giulianelli",
      "Martha Lewis",
      "Martin Potthast",
      "Matthew L Leavitt",
      "Matthias Hagen",
      "MÃ¡tyÃ¡s Schubert",
      "Medina Orduna Baitemirova",
      "Melody Arnaud",
      "Melvin McElrath",
      "Michael Andrew Yee",
      "Michael Cohen",
      "Michael Gu",
      "Michael Ivanitskiy",
      "Michael Starritt",
      "Michael Strube",
      "MichaÅ SwÄdrowski",
      "Michele Bevilacqua",
      "Michihiro Yasunaga",
      "Mihir Kale",
      "Mike Cain",
      "Mimee Xu",
      "Mirac Suzgun",
      "Mitch Walker",
      "Mo Tiwari",
      "Mohit Bansal",
      "Moin Aminnaseri",
      "Mor Geva",
      "Mozhdeh Gheini",
      "Mukund Varma T",
      "Nanyun Peng",
      "Nathan Andrew Chi",
      "Nayeon Lee",
      "Neta Gur-Ari Krakover",
      "Nicholas Cameron",
      "Nicholas Roberts",
      "Nick Doiron",
      "Nicole Martinez",
      "Nikita Nangia",
      "Niklas Deckers",
      "Niklas Muennighoff",
      "Nitish Shirish Keskar",
      "Niveditha S. Iyer",
      "Noah Constant",
      "Noah Fiedel",
      "Nuan Wen",
      "Oliver Zhang",
      "Omar Agha",
      "Omar Elbaghdadi",
      "Omer Levy",
      "Owain Evans",
      "Pablo Antonio Moreno Casares",
      "Parth Doshi",
      "Pascale Fung",
      "Paul Pu Liang",
      "Paul Vicol",
      "Pegah Alipoormolabashi",
      "Peiyuan Liao",
      "Percy Liang",
      "Peter W Chang",
      "Peter Eckersley",
      "Phu Mon Htut",
      "Pinyu Hwang",
      "Piotr MiÅkowski",
      "Piyush Patil",
      "Pouya Pezeshkpour",
      "Priti Oli",
      "Qiaozhu Mei",
      "Qing Lyu",
      "Qinlang Chen",
      "Rabin Banjade",
      "Rachel Etta Rudolph",
      "Raefer Gabriel",
      "Rahel Habacker",
      "Ramon Risco",
      "RaphaÃ«l MilliÃ¨re",
      "Rhythm Garg",
      "Richard Barnes",
      "Rif A. Saurous",
      "Riku Arakawa",
      "Robbe Raymaekers",
      "Robert Frank",
      "Rohan Sikand",
      "Roman Novak",
      "Roman Sitelew",
      "Ronan Le Bras",
      "Rosanne Liu",
      "Rowan Jacobs",
      "Rui Zhang",
      "Russ Salakhutdinov",
      "Ryan Andrew Chi",
      "Seungjae Ryan Lee",
      "Ryan Stovall",
      "Ryan Teehan",
      "Rylan Yang",
      "Sahib Singh",
      "Saif M. Mohammad",
      "Sajant Anand",
      "Sam Dillavou",
      "Sam Shleifer",
      "Sam Wiseman",
      "Samuel Gruetter",
      "Samuel R. Bowman",
      "Samuel Stern Schoenholz",
      "Sanghyun Han",
      "Sanjeev Kwatra",
      "Sarah A. Rous",
      "Sarik Ghazarian",
      "Sayan Ghosh",
      "Sean Casey",
      "Sebastian Bischoff",
      "Sebastian Gehrmann",
      "Sebastian Schuster",
      "Sepideh Sadeghi",
      "Shadi Hamdan",
      "Sharon Zhou",
      "Shashank Srivastava",
      "Sherry Shi",
      "Shikhar Singh",
      "Shima Asaadi",
      "Shixiang Shane Gu",
      "Shubh Pachchigar",
      "Shubham Toshniwal",
      "Shyam Upadhyay",
      "Shyamolima Shammie Debnath",
      "Siamak Shakeri",
      "Simon Thormeyer",
      "Simone Melzi",
      "Siva Reddy",
      "Sneha Priscilla Makini",
      "Soo-Hwan Lee",
      "Spencer Torene",
      "Sriharsha Hatwar",
      "Stanislas Dehaene",
      "Stefan Divic",
      "Stefano Ermon",
      "Stella Biderman",
      "Stephanie Lin",
      "Stephen Prasad",
      "Steven Piantadosi",
      "Stuart Shieber",
      "Summer Misherghi",
      "Svetlana Kiritchenko",
      "Swaroop Mishra",
      "Tal Linzen",
      "Tal Schuster",
      "Tao Li",
      "Tao Yu",
      "Tariq Ali",
      "Tatsunori Hashimoto",
      "Te-Lin Wu",
      "ThÃ©o Desbordes",
      "Theodore Rothschild",
      "Thomas Phan",
      "Tianle Wang",
      "Tiberius Nkinyili",
      "Timo Schick",
      "Timofei Kornev",
      "Titus Tunduny",
      "Tobias Gerstenberg",
      "Trenton Chang",
      "Trishala Neeraj",
      "Tushar Khot",
      "Tyler Shultz",
      "Uri Shaham",
      "Vedant Misra",
      "Vera Demberg",
      "Victoria Nyamai",
      "Vikas Raunak",
      "Vinay Venkatesh Ramasesh",
      "vinay uday prabhu",
      "Vishakh Padmakumar",
      "Vivek Srikumar",
      "William Fedus",
      "William Saunders",
      "William Zhang",
      "Wout Vossen",
      "Xiang Ren",
      "Xiaoyu Tong",
      "Xinran Zhao",
      "Xinyi Wu",
      "Xudong Shen",
      "Yadollah Yaghoobzadeh",
      "Yair Lakretz",
      "Yangqiu Song",
      "Yasaman Bahri",
      "Yejin Choi",
      "Yichi Yang",
      "Sophie Hao",
      "Yifu Chen",
      "Yonatan Belinkov",
      "Yu Hou",
      "Yufang Hou",
      "Yuntao Bai",
      "Zachary Seid",
      "Zhuoye Zhao",
      "Zijian Wang",
      "Zijie J. Wang",
      "Zirui Wang",
      "Ziyi Wu"
    ]
  },
  "https://openreview.net/forum?id=KgfFAI9f3E": {
    "title": "Identification of Negative Transfers in Multitask Learning Using Surrogate Models",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongyue Li",
      "Huy Nguyen",
      "Hongyang Ryan Zhang"
    ]
  },
  "https://openreview.net/forum?id=rAnB7JSMXL": {
    "title": "Patches Are All You Need?",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Asher Trockman",
      "J Zico Kolter"
    ]
  },
  "https://openreview.net/forum?id=11osftjEbF": {
    "title": "Numerical Accounting in the Shuffle Model of Differential Privacy",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antti Koskela",
      "Mikko A. HeikkilÃ¤",
      "Antti Honkela"
    ]
  },
  "https://openreview.net/forum?id=L9othQvPks": {
    "title": "Workflow Discovery from Dialogues in the Low Data Regime",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amine El hattami",
      "Issam H. Laradji",
      "Stefania Raimondo",
      "David Vazquez",
      "Pau Rodriguez",
      "Christopher Pal"
    ]
  },
  "https://openreview.net/forum?id=JwDpZSv3yz": {
    "title": "SPADE: Semi-supervised Anomaly Detection under Distribution Mismatch",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinsung Yoon",
      "Kihyuk Sohn",
      "Chun-Liang Li",
      "Sercan O Arik",
      "Tomas Pfister"
    ]
  },
  "https://openreview.net/forum?id=ZR2CDgADRo": {
    "title": "SolidGen: An Autoregressive Model for Direct B-rep Synthesis",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pradeep Kumar Jayaraman",
      "Joseph George Lambourne",
      "Nishkrit Desai",
      "Karl Willis",
      "Aditya Sanghi",
      "Nigel J. W. Morris"
    ]
  },
  "https://openreview.net/forum?id=KoFOg41haE": {
    "title": "StarCoder: may the source be with you!",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raymond Li",
      "Loubna Ben allal",
      "Yangtian Zi",
      "Niklas Muennighoff",
      "Denis Kocetkov",
      "Chenghao Mou",
      "Marc Marone",
      "Christopher Akiki",
      "Jia LI",
      "Jenny Chim",
      "Qian Liu",
      "Evgenii Zheltonozhskii",
      "Terry Yue Zhuo",
      "Thomas Wang",
      "Olivier Dehaene",
      "Joel Lamy-Poirier",
      "Joao Monteiro",
      "Nicolas Gontier",
      "Ming-Ho Yee",
      "Logesh Kumar Umapathi",
      "Jian Zhu",
      "Ben Lipkin",
      "Muhtasham Oblokulov",
      "Zhiruo Wang",
      "Rudra Murthy",
      "Jason T Stillerman",
      "Siva Sankalp Patel",
      "Dmitry Abulkhanov",
      "Marco Zocca",
      "Manan Dey",
      "Zhihan Zhang",
      "Urvashi Bhattacharyya",
      "Wenhao Yu",
      "Sasha Luccioni",
      "Paulo Villegas",
      "Fedor Zhdanov",
      "Tony Lee",
      "Nadav Timor",
      "Jennifer Ding",
      "Claire S Schlesinger",
      "Hailey Schoelkopf",
      "Jan Ebert",
      "Tri Dao",
      "Mayank Mishra",
      "Alex Gu",
      "Carolyn Jane Anderson",
      "Brendan Dolan-Gavitt",
      "Danish Contractor",
      "Siva Reddy",
      "Daniel Fried",
      "Dzmitry Bahdanau",
      "Yacine Jernite",
      "Carlos MuÃ±oz Ferrandis",
      "Sean Hughes",
      "Thomas Wolf",
      "Arjun Guha",
      "Leandro Von Werra",
      "Harm de Vries"
    ]
  },
  "https://openreview.net/forum?id=vlY9GDCCA6": {
    "title": "PAVI: Plate-Amortized Variational Inference",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Louis Rouillard",
      "Alexandre Le Bris",
      "Thomas Moreau",
      "Demian Wassermann"
    ]
  },
  "https://openreview.net/forum?id=1dwXa9vmOI": {
    "title": "Does âDeep Learning on a Data Diet' reproduce? Overall yes, but GraNd at Initialization does not",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andreas Kirsch"
    ]
  },
  "https://openreview.net/forum?id=KqR3rgooXb": {
    "title": "Numerical Data Imputation for Multimodal Data Sets: A Probabilistic Nearest-Neighbor Kernel Density Approach",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Florian Lalande",
      "Kenji Doya"
    ]
  },
  "https://openreview.net/forum?id=R9CgBkeZ6Z": {
    "title": "Aux-Drop: Handling Haphazard Inputs in Online Learning Using Auxiliary Dropouts",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rohit Agarwal",
      "Deepak Gupta",
      "Alexander Horsch",
      "Dilip K. Prasad"
    ]
  },
  "https://openreview.net/forum?id=jWr41htaB3": {
    "title": "A Stochastic Proximal Polyak Step Size",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fabian Schaipp",
      "Robert M. Gower",
      "Michael Ulbrich"
    ]
  },
  "https://openreview.net/forum?id=RYeRNwRjNE": {
    "title": "Explaining Visual Counterfactual Explainers",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Diego Velazquez",
      "Pau Rodriguez",
      "Alexandre Lacoste",
      "Issam H. Laradji",
      "Xavier Roca",
      "Jordi GonzÃ lez"
    ]
  },
  "https://openreview.net/forum?id=JjbsIYOuNi": {
    "title": "PRUDEX-Compass: Towards Systematic Evaluation of Reinforcement Learning in Financial Markets",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuo Sun",
      "Molei Qin",
      "Xinrun Wang",
      "Bo An"
    ]
  },
  "https://openreview.net/forum?id=AXtFeYjboj": {
    "title": "A Survey on the Possibilities & Impossibilities of AI-generated Text Detection",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soumya Suvra Ghosal",
      "Souradip Chakraborty",
      "Jonas Geiping",
      "Furong Huang",
      "Dinesh Manocha",
      "Amrit Bedi"
    ]
  },
  "https://openreview.net/forum?id=z9EkXfvxta": {
    "title": "Modular Deep Learning",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonas Pfeiffer",
      "Sebastian Ruder",
      "Ivan VuliÄ",
      "Edoardo Ponti"
    ]
  },
  "https://openreview.net/forum?id=cHroS8VIyN": {
    "title": "Benchmarks for Physical Reasoning AI",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrew Melnik",
      "Robin Schiewer",
      "Moritz Lange",
      "Andrei Ioan Muresanu",
      "mozhgan saeidi",
      "Animesh Garg",
      "Helge Ritter"
    ]
  },
  "https://openreview.net/forum?id=qqnttX9LPo": {
    "title": "Causal Reinforcement Learning: A Survey",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihong Deng",
      "Jing Jiang",
      "Guodong Long",
      "Chengqi Zhang"
    ]
  },
  "https://openreview.net/forum?id=mcN0ezbnzO": {
    "title": "Provably Safe Reinforcement Learning: Conceptual Analysis, Survey, and Benchmarking",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanna Krasowski",
      "Jakob Thumm",
      "Marlon MÃ¼ller",
      "Lukas SchÃ¤fer",
      "Xiao Wang",
      "Matthias Althoff"
    ]
  },
  "https://openreview.net/forum?id=9sVCIngrhP": {
    "title": "Private GANs, Revisited",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alex Bie",
      "Gautam Kamath",
      "Guojun Zhang"
    ]
  },
  "https://openreview.net/forum?id=r30yuDPvf2": {
    "title": "A Survey on Transformers in Reinforcement Learning",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenzhe Li",
      "Hao Luo",
      "Zichuan Lin",
      "Chongjie Zhang",
      "Zongqing Lu",
      "Deheng Ye"
    ]
  },
  "https://openreview.net/forum?id=YdMrdhGx9y": {
    "title": "A Survey on Causal Discovery Methods for I.I.D. and Time Series Data",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Uzma Hasan",
      "Emam Hossain",
      "Md Osman Gani"
    ]
  },
  "https://openreview.net/forum?id=lmXMXP74TO": {
    "title": "Data Distillation: A Survey",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Noveen Sachdeva",
      "Julian McAuley"
    ]
  },
  "https://openreview.net/forum?id=3OSISBQPrM": {
    "title": "On the Predictive Accuracy of Neural Temporal Point Process Models for Continuous-time Event Data",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tanguy Bosser",
      "Souhaib Ben Taieb"
    ]
  },
  "https://openreview.net/forum?id=jh7wH2AzKK": {
    "title": "Augmented Language Models: a Survey",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "GrÃ©goire Mialon",
      "Roberto Dessi",
      "Maria Lomeli",
      "Christoforos Nalmpantis",
      "Ramakanth Pasunuru",
      "Roberta Raileanu",
      "Baptiste Roziere",
      "Timo Schick",
      "Jane Dwivedi-Yu",
      "Asli Celikyilmaz",
      "Edouard Grave",
      "Yann LeCun",
      "Thomas Scialom"
    ]
  },
  "https://openreview.net/forum?id=FByH3qL87G": {
    "title": "On Averaging ROC Curves",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jack Hogan",
      "Niall M. Adams"
    ]
  },
  "https://openreview.net/forum?id=VynY6Bk03b": {
    "title": "How to Reuse and Compose Knowledge for a Lifetime of Tasks: A Survey on Continual Learning and Functional Composition",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jorge A Mendez",
      "ERIC EATON"
    ]
  },
  "https://openreview.net/forum?id=Ma25S4ludQ": {
    "title": "Know Your Self-supervised Learning: A Survey on Image-based Generative and Discriminative Training",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Utku Ozbulak",
      "Hyun Jung Lee",
      "Beril Boga",
      "Esla Timothy Anzaku",
      "Ho-min Park",
      "Arnout Van Messem",
      "Wesley De Neve",
      "Joris Vankerschaver"
    ]
  },
  "https://openreview.net/forum?id=A8pqQipwkt": {
    "title": "Forces are not Enough: Benchmark and Critical Evaluation for Machine Learning Force Fields with Molecular Simulations",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Fu",
      "Zhenghao Wu",
      "Wujie Wang",
      "Tian Xie",
      "Sinan Keten",
      "Rafael Gomez-Bombarelli",
      "Tommi Jaakkola"
    ]
  },
  "https://openreview.net/forum?id=e0xaRylNuT": {
    "title": "Partition-Based Active Learning for Graph Neural Networks",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi Ma",
      "Ziqiao Ma",
      "Joyce Chai",
      "Qiaozhu Mei"
    ]
  },
  "https://openreview.net/forum?id=AU4qHN2VkS": {
    "title": "Better Theory for SGD in the Nonconvex World",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahmed Khaled",
      "Peter RichtÃ¡rik"
    ]
  },
  "https://openreview.net/forum?id=WFtTpQ47A7": {
    "title": "SHAP-XRT: The Shapley Value Meets Conditional Independence Testing",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jacopo Teneggi",
      "Beepul Bharti",
      "Yaniv Romano",
      "Jeremias Sulam"
    ]
  },
  "https://openreview.net/forum?id=rq1SaHQg2k": {
    "title": "Pairwise Learning with Adaptive Online Gradient Descent",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Sun",
      "Qingsong Wang",
      "Yunwen Lei",
      "Dongsheng Li",
      "Bao Wang"
    ]
  },
  "https://openreview.net/forum?id=GlhM6XX1wv": {
    "title": "DPVIm: Differentially Private Variational Inference Improved",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joonas JÃ¤lkÃ¶",
      "Lukas Prediger",
      "Antti Honkela",
      "Samuel Kaski"
    ]
  },
  "https://openreview.net/forum?id=vcHwQyNBjW": {
    "title": "Stochastic Batch Acquisition: A Simple Baseline for Deep Active Learning",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andreas Kirsch",
      "Sebastian Farquhar",
      "Parmida Atighehchian",
      "Andrew Jesson",
      "FrÃ©dÃ©ric Branchaud-Charron",
      "Yarin Gal"
    ]
  },
  "https://openreview.net/forum?id=rdHVPPVuXa": {
    "title": "Neural Causal Structure Discovery from Interventions",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nan Rosemary Ke",
      "Olexa Bilaniuk",
      "Anirudh Goyal",
      "Stefan Bauer",
      "Hugo Larochelle",
      "Bernhard SchÃ¶lkopf",
      "Michael Curtis Mozer",
      "Christopher Pal",
      "Yoshua Bengio"
    ]
  },
  "https://openreview.net/forum?id=xuWTFQ4VGO": {
    "title": "Diffusion Models for Constrained Domains",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nic Fishman",
      "Leo Klarner",
      "Valentin De Bortoli",
      "Emile Mathieu",
      "Michael John Hutchinson"
    ]
  },
  "https://openreview.net/forum?id=10hCbu70Sr": {
    "title": "Catastrophic overfitting can be induced with discriminative non-robust features",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guillermo Ortiz-Jimenez",
      "Pau de Jorge",
      "Amartya Sanyal",
      "Adel Bibi",
      "Puneet K. Dokania",
      "Pascal Frossard",
      "GrÃ©gory Rogez",
      "Philip Torr"
    ]
  },
  "https://openreview.net/forum?id=brGgOAXYtr": {
    "title": "POMRL: No-Regret Learning-to-Plan with Increasing Horizons",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Khimya Khetarpal",
      "Claire Vernade",
      "Brendan O'Donoghue",
      "Satinder Singh",
      "Tom Zahavy"
    ]
  },
  "https://openreview.net/forum?id=XnYtGPgG9p": {
    "title": "Off-Policy Evaluation with Out-of-Sample Guarantees",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sofia Ek",
      "Dave Zachariah",
      "Fredrik D. Johansson",
      "Peter Stoica"
    ]
  },
  "https://openreview.net/forum?id=MaDvbLaBiF": {
    "title": "Towards a More Rigorous Science of Blindspot Discovery in Image Classification Models",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gregory Plumb",
      "Nari Johnson",
      "Angel Cabrera",
      "Ameet Talwalkar"
    ]
  },
  "https://openreview.net/forum?id=fvEvDlKko6": {
    "title": "Black-Box Batch Active Learning for Regression",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andreas Kirsch"
    ]
  },
  "https://openreview.net/forum?id=p7UTv2hWgM": {
    "title": "Stochastic gradient updates yield deep equilibrium kernels",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Russell Tsuchida",
      "Cheng Soon Ong"
    ]
  },
  "https://openreview.net/forum?id=akg6kdx0Pk": {
    "title": "Instance-Adaptive Video Compression: Improving Neural Codecs by Training on the Test Set",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ties van Rozendaal",
      "Johann Brehmer",
      "Yunfan Zhang",
      "Reza Pourreza",
      "Auke J. Wiggers",
      "Taco Cohen"
    ]
  },
  "https://openreview.net/forum?id=nHfPXl1ly7": {
    "title": "A Kernel Perspective on Behavioural Metrics for Markov Decision Processes",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pablo Samuel Castro",
      "Tyler Kastner",
      "Prakash Panangaden",
      "Mark Rowland"
    ]
  },
  "https://openreview.net/forum?id=jgMqve6Qhw": {
    "title": "Dual PatchNorm",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manoj Kumar",
      "Mostafa Dehghani",
      "Neil Houlsby"
    ]
  },
  "https://openreview.net/forum?id=NNRIGE8bvF": {
    "title": "Fast Treatment Personalization with Latent Bandits in Fixed-Confidence Pure Exploration",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Newton Mwai Kinyanjui",
      "Emil Carlsson",
      "Fredrik D. Johansson"
    ]
  },
  "https://openreview.net/forum?id=fvyh6mDWFr": {
    "title": "Understanding Noise-Augmented Training for Randomized Smoothing",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ambar Pal",
      "Jeremias Sulam"
    ]
  },
  "https://openreview.net/forum?id=Cj6pLclmwT": {
    "title": "Differentially Private Image Classification from Features",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Harsh Mehta",
      "Walid Krichene",
      "Abhradeep Guha Thakurta",
      "Alexey Kurakin",
      "Ashok Cutkosky"
    ]
  },
  "https://openreview.net/forum?id=eGLdVRvvfQ": {
    "title": "DEUP: Direct Epistemic Uncertainty Prediction",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Salem Lahlou",
      "Moksh Jain",
      "Hadi Nekoei",
      "Victor I Butoi",
      "Paul Bertin",
      "Jarrid Rector-Brooks",
      "Maksym Korablyov",
      "Yoshua Bengio"
    ]
  },
  "https://openreview.net/forum?id=a1meaRy1bN": {
    "title": "Robustness through Data Augmentation Loss Consistency",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianjian Huang",
      "Shaunak Ashish Halbe",
      "Chinnadhurai Sankar",
      "Pooyan Amini",
      "Satwik Kottur",
      "Alborz Geramifard",
      "Meisam Razaviyayn",
      "Ahmad Beirami"
    ]
  },
  "https://openreview.net/forum?id=Uu8WwCFpQv": {
    "title": "Towards Large Scale Transfer Learning for Differentially Private Image Classification",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Harsh Mehta",
      "Abhradeep Guha Thakurta",
      "Alexey Kurakin",
      "Ashok Cutkosky"
    ]
  }
}