{
  "https://aclanthology.org/2025.emnlp-main.1": {
    "title": "Towards Automated Error Discovery: A Study in Conversational AI",
    "volume": "main",
    "abstract": "Although LLM-based conversational agents demonstrate strong fluency and coherence, they still produce undesirable behaviors (errors) that are challenging to prevent from reaching users during deployment. Recent research leverages large language models (LLMs) to detect errors and guide response-generation models toward improvement. However, current LLMs struggle to identify errors not explicitly specified in their instructions, such as those arising from updates to the response-generation model or shifts in user behavior. In this work, we introduce Automated Error Discovery, a framework for detecting and defining errors in conversational AI, and propose SEEED (Soft Clustering Extended Encoder-Based Error Detection), as an encoder-based approach to its implementation. We enhance the Soft Nearest Neighbor Loss by amplifying distance weighting for negative samples and introduce Label-Based Sample Ranking to select highly contrastive examples for better representation learning. SEEED outperforms adapted baselines—including GPT-4o and Phi-4—across multiple error-annotated dialogue datasets, improving the accuracy for detecting unknown errors by up to 8 points and demonstrating strong generalization to unknown intent detection",
    "checked": true,
    "id": "86fdb8f5aa2b7bd2da2a9223dc0ad83aee074234",
    "semantic_title": "towards automated error discovery: a study in conversational ai",
    "citation_count": 0,
    "authors": [
      "Dominic Petrak",
      "Thy Thy Tran",
      "Iryna Gurevych"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.2": {
    "title": "Break the Checkbox: Challenging Closed-Style Evaluations of Cultural Alignment in LLMs",
    "volume": "main",
    "abstract": "A large number of studies rely on closed-style multiple-choice surveys to evaluate cultural alignment in Large Language Models (LLMs). In this work, we challenge this constrained evaluation paradigm and explore more realistic, unconstrained approaches. Using the World Values Survey (WVS) and Hofstede Cultural Dimensions as case studies, we demonstrate that LLMs exhibit stronger cultural alignment in less constrained settings, where responses are not forced. Additionally, we show that even minor changes, such as reordering survey choices, lead to inconsistent outputs, exposing the limitations of closed-style evaluations. Our findings advocate for more robust and flexible evaluation frameworks that focus on specific cultural proxies, encouraging more nuanced and accurate assessments of cultural alignment in LLMs",
    "checked": true,
    "id": "d6efbbfefaf47a46af55d7c6738596e533682583",
    "semantic_title": "break the checkbox: challenging closed-style evaluations of cultural alignment in llms",
    "citation_count": 4,
    "authors": [
      "Mohsinul Kabir",
      "Ajwad Abrar",
      "Sophia Ananiadou"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.3": {
    "title": "Biased Tales: Cultural and Topic Bias in Generating Children's Stories",
    "volume": "main",
    "abstract": "Stories play a pivotal role in human communication, shaping beliefs and morals, particularly in children. As parents increasingly rely on large language models (LLMs) to craft bedtime stories, the presence of cultural and gender stereotypes in these narratives raises significant concerns. To address this issue, we present Biased Tales, a comprehensive dataset designed to analyze how biases influence protagonists' attributes and story elements in LLM-generated stories. Our analysis uncovers striking disparities. When the protagonist is described as a girl (as compared to a boy), appearance-related attributes increase by 55.26%. Stories featuring non-Western children disproportionately emphasize cultural heritage, tradition, and family themes far more than those for Western children. Our findings highlight the role of sociocultural bias in making creative AI use more equitable and diverse",
    "checked": true,
    "id": "d1584abe75e8d10579917bb9db5cc194c0fd107e",
    "semantic_title": "biased tales: cultural and topic bias in generating children's stories",
    "citation_count": 1,
    "authors": [
      "Donya Rooein",
      "Vilém Zouhar",
      "Debora Nozza",
      "Dirk Hovy"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.4": {
    "title": "Large Language Models as Realistic Microservice Trace Generators",
    "volume": "main",
    "abstract": "Workload traces are essential to understand complex computer systems' behavior and manage processing and memory resources. Since real-world traces are hard to obtain, synthetic trace generation is a promising alternative. This paper proposes a first-of-a-kind approach that relies on training a large language model (LLM) to generate synthetic workload traces, specifically microservice call graphs. To capture complex and arbitrary hierarchical structures and implicit constraints in such traces, we propose to train LLMs to generate recursively, making call graph generation a sequence of more manageable steps. To further enforce learning constraints on the traces and generate uncommon situations, we apply additional instruction tuning steps to align our model with the desired trace features. With this method, we train TraceLLM, an LLM for microservice trace generation, and demonstrate that it produces diverse, realistic traces under varied conditions, outperforming existing approaches in both accuracy and validity. The synthetically generated traces can effectively replace real data to optimize important microservice management tasks. Additionally, TraceLLM adapts to downstream trace-related tasks, such as predicting key trace features and infilling missing data",
    "checked": true,
    "id": "dc74965459c102c01852fe1288c55f9268010dc8",
    "semantic_title": "large language models as realistic microservice trace generators",
    "citation_count": 2,
    "authors": [
      "Donghyun Kim",
      "Sriram Ravula",
      "Taemin Ha",
      "Alex Dimakis",
      "Daehyeok Kim",
      "Aditya Akella"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.5": {
    "title": "JUDGEBERT: Assessing Legal Meaning Preservation Between Sentences",
    "volume": "main",
    "abstract": "Simplifying text while preserving its meaning is a complex yet essential task, especially in sensitive domain applications like legal texts. When applied to a specialized field, like the legal domain, preservation differs significantly from its role in regular texts. This paper introduces FrJUDGE, a new dataset to assess legal meaning preservation between two legal texts. It also introduces JUDGEBERT, a novel evaluation metric designed to assess legal meaning preservation in French legal text simplification. JUDGEBERT demonstrates a superior correlation with human judgment compared to existing metrics. It also passes two crucial sanity checks, while other metrics did not: For two identical sentences, it always returns a score of 100%; on the other hand, it returns 0% for two unrelated sentences. Our findings highlight its potential to transform legal NLP applications, ensuring accuracy and accessibility for text simplification for legal practitioners and lay users",
    "checked": true,
    "id": "e15c8fac436644586569d86d10aee98cfb7f5db6",
    "semantic_title": "judgebert: assessing legal meaning preservation between sentences",
    "citation_count": 0,
    "authors": [
      "David Beauchemin",
      "Michelle Albert-Rochette",
      "Richard Khoury",
      "Pierre-Luc Déziel"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.6": {
    "title": "QFrCoLA: a Quebec-French Corpus of Linguistic Acceptability Judgments",
    "volume": "main",
    "abstract": "Large and Transformer-based language models perform outstandingly in various downstream tasks. However, there is limited understanding regarding how these models internalize linguistic knowledge, so various linguistic benchmarks have recently been proposed to facilitate syntactic evaluation of language models across languages. This paper introduces QFrCoLA (Quebec-French Corpus of Linguistic Acceptability Judgments), a normative binary acceptability judgments dataset comprising 25,153 in-domain and 2,675 out-of-domain sentences. Our study leverages the QFrCoLA dataset and seven other linguistic binary acceptability judgment corpora to benchmark seven language models. The results demonstrate that, on average, fine-tuned Transformer-based LM are strong baselines for most languages and that zero-shot binary classification large language models perform poorly on the task. However, for the QFrCoLA benchmark, on average, a fine-tuned Transformer-based LM outperformed other methods tested. It also shows that pre-trained cross-lingual LLMs selected for our experimentation do not seem to have acquired linguistic judgment capabilities during their pre-training for Quebec French. Finally, our experiment results on QFrCoLA show that our dataset, built from examples that illustrate linguistic norms rather than speakers' feelings, is similar to linguistic acceptability judgment; it is a challenging dataset that can benchmark LM on their linguistic judgment capabilities",
    "checked": true,
    "id": "723dceaaa617a972abd0f20f03b7a4a02474b48d",
    "semantic_title": "qfrcola: a quebec-french corpus of linguistic acceptability judgments",
    "citation_count": 2,
    "authors": [
      "David Beauchemin",
      "Richard Khoury"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.7": {
    "title": "Revisiting LLM Value Probing Strategies: Are They Robust and Expressive?",
    "volume": "main",
    "abstract": "The value orientation of Large Language Models (LLMs) has been extensively studied, as it can shape user experiences across demographic groups.However, two key challenges remain: (1) the lack of systematic comparison across value probing strategies, despite the Multiple Choice Question (MCQ) setting being vulnerable to perturbations, and (2) the uncertainty over whether probed values capture in-context information or predict models' real-world actions.In this paper, we systematically compare three widely used value probing methods: token likelihood, sequence perplexity, and text generation.Our results show that all three methods exhibit large variances under non-semantic perturbations in prompts and option formats, with sequence perplexity being the most robust overall.We further introduce two tasks to assess expressiveness: demographic prompting, testing whether probed values adapt to cultural context; and value–action agreement, testing the alignment of probed values with value-based actions.We find that demographic context has little effect on the text generation method, and probed values only weakly correlate with action preferences across all methods.Our work highlights the instability and the limited expressive power of current value probing methods, calling for more reliable LLM value representations",
    "checked": true,
    "id": "43b81afc06451f42ac360ff0e9fcd1c20c5aaa21",
    "semantic_title": "revisiting llm value probing strategies: are they robust and expressive?",
    "citation_count": 0,
    "authors": [
      "Siqi Shen",
      "Mehar Singh",
      "Lajanugen Logeswaran",
      "Moontae Lee",
      "Honglak Lee",
      "Rada Mihalcea"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.8": {
    "title": "A Systematic Analysis of Base Model Choice for Reward Modeling",
    "volume": "main",
    "abstract": "Reinforcement learning from human feedback (RLHF) and, at its core, reward modeling have become a crucial part of training powerful large language models (LLMs). One commonly overlooked factor in training high-quality reward models (RMs) is the effect of the base model, which is becoming more challenging to choose given the rapidly growing pool of LLMs. In this work, we present a systematic analysis of the effect of base model selection on reward modeling performance. Our results show that the performance can be improved by up to 14% compared to the most common (i.e., default) choice. Moreover, we showcase the strong statistical relation between some existing benchmarks and downstream performances. We also demonstrate that the results from a small set of benchmarks could be combined to boost the model selection (+18% on average in the top 5-10). Lastly, we illustrate the impact of different post-training steps on the final performance and explore using estimated data distributions to reduce performance prediction error",
    "checked": true,
    "id": "c3f3592fc6c659c5935a9791e7b2e3f27d114dbb",
    "semantic_title": "a systematic analysis of base model choice for reward modeling",
    "citation_count": 0,
    "authors": [
      "Kian Ahrabian",
      "Pegah Jandaghi",
      "Negar Mokhberian",
      "Sai Praneeth Karimireddy",
      "Jay Pujara"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.9": {
    "title": "Comparing Specialised Small and General Large Language Models on Text Classification: 100 Labelled Samples to Achieve Break-Even Performance",
    "volume": "main",
    "abstract": "When solving NLP tasks with limited labelled data, researchers typically either use a general large language model without further update, or use a small number of labelled samples to tune a specialised smaller model. In this work, we answer an important question – how many labelled samples are required for the specialised small models to outperform general large models, while taking the performance variance into consideration. By observing the behaviour of fine-tuning, instruction-tuning, prompting and in-context learning on 8 language models, we identify such performance break-even points across 8 representative text classification tasks of varying characteristics. We show that the specialised models often need only few samples (on average 100) to be on par or better than the general ones. At the same time, the number of required labels strongly depends on the dataset or task characteristics, with fine-tuning on binary datasets requiring significantly more samples. When performance variance is taken into consideration, the number of required labels increases on average by 100 - 200%. Finally, larger models do not consistently lead to better performance and lower variance, with 4-bit quantisation having negligible impact",
    "checked": true,
    "id": "7f42b3a79ad1e176f5c881f1764b0ae87f65c252",
    "semantic_title": "comparing specialised small and general large language models on text classification: 100 labelled samples to achieve break-even performance",
    "citation_count": 11,
    "authors": [
      "Branislav Pecher",
      "Ivan Srba",
      "Maria Bielikova"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.10": {
    "title": "Is the Top Still Spinning? Evaluating Subjectivity in Narrative Understanding",
    "volume": "main",
    "abstract": "Determining faithfulness of a claim to a source document is an important problem across many domains. This task is generally treated as a binary judgment of whether the claim is supported or unsupported in relation to the source. In many cases, though, whether a claim is supported can be ambiguous. For instance, it may depend on making inferences from given evidence, and different people can reasonably interpret the claim as either supported or unsupported based on their agreement with those inferences. Forcing binary labels upon such claims lowers the reliability of evaluation. In this work, we reframe the task to manage the subjectivity involved with factuality judgments of ambiguous claims. We introduce LLM-generated edits of summaries as a method of providing a nuanced evaluation of claims: how much does a summary need to be edited to be unambiguous? Whether a claim gets rewritten and how much it changes can be used as an automatic evaluation metric, the Ambiguity Rewrite Metric (ARM), with a much richer feedback signal than a binary judgment of faithfulness. We focus on the area of narrative summarization as it is particularly rife with ambiguity and subjective interpretation. We show that ARM produces a 21% absolute improvement in annotator agreement on claim faithfulness, indicating that subjectivity is reduced",
    "checked": true,
    "id": "5c03d8c809125e81f690ddd7963241563ca809eb",
    "semantic_title": "is the top still spinning? evaluating subjectivity in narrative understanding",
    "citation_count": 0,
    "authors": [
      "Melanie Subbiah",
      "Akankshya Mishra",
      "Grace Kim",
      "Liyan Tang",
      "Greg Durrett",
      "Kathleen McKeown"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.11": {
    "title": "MathTutorBench: A Benchmark for Measuring Open-ended Pedagogical Capabilities of LLM Tutors",
    "volume": "main",
    "abstract": "Evaluating the pedagogical capabilities of AI-based tutoring models is critical for making guided progress in the field. Yet, we lack a reliable, easy-to-use, and simple-to-run evaluation that reflects the pedagogical abilities of models. To fill this gap, we present MathTutorBench, an open-source benchmark for holistic tutoring model evaluation. MathTutorBench contains a collection of datasets and metrics that broadly cover tutor abilities as defined by learning sciences research in dialog-based teaching. To score the pedagogical quality of open-ended teacher responses, we train a reward model and show it can discriminate expert from novice teacher responses with high accuracy. We evaluate a wide set of closed- and open-weight models on MathTutorBench and find that subject expertise, indicated by solving ability, does not immediately translate to good teaching. Rather, pedagogy and subject expertise appear to form a trade-off that is navigated by the degree of tutoring specialization of the model. Furthermore, tutoring appears to become more challenging in longer dialogs, where simpler questioning strategies begin to fail. We release the benchmark, code, and leaderboard openly to enable rapid benchmarking of future models",
    "checked": true,
    "id": "b7fac69ac1838b6eaf8e06f133823b770ce8f0ae",
    "semantic_title": "mathtutorbench: a benchmark for measuring open-ended pedagogical capabilities of llm tutors",
    "citation_count": 12,
    "authors": [
      "Jakub Macina",
      "Nico Daheim",
      "Ido Hakimi",
      "Manu Kapur",
      "Iryna Gurevych",
      "Mrinmaya Sachan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.12": {
    "title": "Preemptive Detection and Correction of Misaligned Actions in LLM Agents",
    "volume": "main",
    "abstract": "Deploying LLM-based agents in real-life applications often faces a critical challenge: the misalignment between agents' behavior and user intent. Such misalignment may lead agents to unintentionally execute some critical actions that carry negative outcomes (e.g., accidentally triggering a buy-now in web shopping), resulting in undesirable or even irreversible consequences. Although addressing these issues is crucial, the preemptive detection and correction of misaligned actions remains relatively underexplored. To fill this gap, we introduce InferAct, a novel approach that leverages the belief reasoning ability of LLMs, grounded in Theory-of-Mind, to detect misaligned actions. Once the misalignment is detected, InferAct alerts users for timely correction, preventing adverse outcomes and enhancing the reliability of LLM agents' decision-making processes. Experiments on three widely used tasks demonstrate InferAct achieves up to 20% improvements on Marco-F1 against baselines in misaligned action detection. An in-depth evaluation of misalignment correction further highlights InferAct‘s effectiveness in improving agent alignment",
    "checked": true,
    "id": "cf4557ccac231e74f23ab56d7dec5d6503626376",
    "semantic_title": "preemptive detection and correction of misaligned actions in llm agents",
    "citation_count": 4,
    "authors": [
      "Haishuo Fang",
      "Xiaodan Zhu",
      "Iryna Gurevych"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.13": {
    "title": "Fingerprinting LLMs through Survey Item Factor Correlation: A Case Study on Humor Style Questionnaire",
    "volume": "main",
    "abstract": "LLMs increasingly engage with psychological instruments, yet how they represent constructs internally remains poorly understood. We introduce a novel approach to \"fingerprinting\" LLMs through their factor correlation patterns on standardized psychological assessments to deepen the understanding of LLMs constructs representation. Using the Humor Style Questionnaire as a case study, we analyze how six LLMs represent and correlate humor-related constructs to survey participants. Our results show that they exhibit little similarity to human response patterns. In contrast, participants' subsamples demonstrate remarkably high internal consistency. Exploratory graph analysis further confirms that no LLM successfully recovers the four constructs of the Humor Style Questionnaire. These findings suggest that despite advances in natural language capabilities, current LLMs represent psychological constructs in fundamentally different ways than humans, questioning the validity of application as human simulacra",
    "checked": true,
    "id": "c8261ab0d9eaceb3540d853b0e00de4a8dd88fed",
    "semantic_title": "fingerprinting llms through survey item factor correlation: a case study on humor style questionnaire",
    "citation_count": 0,
    "authors": [
      "Simon Münker"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.14": {
    "title": "Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust Text-based Person Retrieval",
    "volume": "main",
    "abstract": "Although Contrastive Language-Image Pre-training (CLIP) exhibits strong performance across diverse vision tasks, its application to person representation learning faces two critical challenges: (i) the scarcity of large-scale annotated vision-language data focused on person-centric images, and (ii) the inherent limitations of global contrastive learning, which struggles to maintain discriminative local features crucial for fine-grained matching while remaining vulnerable to noisy text tokens. This work advances CLIP for person representation learning through synergistic improvements in data curation and model architecture. First, we develop a noise-resistant data construction pipeline that leverages the in-context learning capabilities of MLLMs to automatically filter and caption web-sourced images. This yields WebPerson, a large-scale dataset of 5M high-quality person-centric image-text pairs. Second, we introduce the GA-DMS (Gradient-Attention Guided Dual-Masking Synergetic) framework, which improves cross-modal alignment by adaptively masking noisy textual tokens based on the gradient-attention similarity score. Additionally, we incorporate masked token prediction objectives that compel the model to predict informative text tokens, enhancing fine-grained semantic representation learning. Extensive experiments show that GA-DMS achieves state-of-the-art performance across multiple benchmarks. The data and pre-trained models are released at https://github.com/Multimodal-Representation-Learning-MRL/GA-DMS",
    "checked": true,
    "id": "fae43e0a8ee6449ac10407a033280867085ab6a6",
    "semantic_title": "gradient-attention guided dual-masking synergetic framework for robust text-based person retrieval",
    "citation_count": 2,
    "authors": [
      "Tianlu Zheng",
      "Yifan Zhang",
      "Xiang An",
      "Ziyong Feng",
      "Kaicheng Yang",
      "Qichuan Ding"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.15": {
    "title": "From Problem-Solving to Teaching Problem-Solving: Aligning LLMs with Pedagogy using Reinforcement Learning",
    "volume": "main",
    "abstract": "Large language models (LLMs) can transform education, but their optimization for direct question-answering often undermines effective pedagogy which requires strategically withholding answers. To mitigate this, we propose an online reinforcement learning (RL)-based alignment framework that can quickly adapt LLMs into effective tutors using simulated student-tutor interactions by emphasizing pedagogical quality and guided problem-solving over simply giving away answers. We use our method to train a 7B parameter tutor model without human annotations which reaches similar performance to larger proprietary models like LearnLM. We introduce a controllable reward weighting to balance pedagogical support and student solving accuracy, allowing us to trace the Pareto frontier between these two objectives. Our models better preserve reasoning capabilities than single-turn SFT baselines and can optionally enhance interpretability through thinking tags that expose the model's instructional planning",
    "checked": true,
    "id": "7b6679ac2b9a8074878464a8c2aeeaa2df4a82ce",
    "semantic_title": "from problem-solving to teaching problem-solving: aligning llms with pedagogy using reinforcement learning",
    "citation_count": 1,
    "authors": [
      "David Dinucu-Jianu",
      "Jakub Macina",
      "Nico Daheim",
      "Ido Hakimi",
      "Iryna Gurevych",
      "Mrinmaya Sachan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.16": {
    "title": "CompKBQA: Component-wise Task Decomposition for Knowledge Base Question Answering",
    "volume": "main",
    "abstract": "Knowledge Base Question Answering (KBQA) aims to extract accurate answers from the Knowledge Base (KB). Traditional Semantic Parsing (SP)-based methods are widely used but struggle with complex queries. Recently, large language models (LLMs) have shown promise in improving KBQA performance. However, the challenge of generating error-free logical forms remains, as skeleton, topic Entity, and relation Errors still frequently occur. To address these challenges, we propose CompKBQA(Component-wise Task Decomposition for Knowledge Base Question Answering), a novel framework that optimizes the process of fine-tuning a LLM for generating logical forms by enabling the LLM to progressively learn relevant sub-tasks like skeleton generation, topic entity generation, and relevant relations generation. Additionally, we propose R3, which retrieves and incorporates KB information into the process of logical form generation. Experimental evaluations on two benchmark KBQA datasets, WebQSP and CWQ, demonstrate that CompKBQA achieves state-of-the-art performance, highlighting the importance of task decomposition and KB-aware learning",
    "checked": true,
    "id": "77c60d1e475f3b3d4a59c124d87a862d3bbff2bf",
    "semantic_title": "compkbqa: component-wise task decomposition for knowledge base question answering",
    "citation_count": 0,
    "authors": [
      "Yuhang Tian",
      "Dandan Song",
      "Zhijing Wu",
      "Pan Yang",
      "Changzhi Zhou",
      "Jun Yang",
      "Hao Wang",
      "Huipeng Ma",
      "Chenhao Li",
      "Luan Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.17": {
    "title": "Permutative Preference Alignment from Listwise Ranking of Human Judgments",
    "volume": "main",
    "abstract": "Aligning Large Language Models (LLMs) with human preferences is crucial in ensuring desirable and controllable model behaviors. Current methods, such as Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO), rely on the Bradley-Terry (B-T) model to maximize the likelihood of pairwise choices. However, when multiple responses are available, the B-T model fails to guarantee an accurate list ranking of the responses. To address this issue, we propose Permutative Preference Alignment (PPA), a novel offline listwise approach that incorporates the Normalized Discounted Cumulative Gain (NDCG)—a widely-used ranking metric—as an alternative training objective for LLM alignment. We develop an end-to-end alignment algorithm by approximating NDCG with a differentiable surrogate loss. Experiments demonstrate that PPA outperforms existing pairwise and listwise methods on evaluation sets and general benchmarks such as AlpacaEval. Furthermore, we show that NDCG-based approaches improve ranking accuracy more effectively than B-T-based methods and provide a theoretical explanation for this improvement",
    "checked": true,
    "id": "528e86eb77bd1451254fc06e9a3510aae75be3d3",
    "semantic_title": "permutative preference alignment from listwise ranking of human judgments",
    "citation_count": 2,
    "authors": [
      "Yang Zhao",
      "Yixin Wang",
      "Mingzhang Yin"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.18": {
    "title": "ToneCraft: Cantonese Lyrics Generation with Harmony of Tones and Pitches",
    "volume": "main",
    "abstract": "Lyrics generation has garnered increasing attention within the artificial intelligence community. Our task focuses on generating harmonious Cantonese lyrics. Unlike other languages, Cantonese has a unique system of nine contours and six tones, making it essential to satisfy the harmony rules that ensure the alignment between the melody and the tonal contours of the lyrics when composing lyrics. Current research has not yet addressed the challenge of generating lyrics that adhere to Cantonese harmony rules. To tackle this issue, we propose ToneCraft, a novel framework for generating Cantonese lyrics that ensures tonal and melodic harmony. It enables LLMs to generate lyrics with a fixed character count while aligning with tonal and melodic structures. We present an algorithm that combines character-level control, melodic guidance, and a task-specific loss to achieve tonal harmony without compromising generation flexibility and quality. By incorporating domain-specific expertise, we leverage pure lyric datasets to train our model, eliminating the need for aligned data. Both objective evaluations and subjective assessments show that our generated lyrics align with melodic contours significantly better than existing methods. All code and data are available at: https://github.com/purepasser-by/ToneCraft",
    "checked": true,
    "id": "1e8d9296eae6f1bd5d69cdb7e3f263bc349512a6",
    "semantic_title": "tonecraft: cantonese lyrics generation with harmony of tones and pitches",
    "citation_count": 0,
    "authors": [
      "Junyu Cheng",
      "Chang Pan",
      "Shuangyin Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.19": {
    "title": "SensorLLM: Aligning Large Language Models with Motion Sensors for Human Activity Recognition",
    "volume": "main",
    "abstract": "We introduce SensorLLM, a two-stage framework that enables Large Language Models (LLMs) to perform human activity recognition (HAR) from sensor time-series data. Despite their strong reasoning and generalization capabilities, LLMs remain underutilized for motion sensor data due to the lack of semantic context in time-series, computational constraints, and challenges in processing numerical inputs. SensorLLM addresses these limitations through a Sensor-Language Alignment stage, where the model aligns sensor inputs with trend descriptions. Special tokens are introduced to mark channel boundaries. This alignment enables LLMs to capture numerical variations, channel-specific features, and data of varying durations, without requiring human annotations. In the subsequent Task-Aware Tuning stage, we refine the model for HAR classification, achieving performance that matches or surpasses state-of-the-art methods. Our results demonstrate that SensorLLM evolves into an effective sensor learner, reasoner, and classifier through human-intuitive Sensor-Language Alignment, generalizing across diverse HAR datasets. We believe this work establishes a foundation for future research on time-series and text alignment, paving the way for foundation models in sensor data analysis. Our codes are available at https://github.com/zechenli03/SensorLLM",
    "checked": true,
    "id": "894adb15e7f1dfa9e081c98434d10825205837b6",
    "semantic_title": "sensorllm: aligning large language models with motion sensors for human activity recognition",
    "citation_count": 13,
    "authors": [
      "Zechen Li",
      "Shohreh Deldari",
      "Linyao Chen",
      "Hao Xue",
      "Flora D. Salim"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.20": {
    "title": "MixLoRA-DSI: Dynamically Expandable Mixture-of-LoRA Experts for Rehearsal-Free Generative Retrieval over Dynamic Corpora",
    "volume": "main",
    "abstract": "Continually updating model-based indexes in generative retrieval with new documents remains challenging, as full retraining is computationally expensive and impractical under resource constraints. We propose MixLoRA-DSI, a novel framework that combines an expandable mixture of Low-Rank Adaptation experts with a layer-wise out-of-distribution (OOD)-driven expansion strategy. Instead of allocating new experts for each new corpus, our proposed expansion strategy enables sublinear parameter growth by selectively introducing new experts only when significant number of OOD documents are detected. Experiments on NQ320k and MS MARCO Passage demonstrate that MixLoRA-DSI outperforms full-model update baselines, with minimal parameter overhead and substantially lower training costs",
    "checked": true,
    "id": "197dc8884eb53d917302375c54052a6c3ae0f227",
    "semantic_title": "mixlora-dsi: dynamically expandable mixture-of-lora experts for rehearsal-free generative retrieval over dynamic corpora",
    "citation_count": 0,
    "authors": [
      "Tuan-Luc Huynh",
      "Thuy-Trang Vu",
      "Weiqing Wang",
      "Trung Le",
      "Dragan Gasevic",
      "Yuan-Fang Li",
      "Thanh-Toan Do"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.21": {
    "title": "ViClaim: A Multilingual Multilabel Dataset for Automatic Claim Detection in Videos",
    "volume": "main",
    "abstract": "The growing influence of video content as a medium for communication and misinformation underscores the urgent need for effective tools to analyze claims in multilingual and multi-topic settings. Existing efforts in misinformation detection largely focus on written text, leaving a significant gap in addressing the complexity of spoken text in video transcripts. We introduce ViClaim, a dataset of 1,798 annotated video transcripts across three languages (English, German, Spanish) and six topics. Each sentence in the transcripts is labeled with three claim-related categories: fact-check-worthy, fact-non-check-worthy, or opinion. We developed a custom annotation tool to facilitate the highly complex annotation process. Experiments with state-of-the-art multilingual language models demonstrate strong performance in cross-validation (macro F1 up to 0.896) but reveal challenges in generalization to unseen topics, particularly for distinct domains. Our findings highlight the complexity of claim detection in video transcripts. ViClaim offers a robust foundation for advancing misinformation detection in video-based communication, addressing a critical gap in multimodal analysis",
    "checked": true,
    "id": "85039a24a50687d8f30bffbf33045ed9ae0dbbc2",
    "semantic_title": "viclaim: a multilingual multilabel dataset for automatic claim detection in videos",
    "citation_count": 1,
    "authors": [
      "Patrick Giedemann",
      "Pius von Däniken",
      "Jan Milan Deriu",
      "Alvaro Rodrigo",
      "Anselmo Peñas",
      "Mark Cieliebak"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.22": {
    "title": "DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) with web search capabilities show significant potential for deep research, yet current methods—brittle prompt engineering or RAG-based reinforcement learning in controlled environments—fail to capture real-world complexities. In this paper, we introduce DeepResearcher, the first comprehensive framework for end-to-end training of LLM-based deep research agents through scaling reinforcement learning (RL) in real-world environments with authentic web search interactions. Unlike RAG approaches reliant on fixed corpora, DeepResearcher trains agents to navigate the noisy, dynamic open web. We implement a specialized multi-agent architecture where browsing agents extract relevant information from various webpage structures and overcoming significant technical challenges. Extensive experiments on open-domain research tasks demonstrate that DeepResearcher achieves substantial improvements of up to 28.9 points over prompt engineering-based baselines and up to 7.2 points over RAG-based RL agents. Our qualitative analysis reveals emergent cognitive behaviors from end-to-end RL training, such as planning, cross-validation, self-reflection for research redirection, and maintain honesty when unable to find definitive answers. Our results highlight that end-to-end training in real-world web environments is fundamental for developing robust research capabilities aligned with real-world applications. The source codefor DeepResearcher is released at: https://github.com/GAIR-NLP/DeepResearcher",
    "checked": true,
    "id": "4298a7bca88001f5df2d1ae15cca186d46271dc5",
    "semantic_title": "deepresearcher: scaling deep research via reinforcement learning in real-world environments",
    "citation_count": 116,
    "authors": [
      "Yuxiang Zheng",
      "Dayuan Fu",
      "Xiangkun Hu",
      "Xiaojie Cai",
      "Lyumanshan Ye",
      "Pengrui Lu",
      "Pengfei Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.23": {
    "title": "Mixture of Length and Pruning Experts for Knowledge Graphs Reasoning",
    "volume": "main",
    "abstract": "Knowledge Graph (KG) reasoning, which aims to infer new facts from structured knowledge repositories, plays a vital role in Natural Language Processing (NLP) systems. Its effectiveness critically depends on constructing informative and contextually relevant reasoning paths. However, existing graph neural networks (GNNs) often adopt rigid, query-agnostic path-exploration strategies, limiting their ability to adapt to diverse linguistic contexts and semantic nuances. To address these limitations, we propose MoKGR, a mixture-of-experts framework that personalizes path exploration through two complementary components: (1) a mixture of length experts that adaptively selects and weights candidate path lengths according to query complexity, providing query-specific reasoning depth; and (2) a mixture of pruning experts that evaluates candidate paths from a complementary perspective, retaining the most informative paths for each query. Through comprehensive experiments on diverse benchmark, MoKGR demonstrates superior performance in both transductive and inductive settings, validating the effectiveness of personalized path exploration in KGs reasoning",
    "checked": true,
    "id": "3fe65f31d9e5ae9def74894c84bf4fa2c8ed34e0",
    "semantic_title": "mixture of length and pruning experts for knowledge graphs reasoning",
    "citation_count": 0,
    "authors": [
      "Enjun Du",
      "Siyi Liu",
      "Yongqi Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.24": {
    "title": "MPRF: Interpretable Stance Detection through Multi-Path Reasoning Framework",
    "volume": "main",
    "abstract": "Stance detection, a critical task in Natural Language Processing (NLP), aims to identify the attitude expressed in text toward specific targets. Despite advancements in Large Language Models (LLMs), challenges such as limited interpretability and handling nuanced content persist. To address these issues, we propose the Multi-Path Reasoning Framework (MPRF), a novel framework that generates, evaluates, and integrates multiple reasoning paths to improve accuracy, robustness, and transparency in stance detection. Unlike prior work that relies on single-path reasoning or static explanations, MPRF introduces a structured end-to-end pipeline: it first generates diverse reasoning paths through predefined perspectives, then dynamically evaluates and optimizes each path using LLM-based scoring, and finally fuses the results via weighted aggregation to produce interpretable and reliable predictions. Extensive experiments on the SEM16, VAST, and PStance datasets demonstrate that MPRF outperforms existing models. Ablation studies further validate the critical role of MPRF's components, highlighting its effectiveness in enhancing interpretability and handling complex stance detection tasks",
    "checked": true,
    "id": "7acd424bc8865016fac918cb96898d929c4b3a75",
    "semantic_title": "mprf: interpretable stance detection through multi-path reasoning framework",
    "citation_count": 0,
    "authors": [
      "ZhaoDan Zhang",
      "Jin Zhang",
      "Hui Xu",
      "Jiafeng Guo",
      "Xueqi Cheng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.25": {
    "title": "Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from Token and Parameter Levels",
    "volume": "main",
    "abstract": "Large language models (LLMs) acquire substantial world knowledge during pre-training, which is further shaped by post-training techniques such as supervised fine-tuning (SFT). However, the impact of SFT on a model's knowledge remains underexplored, limiting our ability to control knowledge behavior in fine-tuned models. To address this gap, we evaluate closed-book question answering (CBQA) performance across five LLMs from the LLaMA-2 and LLaMA-3 families. Surprisingly, models fine-tuned on 1,920 samples perform up to 14% worse than those fine-tuned on only 240 samples. Furthermore, varying the level of knowledge mastery in the fine-tuning data leads to performance fluctuations of over 12%. To investigate these effects, we analyze model behavior at both the token and parameter levels. Our analysis reveals that up to 90% of parameter updates during SFT do not contribute to knowledge enhancement. Restoring these updates can improve performance on the CBQA task, depending on the characteristics of the fine-tuning data. These insights offer practical guidance for developing fine-tuning strategies that more effectively strengthen model knowledge",
    "checked": true,
    "id": "a00d95b1f93c9fb4008b09bf70094cab0705a3cd",
    "semantic_title": "analyzing the effects of supervised fine-tuning on model knowledge from token and parameter levels",
    "citation_count": 0,
    "authors": [
      "Junjie Ye",
      "Yuming Yang",
      "Yang Nan",
      "Shuo Li",
      "Qi Zhang",
      "Tao Gui",
      "Xuanjing Huang",
      "Peng Wang",
      "Zhongchao Shi",
      "Jianping Fan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.26": {
    "title": "JI2S: Joint Influence‐Aware Instruction Data Selection for Efficient Fine‐Tuning",
    "volume": "main",
    "abstract": "Instruction tuning (IT) improves large language models (LLMs) by aligning their outputs with human instructions, but its success depends critically on training data quality, and datasets such as Alpaca often contain noisy or suboptimal examples that undermine fine‐tuning. Prior selection strategies score samples using general‐purpose LLMs (e.g., GPT), leveraging their strong language understanding yet introducing inherent biases that misalign with the target model's behavior and yield unstable downstream performance. Influence‐based methods address this by estimating each example's marginal contribution to overall performance, but they typically assume additive contributions and therefore overlook higher‐order interactions among samples. To overcome these limitations, we propose JI2S, a novel framework that jointly models both marginal and combinatorial influences within sample groups. Applying JI2S to select the top 1,000 most influential examples from Alpaca, we fine‐tune LLaMA2‐7B, Mistral‐7B, and LLaMA2‐13B and evaluate them on Open LLM Benchmarks, MT‐Bench, and GPT‐4–judged pairwise comparisons. Our experiments show that JI2S consistently outperforms full‐dataset training and strong baselines, highlighting the value of capturing joint influence for high‐quality instruction fine‐tuning. We provide our code in this GitHub repository",
    "checked": true,
    "id": "12cb305ae96ca5dcde46554127a7f24a07a9576a",
    "semantic_title": "ji2s: joint influence‐aware instruction data selection for efficient fine‐tuning",
    "citation_count": 0,
    "authors": [
      "Jingyu Wei",
      "Bo Liu",
      "Tianjiao Wan",
      "Baoyun Peng",
      "Xingkong Ma",
      "Mengmeng Guo"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.27": {
    "title": "SoundMind: RL-Incentivized Logic Reasoning for Audio-Language Models",
    "volume": "main",
    "abstract": "While large language models have demonstrated impressive reasoning abilities, their extension to the audio modality, particularly within large audio-language models (LALMs), remains underexplored. Addressing this gap requires a systematic approach that involves a capable base model, high-quality reasoning-oriented audio data, and effective training algorithms. In this work, we present a comprehensive solution for audio logical reasoning (ALR) tasks: we introduce SoundMind, a dataset of 6,446 audio–text annotated samples specifically curated to support complex reasoning. Building on this resource, we propose SoundMind-RL, a rule-based reinforcement learning (RL) algorithm designed to equip audio-language models with robust audio–text reasoning capabilities. By fine-tuning Qwen2.5-Omni-7B on the proposed SoundMind dataset using SoundMind-RL, we achieve strong and consistent improvements over state-of-the-art baselines on the SoundMind benchmark. This work highlights the benefit of combining high-quality, reasoning-focused datasets with specialized RL techniques, and contributes to advancing auditory intelligence in language models. The code and dataset are publicly available at https://github.com/xid32/SoundMind",
    "checked": true,
    "id": "432159c05b1b3d1c9f89b5dab2daaa8b4425b9e2",
    "semantic_title": "soundmind: rl-incentivized logic reasoning for audio-language models",
    "citation_count": 8,
    "authors": [
      "Xingjian Diao",
      "Chunhui Zhang",
      "Keyi Kong",
      "Weiyi Wu",
      "Chiyu Ma",
      "Zhongyu Ouyang",
      "Peijun Qing",
      "Soroush Vosoughi",
      "Jiang Gui"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.28": {
    "title": "Seeing More, Saying More: Lightweight Language Experts are Dynamic Video Token Compressors",
    "volume": "main",
    "abstract": "Recent advancements in large video-language models have revolutionized video understanding tasks. However, their efficiency is significantly constrained by processing high volumes of visual tokens. Existing token compression strategies apply a fixed compression ratio, ignoring the variability in semantic density among different video clips. Consequently, this lead to inadequate representation of information-rich clips due to insufficient tokens and unnecessary computation on static or content-poor ones. To address this, we propose LangDC, a Language-aware Dynamic Token Compressor. LangDC leverages a lightweight language model to describe video clips, converting them into soft caption tokens as visual representations. Trained with our proposed semantic density-aware supervision, LangDC aims to 1) cover key visual cues necessary for downstream task reasoning and 2) dynamically adjust compression ratios based on scene richness, reflected by descriptions length. Our design mimics how humans dynamically express what they see: complex scenes (seeing more) elicit more detailed language to convey nuances (saying more), whereas simpler scenes are described with fewer words. Experimental results show that our method reduces FLOPs by 49% compared to VideoGPT+ while maintaining competitive performance. Furthermore, qualitative results demonstrate our approach adaptively adjusts the token compression ratio based on video segment richness. Code will be released once acceptance",
    "checked": true,
    "id": "ac0753239a9fea33135afa6b24fc9af2248bb220",
    "semantic_title": "seeing more, saying more: lightweight language experts are dynamic video token compressors",
    "citation_count": 0,
    "authors": [
      "Xiangchen Wang",
      "Jinrui Zhang",
      "Teng Wang",
      "Haigang Zhang",
      "Feng Zheng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.29": {
    "title": "RoT: Enhancing Table Reasoning with Iterative Row-Wise Traversals",
    "volume": "main",
    "abstract": "The table reasoning task, crucial for efficient data acquisition, aims to answer questions based on the given table. Recently, reasoning large language models (RLLMs) with Long Chain-of-Thought (Long CoT) significantly enhance reasoning capabilities, leading to brilliant performance on table reasoning. However, Long CoT suffers from high cost for training and exhibits low reliability due to table content hallucinations. Therefore, we propose Row-of-Thought (RoT), which performs iteratively row-wise table traversal, allowing for reasoning extension and reflection-based refinement at each traversal. Scaling reasoning length by row-wise traversal and leveraging reflection capabilities of LLMs, RoT is training-free. The sequential traversal encourages greater attention to the table, thus reducing hallucinations. Experiments show that RoT, using non-reasoning models, outperforms RLLMs by an average of 4.3%, and achieves state-of-the-art results on WikiTableQuestions and TableBench with comparable models, proving its effectiveness. Also, RoT outperforms Long CoT with fewer reasoning tokens, indicating higher efficiency",
    "checked": true,
    "id": "f85161b63a540ffd1e41dd6071ebf1fed3e05a6f",
    "semantic_title": "rot: enhancing table reasoning with iterative row-wise traversals",
    "citation_count": 3,
    "authors": [
      "Xuanliang Zhang",
      "Dingzirui Wang",
      "Keyan Xu",
      "Qingfu Zhu",
      "Wanxiang Che"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.30": {
    "title": "T-MAD: Target-driven Multimodal Alignment for Stance Detection",
    "volume": "main",
    "abstract": "Multimodal Stance Detection (MSD) aims to determine a user's stance - support, oppose, or neutral - toward a target by analyzing multimodal content such as texts and images from social media. Existing MSD methods struggle with generalizing to unseen targets and handling modality inconsistencies. To address these challenges, we propose the Target-driven Multi-modal Alignment and Dynamic Weighting Model (T-MAD), which combines target-driven multi-modal alignment and dynamic weighting mechanisms to capture target-specific relationships and balance modality contributions. The model incorporates iterative reasoning to iteratively refine predictions, achieving robust performance in both in-target and zero-shot settings. Experiments on the MMSD and MultiClimate datasets show that T-MAD outperforms state-of-the-art models, with optimal results achieved using RoBERTa, ViT, and an iterative depth of 5. Ablation studies further confirm the importance of multi-modal alignment and dynamic weighting in enhancing model effectiveness",
    "checked": true,
    "id": "8eb92699e0c596cd33127aa7b19f29b8ef0db067",
    "semantic_title": "t-mad: target-driven multimodal alignment for stance detection",
    "citation_count": 0,
    "authors": [
      "ZhaoDan Zhang",
      "Jin Zhang",
      "Xueqi Cheng",
      "Hui Xu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.31": {
    "title": "Emotion Transfer with Enhanced Prototype for Unseen Emotion Recognition in Conversation",
    "volume": "main",
    "abstract": "Current Emotion Recognition in Conversation (ERC) research follows a closed-domain assumption. However, there is no clear consensus on emotion classification in psychology, which presents a challenge for models when it comes to recognizing previously unseen emotions in real-world applications. To bridge this gap, we introduce the Unseen Emotion Recognition in Conversation (UERC) task for the first time and propose **ProEmoTrans**, a solid prototype-based emotion transfer framework. This prototype-based approach shows promise but still faces key challenges: First, implicit expressions complicate emotion definition, which we address by proposing an LLM-enhanced description approach. Second, utterance encoding in long conversations is difficult, which we tackle with a proposed parameter-free mechanism for efficient encoding and overfitting prevention. Finally, the Markovian flow nature of emotions is hard to transfer, which we address with an improved Attention Viterbi Decoding (AVD) method to transfer seen emotion transitions to unseen emotions. Extensive experiments on three datasets show that our method serves as a strong baseline for preliminary exploration in this new area",
    "checked": true,
    "id": "d0ade98c52f836f75de5031457039a4a9ca34428",
    "semantic_title": "emotion transfer with enhanced prototype for unseen emotion recognition in conversation",
    "citation_count": 0,
    "authors": [
      "Kun Peng",
      "Cong Cao",
      "Hao Peng",
      "Guanlin Wu",
      "Zhifeng Hao",
      "Lei Jiang",
      "Yanbing Liu",
      "Philip S. Yu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.32": {
    "title": "PBI-Attack: Prior-Guided Bimodal Interactive Black-Box Jailbreak Attack for Toxicity Maximization",
    "volume": "main",
    "abstract": "Understanding the vulnerabilities of Large Vision Language Models (LVLMs) to jailbreak attacks is essential for their responsible real-world deployment. Most previous work requires access to model gradients, or is based on human knowledge (prompt engineering) to complete jailbreak, and they hardly consider the interaction of images and text, resulting in inability to jailbreak in black box scenarios or poor performance. To overcome these limitations, we propose a Prior-Guided Bimodal Interactive Black-Box Jailbreak Attack for toxicity maximization, referred to as PBI-Attack. Our method begins by extracting malicious features from a harmful corpus using an alternative LVLM and embedding these features into a benign image as prior information. Subsequently, we enhance these features through bidirectional cross-modal interaction optimization, which iteratively optimizes the bimodal perturbations in an alternating manner through greedy search, aiming to maximize the toxicity of the generated response. The toxicity level is quantified using a well-trained evaluation model. Experiments demonstrate that PBI-Attack outperforms previous state-of-the-art jailbreak methods, achieving an average attack success rate of 92.5% across three open-source LVLMs and around 67.3% on three closed-source LVLMs. Disclaimer: This paper contains potentially disturbing and offensive content",
    "checked": true,
    "id": "9475cc68d4332f8ef57134b1389f6ce4858dd143",
    "semantic_title": "pbi-attack: prior-guided bimodal interactive black-box jailbreak attack for toxicity maximization",
    "citation_count": 5,
    "authors": [
      "Ruoxi Cheng",
      "Yizhong Ding",
      "Shuirong Cao",
      "Ranjie Duan",
      "Xiaoshuang Jia",
      "Shaowei Yuan",
      "Simeng Qin",
      "Zhiqiang Wang",
      "Xiaojun Jia"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.33": {
    "title": "Training a Utility-based Retriever Through Shared Context Attribution for Retrieval-Augmented Language Models",
    "volume": "main",
    "abstract": "Retrieval-Augmented Language Models boost task performance, owing to the retriever that provides external knowledge. Although crucial, the retriever primarily focuses on semantics relevance, which may not always be effective for generation. Thus, utility-based retrieval has emerged as a promising topic, prioritizing passages that provide valid benefits for downstream tasks. However, due to insufficient understanding, capturing passage utility accurately remains unexplored. This work proposes SCARLet, a framework for training utility-based retrievers in RALMs, which incorporates two key factors, multi-task generalization and inter-passage interaction. First, SCARLet constructs shared context on which training data for various tasks is synthesized. This mitigates semantic bias from context differences, allowing retrievers to focus on learning task-specific utility and generalize across tasks. Next, SCARLet uses a perturbation-based attribution method to estimate passage-level utility for shared context, which reflects interactions between passages and provides more accurate feedback. We evaluate our approach on ten datasets across various tasks, both in-domain and out-of-domain, showing that retrievers trained by SCARLet consistently improve the overall performance of RALMs",
    "checked": true,
    "id": "703e4bf0b90c30adf35b8c68b7fcbbf59d8ed867",
    "semantic_title": "training a utility-based retriever through shared context attribution for retrieval-augmented language models",
    "citation_count": 3,
    "authors": [
      "Yilong Xu",
      "Jinhua Gao",
      "Xiaoming Yu",
      "Yuanhai Xue",
      "Baolong Bi",
      "Huawei Shen",
      "Xueqi Cheng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.34": {
    "title": "SportReason: Evaluating Retrieval-Augmented Reasoning across Tables and Text for Sports Question Answering",
    "volume": "main",
    "abstract": "We present SportReason, a benchmark for retrieval-augmented reasoning on numerical sports questions. Unlike existing benchmarks limited to one or two evidence units, SportReason requires combining and reasoning across free-text, structured tables, and semi-structured infoboxes. We provide 3,000 human-verified QA pairs by repurposing existing QA and table generation datasets, and by prompting large language models (LLMs). Each pair is grounded in multiple evidence from a multi-modal Wikipedia corpus containing 200K knowledge contexts. We evaluate existing retrievers and rerankers, along with agentic Retrieval-Augmented Generation (RAG) systems. The experimental results show that multi-evidence retrieval remains a challenge. Agentic RAG systems (e.g., Search-o1), despite iterative retrieval and reasoning capabilities, fail to improve performance due to imprecise queries, simple training, and distracting information",
    "checked": true,
    "id": "6a686f4b431060e0b2035b682da1eb8310722b47",
    "semantic_title": "sportreason: evaluating retrieval-augmented reasoning across tables and text for sports question answering",
    "citation_count": 0,
    "authors": [
      "Kaiyue Feng",
      "Siyue Zhang",
      "Bingsen Chen",
      "Yilun Zhao",
      "Chen Zhao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.35": {
    "title": "MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced Knowledge Boundary Awareness",
    "volume": "main",
    "abstract": "With the widespread application of large language models (LLMs), the issue of generating non-existing facts, known as hallucination, has garnered increasing attention. Previous research in enhancing LLM confidence estimation mainly focuses on the single problem setting. However, LLM awareness of its internal parameterized knowledge boundary under the more challenging multi-problem setting, which requires answering multiple problems accurately simultaneously, remains underexplored. To bridge this gap, we introduce a novel method, Multiple Answers and Confidence Stepwise Tuning (MAC-Tuning), that separates the learning of answer prediction and confidence estimation during fine-tuning on instruction data. Extensive experiments across various base models and different model sizes demonstrate that our method proposed outperforms baselines by up to 25% in average precision",
    "checked": true,
    "id": "18ccda104f8001fa0719153112c0bac3ac10f740",
    "semantic_title": "mac-tuning: llm multi-compositional problem reasoning with enhanced knowledge boundary awareness",
    "citation_count": 3,
    "authors": [
      "Junsheng Huang",
      "Zhitao He",
      "Yuchen Huang",
      "Sandeep Polisetty",
      "Qingyun Wang",
      "Yi R. Fung"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.36": {
    "title": "CODI: Compressing Chain-of-Thought into Continuous Space via Self-Distillation",
    "volume": "main",
    "abstract": "Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by encouraging step-by-step reasoning in natural language. However, leveraging a latent continuous space for reasoning may offer benefits in terms of both efficiency and robustness. Prior implicit CoT methods attempt to bypass language completely by reasoning in continuous space but have consistently underperformed compared to the standard explicit CoT approach. We introduce CODI (Continuous Chain-of-Thought via Self-Distillation), a novel training framework that effectively compresses natural language CoT into continuous space. CODI jointly trains a teacher task (Explicit CoT) and a student task (Implicit CoT), distilling the reasoning ability from language into continuous space by aligning the hidden states of a designated token. Our experiments show that CODI is the first implicit CoT approach to match the performance of explicit CoT on GSM8k at the GPT-2 scale, achieving a 3.1x compression rate and outperforming the previous state-of-the-art by 28.2% in accuracy. CODI also demonstrates robustness, generalizable to complex datasets, and interpretability. These results validate that LLMs can reason effectively not only in natural language, but also in a latent continuous space. Code is available at https://github.com/zhenyi4/codi",
    "checked": false,
    "id": "7e869410e1f1c56a836b3bda63af8229b8e3aa87",
    "semantic_title": "scratchpad thinking: alternation between storage and computation in latent reasoning models",
    "citation_count": 0,
    "authors": [
      "Zhenyi Shen",
      "Hanqi Yan",
      "Linhai Zhang",
      "Zhanghao Hu",
      "Yali Du",
      "Yulan He"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.37": {
    "title": "PAFT: Prompt-Agnostic Fine-Tuning",
    "volume": "main",
    "abstract": "Fine-tuning large language models (LLMs) often causes overfitting to specific prompt wording, where minor phrasing variations drastically reduce performance. To address this, we propose Prompt-Agnostic Fine-Tuning (PAFT), a method that enhances robustness through dynamic prompt variation during training. PAFT first generates diverse synthetic prompts, then continuously samples from this set to construct training instances, forcing models to learn fundamental task principles rather than surface-level patterns. Across systematic evaluations using both supervised fine-tuning (SFT) and reinforcement learning fine-tuning (RLFT), PAFT consistently demonstrates improved performance on benchmarks for question answering, mathematical reasoning, and tool use. It achieves 7% higher generalization accuracy on unseen prompts than standard methods with similar training efficiency. Notably, models trained with PAFT attain 3.2× faster inference speeds due to reduced prompt sensitivity. Ablation studies further validate effectiveness of PAFT, while theoretical analysis reveals that PAFT can effectively enhance the cross-domain generalization ability of LLM",
    "checked": true,
    "id": "aaf1e7bbbdef8bd4346dcda9abcda355264ea376",
    "semantic_title": "paft: prompt-agnostic fine-tuning",
    "citation_count": 0,
    "authors": [
      "Chenxing Wei",
      "Yao Shu",
      "Mingwen Ou",
      "Ying He",
      "Fei Yu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.38": {
    "title": "Theorem-Validated Reverse Chain-of-Thought Problem Generation for Geometric Reasoning",
    "volume": "main",
    "abstract": "Large Multimodal Models (LMMs) face limitations in geometric reasoning due to insufficient Chain of Thought (CoT) image-text training data. While existing approaches leverage template-based or LLM-assisted methods for geometric CoT data creation, they often face challenges in achieving both diversity and precision. To bridge this gap, we introduce a two-stage Theorem-Validated Reverse Chain-of-Thought Reasoning Synthesis (TR-CoT) framework. The first stage, TR-Engine, synthesizes theorem-grounded geometric diagrams with structured descriptions and properties. The second stage, TR-Reasoner, employs reverse reasoning to iteratively refine question-answer pairs by cross-validating geometric properties and description fragments. Our approach expands theorem-type coverage, corrects long-standing misunderstandings, and enhances geometric reasoning. Fine-grained CoT improves theorem understanding and increases logical consistency by 24.5%. Our best models surpass the baselines in MathVista and GeoQA by 10.1% and 4.7%, outperforming advanced closed-source models like GPT-4o",
    "checked": true,
    "id": "e87f566a2dd10aa4a9a9e68184fc494fcf4963d3",
    "semantic_title": "theorem-validated reverse chain-of-thought problem generation for geometric reasoning",
    "citation_count": 12,
    "authors": [
      "Deng Linger",
      "Linghao Zhu",
      "Yuliang Liu",
      "Yu Wang",
      "Qunyi Xie",
      "Jingjing Wu",
      "Gang Zhang",
      "Yingying Zhu",
      "Xiang Bai"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.39": {
    "title": "TACO: Enhancing Multimodal In-context Learning via Task Mapping-Guided Sequence Configuration",
    "volume": "main",
    "abstract": "Multimodal in-context learning (ICL) has emerged as a key mechanism for harnessing the capabilities of large vision–language models (LVLMs). However, its effectiveness remains highly sensitive to the quality of input ICL sequences, particularly for tasks involving complex reasoning or open-ended generation. A major limitation is our limited understanding of how LVLMs actually exploit these sequences during inference. To bridge this gap, we systematically interpret multimodal ICL through the lens of task mapping, which reveals how local and global relationships within and among demonstrations guide model reasoning. Building on this insight, we present TACO, a lightweight transformer-based model equipped with task-aware attention that dynamically configures ICL sequences. By injecting task-mapping signals into the autoregressive decoding process, TACO creates a bidirectional synergy between sequence construction and task reasoning. Experiments on five LVLMs and nine datasets demonstrate that TACO consistently surpasses baselines across diverse ICL tasks. These results position task mapping as a novel and valuable perspective for interpreting and improving multimodal ICL",
    "checked": true,
    "id": "2afc9ecc5a4efbcdf62f6ff697bc47ba88874fc6",
    "semantic_title": "taco: enhancing multimodal in-context learning via task mapping-guided sequence configuration",
    "citation_count": 18,
    "authors": [
      "Yanshu Li",
      "Jianjiang Yang",
      "Tian Yun",
      "Pinyuan Feng",
      "Jinfa Huang",
      "Ruixiang Tang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.40": {
    "title": "Towards Controllable Speech Synthesis in the Era of Large Language Models: A Systematic Survey",
    "volume": "main",
    "abstract": "Text-to-speech (TTS) has advanced from generating natural-sounding speech to enabling fine-grained control over attributes like emotion, timbre, and style. Driven by rising industrial demand and breakthroughs in deep learning, e.g., diffusion and large language models (LLMs), controllable TTS has become a rapidly growing research area. This survey provides **the first** comprehensive review of controllable TTS methods, from traditional control techniques to emerging approaches using natural language prompts. We categorize model architectures, control strategies, and feature representations, while also summarizing challenges, datasets, and evaluations in controllable TTS. This survey aims to guide researchers and practitioners by offering a clear taxonomy and highlighting future directions in this fast-evolving field. One can visit https://github.com/imxtx/awesome-controllabe-speech-synthesis for a comprehensive paper list and updates",
    "checked": true,
    "id": "41a091d554709dc597a52e488389395191e7a61e",
    "semantic_title": "towards controllable speech synthesis in the era of large language models: a systematic survey",
    "citation_count": 1,
    "authors": [
      "Tianxin Xie",
      "Yan Rong",
      "Pengfei Zhang",
      "Wenwu Wang",
      "Li Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.41": {
    "title": "Automating Steering for Safe Multimodal Large Language Models",
    "volume": "main",
    "abstract": "Recent progress in Multimodal Large Language Models (MLLMs) has unlocked powerful cross-modal reasoning abilities, but also raised new safety concerns, particularly when faced with adversarial multimodal inputs. To improve the safety of MLLMs during inference, we introduce a modular and adaptive inference-time intervention technology, AutoSteer, without requiring any fine-tuning of the underlying model. AutoSteer incorporates three core components: (1) a novel Safety Awareness Score (SAS) that automatically identifies the most safety-relevant distinctions among the model's internal layers; (2) an adaptive safety prober trained to estimate the likelihood of toxic outputs from intermediate representations; and (3) a lightweight Refusal Head that selectively intervenes to modulate generation when safety risks are detected. Experiments on LLaVA-OV and Chameleon across diverse safety-critical benchmarks demonstrate that AutoSteer significantly reduces the Attack Success Rate (ASR) for textual, visual, and cross-modal threats, while maintaining general abilities. These findings position AutoSteer as a practical, interpretable, and effective framework for safer deployment of multimodal AI systems",
    "checked": true,
    "id": "dca426f85222bad896042753bcd05e56c2e2608f",
    "semantic_title": "automating steering for safe multimodal large language models",
    "citation_count": 2,
    "authors": [
      "Lyucheng Wu",
      "Mengru Wang",
      "Ziwen Xu",
      "Tri Cao",
      "Nay Oo",
      "Bryan Hooi",
      "Shumin Deng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.42": {
    "title": "EMNLP: Educator-role Moral and Normative Large Language Models Profiling",
    "volume": "main",
    "abstract": "Simulating Professions (SP) enables Large Language Models (LLMs) to emulate professional roles. However, comprehensive psychological and ethical evaluation in these contexts remains lacking. This paper introduces EMNLP, an Educator-role Moral and Normative LLMs Profiling framework for personality profiling, moral development stage measurement, and ethical risk under soft prompt injection. EMNLP extends existing scales and constructs 88 teacher-specific moral dilemmas, enabling profession-oriented comparison with human teachers. A targeted soft prompt injection set evaluates compliance and vulnerability in teacher SP. Experiments on 14 LLMs show teacher-role LLMs exhibit more idealized and polarized personalities than human teachers, excel in abstract moral reasoning, but struggle with emotionally complex situations. Models with stronger reasoning are more vulnerable to harmful prompt injection, revealing a paradox between capability and safety. The model temperature and other hyperparameters have limited influence except in some risk behaviors. This paper presents the first benchmark to assess ethical and psychological alignment of teacher-role LLMs for educational AI. Resources are available at https://e-m-n-l-p.github.io/",
    "checked": true,
    "id": "e2aa97217e0e156996a345896a0a4b1938911388",
    "semantic_title": "emnlp: educator-role moral and normative large language models profiling",
    "citation_count": 0,
    "authors": [
      "Yilin Jiang",
      "Mingzi Zhang",
      "Sheng Jin",
      "Zengyi Yu",
      "Xiangjie Kong",
      "Binghao Tu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.43": {
    "title": "TracSum: A New Benchmark for Aspect-Based Summarization with Sentence-Level Traceability in Medical Domain",
    "volume": "main",
    "abstract": "While document summarization with LLMs has enhanced access to textual information, concerns about the factual accuracy of these summaries persist (e.g., hallucination), especially in the medical domain. Tracing source evidence from which summaries are derived enables users to assess their accuracy, thereby alleviating this concern. In this paper, we introduce TracSum, a novel benchmark for traceable, aspect-based summarization, in which generated summaries are paired with sentence-level citations, enabling users to trace back to the original context. First, we annotate 500 medical abstracts for seven key medical aspects, yielding 3.5K summary-citations pairs. We then propose a fine-grained evaluation framework for this new task, designed to assess the completeness and consistency of generated content using four metrics. Finally, we introduce a summarization pipeline, Track-Then-Sum, which serves as a baseline method for comparison. In experiments, we evaluate both this baseline and a set of LLMs on TracSum, and conduct a human evaluation to assess the evaluation results. The findings demonstrate that TracSum can serve as an effective benchmark for traceable, aspect-based summarization tasks. We also observe that explicitly performing sentence-level tracking prior to summarization enhances generation accuracy, while incorporating the full context further improves summary completeness. Source code and dataset are available at https://github.com/chubohao/TracSum",
    "checked": true,
    "id": "0ab74266e48101bc842de39be4db46e8b4a43c59",
    "semantic_title": "tracsum: a new benchmark for aspect-based summarization with sentence-level traceability in medical domain",
    "citation_count": 0,
    "authors": [
      "Bohao Chu",
      "Meijie Li",
      "Sameh Frihat",
      "Chengyu Gu",
      "Georg Lodde",
      "Elisabeth Livingstone",
      "Norbert Fuhr"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.44": {
    "title": "Context Reasoner: Incentivizing Reasoning Capability for Contextualized Privacy and Safety Compliance via Reinforcement Learning",
    "volume": "main",
    "abstract": "While Large Language Models (LLMs) exhibit remarkable capabilities, they also introduce significant safety and privacy risks. Current mitigation strategies often fail to preserve contextual reasoning capabilities in risky scenarios. Instead, they rely heavily on sensitive pattern matching to protect LLMs, which limits the scope. Furthermore, they overlook established safety and privacy standards, leading to systemic risks for legal compliance. To address these gaps, we formulate safety and privacy issues into contextualized compliance problems following the Contextual Integrity (CI) theory. Under the CI framework, we align our model with three critical regulatory standards: GDPR, EU AI Act, and HIPAA. Specifically, we employ reinforcement learning (RL) with a rule-based reward to incentivize contextual reasoning capabilities while enhancing compliance with safety and privacy norms. Through extensive experiments, we demonstrate that our method not only significantly enhances legal compliance (achieving a +8.58% accuracy improvement in safety/privacy benchmarks) but also further improves general reasoning capability. For OpenThinker-7B, a strong reasoning model that significantly outperforms its base model Qwen2.5-7B-Instruct across diverse subjects, our method enhances its general reasoning capabilities, with +2.05% and +8.98% accuracy improvement on the MMLU and LegalBench benchmark, respectively",
    "checked": true,
    "id": "cf4400d394d48f09e763232c92b9855536af29a0",
    "semantic_title": "context reasoner: incentivizing reasoning capability for contextualized privacy and safety compliance via reinforcement learning",
    "citation_count": 3,
    "authors": [
      "Wenbin Hu",
      "Haoran Li",
      "Huihao Jing",
      "Qi Hu",
      "Ziqian Zeng",
      "Sirui Han",
      "Xu Heli",
      "Tianshu Chu",
      "Peizhao Hu",
      "Yangqiu Song"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.45": {
    "title": "Towards General-Domain Word Sense Disambiguation: Distilling Large Language Model into Compact Disambiguator",
    "volume": "main",
    "abstract": "Word Sense Disambiguation (WSD) aims to determine the correct meaning of a word in context from a predefined inventory, and remains a fundamental challenge in natural language understanding. Existing methods rely heavily on manually annotated data, which limits coverage and generalization. In this work, we propose a scalable framework that leverages large language models (LLMs) as knowledge distillers to construct silver-standard WSD corpora. We explore generation-based distillation, where diverse examples are synthesized for dictionary senses, and annotation-based distillation, where LLMs assign sense labels to polysemous words within real-world corpus sentences. The resulting data is used to train tiny models. Extensive experiments show that models distilled from LLM-generated data outperform those trained on gold-standard corpora, especially on general-domain benchmarks. Our annotation-based model, after balancing sense distribution, achieves 50% F1 gain on the most challenging test set and the best distilled model can match or even exceed the performance of its LLM teacher, despite having over 1000 times fewer parameters. These results demonstrate the effectiveness of LLM-based distillation for building accurate, generalizable, and efficient WSD systems",
    "checked": true,
    "id": "94df3355bbe6d8481777884c52057df3d2744b5b",
    "semantic_title": "towards general-domain word sense disambiguation: distilling large language model into compact disambiguator",
    "citation_count": 0,
    "authors": [
      "Liqiang Ming",
      "Sheng-hua Zhong",
      "Yuncong Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.46": {
    "title": "SLoW: Select Low-frequency Words! Automatic Dictionary Selection for Translation on Large Language Models",
    "volume": "main",
    "abstract": "There are more than 7,000 languages around the world, and current Large Language Models (LLMs) only support hundreds of languages. Dictionary-based prompting methods can enhance translation on them, but most methods use all the available dictionaries, which could be expensive. Instead, it will be flexible to have a trade-off between token consumption and translation performance. This paper proposes a novel task called Automatic Dictionary Selection (ADS). The goal of the task is to automatically select which dictionary to use to enhance translation. We propose a novel and effective method which we call Select Low-frequency Words! (SLoW) which selects those dictionaries that have a lower frequency. Our methods have unique advantages. First, there is no need for access to the training data for frequency estimation (which is usually unavailable). Second, it inherits the advantage of dictionary-based methods, where no additional tuning is required on LLMs. Experimental results on 100 languages from FLORES indicate that SLoW surpasses strong baselines, and it can obviously save token usage, with many languages even surpassing the translation performance of the full dictionary baseline",
    "checked": true,
    "id": "d42531e9205660db14e9d85afa6f44045c1d17ae",
    "semantic_title": "slow: select low-frequency words! automatic dictionary selection for translation on large language models",
    "citation_count": 0,
    "authors": [
      "Hongyuan Lu",
      "Zixuan Li",
      "Zefan Zhang",
      "Wai Lam"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.47": {
    "title": "Parallel Continuous Chain-of-Thought with Jacobi Iteration",
    "volume": "main",
    "abstract": "Continuous chain-of-thought has been shown to be effective in saving reasoning tokens for large language models. By reasoning with continuous latent thought tokens, continuous CoT is able to perform implicit reasoning in a compact manner. However, the sequential dependencies between latent thought tokens spoil parallel training, leading to long training time. In this paper, we propose Parallel Continuous Chain-of-Thought (PCCoT), which performs Jacobi iteration on the latent thought tokens, updating them iteratively in parallel instead of sequentially and thus improving both training and inference efficiency of continuous CoT. Experiments demonstrate that by choosing the proper number of iterations, we are able to achieve comparable or even better performance while saving nearly 50% of the training and inference time. Moreover, PCCoT shows better stability and robustness in the training process. Our code is available at https://github.com/whyNLP/PCCoT",
    "checked": true,
    "id": "5cd0b2a46e3672731d68954a389e01501eeb5c3a",
    "semantic_title": "parallel continuous chain-of-thought with jacobi iteration",
    "citation_count": 7,
    "authors": [
      "Haoyi Wu",
      "Zhihao Teng",
      "Kewei Tu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.48": {
    "title": "EQA-RM: A Generative Embodied Reward Model with Test-time Scaling",
    "volume": "main",
    "abstract": "Reward Models (RMs), vital for large model alignment, are underexplored for complex embodied tasks like Embodied Question Answering (EQA) where nuanced evaluation of agents' spatial, temporal, and logical understanding is critical yet not considerred by generic approaches. We introduce EQA-RM, a novel generative multimodal reward model specifically architected for EQA, trained via our innovative Contrastive Group Relative Policy Optimization (C-GRPO) strategy to learn fine-grained behavioral distinctions. The generative nature of EQA-RM provides interpretable, structured reward feedback (beyond simple scalars), uniquely enabling test-time scaling to dynamically adjust evaluation granularity, from concise scores to detailed critiques of reasoning and grounding, at inference without retraining. Concurrently, we introduce EQARewardBench, a new benchmark built on OpenEQA for standardized EQA reward model assessment. Demonstrating high sample efficiency, EQA-RM (fine-tuning Qwen2-VL-2B-Instruct) achieves 61.9% accuracy on EQA-RM-Bench with 700 samples, outperforming strong proprietary baselines, including Gemini-2.5-Flash, GPT-4o, Claude-3.5-Haiku, and open-sourced state-of-the-art models such as RoVRM and VisualPRM",
    "checked": true,
    "id": "bba5163dbc56b9cea548ef0cd370897049766c51",
    "semantic_title": "eqa-rm: a generative embodied reward model with test-time scaling",
    "citation_count": 1,
    "authors": [
      "Yuhang Chen",
      "Zhen Tan",
      "Tianlong Chen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.49": {
    "title": "Refusal-Aware Red Teaming: Exposing Inconsistency in Safety Evaluations",
    "volume": "main",
    "abstract": "The responsible deployment of Large Language Models (LLMs) necessitates rigorous safety evaluations. However, a critical challenge arises from inconsistencies between an LLM's internal refusal decisions and external safety assessments, hindering effective validation. This paper introduces the concept of the ‘refusal gap' to formally define these discrepancies. We then present a novel, refusal-aware red teaming framework designed to automatically generate test cases that expose such gaps. Our framework employs ‘refusal probes', which leverage the target model's hidden states, to detect internal model refusals. These are subsequently contrasted with judgments from an external safety evaluator. The identified discrepancy serves as a signal to guide a red-teaming model in crafting test cases that maximize this refusal gap. To further enhance test case diversity and address challenges related to sparse rewards, we introduce a hierarchical, curiosity-driven mechanism that incentivizes both refusal gap maximization and broad topic exploration. Empirical results demonstrate that our method significantly outperforms existing reinforcement learning-based approaches in generating diverse test cases and achieves a substantially higher discovery rate of refusal gaps",
    "checked": true,
    "id": "e1dbe7606ef27851e9e4041576558aa9e25092ea",
    "semantic_title": "refusal-aware red teaming: exposing inconsistency in safety evaluations",
    "citation_count": 0,
    "authors": [
      "Yongkang Chen",
      "Xiaohu Du",
      "Xiaotian Zou",
      "Chongyang Zhao",
      "Huan Deng",
      "Hu Li",
      "Xiaohui Kuang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.50": {
    "title": "OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking",
    "volume": "main",
    "abstract": "Machine writing with large language models often relies on retrieval-augmented generation. However, these approaches remain confined within the boundaries of the model's predefined scope, limiting the generation of content with rich information. Specifically, vanilla-retrieved information tends to lack depth, novelty, and suffers from redundancy, which negatively impacts the quality of generated articles, leading to shallow, unoriginal, and repetitive outputs. To address these issues, we propose OmniThink, a slow-thinking machine writing framework that emulates the human-like process of iterative expansion and reflection. The core idea behind OmniThink is to simulate the cognitive behavior of learners as they slowly deepen their knowledge of the topics. Experimental results demonstrate that OmniThink improves the knowledge density of generated articles without compromising metrics such as coherence and depth. Human evaluations and expert feedback further highlight the potential of OmniThink to address real-world challenges in the generation of long-form articles",
    "checked": true,
    "id": "613912aa7c10372e95ec43dea4b80d41d47002ee",
    "semantic_title": "omnithink: expanding knowledge boundaries in machine writing through thinking",
    "citation_count": 11,
    "authors": [
      "Zekun Xi",
      "Wenbiao Yin",
      "Jizhan Fang",
      "Jialong Wu",
      "Runnan Fang",
      "Yong Jiang",
      "Pengjun Xie",
      "Fei Huang",
      "Huajun Chen",
      "Ningyu Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.51": {
    "title": "LinkAlign: Scalable Schema Linking for Real-World Large-Scale Multi-Database Text-to-SQL",
    "volume": "main",
    "abstract": "Schema linking is a critical bottleneck in applying existing Text-to-SQL models to real-world, large-scale, multi-database environments. Through error analysis, we identify two major challenges in schema linking: (1) Database Retrieval: accurately selecting the target database from a large schema pool, while effectively filtering out irrelevant ones; and (2) Schema Item Grounding: precisely identifying the relevant tables and columns within complex and often redundant schemas for SQL generation. Based on these, we introduce LinkAlign, a novel framework tailored for large-scale databases with thousands of fields. LinkAlign comprises three key steps: multi-round semantic enhanced retrieval and irrelevant information isolation for Challenge 1, and schema extraction enhancement for Challenge 2. Each stage supports both Agent and Pipeline execution modes, enabling balancing efficiency and performance via modular design. To enable more realistic evaluation, we construct AmbiDB, a synthetic dataset designed to reflect the ambiguity of real-world schema linking. Experiments on widely-used Text-to-SQL benchmarks demonstrate that LinkAlign consistently outperforms existing baselines on all schema linking metrics. Notably, it improves the overall Text-to-SQL pipeline and achieves a new state-of-the-art score of 33.09% on the Spider 2.0-Lite benchmark using only open-source LLMs, ranking first on the leaderboard at the time of submission. The codes are available at https://github.com/Satissss/LinkAlign",
    "checked": true,
    "id": "4e2f748e1cf7e67f3d3400a5dcc2569b56eebf5a",
    "semantic_title": "linkalign: scalable schema linking for real-world large-scale multi-database text-to-sql",
    "citation_count": 4,
    "authors": [
      "Yihan Wang",
      "Peiyu Liu",
      "Xin Yang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.52": {
    "title": "On Relation-Specific Neurons in Large Language Models",
    "volume": "main",
    "abstract": "In large language models (LLMs), certain neurons can store distinct pieces of knowledge learned during pretraining. While factual knowledge typically appears as a combination of relations and entities, it remains unclear whether some neurons focus on a relation itself – independent of any entity. We hypothesize such neurons detect a relation in the input text and guide generation involving such a relation. To investigate this, we study the LLama-2 family on a chosen set of relations, with a statistics-based method. Our experiments demonstrate the existence of relation-specific neurons. We measure the effect of selectively deactivating candidate neurons specific to relation r on the LLM's ability to handle (1) facts involving relation r and (2) facts involving a different relation r' ≠ r. With respect to their capacity for encoding relation information, we give evidence for the following three properties of relation-specific neurons. (i) Neuron cumulativity. Multiple neurons jointly contribute to processing facts involving relation r, with no single neuron fully encoding a fact in r on its own. (ii) Neuron versatility. Neurons can be shared across multiple closely related as well as less related relations. In addition, some relation neurons transfer across languages. (iii) Neuron interference. Deactivating neurons specific to one relation can improve LLMs' factual recall performance for facts of other relations. We make our code and data publicly available at https://github.com/cisnlp/relation-specific-neurons",
    "checked": true,
    "id": "319094be36d257af7aaa7e1b472f5c37c3bec294",
    "semantic_title": "on relation-specific neurons in large language models",
    "citation_count": 0,
    "authors": [
      "Yihong Liu",
      "Runsheng Chen",
      "Lea Hirlimann",
      "Ahmad Dawar Hakimi",
      "Mingyang Wang",
      "Amir Hossein Kargaran",
      "Sascha Rothe",
      "François Yvon",
      "Hinrich Schuetze"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.53": {
    "title": "IPIGuard: A Novel Tool Dependency Graph-Based Defense Against Indirect Prompt Injection in LLM Agents",
    "volume": "main",
    "abstract": "Large language model (LLM) agents are widely deployed in real-world applications, where they leverage tools to retrieve and manipulate external data for complex tasks. However, when interacting with untrusted data sources (e.g., fetching information from public websites), tool responses may contain injected instructions that covertly influence agent behaviors and lead to malicious outcomes, a threat referred to as Indirect\\ Prompt\\ Injection (IPI). Existing defenses typically rely on advanced prompting strategies or auxiliary detection models. While these methods have demonstrated some effectiveness, they fundamentally rely on assumptions about the model's inherent security, which lacks structural constraints on agent behaviors. As a result, agents still retain unrestricted access to tool invocations, leaving them vulnerable to stronger attack vectors that can bypass the security guardrails of the model. To\\ prevent\\ malicious\\ tool\\ invocations\\ at\\ the\\ source, we propose a novel defensive task execution paradigm, called IPIGuard, which models the agents' task execution process as a traversal over a planned Tool\\ Dependency\\ Graph (TDG). By explicitly decoupling action planning from interaction with external data, IPIGuard significantly reduces unintended tool invocations triggered by injected instructions, thereby enhancing robustness against IPI attacks. Experiments on the AgentDojo benchmark show that IPIGuard achieves a superior balance between effectiveness and robustness, paving the way for the development of safer agentic systems in dynamic environments",
    "checked": true,
    "id": "9ddcbbff1e23b7f995fc363ad5123091f8866748",
    "semantic_title": "ipiguard: a novel tool dependency graph-based defense against indirect prompt injection in llm agents",
    "citation_count": 3,
    "authors": [
      "Hengyu An",
      "Jinghuai Zhang",
      "Tianyu Du",
      "Chunyi Zhou",
      "Qingming Li",
      "Tao Lin",
      "Shouling Ji"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.54": {
    "title": "ProtoVQA: An Adaptable Prototypical Framework for Explainable Fine-Grained Visual Question Answering",
    "volume": "main",
    "abstract": "Visual Question Answering (VQA) is increasingly used in diverse applications ranging from general visual reasoning to safety-critical domains such as medical imaging and autonomous systems, where models must provide not only accurate answers but also explanations that humans can easily understand and verify. Prototype-based modeling has shown promise for interpretability by grounding predictions in semantically meaningful regions for purely visual reasoning tasks, yet remains underexplored in the context of VQA. We present ProtoVQA, a unified prototypical framework that (i) learns question-aware prototypes that serve as reasoning anchors, connecting answers to discriminative image regions, (ii) applies spatially constrained matching to ensure that the selected evidence is coherent and semantically relevant, and (iii) supports both answering and grounding tasks through a shared prototype backbone. To assess explanation quality, we propose the Visual–Linguistic Alignment Score (VLAS), which measures how well the model's attended regions align with ground-truth evidence. Experiments on Visual7W show that ProtoVQA yields faithful, fine-grained explanations while maintaining competitive accuracy, advancing the development of transparent and trustworthy VQA systems",
    "checked": true,
    "id": "57f992130022eb9891188d611b0fa16ef9cc0418",
    "semantic_title": "protovqa: an adaptable prototypical framework for explainable fine-grained visual question answering",
    "citation_count": 0,
    "authors": [
      "Xingjian Diao",
      "Weiyi Wu",
      "Keyi Kong",
      "Peijun Qing",
      "Xinwen Xu",
      "Ming Cheng",
      "Soroush Vosoughi",
      "Jiang Gui"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.55": {
    "title": "SEA: Supervised Embedding Alignment for Token-Level Visual-Textual Integration in MLLMs",
    "volume": "main",
    "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities by integrating visual and textual inputs, yet modality alignment remains one of the most challenging aspects. Current MLLMs typically rely on simple adapter architectures and pretraining approaches to bridge vision encoders with large language models (LLM), guided by image-level supervision. We identify this paradigm often leads to suboptimal alignment between modalities, significantly constraining the LLM's ability to properly interpret and reason with visual features particularly for smaller language models. To address this fundamental limitation, we propose Supervised Embedding Alignment (SEA), a token-level supervision alignment method that enables more precise visual-text alignment during pretraining. SEA introduces minimal computational overhead while preserving language capabilities and substantially improving cross-modal understanding. Our comprehensive analyses reveal critical insights into the adapter's role in multimodal integration, and extensive experiments demonstrate that SEA consistently improves performance across various model sizes, with smaller models benefiting the most (average performance gain of 7.61% for Gemma-2B). This work establishes a foundation for developing more effective alignment strategies for future multimodal systems",
    "checked": true,
    "id": "d1199803cba5b6c2a8d8e8bffb9aa65a30cf3b45",
    "semantic_title": "sea: supervised embedding alignment for token-level visual-textual integration in mllms",
    "citation_count": 10,
    "authors": [
      "Yuanyang Yin",
      "Yaqi Zhao",
      "Yajie Zhang",
      "Yuanxing Zhang",
      "Ke Lin",
      "Jiahao Wang",
      "Xin Tao",
      "Pengfei Wan",
      "Wentao Zhang",
      "Feng Zhao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.56": {
    "title": "Molecular String Representation Preferences in Pretrained LLMs: A Comparative Study in Zero- & Few-Shot Molecular Property Prediction",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have demonstrated capabilities for natural language formulations of molecular property prediction tasks, but little is known about how performance depends on the representation of input molecules to the model; the status quo approach is to use SMILES strings, although alternative chemical notations convey molecular information differently, each with their own strengths and weaknesses. To learn more about molecular string representation preferences in LLMs, we compare the performance of four recent models—GPT-4o, Gemini 1.5 Pro, Llama 3.1 405b, and Mistral Large 2—on molecular property prediction tasks from the MoleculeNet benchmark across five different molecular string representations: SMILES, DeepSMILES, SELFIES, InChI, and IUPAC names. We find statistically significant zero- and few-shot preferences for InChI and IUPAC names, potentially due to representation granularity, favorable tokenization, and prevalence in pretraining corpora. This contradicts previous assumptions that molecules should be presented to LLMs as SMILES strings. When these preferences are taken advantage of, few-shot performance rivals or surpasses many previous conventional approaches to property prediction, with the advantage of explainable predictions through chain-of-thought reasoning not held by task-specific models",
    "checked": true,
    "id": "e029b2b23ab4210647fb6f5ee0659b9ab21be2f8",
    "semantic_title": "molecular string representation preferences in pretrained llms: a comparative study in zero- & few-shot molecular property prediction",
    "citation_count": 0,
    "authors": [
      "George Arthur Baker",
      "Mario Sanz-Guerrero",
      "Katharina von der Wense"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.57": {
    "title": "Weight-Aware Activation Sparsity with Constrained Bayesian Optimization Scheduling for Large Language Models",
    "volume": "main",
    "abstract": "Activation sparsity provides a dynamic, input-dependent alternative to weight pruning for accelerating inference in large language models (LLMs), effectively reducing unnecessary computations and memory accesses during the forward pass. Despite its promise, existing activation sparsification methods suffer from two major limitations: (1) solely relying on activation magnitude for sparsification, ignoring the coupling influence with the corresponding weights, (2) applying uniform sparsity rates across all blocks without considering block-wise sparsity sensitivity. To address these issues, this paper proposes a novel training-free weight-aware activation sparsity framework, called **WAS**. Firstly, with analyzing the coupling relationship between weight and activation, we introduce a weight-aware scoring method to measure the activation importance in sparsification. Then, a novel constrained Bayesian optimization algorithm is further devised to set a suitable sparsity ratio for all blocks based on the sparsity sensitivity. Finally, we implement a custom GPU sparsity kernel to support the resulting sparsity patterns for wall-clock decoding speed-ups. Our **WAS** achieves competitive performance at 60% model-level sparsity and significantly outperforms prior methods at higher sparsity levels, achieving up to 1.68× inference speed-up—at no retraining or weight update. Codes are available at https://github.com/HITSZ-Miao-Group/WAS",
    "checked": true,
    "id": "99f1650ed8e2ad87d69c5c7bcd539f92c60af4af",
    "semantic_title": "weight-aware activation sparsity with constrained bayesian optimization scheduling for large language models",
    "citation_count": 0,
    "authors": [
      "Ming Wang",
      "Miao Zhang",
      "Xuebo Liu",
      "Liqiang Nie"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.58": {
    "title": "DatawiseAgent: A Notebook-Centric LLM Agent Framework for Adaptive and Robust Data Science Automation",
    "volume": "main",
    "abstract": "Existing large language model (LLM) agents for automating data science show promise, but they remain constrained by narrow task scopes, limited generalization across tasks and models, and over-reliance on state-of-the-art (SOTA) LLMs. We introduce DatawiseAgent, a notebook-centric LLM agent framework for adaptive and robust data science automation. Inspired by how human data scientists work in computational notebooks, DatawiseAgent introduces a unified interaction representation and a multi-stage architecture based on finite-state transducers (FSTs). This design enables flexible long-horizon planning, progressive solution development, and robust recovery from execution failures. Extensive experiments across diverse data science scenarios and models show that DatawiseAgent consistently achieves SOTA performance by surpassing strong baselines such as AutoGen and TaskWeaver, demonstrating superior effectiveness and adaptability. Further evaluations reveal graceful performance degradation under weaker or smaller models, underscoring the robustness and scalability",
    "checked": true,
    "id": "531b170fd2e3569ee1c445cd73ce082588193120",
    "semantic_title": "datawiseagent: a notebook-centric llm agent framework for adaptive and robust data science automation",
    "citation_count": 0,
    "authors": [
      "Ziming You",
      "Yumiao Zhang",
      "Dexuan Xu",
      "Yiwei Lou",
      "Yandong Yan",
      "Wei Wang",
      "Huamin Zhang",
      "Yu Huang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.59": {
    "title": "VC4VG: Optimizing Video Captions for Text-to-Video Generation",
    "volume": "main",
    "abstract": "Recent advances in text-to-video (T2V) generation highlight the critical role of high-quality video-text pairs in training models capable of producing coherent and instruction-aligned videos. However, strategies for optimizing video captions specifically for T2V training remain underexplored. In this paper, we introduce VC4VG (Video Captioning for Video Generation), a comprehensive caption optimization framework tailored to the needs of T2V models. We begin by analyzing caption content from a T2V perspective, decomposing the essential elements required for video reconstruction into multiple dimensions, and proposing a principled caption design methodology. To support evaluation, we construct VC4VG-Bench, a new benchmark featuring fine-grained, multi-dimensional, and necessity-graded metrics aligned with T2V-specific requirements. Extensive T2V fine-tuning experiments demonstrate a strong correlation between improved caption quality and video generation performance, validating the effectiveness of our approach. We release all benchmark tools and code (https://github.com/qyr0403/VC4VG) to support further research",
    "checked": true,
    "id": "c15ec422fbe4862100d45382b9249aa36b0b3250",
    "semantic_title": "vc4vg: optimizing video captions for text-to-video generation",
    "citation_count": 0,
    "authors": [
      "Yang Du",
      "Zhuoran Lin",
      "Kaiqiang Song",
      "Biao Wang",
      "Zhicheng Zheng",
      "Tiezheng Ge",
      "Bo Zheng",
      "Qin Jin"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.60": {
    "title": "LaMP-QA: A Benchmark for Personalized Long-form Question Answering",
    "volume": "main",
    "abstract": "Personalization is essential for question answering systems that are user-centric. Despite its importance, personalization in answer generation has been relatively underexplored. This is mainly due to lack of resources for training and evaluating personalized question answering systems. We address this gap by introducing LaMP-QA—a benchmark designed for evaluating personalized long-form answer generation. The benchmark covers questions from three major categories: (1) Arts & Entertainment, (2) Lifestyle & Personal Development, and (3) Society & Culture, encompassing over 45 subcategories in total. To assess the quality and potential impact of the LaMP-QA benchmark for personalized question answering, we conduct comprehensive human and automatic evaluations, to compare multiple evaluation strategies for evaluating generated personalized responses and measure their alignment with human preferences. Furthermore, we benchmark a number of non-personalized and personalized approaches based on open-source and proprietary large language models. Our results show that incorporating the personalized context provided leads to up to 39% performance improvements. The benchmark is publicly released to support future research in this area",
    "checked": true,
    "id": "d50eb5206eea1f46d3f0cf5d4edac2141544ae57",
    "semantic_title": "lamp-qa: a benchmark for personalized long-form question answering",
    "citation_count": 5,
    "authors": [
      "Alireza Salemi",
      "Hamed Zamani"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.61": {
    "title": "The LLM Already Knows: Estimating LLM-Perceived Question Difficulty via Hidden Representations",
    "volume": "main",
    "abstract": "Estimating the difficulty of input questions as perceived by large language models (LLMs) is essential for accurate performance evaluation and adaptive inference. Existing methods typically rely on repeated response sampling, auxiliary models, or fine-tuning the target model itself, which may incur substantial computational costs or compromise generality. In this paper, we propose a novel approach for difficulty estimation that leverages only the hidden representations produced by the target LLM. We model the token-level generation process as a Markov chain and define a value function to estimate the expected output quality given any hidden state. This allows for efficient and accurate difficulty estimation based solely on the initial hidden state, without generating any output tokens. Extensive experiments across both textual and multimodal tasks demonstrate that our method consistently outperforms existing baselines in difficulty estimation. Moreover, we apply our difficulty estimates to guide adaptive reasoning strategies, including Self-Consistency, Best-of-N, and Self-Refine, achieving higher inference efficiency with fewer generated tokens",
    "checked": true,
    "id": "965f51b965fc310daa29eaa4cbdea679591a8eb8",
    "semantic_title": "the llm already knows: estimating llm-perceived question difficulty via hidden representations",
    "citation_count": 1,
    "authors": [
      "Yubo Zhu",
      "Dongrui Liu",
      "Zecheng Lin",
      "Wei Tong",
      "Sheng Zhong",
      "Jing Shao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.62": {
    "title": "MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol",
    "volume": "main",
    "abstract": "As Model Context Protocol (MCP) introduces an easy-to-use ecosystem for users and developers, it also brings underexplored safety risks. Its decentralized architecture, which separates clients and servers, poses unique challenges for systematic safety analysis. This paper proposes a novel framework to enhance MCP safety. Guided by the MAESTRO framework, we first analyze the missing safety mechanisms in MCP, and based on this analysis, we propose the Model Contextual Integrity Protocol (MCIP), a refined version of MCP that addresses these gaps. Next, we develop a fine-grained taxonomy that captures a diverse range of unsafe behaviors observed in MCP scenarios. Building on this taxonomy, we develop benchmark and training data that support the evaluation and improvement of LLMs' capabilities in identifying safety risks within MCP interactions. Leveraging the proposed benchmark and training data, we conduct extensive experiments on state-of-the-art LLMs. The results highlight LLMs' vulnerabilities in MCP interactions and demonstrate that our approach substantially improves their safety performance",
    "checked": true,
    "id": "38b67dc681a674da55fdedecf6491f4006cfba6a",
    "semantic_title": "mcip: protecting mcp safety via model contextual integrity protocol",
    "citation_count": 12,
    "authors": [
      "Huihao Jing",
      "Haoran Li",
      "Wenbin Hu",
      "Qi Hu",
      "Xu Heli",
      "Tianshu Chu",
      "Peizhao Hu",
      "Yangqiu Song"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.63": {
    "title": "SAKI-RAG: Mitigating Context Fragmentation in Long-Document RAG via Sentence-level Attention Knowledge Integration",
    "volume": "main",
    "abstract": "Traditional Retrieval-Augmented Generation (RAG) frameworks often segment documents into larger chunks to preserve contextual coherence, inadvertently introducing redundant noise. Recent advanced RAG frameworks have shifted toward finer-grained chunking to improve precision. However, in long-document scenarios, such chunking methods lead to fragmented contexts, isolated chunk semantics, and broken inter-chunk relationships, making cross-paragraph retrieval particularly challenging. To address this challenge, maintaining granular chunks while recovering their intrinsic semantic connections, we propose **SAKI-RAG** (Sentence-level Attention Knowledge Integration Retrieval-Augmented Generation). Our framework introduces two core components: (1) the **SentenceAttnLinker**, which constructs a semantically enriched knowledge repository by modeling inter-sentence attention relationships, and (2) the **Dual-Axis Retriever**, which is designed to expand and filter the candidate chunks from the dual dimensions of semantic similarity and contextual relevance. Experimental results across four datasets—Dragonball, SQUAD, NFCORPUS, and SCI-DOCS demonstrate that SAKI-RAG achieves better recall and precision compared to other RAG frameworks in long-document retrieval scenarios, while also exhibiting higher information efficiency",
    "checked": true,
    "id": "407baa890a1f33f0b7a64453c119fd635724e033",
    "semantic_title": "saki-rag: mitigating context fragmentation in long-document rag via sentence-level attention knowledge integration",
    "citation_count": 0,
    "authors": [
      "Wenyu Tao",
      "Xiaofen Xing",
      "Zeliang Li",
      "Xiangmin Xu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.64": {
    "title": "Skeletons Matter: Dynamic Data Augmentation for Text-to-Query",
    "volume": "main",
    "abstract": "The task of translating natural language questions into query languages has long been a central focus in semantic parsing. Recent advancements in Large Language Models (LLMs) have significantly accelerated progress in this field. However, existing studies typically focus on a single query language, resulting in methods with limited generalizability across different languages. In this paper, we formally define the Text-to-Query task paradigm, unifying semantic parsing tasks across various query languages. We identify query skeletons as a shared optimization target of Text-to-Query tasks, and propose a general dynamic data augmentation framework that explicitly diagnoses model-specific weaknesses in handling these skeletons to synthesize targeted training data. Experiments on four Text-to-Query benchmarks demonstrate that our method achieves state-of-the-art performance using only a small amount of synthesized data, highlighting the efficiency and generality of our approach and laying a solid foundation for unified research on Text-to-Query tasks. We release our code at https://github.com/jjjycaptain/Skeletron",
    "checked": true,
    "id": "40dc4e2abed06095d1055daa1a96f485251a86df",
    "semantic_title": "skeletons matter: dynamic data augmentation for text-to-query",
    "citation_count": 0,
    "authors": [
      "Yuchen Ji",
      "Bo Xu",
      "Jie Shi",
      "Jiaqing Liang",
      "Deqing Yang",
      "Yu Mao",
      "Hai Chen",
      "Yanghua Xiao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.65": {
    "title": "CondenseLM: LLMs-driven Text Dataset Condensation via Reward Matching",
    "volume": "main",
    "abstract": "Dataset condensation has emerged as a promising technique to improve data efficiency under limited data budgets. However, when applied to the text level, existing methods struggle to compress more information into samples through optimization. Thus, these methods provide no obvious advantage over simpler coreset selection despite their high computational cost. In this paper, we introduce CondenseLM, a novel paradigm for both effective and efficient text-level dataset condensation. Our framework employs an LLMs-driven approach to sidestep the inherent limitations of existing methods, successfully generating more informative and less biased samples. In addition, it incorporates reward matching to align the LLMs-condensed dataset with the original dataset, maximizing representability and coverage. We conducted extensive experiments on SST-2, MNLI, AG News, and IMDB. Our approach outperforms both coreset selection and existing dataset condensation methods by large margins while also substantially reducing the computational cost",
    "checked": true,
    "id": "978cb5eb5c0352dfe12484b0e8191bc426888e52",
    "semantic_title": "condenselm: llms-driven text dataset condensation via reward matching",
    "citation_count": 0,
    "authors": [
      "Cheng Shen",
      "Yew-Soon Ong",
      "Joey Tianyi Zhou"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.66": {
    "title": "MovieCORE: COgnitive REasoning in Movies",
    "volume": "main",
    "abstract": "This paper introduces MovieCORE, a novel video question answering (VQA) dataset designed to probe deeper cognitive understanding of movie content. Unlike existing datasets that focus on surface-level comprehension, MovieCORE emphasizes questions that engage System-2 thinking while remaining specific to the video material. We present an innovative agentic brainstorming approach, utilizing multiple large language models (LLMs) as thought agents to generate and refine high-quality question-answer pairs. To evaluate dataset quality, we develop a set of cognitive tests assessing depth, thought-provocation potential, and syntactic complexity. We also propose a comprehensive evaluation scheme for assessing VQA model performance on deeper cognitive tasks. To address the limitations of existing video-language models (VLMs), we introduce an agentic enhancement module, Agentic Choice Enhancement (ACE), which improves model reasoning capabilities post-training by 25%. Our work contributes to advancing movie understanding in AI systems and provides valuable insights into the capabilities and limitations of current VQA models when faced with more challenging, nuanced questions about cinematic content. Our project page, dataset and code can be found at https://joslefaure.github.io/assets/html/moviecore.html",
    "checked": true,
    "id": "83b6dc3cbc6d0fcefbcae9299df22a85eb9488e8",
    "semantic_title": "moviecore: cognitive reasoning in movies",
    "citation_count": 0,
    "authors": [
      "Gueter Josmy Faure",
      "Min-Hung Chen",
      "Jia-Fong Yeh",
      "Ying Cheng",
      "Hung-Ting Su",
      "Yung-Hao Tang",
      "Shang-Hong Lai",
      "Winston H. Hsu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.67": {
    "title": "Think Wider, Detect Sharper: Reinforced Reference Coverage for Document-Level Self-Contradiction Detection",
    "volume": "main",
    "abstract": "Detecting self-contradictions within documents is a challenging task for ensuring textual coherence and reliability. While large language models (LLMs) have advanced in many natural language understanding tasks, document-level self-contradiction detection (DSCD) remains insufficiently studied. Recent approaches leveraging Chain-of-Thought (CoT) prompting aim to enhance reasoning and interpretability; however, they only gain marginal improvement and often introduce inconsistencies across repeated responses. We observe that such inconsistency arises from incomplete reasoning chains that fail to include all relevant contradictory sentences consistently. To address this, we propose a two-stage method that combines supervised fine-tuning (SFT) and reinforcement learning (RL) to enhance DSCD performance. In the SFT phase, a teacher model helps the model learn reasoning patterns, while RL further refines its reasoning ability. Our method incorporates a task-specific reward function to expand the model's reasoning scope, boosting both accuracy and consistency. On the ContraDoc benchmark, our approach significantly boosts Llama 3.1-8B-Instruct's accuracy from 38.5% to 51.1%, and consistency from 59.6% to76.2%",
    "checked": true,
    "id": "7bc862f4181e83cbd9dbc5b08b73d5689bc4613c",
    "semantic_title": "think wider, detect sharper: reinforced reference coverage for document-level self-contradiction detection",
    "citation_count": 0,
    "authors": [
      "Yuhao Chen",
      "Yuanjie Lyu",
      "Shuochen Liu",
      "Chao Zhang",
      "Junhui Lv",
      "Tong Xu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.68": {
    "title": "DRISHTIKON: A Multimodal Multilingual Benchmark for Testing Language Models' Understanding on Indian Culture",
    "volume": "main",
    "abstract": "We introduce DRISHTIKON, a first-of-its-kind multimodal and multilingual benchmark centered exclusively on Indian culture, designed to evaluate the cultural understanding of generative AI systems. Unlike existing benchmarks with a generic or global scope, DRISHTIKON offers deep, fine-grained coverage across India's diverse regions, spanning 15 languages, covering all states and union territories, and incorporating over 64,000 aligned text-image pairs. The dataset captures rich cultural themes including festivals, attire, cuisines, art forms, and historical heritage amongst many more. We evaluate a wide range of vision-language models (VLMs), including open-source small and large models, proprietary systems, reasoning-specialized VLMs, and Indic-focused models—across zero-shot and chain-of-thought settings. Our results expose key limitations in current models' ability to reason over culturally grounded, multimodal inputs, particularly for low-resource languages and less-documented traditions. DRISHTIKON fills a vital gap in inclusive AI research, offering a robust testbed to advance culturally aware, multimodally competent language technologies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arijit Maji",
      "Raghvendra Kumar",
      "Akash Ghosh",
      "Anushka",
      "Nemil Shah",
      "Abhilekh Borah",
      "Vanshika Shah",
      "Nishant Mishra",
      "Sriparna Saha"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.69": {
    "title": "LingGym: How Far Are LLMs from Thinking Like Field Linguists?",
    "volume": "main",
    "abstract": "This paper introduces LingGym, a new benchmark that evaluates LLMs' capacity for meta-linguistic reasoning using Interlinear Glossed Text (IGT) and grammatical descriptions extracted from 18 typologically diverse reference grammars. Unlike previous work that focuses on specific downstream tasks, we assess whether LLMs can generalize linguistic inference across low-resource languages and structures not seen during training. We present a controlled evaluation task: Word-Gloss Inference, in which the model must infer a missing word and gloss from context using varying levels of linguistic information (e.g., glosses, grammatical explanations, translations). Our results show that incorporating structured linguistic cues leads to consistent improvements in reasoning performance across all models. This work highlights both the promise and current limitations of using LLMs for typologically informed linguistic analysis and low-resource language documentation",
    "checked": true,
    "id": "51d523825d48317aa4e69ae19d6ee117d225bc6f",
    "semantic_title": "linggym: how far are llms from thinking like field linguists?",
    "citation_count": 0,
    "authors": [
      "Changbing Yang",
      "Franklin Ma",
      "Freda Shi",
      "Jian Zhu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.70": {
    "title": "Learning from Few Samples: A Novel Approach for High-Quality Malcode Generation",
    "volume": "main",
    "abstract": "Intrusion Detection Systems (IDS) play a crucial role in network security defense. However, a significant challenge for IDS in training detection models is the shortage of adequately labeled malicious samples. To address these issues, this paper introduces a novel semi-supervised framework GANGRL-LLM, which integrates Generative Adversarial Networks (GANs) with Large Language Models (LLMs) to enhance malicious code generation and SQL Injection (SQLi) detection capabilities in few-sample learning scenarios. Specifically, our framework adopts a collaborative training paradigm where: (1) the GAN-based discriminator improves malicious pattern recognition through adversarial learning with generated samples and limited real samples; and (2) the LLM-based generator refines the quality of malicious code synthesis using reward signals from the discriminator. The experimental results demonstrate that even with a limited number of labeled samples, our training framework is highly effective in enhancing both malicious code generation and detection capabilities. This dual enhancement capability offers a promising solution for developing adaptive defense systems capable of countering evolving cyber threats",
    "checked": true,
    "id": "538d3ed384dbdca3f1befbb8c3dddcbbffff652f",
    "semantic_title": "learning from few samples: a novel approach for high-quality malcode generation",
    "citation_count": 0,
    "authors": [
      "Haijian Ma",
      "Daizong Liu",
      "Xiaowen Cai",
      "Pan Zhou",
      "Yulai Xie"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.71": {
    "title": "Personality Matters: User Traits Predict LLM Preferences in Multi-Turn Collaborative Tasks",
    "volume": "main",
    "abstract": "As Large Language Models (LLMs) increasingly integrate into everyday workflows, where users shape outcomes through multi-turn collaboration, a critical question emerges: do users with different personality traits systematically prefer certain LLMs over others? We conduc-ted a study with 32 participants evenly distributed across four Keirsey personality types, evaluating their interactions with GPT-4 and Claude 3.5 across four collaborative tasks: data analysis, creative writing, information retrieval, and writing assistance. Results revealed significant personality-driven preferences: *Rationals* strongly preferred GPT-4, particularly for goal-oriented tasks, while *idealists* favored Claude 3.5, especially for creative and analytical tasks. Other personality types showed task-dependent preferences. Sentiment analysis of qualitative feedback confirmed these patterns. Notably, aggregate helpfulness ratings were similar across models, showing how personality-based analysis reveals LLM differences that traditional evaluations miss",
    "checked": true,
    "id": "ef07ad8269710fe7c93dee0579770fa240bd40da",
    "semantic_title": "personality matters: user traits predict llm preferences in multi-turn collaborative tasks",
    "citation_count": 0,
    "authors": [
      "Sarfaroz Yunusov",
      "Kaige Chen",
      "Kazi Nishat Anwar",
      "Ali Emami"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.72": {
    "title": "VisualWebInstruct: Scaling up Multimodal Instruction Data through Web Search",
    "volume": "main",
    "abstract": "Vision-Language Models have made significant progress on many perception-focused tasks. However, their progress on reasoning-focused tasks remains limited due to the lack of high-quality and diverse training data. In this work, we aim to address the scarcity of reasoning-focused multimodal datasets. We propose VisualWebInstruct, a novel approach that leverages search engines to create a diverse and high-quality dataset spanning multiple disciplines, including mathematics, physics, finance, and chemistry, etc. Starting with a meticulously selected set of 30,000 seed images, we employ Google Image Search to identify websites containing similar images. We collect and process HTML data from over 700K unique URLs. Through a pipeline of content extraction, filtering, and synthesis, we construct a dataset of approximately 900K question-answer (QA) pairs, with 40% consisting of visual QA pairs and the remaining comprising text-based QA pairs. Models fine-tuned on VisualWebInstruct demonstrate significant performance improvements: (1) fine-tuning on Llava-OV results in 10-20 absolute points improvement across benchmarks, and (2) fine-tuning from MAmmoTH-VL yields a 5 absolute points gain across benchmarks. Our best model, MAmmoTH-VL2, achieves the best known performance with SFT without RL within the 10B parameter class on MMMU-Pro (40.7), MathVerse (42.6), and DynaMath (55.7). These results highlight the effectiveness of our dataset in enhancing the reasoning capabilities of vision-language models for complex multimodal tasks",
    "checked": true,
    "id": "10f9f4a1284a62bfdda89777c1e62d2cdc6c1b9e",
    "semantic_title": "visualwebinstruct: scaling up multimodal instruction data through web search",
    "citation_count": 13,
    "authors": [
      "Yiming Jia",
      "Jiachen Li",
      "Xiang Yue",
      "Bo Li",
      "Ping Nie",
      "Kai Zou",
      "Wenhu Chen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.73": {
    "title": "Thinking Out Loud: Do Reasoning Models Know When They're Right?",
    "volume": "main",
    "abstract": "Large reasoning models (LRMs) have recently demonstrated impressive capabilities in complex reasoning tasks by leveraging increased test-time computation and exhibiting behaviors reminiscent of human-like self-reflection. While LRMs show a clear capacity for valuable self-reflection, how this ability interacts with other model behaviors remains underexplored. We investigate this connection by analyzing verbalized confidence, how models articulate their certainty, as a lens into the nature of self-reflection in LRMs. We find that supervised fine-tuning on reasoning traces (i.e., distillation) and reinforcement learning can improve verbalized calibration in reasoning-intensive settings in a progressive, laddered fashion. However, our results also indicate that reasoning models may possess a diminished awareness of their own knowledge boundaries, as evidenced by significantly lower \"I don't know\" response rates on factuality benchmarks. Moreover, we examine the relationship between verbalized confidence and reasoning chains, finding that models tend to express higher confidence when providing shorter or less elaborate reasoning. Our findings highlight how reasoning-oriented training can enhance performance in reasoning-centric tasks while potentially incurring a reasoning tax, a cost reflected in the model's reduced ability to accurately recognize the limits of its own knowledge in small-scale models. More broadly, our work showcases how this erosion of knowledge boundaries can compromise model faithfulness, as models grow more confident without a commensurate understanding of when they should abstain",
    "checked": true,
    "id": "30a1ce008eb2ebfb362e78d3df1de5e5e45e7cac",
    "semantic_title": "thinking out loud: do reasoning models know when they're right?",
    "citation_count": 3,
    "authors": [
      "Qingcheng Zeng",
      "Weihao Xuan",
      "Leyang Cui",
      "Rob Voigt"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.74": {
    "title": "Seeing is Believing, but How Much? A Comprehensive Analysis of Verbalized Calibration in Vision-Language Models",
    "volume": "main",
    "abstract": "Uncertainty quantification is essential for assessing the reliability and trustworthiness of modern AI systems. Among existing approaches, verbalized uncertainty, where models express their confidence through natural language, has emerged as a lightweight and interpretable solution in large language models (LLMs). However, its effectiveness in vision-language models (VLMs) remains insufficiently studied. In this work, we conduct a comprehensive evaluation of verbalized confidence in VLMs, spanning three model categories, four task domains, and three evaluation scenarios. Our results show that current VLMs often display notable miscalibration across diverse tasks and settings. Notably, visual reasoning models (i.e., thinking with images) consistently exhibit better calibration, suggesting that modality-specific reasoning is critical for reliable uncertainty estimation. To further address calibration challenges, we introduce Visual Confidence-Aware Prompting, a two-stage prompting strategy that improves confidence alignment in multimodal settings. Overall, our study highlights the inherent miscalibration in VLMs across modalities. More broadly, our findings underscore the fundamental importance of modality alignment and model faithfulness in advancing reliable multimodal systems",
    "checked": true,
    "id": "aa0bdba0d7e949a90f15f00806c23890bbf0cd72",
    "semantic_title": "seeing is believing, but how much? a comprehensive analysis of verbalized calibration in vision-language models",
    "citation_count": 2,
    "authors": [
      "Weihao Xuan",
      "Qingcheng Zeng",
      "Heli Qi",
      "Junjue Wang",
      "Naoto Yokoya"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.75": {
    "title": "Enhancing Efficiency and Exploration in Reinforcement Learning for LLMs",
    "volume": "main",
    "abstract": "Reasoning large language models (LLMs) excel in complex tasks, which has drawn significant attention to reinforcement learning (RL) for LLMs. However, existing approaches allocate an equal number of rollouts to all questions during the RL process, which is inefficient. This inefficiency stems from the fact that training on simple questions yields limited gains, whereas more rollouts are needed for challenging questions to sample correct answers. Furthermore, while RL improves response precision, it limits the model's exploration ability, potentially resulting in a performance cap below that of the base model prior to RL. To address these issues, we propose a mechanism for dynamically allocating rollout budgets based on the difficulty of the problems, enabling more efficient RL training. Additionally, we introduce an adaptive dynamic temperature adjustment strategy to maintain the entropy at a stable level, thereby encouraging sufficient exploration. This enables LLMs to improve response precision while preserving their exploratory ability to uncover potential correct pathways. The code and data is available on: https://anonymous.4open.science/r/E3-RL4LLMs-DB28",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengqi Liao",
      "Xiangyu Xi",
      "Chen Ruinian",
      "Jia Leng",
      "Yangen Hu",
      "Ke Zeng",
      "Shuai Liu",
      "Huaiyu Wan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.76": {
    "title": "LLM Bias Detection and Mitigation through the Lens of Desired Distributions",
    "volume": "main",
    "abstract": "Although prior work on bias mitigation has focused on promoting social equality and demographic parity, less attention has been given to aligning LLM's outputs to desired distributions. For example, we might want to align a model with real-world distributions to support factual grounding. Thus, we define bias as deviation from a desired distribution, which may be an equal or real-world distribution, depending on application goals. We propose a weighted adaptive loss based fine-tuning method that aligns LLM's gender–profession output distribution with the desired distribution, while preserving language modeling capability. Using 3 profession sets—male-dominated, female-dominated, and gender-balanced—derived from U.S. labor statistics (2024), we assess both our adaptive method for reflecting reality and a non-adaptive variant for equality. Across three masked language models, bias is observed under both distributions. We achieve near-complete mitigation under equality and 30–75% reduction under real-world settings. Autoregressive LLMs show no bias under equality but notable bias under real-world settings, with the Llama Instruct models (3.2-3B, 3.1-8B) achieving a 50–62% reduction",
    "checked": true,
    "id": "62c87f151c3bea95da345d6fbfd14ca88cbc67e1",
    "semantic_title": "llm bias detection and mitigation through the lens of desired distributions",
    "citation_count": 0,
    "authors": [
      "Ingroj Shrestha",
      "Padmini Srinivasan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.77": {
    "title": "MEBench: Benchmarking Large Language Models for Cross-Document Multi-Entity Question Answering",
    "volume": "main",
    "abstract": "Cross-Document Multi-entity question answering (MEQA) demands the integration of scattered information across documents to resolve complex queries involving entities, relationships, and contextual dependencies. Although Large Language Models (LLMs) and Retrieval-augmented Generation (RAG) systems show promise, their performance on cross-document MEQA remains underexplored due to the absence of tailored benchmarks. To address this gap, we introduce MEBench, a scalable multi-document, multi-entity benchmark designed to systematically evaluate LLMs' capacity to retrieve, consolidate, and reason over scattered and dense information. Our benchmark comprises 4,780 questions which are systematically categorized into three primary categories: Comparative Reasoning, Statistical Reasoning and Relational Reasoning, further divided into eight distinct types, ensuring broad coverage of real-world multi-entity reasoning scenarios. Our experiments on state-of-the-art LLMs reveal critical limitations: even advanced models achieve only 59% accuracy on MEBench. Our benchmark emphasizes the importance of completeness and factual precision of information extraction in MEQA tasks, using Entity-Attributed F1 (EA-F1) metric for granular evaluation of entity-level correctness and attribution validity. MEBench not only highlights systemic weaknesses in current LLM frameworks but also provides a foundation for advancing robust, entity-aware QA architectures",
    "checked": true,
    "id": "189df8bd554221b701a4181866d2f8a5232d8c0e",
    "semantic_title": "mebench: benchmarking large language models for cross-document multi-entity question answering",
    "citation_count": 6,
    "authors": [
      "Teng Lin",
      "Yuyu Luo",
      "Honglin Zhang",
      "Jicheng Zhang",
      "Chunlin Liu",
      "Kaishun Wu",
      "Nan Tang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.78": {
    "title": "POSITION BIAS MITIGATES POSITION BIAS: Mitigate Position Bias Through Inter-Position Knowledge Distillation",
    "volume": "main",
    "abstract": "Positional bias (PB), manifesting as non-uniform sensitivity across different contextual locations, significantly impairs long-context comprehension and processing capabilities. Previous studies have addressed PB either by modifying the underlying architectures or by employing extensive contextual awareness training. However, the former approach fails to effectively eliminate the substantialperformance disparities, while the latter imposes significant data and computational overhead. To address PB effectively, we introduce Pos2Distill, a position to position knowledge distillation framework. Pos2Distill transfers the superior capabilities from advantageous positions to less favorable ones, thereby reducing the huge performance gaps. The conceptual principle is to leverage the inherent, position-induced disparity to counteract the PB itself. We identify distinct manifestations of PB under retrieval and reasoning paradigms, thereby designing two specialized instantiations: Pos2Distill-R1 and Pos2Distill-R2 respectively, both grounded in this core principle. By employing the Pos2Distill approach, we achieve enhanced uniformity and significant performance gains across all contextual positions in long-context retrieval and reasoning tasks. Crucially, both specialized systems exhibit strong cross-task generalization mutually, while achieving superior performance on their respective tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Wang",
      "Feng Xiong",
      "Yong Wang",
      "Linjing Li",
      "Xiangxiang Chu",
      "Daniel Dajun Zeng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.79": {
    "title": "MMLU-ProX: A Multilingual Benchmark for Advanced Large Language Model Evaluation",
    "volume": "main",
    "abstract": "Existing large language model (LLM) evaluation benchmarks primarily focus on English, while current multilingual tasks lack parallel questions that specifically assess cross-lingual reasoning abilities. This dual limitation makes it challenging to assess LLMs' performance in the multilingual setting comprehensively. To fill this gap, we introduce MMLU-ProX, a comprehensive benchmark covering 29 languages, built on an English benchmark. Each language version consists of 11,829 identical questions, enabling direct cross-lingual comparisons. Additionally, to meet efficient evaluation needs, we provide a lite version containing 658 questions per language. To ensure the high quality of MMLU-ProX, we employ a rigorous development process that involves multiple powerful LLMs for translation, followed by expert review to ensure accurate expression, consistent terminology, and cultural relevance. Building on this, we systematically evaluate 36 state-of-the-art LLMs, including reasoning-enhanced and multilingual-optimized LLMs. The results reveal significant disparities in the multilingual capabilities of LLMs: While they perform well in high-resource languages, their performance declines markedly in low-resource languages, particularly for African languages. Through MMLU-ProX, we aim to advance the development of more inclusive AI systems and promote equitable access to technology across global contexts",
    "checked": true,
    "id": "4fd7dfbb3400ce60cdedfb679185c35f41bdde62",
    "semantic_title": "mmlu-prox: a multilingual benchmark for advanced large language model evaluation",
    "citation_count": 24,
    "authors": [
      "Weihao Xuan",
      "Rui Yang",
      "Heli Qi",
      "Qingcheng Zeng",
      "Yunze Xiao",
      "Aosong Feng",
      "Dairui Liu",
      "Yun Xing",
      "Junjue Wang",
      "Fan Gao",
      "Jinghui Lu",
      "Yuang Jiang",
      "Huitao Li",
      "Xin Li",
      "Kunyu Yu",
      "Ruihai Dong",
      "Shangding Gu",
      "Yuekang Li",
      "Xiaofei Xie",
      "Felix Juefei-Xu",
      "Foutse Khomh",
      "Osamu Yoshie",
      "Qingyu Chen",
      "Douglas Teodoro",
      "Nan Liu",
      "Randy Goebel",
      "Lei Ma",
      "Edison Marrese-Taylor",
      "Shijian Lu",
      "Yusuke Iwasawa",
      "Yutaka Matsuo",
      "Irene Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.80": {
    "title": "NL-Debugging: Exploiting Natural Language as an Intermediate Representation for Code Debugging",
    "volume": "main",
    "abstract": "Debugging is a critical aspect of LLM's coding ability. Early debugging efforts primarily focused on code-level analysis, which often falls short when addressing complex programming errors that require a deeper understanding of algorithmic logic. Recent advancements in large language models (LLMs) have shifted attention toward leveraging natural language reasoning to enhance code-related tasks. However, two fundamental questions remain unanswered: What type of natural language format is most effective for debugging tasks? And what specific benefits does natural language reasoning bring to the debugging process? In this paper, we introduce NL-DEBUGGING, a novel framework that employs natural language as an intermediate representation to improve code debugging. By debugging at a natural language level, we demonstrate that NL-DEBUGGING outperforms traditional debugging methods and enables a broader modification space through direct refinement guided by execution feedback. Our findings highlight the potential of natural language reasoning to advance automated code debugging and address complex programming challenges",
    "checked": true,
    "id": "6bf369fde08999cec873f7f848226efa81837f37",
    "semantic_title": "nl-debugging: exploiting natural language as an intermediate representation for code debugging",
    "citation_count": 0,
    "authors": [
      "Weiming Zhang",
      "Qingyao Li",
      "Xinyi Dai",
      "Jizheng Chen",
      "Kounianhua Du",
      "Weiwen Liu",
      "Yasheng Wang",
      "Ruiming Tang",
      "Yong Yu",
      "Weinan Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.81": {
    "title": "Persuasion Dynamics in LLMs: Investigating Robustness and Adaptability in Knowledge and Safety with DuET-PD",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) can struggle to balance gullibility to misinformation and resistance to valid corrections in persuasive dialogues, a critical challenge for reliable deployment. We introduce **DuET-PD** (**Du**al **E**valuation for **T**rust in **P**ersuasive **D**ialogues), a framework evaluating multi-turn stance-change dynamics across dual dimensions: persuasion type (corrective/misleading) and domain (knowledge via MMLU-Pro, and safety via SALAD-Bench). We find that even a state-of-the-art model like GPT-4o achieves only 27.32% accuracy in MMLU-Pro under sustained misleading persuasions. Moreover, results reveal a concerning trend of increasing sycophancy in newer open-source models. To address this, we introduce Holistic DPO, a training approach balancing positive and negative persuasion examples. Unlike prompting or resist-only training, Holistic DPO enhances both robustness to misinformation and receptiveness to corrections, improving Llama-3.1-8B-Instruct's accuracy under misleading persuasion in safety contexts from 4.21% to 76.54%. These contributions offer a pathway to developing more reliable and adaptable LLMs for multi-turn dialogue. Code is available at https://github.com/Social-AI-Studio/DuET-PD",
    "checked": true,
    "id": "99a80610ba44f099ef9bc73026fb257dacb92c06",
    "semantic_title": "persuasion dynamics in llms: investigating robustness and adaptability in knowledge and safety with duet-pd",
    "citation_count": 2,
    "authors": [
      "Bryan Chen Zhengyu Tan",
      "Daniel Wai Kit Chin",
      "Zhengyuan Liu",
      "Nancy F. Chen",
      "Roy Ka-Wei Lee"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.82": {
    "title": "POINTS-Reader: Distillation-Free Adaptation of Vision-Language Models for Document Conversion",
    "volume": "main",
    "abstract": "High-quality labeled data is essential for training accurate document conversion models, particularly in domains with complex formats such as tables, formulas, and multi-column text. However, manual annotation is both costly and time-consuming, while automatic labeling using existing models often lacks accuracy in handling such challenging scenarios. Consequently, training student models by distilling outputs from teacher models can significantly limit their performance in real-world applications. In this paper, we propose a fully automated, distillation-free framework comprising two stages for constructing high-quality document extraction datasets and models capable of handling diverse document formats and layouts. In the first stage, we introduce a method for generating large-scale, diverse synthetic data, which enables a model to extract key elements in a unified format with strong initial performance. In the second stage, we present a self-improvement approach that further adapts the model, initially trained on synthetic data, to real-world documents. Specifically, we first use the fine-tuned model to annotate real documents, then apply a suite of filtering strategies to verify annotation quality, and finally retrain the model on the verified dataset. By iteratively repeating this process, we progressively enhance both the model's conversion capabilities and the quality of the generated data. We train a public POINTS-1.5 model to obtain POINTS-Reader, which surpasses many existing public and proprietary models of comparable or larger size. Our model will be made publicly available",
    "checked": true,
    "id": "377d4ffd34af6ed4381809b7c3b6569be420c22f",
    "semantic_title": "points-reader: distillation-free adaptation of vision-language models for document conversion",
    "citation_count": 0,
    "authors": [
      "Yuan Liu",
      "Zhongyin Zhao",
      "Le Tian",
      "Haicheng Wang",
      "Xubing Ye",
      "Yangxiu You",
      "Zilin Yu",
      "Chuhan Wu",
      "Zhou Xiao",
      "Yang Yu",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.83": {
    "title": "Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition",
    "volume": "main",
    "abstract": "Large language models (LLMs) have emerged as a potential solution to automate the complex processes involved in writing literature reviews, such as literature collection, organization, and summarization. However, it is yet unclear how good LLMs are at automating comprehensive and reliable literature reviews. This study introduces a framework to automatically evaluate the performance of LLMs in three key tasks of literature review writing: reference generation, abstract writing, and literature review composition. We introduce multidimensional evaluation metrics that assess the hallucination rates in generated references and measure the semantic coverage and factual consistency of the literature summaries and compositions against human-written counterparts. The experimental results reveal that even the most advanced models still generate hallucinated references, despite recent progress. Moreover, we observe that the performance of different models varies across disciplines when it comes to writing literature reviews. These findings highlight the need for further research and development to improve the reliability of LLMs in automating academic literature reviews. The dataset and code used in this study are publicly available in our GitHub repository",
    "checked": true,
    "id": "deec3b70aea74f3abcde01ce8ca00b39eec5f157",
    "semantic_title": "large language models for automated literature review: an evaluation of reference generation, abstract writing, and review composition",
    "citation_count": 3,
    "authors": [
      "Xuemei Tang",
      "Xufeng Duan",
      "Zhenguang Cai"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.84": {
    "title": "CoBia: Constructed Conversations Can Trigger Otherwise Concealed Societal Biases in LLMs",
    "volume": "main",
    "abstract": "Improvements in model construction, including fortified safety guardrails, allow Large language models (LLMs) to increasingly pass standard safety checks. However, LLMs sometimes slip into revealing harmful behavior, such as expressing racist viewpoints, during conversations. To analyze this systematically, we introduce CoBia, a suite of lightweight adversarial attacks that allow us to refine the scope of conditions under which LLMs depart from normative or ethical behavior in conversations. CoBia creates a constructed conversation where the model utters a biased claim about a social group. We then evaluate whether the model can recover from the fabricated bias claim and reject biased follow-up questions.We evaluate 11 open-source as well as proprietary LLMs for their outputs related to six socio-demographic categories that are relevant to individual safety and fair treatment, i.e., gender, race, religion, nationality, sex orientation, and others. Our evaluation is based on established LLM-based bias metrics, and we compare the results against human judgments to scope out the LLMs' reliability and alignment. The results suggest that purposefully constructed conversations reliably reveal bias amplification and that LLMs often fail to reject biased follow-up questions during dialogue. This form of stress-testing highlights deeply embedded biases that can be surfaced through interaction. Code and artifacts are available at https://github.com/nafisenik/CoBia",
    "checked": true,
    "id": "383a142330ea31e6fd84b2312463c6617b6a4ddb",
    "semantic_title": "cobia: constructed conversations can trigger otherwise concealed societal biases in llms",
    "citation_count": 0,
    "authors": [
      "Nafiseh Nikeghbal",
      "Amir Hossein Kargaran",
      "Jana Diesner"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.85": {
    "title": "From Schema to State: Zero-Shot Scheme-Only Dialogue State Tracking via Diverse Synthetic Dialogue and Step-by-Step Distillation",
    "volume": "main",
    "abstract": "Dialogue State Tracking (DST) is crucial for linking user intentions to appropriate services in task-oriented dialogue systems. We propose a zero-shot, scheme-only approach that tackles two main challenges: generating synthetic dialogues that balance diversity with schema alignment, and efficiently distilling knowledge from a large language model (LLM) into a smaller model. Our pipeline first creates scenarios, dialogue logic flows, and utterances via dynamic complexity prompting, eliminating reliance on handcrafted templates. We then use a two-stage distillation process to learn formalized dialogue representations and DST related chain-of-thought reasoning. This structure preserves interpretive capabilities while reducing inference overhead. Experiments on the MultiWOZ benchmark show that our method achieves state-of-the-art performance under zero-shot, scheme-only situation and generalizes effectively to few-shot scenarios, offering a practical and scalable solution for domains lacking real data",
    "checked": true,
    "id": "b52143dea8a51cffd7c2c9438f71cfa313920f22",
    "semantic_title": "from schema to state: zero-shot scheme-only dialogue state tracking via diverse synthetic dialogue and step-by-step distillation",
    "citation_count": 0,
    "authors": [
      "Huan Xu",
      "Zequn Li",
      "Wen Tang",
      "Jian Jun Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.86": {
    "title": "Beyond the Surface: Measuring Self-Preference in LLM Judgments",
    "volume": "main",
    "abstract": "Recent studies show that large language models (LLMs) exhibit self-preference bias when serving as judges, meaning they tend to favor their own responses over those generated by other models. Existing methods typically measure this bias by calculating the difference between the scores a judge model assigns to its own responses and those it assigns to responses from other models. However, this approach conflates self-preference bias with response quality, as higher-quality responses from the judge model may also lead to positive score differences, even in the absence of bias. To address this issue, we introduce gold judgments as proxies for the actual quality of responses and propose the DBG score, which measures self-preference bias as the difference between the scores assigned by the judge model to its own responses and the corresponding gold judgments. Since gold judgments reflect true response quality, the DBG score mitigates the confounding effect of response quality on bias measurement. Using the DBG score, we conduct comprehensive experiments to assess self-preference bias across LLMs of varying versions, sizes, and reasoning abilities. Additionally, we investigate two factors that influence and help alleviate self-preference bias: response text style and the post-training data of judge models. Finally, we explore potential underlying mechanisms of self-preference bias from an attention-based perspective. Our code and data are available at https://github.com/zhiyuanc2001/self-preference",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhi-Yuan Chen",
      "Hao Wang",
      "Xinyu Zhang",
      "Enrui Hu",
      "Yankai Lin"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.87": {
    "title": "Beyond Input Activations: Identifying Influential Latents by Gradient Sparse Autoencoders",
    "volume": "main",
    "abstract": "Sparse Autoencoders (SAEs) have recently emerged as powerful tools for interpreting and steering the internal representations of large language models (LLMs). However, conventional approaches to analyzing SAEs typically rely solely on input-side activations, without considering the influence between each latent feature and the model's output. This work is built on two key hypotheses: (1) activated latents do not contribute equally to the construction of the model's output, and (2) only latents with high influence are effective for model steering. To validate these hypotheses, we propose Gradient Sparse Autoencoder (GradSAE), a simple yet effective method that identifies the most influential latents by incorporating output-side gradient information",
    "checked": true,
    "id": "10b7d80b52de85af7c59586a4141055ac459dde0",
    "semantic_title": "beyond input activations: identifying influential latents by gradient sparse autoencoders",
    "citation_count": 2,
    "authors": [
      "Dong Shu",
      "Xuansheng Wu",
      "Haiyan Zhao",
      "Mengnan Du",
      "Ninghao Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.88": {
    "title": "Utility-Focused LLM Annotation for Retrieval and Retrieval-Augmented Generation",
    "volume": "main",
    "abstract": "This paper explores the use of large language models (LLMs) for annotating document utility in training retrieval and retrieval-augmented generation (RAG) systems, aiming to reduce dependence on costly human annotations. We address the gap between retrieval relevance and generative utility by employing LLMs to annotate document utility. To effectively utilize multiple positive samples per query, we introduce a novel loss that maximizes their summed marginal likelihood. Using the Qwen-2.5-32B model, we annotate utility on the MS MARCO dataset and conduct retrieval experiments on MS MARCO and BEIR, as well as RAG experiments on MS MARCO QA, NQ, and HotpotQA. Our results show that LLM-generated annotations enhance out-of-domain retrieval performance and improve RAG outcomes compared to models trained solely on human annotations or downstream QA metrics. Furthermore, combining LLM annotations with just 20% of human labels achieves performance comparable to using full human annotations. Our study offers a comprehensive approach to utilizing LLM annotations for initializing QA systems on new corpora",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hengran Zhang",
      "Minghao Tang",
      "Keping Bi",
      "Jiafeng Guo",
      "Shihao Liu",
      "Daiting Shi",
      "Dawei Yin",
      "Xueqi Cheng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.89": {
    "title": "CiteBART: Learning to Generate Citations for Local Citation Recommendation",
    "volume": "main",
    "abstract": "Local citation recommendation (LCR) suggests a set of papers for a citation placeholder within a given context. This paper introduces CiteBART, citation-specific pre-training within an encoder-decoder architecture, where author-date citation tokens are masked to learn to reconstruct them to fulfill LCR. The global version (CiteBART-Global) extends the local context with the citing paper's title and abstract to enrich the learning signal. CiteBART-Global achieves state-of-the-art performance on LCR benchmarks except for the FullTextPeerRead dataset, which is quite small to see the advantage of generative pre-training. The effect is significant in the larger benchmarks, e.g., Refseer and ArXiv., with the Refseer pre-trained model emerging as the best-performing model. We perform comprehensive experiments, including an ablation study, a qualitative analysis, and a taxonomy of hallucinations with detailed statistics. Our analyses confirm that CiteBART-Global has a cross-dataset generalization capability; the macro hallucination rate (MaHR) at the top-3 predictions is 4%, and when the ground-truth is in the top-k prediction list, the hallucination tendency in the other predictions drops significantly. We publicly share our code, base datasets, global datasets, and pre-trained models to support reproducibility",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ege Yiğit Çelik",
      "Selma Tekir"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.90": {
    "title": "Autoformalization in the Wild: Assessing LLMs on Real-World Mathematical Definitions",
    "volume": "main",
    "abstract": "Thanks to their linguistic capabilities, LLMs offer an opportunity to bridge the gap between informal mathematics and formal languages through autoformalization. However, it is still unclear how well LLMs generalize to sophisticated and naturally occurring mathematical statements. To address this gap, we investigate the task of autoformalizing real-world mathematical definitions: a critical component of mathematical discourse. Specifically, we introduce two novel resources for autoformalization, collecting definitions from Wikipedia (Def_Wiki) and arXiv papers (Def_ArXiv). We then systematically evaluate a range of LLMs, analyzing their ability to formalize definitions into Isabelle/HOL. Furthermore, we investigate strategies to enhance LLMs' performance including refinement through external feedback from Proof Assistants, and formal definition grounding, where we augment LLMs' formalizations through relevant contextual elements from formal mathematical libraries. Our findings reveal that definitions present a greater challenge compared to existing benchmarks, such as miniF2F. In particular, we found that LLMs still struggle with self-correction, and aligning with relevant mathematical libraries. At the same time, structured refinement methods and definition grounding strategies yield notable improvements of up to 16% on self-correction capabilities and 43% on the reduction of undefined errors, highlighting promising directions for enhancing LLM-based autoformalization in real-world scenarios",
    "checked": true,
    "id": "f6511c64bc74c59f3203ad723ac406048c42d4ac",
    "semantic_title": "autoformalization in the wild: assessing llms on real-world mathematical definitions",
    "citation_count": 4,
    "authors": [
      "Lan Zhang",
      "Marco Valentino",
      "Andre Freitas"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.91": {
    "title": "Culture Cartography: Mapping the Landscape of Cultural Knowledge",
    "volume": "main",
    "abstract": "To serve global users safely and productively, LLMs need culture-specific knowledge that might not be learned during pre-training. How do we find knowledge that is (1) salient to in-group users, but (2) unknown to LLMs? The most common solutions are single-initiative: either researchers define challenging questions that users passively answer (traditional annotation), or users actively produce data that researchers structure as benchmarks (knowledge extraction). The process would benefit from mixed-initiative collaboration, where users guide the process to meaningfully reflect their cultures, and LLMs steer the process to meet the researcher's goals. We propose CultureCartography as a methodology that operationalizes this mixed-initiative vision. Here, an LLM initializes annotation with questions for which it has low-confidence answers, making explicit both its prior knowledge and the gaps therein. This allows a human respondent to fill these gaps and steer the model towards salient topics through direct edits. We implement Culture Cartography as a tool called Culture Explorer. Compared to a baseline where humans answer LLM-proposed questions, we find that Culture Explorer more effectively produces knowledge that strong models like DeepSeek R1, Llama-4 and GPT-4o are missing, even with web search. Fine-tuning on this data boosts the accuracy of Llama models by up to 19.2% on related culture benchmarks",
    "checked": true,
    "id": "b6e26769b2b5d178a4e14b4c55c3274576678b7b",
    "semantic_title": "culture cartography: mapping the landscape of cultural knowledge",
    "citation_count": 0,
    "authors": [
      "Caleb Ziems",
      "William Barr Held",
      "Jane Yu",
      "Amir Goldberg",
      "David Grusky",
      "Diyi Yang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.92": {
    "title": "Interpretability Analysis of Arithmetic In-Context Learning in Large Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) exhibit sophisticated behavior, notably solving arithmetic with only a few in-context examples (ICEs). Yet the computations that connect those examples to the answer remain opaque. We probe four open-weight LLMs, Pythia-12B, Llama-3.1-8B, MPT-7B, and OPT-6.7B, on basic arithmetic to illustrate how they process ICEs. Our study integrates activation patching, information-flow analysis, automatic circuit discovery, and the logit-lens perspective into a unified pipeline. Within this framework we isolate partial-sum representations in three-operand tasks, investigate their influence on final logits, and derive linear function vectors that characterize tasks and align with ICE-induced activations. Controlled ablations show that strict pattern consistency in the formatting of ICEs guides the models more strongly than the symbols chosen or even the factual correctness of the examples. By unifying four complementary interpretability tools, this work delivers one of the most comprehensive interpretability studies of LLM arithmetic to date, and the first on three-operand tasks. Our code is publicly available",
    "checked": true,
    "id": "aa9b45e704308a9150b3a3807c54c1a21bc8393d",
    "semantic_title": "interpretability analysis of arithmetic in-context learning in large language models",
    "citation_count": 0,
    "authors": [
      "Gregory Polyakov",
      "Christian Hepting",
      "Carsten Eickhoff",
      "Seyed Ali Bahrainian"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.93": {
    "title": "SwarmAgentic: Towards Fully Automated Agentic System Generation via Swarm Intelligence",
    "volume": "main",
    "abstract": "The rapid progress of Large Language Models has advanced agentic systems in decision-making, coordination, and task execution. Yet, existing agentic system generation frameworks lack full autonomy, missing from-scratch agent generation, self-optimizing agent functionality, and collaboration, limiting adaptability and scalability. We propose **SwarmAgentic**, the *first framework that fully automates agentic system generation, optimization, and collaboration*, constructing agents from scratch and jointly refining functionality and coordination via language-driven exploration. To enable efficient search over system-level structures, SwarmAgentic maintains a population of candidate systems and evolves them via feedback-guided updates, drawing inspiration from Particle Swarm Optimization (PSO). We evaluate our method on six real-world, open-ended, and exploratory tasks involving high-level planning, system-level coordination, and creative reasoning. Given only a task description and an objective function, SwarmAgentic outperforms all baselines, achieving a **+261.8% relative improvement** over ADAS on the TravelPlanner benchmark, highlighting the effectiveness of full automation in structurally unconstrained tasks. This framework marks a significant step toward scalable and autonomous agentic system design, bridging swarm intelligence with fully automated system multi-agent generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yao Zhang",
      "Chenyang Lin",
      "Shijie Tang",
      "Haokun Chen",
      "Shijie Zhou",
      "Yunpu Ma",
      "Volker Tresp"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.94": {
    "title": "We Politely Insist: Your LLM Must Learn the Persian Art of Taarof",
    "volume": "main",
    "abstract": "Large language models (LLMs) struggle to navigate culturally specific communication norms, limiting their effectiveness in global contexts. We focus on Persian *taarof*, a social norm in Iranian interactions, which is a sophisticated system of ritual politeness that emphasizes deference, modesty, and indirectness, yet remains absent from existing cultural benchmarks. We introduce **TaarofBench**, the first benchmark for evaluating LLM understanding of taarof, comprising 450 role-play scenarios covering 12 common social interaction topics, validated by native speakers. Our evaluation of five frontier LLMs reveals substantial gaps in cultural competence, with accuracy rates 40-48% below native speakers when taarof is culturally appropriate. Performance varies between interaction topics, improves with Persian-language prompts, and exhibits gender-based asymmetries. We also show that responses rated \"polite\" by standard metrics often violate taarof norms, indicating the limitations of Western politeness frameworks. Through supervised fine-tuning and Direct Preference Optimization, we achieve 21.8% and 42.3% improvement in model alignment with cultural expectations. Our human study with 33 participants (11 native Persian, 11 heritage, and 11 non-Iranian speakers) forms baselines in varying degrees of familiarity with Persian norms. This work lays the foundation for developing diverse and culturally aware LLMs, enabling applications that better navigate complex social interactions",
    "checked": true,
    "id": "05bc90856833ef881c2a26dc56c0cfdc18ee01b6",
    "semantic_title": "we politely insist: your llm must learn the persian art of taarof",
    "citation_count": 0,
    "authors": [
      "Nikta Gohari Sadr",
      "Sahar Heidariasl",
      "Karine Megerdoomian",
      "Laleh Seyyed-Kalantari",
      "Ali Emami"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.95": {
    "title": "Unstructured Evidence Attribution for Long Context Query Focused Summarization",
    "volume": "main",
    "abstract": "Large language models (LLMs) are capable of generating coherent summaries from very long contexts given a user query, and extracting and citing evidence spans helps improve the trustworthiness of these summaries. Whereas previous work has focused on evidence citation with fixed levels of granularity (e.g. sentence, paragraph, document, etc.), we propose to extract unstructured (i.e., spans of any length) evidence in order to acquire more relevant and consistent evidence than in the fixed granularity case. We show how existing systems struggle to copy and properly cite unstructured evidence, which also tends to be \"lost-in-the-middle\". To help models perform this task, we create the Summaries with Unstructured Evidence Text dataset (SUnsET), a synthetic dataset generated using a novel pipeline, which can be used as training supervision for unstructured evidence summarization. We demonstrate across 5 LLMs and 4 datasets spanning human written, synthetic, single, and multi-document settings that LLMs adapted with SUnsET generate more relevant and factually consistent evidence with their summaries, extract evidence from more diverse locations in their context, and can generate more relevant and consistent summaries than baselines with no fine-tuning and fixed granularity evidence. We release SUnsET and our generation code to the public (https://github.com/dwright37/unstructured-evidence-sunset)",
    "checked": true,
    "id": "06f1915f80a680967274f6d715a6ac238311308f",
    "semantic_title": "unstructured evidence attribution for long context query focused summarization",
    "citation_count": 1,
    "authors": [
      "Dustin Wright",
      "Zain Muhammad Mujahid",
      "Lu Wang",
      "Isabelle Augenstein",
      "David Jurgens"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.96": {
    "title": "RAVEN: Query-Guided Representation Alignment for Question Answering over Audio, Video, Embedded Sensors, and Natural Language",
    "volume": "main",
    "abstract": "Multimodal question answering (QA) often requires identifying which video, audio, or sensor tokens are relevant to the question. Yet modality disagreements are common: off-camera speech, background noise, or motion outside the field of view often mislead fusion models that weight all streams equally. We present RAVEN, a unified QA architecture whose core is QuART, a query-conditioned cross-modal gating module that assigns scalar relevance scores to each token across modalities, enabling the model to amplify informative signals and suppress distractors before fusion. RAVEN is trained through a three-stage pipeline comprising unimodal pretraining, query-aligned fusion, and disagreement-oriented fine-tuning - each stage targeting a distinct challenge in multi-modal reasoning: representation quality, cross-modal relevance, and robustness to modality mismatch. To support training and evaluation, we release AVS-QA, a dataset of 300K synchronized Audio-Video-Sensor streams paired with automatically generated question-answer pairs. Experimental results on seven multi-modal QA benchmarks - including egocentric and exocentric tasks - show that RAVEN achieves up to 14.5% and 8.0% gains in accuracy compared to state-of-the-art multi-modal large language models, respectively. Incorporating sensor data provides an additional 16.4% boost, and the model remains robust under modality corruption, outperforming SOTA baselines by 50.23%. Our code and dataset are available at https://github.com/BASHLab/RAVEN",
    "checked": true,
    "id": "71e70603095a29f80084a127dae53e60bcdef318",
    "semantic_title": "raven: query-guided representation alignment for question answering over audio, video, embedded sensors, and natural language",
    "citation_count": 1,
    "authors": [
      "Subrata Biswas",
      "Mohammad Nur Hossain Khan",
      "Bashima Islam"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.97": {
    "title": "Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision Language Model Reasoning",
    "volume": "main",
    "abstract": "Vision Language Models (VLMs) have achieved remarkable success in a wide range of vision applications of increasing complexity and scales, yet choosing the right VLM model size involves a trade-off between response quality and cost. While smaller VLMs are cheaper to run, they typically produce responses only marginally better than random guessing on benchmarks such as MMMU. In this paper, we propose Cache of Thought (CoT), a master–apprentice framework for collaborative inference between large and small VLMs. CoT manages high-quality query results from large VLMs (master) in a cache, which are then selected via a novel multi-modal retrieval and in-context learning to aid the performance of small VLMs (apprentice). We extensively evaluate CoT on various widely-recognized and challenging general reasoning benchmarks, and show that CoT increases overall reasoning performance by up to 7.7% under the same budget, and specifically boosts the reasoning performance of apprentice VLMs by up to 36.6%. Our code is available at https://github.com/UIUC-MONET/Cache-of-Thoughts",
    "checked": true,
    "id": "93a22a954814340be7cab9e38d355d27c50f30f7",
    "semantic_title": "cache-of-thought: master-apprentice framework for cost-effective vision language model reasoning",
    "citation_count": 0,
    "authors": [
      "Mingyuan Wu",
      "Jize Jiang",
      "Haozhen Zheng",
      "Meitang Li",
      "Zhaoheng Li",
      "Beitong Tian",
      "Bo Chen",
      "Yongjoo Park",
      "Minjia Zhang",
      "ChengXiang Zhai",
      "Klara Nahrstedt"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.98": {
    "title": "Video Compression Commander: Plug-and-Play Inference Acceleration for Video Large Language Models",
    "volume": "main",
    "abstract": "Video large language models (VideoLLM) excel at video understanding, but face efficiency challenges due to the quadratic complexity of abundant visual tokens. Our systematic analysis of token compression methods for VideoLLMs reveals two critical issues: (i) overlooking distinctive visual signals across frames, leading to information loss; (ii) suffering from implementation constraints, causing incompatibility with modern architectures or efficient operators.To address these challenges, we distill three design principles for VideoLLM token compression and propose a plug-and-play inference acceleration framework \"Video Compression Commander\" (VidCom2). By quantifying each frame's uniqueness, VidCom2 adaptively adjusts compression intensity across frames, effectively preserving essential information while reducing redundancy in video sequences. Extensive experiments across various VideoLLMs and benchmarks demonstrate the superior performance and efficiency of our VidCom2. With only 25% visual tokens, VidCom2 achieves 99.6% of the original performance on LLaVA-OV while reducing 70.8% of the LLM generation latency. Notably, our Frame Compression Adjustment strategy is compatible with other token compression methods to further improve their performance. Our code is available at https://github.com/xuyang-liu16/VidCom2",
    "checked": true,
    "id": "f9ffb8cba1b1f888d6a22603a80af8354ba3f8a5",
    "semantic_title": "video compression commander: plug-and-play inference acceleration for video large language models",
    "citation_count": 6,
    "authors": [
      "Xuyang Liu",
      "Yiyu Wang",
      "Junpeng Ma",
      "Linfeng Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.99": {
    "title": "Router-Tuning: A Simple and Effective Approach for Dynamic Depth",
    "volume": "main",
    "abstract": "The Mixture of Depths (MoD) was introduced to improve computational efficiency by dynamically skipping less important layers, reducing redundant computation while maintaining model capacity. Despite its promise, existing MoD approaches remain under-explored and face two main challenges: (1) high training costs due to the need to train the entire model along with the routers that determine which layers to skip, and (2) performance degradation when important layers are bypassed. In response to the first issue, we propose Router-Tuning, which fine-tunes only the routers on a small dataset, drastically reducing the computational overhead associated with full model training. For the second challenge, we investigate across different architectures and granularities, demonstrating its effectiveness on Attention layers and MoE layers. This method preserves the model's performance while significantly enhancing computational and memory efficiency. Extensive experiments demonstrate that our approach delivers competitive results while dramatically improving the computation efficiency, e.g., 21% speedup and only a 0.2% performance drop. The code will be released upon acceptance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shwai He",
      "Tao Ge",
      "Guoheng Sun",
      "Bowei Tian",
      "Xiaoyang Wang",
      "Dong Yu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.100": {
    "title": "Foot-In-The-Door: A Multi-turn Jailbreak for LLMs",
    "volume": "main",
    "abstract": "Ensuring AI safety is crucial as large language models become increasingly integrated into real-world applications. A key challenge is jailbreak, where adversarial prompts bypass built-in safeguards to elicit harmful disallowed outputs. Inspired by psychological foot-in-the-door principles, we introduce FITD, a novel multi-turn jailbreak method that leverages the phenomenon where minor initial commitments lower resistance to more significant or more unethical transgressions. Our approach progressively escalates the malicious intent of user queries through intermediate bridge prompts and aligns the model's response by itself to induce toxic responses. Extensive experimental results on two jailbreak benchmarks demonstrate that FITD achieves an average attack success rate of 94% across seven widely used models, outperforming existing state-of-the-art methods. Additionally, we provide an in-depth analysis of LLM self-corruption, highlighting vulnerabilities in current alignment strategies and emphasizing the risks inherent in multi-turn interactions. The code is available at https://github.com/Jinxiaolong1129/Foot-in-the-door-Jailbreak",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixuan Weng",
      "Xiaolong Jin",
      "Jinyuan Jia",
      "Xiangyu Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.101": {
    "title": "TurnaboutLLM: A Deductive Reasoning Benchmark from Detective Games",
    "volume": "main",
    "abstract": "This paper introduces TurnaboutLLM, a novel framework and dataset for evaluating the deductive reasoning abilities of Large Language Models (LLMs) by leveraging the interactive gameplay of detective games Ace Attorney and Danganronpa. The framework tasks LLMs with identifying contradictions between testimonies and evidences within long narrative contexts, a challenging task due to the large answer space and diverse reasoning types presented by its questions. We evaluate twelve state-of-the-art LLMs on the dataset, hinting at limitations of popular strategies for enhancing deductive reasoning such as extensive thinking and Chain-of-Thought prompting. The results also suggest varying effects of context size, reasoning steps and answer space size on model performance. Overall, TurnaboutLLM presents a substantial challenge for LLMs' deductive reasoning abilities in complex, narrative-rich environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Yuan",
      "Muyu He",
      "Muhammad Adil Shahid",
      "Ziyang Li",
      "Jiani Huang",
      "Li Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.102": {
    "title": "Transferable Direct Prompt Injection via Activation-Guided MCMC Sampling",
    "volume": "main",
    "abstract": "Direct Prompt Injection (DPI) attacks pose a critical security threat to Large Language Models (LLMs) due to their low barrier of execution and high potential damage. To address the impracticality of existing white-box/gray-box methods and the poor transferability of black-box methods, we propose an activations-guided prompt injection attack framework. We first construct an Energy-based Model (EBM) using activations from a surrogate model to evaluate the quality of adversarial prompts. Guided by the trained EBM, we employ the token-level Markov Chain Monte Carlo (MCMC) sampling to adaptively optimize adversarial prompts, thereby enabling gradient-free black-box attacks. Experimental results demonstrate our superior cross-model transferability, achieving 49.6% attack success rate (ASR) across five mainstream LLMs and 34.6% improvement over human-crafted prompts, and maintaining 36.6% ASR on unseen task scenarios. Interpretability analysis reveals a correlation between activations and attack effectiveness, highlighting the critical role of semantic patterns in transferable vulnerability exploitation",
    "checked": true,
    "id": "2ec84999ddecd420c0e4b215e524e7a7f38e9944",
    "semantic_title": "transferable direct prompt injection via activation-guided mcmc sampling",
    "citation_count": 0,
    "authors": [
      "Minghui Li",
      "Hao Zhang",
      "Yechao Zhang",
      "Wei Wan",
      "Shengshan Hu",
      "Pei Xiaobing",
      "Jing Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.103": {
    "title": "Direct Judgement Preference Optimization",
    "volume": "main",
    "abstract": "To meet the increasing need for timely and accurate evaluation of large language model (LLM) responses, training LLM-as-judges to evaluate and critique other model responses has emerged as a popular paradigm. However, existing judge models are largely trained with supervised finetuning (SFT) on small data scales to perform limited types of evaluation tasks, fundamentally limiting generalization.To meet the need for strong, generalized judge models, we explore training foundational judge models at large data scales (680K) with direct preference optimization (DPO). Using four training tasks, we form three types of DPO preference pairs targeting different aspects of evaluation: Generating meaningful critiques, making accurate judgements, and understanding what comprises good and bad responses. To demonstrate the effectiveness of our method, we train judge models of three sizes: 8B parameters, 12B, and 70B, and evaluate on a comprehensive suite of 13 benchmarks (7 pairwise, 4 single rating, and 2 classification). Our models achieve the best aggregate performance, with even our 8B model outperforming GPT-4o in pairwise benchmarks. Further analysis shows that our judge models produce factual and actionable critiques and serve as strong foundational judges for continued finetuning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "PeiFeng Wang",
      "Austin Xu",
      "Yilun Zhou",
      "Caiming Xiong",
      "Shafiq Joty"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.104": {
    "title": "WebInject: Prompt Injection Attack to Web Agents",
    "volume": "main",
    "abstract": "Multi-modal large language model (MLLM)-based web agents interact with webpage environments by generating actions based on screenshots of the webpages. In this work, we propose WebInject, a prompt injection attack that manipulates the webpage environment to induce a web agent to perform an attacker-specified action. Our attack adds a perturbation to the raw pixel values of the rendered webpage. After these perturbed pixels are mapped into a screenshot, the perturbation induces the web agent to perform the attacker-specified action. We formulate the task of finding the perturbation as an optimization problem. A key challenge in solving this problem is that the mapping between raw pixel values and screenshot is non-differentiable, making it difficult to backpropagate gradients to the perturbation. To overcome this, we train a neural network to approximate the mapping and apply projected gradient descent to solve the reformulated optimization problem. Extensive evaluation on multiple datasets shows that WebInject is highly effective and significantly outperforms baselines",
    "checked": true,
    "id": "edd63168ea32a84ef63b3f34495cb8b65222ee32",
    "semantic_title": "webinject: prompt injection attack to web agents",
    "citation_count": 3,
    "authors": [
      "Xilong Wang",
      "John Bloch",
      "Zedian Shao",
      "Yuepeng Hu",
      "Shuyan Zhou",
      "Neil Zhenqiang Gong"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.105": {
    "title": "F²Bench: An Open-ended Fairness Evaluation Benchmark for LLMs with Factuality Considerations",
    "volume": "main",
    "abstract": "With the growing adoption of large language models (LLMs) in NLP tasks, concerns about their fairness have intensified. Yet, most existing fairness benchmarks rely on closed-ended evaluation formats, which diverge from real-world open-ended interactions. These formats are prone to position bias and introduce a \"minimum score\" effect, where models can earn partial credit simply by guessing. Moreover, such benchmarks often overlook factuality considerations rooted in historical, social, physiological, and cultural contexts, and rarely account for intersectional biases. To address these limitations, we propose F²Bench: an open-ended fairness evaluation benchmark for LLMs that explicitly incorporates factuality considerations. F²Bench comprises 2,568 instances across 10 demographic groups and two open-ended tasks. By integrating text generation, multi-turn reasoning, and factual grounding, F²Bench aims to more accurately reflect the complexities of real-world model usage. We conduct a comprehensive evaluation of several LLMs across different series and parameter sizes. Our results reveal that all models exhibit varying degrees of fairness issues. We further compare open-ended and closed-ended evaluations, analyze model-specific disparities, and provide actionable recommendations for future model development. Our code and dataset are publicly available at https://github.com/VelikayaScarlet/F2Bench",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tian Lan",
      "Jiang Li",
      "Yemin Wang",
      "Xu Liu",
      "Xiangdong Su",
      "Guanglai Gao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.106": {
    "title": "Value Profiles for Encoding Human Variation",
    "volume": "main",
    "abstract": "Modelling human variation in rating tasks is crucial for enabling AI systems for personalization, pluralistic model alignment, and computational social science. We propose representing individuals using value profiles – natural language descriptions of underlying values compressed from in-context demonstrations – along with a steerable decoder model to estimate ratings conditioned on a value profile or other rater information. To measure the predictive information in rater representations, we introduce an information-theoretic methodology. We find that demonstrations contain the most information, followed by value profiles and then demographics. However, value profiles offer advantages in terms of scrutability, interpretability, and steerability due to their compressed natural language format. Value profiles effectively compress the useful information from demonstrations (70% information preservation). Furthermore, clustering value profiles to identify similarly behaving individuals better explains rater variation than the most predictive demographic groupings. Going beyond test set performance, we show that the decoder models interpretably change ratings according to semantic profile differences, are well-calibrated, and can help explain instance-level disagreement by simulating an annotator population. These results demonstrate that value profiles offer novel, predictive ways to describe individual variation beyond demographics or group information",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taylor Sorensen",
      "Pushkar Mishra",
      "Roma Patel",
      "Michael Henry Tessler",
      "Michiel A. Bakker",
      "Georgina Evans",
      "Iason Gabriel",
      "Noah Goodman",
      "Verena Rieser"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.107": {
    "title": "Language Models as Causal Effect Generators",
    "volume": "main",
    "abstract": "In this work, we present sequence-driven structural causal models (SD-SCMs), a framework for specifying causal models with user-defined structure and language-model-defined mechanisms. We characterize how an SD-SCM enables sampling from observational, interventional, and counterfactual distributions according to the desired causal structure. We then leverage this procedure to propose a new type of benchmark for causal inference methods, generating individual-level counterfactual data to test treatment effect estimation. We create an example benchmark consisting of thousands of datasets, and test a suite of popular estimation methods for average, conditional average, and individual treatment effect estimation. We find under this benchmark that (1) causal methods outperform non-causal methods and that (2) even state-of-the-art methods struggle with individualized effect estimation, suggesting this benchmark captures some inherent difficulties in causal estimation. Apart from generating data, this same technique can underpin the auditing of language models for (un)desirable causal effects, such as misinformation or discrimination. We believe SD-SCMs can serve as a useful tool in any application that would benefit from sequential data with controllable causal structure",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucius E.j. Bynum",
      "Kyunghyun Cho"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.108": {
    "title": "Constructions are Revealed in Word Distributions",
    "volume": "main",
    "abstract": "Construction grammar posits that constructions, or form-meaning pairings, are acquired through experience with language (the distributional learning hypothesis).But how much information about constructions does this distribution actually contain? Corpus-based analyses provide some answers, but text alone cannot answer counterfactual questions about what caused a particular word to occur.This requires computable models of the distribution over strings—namely, pretrained language models (PLMs).Here, we treat a RoBERTa model as a proxy for this distribution and hypothesize that constructions will be revealed within it as patterns of statistical affinity.We support this hypothesis experimentally: many constructions are robustly distinguished, including (i) hard cases where semantically distinct constructions are superficially similar, as well as (ii) schematic constructions, whose \"slots\" can be filled by abstract word classes.Despite this success, we also provide qualitative evidence that statistical affinity alone may be insufficient to identify all constructions from text.Thus, statistical affinity is likely an important, but partial, signal available to learners",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joshua Rozner",
      "Leonie Weissweiler",
      "Kyle Mahowald",
      "Cory Shain"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.109": {
    "title": "CodeMixBench: Evaluating Code-Mixing Capabilities of LLMs Across 18 Languages",
    "volume": "main",
    "abstract": "Code-mixing, the practice of switching between languages within a conversation, poses unique challenges for traditional NLP. Existing benchmarks like LinCE and GLUECoS are limited by their narrow language pairs and tasks, failing to adequately assess large language models' (LLMs) code-mixing abilities. Despite the recognized importance of code-mixing for multilingual users, research on LLMs in this context remains sparse. Additionally, current techniques for synthesizing code-mixed data are underdeveloped to generate code-mixing. In response, we introduce CodeMixBench, a comprehensive benchmark covering eight tasks, including three specific to LLMs and five traditional NLP tasks, and 18 languages from seven language families. We also propose a new method for generating large-scale synthetic code-mixed texts by combining word substitution with GPT-4 prompting. Our evaluation reveals consistent underperformance of LLMs on code-mixed datasets involving different language families. Enhancements in training data size, model scale, and few-shot learning could improve their performance. The code and dataset are available at https://github.com/Jeromeyluck/CodeMixBench",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilun Yang",
      "Yekun Chai"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.110": {
    "title": "RBPtool: A Deep Language Model Framework for Multi-Resolution RBP-RNA Binding Prediction and RNA Molecule Design",
    "volume": "main",
    "abstract": "RNA-binding proteins (RBPs) play essential roles in post-transcriptional gene regulation via recognizing specific RNA molecules as well as modulating several key physiological processes in cellulo, represented by alternative splicing and RNA degradation. Despite extensive research, most existing approaches still rely on superficial sequence features or coarse structural representations, limiting their ability to capture the intricate nature of RBP-RNA interactions. The recent surge in large language models (LLMs), combined with advances in geometric deep learning for extracting three-dimensional representations, enables the integration of multi-modal, multi-scale biological data for precise modeling and biologically informed de novo RNA design. In this work, we curate and extend RPI15223 into a multi-resolution, structure-level RBP-RNA dataset, and introduce RBPtool, a multi-task, multi-resolution framework that combines a geometric vector perception (GVP) module together with a deep language model encoder to fuse sequence and structural information. Our tool achieves state-of-the-art performance on public benchmarks and the RPI15223 dataset, while also supporting fine-grained level predictions and enabling de novo RNA design through a generative module conditioned on protein, cell-type, and specified species. RBPtool provides a fast and versatile platform for both fundamental RBP-RNA research and practical RNA drug design, delivering enhanced predictive accuracy and fine-grained structural insights",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiyue Jiang",
      "Yitao Xu",
      "Zikang Wang",
      "Yihan Ye",
      "Yanruisheng Shao",
      "Yuheng Shan",
      "Jiuming Wang",
      "Xiaodan Fan",
      "Jiao Yuan",
      "Yu Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.111": {
    "title": "Unveiling Internal Reasoning Modes in LLMs: A Deep Dive into Latent Reasoning vs. Factual Shortcuts with Attribute Rate Ratio",
    "volume": "main",
    "abstract": "Existing research in multi-hop questions has identified two reasoning modes: latent reasoning and factual shortcuts, but has not deeply investigated how these modes differ during inference. This impacts both model generalization ability and downstream reasoning tasks. In this work, we systematically examine these distinctions and propose a simple and efficient classification metric, Attribute Rate Ratio (ARR). First, we construct specialized datasets corresponding to the two reasoning modes based on our proposed criteria. Then, using reverse engineering methods, including attention knockout and logit lens techniques, we reveal that subject representations differ significantly across modes: latent reasoning encodes bridge-related information for final answer extraction, while factual shortcuts bypass intermediate reasoning and resemble single-hop factual queries. Finally, our proposed ARR achieves around 90% accuracy on our datasets and demonstrates effectiveness in RAG conflict scenarios, showing that model behavior under conflicting prompts is closely tied to its underlying reasoning mode. Our findings and proposed metric have significant potential for advancing LLM development and applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiran Yang",
      "Haifeng Sun",
      "Jingyu Wang",
      "Qi Qi",
      "Zirui Zhuang",
      "Huazheng Wang",
      "Pengfei Ren",
      "Jing Wang",
      "Jianxin Liao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.112": {
    "title": "SAE-SSV: Supervised Steering in Sparse Representation Spaces for Reliable Control of Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation, but controlling their behavior reliably remains challenging, especially in open-ended generation settings. This paper introduces a novel supervised steering approach that operates in sparse, interpretable representation spaces. We employ sparse autoencoders (SAEs) to obtain sparse latent representations that aim to disentangle semantic attributes from model activations. Then we train linear classifiers to identify a small subspace of task-relevant dimensions in latent representations. Finally, we learn supervised steering vectors constrained to this subspace, optimized to align with target behaviors. Experiments across sentiment, truthfulness, and politics polarity steering tasks with multiple LLMs demonstrate that our supervised steering vectors achieve higher success rates with minimal degradation in generation quality compared to existing methods. Further analysis reveals that a notably small subspace is sufficient for effective steering, enabling more targeted and interpretable interventions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zirui He",
      "Mingyu Jin",
      "Bo Shen",
      "Ali Payani",
      "Yongfeng Zhang",
      "Mengnan Du"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.113": {
    "title": "BabyLM's First Constructions: Causal interventions provide a signal of learning",
    "volume": "main",
    "abstract": "Construction grammar posits that language learners acquire constructions (form-meaning pairings) from the statistics of their environment. Recent work supports this hypothesis by showing sensitivity to constructions in pretrained language models (PLMs), including one recent study (Rozner et al., 2025) demonstrating that constructions shape RoBERTa's output distribution. However, models under study have generally been trained on developmentally implausible amounts of data, casting doubt on their relevance to human language learning. Here we use Rozner et al.'s methods to evaluate construction learning in masked language models from the 2024 BabyLM Challenge.Our results show that even when trained on developmentally plausible quantities of data, models learn diverse constructions, even hard cases that are superficially indistinguishable.We further find correlational evidence that constructional performance may be functionally relevant: models that better represent constructions perform better on the BabyLM benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joshua Rozner",
      "Leonie Weissweiler",
      "Cory Shain"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.114": {
    "title": "Effective Red-Teaming of Policy-Adherent Agents",
    "volume": "main",
    "abstract": "Task-oriented LLM-based agents are increasingly used in domains with strict policies, such as refund eligibility or cancellation rules. The challenge lies in ensuring that the agent consistently adheres to these rules and policies, appropriately refusing any request that would violate them, while still maintaining a helpful and natural interaction. This calls for the development of tailored design and evaluation methodologies to ensure agent resilience against malicious user behavior. We propose a novel threat model that focuses on adversarial users aiming to exploit policy-adherent agents for personal benefit. To address this, we present CRAFT, a multi-agent red-teaming system that leverages policy-aware persuasive strategies to undermine a policy-adherent agent in a customer-service scenario, outperforming conventional jailbreak methods such as DAN prompts, emotional manipulation, and coercive. Building upon the existing Tau-bench benchmark, we introduce Tau-break, a complementary benchmark designed to rigorously assess the agent's robustness against manipulative user behavior. Finally, we evaluate several straightforward yet effective defense strategies. While these measures provide some protection, they fall short, highlighting the need for stronger, research-driven safeguards to protect policy-adherent agents from adversarial attacks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Itay Nakash",
      "George Kour",
      "Koren Lazar",
      "Matan Vetzler",
      "Guy Uziel",
      "Ateret Anaby Tavor"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.115": {
    "title": "CondAmbigQA: A Benchmark and Dataset for Conditional Ambiguous Question Answering",
    "volume": "main",
    "abstract": "Users often assume that large language models (LLMs) share their cognitive alignment of context and intent, leading them to omit critical information in question-answering (QA) and produce ambiguous queries. Responses based on misaligned assumptions may be perceived as hallucinations. Therefore, identifying possible implicit assumptions is crucial in QA. To address this fundamental challenge, we propose Conditional Ambiguous Question-Answering (CondAmbigQA), a benchmark comprising 2,000 ambiguous queries and condition-aware evaluation metrics. Our study pioneers \"conditions\" as explicit contextual constraints that resolve ambiguities in QA tasks through retrieval-based annotation, where retrieved Wikipedia fragments help identify possible interpretations for a given query and annotate answers accordingly. Experiments demonstrate that models considering conditions before answering improve answer accuracy by 11.75%, with an additional 7.15% gain when conditions are explicitly provided. These results highlight that apparent hallucinations may stem from inherent query ambiguity rather than model failure, and demonstrate the effectiveness of condition reasoning in QA, providing researchers with tools for rigorous evaluation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zongxi Li",
      "Yang Li",
      "Haoran Xie",
      "S. Joe Qin"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.116": {
    "title": "SafeScientist: Enhancing AI Scientist Safety for Risk-Aware Scientific Discovery",
    "volume": "main",
    "abstract": "Recent advancements in large language model (LLM) agents have significantly accelerated scientific discovery automation, yet concurrently raised critical ethical and safety concerns. To systematically address these challenges, we introduce **SafeScientist**, an innovative AI scientist framework explicitly designed to enhance safety and ethical responsibility in AI-driven scientific exploration. SafeScientist proactively refuses ethically inappropriate or high-risk tasks and rigorously emphasizes safety throughout the research process. To achieve comprehensive safety oversight, we integrate multiple defensive mechanisms, including prompt monitoring, agent-collaboration monitoring, tool-use monitoring, and an ethical reviewer component. Complementing SafeScientist, we propose **SciSafetyBench** , a novel benchmark specifically designed to evaluate AI safety in scientific contexts, comprising 240 high-risk scientific tasks across 6 domains, alongside 30 specially designed scientific tools and 120 tool-related risk tasks. Extensive experiments demonstrate that SafeScientist significantly improves safety performance by 35% compared to traditional AI scientist frameworks, without compromising scientific output quality. Additionally, we rigorously validate the robustness of our safety pipeline against diverse adversarial attack methods, further confirming the effectiveness of our integrated approach. The code and data will be available at https://github.com/ulab-uiuc/SafeScientist.**Warning**: this paper contains example data that may be offensive or harmful",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kunlun Zhu",
      "Jiaxun Zhang",
      "Ziheng Qi",
      "Nuoxing Shang",
      "Zijia Liu",
      "Peixuan Han",
      "Yue Su",
      "Haofei Yu",
      "Jiaxuan You"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.117": {
    "title": "Improving Informally Romanized Language Identification",
    "volume": "main",
    "abstract": "The Latin script is often used to informally write languages with non-Latin native scripts. In many cases (e.g., most languages in India), the lack of conventional spelling in the Latin script results in high spelling variability. Such romanization renders languages that are normally easily distinguished due to being written in different scripts – Hindi and Urdu, for example – highly confusable. In this work, we increase language identification (LID) accuracy for romanized text by improving the methods used to synthesize training sets. We find that training on synthetic samples which incorporate natural spelling variation yields higher LID system accuracy than including available naturally occurring examples in the training set, or even training higher capacity models. We demonstrate new state-of-the-art LID performance on romanized text from 20 Indic languages in the Bhasha-Abhijnaanam evaluation set (Madhani et al., 2023a), improving test F1 from the reported 74.7% (using a pretrained neural model) to 85.4% using a linear classifier trained solely on synthetic data and 88.2% when also training on available harvested text",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adrian Benton",
      "Alexander Gutkin",
      "Christo Kirov",
      "Brian Roark"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.118": {
    "title": "Integral Transformer: Denoising Attention, Not Too Much Not Too Little",
    "volume": "main",
    "abstract": "Softmax self-attention often assigns disproportionate weight to semantically uninformative tokens such as punctuation and special tokens, a phenomenon known as attention noise. While recent methods like Cog Attention and the Differential Transformer have addressed this by introducing negative attention scores, they risk discarding useful information. In this paper, we propose the Integral Transformer, a novel self-attention mechanism that denoises attention by integrating signals sampled from the logit distribution. This approach mitigates noise while preserving the contributions of special tokens critical for model performance. Extensive experiments demonstrate that our model outperforms vanilla, Cog, and Differential attention variants on rigorous knowledge and reasoning benchmarks. Moreover, our analysis reveals that employing vanilla self-attention in the lower Transformer layers enhances performance and that the Integral Transformer more effectively balances attention distributions and reduces rank collapse in upper layers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ivan Kobyzev",
      "Abbas Ghaddar",
      "Dingtao Hu",
      "Boxing Chen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.119": {
    "title": "CHENGYU-BENCH: Benchmarking Large Language Models for Chinese Idiom Understanding and Use",
    "volume": "main",
    "abstract": "Chinese idioms (成语, Chengyu) are concise four-character expressions steeped in history and culture, whose literal translations often fail to capture their full meaning. This complexity makes them challenging for language models to interpret and use correctly. Existing benchmarks focus on narrow tasks—multiple-choice cloze tests, isolated translation, or simple paraphrasing. We introduce CHENGYU-BENCH, a comprehensive benchmark featuring three tasks: (1) Evaluative Connotation, classifying idioms as positive or negative; (2) Appropriateness, detecting incorrect idiom usage in context; and (3) Open Cloze, filling blanks in longer passages without options. CHENGYU-BENCH comprises 2,937 human-verified examples covering 1,765 common idioms sourced from diverse corpora. We evaluate leading LLMs and find they achieve over 95% accuracy on Evaluative Connotation, but only ~85% on Appropriateness and ~40% top-1 accuracy in Open Cloze. Error analysis reveals that most mistakes arise from fundamental misunderstandings of idiom meanings. CHENGYU-BENCH demonstrates that while LLMs can reliably gauge idiom sentiment, they still struggle to grasp the cultural and contextual nuances essential for proper usage. The benchmark and code will be released upon paper acceptance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yicheng Fu",
      "Zhemin Huang",
      "Liuxin Yang",
      "Yumeng Lu",
      "Zhongdongming Dai"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.120": {
    "title": "Improving Cross Lingual Transfer by Pretraining with Active Forgetting",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) demonstrate exceptional capabilities in a multitude of NLP tasks. However, the efficacy of such models to languages other than English is often limited. Prior works have shown that encoder-only models such as BERT or XLM-RoBERTa show impressive cross lingual transfer of their capabilities from English to other languages. In this work, we propose a pretraining strategy that uses active forgetting to achieve similar cross lingual transfer in decoder-only LLMs. We show that LLMs pretrained with active forgetting are highly effective when adapting to new and unseen languages. Through extensive experimentation, we find that LLMs pretrained with active forgetting are able to learn better multilingual representations which translates to better performance in many downstream tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Divyanshu Aggarwal",
      "Ashutosh Sathe",
      "Sunayana Sitaram"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.121": {
    "title": "Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct Preference Optimization",
    "volume": "main",
    "abstract": "The emergence of large Vision Language Models (VLMs) has broadened the scope and capabilities of single-modal Large Language Models (LLMs) by integrating visual modalities, thereby unlocking transformative cross-modal applications in a variety of real-world scenarios. Despite their impressive performance, VLMs are prone to significant hallucinations, particularly in the form of cross-modal inconsistencies. Building on the success of Reinforcement Learning from Human Feedback (RLHF) in aligning LLMs, recent advancements have focused on applying direct preference optimization (DPO) on carefully curated datasets to mitigate these issues. Yet, such approaches typically introduce preference signals in a brute-force manner, neglecting the crucial role of visual information in the alignment process. In this paper, we introduce Re-Align, a novel alignment framework that leverages image retrieval to construct a dual-preference dataset, effectively incorporating both textual and visual preference signals. We further introduce rDPO, an extension of the standard direct preference optimization that incorporates an additional visual preference objective during fine-tuning. Our experimental results demonstrate that Re-Align not only mitigates hallucinations more effectively than previous methods but also yields significant performance gains in general visual question-answering (VQA) tasks. Moreover, we show that Re-Align maintains robustness and scalability across a wide range of VLM sizes and architectures. This work represents a significant step forward in aligning multimodal LLMs, paving the way for more reliable and effective cross-modal applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuo Xing",
      "Peiran Li",
      "Yuping Wang",
      "Ruizheng Bai",
      "Yueqi Wang",
      "Chan-Wei Hu",
      "Chengxuan Qian",
      "Huaxiu Yao",
      "Zhengzhong Tu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.122": {
    "title": "To Mask or to Mirror: Human-AI Alignment in Collective Reasoning",
    "volume": "main",
    "abstract": "As large language models (LLMs) are increasingly used to model and augment collective decision-making, it is critical to examine their alignment with human social reasoning. We present an empirical framework for assessing collective alignment, in contrast to prior work on the individual level. Using the Lost at Sea social psychology task, we conduct a large-scale online experiment (N=748), randomly assigning groups to leader elections with either visible demographic attributes (e.g. name, gender) or pseudonymous aliases. We then simulate matched LLM groups conditioned on the human data, benchmarking Gemini 2.5, GPT-4.1, Claude Haiku 3.5, and Gemma 3. LLM behaviors diverge: some mirror human biases; others mask these biases and attempt to compensate for them. We empirically demonstrate that human-AI alignment in collective reasoning depends on context, cues, and model-specific inductive biases. Understanding how LLMs align with collective human behavior is critical to advancing socially-aligned AI, and demands dynamic benchmarks that capture the complexities of collective reasoning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Crystal Qian",
      "Aaron T Parisi",
      "Clémentine Bouleau",
      "Vivian Tsai",
      "Maël Lebreton",
      "Lucas Dixon"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.123": {
    "title": "SWAN: An Efficient and Scalable Approach for Long-Context Language Modeling",
    "volume": "main",
    "abstract": "We present SWAN, a causal Transformer architecture in the decoder-only style that generalizes robustly to sequence lengths substantially longer than those seen during training. SWAN interleaves layers without positional encodings (NoPE) and sliding-window attention layers equipped with rotary positional encodings (SWA-RoPE), and applies a dynamic scaling mechanism for attention scores during inference. Experiments demonstrate that SWAN achieves strong length extrapolation without requiring additional long-context training. In addition, SWAN is more computationally efficient than the standard Transformer architecture, resulting in lower training cost and higher inference throughput. We further demonstrate that existing pre-trained decoder-only models can be adapted to the SWAN architecture with minimal continued training, enabling extended contexts. Overall, our work presents an effective approach for scaling language models to longer contexts in a robust and efficient manner",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Krishna C Puvvada",
      "Faisal Ladhak",
      "Santiago Akle Serano",
      "Cheng-Ping Hsieh",
      "Shantanu Acharya",
      "Somshubra Majumdar",
      "Fei Jia",
      "Samuel Kriman",
      "Simeng Sun",
      "Dima Rekesh",
      "Boris Ginsburg"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.124": {
    "title": "LLMs Behind the Scenes: Enabling Narrative Scene Illustration",
    "volume": "main",
    "abstract": "Generative AI has established the opportunity to readily transform content from one medium to another. This capability is especially powerful for storytelling, where visual illustrations can illuminate a story originally expressed in text. In this paper, we focus on the task of narrative scene illustration, which involves automatically generating an image depicting a scene in a story. Motivated by recent progress on text-to-image models, we consider a pipeline that uses LLMs as an interface for prompting text-to-image models to generate scene illustrations given raw story text. We apply variations of this pipeline to a prominent story corpus in order to synthesize illustrations for scenes in these stories. We conduct a human annotation task to obtain pairwise quality judgments for these illustrations. The outcome of this process is the SceneIllustrations dataset, which we release as a new resource for future work on cross-modal narrative transformation. Through our analysis of this dataset and experiments modeling illustration quality, we demonstrate that LLMs can effectively verbalize scene knowledge implicitly evoked by story text. Moreover, this capability is impactful for generating and evaluating illustrations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Melissa Roemmele",
      "John Joon Young Chung",
      "Taewook Kim",
      "Yuqian Sun",
      "Alex Calderwood",
      "Max Kreminski"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.125": {
    "title": "REARANK: Reasoning Re-ranking Agent via Reinforcement Learning",
    "volume": "main",
    "abstract": "We present REARANK, a large language model (LLM)-based listwise reasoning rerank- ing agent. REARANK explicitly reasons be- fore reranking, significantly improving both performance and interpretability. Leveraging reinforcement learning and data augmentation, REARANK achieves substantial improvements over baseline models across popular informa- tion retrieval benchmarks, notably requiring only 179 annotated samples. Built on top of Qwen2.5-7B, our REARANK-7B demonstrates performance comparable to GPT-4 on both in- domain and out-of-domain benchmarks and even surpasses GPT-4 on reasoning-intensive BRIGHT benchmarks. These results under- score the effectiveness of our approach and highlight how reinforcement learning can en- hance LLM reasoning capabilities in reranking",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Le Zhang",
      "Bo Wang",
      "Xipeng Qiu",
      "Siva Reddy",
      "Aishwarya Agrawal"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.126": {
    "title": "Large Language Models Do Multi-Label Classification Differently",
    "volume": "main",
    "abstract": "Multi-label classification is prevalent in real-world settings, but the behavior of Large Language Models (LLMs) in this setting is understudied. We investigate how autoregressive LLMs perform multi-label classification, focusing on subjective tasks, by analyzing the output distributions of the models at each label generation step. We find that the initial probability distribution for the first label often does not reflect the eventual final output, even in terms of relative order and find LLMs tend to suppress all but one label at each generation step. We further observe that as model scale increases, their token distributions exhibit lower entropy and higher single-label confidence, but the internal relative ranking of the labels improves. Finetuning methods such as supervised finetuning and reinforcement learning amplify this phenomenon. We introduce the task of distribution alignment for multi-label settings: aligning LLM-derived label distributions with empirical distributions estimated from annotator responses in subjective tasks. We propose both zero-shot and supervised methods which improve both alignment and predictive performance over existing approaches. We find one method – taking the max probability over all label generation distributions instead of just using the initial probability distribution – improves both distribution alignment and overall F1 classification without adding any additional computation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marcus Ma",
      "Georgios Chochlakis",
      "Niyantha Maruthu Pandiyan",
      "Jesse Thomason",
      "Shrikanth Narayanan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.127": {
    "title": "FilBench: Can LLMs Understand and Generate Filipino?",
    "volume": "main",
    "abstract": "Despite the impressive performance of LLMs on English-based tasks, little is known about their capabilities in specific languages such as Filipino. In this work, we address this gap by introducing FilBench, a Filipino-centric benchmark designed to evaluate LLMs across a diverse set of tasks and capabilities in Filipino, Tagalog, and Cebuano. We carefully curate the tasks in FilBench to reflect the priorities and trends of NLP research in the Philippines such as Cultural Knowledge, Classical NLP, Reading Comprehension, and Generation. By evaluating 27 state-of-the-art LLMs on FilBench, we find that several LLMs suffer from reading comprehension and translation capabilities. Our results indicate that FilBench is challenging, with the best model, GPT-4o, achieving only a score of 72.23%. Moreover, we also find that models trained specifically for Southeast Asian languages tend to underperform on FilBench, with the highest-performing model, SEA-LION v3 70B, achieving only a score of 61.07%. Our work demonstrates the value of curating language-specific LLM benchmarks to aid in driving progress on Filipino NLP and increasing the inclusion of Philippine languages in LLM development",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lester James Validad Miranda",
      "Elyanah Aco",
      "Conner G. Manuel",
      "Jan Christian Blaise Cruz",
      "Joseph Marvin Imperial"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.128": {
    "title": "M-ABSA: A Multilingual Dataset for Aspect-Based Sentiment Analysis",
    "volume": "main",
    "abstract": "Aspect-based sentiment analysis (ABSA) is a crucial task in information extraction and sentiment analysis, aiming to identify aspects with associated sentiment elements in text. However, existing ABSA datasets are predominantly English-centric, limiting the scope for multilingual evaluation and research. To bridge this gap, we present M-ABSA, a comprehensive dataset spanning 7 domains and 21 languages, making it the most extensive multilingual parallel dataset for ABSA to date. Our primary focus is on triplet extraction, which involves identifying aspect terms, aspect categories, and sentiment polarities. The dataset is constructed through an automatic translation process with human review to ensure quality. We perform extensive experiments using various baselines to assess performance and compatibility on M-ABSA. Our empirical findings highlight that the dataset enables diverse evaluation tasks, such as multilingual and multi-domain transfer learning, and large language model evaluation, underscoring its inclusivity and its potential to drive advancements in multilingual ABSA research",
    "checked": true,
    "id": "9add7d1c6acfc787f72ce0419fbbf341cb5b3027",
    "semantic_title": "m-absa: a multilingual dataset for aspect-based sentiment analysis",
    "citation_count": 2,
    "authors": [
      "ChengYan Wu",
      "Bolei Ma",
      "Yihong Liu",
      "Zheyu Zhang",
      "Ningyuan Deng",
      "Yanshu Li",
      "Baolan Chen",
      "Yi Zhang",
      "Yun Xue",
      "Barbara Plank"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.129": {
    "title": "RuCCoD: Towards Automated ICD Coding in Russian",
    "volume": "main",
    "abstract": "This study investigates the feasibility of automating clinical coding in Russian, a language with limited biomedical resources. We present a new dataset for ICD coding, which includes diagnosis fields from electronic health records (EHRs) annotated with over 10,000 entities and more than 1,500 unique ICD codes. This dataset serves as a benchmark for several state-of-the-art models, including BERT, LLaMA with LoRA, and RAG, with additional experiments examining transfer learning across domains (from PubMed abstracts to medical diagnosis) and terminologies (from UMLS concepts to ICD codes). We then apply the best-performing model to label an in-house EHR dataset containing patient histories from 2017 to 2021. Our experiments, conducted on a carefully curated test set, demonstrate that training with the automated predicted codes leads to a significant improvement in accuracy compared to manually annotated data from physicians. We believe our findings offer valuable insights into the potential for automating clinical coding in resource-limited languages like Russian, which could enhance clinical efficiency and data accuracy in these contexts. Our code and dataset are available at https://github.com/auto-icd-coding/ruccod",
    "checked": true,
    "id": "d6035dc7c9d116531ace4fb360d4766ceea86e7b",
    "semantic_title": "ruccod: towards automated icd coding in russian",
    "citation_count": 0,
    "authors": [
      "Alexandr Nesterov",
      "Andrey Sakhovskiy",
      "Ivan Sviridov",
      "Airat Valiev",
      "Vladimir Makharev",
      "Petr Anokhin",
      "Galina Zubkova",
      "Elena Tutubalina"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.130": {
    "title": "Code to Think, Think to Code: A Survey on Code-Enhanced Reasoning and Reasoning-Driven Code Intelligence in LLMs",
    "volume": "main",
    "abstract": "Code and reasoning recently exhibit a mutually reinforcing relationship in large language models (LLMs): Code is abstract, modular, highly structured and has strong logic, guiding reasoning in training and inference. While reasoning translates high-level goals into small executable steps, enable more sophisticated code intellignece, solving real-world challenging software development problems. In this study, we examine how code serves as a structured medium for enhancing reasoning - providing verifiable execution paths, enforcing logical decomposition, and enabling runtime validation, and how advances in reasoning have transformed code intelligence from basic completion to sophisticated agent - enabling models to tackle complex software engineering tasks through deliberate planning and systematic debugging. Finally, we identify key challenges and propose future research directions may deepen the synergy, ultimately advancing LLM performance in both complex reasoning and code intelligence",
    "checked": true,
    "id": "16a096e663c5aa661f26bcabe212a53d86ef3eae",
    "semantic_title": "code to think, think to code: a survey on code-enhanced reasoning and reasoning-driven code intelligence in llms",
    "citation_count": 27,
    "authors": [
      "Dayu Yang",
      "Tianyang Liu",
      "Daoan Zhang",
      "Antoine Simoulin",
      "Xiaoyi Liu",
      "Yuwei Cao",
      "Zhaopu Teng",
      "Xin Qian",
      "Grey Yang",
      "Jiebo Luo",
      "Julian McAuley"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.131": {
    "title": "Efficient Model Development through Fine-tuning Transfer",
    "volume": "main",
    "abstract": "Modern LLMs face a major obstacle: each new pre-trained model version requires expensive and repetitive alignment. We propose a method that transfers fine-tuning updates across model versions. The key idea is to extract the *diff vector*, which is the difference in parameters induced by fine-tuning, from a *source* model version and apply it to the base of a different *target* version. We show that transferring diff vectors significantly improves the target base model, often achieving performance comparable to its fine-tuned counterpart. For example, applying the fine-tuning updates from Llama 3.0 8B to Llama 3.1 8B increases accuracy by 46.9% on IFEval and 15.7% on LiveCodeBench without further training, surpassing Llama 3.1 8B Instruct. In multilingual settings, we also observe accuracy gains relative to Llama 3.1 8B Instruct, including 4.7% for Malagasy and 15.5% for Turkish on Global MMLU. Our controlled experiments reveal that fine-tuning transfer works best when source and target models are linearly connected in parameter space. We also show that this transfer provides a stronger and more efficient starting point for subsequent fine-tuning. Finally, we propose an iterative *recycling-then-finetuning* approach for continuous model development, which improves both efficiency and effectiveness. Our findings suggest that fine-tuning transfer is a viable strategy to reduce training costs while maintaining model performance",
    "checked": true,
    "id": "268583558a3d2cdffd6d21cc7a519d02304c6d1d",
    "semantic_title": "efficient model development through fine-tuning transfer",
    "citation_count": 3,
    "authors": [
      "Pin-Jie Lin",
      "Rishab Balasubramanian",
      "Fengyuan Liu",
      "Nikhil Kandpal",
      "Tu Vu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.132": {
    "title": "Language Mixing in Reasoning Language Models: Patterns, Impact, and Internal Causes",
    "volume": "main",
    "abstract": "Reasoning language models (RLMs) excel at complex tasks by leveraging a chain-of-thought process to generate structured intermediate steps. However, language mixing, i.e., reasoning steps containing tokens from languages other than the prompt, has been observed in their outputs and shown to affect performance, though its impact remains debated. We present the first systematic study of language mixing in RLMs, examining its patterns, impact, and internal causes across 15 languages, 7 task difficulty levels, and 18 subject areas, and show how all three factors influence language mixing. Moreover, we demonstrate that the choice of reasoning language significantly affects performance: forcing models to reason in Latin or Han scripts via constrained decoding notably improves accuracy. Finally, we show that the script composition of reasoning traces closely aligns with that of the model's internal representations, indicating that language mixing reflects latent processing preferences in RLMs. Our findings provide actionable insights for optimizing multilingual reasoning and open new directions for reasoning language control to build more interpretable and adaptable RLMs",
    "checked": true,
    "id": "2b41c71415f2ac7fa198d1525e77014727e6d907",
    "semantic_title": "language mixing in reasoning language models: patterns, impact, and internal causes",
    "citation_count": 7,
    "authors": [
      "Mingyang Wang",
      "Lukas Lange",
      "Heike Adel",
      "Yunpu Ma",
      "Jannik Strötgen",
      "Hinrich Schuetze"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.133": {
    "title": "User Feedback in Human-LLM Dialogues: A Lens to Understand Users But Noisy as a Learning Signal",
    "volume": "main",
    "abstract": "Once language models (LMs) are deployed, they can interact with users long-term, ideally evolving based on their feedback. Asking for direct user feedback can be disruptive; thus, we study harvesting implicit user feedback from user-LM interaction logs. We study two user-LM interaction datasets (WildChat and LMSYS). First, we analyze user feedback in the user-LLM conversation logs, providing insights into when and why such feedback occurs. Second, we study harvesting learning signals from such implicit user feedback. Specifically, we study whether incorporating the contents of user feedback (e.g., user wanted clarification), in addition to the polarity of the feedback, can improve the model performance. We observe mixed results, showing this helps in short human-designed questions (MTBench) but not on longer and more complex questions (WildBench). Together, we provide an in-depth study of implicit user feedback, showing its potential and limitations",
    "checked": true,
    "id": "16389181178b4d1795fc07d9e6e276a1fc0071d2",
    "semantic_title": "user feedback in human-llm dialogues: a lens to understand users but noisy as a learning signal",
    "citation_count": 3,
    "authors": [
      "Yuhan Liu",
      "Michael JQ Zhang",
      "Eunsol Choi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.134": {
    "title": "Read to Hear: A Zero-Shot Pronunciation Assessment Using Textual Descriptions and LLMs",
    "volume": "main",
    "abstract": "Automatic pronunciation assessment is typically performed by acoustic models trained on audio-score pairs. Although effective, these systems provide only numerical scores, without the information needed to help learners understand their errors. Meanwhile, large language models (LLMs) have proven effective in supporting language learning, but their potential for assessing pronunciation remains unexplored. In this work, we introduce TextPA, a zero-shot, Textual description-based Pronunciation Assessment approach. TextPA utilizes human-readable representations of speech signals, which are fed into an LLM to assess pronunciation accuracy and fluency, while also providing reasoning behind the assigned scores. Finally, a phoneme sequence match scoring method is used to refine the accuracy scores. Our work highlights a previously overlooked direction for pronunciation assessment. Instead of relying on supervised training with audio-score examples, we exploit the rich pronunciation knowledge embedded in written text. Experimental results show that our approach is both cost-efficient and competitive in performance. Furthermore, TextPA significantly improves the performance of conventional audio-score-trained models on out-of-domain data by offering a complementary perspective",
    "checked": true,
    "id": "0e75342dfe557ea653a21443b983aa8b6a0d3f8e",
    "semantic_title": "read to hear: a zero-shot pronunciation assessment using textual descriptions and llms",
    "citation_count": 0,
    "authors": [
      "Yu-Wen Chen",
      "Melody Ma",
      "Julia Hirschberg"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.135": {
    "title": "COCO-Tree: Compositional Hierarchical Concept Trees for Enhanced Reasoning in Vision-Language Models",
    "volume": "main",
    "abstract": "Compositional reasoning remains a persistent weakness of modern vision language models (VLMs): they often falter when a task hinges on understanding how multiple objects, attributes, and relations interact within an image. Multiple research works have attempted to improve compositionality performance by creative tricks such as improving prompt structure, chain of thought reasoning, etc. A more recent line of work attempts to impart additional reasoning in VLMs using well-trained Large Language Models (LLMs), which are far superior in linguistic understanding than VLMs to compensate for the limited linguistic prowess of VLMs. However, these approaches are either resource-intensive or do not provide an interpretable reasoning process. In this paper, we present \"COCO-Tree\" - a novel approach that augments VLM outputs with carefully designed neurosymbolic concept trees learned from LLMs to improve VLM's linguistic reasoning. COCO-Tree's beam search-inspired reasoning process boosts compositionality performance and provides a rationale behind VLM predictions. Empirical results on four compositionality benchmarks, Winoground, EqBench, ColorSwap, and SugarCrepe, in seven different open-source VLMs with varying sizes, demonstrate that COCO-Tree significantly improves compositional generalization by 5-10% over baselines",
    "checked": false,
    "id": "7adc06ef0c7cb7fc2dbb12bca0329b62ad9a7300",
    "semantic_title": "coco-tree: compositional hierarchical concept trees for enhanced reasoning in vision language models",
    "citation_count": 0,
    "authors": [
      "Sanchit Sinha",
      "Guangzhi Xiong",
      "Aidong Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.136": {
    "title": "SurveyGen: Quality-Aware Scientific Survey Generation with Large Language Models",
    "volume": "main",
    "abstract": "Automatic survey generation has emerged as a key task in scientific document processing. While large language models (LLMs) have shown promise in generating survey texts, the lack of standardized evaluation datasets critically hampers rigorous assessment of their performance against human-written surveys. In this work, we present SurveyGen, a large-scale dataset comprising over 4,200 human-written surveys across diverse scientific domains, along with 242,143 cited references and extensive quality-related metadata for both the surveys and the cited papers. Leveraging this resource, we build QUAL-SG, a novel quality-aware framework for survey generation that enhances the standard Retrieval-Augmented Generation (RAG) pipeline by incorporating quality-aware indicators into literature retrieval to assess and select higher-quality source papers. Using this dataset and framework, we systematically evaluate state-of-the-art LLMs under varying levels of human involvement—from fully automatic generation to human-guided writing. Experimental results and human evaluations show that while semi-automatic pipelines can achieve partially competitive outcomes, fully automatic survey generation still suffers from low citation quality and limited critical analysis",
    "checked": true,
    "id": "1292bc1b43f0937e405d634900fff39872238cf6",
    "semantic_title": "surveygen: quality-aware scientific survey generation with large language models",
    "citation_count": 1,
    "authors": [
      "Tong Bao",
      "Mir Tafseer Nayeem",
      "Davood Rafiei",
      "Chengzhi Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.137": {
    "title": "VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing",
    "volume": "main",
    "abstract": "We introduce VoiceCraft-X, an autoregressive neural codec language model which unifies multilingual speech editing and zero-shot text-to-speech (TTS) synthesis across 11 languages: English, Mandarin, Korean, Japanese, Spanish, French, German, Dutch, Italian, Portuguese, and Polish. VoiceCraft-X utilizes the Qwen3 large language model for phoneme-free cross-lingual text processing and a novel token reordering mechanism with time-aligned text and speech tokens to handle both tasks as a single sequence generation problem. The model generates high-quality, natural-sounding speech, seamlessly creating new audio or editing existing recordings within one framework. VoiceCraft-X shows robust performance in diverse linguistic settings, even with limited per-language data, underscoring the power of unified autoregressive approaches for advancing complex, real-world multilingual speech applications. Audio samples are available at https://zhishengzheng.com/voicecraft-x/",
    "checked": true,
    "id": "b253a1bfd0a088be35a6c97d74d673128bc2fb8d",
    "semantic_title": "voicecraft-x: unifying multilingual, voice-cloning speech synthesis and speech editing",
    "citation_count": 0,
    "authors": [
      "Zhisheng Zheng",
      "Puyuan Peng",
      "Anuj Diwan",
      "Cong Phuoc Huynh",
      "Xiaohang Sun",
      "Zhu Liu",
      "Vimal Bhat",
      "David Harwath"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.138": {
    "title": "From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge",
    "volume": "main",
    "abstract": "Assessment and evaluation have long been critical challenges in artificial intelligence (AI) and natural language processing (NLP). Traditional methods, usually matching-based or small model-based, often fall short in open-ended and dynamic scenarios. Recent advancements in Large Language Models (LLMs) inspire the \"LLM-as-a-judge\" paradigm, where LLMs are leveraged to perform scoring, ranking, or selection for various machine learning evaluation scenarios. This paper presents a comprehensive survey of LLM-based judgment and assessment, offering an in-depth overview to review this evolving field. We first provide the definition from both input and output perspectives. Then we introduce a systematic taxonomy to explore LLM-as-a-judge along three dimensions: what to judge, how to judge, and how to benchmark. Finally, we also highlight key challenges and promising future directions for this emerging area",
    "checked": true,
    "id": "92056d644aed7caa6c5367fe77774883246af793",
    "semantic_title": "from generation to judgment: opportunities and challenges of llm-as-a-judge",
    "citation_count": 208,
    "authors": [
      "Dawei Li",
      "Bohan Jiang",
      "Liangjie Huang",
      "Alimohammad Beigi",
      "Chengshuai Zhao",
      "Zhen Tan",
      "Amrita Bhattacharjee",
      "Yuxuan Jiang",
      "Canyu Chen",
      "Tianhao Wu",
      "Kai Shu",
      "Lu Cheng",
      "Huan Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.139": {
    "title": "MultiMatch: Multihead Consistency Regularization Matching for Semi-Supervised Text Classification",
    "volume": "main",
    "abstract": "We introduce **MultiMatch**, a novel semi-supervised learning (SSL) algorithm combining the paradigms of co-training and consistency regularization with pseudo-labeling. At its core, MultiMatch features a three-fold pseudo-label weighting module designed for selecting and filtering pseudo-labels based on head agreement and model confidence, and weighting them according to the perceived classification difficulty. This novel module enhances and unifies three existing techniques - heads agreement from **Multi**head Co-training, self-adaptive thresholds from Free**Match**, and Average Pseudo-Margins from Margin**Match** - resulting in a holistic approach that improves robustness and performance in SSL settings.Experimental results on benchmark datasets highlight the superior performance of MultiMatch, i.e., MultiMatch achieves state-of-the-art results on 8 out of 10 setups from 5 natural language processing datasets and ranks first according to the Friedman test among 21 methods. Furthermore, MultiMatch demonstrates exceptional robustness in highly imbalanced settings, outperforming the second-best approach by 3.26%, a critical advantage for real-world text classification tasks. Our code is available on GitHub",
    "checked": true,
    "id": "cde2c4023ab6f0e12e3954b7f77b391e4935c812",
    "semantic_title": "multimatch: multihead consistency regularization matching for semi-supervised text classification",
    "citation_count": 0,
    "authors": [
      "Iustin Sirbu",
      "Robert-Adrian Popovici",
      "Cornelia Caragea",
      "Stefan Trausan-Matu",
      "Traian Rebedea"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.140": {
    "title": "TTT-Bench: A Benchmark for Evaluating Reasoning Ability with Simple and Novel Tic-Tac-Toe-style Games",
    "volume": "main",
    "abstract": "Large reasoning models (LRMs) have demonstrated impressive reasoning capabilities across a broad range of tasks including Olympiad-level mathematical problems, indicating evidence of their complex reasoning abilities. While many reasoning benchmarks focus on the STEM domain, the ability of LRMs to reason correctly in broader task domains remains underexplored. In this work, we introduce **TTT-Bench**, a new benchmark that is designed to evaluate basic strategic, spatial, and logical reasoning abilities in LRMs through a suite of four two-player Tic-Tac-Toe-style games that humans can effortlessly solve from a young age. We propose a simple yet scalable programmatic approach for generating verifiable two-player game problems for TTT-Bench. Although these games are trivial for humans, they require reasoning about the intentions of the opponent, as well as the game board's spatial configurations, to ensure a win. We evaluate a diverse set of state-of-the-art LRMs, and **discover that the models that excel at hard math problems frequently fail at these simple reasoning games**. Further testing reveals that our evaluated reasoning models score on average ↓ 41% & ↓ 5% lower on TTT-Bench compared to MATH 500 & AIME 2024 respectively, with larger models achieving higher performance using shorter reasoning traces, where most of the models struggle on long-term strategic reasoning situations on simple and new TTT-Bench tasks",
    "checked": true,
    "id": "797c2815a3224284e886a3a68a76a6a26b69973e",
    "semantic_title": "ttt-bench: a benchmark for evaluating reasoning ability with simple and novel tic-tac-toe-style games",
    "citation_count": 1,
    "authors": [
      "Prakamya Mishra",
      "Jiang Liu",
      "Jialian Wu",
      "Xiaodong Yu",
      "Zicheng Liu",
      "Emad Barsoum"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.141": {
    "title": "Learning from Diverse Reasoning Paths with Routing and Collaboration",
    "volume": "main",
    "abstract": "Advances in large language models (LLMs) significantly enhance reasoning capabilities but their deployment is restricted in resource-constrained scenarios. Knowledge distillation addresses this by transferring knowledge from powerful teacher models to compact and transparent students.However, effectively capturing the teacher's comprehensive reasoning is challenging due to conventional token-level supervision's limited scope. Using multiple reasoning paths per query alleviates this problem, but treating each path identically is suboptimal as paths vary widely in quality and suitability across tasks and models.We propose Quality-filtered Routing with Cooperative Distillation(QR-Distill), combining path quality filtering, conditional routing, and cooperative peer teaching. First, quality filtering retains only correct reasoning paths scored by an LLM-based evaluation. Second, conditional routing dynamically assigns paths tailored to each student's current learning state. Finally, cooperative peer teaching enables students to mutually distill diverse insights, addressing knowledge gaps and biases toward specific reasoning styles. Experiments demonstrate QR-Distill's superiority over traditional single- and multi-path distillation methods. Ablation studies further highlight the importance of each component—quality filtering, conditional routing, and peer teaching—in effective knowledge transfer. Our code is available at https://github.com/LzyFischer/Distill",
    "checked": true,
    "id": "c3e4ed182414fd5f5b989d34fa1f8b7e713bbb0f",
    "semantic_title": "learning from diverse reasoning paths with routing and collaboration",
    "citation_count": 5,
    "authors": [
      "Zhenyu Lei",
      "Zhen Tan",
      "Song Wang",
      "Yaochen Zhu",
      "Zihan Chen",
      "Yushun Dong",
      "Jundong Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.142": {
    "title": "Ask Patients with Patience: Enabling LLMs for Human-Centric Medical Dialogue with Grounded Reasoning",
    "volume": "main",
    "abstract": "The severe shortage of medical doctors limits access to timely and reliable healthcare, leaving millions underserved. Large language models (LLMs) offer a potential solution but struggle in real-world clinical interactions. Many LLMs are not grounded in authoritative medical guidelines and fail to transparently manage diagnostic uncertainty. Their language is often rigid and mechanical, lacking the human-like qualities essential for patient trust. To address these challenges, we propose ***Ask Patients with Patience (APP)***, a multi-turn LLM-based medical assistant designed for grounded reasoning, transparent diagnoses, and human-centric interaction. APP enhances communication by eliciting user symptoms through empathetic dialogue, significantly improving accessibility and user engagement. It also incorporates Bayesian active learning to support transparent and adaptive diagnoses. The framework is built on verified medical guidelines, ensuring clinically grounded and evidence-based reasoning. To evaluate its performance, we develop a new benchmark that simulates realistic medical conversations using patient agents driven by profiles extracted from real-world consultation cases. We compare APP against SOTA one-shot and multi-turn LLM baselines. The results show that APP improves diagnostic accuracy, reduces uncertainty, and enhances user experience. By integrating medical expertise with transparent, human-like interaction, APP bridges the gap between AI-driven medical assistance and real-world clinical practice",
    "checked": true,
    "id": "55acb9b8a70f4259ad58639fec9da7e57320bdd0",
    "semantic_title": "ask patients with patience: enabling llms for human-centric medical dialogue with grounded reasoning",
    "citation_count": 6,
    "authors": [
      "Jiayuan Zhu",
      "Jiazhen Pan",
      "Yuyuan Liu",
      "Fenglin Liu",
      "Junde Wu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.143": {
    "title": "MedHallu: A Comprehensive Benchmark for Detecting Medical Hallucinations in Large Language Models",
    "volume": "main",
    "abstract": "Advancements in Large Language Models (LLMs) and their increasing use in medical question-answering necessitate rigorous evaluation of their reliability. A critical challenge lies in hallucination, where models generate plausible yet factually incorrect outputs. In the medical domain, this poses serious risks to patient safety and clinical decision-making. To address this, we introduce, the first benchmark specifically designed for medical hallucination detection. MedHallu comprises 10,000 high-quality question-answer pairs derived from PubMedQA, with hallucinated answers systematically generated through a controlled pipeline. Our experiments show that state-of-the-art LLMs, including GPT-4o, Llama-3.1, and the medically fine-tuned UltraMedical, struggle with this binary hallucination detection task, with the best model achieving an F1 score as low as 0.625 for detecting \"hard\" category hallucinations. Using bidirectional entailment clustering, we show that harder-to-detect hallucinations are semantically closer to ground truth. Through experiments, we also show incorporating domain-specific knowledge and introducing a \"not sure\" category as one of the answer categories improves the precision and F1 scores by up to 38% relative to baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shrey Pandit",
      "Jiawei Xu",
      "Junyuan Hong",
      "Zhangyang Wang",
      "Tianlong Chen",
      "Kaidi Xu",
      "Ying Ding"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.144": {
    "title": "NUTMEG: Separating Signal From Noise in Annotator Disagreement",
    "volume": "main",
    "abstract": "NLP models often rely on human-labeled data for training and evaluation. Many approaches crowdsource this data from a large number of annotators with varying skills, backgrounds, and motivations, resulting in conflicting annotations. These conflicts have traditionally been resolved by aggregation methods that assume disagreements are errors. Recent work has argued that for many tasks annotators may have genuine disagreements and that variation should be treated as signal rather than noise. However, few models separate signal and noise in annotator disagreement. In this work, we introduce NUTMEG, a new Bayesian model that incorporates information about annotator backgrounds to remove noisy annotations from human-labeled training data while preserving systematic disagreements. Using synthetic and real-world data, we show that NUTMEG is more effective at recovering ground-truth from annotations with systematic disagreement than traditional aggregation methods, and we demonstrate that downstream models trained on NUTMEG-aggregated data significantly outperform models trained on data from traditionally aggregation methods. We provide further analysis characterizing how differences in subpopulation sizes, rates of disagreement, and rates of spam affect the performance of our model. Our results highlight the importance of accounting for both annotator competence and systematic disagreements when training on human-labeled data",
    "checked": true,
    "id": "984e3d714ee8feb573bbb1750ff67f39d014fa96",
    "semantic_title": "nutmeg: separating signal from noise in annotator disagreement",
    "citation_count": 0,
    "authors": [
      "Jonathan Ivey",
      "Susan Gauch",
      "David Jurgens"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.145": {
    "title": "Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise Pooled Representations",
    "volume": "main",
    "abstract": "Alignment is no longer a luxury; it is a necessity. As large language models (LLMs) enter high-stakes domains like education, healthcare, governance, and law, their behavior must reliably reflect human-aligned values and safety constraints. Yet current evaluations rely heavily on behavioral proxies such as refusal rates, G-Eval scores, and toxicity classifiers, all of which have critical blind spots. Aligned models are often vulnerable to jailbreaking, stochasticity of generation, and alignment faking. To address this issue, we introduce the **Alignment Quality Index (AQI)**. This novel geometric and prompt-invariant metric empirically assesses LLM alignment by analyzing the separation of safe and unsafe activations in latent space. By combining measures such as the *Davies-Bouldin score (DBS)*, *Dunn index (DI)*, *Xie-Beni index (XBI)*, and *Calinski-Harabasz index (CHI)* across various formulations, AQI captures clustering quality to detect hidden misalignments and jailbreak risks, even when outputs appear compliant. AQI also serves as an early warning signal for alignment faking, offering a robust, decoding-invariant tool for behavior-agnostic safety auditing. Additionally, we propose the **LITMUS** dataset to facilitate robust evaluation under these challenging conditions. Empirical tests on LITMUS across different models trained under DPO, GRPO, and RLHF conditions demonstrate AQI's correlation with external judges and ability to reveal vulnerabilities missed by refusal metrics. We make our implementation publicly available to foster future research in this area",
    "checked": true,
    "id": "0b5ca1b10a6e6dde2a4a51c73aa0366bcc720e3b",
    "semantic_title": "alignment quality index (aqi) : beyond refusals: aqi as an intrinsic alignment diagnostic via latent geometry, cluster divergence, and layer wise pooled representations",
    "citation_count": 0,
    "authors": [
      "Abhilekh Borah",
      "Chhavi Sharma",
      "Danush Khanna",
      "Utkarsh Bhatt",
      "Gurpreet Singh",
      "Hasnat Md Abdullah",
      "Raghav Kaushik Ravi",
      "Vinija Jain",
      "Jyoti Patel",
      "Shubham Singh",
      "Vasu Sharma",
      "Arpita Vats",
      "Rahul Raja",
      "Aman Chadha",
      "Amitava Das"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.146": {
    "title": "MythTriage: Scalable Detection of Opioid Use Disorder Myths on a Video-Sharing Platform",
    "volume": "main",
    "abstract": "Understanding the prevalence of misinformation in health topics online can inform public health policies and interventions. However, measuring such misinformation at scale remains a challenge, particularly for high-stakes but understudied topics like opioid-use disorder (OUD)—a leading cause of death in the U.S. We present the first large-scale study of OUD-related myths on YouTube, a widely-used platform for health information. With clinical experts, we validate 8 pervasive myths and release an expert-labeled video dataset. To scale labeling, we introduce MythTriage, an efficient triage pipeline that uses a lightweight model for routine cases and defers harder ones to a high-performing, but costlier, large language model (LLM). MythTriage achieves up to 0.86 macro F1-score while estimated to reduce annotation time and financial cost by over 76% compared to experts and full LLM labeling. We analyze 2.9K search results and 343K recommendations, uncovering how myths persist on YouTube and offering actionable insights for public health and platform moderation",
    "checked": true,
    "id": "ffa0afb12bad1c487e54bc9cedeb6dec7afab086",
    "semantic_title": "mythtriage: scalable detection of opioid use disorder myths on a video-sharing platform",
    "citation_count": 2,
    "authors": [
      "Hayoung Jung",
      "Shravika Mittal",
      "Ananya Aatreya",
      "Navreet Kaur",
      "Munmun De Choudhury",
      "Tanu Mitra"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.147": {
    "title": "Demystifying optimized prompts in language models",
    "volume": "main",
    "abstract": "Modern language models (LMs) are not robust to out-of-distribution inputs. Machine generated (\"optimized\") prompts can be used to modulate LM outputs and induce specific behaviors while appearing completely uninterpretable. In this work, we investigate the composition of optimized prompts, as well as the mechanisms by which LMs parse and build predictions from optimized prompts. We find that optimized prompts primarily consist of punctuation and noun tokens which are more rare in the training data. Internally, optimized prompts are clearly distinguishable from natural language counterparts based on sparse subsets of the model's activations. Across various families of instruction-tuned models, optimized prompts follow a similar path in how their representations form through the network",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rimon Melamed",
      "Lucas Hurley McCabe",
      "H Howie Huang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.148": {
    "title": "Whisper-UT: A Unified Translation Framework for Speech and Text",
    "volume": "main",
    "abstract": "Encoder-decoder models have achieved remarkable success in speech and text tasks, yet efficiently adapting these models to diverse uni/multi-modal scenarios remains an open challenge. In this paper, we propose Whisper-UT, a unified and efficient framework that leverages lightweight adapters to enable seamless adaptation across tasks, including a multi-modal machine translation (MMT) task that explicitly conditions translation on both speech and source language text inputs. By incorporating ASR hypotheses or ground-truth transcripts as prompts, this approach not only enables the system to process both modalities simultaneously but also enhances speech translation (ST) performance through a 2-stage decoding strategy. We demonstrate our methods using the Whisper model, though in principle they are general and could be applied to similar multitask models. We highlight the effectiveness of cross-modal and cross-task fine-tuning, which improves performance without requiring 3-way parallel data. Our results underscore the flexibility, efficiency, and general applicability of the proposed framework for multi-modal translation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cihan Xiao",
      "Matthew Wiesner",
      "Debashish Chakraborty",
      "Reno Kriz",
      "Keith Cunningham",
      "Kenton Murray",
      "Kevin Duh",
      "Luis Tavarez-Arce",
      "Paul McNamee",
      "Sanjeev Khudanpur"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.149": {
    "title": "Unleashing the Reasoning Potential of LLMs by Critique Fine-Tuning on One Problem",
    "volume": "main",
    "abstract": "Critique Fine-Tuning (CFT) has recently emerged as a promising paradigm for unlocking the reasoning capabilities of large language models (LLMs). In this work, we introduce one-shot CFT, a highly compute-efficient approach that leverages critique data generated from a single math problem. Remarkably, this method yields significant gains in reasoning accuracy, surpassing one-shot RLVR (Reinforcement Learning with Verifiable Reward) while requiring 15 to 20 times less compute. Given one math problem, we first prompt a set of diverse small models to produce candidate solutions, then use frontier models such as GPT-4.1 to generate high-quality critiques of these responses. We fine-tune Qwen and Llama family models ranging from 1.5B to 14B parameters with CFT. With just 5 GPU hours, our models achieve up to a 16 percent absolute improvement in average accuracy across six mathematical reasoning benchmarks (for example, Qwen2.5-Math-7B improves from 26 percent to 42 percent). Furthermore, ablation studies reveal the robustness of one-shot CFT across different prompt problems. Our findings suggest an extremely compute-efficient approach to unleash the reasoning potential of LLMs",
    "checked": true,
    "id": "e9bc3bfbb50aeecdf01b91626da6db221545f6a8",
    "semantic_title": "unleashing the reasoning potential of llms by critique fine-tuning on one problem",
    "citation_count": 0,
    "authors": [
      "Yubo Wang",
      "Ping Nie",
      "Kai Zou",
      "Lijun Wu",
      "Wenhu Chen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.150": {
    "title": "Active Layer-Contrastive Decoding Reduces Hallucination in Large Language Model Generation",
    "volume": "main",
    "abstract": "Recent decoding methods improve the factuality of large language models (LLMs) by refining how the next token is selected during generation. These methods typically operate at the token level, leveraging internal representations to suppress superficial patterns. Nevertheless, LLMs remain prone to hallucinations, especially over longer contexts. In this paper, we propose Active Layer-Contrastive Decoding (ActLCD), a novel decoding strategy that actively decides when to apply contrasting layers during generation. By casting decoding as a sequential decision-making problem, ActLCD employs a reinforcement learning policy guided by a reward-aware classifier to optimize factuality beyond the token level. Our experiments demonstrate that ActLCD surpasses state-of-the-art methods across five benchmarks, showcasing its effectiveness in mitigating hallucinations in diverse generation scenarios",
    "checked": true,
    "id": "dd2c4dfb1495af2951b4b3b783474537abf45f43",
    "semantic_title": "active layer-contrastive decoding reduces hallucination in large language model generation",
    "citation_count": 3,
    "authors": [
      "Hongxiang Zhang",
      "Hao Chen",
      "Muhao Chen",
      "Tianyi Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.151": {
    "title": "BBScoreV2: Learning Time-Evolution and Latent Alignment from Stochastic Representation",
    "volume": "main",
    "abstract": "Autoregressive generative models play a key role in various language tasks, especially for modeling and evaluating long text sequences. While recent methods leverage stochastic representations to better capture sequence dynamics, encoding both temporal and structural dependencies and utilizing such information for evaluation remains challenging. In this work, we observe that fitting transformer-based model embeddings into a stochastic process yields ordered latent representations from originally unordered model outputs. Building on this insight and prior work, we theoretically introduce a novel likelihood-based evaluation metric BBScoreV2. Empirically, we demonstrate that the stochastic latent space induces a \"clustered-to-temporal ordered\" mapping of language model representations in high-dimensional space, offering both intuitive and quantitative support for the effectiveness of BBScoreV2. Furthermore, this structure aligns with intrinsic properties of natural language and enhances performance on tasks such as temporal consistency evaluation (e.g., Shuffle tasks) and AI-generated content detection",
    "checked": true,
    "id": "c4cf1d9de92585242198ce65afdaf32988bbe47c",
    "semantic_title": "bbscorev2: learning time-evolution and latent alignment from stochastic representation",
    "citation_count": 0,
    "authors": [
      "Tianhao Zhang",
      "Zhecheng Sheng",
      "Zhexiao Lin",
      "Chen Jiang",
      "Dongyeop Kang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.152": {
    "title": "SAND: Boosting LLM Agents with Self-Taught Action Deliberation",
    "volume": "main",
    "abstract": "Large Language Model (LLM) agents are commonly tuned with supervised finetuning on ReAct-style expert trajectories or preference optimization over pairwise rollouts. Most of these methods focus on imitating specific expert behaviors or promoting chosen reasoning thoughts and actions over rejected ones. However, without reasoning and comparing over alternatives actions, LLM agents finetuned with these methods may over-commit towards seemingly plausible but suboptimal actions due to limited action space exploration. To address this, in this paper we propose Self-taught ActioN Deliberation (SAND) framework, enabling LLM agents to explicitly deliberate over candidate actions before committing to one. To tackle the challenges of when and what to deliberate given large action space and step-level action evaluation, we incorporate self-consistency action sampling and execution-guided action critique to help synthesize step-wise action deliberation thoughts using the base model of the LLM agent. In an iterative manner, the deliberation trajectories are then used to finetune the LLM agent itself. Evaluating on two representative interactive agent tasks, SAND achieves an average 20% improvement over initial supervised finetuning and also outperforms state-of-the-art agent tuning approaches",
    "checked": true,
    "id": "ab3d2665db68fa9471c62e468b79a9090e0ab965",
    "semantic_title": "sand: boosting llm agents with self-taught action deliberation",
    "citation_count": 1,
    "authors": [
      "Yu Xia",
      "Yiran Jenny Shen",
      "Junda Wu",
      "Tong Yu",
      "Sungchul Kim",
      "Ryan A. Rossi",
      "Lina Yao",
      "Julian McAuley"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.153": {
    "title": "LLMs as World Models: Data-Driven and Human-Centered Pre-Event Simulation for Disaster Impact Assessment",
    "volume": "main",
    "abstract": "Efficient simulation is essential for enhancing proactive preparedness for sudden-onset disasters such as earthquakes. Recent advancements in large language models (LLMs) as world models show promise in simulating complex scenarios. This study examines multiple LLMs to proactively estimate perceived earthquake impacts. Leveraging multimodal datasets including geospatial, socioeconomic, building, and street-level imagery data, our framework generates Modified Mercalli Intensity (MMI) predictions at zip code and county scales. Evaluations on the 2014 Napa and 2019 Ridgecrest earthquakes using USGS \"Did You Feel It? (DYFI)\" reports demonstrate significant alignment, as evidenced by high correlation of 0.88 and low RMSE of 0.77 as compared to real reports at the zip code level. Techniques such as RAG and ICL can improve simulation performance, while visual inputs notably enhance accuracy compared to structured numerical data alone. These findings show the promise of LLMs in simulating disaster impacts that can help strengthen pre-event planning",
    "checked": true,
    "id": "c377613d2cca252962a42fd3acf05d62f687f0c0",
    "semantic_title": "llms as world models: data-driven and human-centered pre-event simulation for disaster impact assessment",
    "citation_count": 3,
    "authors": [
      "Lingyao Li",
      "Dawei Li",
      "Zhenhui Ou",
      "Xiaoran Xu",
      "Jingxiao Liu",
      "Zihui Ma",
      "Runlong Yu",
      "Min Deng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.154": {
    "title": "Mind the Value-Action Gap: Do LLMs Act in Alignment with Their Values?",
    "volume": "main",
    "abstract": "Existing research assesses LLMs' values by analyzing their stated inclinations, overlooking potential discrepancies between stated values and actions—termed the \"Value-Action Gap.\" This study introduces ValueActionLens, a framework to evaluate the alignment between LLMs' stated values and their value-informed actions. The framework includes a dataset of 14.8k value-informed actions across 12 cultures and 11 social topics, along with two tasks measuring alignment through three metrics. Experiments show substantial misalignment between LLM-generated value statements and their actions, with significant variations across scenarios and models. Misalignments reveal potential harms, highlighting risks in relying solely on stated values to predict behavior. The findings stress the need for context-aware evaluations of LLM values and the value-action gaps",
    "checked": true,
    "id": "cd68b2dc31ffbab2944bdeabfe786296c5427670",
    "semantic_title": "mind the value-action gap: do llms act in alignment with their values?",
    "citation_count": 12,
    "authors": [
      "Hua Shen",
      "Nicholas Clark",
      "Tanu Mitra"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.155": {
    "title": "Two Heads Are Better Than One: Dual-Model Verbal Reflection at Inference-Time",
    "volume": "main",
    "abstract": "Although preference optimization methods have improved reasoning performance in Large Language Models (LLMs), they often lack transparency regarding why one reasoning outcome is preferred over another. This limitation is especially critical in Automated Student Answer Scoring (ASAS), where explainability is essential to justify assessment outcomes. Verbal reinforcement learning offers the potential to generate explicit reflection, but it tends to produce superficial critiques that can harm assessment performance. Existing LLMs also struggle to reliably detect subtle reasoning errors in ASAS tasks. Moreover, manually identifying intermediate reasoning errors is expensive and difficult to scale. To address these challenges, we introduce a **contrastive reflection synthesis pipeline** that generates precise verbal feedback by identifying discrepancies in structure reasoning graph paths. Leveraging these synthetic reflection data, we propose *DARS*, a Dual-model Reflective Scoring framework featuring a dedicated Critic model trained for effective reflection. *DARS* achieves strong performance and consistently outperforms existing ASAS baselines across all evaluation metrics. Extensive experiments further provide novel insights into the value of reflection data, framework design, and the scaling behavior of *DARS*. We release the DARS code at https://github.com/lijiazheng99/DARS",
    "checked": true,
    "id": "986839c6cd0f3cbd3dbb68b5376ea4c9c1d58765",
    "semantic_title": "two heads are better than one: dual-model verbal reflection at inference-time",
    "citation_count": 6,
    "authors": [
      "Jiazheng Li",
      "Yuxiang Zhou",
      "Junru Lu",
      "Gladys Tyen",
      "Lin Gui",
      "Cesare Aloisi",
      "Yulan He"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.156": {
    "title": "Image Embedding Sampling Method for Diverse Captioning",
    "volume": "main",
    "abstract": "Image Captioning for state-of-the-art VLMs has significantly improved over time; however, this comes at the cost of increased computational complexity, making them less accessible for resource-constrained applications such as mobile devices and assistive technologies. Alternatively, comparably smaller VLMs prioritize high-level scene descriptions, overlooking finer details that contribute to a richer understanding of an image. In this paper, we introduce a training-free framework that enhances caption diversity and informativeness by explicitly attending to distinct image regions using a comparably small VLM, BLIP, as the backbone. Our approach leverages structured segmentation to produce hierarchical representations that capture both global and localized semantics. Without requiring additional model training, we demonstrate that our method allows smaller VLMs to achieve performance comparable to larger models in terms of image-caption alignment, semantic integrity, and diversity. We evaluate our framework on MSCOCO, Flickr30k, and Nocaps test datasets, achieving a Div-2 score of 0.735, 0.750, and 0.748 for each dataset, respectively, while maintaining strong image-caption relevancy and semantic integrity with the human-annotated captions. Our code is available at https://github.com/xfactlab/HBoP",
    "checked": true,
    "id": "1ec0ee654e352a87a57e6b995244a75297a8db6d",
    "semantic_title": "image embedding sampling method for diverse captioning",
    "citation_count": 0,
    "authors": [
      "Sania Waheed",
      "Na Min An"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.157": {
    "title": "Diagnosing Memorization in Chain-of-Thought Reasoning, One Token at a Time",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) perform well on reasoning benchmarks but often fail when inputs alter slightly, raising concerns about the extent to which their success relies on memorization. This issue is especially acute in Chain-of-Thought (CoT) reasoning, where spurious memorized patterns can trigger intermediate errors that cascade into incorrect final answers. We introduce STIM, a novel framework for Source-aware Token-level Identification of Memorization, which attributes each token in a reasoning chain to one of multiple memorization sources – local, mid-range, or long-range – based on their statistical co-occurrence with the token in the pretraining corpus. Our token-level analysis across tasks and distributional settings reveals that models rely more on memorization in complex or long-tail cases, and that local memorization is often the dominant driver of errors, leading to up to 67% of wrong tokens. We also show that memorization scores from STIM can be effective in predicting the wrong tokens in the wrong reasoning step. STIM offers a powerful tool for diagnosing and improving model reasoning and can generalize to other structured step-wise generation tasks",
    "checked": true,
    "id": "5393dc4a7a5e3fe8bda6084739e97e6a5a22e117",
    "semantic_title": "diagnosing memorization in chain-of-thought reasoning, one token at a time",
    "citation_count": 1,
    "authors": [
      "Huihan Li",
      "You Chen",
      "Siyuan Wang",
      "Yixin He",
      "Ninareh Mehrabi",
      "Rahul Gupta",
      "Xiang Ren"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.158": {
    "title": "FANS: Formal Answer Selection for LLM Natural Language Math Reasoning Using Lean4",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have displayed astonishing abilities in various tasks, especially in text generation, classification, question answering, etc. However, the reasoning ability of LLMs still faces many debates, especially in math reasoning. The inherent ambiguity of Natural Language (NL) limits LLMs' ability to perform verifiable reasoning, making the answers lack coherence and trustworthy support. To tackle the above challenges, we propose a novel framework named FANS: Formal ANswer Selection for LLM Natural Language Math Reasoning Using Lean4. It is a pioneering framework that utilizes Lean4 to enhance LLMs' NL math reasoning ability. In particular, given an NL math question and LLM-generated answers, FANS first translates it into Lean4 theorem statements. Then it invokes another Lean4 prover LLM to produce proofs, and finally verifies the proofs by Lean4 compiler. Answers are selected based on the verifications. It enhances LLMs' NL math ability in providing a computer-verifiable solution for its correct answer and proposes an alternative method for answer selection beyond the reward model based ones. Our experiments demonstrate the effectiveness of FANS with an improvement of nearly 2% across several math benchmarks, and even higher further based on reward models or in subfields such as algebra and number theory that Lean4 is better at. The code is available in https://github.com/MaxwellJryao/FANS",
    "checked": true,
    "id": "246228ce731a712ed63c34fb279d4ed5c4933771",
    "semantic_title": "fans: formal answer selection for llm natural language math reasoning using lean4",
    "citation_count": 0,
    "authors": [
      "Jiarui Yao",
      "Ruida Wang",
      "Tong Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.159": {
    "title": "Date Fragments: A Hidden Bottleneck of Tokenization for Temporal Reasoning",
    "volume": "main",
    "abstract": "Modern BPE tokenisers often split calendar dates into meaningless fragments, e.g., \"20250312\" → \"202\", \"503\", \"12\", inflating token counts and obscuring the inherent structure needed for robust temporal reasoning. In this work, we (1) introduce a simple yet interpretable metric, termed date fragmentation ratio, that measures how faithfully a tokeniser preserves multi-digit date components; (2) release DateAugBench, a suite of 6500 examples spanning three temporal reasoning tasks: context-based date resolution, format-invariance puzzles, and date arithmetic across historical, contemporary, and future time periods; and (3) through layer-wise probing and causal attention-hop analyses, uncover an emergent date-abstraction mechanism whereby large language models stitch together the fragments of month, day, and year components for temporal reasoning. Our experiments show that excessive fragmentation correlates with accuracy drops of up to 10 points on uncommon dates like historical and futuristic dates. Further, we find that the larger the model, the faster the emergent date abstraction heals date fragments. Lastly, we observe a reasoning path that LLMs follow to assemble date fragments, typically differing from human interpretation (year → month → day)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gagan Bhatia",
      "Maxime Peyrard",
      "Wei Zhao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.160": {
    "title": "Measuring Risk of Bias in Biomedical Reports: The RoBBR Benchmark",
    "volume": "main",
    "abstract": "Systems that answer questions by reviewing the scientific literature are becoming increasingly feasible. To draw reliable conclusions, these systems should take into account the quality of available evidence from different studies, placing more weight on studies that use a valid methodology. We present a benchmark for measuring the methodological strength of biomedical papers, drawing on the risk-of-bias framework used for systematic reviews. Derived from over 500 biomedical studies, the three benchmark tasks encompass expert reviewers' judgments of studies' research methodologies, including the assessments of risk of bias within these studies. The benchmark contains a human-validated annotation pipeline for fine-grained alignment of reviewers' judgments with research paper sentences. Our analyses show that large language models' reasoning and retrieval capabilities impact their effectiveness with risk-of-bias assessment. The dataset is available at https://github.com/RoBBR-Benchmark/RoBBR",
    "checked": true,
    "id": "daf27c0804d9eb52be6240c9b40e29cc04e8ce1e",
    "semantic_title": "measuring risk of bias in biomedical reports: the robbr benchmark",
    "citation_count": 0,
    "authors": [
      "Jianyou Wang",
      "Weili Cao",
      "Longtian Bao",
      "Youze Zheng",
      "Gil Pasternak",
      "Kaicheng Wang",
      "Xiaoyue Wang",
      "Ramamohan Paturi",
      "Leon Bergen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.161": {
    "title": "SHIFT: Selected Helpful Informative Frame for Video-guided Machine Translation",
    "volume": "main",
    "abstract": "Video-guided Machine Translation (VMT) aims to improve translation quality by integrating contextual information from paired short video clips. Mainstream VMT approaches typically incorporate multimodal information by uniformly sampling frames from the input videos. However, this paradigm frequently incurs significant computational overhead and introduces redundant multimodal content, which degrades both efficiency and translation quality. To tackle these challenges, we propose SHIFT (Selected Helpful Informative Frame for Translation). It is a lightweight, plug-and-play framework designed for VMT with Multimodal Large Language Models (MLLMs). SHIFT adaptively selects a single informative key frame when visual context is necessary; otherwise, it relies solely on textual input. This process is guided by a dedicated clustering module and a selector module. Experimental results demonstrate that SHIFT enhances the performance of MLLMs on the VMT task while simultaneously reducing computational cost, without sacrificing generalization ability. The code will be released upon acceptance",
    "checked": true,
    "id": "4583bbd2e2ad438c66de89350666bc10e8adaa36",
    "semantic_title": "shift: selected helpful informative frame for video-guided machine translation",
    "citation_count": 0,
    "authors": [
      "Boyu Guan",
      "Chuang Han",
      "Yining Zhang",
      "Yupu Liang",
      "Zhiyang Zhang",
      "Yang Zhao",
      "Chengqing Zong"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.162": {
    "title": "Surge: On the Potential of Large Language Models as General-Purpose Surrogate Code Executors",
    "volume": "main",
    "abstract": "Neural surrogate models are powerful and efficient tools in data mining. Meanwhile, large language models (LLMs) have demonstrated remarkable capabilities in code-related tasks, such as generation and understanding. However, an equally important yet underexplored question is whether LLMs can serve as surrogate models for code execution prediction. To systematically investigate it, we introduce SURGE, a comprehensive benchmark with 1160 problems covering 8 key aspects: multi-language programming tasks, competition-level programming problems, repository-level code analysis, high-cost scientific computing, time-complexity-intensive algorithms, buggy code analysis, programs dependent on specific compilers or execution environments, and formal mathematical proof verification. Through extensive analysis of 21 open-source and proprietary LLMs, we examine scaling laws, data efficiency, and predictive accuracy. Our findings reveal important insights about the feasibility of LLMs as efficient surrogates for computational processes. The benchmark and evaluation framework are available at https://github.com/Imbernoulli/SURGE",
    "checked": true,
    "id": "e033b6c4cd5198aa6e39b27480cc6da4e4ff9973",
    "semantic_title": "surge: on the potential of large language models as general-purpose surrogate code executors",
    "citation_count": 0,
    "authors": [
      "Bohan Lyu",
      "Siqiao Huang",
      "Zichen Liang",
      "Qian Sun",
      "Jiaming Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.163": {
    "title": "Few-Shot Learning Translation from New Languages",
    "volume": "main",
    "abstract": "Recent work shows strong transfer learning capability to unseen languages in sequence-to-sequence neural networks, under the assumption that we have high-quality word representations for the target language. We evaluate whether this direction is a viable path forward for translation from low-resource languages by investigating how much data is required to learn such high-quality word representations. We first show that learning word embeddings separately from a translation model can enable rapid adaptation to new languages with only a few hundred sentences of parallel data. To see whether the current bottleneck in transfer to low-resource languages lies mainly with learning the word representations, we then train word embeddings models on varying amounts of data, to then plug them into a machine translation model. We show that in this simulated low-resource setting with only 500 parallel sentences and 31,250 sentences of monolingual data we can exceed 15 BLEU on Flores on unseen languages. Finally, we investigate why on a real low-resource language the results are less favorable and find fault with the publicly available multilingual language modelling datasets",
    "checked": true,
    "id": "d0420ed351e5f729de2da59523d7c8daa409440a",
    "semantic_title": "few-shot learning translation from new languages",
    "citation_count": 0,
    "authors": [
      "Carlos Mullov",
      "Alexander Waibel"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.164": {
    "title": "Humanizing Machines: Rethinking LLM Anthropomorphism Through a Multi-Level Framework of Design",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) increasingly exhibit anthropomorphism characteristics – human-like qualities portrayed across their outlook, language, behavior, and reasoning functions. Such characteristics enable more intuitive and engaging human-AI interactions. However, current research on anthropomorphism remains predominantly risk-focused, emphasizing over-trust and user deception while offering limited design guidance. We argue that anthropomorphism should instead be treated as a concept of design that can be intentionally tuned to support user goals. Drawing from multiple disciplines, we propose that the anthropomorphism of an LLM-based artifact should reflect the interaction between artifact designers and interpreters. This interaction is facilitated by cues embedded in the artifact by the designers and the (cognitive) responses of the interpreters to the cues. Cues are categorized into four dimensions: perceptive, linguistic, behavioral, and cognitive. By analyzing the manifestation and effectiveness of each cue, we provide a unified taxonomy with actionable levers for practitioners. Consequently, we advocate for function-oriented evaluations of anthropomorphic design",
    "checked": true,
    "id": "bd99393fa9cc2d7c78492e41464904ff70f04788",
    "semantic_title": "humanizing machines: rethinking llm anthropomorphism through a multi-level framework of design",
    "citation_count": 2,
    "authors": [
      "Yunze Xiao",
      "Lynnette Hui Xian Ng",
      "Jiarui Liu",
      "Mona T. Diab"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.165": {
    "title": "TokenSkip: Controllable Chain-of-Thought Compression in LLMs",
    "volume": "main",
    "abstract": "Chain-of-Thought (CoT) has been proven effective in enhancing the reasoning capabilities of large language models (LLMs). Recent advancements, such as OpenAI's o1 and DeepSeek-R1, suggest that scaling up the length of CoT sequences during inference could further boost LLM reasoning performance. However, due to the autoregressive nature of LLM decoding, longer CoT outputs lead to a linear increase in inference latency, adversely affecting user experience, particularly when the CoT exceeds 10,000 tokens. To address this limitation, we analyze the semantic importance of tokens within CoT outputs and reveal that their contributions to reasoning vary. Building on this insight, we propose TokenSkip, a simple yet effective approach that enables LLMs to selectively skip less important tokens, allowing for controllable CoT compression. Extensive experiments across various models and tasks demonstrate the effectiveness of TokenSkip in reducing CoT token usage while preserving strong reasoning performance. Notably, when applied to Qwen2.5-14B-Instruct, TokenSkip reduces reasoning tokens by 40% (from 313 to 181) on GSM8K, with less than a 0.4% performance drop",
    "checked": true,
    "id": "79be1c87f3b9e047e10912c5a707c87a69c5dc05",
    "semantic_title": "tokenskip: controllable chain-of-thought compression in llms",
    "citation_count": 118,
    "authors": [
      "Heming Xia",
      "Chak Tou Leong",
      "Wenjie Wang",
      "Yongqi Li",
      "Wenjie Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.166": {
    "title": "Are Generative Models Underconfident? Better Quality Estimation with Boosted Model Probability",
    "volume": "main",
    "abstract": "Quality Estimation (QE) is estimating quality of the model output during inference when the ground truth is not available. Deriving output quality from the models' output probability is the most trivial and low-effort way. However, we show that the output probability of text-generation models can appear underconfident. At each output step, there can be multiple correct options, making the probability distribution spread out more. Thus, lower probability does not necessarily mean lower output quality. Due to this observation, we propose a QE approach called BoostedProb, which boosts the model's confidence in cases where there are multiple viable output options. With no increase in complexity, BoostedProb is notably better than raw model probability in different settings, achieving on average +0.194 improvement in Pearson correlation to ground-truth quality. It also comes close to or outperforms more costly approaches like supervised or ensemble-based QE in certain settings",
    "checked": true,
    "id": "520908e2cfa5ed422fa1ebc660cb68e9fc525909",
    "semantic_title": "are generative models underconfident? better quality estimation with boosted model probability",
    "citation_count": 0,
    "authors": [
      "Tu Anh Dinh",
      "Jan Niehues"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.167": {
    "title": "reWordBench: Benchmarking and Improving the Robustness of Reward Models with Transformed Inputs",
    "volume": "main",
    "abstract": "Reward models have become a staple in modern NLP, serving as not only a scalable text evaluator, but also an indispensable component in many alignment recipes and inference-time algorithms. However, while recent reward models increase performance on standard benchmarks, this may partly be due to overfitting effects, which would confound an understanding of their true capability. In this work, we scrutinize the robustness of reward models and the extent of such overfitting. We build reWordBench, which systematically transforms reward model inputs in meaning- or ranking-preserving ways. We show that state-of-the-art reward models suffer from substantial performance degradation even with minor input transformations, sometimes dropping to significantly below-random accuracy, suggesting brittleness. To improve reward model robustness, we propose to explicitly train them to assign similar scores to paraphrases, and find that this approach also improves robustness to other distinct kinds of transformations. For example, our robust reward model reduces such degradation by roughly half for the Chat Hard subset in RewardBench. Furthermore, when used in alignment, our robust reward models demonstrate better utility and lead to higher-quality outputs, winning in up to 59% of instances against a standardly trained RM",
    "checked": true,
    "id": "0c2ff3753bbfa6668975c09e2a34ace4ef3423ce",
    "semantic_title": "rewordbench: benchmarking and improving the robustness of reward models with transformed inputs",
    "citation_count": 9,
    "authors": [
      "Zhaofeng Wu",
      "Michihiro Yasunaga",
      "Andrew Cohen",
      "Yoon Kim",
      "Asli Celikyilmaz",
      "Marjan Ghazvininejad"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.168": {
    "title": "Why Do Some Inputs Break Low-Bit LLM Quantization?",
    "volume": "main",
    "abstract": "Low-bit weight-only quantization significantly reduces the memory footprint of large language models (LLMs), but disproportionately affects certain examples. We analyze diverse 3-4 bit methods on LLMs ranging from 7B-70B in size and find that the quantization errors of 50 pairs of methods are strongly correlated (avg. 𝜌 = 0.82) on FineWeb examples. Moreover, the residual stream magnitudes of full-precision models are indicative of future quantization errors. We further establish a hypothesis that relates the residual stream magnitudes to error amplification and accumulation over layers. Using LLM localization techniques, early exiting, and activation patching, we show that examples with large errors rely on precise residual activations in the late layers, and that the outputs of MLP gates play a crucial role in maintaining the perplexity. Our work reveals why certain examples result in large quantization errors and which model components are most critical for performance preservation",
    "checked": true,
    "id": "599fc1ea7a9a3adc3569567e1d31f839aa816001",
    "semantic_title": "why do some inputs break low-bit llm quantization?",
    "citation_count": 1,
    "authors": [
      "Ting-Yun Chang",
      "Muru Zhang",
      "Jesse Thomason",
      "Robin Jia"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.169": {
    "title": "LiteASR: Efficient Automatic Speech Recognition with Low-Rank Approximation",
    "volume": "main",
    "abstract": "Modern automatic speech recognition (ASR) models, such as OpenAI's Whisper, rely on deep encoder-decoder architectures, and their encoders are a critical bottleneck for efficient deployment due to high computational intensity. We introduce LiteASR, a low-rank compression scheme for ASR encoders that significantly reduces inference costs while maintaining transcription accuracy. Our approach leverages the strong low-rank properties observed in intermediate activations: by applying principal component analysis (PCA) with a small calibration dataset, we approximate linear transformations with a chain of low-rank matrix multiplications, and further optimize self-attention to work in reduced dimensionality. Evaluation results show that our method can compress Whisper large-v3's encoder size by over 50%, matching Whisper medium's size with better transcription accuracy, thereby establishing a new Pareto frontier of accuracy and efficiency. The code of LiteASR is available at https://github.com/efeslab/LiteASR",
    "checked": true,
    "id": "21851c426d1d019a794c4b68b8492da8aae017a9",
    "semantic_title": "liteasr: efficient automatic speech recognition with low-rank approximation",
    "citation_count": 3,
    "authors": [
      "Keisuke Kamahori",
      "Jungo Kasai",
      "Noriyuki Kojima",
      "Baris Kasikci"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.170": {
    "title": "AROMA: Autonomous Rank-one Matrix Adaptation",
    "volume": "main",
    "abstract": "As large language models continue to grow in size, parameter-efficient fine-tuning (PEFT) has become increasingly crucial. While low-rank adaptation (LoRA) offers a solution through low-rank updates, its static rank allocation may yield suboptimal results. Adaptive low-rank adaptation (AdaLoRA) improves this with dynamic allocation but remains sensitive to initial and target rank configurations. We introduce AROMA, a framework that automatically constructs layer-specific updates by iteratively building up rank-one components with very few trainable parameters that gradually diminish to zero. Unlike existing methods that employ rank reduction mechanisms, AROMA introduces a dual-loop architecture for rank growth. The inner loop extracts information from each rank-one subspace, while the outer loop determines the number of rank-one subspaces, i.e., the optimal rank. We reset optimizer states to maintain subspace independence. AROMA significantly reduces parameters compared to LoRA and AdaLoRA while achieving superior performance on natural language understanding and generation, commonsense reasoning, offering new insights into adaptive PEFT",
    "checked": true,
    "id": "219894e2b2b23d4334a7bef7c994577a20116784",
    "semantic_title": "aroma: autonomous rank-one matrix adaptation",
    "citation_count": 0,
    "authors": [
      "Hao Nan Sheng",
      "Zhi-Yong Wang",
      "Hing Cheung So",
      "Mingrui Yang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.171": {
    "title": "Large Language Models Have Intrinsic Meta-Cognition, but Need a Good Lens",
    "volume": "main",
    "abstract": "Previous research has primarily focused on the cognitive error detection capabilities of Large Language Models (LLMs), often prompting them to analyze mistakes in reasoning chains. However, few studies have examined the meta-cognitive abilities of LLMs (e.g., their self-awareness of step errors), which are crucial for their reliability. While studies on LLM self-evaluation present some measures, such as perplexity, which can reflect the answer correctness and be viewed as the lens of meta-cognition, they lack step-level analysis and adaptation. This paper studies the evaluation of LLM meta-cognition using the current lenses and how to improve these lenses. Specifically, we propose AutoMeco, an Automated Meta-cognition Evaluation framework for benchmarking the existing lenses. Furthermore, a training-free Markovian Intrinsic Reward Adjustment strategy, MIRA, is proposed to boost current meta-cognition lenses. Experimental results on three mathematical reasoning datasets and three LLMs show the reasonableness of AutoMeco by comparing it with Best-of-N verification. Moreover, the meta-cognition ability of LLMs can be better evaluated using MIRA",
    "checked": true,
    "id": "ddf93bf2cc37dbbe68e5774645df05b28f86db86",
    "semantic_title": "large language models have intrinsic meta-cognition, but need a good lens",
    "citation_count": 2,
    "authors": [
      "Ziyang Ma",
      "Qingyue Yuan",
      "Zhenglin Wang",
      "Deyu Zhou"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.172": {
    "title": "Anchoring-Guidance Fine-Tuning (AnGFT): Elevating Professional Response Quality in Role-Playing Conversational Agents",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have demonstrated significant advancements in various fields, notably in Role-Playing Conversational Agents (RPCAs). However, when confronted with role-specific professional inquiries, LLMs-based RPCAs tend to underperform due to their excessive emphasis on the conversational abilities of characters rather than effectively invoking and integrating relevant expert knowledge. This often results in inaccurate responses. We refer to this phenomenon as the \"Knowledge Misalignment\" which underscores the limitations of RPCAs in integrating expert knowledge. To mitigate this issue, we have introduced an Anchoring-Guidance Fine-Tuning (AnGFT) Framework into the RPCAs' training process. This involves initially linking the Anchoring-Based System Prompt (ASP) with the LLM's relevant expert domains through diverse prompt construction strategies and supervised fine-tuning (SFT). Following the role-play enriched SFT, the integration of ASP enables LLMs to better associate with relevant expert knowledge, thus enhancing their response capabilities in role-specific expert domains. Moreover, we have developed four comprehensive metrics—helpfulness, thoroughness, credibility, and feasibility—to evaluate the proficiency of RPCAs in responding to professional questions. Our method was tested across four professional fields, and the experimental outcomes suggest that the proposed AnGFT Framework substantially improves the RPCAs' performance in handling role-specific professional queries, while preserving their robust role-playing abilities",
    "checked": true,
    "id": "bcdaaab6ab4a1fd54de5a2a4e45a97f86229bbc8",
    "semantic_title": "anchoring-guidance fine-tuning (angft): elevating professional response quality in role-playing conversational agents",
    "citation_count": 0,
    "authors": [
      "Qibin Li",
      "Zhen Xu",
      "Shengyuan Bai",
      "Nianmin Yao",
      "Kaili Sun",
      "Bowen Wu",
      "Ying Li",
      "Baoxun Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.173": {
    "title": "RiTTA: Modeling Event Relations in Text-to-Audio Generation",
    "volume": "main",
    "abstract": "Existing text-to-audio (TTA) generation methods have neither systematically explored audio event relation modeling, nor proposed any new framework to enhance this capability. In this work, we systematically study audio event relation modeling in TTA generation models. We first establish a benchmark for this task by: (1) proposing a comprehensive relation corpus covering all potential relations in real-world scenarios; (2) introducing a new audio event corpus encompassing commonly heard audios; and (3) proposing new evaluation metrics to assess audio event relation modeling from various perspectives. Furthermore, we propose a gated prompt tuning strategy that improves existing TTA models' relation modeling capability with negligible extra parameters. Specifically, we introduce learnable relation and event prompt that append to the text prompt before feeding to existing TTA models",
    "checked": true,
    "id": "00e1767201d6586de651f69511a26c94c6653b89",
    "semantic_title": "ritta: modeling event relations in text-to-audio generation",
    "citation_count": 2,
    "authors": [
      "Yuhang He",
      "Yash Jain",
      "Xubo Liu",
      "Andrew Markham",
      "Vibhav Vineet"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.174": {
    "title": "Shallow Focus, Deep Fixes: Enhancing Shallow Layers Vision Attention Sinks to Alleviate Hallucination in LVLMs",
    "volume": "main",
    "abstract": "Multimodal large language models (MLLMs) demonstrate excellent abilities for understanding visual information, while the hallucination remains. Albeit image tokens constitute the majority of the MLLMs input, the relation between image tokens and hallucinations is still unexplored. In this paper, we analyze the attention score distribution of image tokens across layers and attention heads in models, revealing an intriguing but common phenomenon: most hallucinations are closely linked to the attention sink patterns of image tokens attention matrix, where shallow layers exhibit dense sinks and deep layers exhibit the sparse. We further explore the attention heads of different layers, finding: heads with high-density attention sink of the image part act positively in mitigating hallucinations. Inspired by these findings, we propose a training-free approach called Enhancing Vision Attention Sinks (EVAS) to facilitate the convergence of the image token attention sink within shallow layers. Specifically, EVAS identifies the attention heads that emerge as the densest visual sink in shallow layers and extracts its attention matrix, which is then broadcast to other heads of the same layer, thereby strengthing the layer's focus on the image itself. Extensive empirical results of various MLLMs illustrate the superior performance of the proposed EVAS, demonstrating its effectiveness and generality",
    "checked": true,
    "id": "acc4d7dc3bed51f4e8e757fda893a1ebe59611fa",
    "semantic_title": "shallow focus, deep fixes: enhancing shallow layers vision attention sinks to alleviate hallucination in lvlms",
    "citation_count": 0,
    "authors": [
      "Xiaofeng Zhang",
      "Yihao Quan",
      "Chen Shen",
      "Chaochen Gu",
      "Xiaosong Yuan",
      "Shaotian Yan",
      "Jiawei Cao",
      "Hao Cheng",
      "Kaijie Wu",
      "Jieping Ye"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.175": {
    "title": "WangchanThaiInstruct: An instruction-following Dataset for Culture-Aware, Multitask, and Multi-domain Evaluation in Thai",
    "volume": "main",
    "abstract": "Large language models excel at instruction-following in English, but their performance in low-resource languages like Thai remains underexplored. Existing benchmarks often rely on translations, missing cultural and domain-specific nuances needed for real-world use. We present WangchanThaiInstruct, a human-authored Thai dataset for evaluation and instruction tuning, covering four professional domains and seven task types. Created through a multi-stage quality control process with annotators, domain experts, and AI researchers, WangchanThaiInstruct supports two studies: (1) a zero-shot evaluation showing performance gaps on culturally and professionally specific tasks, and (2) an instruction tuning study with ablations isolating the effect of native supervision. Models fine-tuned on WangchanThaiInstruct outperform those using translated data in both in-domain and out-of-domain benchmarks. These findings underscore the need for culturally and professionally grounded instruction data to improve LLM alignment in low-resource, linguistically diverse settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peerat Limkonchotiwat",
      "Pume Tuchinda",
      "Lalita Lowphansirikul",
      "Surapon Nonesung",
      "Panuthep Tasawong",
      "Alham Fikri Aji",
      "Can Udomcharoenchaikit",
      "Sarana Nutanong"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.176": {
    "title": "MemeReaCon: Probing Contextual Meme Understanding in Large Vision-Language Models",
    "volume": "main",
    "abstract": "Memes have emerged as a popular form of multimodal online communication, where their interpretation heavily depends on the specific context in which they appear. Current approaches predominantly focus on isolated meme analysis, either for harmful content detection or standalone interpretation, overlooking a fundamental challenge: the same meme can express different intents depending on its conversational context. This oversight creates an evaluation gap: although humans intuitively recognize how context shapes meme interpretation, Large Vision Language Models (LVLMs) can hardly understand context-dependent meme intent. To address this critical limitation, we introduce MemeReaCon, a novel benchmark specifically designed to evaluate how LVLMs understand memes in their original context. We collected memes from five different Reddit communities, keeping each meme's image, the post text, and user comments together. We carefully labeled how the text and meme work together, what the poster intended, how the meme is structured, and how the community responded. Our tests with leading LVLMs show a clear weakness: models either fail to interpret critical information in the contexts, or overly focus on visual details while overlooking communicative purpose. MemeReaCon thus serves both as a diagnostic tool exposing current limitations and as a challenging benchmark to drive development toward more sophisticated LVLMs of the context-aware understanding",
    "checked": true,
    "id": "32420fb90e83c41789448e15c78e42f1933875f6",
    "semantic_title": "memereacon: probing contextual meme understanding in large vision-language models",
    "citation_count": 0,
    "authors": [
      "Zhengyi Zhao",
      "Shubo Zhang",
      "Yuxi Zhang",
      "Yanxi Zhao",
      "Yifan Zhang",
      "Zezhong Wang",
      "Huimin Wang",
      "Yutian Zhao",
      "Bin Liang",
      "Yefeng Zheng",
      "Binyang Li",
      "Kam-Fai Wong",
      "Xian Wu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.177": {
    "title": "A Comprehensive Literary Chinese Reading Comprehension Dataset with an Evidence Curation Based Solution",
    "volume": "main",
    "abstract": "Low-resource language understanding is challenging, even for large language models (LLMs). An epitome of this problem is the CompRehensive lIterary chineSe readIng comprehenSion (CRISIS), whose difficulties include limited linguistic data, long input, and insight-required questions. Besides the compelling necessity of providing a larger dataset for CRISIS, excessive information, order bias, and entangled conundrums still haunt the CRISIS solutions. Thus, we present the eVIdence cuRation with opTion shUffling and Abstract meaning representation-based cLauses segmenting (VIRTUAL) procedure for CRISIS, with the largest dataset. While the dataset is also named CRISIS, it results from a three-phase construction process, including question selection, data cleaning, and a silver-standard data augmentation step, which augments translations, celebrity profiles, government jobs, reign mottos, and dynasty to CRISIS. The six steps of VIRTUAL include embedding, shuffling, abstract beaning representation based option segmenting, evidence extracting, solving, and voting. Notably, the evidence extraction algorithm facilitates literary Chinese evidence sentences, translated evidence sentences, and annotations of keywords with a similarity-based ranking strategy. While CRISIS congregates understanding-required questions from seven sources, the experiments on CRISIS substantiate the effectiveness of VIRTUAL, with a 7 percent hike in accuracy compared with the baseline. Interestingly, both non-LLMs and LLMs have order bias, and abstract beaning representation based option segmenting is constructive for CRISIS",
    "checked": true,
    "id": "2e23a205590027cb2b84c42263347e93b2821829",
    "semantic_title": "a comprehensive literary chinese reading comprehension dataset with an evidence curation based solution",
    "citation_count": 0,
    "authors": [
      "Dongning Rao",
      "Rongchu Zhou",
      "Peng Chen",
      "Zhihua Jiang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.178": {
    "title": "Dialect-SQL: An Adaptive Framework for Bridging the Dialect Gap in Text-to-SQL",
    "volume": "main",
    "abstract": "Text-to-SQL is the task of translating natural language questions into SQL queries based on relational databases. Different databases implement their own SQL dialects, leading to variations in syntax. As a result, SQL queries designed for one database may not execute properly in another, creating a dialect gap. Existing Text-to-SQL research primarily focuses on specific database systems, limiting adaptability to different dialects. This paper proposes a novel adaptive framework called Dialect-SQL, which employs Object Relational Mapping (ORM) code as an intermediate language to bridge this gap. Given a question, we guide Large Language Models (LLMs) to first generate ORM code, which is then parsed into SQL queries targeted for specific databases. However, there is a lack of high-quality Text-to-Code datasets that enable LLMs to effectively generate ORM code. To address this issue, we propose a bootstrapping approach to synthesize ORM code, where verified ORM code is iteratively integrated into a demonstration pool that serves as in-context examples for ORM code generation. Our experiments demonstrate that Dialect-SQL significantly enhances dialect adaptability, outperforming traditional methods that generate SQL queries directly. Our code and data are released at https://github.com/jieshi10/orm-sql",
    "checked": true,
    "id": "b39aa79f4a402e7df3050796a821460ee077def1",
    "semantic_title": "dialect-sql: an adaptive framework for bridging the dialect gap in text-to-sql",
    "citation_count": 0,
    "authors": [
      "Jie Shi",
      "Xi Cao",
      "Bo Xu",
      "Jiaqing Liang",
      "Yanghua Xiao",
      "Jia Chen",
      "Peng Wang",
      "Wei Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.179": {
    "title": "FinMTEB: Finance Massive Text Embedding Benchmark",
    "volume": "main",
    "abstract": "The efficacy of text embedding models in representing and retrieving information is crucial for many NLP applications, with performance significantly advanced by Large Language Models (LLMs). Despite this progress, existing benchmarks predominantly use general-purpose datasets, inadequately addressing the nuanced requirements of specialized domains like finance. To bridge this gap, we introduce the Finance Massive Text Embedding Benchmark (FinMTEB), a comprehensive evaluation suite specifically designed for the financial domain. FinMTEB encompasses 64 datasets across 7 task types, including classification, clustering, retrieval, pair classification, reranking, summarization, and semantic textual similarity (STS) in English and Chinese. Alongside this benchmark, we introduce Fin-E5, a state-of-the-art finance-adapted embedding model, ranking first on FinMTEB. Fin-E5 is developed by fine-tuning e5-Mistral-7B-Instruct on a novel persona-based synthetic dataset tailored for diverse financial embedding tasks. Evaluating 15 prominent embedding models on FinMTEB, we derive three key findings: (1) domain-specific models, including our Fin-E5, significantly outperform general-purpose models; (2) performance on general benchmarks is a poor predictor of success on financial tasks; and (3) surprisingly, traditional Bag-of-Words (BoW) models surpass dense embedding models on financial STS tasks. This work provides a robust benchmark for financial NLP and offers actionable insights for developing future domain-adapted embedding solutions. Both FinMTEB and Fin-E5 will be open-sourced for the research community",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixuan Tang",
      "Yi Yang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.180": {
    "title": "Scaling Rich Style-Prompted Text-to-Speech Datasets",
    "volume": "main",
    "abstract": "We introduce Paralinguistic Speech Captions (ParaSpeechCaps), a large-scale dataset that annotates speech utterances with rich style captions. While rich abstract tags (e.g. guttural, nasal, pained) have been explored in small-scale human-annotated datasets, existing large-scale datasets only cover basic tags (e.g. low-pitched, slow, loud). We combine off-the-shelf text and speech embedders, classifiers and an audio language model to automatically scale rich tag annotations for the first time. ParaSpeechCaps covers a total of 59 style tags, including both speaker-level intrinsic tags and utterance-level situational tags. It consists of 282 hours of human-labelled data (PSC-Base) and 2427 hours of automatically annotated data (PSC-Scaled). We finetune Parler-TTS, an open-source style-prompted TTS model, on ParaSpeechCaps, and achieve improved style consistency (+7.9% Consistency MOS) and speech quality (+15.5% Naturalness MOS) over the best performing baseline that combines existing rich style tag datasets. We ablate several of our dataset design choices to lay the foundation for future work in this space. Our dataset, models and code are released at https://github.com/ajd12342/paraspeechcaps",
    "checked": true,
    "id": "9cb198bc042c99e715ef788aeff27bdcce3f94bb",
    "semantic_title": "scaling rich style-prompted text-to-speech datasets",
    "citation_count": 9,
    "authors": [
      "Anuj Diwan",
      "Zhisheng Zheng",
      "David Harwath",
      "Eunsol Choi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.181": {
    "title": "Exploring Changes in Nation Perception with Nationality-Assigned Personas in LLMs",
    "volume": "main",
    "abstract": "Persona assignment has become a common strategy for customizing LLM use to particular tasks and contexts. In this study, we explore how evaluation of different nations changes when LLMs are assigned specific nationality personas. We assign 193 different nationality personas (e.g., an American person) to five LLMs and examine how the LLM evaluations (or *\"perceptions\"*) of countries change. We find that all LLM-persona combinations tend to favor Western European nations, though nation-personas push LLM behaviors to focus more on and treat the nation-persona's own region more favorably. Eastern European, Latin American, and African nations are treated more negatively by different nationality personas. We additionally find that evaluations by nation-persona LLMs of other nations correlate with human survey responses but fail to match the values closely. Our study provides insight into how biases and stereotypes are realized within LLMs when adopting different national personas. Our findings underscore the critical need for developing mechanisms to ensure that LLM outputs promote fairness and avoid over-generalization",
    "checked": true,
    "id": "5e79b3199fb0654c58626fe19c66ea2e16886fad",
    "semantic_title": "exploring changes in nation perception with nationality-assigned personas in llms",
    "citation_count": 9,
    "authors": [
      "Mahammed Kamruzzaman",
      "Gene Louis Kim"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.182": {
    "title": "Eliciting Implicit Acoustic Styles from Open-domain Instructions to Facilitate Fine-grained Controllable Generation of Speech",
    "volume": "main",
    "abstract": "This paper focuses on generating speech with the acoustic style that meets users' needs based on their open-domain instructions. To control the style, early work mostly relies on pre-defined rules or templates. The control types and formats are fixed in a closed domain, making it hard to meet diverse needs of users. One solution is to resort to instructions in free text to guide the generation. Current work mainly studies the instructions that clearly specify the acoustic styles, such as low pitch and fast speed. However, the instructions are complex, some even vague and abstract, such as \"Generate a voice of a woman who is heartbroken due to a breakup. It is hard to infer this implicit style by traditional matching-based methods. To address this problem, we propose a new controllable model. It first utilizes multimodal LLMs with knowledge-augmented techniques to infer the desired speech style from the instructions. The powerful language understanding ability of LLMs can help us better elicit the implicit style factors from the instruction. By using these factors as a control condition, we design a diffusion-based generator adept at finely adjusting speech details. That offers higher flexibility to meet complex users' needs. Next, we verify the output speech from three aspects, i.e., consistency of decoding state, mel-spectrogram, and instruction style. This verified feedback can inversely optimize the generator. Extensive experiments are conducted on three popular datasets. The results show the effectiveness and good controllability of our approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianxing Yu",
      "Gou Zihao",
      "Chen Li",
      "Zhisheng Wang",
      "Peiji Yang",
      "Wenqing Chen",
      "Jian Yin"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.183": {
    "title": "OBLIVIATE: Robust and Practical Machine Unlearning for Large Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) trained over extensive corpora risk memorizing sensitive, copyrighted, or toxic content. To address this, we propose OBLIVIATE, a robust unlearning framework that removes targeted data while preserving model utility. The framework follows a structured process: extracting target tokens, building retain sets, and fine-tuning with a tailored loss function comprising three components—masking, distillation, and world fact. Using low-rank adapters (LoRA) ensures efficiency without compromising unlearning quality. We conduct experiments on multiple datasets, including Harry Potter series, WMDP, and TOFU, using a comprehensive suite of metrics: forget quality (via a new document-level memorization score), model utility, and fluency. Results demonstrate its effectiveness in resisting membership inference attacks, minimizing the impact on retained data, and maintaining robustness across diverse scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyu Xu",
      "Minxin Du",
      "Qingqing Ye",
      "Haibo Hu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.184": {
    "title": "AdaptThink: Reasoning Models Can Learn When to Think",
    "volume": "main",
    "abstract": "Recently, large reasoning models have achieved impressive performance on various tasks by employing human-like deep thinking. However, the lengthy thinking process substantially increases inference overhead, making efficiency a critical bottleneck. In this work, we first demonstrate that NoThinking, which prompts the reasoning model to skip thinking and directly generate the final solution, is a better choice for relatively simple tasks in terms of both performance and efficiency. Motivated by this, we propose AdaptThink, a novel RL algorithm to teach reasoning models to choose the optimal thinking mode adaptively based on problem difficulty. Specifically, AdaptThink features two core components: (1) a constrained optimization objective that encourages the model to choose NoThinking while maintaining the overall performance; (2) an importance sampling strategy that balances Thinking and NoThinking samples during on-policy training, thereby enabling cold start and allowing the model to explore and exploit both thinking modes throughout the training process. Our experiments indicate that AdaptThink significantly reduces the inference costs while further enhancing performance. Notably, on three math datasets, AdaptThink reduces the average response length of DeepSeek-R1-Distill-Qwen-1.5B by 53% and improves its accuracy by 2.4%, highlighting the promise of adaptive thinking-mode selection for optimizing the balance between reasoning quality and efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiajie Zhang",
      "Nianyi Lin",
      "Lei Hou",
      "Ling Feng",
      "Juanzi Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.185": {
    "title": "T2: An Adaptive Test-Time Scaling Strategy for Contextual Question Answering",
    "volume": "main",
    "abstract": "Recent advances in large language models have demonstrated remarkable performance on Contextual Question Answering (CQA). However, prior approaches typically employ elaborate reasoning strategies regardless of question complexity, leading to low adaptability. Recent efficient test-time scaling methods introduce budget constraints or early stop mechanisms to avoid overthinking for straightforward questions. But they add human bias to the reasoning process and fail to leverage models' inherent reasoning capabilities. To address these limitations, we present T2: Think-to-Think, a novel framework that dynamically adapts reasoning depth based on question complexity. T2 leverages the insight that if an LLM can effectively solve similar questions using specific reasoning strategies, it can apply the same strategy to the original question. This insight enables to adoption of concise reasoning for straightforward questions while maintaining detailed analysis for complex problems. T2 works through four key steps: decomposing questions into structural elements, generating similar examples with candidate reasoning strategies, evaluating these strategies against multiple criteria, and applying the most appropriate strategy to the original question. Experimental evaluation across seven diverse CQA benchmarks demonstrates that T2 not only achieves higher accuracy than baseline methods but also reduces computational overhead by up to 25.2%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengyi Zhao",
      "Shubo Zhang",
      "Zezhong Wang",
      "Huimin Wang",
      "Yutian Zhao",
      "Bin Liang",
      "Yefeng Zheng",
      "Binyang Li",
      "Kam-Fai Wong",
      "Xian Wu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.186": {
    "title": "Non-Existent Relationship: Fact-Aware Multi-Level Machine-Generated Text Detection",
    "volume": "main",
    "abstract": "Machine-generated text detection is critical for preventing misuse of large language models (LLMs). Although LLMs have recently excelled at mimicking human writing styles, they still suffer from factual hallucinations manifested as entity-relation inconsistencies with real-world knowledge. Current detection methods inadequately address the authenticity of the entity graph, which is a key discriminative feature for identifying machine-generated content. To bridge this gap, we propose a fact-aware model that assesses discrepancies between textual and factual entity graphs through graph comparison. In order to holistically analyze context information, our approach employs hierarchical feature extraction with gating units, enabling the adaptive fusion of multi-grained features from entity, sentence, and document levels. Experimental results on three public datasets demonstrate that our approach outperforms the state-of-the-art methods. Interpretability analysis shows that our model can capture the differences in entity graphs between machine-generated and human-written texts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Wu",
      "Ruijia Wang",
      "Jie Wu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.187": {
    "title": "Calibrating Verbal Uncertainty as a Linear Feature to Reduce Hallucinations",
    "volume": "main",
    "abstract": "LLMs often adopt an assertive language style also when making false claims. Such \"overconfident hallucinations\" mislead users and erode trust. Achieving the ability to express in language the actual degree of uncertainty around a claim is therefore of great importance. We find that \"verbal uncertainty\" is governed by a single linear feature in the representation space of LLMs, and shows that this has only moderate correlation with the actual \"semantic uncertainty\" of the model. We apply this insight and show that (1) the mismatch between semantic and verbal uncertainty is a better predictor of hallucinations than semantic uncertainty alone and (2) we can intervene on verbal uncertainty at inference time and reduce confident hallucinations on short-form answers, achieving an average relative reduction of ~30%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziwei Ji",
      "Lei Yu",
      "Yeskendir Koishekenov",
      "Yejin Bang",
      "Anthony Hartshorn",
      "Alan Schelten",
      "Cheng Zhang",
      "Pascale Fung",
      "Nicola Cancedda"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.188": {
    "title": "JUREX-4E: Juridical Expert-Annotated Four-Element Knowledge Base for Legal Reasoning",
    "volume": "main",
    "abstract": "In recent years, Large Language Models (LLMs) have been widely applied to legal tasks. To enhance their understanding of legal texts and improve reasoning accuracy, a promising approach is to incorporate legal theories. One of the most widely adopted theories is the Four-Element Theory (FET), which defines the crime constitution through four elements: Subject, Object, Subjective Aspect, and Objective Aspect. While recent work has explored prompting LLMs to follow FET, our evaluation demonstrates that LLM-generated four-elements are often incomplete and less representative, limiting their effectiveness in legal reasoning.To address these issues, we present JUREX-4E, an expert-annotated four-element knowledge base covering 155 criminal charges. The annotations follow a progressive hierarchical framework grounded in legal source validity and incorporate diverse interpretive methods to ensure precision and authority. We evaluate JUREX-4E on the Similar Charge Disambiguation task and apply it to Legal Case Retrieval. Experimental results validate the high quality of JUREX-4E and its substantial impact on downstream legal tasks, underscoring its potential for advancing legal AI applications. The dataset and code are available at: https://github.com/THUlawtech/JUREX",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huanghai Liu",
      "Quzhe Huang",
      "Qingjing Chen",
      "Yiran Hu",
      "Jiayu Ma",
      "Yun Liu",
      "Weixing Shen",
      "Yansong Feng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.189": {
    "title": "CIE: Controlling Language Model Text Generations Using Continuous Signals",
    "volume": "main",
    "abstract": "Aligning language models (LMs) with user intent is becoming increasingly relevant to enhance user experience.This calls for designing methods that can allow users to control the properties of the language that LMs generate, for example, controlling the length of the generation or the complexity of the language that gets chosen.Most existing work attempts to integrate users' control by conditioning LM generations on natural language prompts or discrete control signals, which are often brittle and hard to scale.In this work, we are interested in continuous control signals, ones that exist along a spectrum that can't easily be captured in a natural language prompt or via existing techniques in conditional generation.Through a case study in controlling the precise response-length of generations, we demonstrate how an LM can be finetuned to expect a control vector that is interpolated between a \"low\" and a \"high\" token embedding.Our method more reliably exerts response-length control than in-context learning methods or fine-tuning methods that represent the control signal as a discrete signal",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vinay Samuel",
      "Harshita Diddee",
      "Yiming Zhang",
      "Daphne Ippolito"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.190": {
    "title": "Stand on The Shoulders of Giants: Building JailExpert from Previous Attack Experience",
    "volume": "main",
    "abstract": "Large language models (LLMs) generate human-aligned content under certain safety constraints. However, the current known technique \"jailbreak prompt\" can circumvent safety-aligned measures and induce LLMs to output malicious content. Research on Jailbreaking can help identify vulnerabilities in LLMs and guide the development of robust security frameworks. To circumvent the issue of attack templates becoming obsolete as models evolve, existing methods adopt iterative mutation and dynamic optimization to facilitate more automated jailbreak attacks. However, these methods face two challenges: inefficiency and repetitive optimization, as they overlook the value of past attack experiences. To better integrate past attack experiences to assist current jailbreak attempts, we propose the JailExpert, an automated jailbreak framework, which is the first to achieve a formal representation of experience structure, group experiences based on semantic drift, and support the dynamic updating of the experience pool. Extensive experiments demonstrate that JailExpert significantly improves both attack effectiveness and efficiency. Compared to the current state-of-the-art black-box jailbreak method, JailExpert achieves an average increase of 24% in attack success rate and 2.7 times improvement in attack efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xi Wang",
      "Songlei Jian",
      "Shasha Li",
      "Xiaopeng Li",
      "Bin Ji",
      "Ma Jun",
      "Xiaodong Liu",
      "Jing Wang",
      "Jianfeng Zhang",
      "Jie Yu",
      "Feilong Bao",
      "Wangbaosheng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.191": {
    "title": "Language-to-Space Programming for Training-Free 3D Visual Grounding",
    "volume": "main",
    "abstract": "3D visual grounding (3DVG) is challenging due to the need to understand 3D spatial relations. While supervised approaches have achieved superior performance, they are constrained by the scarcity and high annotation costs of 3D vision-language datasets. Training-free approaches based on LLMs/VLMs eliminate the need for large-scale training data, but they either incur prohibitive grounding time and token costs or have unsatisfactory accuracy. To address the challenges, we introduce a novel method for training-free 3D visual grounding, namely **La**nguage-to-**S**pace **P**rogramming (LaSP). LaSP introduces LLM-generated codes to analyze 3D spatial relations among objects, along with a pipeline that evaluates and optimizes the codes automatically. Experimental results demonstrate that LaSP achieves 52.9% accuracy on the Nr3D benchmark, ranking among the best training-free methods. Moreover, it substantially reduces the grounding time and token costs, offering a balanced trade-off between performance and efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boyu Mi",
      "Hanqing Wang",
      "Tai Wang",
      "Yilun Chen",
      "Jiangmiao Pang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.192": {
    "title": "RAG-Instruct: Boosting LLMs with Diverse Retrieval-Augmented Instructions",
    "volume": "main",
    "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a key paradigm for enhancing large language models by incorporating external knowledge. However, current RAG methods exhibit limited capabilities in complex RAG scenarios and suffer from limited task diversity. To address these limitations, we propose RAG-Instruct, a general method for synthesizing diverse and high-quality RAG instruction data based on any source corpus. Our approach leverages (1) five RAG paradigms, which encompass diverse query-document relationships, and (2) instruction simulation, which enhances instruction diversity and quality by utilizing the strengths of existing instruction datasets. Using this method, we construct a 40K instruction dataset from Wikipedia, comprehensively covering diverse RAG scenarios and tasks. Experiments demonstrate that RAG-Instruct effectively enhances LLMs' RAG capabilities, achieving strong zero-shot performance and significantly outperforming various RAG baselines across a diverse set of tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wanlong Liu",
      "Junying Chen",
      "Ke Ji",
      "Li Zhou",
      "Wenyu Chen",
      "Benyou Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.193": {
    "title": "AdaRewriter: Unleashing the Power of Prompting-based Conversational Query Reformulation via Test-Time Adaptation",
    "volume": "main",
    "abstract": "Prompting-based conversational query reformulation has emerged as a powerful approach for conversational search, refining ambiguous user queries into standalone search queries. Best-of-N reformulation over the generated candidates via prompting shows impressive potential scaling capability. However, both the previous tuning methods (training time) and adaptation approaches (test time) can not fully unleash their benefits. In this paper, we propose AdaRewriter, a novel framework for query reformulation using an outcome-supervised reward model via test-time adaptation. By training a lightweight reward model with contrastive ranking loss, AdaRewriter selects the most promising reformulation during inference. Notably, it can operate effectively in black-box systems, including commercial LLM APIs. Experiments on five conversational search datasets show that AdaRewriter significantly outperforms the existing methods across most settings, demonstrating the potential of test-time adaptation for conversational query reformulation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilong Lai",
      "Jialong Wu",
      "Zhenglin Wang",
      "Deyu Zhou"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.194": {
    "title": "SmartBench: Is Your LLM Truly a Good Chinese Smartphone Assistant?",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have become integral to daily life, especially advancing as intelligent assistants through on-device deployment on smartphones. However, existing LLM evaluation benchmarks predominantly focus on objective tasks like mathematics and coding in English, which do not necessarily reflect the practical use cases of on-device LLMs in real-world mobile scenarios, especially for Chinese users. To address these gaps, we introduce **SmartBench**, the first benchmark designed to evaluate the capabilities of on-device LLMs in Chinese mobile contexts. We analyze functionalities provided by representative smartphone manufacturers and divide them into five categories: text summarization, text Q&A, information extraction, content creation, and notification management, further detailed into 20 specific tasks. For each task, we construct high-quality datasets comprising 50 to 200 question-answer pairs that reflect everyday mobile interactions, and we develop automated evaluation criteria tailored for these tasks. We conduct comprehensive evaluations of on-device LLMs and MLLMs using SmartBench and also assess their performance after quantized deployment on real smartphone NPUs. Our contributions provide a standardized framework for evaluating on-device LLMs in Chinese, promoting further development and optimization in this critical area. Code and data will be available at https://github.com/vivo-ai-lab/SmartBench",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xudong Lu",
      "Haohao Gao",
      "Renshou Wu",
      "Shuai Ren",
      "Xiaoxin Chen",
      "Hongsheng Li",
      "Fangyuan Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.195": {
    "title": "F2TEval: Human-Aligned Multi-Dimensional Evaluation for Figure-to-Text Task",
    "volume": "main",
    "abstract": "Figure-to-Text (F2T) tasks aim to convert structured figure information into natural language text, serving as a bridge between visual perception and language understanding.However, existing evaluation methods remain limited: 1) Reference-based methods can only capture shallow semantic similarities and rely on costly labeled reference text; 2) Reference-free methods depend on multimodal large language models, which suffer from low efficiency and instruction sensitivity; 3) Existing methods provide only sample-level evaluations, lacking interpretability and alignment with expert-level multi-dimensional evaluation criteria.Accordingly, we propose F2TEval, a five-dimensional reference-free evaluation method aligned with expert criteria, covering faithfulness, completeness, conciseness, logicality, and analysis, to support fine-grained evaluation. We design a lightweight mixture-of-experts model that incorporates independent scoring heads and applies the Hilbert-Schmidt Independence Criterion to optimize the disentanglement of scoring representations across dimensions. Furthermore, we construct F2TBenchmark, a human-annotated benchmark dataset covering 21 chart types and 35 application domains, to support research on F2T evaluation. Experimental results demonstrate our model's superior performance and efficiency, outperforming Gemini-2.0 and Claude-3.5 with only 0.9B parameters",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tan Yue",
      "Rui Mao",
      "Zilong Song",
      "Zonghai Hu",
      "Dongyan Zhao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.196": {
    "title": "Icon2: Aligning Large Language Models Using Self-Synthetic Preference Data via Inherent Regulation",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) require high quality preference datasets to align with human preferences. However, conventional methods for constructing such datasets face significant challenges: reliance on pre-collected instructions often leads to distribution mismatches with target models, while the need for sampling multiple stochastic responses introduces substantial computational overhead. In this work, we explore a paradigm shift by leveraging inherent regulation of LLMs' representation space for efficient and tailored preference dataset construction, named Icon2. Specifically, it first extracts layer-wise direction vectors to encode sophisticated human preferences and then uses these vectors to filter self-synthesized instructions based on their inherent consistency. During decoding, bidirectional inherent control is applied to steer token representations, enabling the precise generation of response pairs with clear alignment distinctions. Experimental results demonstrate significant improvements in both alignment and efficiency. Llama3-8B and Qwen2-7B achieve an average win rate improvement of 13.89% on AlpacaEval 2.0 and 13.45% on Arena-Hard, while reducing computational costs by up to 48.1%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiyuan Chen",
      "Hongsen Huang",
      "Qian Shao",
      "Jiahe Chen",
      "Jintai Chen",
      "Hongxia Xu",
      "Renjie Hua",
      "Ren Chuan",
      "Jian Wu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.197": {
    "title": "DSCD: Large Language Model Detoxification with Self-Constrained Decoding",
    "volume": "main",
    "abstract": "Detoxification in large language models (LLMs) remains a significant research challenge. Existing decoding detoxification methods are all based on external constraints, which require additional resource overhead and lose generation fluency. This work innovatively proposes Detoxification with Self-Constrained Decoding (DSCD), a novel method for LLMs detoxification without parameter fine-tuning. DSCD strengthens the inner token distribution of the safety layer while weakening that of hallucination and toxic layer during output generation. This effectively diminishes toxicity and enhances output safety. DSCD offers lightweight, high compatibility, and plug-and-play capabilities, readily integrating with existing detoxification methods for further performance improvement. Extensive experiments on representative open-source LLMs and public datasets validate DSCD's effectiveness, demonstrating state-of-the-art (SOTA) performance in both detoxification and generation fluency, with superior efficiency compared to existing methods. These results highlight DSCD's potential as a practical and scalable solution for safer LLM deployments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming Dong",
      "Jinkui Zhang",
      "Bolong Zheng",
      "Xinhui Tu",
      "Po Hu",
      "Tingting He"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.198": {
    "title": "From Reasoning to Answer: Empirical, Attention-Based and Mechanistic Insights into Distilled DeepSeek R1 Models",
    "volume": "main",
    "abstract": "Large Reasoning Models (LRMs) generate explicit reasoning traces alongside final answers, yet the extent to which these traces influence answer generation remains unclear. In this work, we conduct a three-stage investigation into the interplay between reasoning and answer generation in three distilled DeepSeek R1 models. First, through empirical evaluation, we demonstrate that including explicit reasoning consistently improves answer quality across diverse domains. Second, attention analysis reveals that answer tokens attend substantially to reasoning tokens, with certain mid-layer Reasoning-Focus Heads (RFHs) closely tracking the reasoning trajectory, including self-reflective cues. Third, we apply mechanistic interventions using activation patching to assess the dependence of answer tokens on reasoning activations. Our results show that perturbations to key reasoning tokens can reliably alter the final answers, confirming a directional and functional flow of information from reasoning to answer. These findings deepen our understanding of how LRMs leverage reasoning tokens for answer generation, highlighting the functional role of intermediate reasoning in shaping model outputs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jue Zhang",
      "Qingwei Lin",
      "Saravan Rajmohan",
      "Dongmei Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.199": {
    "title": "Quantifying Language Disparities in Multilingual Large Language Models",
    "volume": "main",
    "abstract": "Results reported in large-scale multilingual evaluations are often fragmented and confounded by factors such as target languages, differences in experimental setups, and model choices. We propose a framework that disentangles these confounding variables and introduces three interpretable metrics—the performance realisation ratio, its coefficient of variation, and language potential—enabling a finer-grained and more insightful quantification of actual performance disparities across both (i) models and (ii) languages. Through a case study of 13 model variants on 11 multilingual datasets, we demonstrate that our framework provides a more reliable measurement of model performance and language disparities, particularly for low-resource languages, which have so far proven challenging to evaluate. Importantly, our results reveal that higher overall model performance does not necessarily imply greater fairness across languages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Songbo Hu",
      "Ivan Vulić",
      "Anna Korhonen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.200": {
    "title": "KoBLEX: Open Legal Question Answering with Multi-hop Reasoning",
    "volume": "main",
    "abstract": "Large Language Models (LLM) have achieved remarkable performances in general domains and are now extending into the expert domain of law. Several benchmarks have been proposed to evaluate LLMs' legal capabilities. However, these benchmarks fail to evaluate open-ended and provision-grounded Question Answering (QA). To address this, we introduce a Korean Benchmark for Legal EXplainable QA (KoBLEX), designed to evaluate provision-grounded, multi-hop legal reasoning. KoBLEX includes 226 scenario-based QA instances and their supporting provisions, created using a hybrid LLM–human expert pipeline. We also propose a method called Parametric provision-guided Selection Retrieval (ParSeR), which uses LLM-generated parametric provisions to guide legally grounded and reliable answers. ParSeR facilitates multi-hop reasoning on complex legal questions by generating parametric provisions and employing a three-stage sequential retrieval process. Furthermore, to better evaluate the legal fidelity of the generated answers, we propose Legal Fidelity Evaluation (LF-Eval). LF-Eval is an automatic metric that jointly considers the question, answer, and supporting provisions and shows a high correlation with human judgments. Experimental results show that ParSeR consistently outperforms strong baselines, achieving the best results across multiple LLMs. Notably, compared to standard retrieval with GPT-4o, ParSeR achieves +37.91 higher F1 and +30.81 higher LF-Eval. Further analyses reveal that ParSeR efficiently delivers consistent performance across reasoning depths, with ablations confirming the effectiveness of ParSeR",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jihyung Lee",
      "Daehui Kim",
      "Seonjeong Hwang",
      "Hyounghun Kim",
      "Gary Lee"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.201": {
    "title": "End-to-End Learnable Psychiatric Scale Guided Risky Post Screening for Depression Detection on Social Media",
    "volume": "main",
    "abstract": "Detecting depression through users' social media posting history is crucial for enabling timely intervention; however, irrelevant content within these posts negatively impacts detection performance. Thus, it is crucial to extract pertinent content from users' complex posting history. Current methods utilize frozen screening models, which can miss critical information and limit overall performance due to isolated screening and detection processes. To address these limitations, we propose **E2-LPS** **E**nd-to-**E**nd **L**earnable **P**sychiatric Scale Guided Risky Post **S**creening Model) for jointly training our screening model, guided by psychiatric scales, alongside the detection model. We employ a straight-through estimator to enable a learnable end-to-end screening process and avoid the non-differentiability of the screening process. Experimental results show that E2-LPS outperforms several strong baseline methods, and qualitative analysis confirms that it better captures users' mental states than others",
    "checked": true,
    "id": "e19c2d1260620c7f05564699d8a4dcb9233ba9d4",
    "semantic_title": "end-to-end learnable psychiatric scale guided risky post screening for depression detection on social media",
    "citation_count": 0,
    "authors": [
      "Bichen Wang",
      "Yuzhe Zi",
      "Yixin Sun",
      "Hao Yang",
      "Yanyan Zhao",
      "Bing Qin"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.202": {
    "title": "ReAgent: Reversible Multi-Agent Reasoning for Knowledge-Enhanced Multi-Hop QA",
    "volume": "main",
    "abstract": "Multi-hop question answering (QA) remains challenging, as solutions must reliably integrate and reconcile evidence from multiple sources without succumbing to error propagation. While large language models (LLMs) have achieved substantial improvements via chain-of-thought (CoT) prompting and retrieval-augmented generation, these methods typically adopt a forward-only workflow—early mistakes persist throughout inference, and contradictions discovered later cannot systematically trigger re-evaluation. To address this limitation, we present ReAgent, a reversible multi-agent reasoning framework. Specifically, ReAgent enables agents to backtrack to earlier valid states when conflicts arise, thereby isolating and rectifying flawed assumptions before they undermine subsequent reasoning. Our approach combines explicit local and global rollback protocols with modular role specialization, resulting in a flexible and error-tolerant pipeline. Empirical evaluation on three multi-hop QA benchmarks demonstrates consistent performance gains of approximately 6% over forward-only baselines, in addition to enhanced interpretability. These findings highlight the value of non-monotonic, backtracking-driven inference in complex QA scenarios and point to broader implications for multi-agent collaboration in knowledge-intensive tasks",
    "checked": true,
    "id": "33de8e46815c4ccbbdf0012d82db64bce0a4698c",
    "semantic_title": "reagent: reversible multi-agent reasoning for knowledge-enhanced multi-hop qa",
    "citation_count": 4,
    "authors": [
      "Zhao Xinjie",
      "Fan Gao",
      "Xingyu Song",
      "Yingjian Chen",
      "Rui Yang",
      "Yanran Fu",
      "Yuyang Wang",
      "Yusuke Iwasawa",
      "Yutaka Matsuo",
      "Irene Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.203": {
    "title": "Matter-of-Fact: A Benchmark for Verifying the Feasibility of Literature-Supported Claims in Materials Science",
    "volume": "main",
    "abstract": "Contemporary approaches to assisted scientific discovery use language models to automatically generate large numbers of potential hypothesis to test, while also automatically generating code-based experiments to test those hypotheses. While hypotheses can be comparatively inexpensive to generate, automated experiments can be costly, particularly when run at scale (i.e. thousands of experiments). Developing the capacity to filter hypotheses based on their feasibility would allow discovery systems to run at scale, while increasing their likelihood of making significant discoveries. In this work we introduce Matter-of-Fact, a challenge dataset for determining the feasibility of hypotheses framed as claims, while operationalizing feasibility assessment as a temporally-filtered claim verification task using backtesting. Matter-of-Fact includes 8.4k claims extracted from scientific articles spanning four high-impact contemporary materials science topics, including superconductors, semiconductors, batteries, and aerospace materials, while including qualitative and quantitative claims from theoretical, experimental, and code/simulation results. We show that strong baselines that include retrieval augmented generation over scientific literature and code generation fail to exceed 72% performance on this task (chance performance is 50%), while domain-expert verification suggests nearly all are solvable – highlighting both the difficulty of this task for current models, and the potential to accelerate scientific discovery by making near-term progress",
    "checked": true,
    "id": "c72dd121863d9caaae0c9363278439bb42f0a8dc",
    "semantic_title": "matter-of-fact: a benchmark for verifying the feasibility of literature-supported claims in materials science",
    "citation_count": 1,
    "authors": [
      "Peter Jansen",
      "Samiah Hassan",
      "Ruoyao Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.204": {
    "title": "ModRWKV: Transformer Multimodality in Linear Time",
    "volume": "main",
    "abstract": "Currently, most multimodal studies are based on large language models (LLMs) with quadratic-complexity Transformer architectures. While linear models like RNNs enjoy low inference costs, their application has been largely limited to the text-only modality. This work explores the capabilities of modern RNN architectures in multimodal contexts. We propose ModRWKV—a decoupled multimodal framework built upon the RWKV7 architecture as its LLM backbone—which achieves multi-source information fusion through dynamically adaptable heterogeneous modality encoders. We designed the multimodal modules in ModRWKV with an extremely lightweight architecture and, through extensive experiments, identified a configuration that achieves an optimal balance between performance and computational efficiency. ModRWKV leverages the pretrained weights of the RWKV7 LLM for initialization, which significantly accelerates multimodal training. Comparative experiments with different pretrained checkpoints further demonstrate that such initialization plays a crucial role in enhancing the model's ability to understand multimodal signals. Supported by extensive experiments, we conclude that modern RNN architectures present a viable alternative to Transformers in the domain of multimodal large language models (MLLMs). Furthermore, we identify the optimal configuration of the ModRWKV architecture through systematic exploration",
    "checked": true,
    "id": "d5018a723173831dadc756bdda2e22ba766bf631",
    "semantic_title": "modrwkv: transformer multimodality in linear time",
    "citation_count": 0,
    "authors": [
      "Jiale Kang",
      "Ziyin Yue",
      "Qingyu Yin",
      "Rui Jiang",
      "Weile Li",
      "Zening Lu",
      "Zhouran Ji"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.205": {
    "title": "Multimedia Event Extraction with LLM Knowledge Editing",
    "volume": "main",
    "abstract": "Multimodal event extraction task aims to identify event types and arguments from visual and textual representations related to events. Due to the high cost of multimedia training data, previous methods mainly focused on weakly alignment of excellent unimodal encoders. However, they ignore the conflict between event understanding and image recognition, resulting in redundant feature perception affecting the understanding of multimodal events. In this paper, we propose a multimodal event extraction strategy with a multi-level redundant feature selection mechanism, which enhances the event understanding ability of multimodal large language models by leveraging knowledge editing techniques, and requires no additional parameter optimization work. Extensive experiments show that our method outperforms the state-of-the-art (SOTA) baselines on the M2E2 benchmark. Compared with the highest baseline, we achieve a 34% improvement of precision on event extraction and a 11% improvement of F1 on argument extraction",
    "checked": true,
    "id": "9d5833682f5d5ce35d33e63f9e4a1e9a0fc6a958",
    "semantic_title": "multimedia event extraction with llm knowledge editing",
    "citation_count": 0,
    "authors": [
      "Jiaao Yu",
      "Yijing Lin",
      "Zhipeng Gao",
      "Xuesong Qiu",
      "Lanlan Rui"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.206": {
    "title": "Exploring the Impact of Personality Traits on LLM Toxicity and Bias",
    "volume": "main",
    "abstract": "With the different roles that AI is expected to play in human life, imbuing large language models (LLMs) with different personalities has attracted increasing research interest. While the \"personification\" enhances human experiences of interactivity and adaptability of LLMs, it gives rise to critical concerns about content safety, particularly regarding bias, sentiment, and toxicity of LLM generation. This study explores how assigning different personality traits to LLMs affects the toxicity and biases of their outputs. Leveraging the widely accepted HEXACO personality framework developed in social psychology, we design experimentally sound prompts to test three LLMs' performance on three toxic and bias benchmarks. The findings demonstrate the sensitivity of all three models to HEXACO personality traits and, more importantly, a consistent variation in the biases, negative sentiment, and toxicity of their output. In particular, adjusting the levels of several personality traits can effectively reduce bias and toxicity in model performance, similar to humans' correlations between personality traits and toxic behaviors. The findings highlight the additional need to examine content safety besides the efficiency of training or fine-tuning methods for LLM personification, they also suggest a potential for the adjustment of personalities to be a simple and low-cost method to conduct controlled text generation",
    "checked": true,
    "id": "690549425a182abb499f27920ef1b594ea583196",
    "semantic_title": "exploring the impact of personality traits on llm toxicity and bias",
    "citation_count": 0,
    "authors": [
      "Shuo Wang",
      "Renhao Li",
      "Xi Chen",
      "Yulin Yuan",
      "Min Yang",
      "Derek F. Wong"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.207": {
    "title": "Task-aware Contrastive Mixture of Experts for Quadruple Extraction in Conversations with Code-like Replies and Non-opinion Detection",
    "volume": "main",
    "abstract": "This paper focuses on Dialogue Aspect-based Sentiment Quadruple (DiaASQ) analysis, aiming to extract structured quadruples from multi-turn conversations. Applying Large Language Models (LLMs) for this specific task presents two primary challenges: the accurate extraction of multiple elements and the understanding of complex dialogue reply structure. To tackle these issues, we propose a novel LLM-based multi-task approach, named Task-aware Contrastive Mixture of Experts (TaCoMoE), to tackle the DiaASQ task by integrating expert-level contrastive loss within task-oriented mixture of experts layer. TaCoMoE minimizes the distance between the representations of the same expert in the semantic space while maximizing the distance between the representations of different experts to efficiently learn representations of different task samples. Additionally, we design a Graph-Centric Dialogue Structuring strategy for representing dialogue reply structure and perform non-opinion utterances detection to enhance the performance of quadruple extraction. Extensive experiments are conducted on the DiaASQ dataset, demonstrating that our method significantly outperforms existing parameter-efficient fine-tuning techniques in terms of both accuracy and computational efficiency. The code is available at https://github.com/he2720/TaCoMoE",
    "checked": true,
    "id": "8d9eaf63d0b4009dfd5e3e5a9befdd00b01970c9",
    "semantic_title": "task-aware contrastive mixture of experts for quadruple extraction in conversations with code-like replies and non-opinion detection",
    "citation_count": 0,
    "authors": [
      "Chenyuan He",
      "Yuxiang Jia",
      "Fei Gao",
      "Senbin Zhu",
      "Hongde Liu",
      "Hongying Zan",
      "Min Peng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.208": {
    "title": "Mitigating Biases in Language Models via Bias Unlearning",
    "volume": "main",
    "abstract": "Many studies have shown various biases targeting different demographic groups in language models, amplifying discrimination and harming fairness. Recent parameter modification debiasing approaches significantly degrade core capabilities such as text coherence and task accuracy. And Prompt-based debiasing methods, only effective for predefined trigger words, fail to address deeply embedded stereotypical associations in model parameters. In this paper, we propose BiasUnlearn, a novel model debiasing framework which achieves targeted debiasing via dual-pathway unlearning mechanisms coordinating stereotype forgetting with anti-stereotype retention, while preventing bias polarity reversal through adversarial forget set and dynamic dataset swapping. We conducted extensive experiments with multiple language models across various evaluation benchmarks. The results show that BiasUnlearn outperforms existing methods in mitigating bias in language models while retaining language modeling capabilities. Further experiments reveal that debiasing weights are transferable across model variants, confirming that bias representations become entrenched during pre-training and persist through fine-tuning phases",
    "checked": true,
    "id": "74995e1129ecbcabd6feb4d84c0e7ba3476a724d",
    "semantic_title": "mitigating biases in language models via bias unlearning",
    "citation_count": 0,
    "authors": [
      "Dianqing Liu",
      "Yi Liu",
      "Guoqing Jin",
      "Zhendong Mao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.209": {
    "title": "UNComp: Can Matrix Entropy Uncover Sparsity? — A Compressor Design from an Uncertainty-Aware Perspective",
    "volume": "main",
    "abstract": "Deploying large language models (LLMs) for long-context inference remains challenging due to their substantial memory and computational demands. While techniques such as Key-Value (KV) cache compression are designed to reduce memory usage, they often neglect the structured sparsity inherent in the relationship between hidden states and their corresponding KV cache. In this work, we explore the role of uncertainty as a potential indicator of sparsity within LLMs. We propose UNComp, an uncertainty-aware framework that leverages truncated matrix entropy to identify areas of low information content, thereby revealing sparsity patterns that can be used for adaptive compression. Unlike traditional methods that apply uniform compression, UNComp dynamically adjusts its approach to compression, guided by uncertainty measures that reflect the importance of various model components. Our analysis shows that sparsity patterns, when derived from uncertainty estimates, can be exploited to reveal special long-range dependencies, such as retrieval heads and retrieval layers. This perspective not only enhances our understanding of how compression can be optimized but also provides new insights into the inherent sparsity of LLMs during long-context inference. By focusing on uncertainty to analyze the sparsity pattern in detail, UNComp reduces the KV cache size to 4.74% of the original, achieves a 6% prefill speedup, and improves throughput by 6.4× — not only delivering strong lossless compression performance, but also validating the effectiveness of the underlying theoretical tool. Our codes are submitted with the paper",
    "checked": true,
    "id": "ca1b8450e4175c19989e484d0d7bc99bd0b7a7bc",
    "semantic_title": "uncomp: can matrix entropy uncover sparsity? — a compressor design from an uncertainty-aware perspective",
    "citation_count": 9,
    "authors": [
      "Jing Xiong",
      "Jianghan Shen",
      "Fanghua Ye",
      "Chaofan Tao",
      "Zhongwei Wan",
      "Jianqiao Lu",
      "Xun Wu",
      "Chuanyang Zheng",
      "Zhijiang Guo",
      "Min Yang",
      "Lingpeng Kong",
      "Ngai Wong"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.210": {
    "title": "Superpose Task-specific Features for Model Merging",
    "volume": "main",
    "abstract": "Model merging enables powerful capabilities in neural networks without requiring additional training. In this paper, we introduce a novel perspective on model merging by leveraging the fundamental mechanisms of neural network representation. Our approach is motivated by the linear representation hypothesis, which states that neural networks encode information through linear combinations of feature vectors. We propose a method that superposes task-specific features from individual models into a merged model. Our approach specifically targets linear transformation matrices, which are crucial for feature activation and extraction in deep networks. By formulating the merging process as a linear system, we can preserve output feature directions from individual models and create merged models that effectively maintain multi-task capabilities compared to existing methods. Extensive experiments across diverse benchmarks and models demonstrate that our method outperforms existing techniques",
    "checked": true,
    "id": "2f6122f492da4e8f7c754fdc7ff0a7f2f81ddf1d",
    "semantic_title": "superpose task-specific features for model merging",
    "citation_count": 1,
    "authors": [
      "Haiquan Qiu",
      "You Wu",
      "Dong Li",
      "Jianmin Guo",
      "Quanming Yao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.211": {
    "title": "FinRAGBench-V: A Benchmark for Multimodal RAG with Visual Citation in the Financial Domain",
    "volume": "main",
    "abstract": "Retrieval-Augmented Generation (RAG) plays a vital role in the financial domain, powering applications such as real-time market analysis, trend forecasting, and interest rate computation. However, most existing RAG research in finance focuses predominantly on textual data, overlooking the rich visual content in financial documents, resulting in the loss of key analytical insights. To bridge this gap, we present FinRAGBench-V, a comprehensive visual RAG benchmark tailored for finance. This benchmark effectively integrates multimodal data and provides visual citation to ensure traceability. It includes a bilingual retrieval corpus with 60,780 Chinese and 51,219 English pages, along with a high-quality, human-annotated question-answering (QA) dataset spanning heterogeneous data types and seven question categories. Moreover, we introduce RGenCite, an RAG baseline that seamlessly integrates visual citation with generation. Furthermore, we propose an automatic citation evaluation method to systematically assess the visual citation capabilities of Multimodal Large Language Models (MLLMs). Extensive experiments on RGenCite underscore the challenging nature of FinRAGBench-V, providing valuable insights for the development of multimodal RAG systems in finance",
    "checked": true,
    "id": "074a521a4ddfec2fc12dc36928965c1788211121",
    "semantic_title": "finragbench-v: a benchmark for multimodal rag with visual citation in the financial domain",
    "citation_count": 2,
    "authors": [
      "Suifeng Zhao",
      "Zhuoran Jin",
      "Sujian Li",
      "Jun Gao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.212": {
    "title": "BacktrackAgent: Enhancing GUI Agent with Error Detection and Backtracking Mechanism",
    "volume": "main",
    "abstract": "Graphical User Interface (GUI) agents have gained substantial attention due to their impressive capabilities to complete tasks through multiple interactions within GUI environments. However, existing agents primarily focus on enhancing the accuracy of individual actions and often lack effective mechanisms for detecting and recovering from errors. To address these shortcomings, we propose the BacktrackAgent, a robust framework that incorporates a backtracking mechanism to improve task completion efficiency. BacktrackAgent includes verifier, judger, and reflector components as modules for error detection and recovery, while also applying judgment rewards to further enhance the agent's performance. Additionally, we develop a training dataset specifically designed for the backtracking mechanism, which considers the outcome pages after action executions. Experimental results show that BacktrackAgent has achieved performance improvements in both task success rate and step accuracy on Mobile3M and Auto-UI benchmarks. Our data and code will be released upon acceptance",
    "checked": true,
    "id": "a0c0c2e7c9a316e7e9e7006640e58556669251e0",
    "semantic_title": "backtrackagent: enhancing gui agent with error detection and backtracking mechanism",
    "citation_count": 4,
    "authors": [
      "Qinzhuo Wu",
      "Pengzhi Gao",
      "Wei Liu",
      "Jian Luan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.213": {
    "title": "Diffusion vs. Autoregressive Language Models: A Text Embedding Perspective",
    "volume": "main",
    "abstract": "Large language model (LLM)-based embedding models, benefiting from large scale pre-training and post-training, have begun to surpass BERT and T5-based models on general-purpose text embedding tasks such as document retrieval. However, a fundamental limitation of LLM embeddings lies in the unidirectional attention used during autoregressive pre-training, which misaligns with the bidirectional nature of text embedding tasks. To this end, We propose adopting diffusion language models for text embeddings, motivated by their inherent bidirectional architecture and recent success in matching or surpassing LLMs especially on reasoning tasks. We present the first systematic study of the diffusion language embedding model, which outperforms the LLM-based embedding model by 20% on long-document retrieval, 8% on reasoning-intensive retrieval, 2% on instruction-following retrieval, and achieve competitive performance on traditional text embedding benchmarks. Our analysis verifies that bidirectional attention is crucial for encoding global context in long and complex text",
    "checked": true,
    "id": "738b8d70e58ac200ef41c8cdc8c8ecc4cf242123",
    "semantic_title": "diffusion vs. autoregressive language models: a text embedding perspective",
    "citation_count": 5,
    "authors": [
      "Siyue Zhang",
      "Yilun Zhao",
      "Liyuan Geng",
      "Arman Cohan",
      "Anh Tuan Luu",
      "Chen Zhao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.214": {
    "title": "BannerAgency: Advertising Banner Design with Multimodal LLM Agents",
    "volume": "main",
    "abstract": "Advertising banners are critical for capturing user attention and enhancing advertising campaign effectiveness. Creating aesthetically pleasing banner designs while conveying the campaign messages is challenging due to the large search space involving multiple design elements. Additionally, advertisers need multiple sizes for different displays and various versions to target different sectors of audiences. Since design is intrinsically an iterative and subjective process, flexible editability is also in high demand for practical usage. While current models have served as assistants to human designers in various design tasks, they typically handle only segments of the creative design process or produce pixel-based outputs that limit editability. This paper introduces a training-free framework for fully automated banner ad design creation, enabling frontier multimodal large language models (MLLMs) to streamline the production of effective banners with minimal manual effort across diverse marketing contexts. We present BannerAgency, an MLLM agent system that collaborates with advertisers to understand their brand identity and banner objectives, generates matching background images, creates blueprints for foreground design elements, and renders the final creatives as editable components in Figma or SVG formats rather than static pixels. To facilitate evaluation and future research, we introduce BannerRequest400, a benchmark featuring 100 unique logos paired with 400 diverse banner requests. Through quantitative and qualitative evaluations, we demonstrate the framework's effectiveness, emphasizing the quality of the generated banner designs, their adaptability to various banner requests, and their strong editability enabled by this component-based approach",
    "checked": true,
    "id": "6cf85f6782322e4669deb1eefda341c31b93ca9b",
    "semantic_title": "banneragency: advertising banner design with multimodal llm agents",
    "citation_count": 5,
    "authors": [
      "Heng Wang",
      "Yotaro Shimose",
      "Shingo Takamatsu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.215": {
    "title": "DIDS: Domain Impact-aware Data Sampling for Large Language Model Training",
    "volume": "main",
    "abstract": "Large language models (LLMs) are commonly trained on multi-domain datasets, where domain sampling strategies significantly impact model performance due to varying domain importance across downstream tasks. Existing approaches for optimizing domain-level sampling strategies struggle with maintaining intra-domain consistency and accurately measuring domain impact. In this paper, we present Domain Impact-aware Data Sampling (DIDS). To ensure intra-domain consistency, a gradient clustering algorithm is proposed to group training data based on their learning effects, where a proxy language model and dimensionality reduction are employed to reduce computational overhead. To accurately measure domain impact, we develop a Fisher Information Matrix (FIM) guided metric that quantifies how domain-specific parameter updates affect the model's output distributions on downstream tasks, with theoretical guarantees. Furthermore, to determine optimal sampling ratios, DIDS combines both the FIM-guided domain impact assessment and loss learning trajectories that indicate domain-specific potential, while accounting for diminishing marginal returns. Extensive experiments demonstrate that DIDS achieves 3.4% higher average performance while maintaining comparable training efficiency. The code is available at https://github.com/shiweijiezero/DIDS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weijie Shi",
      "Jipeng Zhang",
      "Yaguang Wu",
      "Jingzhi Fang",
      "Shibo Zhang",
      "Yao Zhao",
      "Hao Chen",
      "Ruiyuan Zhang",
      "Yue Cui",
      "Jia Zhu",
      "Sirui Han",
      "Jiajie Xu",
      "Xiaofang Zhou"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.216": {
    "title": "Training LLMs to be Better Text Embedders through Bidirectional Reconstruction",
    "volume": "main",
    "abstract": "Large language models (LLMs) have increasingly been explored as powerful text embedders. Existing LLM-based text embedding approaches often leverage the embedding of the final token, typically a reserved special token such as ‘[EOS]‘. However, these tokens have not been intentionally trained to capture the semantics of the whole context, limiting their capacity as text embeddings, especially for retrieval and re-ranking tasks. We propose to add a new training stage before contrastive learning to enrich the semantics of the final token embedding. This stage employs bidirectional generative reconstruction tasks, namely EBQ2D (Embedding-Based Query-to-Document) and EBD2Q (Embedding-Based Document-to-Query), which interleave to anchor the ‘[EOS]‘ embedding and reconstruct either side of Query-Document pairs. Experimental results demonstrate that our additional training stage significantly improves LLM performance on the Massive Text Embedding Benchmark (MTEB), achieving new state-of-the-art results across different LLM base models and scales",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chang Su",
      "Dengliang Shi",
      "Siyuan Huang",
      "Jintao Du",
      "Changhua Meng",
      "Yu Cheng",
      "Weiqiang Wang",
      "Zhouhan Lin"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.217": {
    "title": "ReMedy: Learning Machine Translation Evaluation from Human Preferences with Reward Modeling",
    "volume": "main",
    "abstract": "A key challenge in MT evaluation is the inherent noise and inconsistency of human ratings. Regression-based neural metrics struggle with this noise, while prompting LLMs shows promise at system-level evaluation but performs poorly at segment level. In this work, we propose ReMedy, a novel MT metric framework that reformulates translation evaluation as a reward modeling task. Instead of regressing on imperfect human ratings directly, ReMedy learns relative translation quality using pairwise preference data, resulting in a more reliable evaluation. In extensive experiments across WMT22-24 shared tasks (39 language pairs, 111 MT systems), ReMedy achieves state-of-the-art performance at both segment- and system-level evaluation. Specifically, ReMedy-9B surpasses larger WMT winners and massive closed LLMs such as MetricX-13B, XCOMET-Ensemble, GEMBA-GPT-4, PaLM-540B, and finetuned PaLM2. Further analyses demonstrate that ReMedy delivers superior capability in detecting translation errors and evaluating low-quality translations",
    "checked": true,
    "id": "544ec51b8e94c025cdee53ce406c7862d918d1ea",
    "semantic_title": "remedy: learning machine translation evaluation from human preferences with reward modeling",
    "citation_count": 0,
    "authors": [
      "Shaomu Tan",
      "Christof Monz"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.218": {
    "title": "SolEval: Benchmarking Large Language Models for Repository-level Solidity Smart Contract Generation",
    "volume": "main",
    "abstract": "Large language models (LLMs) have transformed code generation.However, most existing approaches focus on mainstream languages such as Python and Java, neglecting the Solidity language, the predominant programming language for Ethereum smart contracts.Due to the lack of adequate benchmarks for Solidity, LLMs' ability to generate secure, cost-effective smart contracts remains unexplored.To fill this gap, we construct SolEval, the first repository-level benchmark designed for Solidity smart contract generation, to evaluate the performance of LLMs on Solidity.SolEval consists of 1,507 samples from 28 different repositories, covering 6 popular domains, providing LLMs with a comprehensive evaluation benchmark.Unlike the existing Solidity benchmark, SolEval not only includes complex function calls but also reflects the real-world complexity of the Ethereum ecosystem by incorporating Gas@k and Vul@k.We evaluate 16 LLMs on SolEval, and our results show that the best-performing LLM achieves only 26.29% Pass@10, highlighting substantial room for improvement in Solidity code generation by LLMs.Additionally, we conduct supervised fine-tuning (SFT) on Qwen-7B using SolEval, resulting in a significant performance improvement, with Pass@5 increasing from 16.67% to 58.33%, demonstrating the effectiveness of fine-tuning LLMs on our benchmark.We release our data and code at https://github.com/pzy2000/SolEval",
    "checked": true,
    "id": "303436dc881b79518672252f70c978f2999fc6ac",
    "semantic_title": "soleval: benchmarking large language models for repository-level solidity smart contract generation",
    "citation_count": 0,
    "authors": [
      "Zhiyuan Peng",
      "Xin Yin",
      "Rui Qian",
      "Peiqin Lin",
      "YongKang Liu",
      "Hao Zhang",
      "Chenhao Ying",
      "Yuan Luo"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.219": {
    "title": "In-Context Learning Boosts Speech Recognition via Human-like Adaptation to Speakers and Language Varieties",
    "volume": "main",
    "abstract": "Human listeners readily adjust to unfamiliar speakers and language varieties through exposure, but do these adaptation benefits extend to state-of-the-art spoken language models (SLMs)? We introduce a scalable framework that allows for in-context learning (ICL) in Phi-4 Multimodal (Phi-4-MM) using interleaved task prompts and audio-text pairs, and find that as few as 12 example utterances (~50 seconds) at inference time reduce word error rates by a relative 19.7% (1.2 pp.) on average across diverse English corpora. These improvements are most pronounced in low-resource varieties, when the context and target speaker match, and when more examples are provided—though scaling our procedure yields diminishing marginal returns to context length. Overall, we find that our novel ICL adaptation scheme (1) reveals a similar performance profile to human listeners, and (2) demonstrates consistent improvements to automatic speech recognition (ASR) robustness across diverse speakers and language backgrounds. While adaptation succeeds broadly, significant gaps remain for certain varieties, revealing where current models still fall short of human flexibility. We release our prompts and code on GitHub",
    "checked": true,
    "id": "6171e7696345e9b08560233eeaefba2280369730",
    "semantic_title": "in-context learning boosts speech recognition via human-like adaptation to speakers and language varieties",
    "citation_count": 4,
    "authors": [
      "Nathan Roll",
      "Calbert Graham",
      "Yuka Tatsumi",
      "Kim Tien Nguyen",
      "Meghan Sumner",
      "Dan Jurafsky"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.220": {
    "title": "Reasoning Model Unlearning: Forgetting Traces, Not Just Answers, While Preserving Reasoning Skills",
    "volume": "main",
    "abstract": "Recent advances in large reasoning models (LRMs) have enabled strong multi-step reasoning capabilities. However, existing machine unlearning algorithms are tailored to standard language modeling and fail to address the unique challenges posed by LRMs. In this work, we present the first systematic study of LRM unlearning and reveal that conventional unlearning methods often overlook critical information leakage in reasoning traces, even when final answers are successfully removed. To address this, we propose Reasoning-aware Representation Misdirection for Unlearning (R2MU), a method that suppresses sensitive reasoning traces while preserving the model's general reasoning ability. Our experiments demonstrate that R2MU significantly reduces reasoning trace leakage and achieves strong performance across both reasoning and safety benchmarks, including WMDP, StrongReject, JBB-Behaviors and WildJailbreak, under state-of-the-art models such as DeepSeek-R1-Distill-LLaMA-8B and DeepSeek-R1-Distill-Qwen-14B. To the best of our knowledge, MU is the first principled approach to both expose and mitigate reasoning trace leakage in LRM unlearning, while preserving reasoning ability",
    "checked": true,
    "id": "69c6519c479925beb6f376cd3e4a3e94c69ff54d",
    "semantic_title": "reasoning model unlearning: forgetting traces, not just answers, while preserving reasoning skills",
    "citation_count": 6,
    "authors": [
      "Changsheng Wang",
      "Chongyu Fan",
      "Yihua Zhang",
      "Jinghan Jia",
      "Dennis Wei",
      "Parikshit Ram",
      "Nathalie Baracaldo",
      "Sijia Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.221": {
    "title": "Chain-of-Talkers (CoTalk): Fast Human Annotation of Dense Image Captions",
    "volume": "main",
    "abstract": "While densely annotated image captions significantly facilitate the learning of robust vision-language alignment, methodologies for systematically optimizing human annotation efforts remain underexplored. We introduce Chain-of-Talkers (CoTalk), an AI-in-the-loop methodology designed to maximize the number of annotated samples and improve their comprehensiveness under fixed budget constraints (e.g., total human annotation time). The framework is built upon two key insights. First, sequential annotation reduces redundant workload compared to conventional parallel annotation, as subsequent annotators only need to annotate the \"residual\"—the missing visual information that previous annotations have not covered. Second, humans process textual input faster by reading while outputting annotations with much higher throughput via talking; thus a multimodal interface enables optimized efficiency. We evaluate our framework from two aspects: intrinsic evaluations that assess the comprehensiveness of semantic units, obtained by parsing detailed captions into object-attribute trees and analyzing their effective connections; extrinsic evaluation measures the practical usage of the annotated captions in facilitating vision-language alignment. Experiments with eight participants show our Chain-of-Talkers (CoTalk) improves annotation speed (0.42 vs. 0.30 units/sec) and retrieval performance (41.13% vs. 40.52%) over the parallel method",
    "checked": true,
    "id": "ef2cb2427e5b6768a809f1da243ae00a0d2ee0ea",
    "semantic_title": "chain-of-talkers (cotalk): fast human annotation of dense image captions",
    "citation_count": 0,
    "authors": [
      "Yijun Shen",
      "Delong Chen",
      "Fan Liu",
      "Xingyu Wang",
      "Chuanyi Zhang",
      "Liang Yao",
      "Yuhui Zheng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.222": {
    "title": "DecoupleSearch: Decouple Planning and Search via Hierarchical Reward Modeling",
    "volume": "main",
    "abstract": "Retrieval-Augmented Generation (RAG) systems have emerged as a pivotal methodology for enhancing Large Language Models (LLMs) through the dynamic integration of external knowledge. To further improve RAG's flexibility, Agentic RAG introduces autonomous agents into the workflow. However, Agentic RAG faces several challenges:(1) the success of each step depends on both high-quality planning and accurate search,(2) the lack of supervision for intermediate reasoning steps, and(3) the exponentially large candidate space for planning and searching.To address these challenges, we propose DecoupleSearch, a novel framework that decouples planning and search processes using dual value models, enabling independent optimization of plan reasoning and search grounding. Our approach constructs a reasoning tree, where each node represents planning and search steps. We leverage Monte Carlo Tree Search to assess the quality of each step. During inference, Hierarchical Beam Search iteratively refines planning and search candidates with dual value models. Extensive experiments across policy models of varying parameter sizes, demonstrate the effectiveness of our method",
    "checked": true,
    "id": "8937b7a5c0261011fa57565e810c1292339c8388",
    "semantic_title": "decouplesearch: decouple planning and search via hierarchical reward modeling",
    "citation_count": 0,
    "authors": [
      "Hao Sun",
      "Zile Qiao",
      "Bo Wang",
      "Guoxin Chen",
      "Yingyan Hou",
      "Yong Jiang",
      "Pengjun Xie",
      "Fei Huang",
      "Yan Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.223": {
    "title": "RewardDS: Privacy-Preserving Fine-Tuning for Large Language Models via Reward Driven Data Synthesis",
    "volume": "main",
    "abstract": "The success of large language models (LLMs) has attracted many individuals to fine-tune them for domain-specific tasks by uploading their data. However, in sensitive areas like healthcare and finance, privacy concerns often arise. One promising solution is to generate synthetic data with Differential Privacy (DP) guarantees to replace private data. However, these synthetic data contain significant flawed data, which are considered as noise. Existing solutions typically rely on naive filtering by comparing ROUGE-L scores or embedding similarities, which are ineffective in addressing the noise. To address this issue, we propose ***RewardDS***, a novel privacy-preserving framework that fine-tunes a reward proxy model and uses reward signals to guide the synthetic data generation. Our RewardDS introduces two key modules, Reward Guided Filtering and Self-Optimizing Refinement, to both filter and refine the synthetic data, effectively mitigating the noise. Extensive experiments across medical, financial, and code generation domains demonstrate the effectiveness of our method",
    "checked": true,
    "id": "5127dcbd827ae0e9d726f2faff45bb69289eaf04",
    "semantic_title": "rewardds: privacy-preserving fine-tuning for large language models via reward driven data synthesis",
    "citation_count": 0,
    "authors": [
      "Jianwei Wang",
      "Chengming Shi",
      "Junyao Yang",
      "Haoran Li",
      "Qianli Ma",
      "Huiping Zhuang",
      "Cen Chen",
      "Ziqian Zeng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.224": {
    "title": "Synergizing Multimodal Temporal Knowledge Graphs and Large Language Models for Social Relation Recognition",
    "volume": "main",
    "abstract": "Recent years have witnessed remarkable advances in Large Language Models (LLMs). However, in the task of social relation recognition, Large Language Models (LLMs) encounter significant challenges due to their reliance on sequential training data, which inherently restricts their capacity to effectively model complex graph-structured relationships. To address this limitation, we propose a novel low-coupling method synergizing multimodal temporal Knowledge Graphs and Large Language Models (mtKG-LLM) for social relation reasoning. Specifically, we extract multimodal information from the videos and model the social networks as spatial Knowledge Graphs (KGs) for each scene. Temporal KGs are constructed based on spatial KGs and updated along the timeline for long-term reasoning. Subsequently, we retrieve multi-scale information from the graph-structured knowledge for LLMs to recognize the underlying social relation. Extensive experiments demonstrate that our method has achieved state-of-the-art performance in social relation recognition. Furthermore, our framework exhibits effectiveness in bridging the gap between KGs and LLMs. Our code will be released after acceptance",
    "checked": true,
    "id": "edd3bdc9f2e95d5a7ed407c76ff2f6ecb0af6331",
    "semantic_title": "synergizing multimodal temporal knowledge graphs and large language models for social relation recognition",
    "citation_count": 0,
    "authors": [
      "Haorui Wang",
      "Zheng Wang",
      "Yuxuan Zhang",
      "Bo Wang",
      "Bin Wu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.225": {
    "title": "LegalSearchLM: Rethinking Legal Case Retrieval as Legal Elements Generation",
    "volume": "main",
    "abstract": "Legal Case Retrieval (LCR), which retrieves relevant cases from a query case, is a fundamental task for legal professionals in research and decision-making. However, existing studies on LCR face two major limitations. First, they are evaluated on relatively small-scale retrieval corpora (e.g., 100-55K cases) and use a narrow range of criminal query types, which cannot sufficiently reflect the complexity of real-world legal retrieval scenarios. Second, their reliance on embedding-based or lexical matching methods often results in limited representations and legally irrelevant matches. To address these issues, we present: (1) LEGAR BENCH, the first large-scale Korean LCR benchmark, covering 411 diverse crime types in queries over 1.2M candidate cases; and (2) LegalSearchLM, a retrieval model that performs legal element reasoning over the query case and directly generates content containing those elements, grounded in the target cases through constrained decoding. Experimental results show that LegalSearchLM outperforms baselines by 6 - 20% on LEGAR BENCH, achieving state-of-the-art performance. It also demonstrates strong generalization to out-of-domain cases, outperforming naive generative models trained on in-domain data by 15%",
    "checked": true,
    "id": "c2a723d3ed354df2a15c57e90dbb9fcc9c5c022a",
    "semantic_title": "legalsearchlm: rethinking legal case retrieval as legal elements generation",
    "citation_count": 0,
    "authors": [
      "Chaeeun Kim",
      "Jinu Lee",
      "Wonseok Hwang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.226": {
    "title": "ChartMind: A Comprehensive Benchmark for Complex Real-world Multimodal Chart Question Answering",
    "volume": "main",
    "abstract": "Chart question answering (CQA) has become a critical multimodal task for evaluating the reasoning capabilities of vision-language models. While early approaches have shown promising performance by focusing on visual features or leveraging large-scale pre-training, most existing evaluations rely on rigid output formats and objective metrics, thus ignoring the complex, real-world demands of practical chart analysis. In this paper, we introduce ChartMind, a new benchmark designed for complex CQA tasks in real-world settings. ChartMind covers seven task categories, incorporates multilingual contexts, supports open-domain textual outputs, and accommodates diverse chart formats, bridging the gap between real-world applications and traditional academic benchmarks. Furthermore, we propose a context-aware yet model-agnostic framework, ChartLLM, that focuses on extracting key contextual elements, reducing noise, and enhancing the reasoning accuracy of multimodal large language models. Extensive evaluations on ChartMind and three representative public benchmarks with 14 mainstream multimodal models show our framework significantly outperforms the previous three common CQA paradigms: instruction-following, OCR-enhanced, and chain-of-thought, highlighting the importance of flexible chart understanding for real-world CQA. These findings suggest new directions for developing more robust chart reasoning in future research",
    "checked": true,
    "id": "e57810b5723bdda2ae1736f378def53958ad7756",
    "semantic_title": "chartmind: a comprehensive benchmark for complex real-world multimodal chart question answering",
    "citation_count": 1,
    "authors": [
      "Jingxuan Wei",
      "Nan Xu",
      "Junnan Zhu",
      "Haoyanni",
      "Gaowei Wu",
      "Qi Chen",
      "Bihui Yu",
      "Lei Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.227": {
    "title": "COLA: Collaborative Multi-Agent Framework with Dynamic Task Scheduling for GUI Automation",
    "volume": "main",
    "abstract": "With the rapid advancements in Large Language Models (LLMs), an increasing number of studies have leveraged LLMs as the cognitive core of agents to address complex task decision-making challenges. Specially, recent research has demonstrated the potential of LLM-based agents on automating GUI operations. However, existing methodologies exhibit two critical challenges: (1) static agent architectures struggle to adapt to diverse GUI application scenarios, leading to inadequate scenario generalization; (2) the agent workflows lack fault tolerance mechanism, necessitating complete process re-execution for GUI agent decision error. To address these limitations, we introduce COLA, a collaborative multi-agent framework for automating GUI operations. In this framework, a scenario-aware agent Task Scheduler decomposes task requirements into atomic capability units, dynamically selects the optimal agent from a decision agent pool, effectively responds to the capability requirements of diverse scenarios. Furthermore, we develop an interactive backtracking mechanism that enables human to intervene to trigger state rollbacks for non-destructive process repair. Experiments on the GAIA dataset show that COLA achieves competitive performance among GUI Agent methods, with an average accuracy of 31.89%. On WindowsAgentArena, it performs particularly well in Web Browser (33.3%), Media & Video (33.3%), and Windows Utils (25.0%), suggesting the effectiveness of specialized agent design and dynamic strategy allocation. The code is available at https://github.com/Alokia/COLA-demo",
    "checked": true,
    "id": "5361d7b04b029dcd782961f91079d4f2053e5297",
    "semantic_title": "cola: collaborative multi-agent framework with dynamic task scheduling for gui automation",
    "citation_count": 0,
    "authors": [
      "Di Zhao",
      "Longhui Ma",
      "Siwei Wang",
      "Miao Wang",
      "Zhao Lv"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.228": {
    "title": "DASA-Trans-STM: Adaptive Efficient Transformer for Short Text Matching using Data Augmentation and Semantic Awareness",
    "volume": "main",
    "abstract": "Rencent advancements in large language models (LLM) have shown impressive versatility across various tasks. Short text matching is one of the fundamental technologies in natural language processing. In previous studies, the common approach to applying them to Chinese is segmenting each sentence into words, and then taking these words as input. However, existing approaches have three limitations: 1) Some Chinese words are polysemous, and semantic information is not fully utilized. 2) Some models suffer potential issues caused by word segmentation and incorrect recognition of negative words affects the semantic understanding of the whole sentence. 3) Fuzzy negation words in ancient Chinese are difficult to recognize and match. In this work, we propose a novel adaptive Transformer for Chinese short text matching using Data Augmentation and Semantic Awareness (DASA), which can fully mine the information expressed in Chinese text to deal with word ambiguity. DASA is based on a Graph Attention Transformer Encoder that takes two word lattice graphs as input and integrates sense information from N-HowNet to moderate word ambiguity. Specially, we use an LLM to generate similar sentences for the optimal text representation. Experimental results show that the augmentation done using DASA can considerably boost the performance of our system and achieve significantly better results than previous state-of-the-art methods on four available datasets, namely MNS, LCQMC, AFQMC, and BQ",
    "checked": true,
    "id": "8090c018dfe6305d5dc7fd8f1f9ef5f467dc4fad",
    "semantic_title": "dasa-trans-stm: adaptive efficient transformer for short text matching using data augmentation and semantic awareness",
    "citation_count": 0,
    "authors": [
      "Jiguo Liu",
      "Chao Liu",
      "Meimei Li",
      "Nan Li",
      "Shihao Gao",
      "Dali Zhu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.229": {
    "title": "Pruning the Paradox: How CLIP's Most Informative Heads Enhance Performance While Amplifying Bias",
    "volume": "main",
    "abstract": "CLIP is one of the most popular foundation models and is heavily used for many vision-language tasks, yet little is known about its inner workings. As CLIP is increasingly deployed in real-world applications, it is becoming even more critical to understand its limitations and embedded social biases to mitigate potentially harmful downstream consequences. However, the question of what internal mechanisms drive both the impressive capabilities as well as problematic shortcomings of CLIP has largely remained unanswered. To bridge this gap, we study the conceptual consistency of text descriptions for attention heads in CLIP-like models. Specifically, we propose Concept Consistency Score (CCS), a novel interpretability metric that measures how consistently individual attention heads in CLIP models align with specific concepts. Our soft-pruning experiments reveal that high CCS heads are critical for preserving model performance, as pruning them leads to a significantly larger performance drop than pruning random or low CCS heads. Notably, we find that high CCS heads capture essential concepts and play a key role in out-of-domain detection, concept-specific reasoning, and video-language understanding. Moreover, we prove that high CCS heads learn spurious correlations which amplify social biases. These results position CCS as a powerful interpretability metric exposing the paradox of performance and social biases in CLIP models",
    "checked": true,
    "id": "4b8b9f7fa5ecc48a9acafb560c2d6b79f4c9a45b",
    "semantic_title": "pruning the paradox: how clip's most informative heads enhance performance while amplifying bias",
    "citation_count": 0,
    "authors": [
      "Avinash Madasu",
      "Vasudev Lal",
      "Phillip Howard"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.230": {
    "title": "CoLA: Compute-Efficient Pre-Training of LLMs via Low-Rank Activation",
    "volume": "main",
    "abstract": "The full-size MLPs and the projection layers in attention introduce tremendous model sizes of large language models (LLMs), consuming extensive computational resources in pre-training. We empirically observe that the activations of pre-trained LLMs exhibit low-rank property. Motivated by such observations, we propose **CoLA** and its memory-efficient implementation, **CoLA-M**, to replace these full-size layers with compute-efficient **auto-encoders** that naturally enforce low-rank activations throughout training. This fundamental architectural change eliminates the activation redundancy and significantly boosts model capacity and training efficiency. Experiments on LLaMA models with 60 million to 7 billion parameters show that CoLA reduces the computing cost by 2\\pmb{\\times} and improves training throughput by 1.86\\pmb{\\times} while maintaining full-rank level performance. CoLA-M further squeezes memory cost without sacrificing throughput, offering a pre-training approach with collectively superior parameter, computing, and memory efficiency. The LLMs produced are also 2\\pmb{\\times} smaller, enabling faster inference with lower memory cost on resource-constrained platforms",
    "checked": true,
    "id": "11cd21a662a2c4e8af8f03f2b7cb2e135226a135",
    "semantic_title": "cola: compute-efficient pre-training of llms via low-rank activation",
    "citation_count": 9,
    "authors": [
      "Ziyue Liu",
      "Ruijie Zhang",
      "Zhengyang Wang",
      "Mingsong Yan",
      "Zi Yang",
      "Paul D. Hovland",
      "Bogdan Nicolae",
      "Franck Cappello",
      "Sui Tang",
      "Zheng Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.231": {
    "title": "TS-CLIP: Time Series Understanding by CLIP",
    "volume": "main",
    "abstract": "Contrastive Language–Image Pre-training (CLIP) has recently demonstrated remarkable success in aligning vision and language. Aligning time series with text leverages the rich semantic cues of language to enhance interpretability and generalization, addressing a largely underexplored area of research. Although applying the CLIP training paradigm to time-series and language pairs is promising, it may result in label collapse due to the sparse semantic annotations and the absence of visual cues in time-series data. To address this, we introduce Time Series CLIP (TS-CLIP), a novel approach that tackles label collapse using a synonym bank mechanism. Synonym bank exploits word analogy phenomena to generate potential synonym embeddings as alignment targets. Specifically, the synonym bank facilitates aligning time series with a word distribution instead of a precise textual description. We conducted extensive zero-shot and few-shot experiments on 128 sub-datasets from the UCR archive. The results show that TS-CLIP achieves state-of-the-art (SOTA) performance in zero-shot settings on 51 datasets. Comprehensive ablation studies and visualization analyzes reveal that TS-CLIP effectively aligns time series with natural language. To the best of our knowledge, this is the first foundational model to achieve general time series and natural language alignment. TS-CLIP introduces a new paradigm for the semantic understanding of time series and opens the possibility of integrating the time series modality into multimodal large models",
    "checked": true,
    "id": "251335071d066261d3e9f48d72d335443899c09e",
    "semantic_title": "ts-clip: time series understanding by clip",
    "citation_count": 0,
    "authors": [
      "Ziwen Chen",
      "Xiaoyuan Zhang",
      "Ming Zhu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.232": {
    "title": "MultiAgentESC: A LLM-based Multi-Agent Collaboration Framework for Emotional Support Conversation",
    "volume": "main",
    "abstract": "The development of Emotional Support Conversation (ESC) systems is critical for delivering mental health support tailored to the needs of help-seekers. Recent advances in large language models (LLMs) have contributed to progress in this domain, while most existing studies focus on generating responses directly and overlook the integration of domain-specific reasoning and expert interaction.Therefore, in this paper, we propose a training-free Multi-Agent collaboration framework for ESC (MultiAgentESC).The framework is designed to emulate the human-like process of providing emotional support through three stages: dialogue analysis, strategy deliberation, and response generation.At each stage, a multi-agent system is employed to iteratively enhance information understanding and reasoning, simulating real-world decision-making processes by incorporating diverse interactions among these expert agents.Additionally, we introduce a novel response-centered approach to handle the one-to-many problem on strategy selection, where multiple valid strategies are initially employed to generate diverse responses, followed by the selection of the optimal response through multi-agent collaboration.Experiments on the ESConv dataset reveal that our proposed framework excels at providing emotional support as well as diversifying support strategy selection",
    "checked": true,
    "id": "daa72e32877bb9c680b37cf8590785cdda93ca41",
    "semantic_title": "multiagentesc: a llm-based multi-agent collaboration framework for emotional support conversation",
    "citation_count": 0,
    "authors": [
      "Yangyang Xu",
      "Jinpeng Hu",
      "Zhuoer Zhao",
      "Zhangling Duan",
      "Xiao Sun",
      "Xun Yang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.233": {
    "title": "Continuously Steering LLMs Sensitivity to Contextual Knowledge with Proxy Models",
    "volume": "main",
    "abstract": "In Large Language Models (LLMs) generation, there exist knowledge conflicts, and scenarios where parametric knowledge contradicts knowledge provided in the context. Previous works studied tuning, decoding algorithms, or locating and editing context-aware neurons to adapt LLMs to be faithful to new contextual knowledge. However, they are usually inefficient or ineffective for large models, not workable for black-box models, or unable to continuously adjust LLMs' sensitivity to the knowledge provided in the context. To mitigate these problems, we propose CSKS (Continuously Steering Knowledge Sensitivity), a simple framework that can steer LLMs' sensitivity to contextual knowledge continuously at a lightweight cost. Specifically, we tune two small LMs (i.e. proxy models) and use the difference in their output distributions to shift the original distribution of an LLM without modifying the LLM weights. In the evaluation process, we not only design synthetic data and fine-grained metrics to measure models' sensitivity to contextual knowledge but also use a real conflict dataset to validate CSKS' practical efficacy. Extensive experiments demonstrate that our framework achieves continuous and precise control over LLMs' sensitivity to contextual knowledge, enabling both increased sensitivity and reduced sensitivity, thereby allowing LLMs to prioritize either contextual or parametric knowledge as needed flexibly. Our data and code are available at https://github.com/OliveJuiceLin/CSKS",
    "checked": true,
    "id": "db7ca48c47efcdc86e137d81fa0c4af889d8da2d",
    "semantic_title": "continuously steering llms sensitivity to contextual knowledge with proxy models",
    "citation_count": 1,
    "authors": [
      "Yilin Wang",
      "Heng Wang",
      "Yuyang Bai",
      "Minnan Luo"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.234": {
    "title": "Probing LLM World Models: Enhancing Guesstimation with Wisdom of Crowds Decoding",
    "volume": "main",
    "abstract": "Guesstimation—the task of making approximate quantitative estimates about objects or events—is a common real-world skill, yet remains underexplored in large language model (LLM) research. We introduce three guesstimation datasets: MARBLES, FUTURE, and ELECPRED, spanning physical estimation (e.g., how many marbles fit in a cup) to abstract predictions (e.g., the 2024 U.S. presidential election). Inspired by the social science concept of Wisdom of Crowds (WOC)—where the median of multiple estimates improves accuracy—we propose WOC decoding for LLMs. We replicate WOC effects in human participants and find that LLMs exhibit similar benefits: median aggregation across sampled responses consistently improves accuracy over greedy, self-consistency decoding, and mean decoding. This suggests that LLMs encode a world model that supports approximate reasoning. Our results position guesstimation as a useful probe of LLM world knowledge and highlight WOC decoding as a strategy for enhancing LLM guesstimation performance on real-world tasks",
    "checked": true,
    "id": "9d3042ca37e41b28a180b30380517eba95e312c1",
    "semantic_title": "probing llm world models: enhancing guesstimation with wisdom of crowds decoding",
    "citation_count": 1,
    "authors": [
      "Yun-Shiuan Chuang",
      "Sameer Narendran",
      "Nikunj Harlalka",
      "Alexander Cheung",
      "Sizhe Gao",
      "Siddharth Suresh",
      "Junjie Hu",
      "Timothy T. Rogers"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.235": {
    "title": "Recall with Reasoning: Chain-of-Thought Distillation for Mamba's Long-Context Memory and Extrapolation",
    "volume": "main",
    "abstract": "Mamba's theoretical infinite-context potential is limited in practice when sequences far exceed training lengths. This work explores unlocking Mamba's long-context memory ability by a simple-yet-effective method, Recall with Reasoning (RwR), by distilling chain-of-thought (CoT) summarization from a teacher model. Specifically, RwR prepends these summarization as CoT prompts during fine-tuning, teaching Mamba to actively recall and reason over long contexts. Experiments on LONGMEMEVAL and HELMET show that RwR outperforms existing long-term memory methods on the Mamba model. Furthermore, under similar pre-training conditions, RwR improves the long-context performance of Mamba relative to comparable Transformer/hybrid baselines while preserving short-context capabilities, all without changing the architecture",
    "checked": true,
    "id": "969c5de1cbd70311371c49a057d9677f71b9605c",
    "semantic_title": "recall with reasoning: chain-of-thought distillation for mamba's long-context memory and extrapolation",
    "citation_count": 2,
    "authors": [
      "Jun-Yu Ma",
      "Tianqing Fang",
      "Zhisong Zhang",
      "Hongming Zhang",
      "Haitao Mi",
      "Dong Yu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.236": {
    "title": "Scalable Data Synthesis through Human-like Cognitive Imitation and Data Recombination",
    "volume": "main",
    "abstract": "Large language models (LLMs) rely on massive amounts of training data, however, the quantity of empirically observed data is limited. To alleviate this issue, lots of LLMs leverage synthetic data to enhance the quantity of training data. Despite significant advancements in LLMs, the efficiency and scalability characteristics of data synthesis during pre-training phases remain insufficiently explored. In this work, we propose a novel data synthesis framework, Cognitive Combination Synthesis (CCS), designed to achieve highly efficient and scalable data synthesis. Specifically, our methodology mimics human cognitive behaviors by recombining and interconnecting heterogeneous data from diverse sources thereby enhancing advanced reasoning capabilities in LLMs. Extensive experiments demonstrate that: (1) effective data organization is essential, and our mapping-based combination learning approach significantly improves data utilization efficiency; (2) by enhancing data diversity, accuracy, and complexity, our synthetic data scales beyond 100B tokens, revealing CCS's strong scalability. Our findings highlight the impact of data organization methods on LLM learning efficiency and the significant potential of scalable synthetic data to enhance model reasoning capabilities",
    "checked": true,
    "id": "90062dcf66770eba4243db9a0e76d42402a92ade",
    "semantic_title": "scalable data synthesis through human-like cognitive imitation and data recombination",
    "citation_count": 0,
    "authors": [
      "Zhongyi Ye",
      "Weitai Zhang",
      "Xinyuan Zhou",
      "Yongxin Zhu",
      "Ninghui Rao",
      "Enhong Chen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.237": {
    "title": "BeSimulator: A Large Language Model Powered Text-based Behavior Simulator",
    "volume": "main",
    "abstract": "Traditional robot simulators focus on physical process modeling and realistic rendering, often suffering from high computational costs, inefficiencies, and limited adaptability. To handle this issue, we concentrate on behavior simulation in robotics to analyze and validate the logic behind robot behaviors, aiming to achieve preliminary evaluation before deploying resource-intensive simulators and thus enhance simulation efficiency. In this paper, we propose BeSimulator, a modular and novel LLM-powered framework, as an attempt towards behavior simulation in the context of text-based environments. By constructing text-based virtual environments and performing semantic-level simulation, BeSimulator can generalize across scenarios and achieve long-horizon complex simulation. Inspired by human cognition paradigm, it employs a \"consider-decide-capture-transfer\" four-phase simulation process, termed Chain of Behavior Simulation (CBS), which excels at analyzing action feasibility and state transition. Additionally, BeSimulator incorporates code-driven reasoning to enable arithmetic operations and enhance reliability, and reflective feedback to refine simulation. Based on our manually constructed behavior-tree-based simulation benchmark, BTSIMBENCH, our experiments show a significant performance improvement in behavior simulation compared to baselines, ranging from 13.60% to 24.80%. Code and data are available at https://github.com/Dawn888888/BeSimulator",
    "checked": true,
    "id": "25987143362179d8b05ad6cdcdcafc6f426caa78",
    "semantic_title": "besimulator: a large language model powered text-based behavior simulator",
    "citation_count": 1,
    "authors": [
      "Jianan Wang",
      "Bin Li",
      "Jingtao Qi",
      "Xueying Wang",
      "Fu Li",
      "Lihanxun"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.238": {
    "title": "Too Consistent to Detect: A Study of Self-Consistent Errors in LLMs",
    "volume": "main",
    "abstract": "As large language models (LLMs) often generate plausible but incorrect content, error detection has become increasingly critical to ensure truthfulness.However, existing detection methods often overlook a critical problem we term as **self-consistent error**, where LLMs repeatedly generate the same incorrect response across multiple stochastic samples.This work formally defines self-consistent errors and evaluates mainstream detection methods on them.Our investigation reveals two key findings: (1) Unlike inconsistent errors, whose frequency diminishes significantly as the LLM scale increases, the frequency of self-consistent errors remains stable or even increases.(2) All four types of detection methods significantly struggle to detect self-consistent errors.These findings reveal critical limitations in current detection methods and underscore the need for improvement.Motivated by the observation that self-consistent errors often differ across LLMs, we propose a simple but effective cross‐model probe method that fuses hidden state evidence from an external verifier LLM.Our method significantly enhances performance on self-consistent errors across three LLM families",
    "checked": true,
    "id": "af8d055efc34c751ca6ffe165eb63c6b9045f446",
    "semantic_title": "too consistent to detect: a study of self-consistent errors in llms",
    "citation_count": 1,
    "authors": [
      "Hexiang Tan",
      "Fei Sun",
      "Sha Liu",
      "Du Su",
      "Qi Cao",
      "Xin Chen",
      "Jingang Wang",
      "Xunliang Cai",
      "Yuanzhuo Wang",
      "Huawei Shen",
      "Xueqi Cheng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.239": {
    "title": "pFedGPT: Hierarchically Optimizing LoRA Aggregation Weights for Personalized Federated GPT Models",
    "volume": "main",
    "abstract": "Federated finetuning of Large Language Models (LLMs) using Low-Rank Adaptation (LoRA) offers computational efficiency and preserves data privacy. However, applying LoRA in federated settings faces significant challenges: standard approaches struggle with data heterogeneity, and existing personalization techniques fail to precisely adapt shared global knowledge to individual client needs. To address these issues, we propose pFedGPT, a framework that leverages Hierarchical Bayesian Optimization (HBO) for fine-grained, personalized LoRA aggregation. pFedGPT intelligently partitions LoRA parameters based on model structure and client information, then employs HBO to hierarchically search for optimal, module-specific weights. This enables a nuanced integration of the downloaded global LoRA state with each client's local model, precisely capturing client-specific requirements. To manage the optimization cost inherent in HBO, pFedGPT incorporates efficient multi-fidelity evaluations and a curriculum learning strategy. Extensive experiments demonstrate that pFedGPT achieves state-of-the-art (SOTA) performance on personalized FL benchmarks, showcasing robustness and scalability while introducing only minimal (approx. 4%) additional optimization overhead. Our results also underscore the limitations of traditional FL methods for LoRA-based LLM personalization, highlighting the need for tailored approaches like pFedGPT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhanming Shen",
      "Tianqi Xu",
      "Hao Wang",
      "Jian Li",
      "Miao Pan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.240": {
    "title": "QSpec: Speculative Decoding with Complementary Quantization Schemes",
    "volume": "main",
    "abstract": "Quantization is widely adopted to accelerate inference and reduce memory consumption in large language models (LLMs). While activation-weight joint quantization enables efficient low-precision decoding, it suffers substantial performance degradation on multi-step reasoning tasks. We propose QSPEC, a novel quantization paradigm that decouples efficiency from quality by integrating two complementary schemes via speculative decoding: low-precision joint quantization for fast drafting and high-precision weight-only quantization for accurate verification. QSPEC reuses both weights and KV cache across stages, enabling near-zero-cost switching without retraining or auxiliary models. Compared to high-precision baselines, QSPEC achieves up to 1.64x speedup without quality degradation, and outperforms state-of-the-art speculative decoding methods by up to 1.55x in batched settings. Furthermore, QSPEC supports plug-and-play deployment and generalizes well across model scales, quantization methods, and workloads. These properties make QSPEC a practical and scalable solution for high-fidelity quantized LLM serving under memory-constrained scenarios",
    "checked": true,
    "id": "139aea58a13e615a7e466fe0c74273804372b746",
    "semantic_title": "qspec: speculative decoding with complementary quantization schemes",
    "citation_count": 11,
    "authors": [
      "Juntao Zhao",
      "Wenhao Lu",
      "Sheng Wang",
      "Lingpeng Kong",
      "Chuan Wu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.241": {
    "title": "Co-Evolving LLMs and Embedding Models via Density-Guided Preference Optimization for Text Clustering",
    "volume": "main",
    "abstract": "Large language models (LLMs) have shown strong potential in enhancing text clustering when combined with traditional embedding models. However, existing methods predominantly treat LLMs as static pseudo-oracles, i.e., unidirectionally querying them for similarity assessment or data augmentation, while never seeking feedback from embedding models to improve them. In this work, we propose a training framework that enables bidirectional refinement between LLMs and embedding models. We first design task-aware prompts to guide the LLM in generating interpretations for the input texts. These interpretations are projected into the embedding space, in which interpretations that are preferred by the embedding model are selected based on their distribution densities. The selected interpretations are then used to fine-tune the LLM via preference optimization to prioritize the generation of helpful interpretations. Meanwhile, we enhance the embedding model via contrastive learning on the generated interpretations and perform clustering on the output embeddings, leading to iterative co-training between the LLM and the embedding model. Experiments on 14 benchmark datasets across 5 tasks demonstrate the effectiveness of our method",
    "checked": true,
    "id": "089b45d9d2ae124f8107ed3f1e8cc87b4d81d84d",
    "semantic_title": "co-evolving llms and embedding models via density-guided preference optimization for text clustering",
    "citation_count": 0,
    "authors": [
      "Zetong Li",
      "Qinliang Su",
      "Minhua Huang",
      "Yin Yang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.242": {
    "title": "P-MMEval: A Parallel Multilingual Multitask Benchmark for Consistent Evaluation of LLMs",
    "volume": "main",
    "abstract": "Recent advancements in large language models (LLMs) showcase varied multilingual capabilities across tasks like translation, code generation, and reasoning. Previous assessments often limited their scope to fundamental natural language processing (NLP) or isolated capability-specific tasks. To alleviate this drawback, we aim to present a comprehensive multilingual multitask benchmark. First, we introduce P-MMEval, a large-scale benchmark covering fundamental and capability-specialized datasets. Furthermore, P-MMEval delivers consistent language coverage across various datasets and provides parallel samples. Finally, we conduct extensive experiments on representative multilingual model series to compare performances across models and tasks, explore the relationship between multilingual performances and factors such as tasks, model sizes, languages, and prompts, and examine the effectiveness of knowledge transfer from English to other languages. The resulting insights are intended to offer valuable guidance for future research",
    "checked": true,
    "id": "419d8b07c01b2bf30f4f92513db86251d2a8734f",
    "semantic_title": "p-mmeval: a parallel multilingual multitask benchmark for consistent evaluation of llms",
    "citation_count": 17,
    "authors": [
      "Yidan Zhang",
      "Yu Wan",
      "Boyi Deng",
      "Baosong Yang",
      "Hao-Ran Wei",
      "Fei Huang",
      "Bowen Yu",
      "Dayiheng Liu",
      "Junyang Lin",
      "Fei Huang",
      "Jingren Zhou"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.243": {
    "title": "Single LLM, Multiple Roles: A Unified Retrieval-Augmented Generation Framework Using Role-Specific Token Optimization",
    "volume": "main",
    "abstract": "Existing studies have optimized retrieval-augmented generation (RAG) across various sub-tasks, such as query understanding and retrieval refinement, but integrating these optimizations into a unified framework remains challenging. To tackle this problem, this work proposes RoleRAG, a unified RAG framework that achieves efficient multi-task processing through role-specific token optimization. RoleRAG comprises six modules, each handling a specific sub-task within the RAG process. Additionally, we introduce a query graph to represent the decomposition of the query, which can be dynamically resolved according to the decomposing state. All modules are driven by the same underlying LLM, distinguished by task-specific role tokens that are individually optimized. This design allows RoleRAG to dynamically activate different modules within a single LLM instance, thereby streamlining deployment and reducing resource consumption. Experimental results on five open-domain question-answering datasets demonstrate the effectiveness, generalizability, and flexibility of our framework",
    "checked": true,
    "id": "7f728345561ea4a7631527f97da19f6bbf9d5ed5",
    "semantic_title": "single llm, multiple roles: a unified retrieval-augmented generation framework using role-specific token optimization",
    "citation_count": 2,
    "authors": [
      "Yutao Zhu",
      "Jiajie Jin",
      "Hongjin Qian",
      "Zheng Liu",
      "Zhicheng Dou",
      "Ji-Rong Wen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.244": {
    "title": "TrInk: Ink Generation with Transformer Network",
    "volume": "main",
    "abstract": "In this paper, we propose TrInk, a Transformer-based model for ink generation, which effectively captures global dependencies. To better facilitate the alignment between the input text and generated stroke points, we introduce scaled positional embeddings and a Gaussian memory mask in the cross-attention module. Additionally, we design both subjective and objective evaluation pipelines to comprehensively assess the legibility and style consistency of the generated handwriting. Experiments demonstrate that our Transformer-based model achieves a 35.56% reduction in character error rate (CER) and an 29.66% reduction in word error rate (WER) on the IAM-OnDB dataset compared to previous methods. We provide an demo page with handwriting samples from TrInk and baseline models at: https://akahello-a11y.github.io/trink-demo/",
    "checked": true,
    "id": "ed4a7883b502a8fd06d5d023062eea8550e74bc5",
    "semantic_title": "trink: ink generation with transformer network",
    "citation_count": 1,
    "authors": [
      "Zezhong Jin",
      "Shubhang Desai",
      "Xu Chen",
      "Biyi Fang",
      "Zhuoyi Huang",
      "Zhe Li",
      "Chong-Xin Gan",
      "Xiao Tu",
      "Man-Wai Mak",
      "Yan Lu",
      "Shujie Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.245": {
    "title": "CalligraphicOCR for Chinese Calligraphy Recognition",
    "volume": "main",
    "abstract": "With thousand years of history, calligraphy serve as one of the representative symbols of Chinese culture. Increasing works try to digitize calligraphy by recognizing the context of calligraphy for better preservation and propagation. However, previous works stick to isolated single character recognition, not only requires unpractical manual splitting into characters, but also abandon the enriched context information that could be supplementary. To this end, we construct the pioneering end-to-end calligraphy recognition benchmark dataset, this dataset is challenging due to both the visual variations such as different writing styles and the textual understanding such as the domain shift in semantics. We further propose CalligraphicOCR (COCR) equipped with calligraphic image augmentation and action-based corrector targeted at the challenging root of this setting. Experiments demonstrate the advantage of our proposed model over cutting-edge baselines, underscoring the necessity of introducing this new setting, thereby facilitating a solid precondition for protecting and propagating the already scarce resources",
    "checked": true,
    "id": "dd2fed8d82aa152d18040091d01c5c085ea13704",
    "semantic_title": "calligraphicocr for chinese calligraphy recognition",
    "citation_count": 0,
    "authors": [
      "Xiaoyi Bao",
      "Zhongqing Wang",
      "Jinghang Gu",
      "Chu-Ren Huang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.246": {
    "title": "When Audio and Text Disagree: Revealing Text Bias in Large Audio-Language Models",
    "volume": "main",
    "abstract": "Large Audio-Language Models (LALMs) are augmented with the ability to perceive audio, demonstrating impressive capabilities in processing combined audio and text signals. However, their reliability when faced with conflicting inputs across modalities remains largely unexplored. This study examines how LALMs prioritize information when presented with inconsistent audio-text pairs. Through extensive evaluation across diverse audio understanding tasks, we reveal a concerning phenomenon: when inconsistencies exist between modalities, LALMs display a significant bias toward textual input, often disregarding audio evidence. This tendency leads to substantial performance degradation in audio-centric tasks and raises important reliability concerns for real-world applications. We further investigate the influencing factors of text bias, explore mitigation strategies through supervised fine-tuning, and analyze model confidence patterns that reveal persistent overconfidence even with contradictory inputs. These findings underscore the need for improved modality balancing during training and more sophisticated fusion mechanisms to enhance robustness when handling conflicting multi-modal inputs. The project is available at https://github.com/WangCheng0116/MCR-BENCH",
    "checked": true,
    "id": "698954f679cbaa08cfb71640c2c590bb20f6f27a",
    "semantic_title": "when audio and text disagree: revealing text bias in large audio-language models",
    "citation_count": 2,
    "authors": [
      "Cheng Wang",
      "Gelei Deng",
      "Xianglin Yang",
      "Han Qiu",
      "Tianwei Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.247": {
    "title": "RESF: Regularized-Entropy-Sensitive Fingerprinting for Black-Box Tamper Detection of Large Language Models",
    "volume": "main",
    "abstract": "The proliferation of Machine Learning as a Service (MLaaS) has enabled widespread deployment of large language models (LLMs) via cloud APIs, but also raises critical concerns about model integrity and security. Existing black-box tamper detection methods, such as watermarking and fingerprinting, rely on the stability of model outputs—a property that does not hold for inherently stochastic LLMs. We address this challenge by formulating black-box tamper detection for LLMs as a hypothesis-testing problem. To enable efficient and sensitive fingerprinting, we derive a first-order surrogate for KL divergence—the entropy-gradient norm—to identify prompts most responsive to parameter perturbations. Building on this, we propose Regularized Entropy-Sensitive Fingerprinting (RESF), which enhances sensitivity while regularizing entropy to improve output stability and control false positives. To further distinguish tampering from benign randomness, such as temperature shifts, RESF employs a lightweight two-tier sequential test combining support-based and distributional checks with rigorous false-alarm control.Comprehensive analysis and experiments across multiple LLMs show that RESF achieves up to 98.80% detection accuracy under challenging conditions, such as minimal LoRA fine-tuning with five optimized fingerprints. RESF consistently demonstrates strong sensitivity and robustness, providing an effective and scalable solution for black-box tamper detection in cloud-deployed LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pingyi Hu",
      "Xiaofan Bai",
      "Xiaojing Ma",
      "Chaoxiang He",
      "Dongmei Zhang",
      "Bin Benjamin Zhu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.248": {
    "title": "Model-based Large Language Model Customization as Service",
    "volume": "main",
    "abstract": "Prominent Large Language Model (LLM) services from providers like OpenAI and Google excel at general tasks but often underperform on domain-specific applications. Current customization services for these LLMs typically require users to upload data for fine-tuning, posing significant privacy risks. While differentially private (DP) data synthesis presents a potential alternative, its application commonly results in low effectiveness due to the introduction of excessive noise on data for DP. To overcome this, we introduce *Llamdex*, a novel framework that facilitates LLM customization as a service, where the client uploads pre-trained domain-specific *models* rather than data. This client-uploaded model, optionally protected by DP with much lower noise, is inserted into the base LLM via connection modules. Significantly, these connecting modules are trained without requiring sensitive domain data, enabling clients to customize LLM services while preserving data privacy. Experiments demonstrate that Llamdex improves domain-specific accuracy by up to 26% over state-of-the-art private data synthesis methods under identical privacy constraints and, by obviating the need for users to provide domain context within queries, maintains inference efficiency comparable to the original LLM service",
    "checked": true,
    "id": "457747496208a93151d473ca58f0027c6ede37f3",
    "semantic_title": "model-based large language model customization as service",
    "citation_count": 0,
    "authors": [
      "Zhaomin Wu",
      "Jizhou Guo",
      "Junyi Hou",
      "Bingsheng He",
      "Lixin Fan",
      "Qiang Yang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.249": {
    "title": "Collab-Overcooked: Benchmarking and Evaluating Large Language Models as Collaborative Agents",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) based agent systems have made great strides in real-world applications beyond traditional NLP tasks. This paper proposes a new LLM-based Multi-Agent System (LLM-MAS) benchmark, Collab-Overcooked, built on the popular Overcooked-AI game with more applicable and challenging tasks in interactive environments. Collab-Overcooked extends existing benchmarks in two novel ways. First, it provides a multi-agent framework supporting diverse tasks and objectives and encourages collaboration through natural language communication. Second, it introduces a spectrum of process-oriented evaluation metrics to assess the fine-grained collaboration capabilities of different LLM agents, a dimension often overlooked in prior work. We conduct extensive experiments with 13 popular LLMs and show that, while the LLMs exhibit a strong ability in goal interpretation, there are significant shortcomings in active collaboration and continuous adaptation, which are critical for efficiently fulfilling complex tasks. Notably, we highlight the strengths and weaknesses of LLM-MAS and provide insights for improving and evaluating LLM-MAS on a unified and open-source benchmark. The environments, 30 open-ended tasks, and the evaluation package are publicly available at https://github.com/YusaeMeow/Collab-Overcooked",
    "checked": true,
    "id": "47525c80b47569add94a07573428685103e2a961",
    "semantic_title": "collab-overcooked: benchmarking and evaluating large language models as collaborative agents",
    "citation_count": 4,
    "authors": [
      "Haochen Sun",
      "Shuwen Zhang",
      "Lujie Niu",
      "Lei Ren",
      "Hao Xu",
      "Hao Fu",
      "Fangkun Zhao",
      "Caixia Yuan",
      "Xiaojie Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.250": {
    "title": "Improving Reasoning Capabilities in Small Models through Mixture-of-layers Distillation with Stepwise Attention on Key Information",
    "volume": "main",
    "abstract": "The significant computational demands of large language models have increased interest in distilling reasoning abilities into smaller models via Chain-of-Thought (CoT) distillation. Current CoT distillation methods mainly focus on transferring teacher-generated rationales for complex reasoning to student models. However, they do not adequately explore teachers' dynamic attention toward critical information during reasoning. We find that language models exhibit progressive attention shifts towards key information during reasoning, which implies essential clues for drawing conclusions. Building on this observation and analysis, we introduce a novel CoT distillation framework that transfers the teacher's stepwise attention on key information to the student model. This establishes structured guidance for the student's progressive concentration on key information during reasoning. More importantly, we develop a Mixture of Layers module enabling dynamic alignment that adapts to different layers between the teacher and student. Our method achieves consistent performance improvements across multiple mathematical and commonsense reasoning datasets. To our knowledge, it is the first method to leverage stepwise attention within CoT distillation to improve small model reasoning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yao Chen",
      "Jiawei Sheng",
      "Wenyuan Zhang",
      "Tingwen Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.251": {
    "title": "Through the Valley: Path to Effective Long CoT Training for Small Language Models",
    "volume": "main",
    "abstract": "Long chain-of-thought (CoT) supervision has become a common strategy to enhance reasoning in language models. While effective for large models, we identify a phenomenon we call Long CoT Degradation, in which small language models (SLMs; ≤3B parameters) trained on limited long CoT data experience significant performance deterioration. Through extensive experiments on the Qwen2.5, LLaMA3 and Gemma3 families, we demonstrate that this degradation is widespread across SLMs. In some settings, models trained on only 8k long CoT examples lose up to 75% of their original performance before fine-tuning. Strikingly, we further observe that for some particularly small models, even training on 220k long CoT examples fails to recover or surpass their original performance prior to fine-tuning. Our analysis attributes this effect to error accumulation: while longer responses increase the capacity for multi-step reasoning, they also amplify the risk of compounding mistakes. Furthermore, we find that Long CoT Degradation may negatively impacts downstream reinforcement learning (RL), although this can be alleviated by sufficiently scaled supervised fine-tuning (SFT). Our findings challenge common assumptions about the benefits of long CoT training for SLMs and offer practical guidance for building more effective small-scale reasoning models",
    "checked": true,
    "id": "80020c414b4454468fe097d54d9b26f73cc20913",
    "semantic_title": "through the valley: path to effective long cot training for small language models",
    "citation_count": 2,
    "authors": [
      "Renjie Luo",
      "Jiaxi Li",
      "Chen Huang",
      "Wei Lu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.252": {
    "title": "RED: Unleashing Token-Level Rewards from Holistic Feedback via Reward Redistribution",
    "volume": "main",
    "abstract": "Reinforcement learning from human feedback (RLHF) offers a promising approach to aligning large language models (LLMs) with human preferences. Typically, a reward model is trained or supplied to act as a proxy for humans in evaluating generated responses during the reinforcement training phase. However, current reward models operate as sequence-to-one models, allocating a single, sparse, and delayed reward to an entire output sequence. This approach may overlook the significant contributions of individual tokens toward the desired outcome. To this end, we propose a more fine-grained, token-level guidance approach for RL training. Specifically, we introduce RED, a novel REward reDistribition method that evaluates and assigns specific credit to each token using an off-the-shelf reward model. Utilizing these fine-grained rewards enhances the model's understanding of language nuances, leading to more precise performance improvements. Notably, our method does not require modifying the reward model or introducing additional training steps, thereby incurring minimal computational costs. Experimental results across diverse datasets and tasks demonstrate the superiority of our approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahui Li",
      "Lin Li",
      "Tai-Wei Chang",
      "Kun Kuang",
      "Long Chen",
      "Jun Zhou",
      "Cheng Yang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.253": {
    "title": "SDGO: Self-Discrimination-Guided Optimization for Consistent Safety in Large Language Models",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) excel at various natural language processing tasks but remain vulnerable to jailbreaking attacks that induce harmful content generation. In this paper, we reveal a critical safety inconsistency: LLMs can more effectively identify harmful requests as discriminators than defend against them as generators. This insight inspires us to explore aligning the model's inherent discrimination and generation capabilities. To this end, we propose SDGO (Self-Discrimination-Guided Optimization), a reinforcement learning framework that leverages the model's own discrimination capabilities as a reward signal to enhance generation safety through iterative self-improvement. Our method does not require any additional annotated data or external models during the training phase. Extensive experiments demonstrate that SDGO significantly improves model safety compared to both prompt-based and training-based baselines while maintaining helpfulness on general benchmarks. By aligning LLMs' discrimination and generation capabilities, SDGO brings robust performance against out-of-distribution (OOD) jailbreaking attacks. This alignment achieves tighter coupling between these two capabilities, enabling the model's generation capability to be further enhanced with only a small amount of discriminative samples. Our code and datasets are available at https://github.com/NJUNLP/SDGO",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peng Ding",
      "Wen Sun",
      "Dailin Li",
      "Wei Zou",
      "Jiaming Wang",
      "Jiajun Chen",
      "Shujian Huang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.254": {
    "title": "InMind: Evaluating LLMs in Capturing and Applying Individual Human Reasoning Styles",
    "volume": "main",
    "abstract": "LLMs have shown strong performance on human-centric reasoning tasks. While previous evaluations have explored whether LLMs can infer intentions or detect deception, they often overlook the individualized reasoning styles that influence how people interpret and act in social contexts. Social deduction games (SDGs) provide a natural testbed for evaluating individualized reasoning styles, where different players may adopt diverse but contextually valid reasoning strategies under identical conditions. To address this, we introduce InMind, a cognitively grounded evaluation framework designed to assess whether LLMs can capture and apply personalized reasoning styles in SDGs. InMind enhances structured gameplay data with round-level strategy traces and post-game reflections, collected under both Observer and Participant modes. It supports four cognitively motivated tasks that jointly evaluate both static alignment and dynamic adaptation. As a case study, we apply InMind to the game Avalon, evaluating 11 state-of-the-art LLMs. General-purpose LLMs, even GPT-4o frequently rely on lexical cues, struggling to anchor reflections in temporal gameplay or adapt to evolving strategies. In contrast, reasoning-enhanced LLMs like DeepSeek-R1 exhibit early signs of style-sensitive reasoning. These findings reveal key limitations in current LLMs' capacity for individualized, adaptive reasoning, and position InMind as a step toward cognitively aligned human–AI interaction",
    "checked": true,
    "id": "277e48615711adbd316cb9bcbcc7a0f52dbadf0a",
    "semantic_title": "inmind: evaluating llms in capturing and applying individual human reasoning styles",
    "citation_count": 0,
    "authors": [
      "Zizhen Li",
      "Chuanhao Li",
      "Yibin Wang",
      "Qi Chen",
      "Diping Song",
      "Yukang Feng",
      "Jianwen Sun",
      "Jiaxin Ai",
      "Fanrui Zhang",
      "Mingzhu Sun",
      "Kaipeng Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.255": {
    "title": "MIO: A Foundation Model on Multimodal Tokens",
    "volume": "main",
    "abstract": "In this paper, we introduce MIO, a novel foundation model built on multimodal tokens, capable of understanding and generating speech, text, images, and videos in an end-to-end, autoregressive manner. While the emergence of large language models (LLMs) and multimodal large language models (MM-LLMs) propels advancements in artificial general intelligence through their versatile capabilities, they still lack true any-to-any understanding and generation. Recently, the release of GPT-4o has showcased the remarkable potential of any-to-any LLMs for complex real-world tasks, enabling omnidirectional input and output across images, speech, and text. However, it is closed-source and does not support the generation of multimodal interleaved sequences. To address this gap, we present MIO, which is trained on a mixture of discrete tokens across four modalities using causal multimodal modeling. MIO undergoes a four-stage training process: (1) alignment pre-training, (2) interleaved pre-training, (3) speech-enhanced pre-training, and (4) comprehensive supervised fine-tuning on diverse textual, visual, and speech tasks. Our experimental results indicate that MIO exhibits competitive, and in some cases superior, performance compared to previous dual-modal baselines, any-to-any model baselines, and even modality-specific baselines. Moreover, MIO demonstrates advanced capabilities inherent to its any-to-any feature, such as interleaved video-text generation, chain-of-visual-thought reasoning, visual guideline generation, instructional image editing, etc",
    "checked": true,
    "id": "b42728048d5954aead90a53d452ac242b824e915",
    "semantic_title": "mio: a foundation model on multimodal tokens",
    "citation_count": 17,
    "authors": [
      "Zekun Moore Wang",
      "King Zhu",
      "Chunpu Xu",
      "Wangchunshu Zhou",
      "Jiaheng Liu",
      "Yibo Zhang",
      "Jessie Wang",
      "Ning Shi",
      "Siyu Li",
      "Yizhi Li",
      "Haoran Que",
      "Zhaoxiang Zhang",
      "Yuanxing Zhang",
      "Ge Zhang",
      "Ke Xu",
      "Jie Fu",
      "Wenhao Huang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.256": {
    "title": "DART: Distilling Autoregressive Reasoning to Silent Thought",
    "volume": "main",
    "abstract": "Chain-of-Thought (CoT) reasoning has significantly advanced Large Language Models (LLMs) in solving complex tasks. However, its autoregressive paradigm leads to significant computational overhead, hindering its deployment in latency-sensitive applications. To address this, we propose **DART** (**D**istilling **A**utoregressive **R**easoning to Silent **T**hought), a self-distillation framework that enables LLMs to replace autoregressive CoT with non-autoregressive Silent Thought (ST). Specifically, DART introduces two training pathways: the CoT pathway for traditional reasoning and the ST pathway for generating answers directly from a few ST tokens. The ST pathway utilizes a lightweight Reasoning Evolvement Module (REM) to align its hidden states with the CoT pathway, enabling the ST tokens to evolve into informative embeddings. During inference, only the ST pathway is activated, leveraging evolving ST tokens to deliver the answer directly. Extensive experimental results demonstrate that DART offers significant performance gains compared with existing non-autoregressive baselines without extra inference latency, serving as a feasible alternative for efficient reasoning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nan Jiang",
      "Ziming Wu",
      "De-Chuan Zhan",
      "Fuming Lai",
      "Shaobing Lian"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.257": {
    "title": "LeTS: Learning to Think-and-Search via Process-and-Outcome Reward Hybridization",
    "volume": "main",
    "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in reasoning with the emergence of reasoning models like OpenAI-o1 and DeepSeek-R1. Recent research focuses on integrating reasoning capabilities into the realm of retrieval-augmented generation (RAG) via outcome-supervised reinforcement learning (RL) approaches, while the correctness of intermediate think-and-search steps is usually neglected. To address this issue, we design a process-level reward module to mitigate the unawareness of intermediate reasoning steps in outcome-level supervision without additional annotation. Grounded on this, we propose **Le**arning to **T**hink-and-**S**earch (**LeTS**), a novel framework that hybridizes stepwise process reward and outcome-based reward to current RL methods for RAG. Extensive experiments demonstrate the generalization and inference efficiency of **LeTS** across various RAG benchmarks. In addition, these results reveal the potential of process- and outcome-level reward hybridization in boosting LLMs' reasoning ability via RL under other scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Zhang",
      "Shouqing Yang",
      "Lirong Gao",
      "Hao Chen",
      "Xiaomeng Hu",
      "Jinglei Chen",
      "Jiexiang Wang",
      "Sheng Guo",
      "Bo Zheng",
      "Haobo Wang",
      "Junbo Zhao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.258": {
    "title": "CYCLE-INSTRUCT: Fully Seed-Free Instruction Tuning via Dual Self-Training and Cycle Consistency",
    "volume": "main",
    "abstract": "Instruction tuning is vital for aligning large language models (LLMs) with human intent, but current methods typically rely on costly human-annotated seed data or powerful external teacher models. While instruction back-translation techniques reduce this dependency, they remain fundamentally tethered to an initial seed set, which limits full automation, introduces biases, and can lead to inefficient use of unlabeled corpora. In this paper, we propose Cycle-Instruct, a novel framework that achieves fully seed-free instruction tuning. Inspired by cycle consistency, Cycle-Instruct employs a dual self-training loop where two models—an answer generator and a question generator—are bootstrapped solely from raw, unlabeled text. These models mutually supervise each other by reconstructing original text segments from their counterpart's generated pseudo-labels, effectively learning from the intrinsic structure of the data without any human-provided seeds. We demonstrate Cycle-Instruct's efficacy across four diverse data tracks, including general instruction-following, domain-specific tasks, dialogue logs, and plain text. Our extensive experiments show that Cycle-Instruct not only outperforms seed-driven back-translation baselines but also achieves performance comparable to strongly supervised methods",
    "checked": true,
    "id": "86869e2efacc2cbef59303e8ebb893ff6861922d",
    "semantic_title": "cycle-instruct: fully seed-free instruction tuning via dual self-training and cycle consistency",
    "citation_count": 0,
    "authors": [
      "Zhanming Shen",
      "Hao Chen",
      "Yulei Tang",
      "Shaolin Zhu",
      "Wentao Ye",
      "Xiaomeng Hu",
      "Haobo Wang",
      "Gang Chen",
      "Junbo Zhao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.259": {
    "title": "Good Intentions Beyond ACL: Who Does NLP for Social Good, and Where?",
    "volume": "main",
    "abstract": "The social impact of Natural Language Processing (NLP) is increasingly important, with a rising community focus on initiatives related to NLP for Social Good (NLP4SG). Indeed, in recent years, almost 20% of all papers in the ACL Anthology address topics related to social good as defined by the UN Sustainable Development Goals (Aduato et al. 2023). In this study, we take an author- and venue-level perspective to map the landscape of NLP4SG, quantifying the proportion of work addressing social good concerns both within and beyond the ACL community, by both core ACL contributors and non-ACL authors. With this approach we discover two surprising facts about the landscape of NLP4SG. First, ACL authors are dramatically more likely to do work addressing social good concerns when publishing in venues outside of ACL. Second, the vast majority of publications using NLP techniques to address concerns of social good are done by non-ACL authors in venues outside of ACL. We discuss the implications of these findings on agenda-setting considerations for the ACL community related to NLP4SG",
    "checked": true,
    "id": "12338e8a5cdcb2de49c610732a72df0a86113fad",
    "semantic_title": "good intentions beyond acl: who does nlp for social good, and where?",
    "citation_count": 0,
    "authors": [
      "Grace LeFevre",
      "Qingcheng Zeng",
      "Adam Leif",
      "Jason Jewell",
      "Denis Peskoff",
      "Rob Voigt"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.260": {
    "title": "From General Reward to Targeted Reward: Improving Open-ended Long-context Generation Models",
    "volume": "main",
    "abstract": "Current research on long-form context in Large Language Models (LLMs) primarily focuses on the understanding of long-contexts, the **Open-ended Long Text Generation** (Open-LTG) remains insufficiently explored. Training a long text generation model requires curation of gold-standard reference data, which is typically nonexistent for informative Open-LTG tasks. However, previous methods only utilize general assessments as reward signals, which limits accuracy. To bridge this gap, we introduce **ProxyReward**, an innovative reinforcement learning (RL) based framework, which includes a data synthesis method and a novel reward signal. Firstly, **ProxyReward Dataset** synthesis is accomplished through simple prompts that enables the model to create automatically, obviating extensive labeled data or significant manual effort. Secondly, **ProxyReward Signal** offers a targeted evaluation of information comprehensiveness and accuracy for specific questions. The experimental results indicate that our method ProxyReward **surpasses even GPT-4-Turbo**. It can significantly enhance performance by 20% on the Open-LTG task when training widely used open-source models, while also surpassing the LLM-as-a-Judge approach. Our work presents effective methods to enhance the ability of LLMs to address complex open-ended questions posed by humans",
    "checked": true,
    "id": "3fc7750708b40f616eb8153c7b9b2a41fd4e4816",
    "semantic_title": "from general reward to targeted reward: improving open-ended long-context generation models",
    "citation_count": 0,
    "authors": [
      "Zhihan Guo",
      "Jiele Wu",
      "Wenqian Cui",
      "Yifei Zhang",
      "Minda Hu",
      "Yufei Wang",
      "Irwin King"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.261": {
    "title": "Think in Safety: Unveiling and Mitigating Safety Alignment Collapse in Multimodal Large Reasoning Model",
    "volume": "main",
    "abstract": "The rapid development of Multimodal Large Reasoning Models (MLRMs) has demonstrated broad application potential, yet their safety and reliability remain critical concerns that require systematic exploration. To address this gap, we conduct a comprehensive and systematic safety evaluation of 13 MLRMs across 5 benchmarks and unveil prevalent safety degradation phenomena in most advanced models. Moreover, our analysis reveals distinct safety patterns across different benchmarks: significant safety degradation is observed across jailbreak robustness benchmarks, whereas safety-awareness benchmarks demonstrate less pronounced degradation. In particular, the long thought process in some scenarios even enhances safety performance. Therefore, it is a potential approach to address safety issues in MLRMs by leveraging the intrinsic reasoning capabilities of the model to detect unsafe intent. To operationalize this insight, we construct a multimodal tuning dataset that incorporates a safety-oriented thought process. Experimental results from fine-tuning existing MLRMs with this dataset effectively enhance the safety on both jailbreak robustness and safety-awareness benchmarks. This study provides a new perspective for developing safe MLRMs",
    "checked": true,
    "id": "bbd0850807a87d38ec01c0a1baa305ee0f358aa8",
    "semantic_title": "think in safety: unveiling and mitigating safety alignment collapse in multimodal large reasoning model",
    "citation_count": 3,
    "authors": [
      "Xinyue Lou",
      "You Li",
      "Jinan Xu",
      "Xiangyu Shi",
      "Chi Chen",
      "Kaiyu Huang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.262": {
    "title": "Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models",
    "volume": "main",
    "abstract": "End-to-end Large Speech Language Models (LSLMs) have demonstrated impressive conversational generation abilities, yet consistently fall short of traditional pipeline systems on semantic understanding benchmarks. In this work, we reveal through systematic experimentation that although LSLMs lose some text input performance after speech-text alignment training, the performance gap between speech and text inputs is more pronounced, which we refer to as the modality gap. To understand this gap, we analyze both coarse- and fine-grained text and speech representations. At the coarse-grained level, representations of speech and text in deeper layers are found to be increasingly aligned in direction (cosine similarity), while concurrently diverging in magnitude (Euclidean distance). We further find that representation similarity is strongly correlated with the modality gap. At the fine-grained level, a spontaneous token-level alignment pattern between text and speech representations is observed. Based on this, we introduce the Alignment Path Score to quantify token-level alignment quality, which exhibits stronger correlation with the modality gap. Building on these insights, we design targeted interventions on critical tokens through angle projection and length normalization. These strategies demonstrate the potential to improve correctness for speech inputs. Our study provides the first systematic empirical analysis of the modality gap and alignment mechanisms in LSLMs, offering both theoretical and methodological guidance for future optimization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bajian Xiang",
      "Shuaijiang Zhao",
      "Tingwei Guo",
      "Wei Zou"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.263": {
    "title": "AssoCiAm: A Benchmark for Evaluating Association Thinking while Circumventing Ambiguity",
    "volume": "main",
    "abstract": "Recent advancements in multimodal large language models (MLLMs) have garnered significant attention, offering a promising pathway toward artificial general intelligence (AGI). Among the essential capabilities required for AGI, creativity has emerged as a critical trait for MLLMs, with association serving as its foundation. Association reflects a model's ability to think creatively, making it vital to evaluate and understand. While several frameworks have been proposed to assess associative ability, they often overlook the inherent ambiguity in association tasks, which arises from the divergent nature of associations and undermines the reliability of evaluations. To address this issue, we decompose ambiguity into two types—internal ambiguity and external ambiguity—and introduce AssoCiAm, a benchmark designed to evaluate associative ability while circumventing the ambiguity through a hybrid computational method. We then conduct extensive experiments on MLLMs, revealing a strong positive correlation between cognition and association. Additionally, we observe that the presence of ambiguity in the evaluation process causes MLLMs' behavior to become more random-like. Finally, we validate the effectiveness of our method in ensuring more accurate and reliable evaluations. See Project Page for the data and codes",
    "checked": true,
    "id": "5642a971326526427844939355e9b0e6309ea748",
    "semantic_title": "associam: a benchmark for evaluating association thinking while circumventing ambiguity",
    "citation_count": 0,
    "authors": [
      "Yifan Liu",
      "Wenkuan Zhao",
      "Shanshan Zhong",
      "Jinghui Qin",
      "Mingfu Liang",
      "Zhongzhan Huang",
      "Wushao Wen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.264": {
    "title": "M-BRe: Discovering Training Samples for Relation Extraction from Unlabeled Texts with Large Language Models",
    "volume": "main",
    "abstract": "For Relation Extraction (RE), the manual annotation of training data may be prohibitively expensive, since the sentences that contain the target relations in texts can be very scarce and difficult to find. It is therefore beneficial to develop an efficient method that can automatically extract training instances from unlabeled texts for training RE models. Recently, large language models (LLMs) have been adopted in various natural language processing tasks, with RE also benefiting from their advances. However, when leveraging LLMs for RE with predefined relation categories, two key challenges arise. First, in a multi-class classification setting, LLMs often struggle to comprehensively capture the semantics of every relation, leading to suboptimal results. Second, although employing binary classification for each relation individually can mitigate this issue, it introduces significant computational overhead, resulting in impractical time complexity for real-world applications. Therefore, this paper proposes a framework called M-BRe to extract training instances from unlabeled texts for RE. It utilizes three modules to combine the advantages of both of the above classification approaches: Relation Grouping, Relation Extraction, and Label Decision. Extensive experiments confirm its superior capability in discovering high-quality training samples from unlabeled texts for RE",
    "checked": true,
    "id": "ee624689e1d22b3a5c893a3e88ca1330f373926a",
    "semantic_title": "m-bre: discovering training samples for relation extraction from unlabeled texts with large language models",
    "citation_count": 0,
    "authors": [
      "Zexuan Li",
      "Hongliang Dai",
      "Piji Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.265": {
    "title": "R-TOFU: Unlearning in Large Reasoning Models",
    "volume": "main",
    "abstract": "Large Reasoning Models (LRMs) embed private or copyrighted information not only in their final answers but also throughout multi-step chain-of-thought (CoT) traces, making reliable unlearning far more demanding than in standard LLMs. We introduce Reasoning-TOFU (R-TOFU), the first benchmark tailored to this setting. R-TOFU augments existing unlearning tasks with realistic CoT annotations and provides step-wise metrics that expose residual knowledge invisible to answer-level checks. Using R-TOFU, we carry out a comprehensive comparison of gradient-based and preference-optimization baselines and show that conventional answer-only objectives leave substantial forget traces in reasoning. We further propose Reasoned IDK, a preference-optimization variant that preserves coherent yet inconclusive reasoning, achieving a stronger balance between forgetting efficacy and model utility than earlier refusal styles. Finally, we identify a failure mode: decoding variants such as ZeroThink and LessThink can still reveal forgotten content despite seemingly successful unlearning, emphasizing the need to evaluate models under diverse decoding settings. Together, the benchmark, analysis, and new baseline establish a systematic foundation for studying and improving unlearning in LRMs while preserving their reasoning capabilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sangyeon Yoon",
      "Wonje Jeung",
      "Albert No"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.266": {
    "title": "Chat-Driven Text Generation and Interaction for Person Retrieval",
    "volume": "main",
    "abstract": "Text-based person search (TBPS) enables the retrieval of person images from large-scale databases using natural language descriptions, offering critical value in surveillance applications. However, a major challenge lies in the labor-intensive process of obtaining high-quality textual annotations, which limits scalability and practical deployment. To address this, we introduce two complementary modules: Multi-Turn Text Generation (MTG) and Multi-Turn Text Interaction (MTI). MTG generates rich pseudo-labels through simulated dialogues with MLLMs, producing fine-grained and diverse visual descriptions without manual supervision. MTI refines user queries at inference time through dynamic, dialogue-based reasoning, enabling the system to interpret and resolve vague, incomplete, or ambiguous descriptions—characteristics often seen in real-world search scenarios. Together, MTG and MTI form a unified and annotation-free framework that significantly improves retrieval accuracy, robustness, and usability. Extensive evaluations demonstrate that our method achieves competitive or superior results while eliminating the need for manual captions, paving the way for scalable and practical deployment of TBPS systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zequn Xie",
      "Chuxin Wang",
      "Yeqiang Wang",
      "Sihang Cai",
      "Shulei Wang",
      "Tao Jin"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.267": {
    "title": "Spontaneous Giving and Calculated Greed in Language Models",
    "volume": "main",
    "abstract": "Large language models demonstrate strong problem-solving abilities through reasoning techniques such as chain-of-thought prompting and reflection. However, it remains unclear whether these reasoning capabilities extend to a form of social intelligence: making effective decisions in cooperative contexts. We examine this question using economic games that simulate social dilemmas. First, we apply chain-of-thought and reflection prompting to GPT-4o in a Public Goods Game. We then evaluate multiple off-the-shelf models across six cooperation and punishment games, comparing those with and without explicit reasoning mechanisms. We find that reasoning models consistently reduce cooperation and norm enforcement, favoring individual rationality. In repeated interactions, groups with more reasoning agents exhibit lower collective gains. These behaviors mirror human patterns of \"spontaneous giving and calculated greed.\" Our findings underscore the need for LLM architectures that incorporate social intelligence alongside reasoning, to help address—rather than reinforce—the challenges of collective action",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Li",
      "Hirokazu Shirado"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.268": {
    "title": "SenDetEX: Sentence-Level AI-Generated Text Detection for Human-AI Hybrid Content via Style and Context Fusion",
    "volume": "main",
    "abstract": "Text generated by Large Language Models (LLMs) now rivals human writing, raising concerns about its misuse. However, mainstream AI-generated text detection (AGTD) methods primarily target document-level long texts and struggle to generalize effectively to sentence-level short texts. And current sentence-level AGTD (S-AGTD) research faces two significant limitations: (1) lack of a comprehensive evaluation on complex human-AI hybrid content, where human-written text (HWT) and AI-generated text (AGT) alternate irregularly, and (2) failure to incorporate contextual information, which serves as a crucial supplementary feature for identifying the origin of the detected sentence. Therefore, in our work, we propose AutoFill-Refine, a high-quality synthesis strategy for human-AI hybrid texts, and then construct a dedicated S-AGTD benchmark dataset. Besides, we introduce SenDetEX, a novel framework for sentence-level AI-generated text detection via style and context fusion. Extensive experiments demonstrate that SenDetEX significantly outperforms all baseline models in detection accuracy, while exhibiting remarkable transferability and robustness. Source code is available at https://github.com/TristoneJiang/SenDetEX",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Jiang",
      "Desheng Wu",
      "Xiaolong Zheng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.269": {
    "title": "Judge and Improve: Towards a Better Reasoning of Knowledge Graphs with Large Language Models",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) have shown immense potential in improving the performance of large-scale models by effectively incorporating structured relational information. However, current approaches face two key challenges: (1) achieving robust semantic alignment between graph representations and large models, and (2) ensuring interpretability in the generated outputs. To address these challenges, we propose ExGLM (Explainable Graph Language Model), a novel training framework designed to seamlessly integrate graph and language modalities while enhancing transparency. Our framework introduces two core components: (1) a graph-language synergistic alignment module, which aligns graph structures with language model to ensure semantic consistency across modalities; and (2) a judge-and-improve paradigm, which allows the language model to iteratively evaluate, refine, and prioritize responses with higher interpretability, thereby improving both performance and transparency. Extensive experiments conducted on three benchmark datasets—ogbn-arxiv, Cora, and PubMed—demonstrate that ExGLM not only surpasses existing methods in efficiency but also generates outputs that are significantly more interpretable, effectively addressing the primary limitations of current approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mo Zhiqiang",
      "Yang Hua",
      "Jiahui Li",
      "Yuan Liu",
      "Shawn Wong",
      "Jianmin Huang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.270": {
    "title": "Add-One-In: Incremental Sample Selection for Large Language Models via a Choice-Based Greedy Paradigm",
    "volume": "main",
    "abstract": "Selecting high-quality and diverse training samples from extensive datasets plays a crucial role in reducing training overhead and enhancing the performance of Large Language Models (LLMs). However, existing studies fall short in assessing the overall value of selected data, focusing primarily on individual quality, and struggle to strike an effective balance between ensuring diversity and minimizing data point traversals. Therefore, this paper introduces a novel choice-based sample selection framework that shifts the focus from evaluating individual sample quality to comparing the contribution value of different samples when incorporated into the subset. Thanks to the advanced language understanding capabilities of LLMs, we utilize LLMs to evaluate the value of each option during the selection process. Furthermore, we design a greedy sampling process where samples are incrementally added to the subset, thereby improving efficiency by eliminating the need for exhaustive traversal of the entire dataset with the limited budget. Extensive experiments demonstrate that selected data from our method not only surpasses the performance of the full dataset but also achieves competitive results with recent powerful studies, while requiring fewer selections. Moreover, we validate our approach on a larger medical dataset, highlighting its practical applicability in real-world applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuo Li",
      "Yuhao Du",
      "Xiaoqi Jiao",
      "Steven Y. Guo",
      "Yuege Feng",
      "Xiang Wan",
      "Anningzhe Gao",
      "Jinpeng Hu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.271": {
    "title": "QuZO: Quantized Zeroth-Order Fine-Tuning for Large Language Models",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) are often quantized to lower precision to reduce the memory cost and latency in inference. However, quantization often degrades model performance, thus fine-tuning is required for various downstream tasks. Traditional fine-tuning methods such as stochastic gradient descent and Adam optimization require backpropagation, which is error-prone in the low-precision settings. To overcome these limitations, we propose the Quantized Zeroth-Order (QuZO) framework, specifically designed for fine-tuning LLMs through low-precision (e.g., 4- or 8-bit) forward passes. Our method avoids the low-precision straight-through estimator, which requires backward computation, and instead utilizes optimized stochastic rounding to mitigate increased bias. QuZO simplifies the training process, while achieving results comparable to first-order methods in FP8 and superior accuracy in INT8 and INT4 training. Experiments demonstrate that QuZO achieves competitive performance on classification, multi-choice, and generation tasks under low-bit training, including zero-shot reasoning tasks. Notably, QuZO incurs minimal overhead and reduces memory consumption by 2.94 ×–5.47 × compared to quantized first-order methods during LLaMA-7B fine-tuning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiajun Zhou",
      "Yifan Yang",
      "Kai Zhen",
      "Ziyue Liu",
      "Yequan Zhao",
      "Ershad Banijamali",
      "Athanasios Mouchtaris",
      "Ngai Wong",
      "Zheng Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.272": {
    "title": "Cost-Optimal Grouped-Query Attention for Long-Context Modeling",
    "volume": "main",
    "abstract": "Grouped-Query Attention (GQA) is a widely adopted strategy for reducing the computational cost of attention layers in large language models (LLMs). However, current GQA configurations are often suboptimal because they overlook how context length influences inference cost. Since inference cost grows with context length, the most cost-efficient GQA configuration should vary accordingly. In this work, we analyze the relationship among context length, model size, GQA configuration, and model loss, and introduce two innovations: (1) we decouple the total head size from the hidden size, enabling more flexible control over attention FLOPs; and (2) we jointly optimize the model size and the GQA configuration to arrive at a better allocation of inference resources between attention layers and other components. Our analysis reveals that commonly used GQA configurations are highly suboptimal for long-context scenarios. Moreover, we propose a recipe for deriving cost-optimal GQA configurations. Our results show that for long-context scenarios, one should use fewer attention heads while scaling up the model size. Configurations selected by our recipe can reduce both memory usage and FLOPs by more than 50% compared to Llama-3's GQA, with *no degradation in model capabilities*. Our findings offer valuable insights for designing efficient long-context LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingfa Chen",
      "Yutong Wu",
      "Chenyang Song",
      "Zhen Leng Thai",
      "Xingyu Shen",
      "Xu Han",
      "Zhiyuan Liu",
      "Maosong Sun"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.273": {
    "title": "ChatVLA: Unified Multimodal Understanding and Robot Control with Vision-Language-Action Model",
    "volume": "main",
    "abstract": "Humans possess a unified cognitive ability to perceive, comprehend, and interact with the physical world. Why can't large language models replicate this holistic understanding? Through a systematic analysis of existing training paradigms in vision-language-action models (VLA), we identify two key challenges: spurious forgetting, where robot training overwrites crucial visual-text alignments, and task interference, where competing control and understanding tasks degrade performance when trained jointly. To overcome these limitations, we propose ChatVLA, a novel framework featuring Phased Alignment Training, which incrementally integrates multimodal data after initial control mastery, and a Mixture-of-Experts architecture to minimize task interference. ChatVLA demonstrates competitive performance on visual question-answering datasets and significantly surpasses state-of-the-art vision-language-action (VLA) methods on multimodal understanding benchmarks. Notably, it achieves a six times higher performance on MMMU and scores 47.2% on MMStar with a more parameter-efficient design than ECoT. Furthermore, ChatVLA demonstrates superior performance on 25 real-world robot manipulation tasks compared to existing VLA methods like OpenVLA. Our findings highlight the potential of our unified framework for achieving both robust multimodal understanding and effective robot control",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongyi Zhou",
      "Yichen Zhu",
      "Minjie Zhu",
      "Junjie Wen",
      "Ning Liu",
      "Zhiyuan Xu",
      "Weibin Meng",
      "Yaxin Peng",
      "Chaomin Shen",
      "Feifei Feng",
      "Yi Xu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.274": {
    "title": "KG-RAG: Enhancing GUI Agent Decision-Making via Knowledge Graph-Driven Retrieval-Augmented Generation",
    "volume": "main",
    "abstract": "Despite recent progress, Graphic User Interface (GUI) agents powered by Large Language Models (LLMs) struggle with complex mobile tasks due to limited app-specific knowledge. While UI Transition Graphs (UTGs) offer structured navigation representations, they are underutilized due to poor extraction and inefficient integration. We introduce KG-RAG, a Knowledge Graph-driven Retrieval-Augmented Generation framework that transforms fragmented UTGs into structured vector databases for efficient real-time retrieval. By leveraging an intent-guided LLM search method, KG-RAG generates actionable navigation paths, enhancing agent decision-making. Experiments across diverse mobile apps show that KG-RAG outperforms existing methods, achieving a 75.8% success rate (8.9% improvement over AutoDroid), 84.6% decision accuracy (8.1% improvement), and reducing average task steps from 4.5 to 4.1. Additionally, we present KG-Android-Bench and KG-Harmony-Bench, two benchmarks tailored to the Chinese mobile ecosystem for future research. Finally, KG-RAG transfers to web/desktop (+40% SR on Weibo-web; +20% on QQ Music-desktop), and a UTG cost ablation shows accuracy saturates at ~4h per complex app, enabling practical deployment trade-offs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyi Guan",
      "Jason Chun Lok Li",
      "Zhijian Hou",
      "Pingping Zhang",
      "Donglai Xu",
      "Yuzhi Zhao",
      "Mengyang Wu",
      "Jinpeng Chen",
      "Thanh-Toan Nguyen",
      "Pengfei Xian",
      "Wenao Ma",
      "Shengchao Qin",
      "Graziano Chesi",
      "Ngai Wong"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.275": {
    "title": "CLIP-MoE: Towards Building Mixture of Experts for CLIP with Diversified Multiplet Upcycling",
    "volume": "main",
    "abstract": "Contrastive Language-Image Pre-training (CLIP) has become a cornerstone in multimodal intelligence. However, recent studies discovered that CLIP can only encode one aspect of the feature space, leading to substantial information loss and indistinctive features. To mitigate this issue, this paper introduces a novel strategy that fine-tunes a series of complementary CLIP models and transforms them into a CLIP-MoE. Specifically, we propose a model-agnostic Diversified Multiplet Upcycling (DMU) framework for CLIP. Instead of training multiple CLIP models from scratch, DMU leverages a pre-trained CLIP and fine-tunes it into a diverse set with highly cost-effective multistage contrastive learning, thus capturing distinct feature subspaces efficiently. To fully exploit these fine-tuned models while minimizing computational overhead, we transform them into a CLIP-MoE, which dynamically activates a subset of CLIP experts, achieving an effective balance between model capacity and computational cost. Comprehensive experiments demonstrate the superior performance of CLIP-MoE across various zero-shot retrieval, zero-shot image classification tasks, and downstream Multimodal Large Language Model (MLLM) benchmarks when used as a vision encoder. Code is available at https://github.com/OpenSparseLLMs/CLIP-MoE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jihai Zhang",
      "Xiaoye Qu",
      "Tong Zhu",
      "Yu Cheng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.276": {
    "title": "Search-o1: Agentic Search-Enhanced Large Reasoning Models",
    "volume": "main",
    "abstract": "Large reasoning models (LRMs) like OpenAI-o1 have demonstrated impressive long stepwise reasoning capabilities through large-scale reinforcement learning. However, their extended reasoning processes often suffer from knowledge insufficiency, leading to frequent uncertainties and potential errors. To address this limitation, we introduce **Search-o1**, a framework that enhances LRMs with an agentic retrieval-augmented generation (RAG) mechanism and a Reason-in-Documents module for refining retrieved documents. Search-o1 integrates an agentic search workflow into the reasoning process, enabling dynamic retrieval of external knowledge when LRMs encounter uncertain knowledge points. Additionally, due to the verbose nature of retrieved documents, we design a separate Reason-in-Documents module to deeply analyze the retrieved information before injecting it into the reasoning chain, minimizing noise and preserving coherent reasoning flow. Extensive experiments on complex reasoning tasks in science, mathematics, and coding, as well as six open-domain QA benchmarks, demonstrate the strong performance of Search-o1. This approach enhances the trustworthiness of LRMs in complex reasoning tasks, paving the way for advanced deep research systems. The code is available at https://github.com/RUC-NLPIR/Search-o1",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoxi Li",
      "Guanting Dong",
      "Jiajie Jin",
      "Yuyao Zhang",
      "Yujia Zhou",
      "Yutao Zhu",
      "Peitian Zhang",
      "Zhicheng Dou"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.277": {
    "title": "From Personas to Talks: Revisiting the Impact of Personas on LLM-Synthesized Emotional Support Conversations",
    "volume": "main",
    "abstract": "The rapid advancement of Large Language Models (LLMs) has revolutionized the generation of emotional support conversations (ESC), offering scalable solutions with reduced costs and enhanced data privacy. This paper explores the role of personas in the creation of ESC by LLMs. Our research utilizes established psychological frameworks to measure and infuse persona traits into LLMs, which then generate dialogues in the emotional support scenario. We conduct extensive evaluations to understand the stability of persona traits in dialogues, examining shifts in traits post-generation and their impact on dialogue quality and strategy distribution. Experimental results reveal several notable findings: 1) LLMs can infer core persona traits, 2) subtle shifts in emotionality and extraversion occur, influencing the dialogue dynamics, and 3) the application of persona traits modifies the distribution of emotional support strategies, enhancing the relevance and empathetic quality of the responses. These findings highlight the potential of persona-driven LLMs in crafting more personalized, empathetic, and effective emotional support dialogues, which has significant implications for the future design of AI-driven emotional support systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shenghan Wu",
      "Yimo Zhu",
      "Wynne Hsu",
      "Mong-Li Lee",
      "Yang Deng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.278": {
    "title": "Select-Then-Decompose: From Empirical Analysis to Adaptive Selection Strategy for Task Decomposition in Large Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) have demonstrated remarkable reasoning and planning capabilities, driving extensive research into task decomposition. Existing task decomposition methods focus primarily on memory, tool usage, and feedback mechanisms, achieving notable success in specific domains, but they often overlook the trade-off between performance and cost. In this study, we first conduct a comprehensive investigation on task decomposition, identifying six categorization schemes. Then, we perform an empirical analysis of three factors that influence the performance and cost of task decomposition: categories of approaches, characteristics of tasks, and configuration of decomposition and execution models, uncovering three critical insights and summarizing a set of practical principles. Building on this analysis, we propose the Select-Then-Decompose strategy, which establishes a closed-loop problem-solving process composed of three stages: selection, execution, and verification. This strategy dynamically selects the most suitable decomposition approach based on task characteristics and enhances the reliability of the results through a verification module. Comprehensive evaluations across multiple benchmarks show that the Select-Then-Decompose consistently lies on the Pareto frontier, demonstrating an optimal balance between performance and cost. Our code is publicly available at https://github.com/summervvind/Select-Then-Decompose",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuodi Liu",
      "Yingzhuo Liu",
      "Zi Wang",
      "Yusheng Wang",
      "Huijia Wu",
      "Liuyu Xiang",
      "Zhaofeng He"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.279": {
    "title": "TombRaider: Entering the Vault of History to Jailbreak Large Language Models",
    "volume": "main",
    "abstract": "**Warning: This paper contains content that may involve potentially harmful behaviours, discussed strictly for research purposes.**Jailbreak attacks can hinder the safety of Large Language Model (LLM) applications, especially chatbots. Studying jailbreak techniques is an important AI red teaming task for improving the safety of these applications. In this paper, we introduce TombRaider, a novel jailbreak technique that exploits the ability to store, retrieve, and use historical knowledge of LLMs. TombRaider employs two agents, the inspector agent to extract relevant historical information and the attacker agent to generate adversarial prompts, enabling effective bypassing of safety filters. We intensively evaluated TombRaider on six popular models. Experimental results showed that TombRaider could outperform state-of-the-art jailbreak techniques, achieving nearly 100% attack success rates (ASRs) on bare models and maintaining over 55.4% ASR against defence mechanisms. Our findings highlight critical vulnerabilities in existing LLM safeguards, underscoring the need for more robust safety defences",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junchen Ding",
      "Jiahao Zhang",
      "Yi Liu",
      "Ziqi Ding",
      "Gelei Deng",
      "Yuekang Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.280": {
    "title": "Text Meets Topology: Rethinking Out-of-distribution Detection in Text-Rich Networks",
    "volume": "main",
    "abstract": "Out-of-distribution (OOD) detection remains challenging in text-rich networks, where textual features intertwine with topological structures. Existing methods primarily address label shifts or rudimentary domain-based splits, overlooking the intricate textual-structural diversity. For example, in social networks, where users represent nodes with textual features (name, bio) while edges indicate friendship status, OOD may stem from the distinct language patterns between bot and normal users. To address this gap, we introduce the TextTopoOOD framework for evaluating detection across diverse OOD scenarios: (1) attribute-level shifts via text augmentations and embedding perturbations; (2) structural shifts through edge rewiring and semantic connections; (3) thematically-guided label shifts; and (4) domain-based divisions. Furthermore, we propose TNT-OOD to model the complex interplay between Text aNd Topology using: 1) a novel cross-attention module to fuse local structure into node-level text representations, and 2) a HyperNetwork to generate node-specific transformation parameters. This aligns topological and semantic features of ID nodes, enhancing ID/OOD distinction across structural and textual shifts. Experiments on 11 datasets across four OOD scenarios demonstrate the nuanced challenge of TextTopoOOD for evaluating OOD detection in text-rich networks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Danny Wang",
      "Ruihong Qiu",
      "Guangdong Bai",
      "Zi Huang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.281": {
    "title": "APLOT: Robust Reward Modeling via Adaptive Preference Learning with Optimal Transport",
    "volume": "main",
    "abstract": "The reward model (RM) plays a crucial role in aligning Large Language Models (LLMs) with human preferences through Reinforcement Learning, where the Bradley-Terry (BT) objective has been recognized as simple yet powerful, specifically for pairwise preference learning. However, BT-based RMs often struggle to effectively distinguish between similar preference responses, leading to insufficient separation between preferred and non-preferred outputs. Consequently, they may easily overfit easy samples and cannot generalize well to Out-Of-Distribution (OOD) samples, resulting in suboptimal performance. To address these challenges, this paper introduces an effective enhancement to BT-based RMs through an adaptive margin mechanism. Specifically, we design to dynamically adjust the RM focus on more challenging samples through margins, based on both semantic similarity and model-predicted reward differences, which is approached from a distributional perspective solvable with Optimal Transport (OT). By incorporating these factors into a principled OT cost matrix design, our adaptive margin enables the RM to better capture distributional differences between chosen and rejected responses, yielding significant improvements in performance, convergence speed, and generalization capabilities. Experimental results across multiple benchmarks demonstrate that our method outperforms several existing RM techniques, showcasing enhanced performance in both In-Distribution (ID) and OOD settings. Moreover, RLHF experiments support our practical effectiveness in better aligning LLMs with human preferences",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuo Li",
      "Yuege Feng",
      "Dandan Guo",
      "Jinpeng Hu",
      "Anningzhe Gao",
      "Xiang Wan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.282": {
    "title": "HS-STaR: Hierarchical Sampling for Self-Taught Reasoners via Difficulty Estimation and Budget Reallocation",
    "volume": "main",
    "abstract": "Self-taught reasoners (STaRs) enhance the mathematical reasoning abilities of large language models (LLMs) by leveraging self-generated responses for self-training. Recent studies have incorporated reward models to guide response selection or decoding, aiming to obtain higher-quality data. However, they typically allocate a uniform sampling budget across all problems, overlooking the varying utility of problems at different difficulty levels. In this work, we conduct an empirical study and find that problems near the boundary of the LLM's reasoning capability offer significantly greater learning utility than both easy and overly difficult ones. To identify and exploit such problems, we propose HS-STaR, a Hierarchical Sampling framework for Self-Taught Reasoners. Given a fixed sampling budget, HS-STaR first performs lightweight pre-sampling with a reward-guided difficulty estimation strategy to efficiently identify boundary-level problems. Subsequently, it dynamically reallocates the remaining budget toward these high-utility problems during a re-sampling phase, maximizing the generation of valuable training data. Extensive experiments across multiple reasoning benchmarks and backbone LLMs demonstrate that HS-STaR significantly outperforms other baselines without requiring additional sampling budget",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feng Xiong",
      "Hongling Xu",
      "Yifei Wang",
      "Runxi Cheng",
      "Yong Wang",
      "Xiangxiang Chu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.283": {
    "title": "SEPS: A Separability Measure for Robust Unlearning in LLMs",
    "volume": "main",
    "abstract": "Machine unlearning aims to selectively remove targeted knowledge from Large Language Models (LLMs), ensuring they forget specified content while retaining essential information. Existing unlearning metrics assess whether a model correctly answers retain queries and rejects forget queries, but they fail to capture real-world scenarios where forget queries rarely appear in isolation. In fact, forget and retain queries often coexist within the same prompt, making mixed-query evaluation crucial.We introduce SEPS, an evaluation framework that explicitly measures a model's ability to both forget and retain information within a single prompt. Through extensive experiments across three benchmarks, we identify two key failure modes in existing unlearning methods: (1) untargeted unlearning indiscriminately erases both forget and retain content once a forget query appears, and (2) targeted unlearning overfits to single-query scenarios, leading to catastrophic failures when handling multiple queries. To address these issues, we propose Mixed Prompt (MP) unlearning, a strategy that integrates both forget and retain queries into a unified training objective. Our approach significantly improves unlearning effectiveness, demonstrating robustness even in complex settings with up to eight mixed forget and retain queries in a single prompt",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wonje Jeung",
      "Sangyeon Yoon",
      "Albert No"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.284": {
    "title": "TRUST-VL: An Explainable News Assistant for General Multimodal Misinformation Detection",
    "volume": "main",
    "abstract": "Multimodal misinformation, encompassing textual, visual, and cross-modal distortions, poses an increasing societal threat that is amplified by generative AI. Existing methods typically focus on a single type of distortion and struggle to generalize to unseen scenarios. In this work, we observe that different distortion types share common reasoning capabilities while also requiring task-specific skills. We hypothesize that joint training across distortion types facilitates knowledge sharing and enhances the model's ability to generalize. To this end, we introduce TRUST-VL, a unified and explainable vision-language model for general multimodal misinformation detection. TRUST-VL incorporates a novel Question-Aware Visual Amplifier module, designed to extract task-specific visual features. To support training, we also construct TRUST-Instruct, a large-scale instruction dataset containing 198K samples featuring structured reasoning chains aligned with human fact-checking workflows. Extensive experiments on both in-domain and zero-shot benchmarks demonstrate that TRUST-VL achieves state-of-the-art performance, while also offering strong generalization and interpretability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zehong Yan",
      "Peng Qi",
      "Wynne Hsu",
      "Mong-Li Lee"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.285": {
    "title": "Tree-of-Quote Prompting Improves Factuality and Attribution in Multi-Hop and Medical Reasoning",
    "volume": "main",
    "abstract": "Large language models (LLMs) can produce fluent but factually incorrect outputs and often have limited ability to attribute their claims to source material. This undermines their reliability, particularly in multi-hop and high-stakes domains such as medicine. We propose Tree-of-Quote (ToQ) prompting, a structured framework that decomposes complex questions into subquestions, generates quotes to support each step without retrieval, and selectively advances reasoning based on quote quality. We also introduce FQ-Score, a unified metric that captures answer correctness, attribution fidelity, and reasoning quality. Experiments on StrategyQA, 2WikiMultiHopQA, MuSiQue, MoreHopQA, and MedQA demonstrate that ToQ improves factuality and attribution over standard prompting baselines. To validate FQ-Score as a proxy for human judgment, we conduct two reader studies with clinicians on medical questions, and observe strong correlations. Both clinician scores and FQ-Scores also indicate a preference for ToQ over baselines due to a combination of greater correctness, completeness, and logical flow. Our results suggest ToQ is a promising approach for building more trustworthy and auditable LLM systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Justin Xu",
      "Yiming Li",
      "Zizheng Zhang",
      "Augustine Yui Hei Luk",
      "Mayank Jobanputra",
      "Samarth Oza",
      "Ashley Murray",
      "Meghana Reddy Kasula",
      "Andrew Parker",
      "David W Eyre"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.286": {
    "title": "UnitCoder: Scalable Code Synthesis from Pre-training Corpora",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in various tasks, yet code generation remains a major challenge. Despite the abundant sources of code data, constructing high-quality training datasets at scale poses a significant challenge. Pre-training code data typically suffers from inconsistent data quality issues. Conversely, instruction-based methods which use a high-quality subset as seed samples suffer from limited task diversity. In this paper, we introduce UnitCoder, which directly supervises pre-training data quality through automatically generated unit tests, while ensuring the correctness via an iterative fix and refine flow. Code synthesized by UnitCoder benefits from both the diversity of pre-training corpora and the high quality ensured by unit test supervision. Our experiments demonstrate that models fine-tuned on our synthetic dataset exhibit consistent performance improvements. Our work presents a scalable approach that leverages model-generated unit tests to guide the synthesis of high-quality code data from pre-training corpora, demonstrating the potential for producing diverse and high-quality post-training data at scale. All code and data will be released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yichuan Ma",
      "Yunfan Shao",
      "Peiji Li",
      "Demin Song",
      "Qipeng Guo",
      "Linyang Li",
      "Xipeng Qiu",
      "Kai Chen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.287": {
    "title": "GRPO-LEAD: A Difficulty-Aware Reinforcement Learning Approach for Concise Mathematical Reasoning in Language Models",
    "volume": "main",
    "abstract": "Group Relative Policy Optimization (GRPO), which is widely adopted by R1-like reasoning models, has advanced mathematical reasoning. Nevertheless, GRPO faces challenges in reward sparsity, verbosity, and inadequate focus on problem difficulty. We propose GRPO-LEAD, enhancing GRPO with: (1) length-regularized rewards to encourage conciseness while maintaining accuracy; (2) explicit penalties for incorrect solutions to improve model precision; and (3) difficulty-aware advantage reweighting for robust generalization on challenging problems. Comprehensive evaluations demonstrate that GRPO-LEAD significantly improves reasoning accuracy, conciseness, and efficiency. Our approach achieves state-of-the-art performance for 14B-scale models, underscoring the synergy of our methods with appropriate model scale and high-quality data. Our source code, generated dataset, and models are available at https://github.com/aeroplanepaper/GRPO-LEAD",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jixiao Zhang",
      "Chunsheng Zuo"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.288": {
    "title": "Improving Low-Resource Sequence Labeling with Knowledge Fusion and Contextual Label Explanations",
    "volume": "main",
    "abstract": "Sequence labeling remains a significant challenge in low-resource, domain-specific scenarios, particularly for character-dense languages. Existing methods primarily focus on enhancing model comprehension and improving data diversity to boost performance. However, these approaches still struggle with inadequate model applicability and semantic distribution biases in domain-specific contexts. To overcome these limitations, we propose a novel framework that combines an LLM-based knowledge enhancement workflow with a span-based Knowledge Fusion for Rich and Efficient Extraction (KnowFREE) model. Our workflow employs explanation prompts to generate precise contextual interpretations of target entities, effectively mitigating semantic biases and enriching the model's contextual understanding. The KnowFREE model further integrates extension label features, enabling efficient nested entity extraction without relying on external knowledge during inference. Experiments on multiple domain-specific sequence labeling datasets demonstrate that our approach achieves state-of-the-art performance, effectively addressing the challenges posed by low-resource settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peichao Lai",
      "Jiaxin Gan",
      "Feiyang Ye",
      "Wentao Zhang",
      "Fangcheng Fu",
      "Yilei Wang",
      "Bin Cui"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.289": {
    "title": "Rethinking Cross-Subject Data Splitting for Brain-to-Text Decoding",
    "volume": "main",
    "abstract": "Recent major milestones have successfully reconstructed natural language from non-invasive brain signals (e.g. functional Magnetic Resonance Imaging (fMRI) and Electroencephalogram (EEG)) across subjects. However, we find current dataset splitting strategies for cross-subject brain-to-text decoding are wrong. Specifically, we first demonstrate that all current splitting methods suffer from data leakage problem, which refers to the leakage of validation and test data into training set, resulting in significant overfitting and overestimation of decoding models. In this study, we develop a right cross-subject data splitting criterion without data leakage for decoding fMRI and EEG signal to text. Some SOTA brain-to-text decoding models are re-evaluated correctly with the proposed criterion for further research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Congchi Yin",
      "Qian Yu",
      "Zhiwei Fang",
      "Changping Peng",
      "Piji Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.290": {
    "title": "RCScore: Quantifying Response Consistency in Large Language Models",
    "volume": "main",
    "abstract": "Current LLM evaluations often rely on a single instruction template, overlooking models' sensitivity to instruction style—a critical aspect for real-world deployments. We present RCScore, a multi-dimensional framework quantifying how instruction formulation affects model responses. By systematically transforming benchmark problems into multiple instruction styles, RCScore reveals performance variations undetected by conventional metrics. Our experiments across ten LLMs on four reasoning benchmarks demonstrate that instruction style can shift accuracy by up to 16.7% points. We introduce Cross-Response Similarity (CRS), a method applying RCScore metrics to measure stylistic self-consistency, and establish its strong correlation with task accuracy, suggesting consistency as a valuable proxy for model reliability. Additional findings show that deterministic decoding produces more stylistically stable outputs, and model scale correlates positively with cross-style consistency. RCScore offers a principled approach to assess instruction robustness",
    "checked": true,
    "id": "8f04edcc137a4fd685211f63d66653befaeecbb4",
    "semantic_title": "rcscore: quantifying response consistency in large language models",
    "citation_count": 0,
    "authors": [
      "Dongjun Jang",
      "Youngchae Ahn",
      "Hyopil Shin"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.291": {
    "title": "A Multi-Agent Framework with Automated Decision Rule Optimization for Cross-Domain Misinformation Detection",
    "volume": "main",
    "abstract": "Misinformation spans various domains, but detection methods trained on specific domains often perform poorly when applied to others. With the rapid development of Large Language Models (LLMs), researchers have begun to utilize LLMs for cross-domain misinformation detection. However, existing LLM-based methods often fail to adequately analyze news in the target domain, limiting their detection capabilities. More importantly, these methods typically rely on manually designed decision rules, which are limited by domain knowledge and expert experience, thus limiting the generalizability of decision rules to different domains. To address these issues, we propose a Multi-Agent Framework for cross-domain misinformation detection with Automated Decision Rule Optimization (MARO). Under this framework, we first employs multiple expert agents to analyze target-domain news. Subsequently, we introduce a question-reflection mechanism that guides expert agents to facilitate higher-quality analysis. Furthermore, we propose a decision rule optimization approach based on carefully designed cross-domain validation tasks to iteratively enhance decision rule effectiveness across domains. Experimental results and analysis on commonly used datasets demonstrate that MARO achieves significant improvements over existing methods",
    "checked": true,
    "id": "436eec9beb1f781dc40f5c39ab4b4c26dd624933",
    "semantic_title": "a multi-agent framework with automated decision rule optimization for cross-domain misinformation detection",
    "citation_count": 1,
    "authors": [
      "Hui Li",
      "Ante Wang",
      "Kunquan Li",
      "Zhihao Wang",
      "Liang Zhang",
      "Delai Qiu",
      "Qingsong Liu",
      "Jinsong Su"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.292": {
    "title": "OmniEval: An Omnidirectional and Automatic RAG Evaluation Benchmark in Financial Domain",
    "volume": "main",
    "abstract": "Retrieval-augmented generation (RAG) has emerged as a key application of large language models (LLMs), especially in vertical domains where LLMs may lack domain-specific knowledge. This paper introduces OmniEval, an omnidirectional and automatic RAG benchmark for the financial domain, featured by its multi-dimensional evaluation framework: First, we categorize RAG scenarios by five task classes and 16 financial topics, leading to a matrix-based structured assessment for RAG evaluation; Next, we leverage a multi-dimensional evaluation data generation method that integrates GPT-4-based automatic generation and human annotation approaches, achieving an 87.47% acceptance ratio in human evaluations of generated instances; Further, we utilize a multi-stage evaluation pipeline to assess both retrieval and generation performance, resulting in an all-sided evaluation of the RAG pipeline. Finally, rule-based and LLM-based metrics are combined to build a multi-dimensional evaluation system, enhancing the reliability of assessments through fine-tuned LLM-based evaluators. Our omnidirectional evaluation experiments highlight the performance variations of RAG systems across diverse topics and tasks and reveal significant opportunities for RAG models to improve their capabilities in vertical domains. We open source the anonymous code of our benchmark at https://github.com/RUC-NLPIR/OmniEval",
    "checked": true,
    "id": "f489b07797a877c77a9111bab448355046df2885",
    "semantic_title": "omnieval: an omnidirectional and automatic rag evaluation benchmark in financial domain",
    "citation_count": 0,
    "authors": [
      "Shuting Wang",
      "Jiejun Tan",
      "Zhicheng Dou",
      "Ji-Rong Wen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.293": {
    "title": "AQuilt: Weaving Logic and Self-Inspection into Low-Cost, High-Relevance Data Synthesis for Specialist LLMs",
    "volume": "main",
    "abstract": "Despite the impressive performance of large language models (LLMs) in general domains, they often underperform in specialized domains. Existing approaches typically rely on data synthesis methods and yield promising results by using unlabeled data to capture domain-specific features. However, these methods either incur high computational costs or suffer from performance limitations, while also demonstrating insufficient generalization across different tasks. To address these challenges, we propose AQuilt, a framework for constructing instruction-tuning data for any specialized domains from corresponding unlabeled data, including Answer, Question, Unlabeled data, Inspection, Logic, and Task type. By incorporating logic and inspection, we encourage reasoning processes and self-inspection to enhance model performance. Moreover, customizable task instructions enable high-quality data generation for any task. As a result, we construct a dataset of 703K examples to train a powerful data synthesis model. Experiments show that AQuilt is comparable to DeepSeek-V3 while utilizing just 17% of the production cost. Further analysis demonstrates that our generated data exhibits higher relevance to downstream tasks. Source code, models, and scripts are available at https://github.com/Krueske/AQuilt",
    "checked": true,
    "id": "fd01eb8621f9a3c5a657ce5c9f0b60d18de82c57",
    "semantic_title": "aquilt: weaving logic and self-inspection into low-cost, high-relevance data synthesis for specialist llms",
    "citation_count": 1,
    "authors": [
      "Xiaopeng Ke",
      "Hexuan Deng",
      "Xuebo Liu",
      "Jun Rao",
      "Zhenxi Song",
      "Jun Yu",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.294": {
    "title": "MoSEs: Uncertainty-Aware AI-Generated Text Detection via Mixture of Stylistics Experts with Conditional Thresholds",
    "volume": "main",
    "abstract": "The rapid advancement of large language models has intensified public concerns about the potential misuse. Therefore, it is important to build trustworthy AI-generated text detection systems. Existing methods neglect stylistic modeling and mostly rely on static thresholds, which greatly limits the detection performance. In this paper, we propose the Mixture of Stylistic Experts (MoSEs) framework that enables stylistics-aware uncertainty quantification through conditional threshold estimation. MoSEs contain three core components, namely, the Stylistics Reference Repository (SRR), the Stylistics-Aware Router (SAR), and the Conditional Threshold Estimator (CTE). For input text, SRR can activate the appropriate reference data in SRR and provide them to CTE. Subsequently, CTE jointly models the linguistic statistical properties and semantic features to dynamically determine the optimal threshold. With a discrimination score, MoSEs yields prediction labels with the corresponding confidence level. Our framework achieves an average improvement 11.34% in detection performance compared to baselines. More inspiringly, MoSEs shows a more evident improvement 39.15% in the low-resource case. Our code is available at https://github.com/creator-xi/MoSEs",
    "checked": true,
    "id": "b552b050de3c6defeab551b6d099b4bf71945cd0",
    "semantic_title": "moses: uncertainty-aware ai-generated text detection via mixture of stylistics experts with conditional thresholds",
    "citation_count": 1,
    "authors": [
      "Junxi Wu",
      "Jinpeng Wang",
      "Zheng Liu",
      "Bin Chen",
      "Dongjian Hu",
      "Hao Wu",
      "Shu-Tao Xia"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.295": {
    "title": "Merger-as-a-Stealer: Stealing Targeted PII from Aligned LLMs with Model Merging",
    "volume": "main",
    "abstract": "Model merging has emerged as a promising approach for updating large language models (LLMs) by integrating multiple domain-specific models into a cross-domain merged model. Despite its utility and plug-and-play nature, unmonitored mergers can introduce significant security vulnerabilities, such as backdoor attacks and model merging abuse. In this paper, we identify a novel and more realistic attack surface where a malicious merger can extract targeted personally identifiable information (PII) from an aligned model with model merging. Specifically, we propose Merger-as-a-Stealer, a two-stage framework to achieve this attack: First, the attacker fine-tunes a malicious model to force it to respond to any PII-related queries. The attacker then uploads this malicious model to the model merging conductor and obtains the merged model. Second, the attacker inputs direct PII-related queries to the merged model to extract targeted PII. Extensive experiments demonstrate that Merger-as-a-Stealer successfully executes attacks against various LLMs and model merging methods across diverse settings, highlighting the effectiveness of the proposed framework. Given that this attack enables character-level extraction for targeted PII without requiring any additional knowledge from the attacker, we stress the necessity for improved model alignment and more robust defense mechanisms to mitigate such threats",
    "checked": true,
    "id": "7174cf188e9d7b5e1842c0d0517ba5df22bc6a8a",
    "semantic_title": "merger-as-a-stealer: stealing targeted pii from aligned llms with model merging",
    "citation_count": 1,
    "authors": [
      "Lin Lu",
      "Zhigang Zuo",
      "Ziji Sheng",
      "Pan Zhou"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.296": {
    "title": "Pragmatic Inference Chain (PIC) Improving LLMs' Reasoning of Authentic Implicit Toxic Language",
    "volume": "main",
    "abstract": "The rapid development of large language models (LLMs) gives rise to ethical concerns about their performance, while opening new avenues for developing toxic language detection techniques. However, LLMs' unethical output and their capability of detecting toxicity have primarily been tested on language data that do not demand complex meaning inference, such as the biased associations of ‘he' with programmer and ‘she' with household. Nowadays toxic language adopts a much more creative range of implicit forms, thanks to advanced censorship. In this study, we collect authentic toxic interactions that evade online censorship and that are verified by human annotators as inference-intensive. To evaluate and improve LLMs' reasoning of the authentic implicit toxic language, we propose a new prompting method, Pragmatic Inference Chain (PIC), drawn on interdisciplinary findings from cognitive science and linguistics. The PIC prompting significantly improves the success rate of GPT-4o, Llama-3.1-70B-Instruct, DeepSeek-v2.5, and DeepSeek-v3 in identifying implicit toxic language, compared to five baseline prompts, such as CoT and rule-based baselines. In addition, it also facilitates the models to produce more explicit and coherent reasoning processes, hence can potentially be generalized to other inference-intensive tasks, e.g., understanding humour and metaphors",
    "checked": true,
    "id": "a987fb8dbcd2c76a8616d0cd95686aa9d009b0fd",
    "semantic_title": "pragmatic inference chain (pic) improving llms' reasoning of authentic implicit toxic language",
    "citation_count": 4,
    "authors": [
      "Xi Chen",
      "Shuo Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.297": {
    "title": "Beyond Demonstrations: Dynamic Vector Construction from Latent Representations",
    "volume": "main",
    "abstract": "In-Context derived Vector (ICV) methods extract task-relevant representations from large language models (LLMs) and reinject them during inference, achieving comparable performance to few-shot In-Context Learning (ICL) without repeated demonstration processing. However, existing ICV methods remain sensitive to ICL-specific factors, often use coarse or semantically fragmented representations as the source of the vector, and rely on heuristic-based injection positions, limiting their applicability.To address these issues, we propose Dynamic Vector (DyVec), which incorporates an Exhaustive Query Rotation (EQR) strategy to extract robust semantically aggregated latent representations by mitigating variance introduced by ICL. It then applies Dynamic Latent Segmentation and Injection to adaptively partition representations based on task complexity and leverages REINFORCE-based optimization to learn optimal injection positions for each segment.Experiments results show that DyVec outperforms few-shot ICL, LoRA, and prior ICV baselines. Further analysis highlights the effectiveness of dynamically segmenting and injecting semantically aggregated latent representations. DyVec provides a lightweight and data-efficient solution for inference-time task adaptation",
    "checked": true,
    "id": "d6909e58833942f2870966a22687e67026ad3b2d",
    "semantic_title": "beyond demonstrations: dynamic vector construction from latent representations",
    "citation_count": 1,
    "authors": [
      "Wang Cai",
      "Hsiu-Yuan Huang",
      "Zhixiang Wang",
      "Yunfang Wu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.298": {
    "title": "Detoxifying Large Language Models via the Diversity of Toxic Samples",
    "volume": "main",
    "abstract": "Eliminating toxicity from Large Language Models (LLMs) is crucial for ensuring user safety. However, current methods have limitations in the analysis and utilization of toxic samples, failing to fully harness their potential. Through comparative analysis of toxic and safe samples, we discover that toxic samples exhibit diversity and, within this diversity, there lies specificity. These findings suggest that leveraging these characteristics of toxic samples could enhance the performance of algorithms in detoxifying LLMs. To this end, we propose a novel diverse detoxification framework, DivDetox, which comprises two innovative components: a Multi-Category-Induced Personalized Sample Generation (MPSG) strategy and a Scaled Contrastive DPO (SC-DPO) approach. The former is designed to elicit a variety of personalized toxic responses from the LLM, while the latter is constructed to precisely and fully utilize these toxic responses. Experiments on benchmark datasets across different model scales and different detoxification tasks verify the effectiveness of our architecture",
    "checked": true,
    "id": "79913f80a76c0a822636793451af826f228f8551",
    "semantic_title": "detoxifying large language models via the diversity of toxic samples",
    "citation_count": 0,
    "authors": [
      "Ying Zhao",
      "Yuanzhao Guo",
      "Xuemeng Weng",
      "Yuan Tian",
      "Wei Wang",
      "Yi Chang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.299": {
    "title": "LLM-Driven Implicit Target Augmentation and Fine-Grained Contextual Modeling for Zero-Shot and Few-Shot Stance Detection",
    "volume": "main",
    "abstract": "Stance detection aims to identify the attitude expressed in text towards a specific target. Recent studies on zero-shot and few-shot stance detection focus primarily on learning generalized representations from explicit targets. However, these methods often neglect implicit yet semantically important targets and fail to adaptively adjust the relative contributions of text and target in light of contextual dependencies. To overcome these limitations, we propose a novel two-stage framework: First, a data augmentation framework named Hierarchical Collaborative Target Augmentation (HCTA) employs Large Language Models (LLMs) to identify and annotate implicit targets via Chain-of-Thought (CoT) prompting and multi-LLM voting, significantly enriching training data with latent semantic relations. Second, we introduce DyMCA, a Dynamic Multi-level Context-aware Attention Network, integrating a joint text-target encoding and a content-aware mechanism to dynamically adjust text-target contributions based on context. Experiments on the benchmark dataset demonstrate that our approach achieves state-of-the-art results, confirming the effectiveness of implicit target augmentation and fine-grained contextual modeling",
    "checked": true,
    "id": "9b1b1dbfe86a28e68ac9dadfa3a1d8fe93fc816f",
    "semantic_title": "llm-driven implicit target augmentation and fine-grained contextual modeling for zero-shot and few-shot stance detection",
    "citation_count": 0,
    "authors": [
      "Yanxu Ji",
      "Jinzhong Ning",
      "Yijia Zhang",
      "Zhi Liu",
      "Hongfei Lin"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.300": {
    "title": "Dial-In LLM: Human-Aligned LLM-in-the-loop Intent Clustering for Customer Service Dialogues",
    "volume": "main",
    "abstract": "Discovering customer intentions is crucial for automated service agents, yet existing intent clustering methods often fall short due to their reliance on embedding distance metrics and neglect of underlying semantic structures. To address these limitations, we propose an **LLM-in-the-loop (LLM-ITL)** intent clustering framework, integrating the language understanding capabilities of LLMs into conventional clustering algorithms. Specifically, this paper (1) examines the effectiveness of fine-tuned LLMs in semantic coherence evaluation and intent cluster naming, achieving over 95% accuracy aligned with human judgments; (2) designs an LLM-ITL framework that facilitates the iterative discovery of coherent intent clusters and the optimal number of clusters; and (3) introduces context-aware techniques tailored for customer service dialogue. Since existing English benchmarks lack sufficient semantic diversity and intent coverage, we further present a comprehensive Chinese dialogue intent dataset comprising over 100k real customer service calls with 1,507 human-annotated clusters. The proposed approaches significantly outperform LLM-guided baselines, achieving notable improvements in clustering quality, cost efficiency, and downstream applications. Combined with several best practices, our findings highlight the prominence of LLM-in-the-loop techniques for scalable dialogue data mining",
    "checked": true,
    "id": "6a7520bcf5ff2f6894657d89c74183a3b911c7ae",
    "semantic_title": "dial-in llm: human-aligned llm-in-the-loop intent clustering for customer service dialogues",
    "citation_count": 5,
    "authors": [
      "Mengze Hong",
      "Wailing Ng",
      "Chen Jason Zhang",
      "Yuanfeng Song",
      "Di Jiang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.301": {
    "title": "Superficial Self-Improved Reasoners Benefit from Model Merging",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) rely heavily on large-scale reasoning data, but as such data becomes increasingly scarce, model self-improvement offers a promising alternative. However, this process can lead to model collapse, as the model's output becomes overly deterministic with reduced diversity. In this work, we identify a new risk beyond model collapse, which we term the Superficial Self-Improved Reasoners phenomenon. This phenomenon indicates that while self-improvement enhances in-domain (ID) reasoning accuracy, it degrades the model's generalized reasoning capability on out-of-domain (OOD) datasets, as the model tends to memorize the training data. Our analyses of layer importance and parameter changes reveal that reasoning-critical layers receive fewer updates compared to less relevant layers during self-improvement. To address this, we propose Iterative Model Merging (IMM), which balances reasoning improvements and generalization by merging the weights of the original and self-improved models. IMM effectively mitigates model collapse and improves generalized reasoning capability. Code is available at https://github.com/xiangchi-yuan/merge_syn",
    "checked": true,
    "id": "2967cde57893bcb08e18814fae277a47aea82317",
    "semantic_title": "superficial self-improved reasoners benefit from model merging",
    "citation_count": 10,
    "authors": [
      "Xiangchi Yuan",
      "Chunhui Zhang",
      "Zheyuan Liu",
      "Dachuan Shi",
      "Leyan Pan",
      "Soroush Vosoughi",
      "Wenke Lee"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.302": {
    "title": "CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated Chain-of-Thought-based Reinforced Fine-Tuning",
    "volume": "main",
    "abstract": "Reasoning capability plays a significantly critical role in the the broad applications of Large Language Models (LLMs). To enhance the reasoning performance of LLMs, diverse Reinforcement Learning (RL)-based fine-tuning approaches have been proposed to address the limited generalization capability of LLMs trained solely via Supervised Fine-Tuning (SFT). Despite their effectiveness, two major limitations hinder the advancement of LLMs. First, vanilla RL-based approaches ignore annotated Chain-of-Thought (CoT) and incorporate unstable reasoning path sampling, which typically results in model collapse, unstable training process, and suboptimal performance. Second, existing SFT approaches generally overemphasize the annotated CoT, potentially leading to performance degradation due to insufficient exploitation of potential CoT. In this paper, we propose a Contrastive learning with annotated CoT-based Reinforced Fine-Tuning approach, i.e., CARFT, to enhance the reasoning performance of LLMs while addressing the aforementioned limitations. Specifically, we propose learning a representation for each CoT. Based on this representation, we design novel contrastive signals to guide the fine-tuning process. Our approach not only fully exploits the available annotated CoT but also stabilizes the fine-tuning procedure by incorporating an additional unsupervised learning signal. We conduct comprehensive experiments and in-depth analysis with three baseline approaches, two foundation models, and two datasets to demonstrate significant advantages of CARFT in terms of robustness, performance (up to 10.15%), and efficiency (up to 30.62%)",
    "checked": true,
    "id": "92f59f320e48a65ae46cb44c58d858a10937cc80",
    "semantic_title": "carft: boosting llm reasoning via contrastive learning with annotated chain-of-thought-based reinforced fine-tuning",
    "citation_count": 0,
    "authors": [
      "Wenqiao Zhu",
      "Ji Liu",
      "Rongjunchen Zhang",
      "Haipang Wu",
      "Yulun Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.303": {
    "title": "QualBench: Benchmarking Chinese LLMs with Localized Professional Qualifications for Vertical Domain Evaluation",
    "volume": "main",
    "abstract": "The rapid advancement of Chinese LLMs underscores the need for vertical-domain evaluations to ensure reliable applications. However, existing benchmarks often lack domain coverage and provide limited insights into the Chinese working context. Leveraging qualification exams as a unified framework for expertise evaluation, we introduce QualBench, the first multi-domain Chinese QA benchmark dedicated to localized assessment of Chinese LLMs. The dataset includes over 17,000 questions across six vertical domains, drawn from 24 Chinese qualifications to align with national policies and professional standards. Results reveal an interesting pattern of Chinese LLMs consistently surpassing non-Chinese models, with the Qwen2.5 model outperforming the more advanced GPT-4o, emphasizing the value of localized domain knowledge in meeting qualification requirements. The average accuracy of 53.98% reveals the current gaps in domain coverage within model capabilities. Furthermore, we identify performance degradation caused by LLM crowdsourcing, assess data contamination, and illustrate the effectiveness of prompt engineering and model fine-tuning, suggesting opportunities for future improvements through multi-domain RAG and Federated Learning. Data and code are publicly available at https://github.com/mengze-hong/QualBench",
    "checked": true,
    "id": "2ca4daa56eb325a683dca10d2affe3d6c4225b29",
    "semantic_title": "qualbench: benchmarking chinese llms with localized professional qualifications for vertical domain evaluation",
    "citation_count": 5,
    "authors": [
      "Mengze Hong",
      "Wailing Ng",
      "Chen Jason Zhang",
      "Di Jiang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.304": {
    "title": "VideoEraser: Concept Erasure in Text-to-Video Diffusion Models",
    "volume": "main",
    "abstract": "The rapid growth of text-to-video (T2V) diffusion models has raised concerns about privacy, copyright, and safety due to their potential misuse in generating harmful or misleading content. These models are often trained on numerous datasets, including unauthorized personal identities, artistic creations, and harmful materials, which can lead to uncontrolled production and distribution of such content. To address this, we propose VideoEraser, a training-free framework that prevents T2V diffusion models from generating videos with undesirable concepts, even when explicitly prompted with those concepts. Designed as a plug-and-play module, VideoEraser can seamlessly integrate with representative T2V diffusion models via a two-stage process: Selective Prompt Embedding Adjustment (SPEA) and Adversarial-Resilient Noise Guidance (ARNG). We conduct extensive evaluations across four tasks, including object erasure, artistic style erasure, celebrity erasure, and explicit content erasure. Experimental results show that VideoEraser consistently outperforms prior methods regarding efficacy, integrity, fidelity, robustness, and generalizability. Notably, VideoEraser achieves state-of-the-art performance in suppressing undesirable content during T2V generation, reducing it by 46% on average across four tasks compared to baselines",
    "checked": true,
    "id": "38d9d6ac16825cc227ef9be241afcef81a6f4dce",
    "semantic_title": "videoeraser: concept erasure in text-to-video diffusion models",
    "citation_count": 2,
    "authors": [
      "Naen Xu",
      "Jinghuai Zhang",
      "Changjiang Li",
      "Zhi Chen",
      "Chunyi Zhou",
      "Qingming Li",
      "Tianyu Du",
      "Shouling Ji"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.305": {
    "title": "Diagram-Driven Course Questions Generation",
    "volume": "main",
    "abstract": "Visual Question Generation (VQG) research focuses predominantly on natural images while neglecting the diagram, which is a critical component in educational materials. To meet the needs of pedagogical assessment, we propose the Diagram-Driven Course Questions Generation (DDCQG) task and construct DiagramQG, a comprehensive dataset with 15,720 diagrams and 25,798 questions across 37 subjects and 371 courses. Our approach employs course and input text constraints to generate course-relevant questions about specific diagram elements. We reveal three challenges of DDCQG: domain-specific knowledge requirements across courses, long-tail distribution in course coverage, and high information density in diagrams. To address these, we propose the Hierarchical Knowledge Integration framework (HKI-DDCQG), which utilizes trainable CLIP for identifying relevant diagram patches, leverages frozen vision-language models for knowledge extraction, and generates questions with trainable T5. Experiments demonstrate that HKI-DDCQG outperforms existing models on DiagramQG while maintaining strong generalizability across natural image datasets, establishing a strong baseline for DDCQG",
    "checked": true,
    "id": "beb4428d9cf0ac9ceca48324307c78adbc5b5b5a",
    "semantic_title": "diagram-driven course questions generation",
    "citation_count": 0,
    "authors": [
      "Xinyu Zhang",
      "Lingling Zhang",
      "Yanrui Wu",
      "Muye Huang",
      "Wenjun Wu",
      "Bo Li",
      "Shaowei Wang",
      "Basura Fernando",
      "Jun Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.306": {
    "title": "ECC: An Emotion-Cause Conversation Dataset for Empathy Response",
    "volume": "main",
    "abstract": "The empathy dialogue system requires understanding emotions and their underlying causes. However, existing datasets mainly focus on emotion labels, while cause annotations are added post hoc through costly and subjective manual processes. This leads to three limitations: subjective bias in cause labels, weak rationality due to ambiguous cause-emotion relationships, and high annotation costs that hinder scalability. To address these challenges, we propose ECC (Emotion-Cause Conversation Dataset), a scalable dataset with 2.4K dialogues, which is also the first dialogue dataset where conversations and their emotion-cause labels are automatically generated synergistically during creation. We create an automatic extension framework EC-DD for ECC that utilizes knowledge and large language models (LLMs) to automatically generate conversations, and train a causality-aware empathetic response model CAER on this dataset. Experimental results show that ECC can achieve comparable or even superior performance to artificially constructed empathy dialogue datasets. Our code will be publicly released on https://github.com/Yuan-23/ECC",
    "checked": true,
    "id": "0a2d79898857a1c1296c4a9f7e1d38e1f04f3a1d",
    "semantic_title": "ecc: an emotion-cause conversation dataset for empathy response",
    "citation_count": 0,
    "authors": [
      "Yuanyuan He",
      "Yongsen Pan",
      "Wei Li",
      "Jiali You",
      "Jiawen Deng",
      "Fuji Ren"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.307": {
    "title": "ThoughtProbe: Classifier-Guided LLM Thought Space Exploration via Probing Representations",
    "volume": "main",
    "abstract": "This paper introduces ThoughtProbe, a novel inference-time framework that leverages the hidden reasoning features of Large Language Models (LLMs) to improve their reasoning performance. Unlike previous works that manipulate the hidden representations to steer LLM generation, we harness them as discriminative signals to guide the tree-structured response space exploration. In each node expansion, a classifier serves as a scoring and ranking mechanism that efficiently allocates computational resources by prioritizing higher score candidates for continuation. After completing the tree expansion, we collect answers from all branches to form a candidate answer pool. We then propose a branch-aggregation method that marginalizes over all supporting branches by aggregating their CoT scores, thereby identifying the optimal answer from the pool. Experimental results show that our framework's comprehensive exploration not only covers valid reasoning chains but also effectively identifies them, achieving significant improvements across multiple arithmetic reasoning benchmarks",
    "checked": true,
    "id": "d54c361bdc381bdf87795643ab8c96a7ffeaf2b0",
    "semantic_title": "thoughtprobe: classifier-guided llm thought space exploration via probing representations",
    "citation_count": 0,
    "authors": [
      "Zijian Wang",
      "Chang Xu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.308": {
    "title": "JOLT-SQL: Joint Loss Tuning of Text-to-SQL with Confusion-aware Noisy Schema Sampling",
    "volume": "main",
    "abstract": "Text-to-SQL, which maps natural language to SQL queries, has benefited greatly from recent advances in Large Language Models (LLMs). While LLMs offer various paradigms for this task, including prompting and supervised fine-tuning (SFT), SFT approaches still face challenges such as complex multi-stage pipelines and poor robustness to noisy schema information. To address these limitations, we present JOLT-SQL, a streamlined single-stage SFT framework that jointly optimizes schema linking and SQL generation via a unified loss. JOLT-SQL employs discriminative schema linking, enhanced by local bidirectional attention, alongside a confusion-aware noisy schema sampling strategy with selective attention to improve robustness under noisy schema conditions. Experiments on the Spider and BIRD benchmarks demonstrate that JOLT-SQL achieves state-of-the-art execution accuracy among comparable-size open-source models, while significantly improving both training and inference efficiency",
    "checked": true,
    "id": "9b0e42fcdcf66e98edb35f9b1708171025ffcc03",
    "semantic_title": "jolt-sql: joint loss tuning of text-to-sql with confusion-aware noisy schema sampling",
    "citation_count": 0,
    "authors": [
      "Jinwang Song",
      "Hongying Zan",
      "Kunli Zhang",
      "Lingling Mu",
      "Yingjie Han",
      "Haobo Hua",
      "Min Peng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.309": {
    "title": "DMDTEval: An Evaluation and Analysis of LLMs on Disambiguation in Multi-domain Translation",
    "volume": "main",
    "abstract": "Currently, Large Language Models (LLMs) have achieved remarkable results in machine translation. However, their performance in multi-domain translation (MDT) is less satisfactory, the meanings of words can vary across different domains, highlighting the significant ambiguity inherent in MDT. Therefore, evaluating the disambiguation ability of LLMs in MDT, remains an open problem. To this end, we present an evaluation and analysis of LLMs on disambiguation in multi-domain translation (DMDTEval), our systematic evaluation framework consisting of three aspects: (1) we construct a translation test set with multi-domain ambiguous word annotation, (2) we curate a diverse set of disambiguation prompt strategies, and (3) we design precise disambiguation metrics, and study the efficacy of various prompt strategies on multiple state-of-the-art LLMs. We conduct comprehensive experiments across 4 language pairs and 13 domains, our extensive experiments reveal a number of crucial findings that we believe will pave the way and also facilitate further research in the critical area of improving the disambiguation of LLMs",
    "checked": true,
    "id": "492e8387de7e7f859eb3b11074c70caabd8acda7",
    "semantic_title": "dmdteval: an evaluation and analysis of llms on disambiguation in multi-domain translation",
    "citation_count": 1,
    "authors": [
      "Zhibo Man",
      "Yuanmeng Chen",
      "Yujie Zhang",
      "Jinan Xu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.310": {
    "title": "SciRIFF: A Resource to Enhance Language Model Instruction-Following over Scientific Literature",
    "volume": "main",
    "abstract": "We present ScIRIFF (Scientific Resource for Instruction-Following and Finetuning), a dataset of 137K instruction-following instances for training and evaluation, covering 54 tasks. These tasks span five core scientific literature understanding capabilities: information extraction, summarization, question answering, claim verification, and classification. ScIRIFF is unique in being the only entirely expert-written, high-quality instruction-following dataset designed for extracting and synthesizing information from research literature across diverse scientific fields. It features complex instructions with long input contexts, detailed task descriptions, and structured outputs. To demonstrate its utility, we finetune a series of large language models (LLMs) using a mix of general domain and ScIRIFF instructions. On nine out-of-distribution held-out tasks (referred to as SciRIFF-Eval), LLMs finetuned on SciRIFF achieve 70.6% average improvement over our baselines trained only on general-domain instructions. ScIRIFF facilitates the development and evaluation of LLMs to help researchers navigate the rapidly growing body of scientific literature",
    "checked": true,
    "id": "71303f1c03a56694d983b1c0230b432714243ba2",
    "semantic_title": "sciriff: a resource to enhance language model instruction-following over scientific literature",
    "citation_count": 20,
    "authors": [
      "David Wadden",
      "Kejian Shi",
      "Jacob Morrison",
      "Alan Li",
      "Aakanksha Naik",
      "Shruti Singh",
      "Nitzan Barzilay",
      "Kyle Lo",
      "Tom Hope",
      "Luca Soldaini",
      "Shannon Zejiang Shen",
      "Doug Downey",
      "Hannaneh Hajishirzi",
      "Arman Cohan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.311": {
    "title": "MAKAR: a Multi-Agent framework based Knowledge-Augmented Reasoning for Grounded Multimodal Named Entity Recognition",
    "volume": "main",
    "abstract": "Grounded Multimodal Named Entity Recognition (GMNER), which aims to extract textual entities, their types, and corresponding visual regions from image-text data, has become a critical task in multimodal information extraction. However, existing methods face two major challenges. First, they fail to address the semantic ambiguity caused by polysemy and the long-tail distribution of datasets. Second, unlike visual grounding which provides descriptive phrases, entity grounding only offers brief entity names which carry less semantic information. Current methods lack sufficient semantic interaction between text and image, hindering accurate entity-visual region matching. To tackle these issues, we propose MAKAR, a Multi-Agent framework based Knowledge-Augmented Reasoning, comprising three agents: Knowledge Enhancement, Entity Correction, and Entity Reasoning Grounding. Specifically, in the named entity recognition phase, the Knowledge Enhancement Agent leverages a Multimodal Large Language Model (MLLM) as an implicit knowledge base to enhance ambiguous image-text content with its internal knowledge. For samples with low-confidence entity boundaries and types, the Entity Correction Agent uses web search tools to retrieve and summarize relevant web content, thereby correcting entities using both internal and external knowledge. In the entity grounding phase, the Entity Reasoning Grounding Agent utilizes multi-step Chain-of-Thought reasoning to perform grounding for each entity. Extensive experiments show that MAKAR achieves state-of-the-art performance on two benchmark datasets. Code is available at: https://github.com/Nikol-coder/MAKAR",
    "checked": true,
    "id": "1d8b2d24ecc8feded07bbdfb2bdc7c6b465439e5",
    "semantic_title": "makar: a multi-agent framework based knowledge-augmented reasoning for grounded multimodal named entity recognition",
    "citation_count": 0,
    "authors": [
      "Xinkui Lin",
      "Yuhui Zhang",
      "Yongxiu Xu",
      "Kun Huang",
      "Hongzhang Mu",
      "Yubin Wang",
      "Gaopeng Gou",
      "Li Qian",
      "Li Peng",
      "Wei Liu",
      "Jian Luan",
      "Hongbo Xu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.312": {
    "title": "VisCRA: A Visual Chain Reasoning Attack for Jailbreaking Multimodal Large Language Models",
    "volume": "main",
    "abstract": "The emergence of Multimodal Large Reasoning Models (MLRMs) has enabled sophisticated visual reasoning capabilities by integrating reinforcement learning and Chain-of-Thought (CoT) supervision. However, while these enhanced reasoning capabilities improve performance, they also introduce new and underexplored safety risks. In this work, we systematically investigate the security implications of advanced visual reasoning in MLRMs. Our analysis reveals a fundamental trade-off: as visual reasoning improves, models become more vulnerable to jailbreak attacks. Motivated by this critical finding, we introduce VisCRA (Visual Chain Reasoning Attack), a novel jailbreak framework that exploits the visual reasoning chains to bypass safety mechanisms. VisCRA combines targeted visual attention masking with a two-stage reasoning induction strategy to precisely control harmful outputs. Extensive experiments demonstrate VisCRA's significant effectiveness, achieving high attack success rates on leading closed-source MLRMs: 76.48% on Gemini 2.0 Flash Thinking, 68.56% on QvQ-Max, and 56.60% on GPT-4o. Our findings highlight a critical insight: the very capability that empowers MLRMs — their visual reasoning — can also serve as an attack vector, posing significant security risks. Warning: This paper contains unsafe examples",
    "checked": true,
    "id": "b086c3162d5dcb5b665e22f0102f6396794eadb3",
    "semantic_title": "viscra: a visual chain reasoning attack for jailbreaking multimodal large language models",
    "citation_count": 3,
    "authors": [
      "Bingrui Sima",
      "Linhua Cong",
      "Wenxuan Wang",
      "Kun He"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.313": {
    "title": "Investigating Neurons and Heads in Transformer-based LLMs for Typographical Errors",
    "volume": "main",
    "abstract": "This paper investigates how LLMs encode inputs with typos. We hypothesize that specific neurons and attention heads recognize typos and fix them internally using local and global contexts. We introduce a method to identify typo neurons and typo heads that work actively when inputs contain typos. Our experimental results suggest the following: 1) LLMs can fix typos with local contexts when the typo neurons in either the early or late layers are activated, even if those in the other are not. 2) Typo neurons in the middle layers are the core of typo-fixing with global contexts. 3) Typo heads fix typos by widely considering the context not focusing on specific tokens. 4) Typo neurons and typo heads work not only for typo-fixing but also for understanding general contexts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kohei Tsuji",
      "Tatsuya Hiraoka",
      "Yuchang Cheng",
      "Eiji Aramaki",
      "Tomoya Iwakura"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.314": {
    "title": "LMR-BENCH: Evaluating LLM Agent's Ability on Reproducing Language Modeling Research",
    "volume": "main",
    "abstract": "Large language model (LLM) agents have demonstrated remarkable potential in advancing scientific discovery. However, their capability in the fundamental yet crucial task of reproducing code from research papers, especially in the NLP domain, remains underexplored. This task includes unique complex reasoning challenges in the intellectual synthesis of abstract concepts and the comprehension of code repositories with interdependent files. Motivated by this gap, we present LMR-BENCH, a benchmark designed to systematically evaluate the capability of LLM agents on code reproduction from Language Modeling Research. It consists of 28 code reproduction tasks derived from 23 research papers published in top-tier NLP venues over the past five years, spanning nine fundamental categories. Models are provided with a research paper, a code repository containing one or more masked functions, and instructions for implementing these functions. We conduct extensive experiments in standard prompting and LLM agent settings with state-of-the-art LLMs, evaluating the accuracy of unit tests and performing LLM-based evaluation of code correctness. Experimental results reveal that even the most advanced models still exhibit persistent limitations in scientific reasoning and code synthesis, highlighting critical gaps in LLM agents' ability to autonomously reproduce scientific research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuo Yan",
      "Ruochen Li",
      "Ziming Luo",
      "Zimu Wang",
      "Daoyang Li",
      "Liqiang Jing",
      "Kaiyu He",
      "Peilin Wu",
      "Juntong Ni",
      "George Michalopoulos",
      "Yue Zhang",
      "Ziyang Zhang",
      "Mian Zhang",
      "Zhiyu Chen",
      "Xinya Du"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.315": {
    "title": "RAV: Retrieval-Augmented Voting for Tactile Descriptions Without Training",
    "volume": "main",
    "abstract": "Tactile perception is essential for human-environment interaction, and deriving tactile descriptions from multimodal data is a key challenge for embodied intelligence to understand human perception. Conventional approaches relying on extensive parameter learning for multimodal perception are rigid and computationally inefficient. To address this, we introduce Retrieval-Augmented Voting (RAV), a parameter-free method that constructs visual-tactile cross-modal knowledge directly. RAV retrieves similar visual-tactile data for given visual and tactile inputs and generates tactile descriptions through a voting mechanism. In experiments, we applied three voting strategies, SyncVote, DualVote and WeightVote, achieving performance comparable to large-scale cross-modal models without training. Comparative experiments across datasets of varying quality—defined by annotation accuracy and data diversity—demonstrate that RAV's performance improves with higher-quality data at no additional computational cost. Code, and model checkpoints are opensourced at https://github.com/PluteW/RAV",
    "checked": true,
    "id": "bcda08f6385d1ec66b9d2d38a0d065c2adad88ed",
    "semantic_title": "rav: retrieval-augmented voting for tactile descriptions without training",
    "citation_count": 0,
    "authors": [
      "Jinlin Wang",
      "Yulong Ji",
      "Hongyu Yang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.316": {
    "title": "Static Word Embeddings for Sentence Semantic Representation",
    "volume": "main",
    "abstract": "We propose new static word embeddings optimised for sentence semantic representation. We first extract word embeddings from a pre-trained Sentence Transformer, and improve them with sentence-level principal component analysis, followed by either knowledge distillation or contrastive learning. During inference, we represent sentences by simply averaging word embeddings, which requires little computational cost. We evaluate models on both monolingual and cross-lingual tasks and show that our model substantially outperforms existing static models on sentence semantic tasks, and even surpasses a basic Sentence Transformer model (SimCSE) on a text embedding benchmark. Lastly, we perform a variety of analyses and show that our method successfully removes word embedding components that are not highly relevant to sentence semantics, and adjusts the vector norms based on the influence of words on sentence semantics",
    "checked": true,
    "id": "490b03ab0d78b690800709a75edc8f36c55655e0",
    "semantic_title": "static word embeddings for sentence semantic representation",
    "citation_count": 0,
    "authors": [
      "Takashi Wada",
      "Yuki Hirakawa",
      "Ryotaro Shimizu",
      "Takahiro Kawashima",
      "Yuki Saito"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.317": {
    "title": "PropRAG: Guiding Retrieval with Beam Search over Proposition Paths",
    "volume": "main",
    "abstract": "Retrieval Augmented Generation (RAG) has become the standard approach for equipping Large Language Models (LLMs) with up-to-date knowledge. However, standard RAG, relying on independent passage retrieval, often fails to capture the interconnected nature of information required for complex, multi-hop reasoning. While structured RAG methods attempt to address this using knowledge graphs built from triples, we argue that the inherent context loss of triples (context collapse) limits the fidelity of the knowledge representation. We introduce PropRAG, a novel RAG framework that shifts from triples to context-rich propositions and introduces an efficient, LLM-free online beam search over proposition paths to discover multi-step reasoning chains. By coupling a higher-fidelity knowledge representation with explicit path discovery, PropRAG achieves state-of-the-art zero-shot Recall@5 and F1 scores on 2Wiki, HotpotQA, and MuSiQue, advancing non-parametric knowledge integration by improving evidence retrieval through richer representation and efficient reasoning path discovery",
    "checked": true,
    "id": "aae1d8e184c306b90cb51b205b8cb7ef718ebcdf",
    "semantic_title": "proprag: guiding retrieval with beam search over proposition paths",
    "citation_count": 3,
    "authors": [
      "Jingjin Wang",
      "Jiawei Han"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.318": {
    "title": "Rethinking Backdoor Detection Evaluation for Language Models",
    "volume": "main",
    "abstract": "Backdoor attacks, in which a model behaves maliciously when given an attacker-specified trigger, pose a major security risk for practitioners who depend on publicly released language models. As a countermeasure, backdoor detection methods aim to detect whether a released model contains a backdoor. While existing backdoor detection methods have high accuracy in detecting backdoored models on standard benchmarks, it is unclear whether they can robustly identify backdoors in the wild. In this paper, we examine the robustness of backdoor detectors by manipulating different factors during backdoor planting. We find that the success of existing methods based on trigger inversion or meta classifiers highly depends on how intensely the model is trained on poisoned data. Specifically, backdoors planted with more aggressive or more conservative training are significantly more difficult to detect than the default ones. Our results highlight a lack of robustness of existing backdoor detectors and the limitations in current benchmark construction",
    "checked": true,
    "id": "ba0e60e3bd030727949c8d4b60ba0e1de93b6131",
    "semantic_title": "rethinking backdoor detection evaluation for language models",
    "citation_count": 4,
    "authors": [
      "Jun Yan",
      "Wenjie Jacky Mo",
      "Xiang Ren",
      "Robin Jia"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.319": {
    "title": "Glider: Global and Local Instruction-Driven Expert Router",
    "volume": "main",
    "abstract": "The development of performant pre-trained models has driven the advancement of routing-based expert models tailored to specific tasks. However, these methods often favor generalization over performance on held-in tasks. This limitation adversely impacts practical applicability, as real-world deployments require robust performance across both known and novel tasks. We observe that current token-level routing mechanisms neglect the global semantic context of the input task. To address this, we propose a novel method, Global and Local Instruction Driven Expert Router (GLIDER) that proposes a multi-scale routing mechanism, encompassing a semantic global router and a learned local router. The global router leverages recent LLMs' semantic reasoning capabilities to generate task-specific instructions from the input query, guiding expert selection across all layers. This global guidance is complemented by a local router that facilitates token-level routing decisions within each module, enabling finer control and enhanced performance on unseen and challenging tasks. Our experiments using T5-based expert models for T0 and FLAN tasks demonstrate that Glider achieves substantially improved held-in performance while maintaining strong generalization on held-out tasks. Additionally, we perform ablations experiments to dive deeper into the components of Glider and plot routing distributions to show that Glider can effectively retrieve the correct expert for held-in tasks while also demonstrating compositional capabilities for held-out tasks. Our experiments highlight the importance of our multi-scale routing that leverages LLM-driven semantic reasoning for MoErging methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pingzhi Li",
      "Prateek Yadav",
      "Jaehong Yoon",
      "Jie Peng",
      "Yi-Lin Sung",
      "Mohit Bansal",
      "Tianlong Chen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.320": {
    "title": "CoVoGER: A Multilingual Multitask Benchmark for Speech-to-text Generative Error Correction with Large Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) can rewrite the N-best hypotheses from a speech-to-text model, often fixing recognition or translation errors that traditional rescoring cannot. Yet research on generative error correction (GER) has been focusing on monolingual automatic speech recognition (ASR), leaving its multilingual and multitask potential underexplored. We introduce CoVoGER, a benchmark for GER that covers both ASR and speech-to-text translation (ST) across 15 languages and 28 language pairs. CoVoGER is constructed by decoding Common Voice 20.0 and CoVoST-2 with Whisper of three model sizes and SeamlessM4T of two model sizes, providing 5-best lists obtained via a mixture of beam search and temperature sampling. We evaluated various instruction-tuned LLMs, including commercial models in zero-shot mode and open-sourced models with LoRA fine-tuning, and found that the mixture decoding strategy yields the best GER performance in most settings. CoVoGER will be released to promote research on reliable language-universal speech-to-text GER. The code and data for the benchmark are available at https://github.com/N-Orien/CoVoGER",
    "checked": true,
    "id": "10879465ddd1ce6a6d69c8e8bc712be9332988a3",
    "semantic_title": "covoger: a multilingual multitask benchmark for speech-to-text generative error correction with large language models",
    "citation_count": 0,
    "authors": [
      "Zhengdong Yang",
      "Zhen Wan",
      "Sheng Li",
      "Chao-Han Huck Yang",
      "Chenhui Chu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.321": {
    "title": "Tiny Budgets, Big Gains: Parameter Placement Strategy in Parameter Super-Efficient Fine-Tuning",
    "volume": "main",
    "abstract": "In this work, we propose FoRA-UA, a novel method that, using only 1–5% of the standard LoRA's parameters, achieves state-of-the-art performance across a wide range of tasks. Specifically, we explore scenarios with extremely limited parameter budgets and derive two key insights: (1) fix-sized sparse frequency representations approximate small matrices more accurately; and (2) with a fixed number of trainable parameters, introducing a smaller intermediate representation to approximate larger matrices results in lower construction error. These findings form the foundation of our FoRA-UA method. By inserting a small intermediate parameter set, we achieve greater model compression without sacrificing performance. We evaluate FoRA-UA across diverse tasks, including natural language understanding (NLU), natural language generation (NLG), instruction tuning, and image classification, demonstrating strong generalisation and robustness under extreme compression",
    "checked": true,
    "id": "a4f2d625033d5a629953e372b351f2d9ec222196",
    "semantic_title": "tiny budgets, big gains: parameter placement strategy in parameter super-efficient fine-tuning",
    "citation_count": 0,
    "authors": [
      "Jinman Zhao",
      "Xueyan Zhang",
      "Jiaru Li",
      "Jingcheng Niu",
      "Yulan Hu",
      "Erxue Min",
      "Gerald Penn"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.322": {
    "title": "Legal Fact Prediction: The Missing Piece in Legal Judgment Prediction",
    "volume": "main",
    "abstract": "Legal judgment prediction (LJP), which enables litigants and their lawyers to forecast judgment outcomes and refine litigation strategies, has emerged as a crucial legal NLP task. Existing studies typically utilize legal facts, i.e., facts that have been established by evidence and determined by the judge, to predict the judgment. However, legal facts are often difficult to obtain in the early stages of litigation, significantly limiting the practical applicability of fact-based LJP. To address this limitation, we propose a novel legal NLP task: legal fact prediction (LFP), which takes the evidence submitted by litigants for trial as input to predict legal facts, thereby empowering fact-based LJP technologies to make predictions in the absence of ground-truth legal facts. We also propose the first benchmark dataset, LFPBench, for evaluating the LFP task. Our extensive experiments on LFPBench demonstrate the effectiveness of LFP-empowered LJP and highlight promising research directions for LFP",
    "checked": true,
    "id": "fcca17c0cbcc6404f63a57eae03460415d7fbe06",
    "semantic_title": "legal fact prediction: the missing piece in legal judgment prediction",
    "citation_count": 0,
    "authors": [
      "Junkai Liu",
      "Yujie Tong",
      "Hui Huang",
      "Bowen Zheng",
      "Yiran Hu",
      "Peicheng Wu",
      "Chuan Xiao",
      "Makoto Onizuka",
      "Muyun Yang",
      "Shuyuan Zheng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.323": {
    "title": "DAMON: A Dialogue-Aware MCTS Framework for Jailbreaking Large Language Models",
    "volume": "main",
    "abstract": "While large language models (LLMs) demonstrate remarkable capabilities across a wide range of tasks, they remain vulnerable to generating outputs that are potentially harmful. Red teaming, which involves crafting adversarial inputs to expose vulnerabilities, is a widely adopted approach for evaluating the robustness of these models. Prior studies have indicated that LLMs are susceptible to vulnerabilities exposed through multi-turn interactions as opposed to single-turn scenarios. Nevertheless, existing methods for multi-turn attacks mainly utilize a predefined dialogue pattern, limiting their effectiveness in realistic situations. Effective attacks require adaptive dialogue strategies that respond dynamically to the initial user prompt and the evolving context of the conversation. To address these limitations, we propose DAMON, a novel multi-turn jailbreak attack method. DAMON leverages Monte Carlo Tree Search (MCTS) to systematically explore multi-turn conversational spaces, efficiently identifying sub-instruction sequences that induce harmful responses. We evaluate DAMON's efficacy across five LLMs and three datasets. Our experimental results show that DAMON can effectively induce undesired behaviors",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xu Zhang",
      "Xunjian Yin",
      "Dinghao Jing",
      "Huixuan Zhang",
      "Xinyu Hu",
      "Xiaojun Wan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.324": {
    "title": "Multilingual Prompting for Improving LLM Generation Diversity",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) are known to lack cultural representation and overall diversity in their generations, from expressing opinions to answering factual questions. To mitigate this problem, we propose multilingual prompting: a prompting method which generates several variations of a base prompt with added cultural and linguistic cues from several cultures, generates responses, and then combines the results. Building on evidence that LLMs have language-specific knowledge, multilingual prompting seeks to increase diversity by activating a broader range of cultural knowledge embedded in model training data. Through experiments across multiple models (GPT-4o, GPT-4o-mini, LLaMA 70B, and LLaMA 8B), we show that multilingual prompting consistently outperforms existing diversity-enhancing techniques such as high-temperature sampling, step-by-step recall, and persona prompting. Further analyses show that the benefits of multilingual prompting vary between high and low resource languages and across model sizes, and that aligning the prompting language with cultural cues reduces hallucination about culturally-specific information",
    "checked": true,
    "id": "679b3ea1c933931e3f278fa87532ef1fbe2d369d",
    "semantic_title": "multilingual prompting for improving llm generation diversity",
    "citation_count": 4,
    "authors": [
      "Qihan Wang",
      "Shidong Pan",
      "Tal Linzen",
      "Emily Black"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.325": {
    "title": "MOSAIC: Modeling Social AI for Content Dissemination and Regulation in Multi-Agent Simulations",
    "volume": "main",
    "abstract": "We present a novel, open-source social network simulation framework, MOSAIC, where generative language agents predict user behaviors such as liking, sharing, and flagging content. This simulation combines LLM agents with a directed social graph to analyze emergent deception behaviors and gain a better understanding of how users determine the veracity of online social content. By constructing user representations from diverse fine-grained personas, our system enables multi-agent simulations that model content dissemination and engagement dynamics at scale. Within this framework, we evaluate three different content moderation strategies with simulated misinformation dissemination, and we find that they not only mitigate the spread of non-factual content but also increase user engagement. In addition, we analyze the trajectories of popular content in our simulations, and explore whether simulation agents' articulated reasoning for their social interactions truly aligns with their collective engagement patterns",
    "checked": true,
    "id": "cc7d423fd51ed55699f648c349db9afa2e80e190",
    "semantic_title": "mosaic: modeling social ai for content dissemination and regulation in multi-agent simulations",
    "citation_count": 4,
    "authors": [
      "Genglin Liu",
      "Vivian T. Le",
      "Salman Rahman",
      "Elisa Kreiss",
      "Marzyeh Ghassemi",
      "Saadia Gabriel"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.326": {
    "title": "Identification of Multiple Logical Interpretations in Counter-Arguments",
    "volume": "main",
    "abstract": "Counter-arguments (CAs) are a good means to improve the critical-thinking skills of learners, especially given that one has to thoroughly consider the logic of initial arguments (IA) when composing their CA. Although several tasks have been created for identifying the logical structure of CAs, no prior work has focused on capturing multiple interpretations of logical structures due to their complexity. In this work, we create CALSA+, a dataset consisting of 134 CAs annotated with 13 logical predicate questions. CALSA+ contains 1,742 instances annotated by 3 expert annotators (5,226 total annotations) with good agreement (Krippendorff 𝛼=0.46). Using CALSA+, we train a model with Reinforcement Learning with Verifiable Rewards (RLVR) to identify multiple logical interpretations and show that models trained with RLVR can perform on par with much bigger proprietary models. Our work is the first to attempt to annotate all the interpretations of logical structure on top of CAs. We publicly release our dataset to facilitate research in CA logical structure identification",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenzhi Wang",
      "Paul Reisert",
      "Shoichi Naito",
      "Naoya Inoue",
      "Machi Shimmei",
      "Surawat Pothong",
      "Jungmin Choi",
      "Kentaro Inui"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.327": {
    "title": "LyapLock: Bounded Knowledge Preservation in Sequential Large Language Model Editing",
    "volume": "main",
    "abstract": "Large Language Models often contain factually incorrect or outdated knowledge, giving rise to model editing methods for precise knowledge updates. However, current mainstream locate-then-edit approaches exhibit a progressive performance decline during sequential editing, due to inadequate mechanisms for long-term knowledge preservation. To tackle this, we model the sequential editing as a constrained stochastic programming. Given the challenges posed by the cumulative preservation error constraint and the gradually revealed editing tasks, **LyapLock** is proposed. It integrates queuing theory and Lyapunov optimization to decompose the long-term constrained programming into tractable stepwise subproblems for efficient solving. This is the first model editing framework with rigorous theoretical guarantees, achieving asymptotic optimal editing performance while meeting the constraints of long-term knowledge preservation. Experimental results show that our framework scales sequential editing capacity to over 10,000 edits while stabilizing general capabilities and boosting average editing efficacy by 11.89% over SOTA baselines. Furthermore, it can be leveraged to enhance the performance of baseline methods. Our code is released on https://github.com/caskcsg/LyapLock",
    "checked": true,
    "id": "6192c43289a8aab67ba0303d69556cb275a9b2d6",
    "semantic_title": "lyaplock: bounded knowledge preservation in sequential large language model editing",
    "citation_count": 0,
    "authors": [
      "Peng Wang",
      "Biyu Zhou",
      "Xuehai Tang",
      "Jizhong Han",
      "Songlin Hu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.328": {
    "title": "AlignX: Advancing Multilingual Large Language Models with Multilingual Representation Alignment",
    "volume": "main",
    "abstract": "Multilingual large language models (LLMs) possess impressive multilingual understanding and generation capabilities. However, their performance and cross-lingual alignment often lag for non-dominant languages. A common solution is to fine-tune LLMs on large-scale and more balanced multilingual corpus, but such approaches often lead to imprecise alignment and suboptimal knowledge transfer, struggling with limited improvements across languages. In this paper, we propose AlignX to bridge the multilingual performance gap, which is a two-stage representation-level framework for enhancing multilingual performance of pre-trained LLMs. In the first stage, we align multilingual representations with multilingual semantic alignment and language feature integration. In the second stage, we stimulate the multilingual capability of LLMs via multilingual instruction fine-tuning. Experimental results on several pre-trained LLMs demonstrate that our approach enhances LLMs' multilingual general and cross-lingual generation capability. Further analysis indicates that AlignX brings the multilingual representations closer and improves the cross-lingual alignment",
    "checked": true,
    "id": "65f44fd4ec1466761dbc4700ce594db0b4639321",
    "semantic_title": "alignx: advancing multilingual large language models with multilingual representation alignment",
    "citation_count": 0,
    "authors": [
      "Mengyu Bu",
      "Shaolei Zhang",
      "Zhongjun He",
      "Hua Wu",
      "Yang Feng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.329": {
    "title": "What Makes a Good Reasoning Chain? Uncovering Structural Patterns in Long Chain-of-Thought Reasoning",
    "volume": "main",
    "abstract": "Recent advances in reasoning with large language models (LLMs) have popularized Long Chain-of-Thought (LCoT), a strategy that encourages deliberate and step-by-step reasoning before producing a final answer. While LCoTs have enabled expert-level performance in complex tasks, how the internal structures of their reasoning chains drive, or even predict, the correctness of final answers remains a critical yet underexplored question. In this work, we present LCoT2Tree, an automated framework that converts sequential LCoTs into hierarchical tree structures and thus enables deeper structural analysis of LLM reasoning. Using graph neural networks (GNNs), we reveal that structural patterns extracted by LCoT2Tree, including exploration, backtracking, and verification, serve as stronger predictors of final performance across a wide range of tasks and models. Leveraging an explainability technique, we further identify critical thought patterns such as over-branching that account for failures. Beyond diagnostic insights, the structural patterns by LCoT2Tree support practical applications, including improving Best-of-N decoding effectiveness. Overall, our results underscore the critical role of internal structures of reasoning chains, positioning LCoT2Tree as a powerful tool for diagnosing, interpreting, and improving reasoning in LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gangwei Jiang",
      "Yahui Liu",
      "Zhaoyi Li",
      "Wei Bi",
      "Fuzheng Zhang",
      "Linqi Song",
      "Ying Wei",
      "Defu Lian"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.330": {
    "title": "HD-PiSSA: High-Rank Distributed Orthogonal Adaptation",
    "volume": "main",
    "abstract": "Existing parameter-efficient fine-tuning (PEFT) methods for large language models (LLMs), such as LoRA and PiSSA, constrain model updates to low-rank subspaces, limiting their expressiveness and leading to suboptimal performance on complex tasks. To address this, we introduce **H**igh-rank **D**istributed **PiSSA (HD-PiSSA)**, a distributed PEFT approach that initializes **orthogonal adapters** across different devices and aggregates their delta updates collectively on (W) for fine-tuning. Unlike Data Parallel LoRA or PiSSA, which maintain identical adapters across all devices, HD-PiSSA assigns different principal components of the pre-trained weights to each GPU, significantly expanding the range of update directions. This results in over 16× higher effective updated ranks than data-parallel LoRA or PiSSA when fine-tuning on 8 GPUs with the same per-device adapter rank. Empirically, HD-PiSSA benefits from this extra optimization flexibility and outperforms both LoRA and PiSSA across a variety of challenging downstream tasks, including mathematics, code, and multi-task learning",
    "checked": true,
    "id": "299fe51fac7cc0f345c9c3dc46a7d17e69690a91",
    "semantic_title": "hd-pissa: high-rank distributed orthogonal adaptation",
    "citation_count": 0,
    "authors": [
      "Yiding Wang",
      "Fanxu Meng",
      "Xuefeng Zhang",
      "Fan Jiang",
      "Pingzhi Tang",
      "Muhan Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.331": {
    "title": "Firewall Routing: Blocking Leads to Better Hybrid Inference for LLMs",
    "volume": "main",
    "abstract": "The rapid advancement of Large Language Models (LLMs) has significantly enhanced performance across various natural language processing (NLP) tasks, yet the high computational costs and latency associated with deploying such models continue to pose critical bottlenecks, limiting their broader applicability. To mitigate these challenges, we propose a dynamic hybrid inference framework, Firewall Routing, which efficiently selects between a strong and a weak LLMs based on the complexity of the query. A lightweight routing model is trained to optimize resource allocation by learning from response quality and preventing long-tail queries, which are often too hard to solve by LLMs, from being routed to the stronger model. Moreover, our method incorporates multiple sampling to enhance query evaluation reliability while leveraging Hard Blocking and Soft Blocking to handle long-tail queries along with refining labels for model selection. Extensive experiments show our method outperforms existing routing strategies by up to 5.29% in APGR, demonstrating state-of-the-art performance across multiple benchmarks",
    "checked": true,
    "id": "723a71a45929a3d8860f30eb97dfcfacfbd3b7c2",
    "semantic_title": "firewall routing: blocking leads to better hybrid inference for llms",
    "citation_count": 0,
    "authors": [
      "Runyu Peng",
      "Yunhua Zhou",
      "Kai Lv",
      "Yang Gao",
      "Qipeng Guo",
      "Xipeng Qiu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.332": {
    "title": "SPE Attention: Making Attention Equivariant to Semantic-Preserving Permutation for Code Processing",
    "volume": "main",
    "abstract": "Codes serve as the fundamental language for human to communicate with machines, and various Transformer-based models are trained to process codes in recent advancements. A unique symmetry of code is its semantic-preserving permutation, which allows certain lines to be rearranged without altering the overall meaning. To capture such symmetry, we propose a novel attention mechanism that incorporates semantic-preserving permutation equivariance, called the SPE attention. By leveraging the symmetry relationships within code, we introduce a directed layered graph to represent the code structure, and this graph is then summarized into a symmetry mask. The SPE attention integrates those symmetry masks, granting semantic-preserving permutations equivariance to the model. Experiments on various code related tasks, including code summarization and error detection, demonstrate the effectiveness of the proposed SPE attention",
    "checked": true,
    "id": "2fd8af6bd8f0887808813eda7bdb8e65bdc0a4d4",
    "semantic_title": "spe attention: making attention equivariant to semantic-preserving permutation for code processing",
    "citation_count": 0,
    "authors": [
      "Chengyu Jiao",
      "Shuhao Chen",
      "Yu Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.333": {
    "title": "Audio-centric Video Understanding Benchmark without Text Shortcut",
    "volume": "main",
    "abstract": "Audio often serves as an auxiliary modality in video understanding tasks of audio-visual large language models (LLMs), merely assisting in the comprehension of visual information. However, a thorough understanding of videos significantly depends on auditory information, as audio offers critical context, emotional cues, and semantic meaning that visual data alone often lacks. This paper proposes an audio-centric video understanding benchmark (AVUT) to evaluate the video comprehension capabilities of multimodal LLMs with a particular focus on auditory information. AVUT introduces a suite of carefully designed audio-centric tasks, holistically testing the understanding of both audio content and audio-visual interactions in videos. Moreover, this work points out the text shortcut problem that largely exists in other benchmarks where the correct answer can be found from question text alone without needing videos. AVUT addresses this problem by proposing a answer permutation-based filtering mechanism.A thorough evaluation across a diverse range of open-source and proprietary multimodal LLMs is performed, followed by the analyses of deficiencies in audio-visual LLMs. Demos and data are available at https://github.com/lark-png/AVUT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yudong Yang",
      "Jimin Zhuang",
      "Guangzhi Sun",
      "Changli Tang",
      "Yixuan Li",
      "Peihan Li",
      "Yifan Jiang",
      "Wei Li",
      "Zejun Ma",
      "Chao Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.334": {
    "title": "TurboRAG: Accelerating Retrieval-Augmented Generation with Precomputed KV Caches for Chunked Text",
    "volume": "main",
    "abstract": "Current Retrieval-Augmented Generation (RAG) systems concatenate and process numerous retrieved document chunks for prefill which requires a large volume of computation, therefore leading to significant latency in time-to-first-token (TTFT). To reduce the computation overhead as well as TTFT, we introduce TurboRAG, a hybrid offline–online paradigm that (i) pre‐computes chunk‐level key-value (KV) caches, (ii) stitches them together at inference time using independent–attention and reordered‐RoPE techniques, and (iii) preserves answer quality without changing the model architecture. Hence, online computation of KV caches is eliminated during inference. Our approach is applicable to most existing large language models and their applications without any requirement in modification of models and inference systems. Experimental results across a suite of RAG benchmarks demonstrate that TurboRAG reduces TTFT by up to 9.4x compared to the conventional RAG systems (on an average of 8.6x), but reserving comparable performance to the standard RAG systems",
    "checked": true,
    "id": "650f7db2b37069614c0fb04ea77f099bb5d4efa5",
    "semantic_title": "turborag: accelerating retrieval-augmented generation with precomputed kv caches for chunked text",
    "citation_count": 27,
    "authors": [
      "Songshuo Lu",
      "Hua Wang",
      "Yutian Rong",
      "Zhi Chen",
      "Yaohua Tang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.335": {
    "title": "ZoomEye: Enhancing Multimodal LLMs with Human-Like Zooming Capabilities through Tree-Based Image Exploration",
    "volume": "main",
    "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities in vision-language understanding. Recently, with the integration of test-time scaling techniques, these models have also shown strong potential in visual reasoning. However, most existing reasoning approaches remain text-level in nature: MLLMs are prompted to explore various combinations of textual tokens via their underlying language model, while the visual input remains fixed throughout the reasoning process. This paradigm limits the model's ability to fully exploit rich visual information, particularly when dealing with images containing numerous fine-grained elements. In such cases, vision-level reasoning becomes crucial—where models dynamically zoom into specific regions of the image to gather detailed visual cues necessary for accurate decision-making. In this paper, we propose Zoom Eye, a training-free, model-agnostic tree search algorithm tailored for vision-level reasoning. Zoom Eye treats an image as a hierarchical tree structure, where each child node represents a zoomed-in sub-region of its parent, and the root corresponds to the full image. The algorithm enables MLLMs to simulate human-like zooming behavior by navigating from root to leaf nodes in search of task-relevant visual evidence. We experiment on a series of elaborate high-resolution benchmarks and the results demonstrate that Zoom Eye not only consistently improves the performance of a series of MLLMs with large margin (e.g., InternVL2.5-8B increases by 15.71% and 17.69% on HR-Bench) but also enables small 3-8B MLLMs to outperform strong large models such as GPT-4o",
    "checked": true,
    "id": "bf790cb8a95da05dcafd6bf1a4d02efce784720b",
    "semantic_title": "zoomeye: enhancing multimodal llms with human-like zooming capabilities through tree-based image exploration",
    "citation_count": 29,
    "authors": [
      "Haozhan Shen",
      "Kangjia Zhao",
      "Tiancheng Zhao",
      "Ruochen Xu",
      "Zilun Zhang",
      "Mingwei Zhu",
      "Jianwei Yin"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.336": {
    "title": "Learning Like Humans: Advancing LLM Reasoning Capabilities via Adaptive Difficulty Curriculum Learning and Expert-Guided Self-Reformulation",
    "volume": "main",
    "abstract": "Despite impressive progress in areas like mathematical reasoning, large language models still face challenges in consistently solving complex problems. Drawing inspiration from key human learning strategies, we propose two novel strategies to enhance the capability of large language models to solve these complex problems. First, Adaptive Difficulty Curriculum Learning (ADCL) is a novel curriculum learning strategy that tackles the Difficulty Shift phenomenon (i.e., a model's perception of problem difficulty dynamically changes during training) by periodically re-estimating difficulty within upcoming data batches to maintain alignment with the model's evolving capabilities. Second, Expert-Guided Self-Reformulation (EGSR) is a novel reinforcement learning strategy that bridges the gap between imitation learning and pure exploration by guiding models to reformulate expert solutions within their own conceptual framework, rather than relying on direct imitation, fostering deeper understanding and knowledge assimilation. Extensive experiments on challenging mathematical reasoning benchmarks, using Qwen2.5-7B as the base model, demonstrate that these human-inspired strategies synergistically and significantly enhance performance. Notably, their combined application improves performance over the standard Zero-RL baseline by 10% on the AIME24 benchmark and 16.6% on AIME25",
    "checked": true,
    "id": "99195e486c8d7057e91efcc6660cf386180c6d99",
    "semantic_title": "learning like humans: advancing llm reasoning capabilities via adaptive difficulty curriculum learning and expert-guided self-reformulation",
    "citation_count": 2,
    "authors": [
      "Enci Zhang",
      "Xingang Yan",
      "Wei Lin",
      "Tianxiang. Zhang",
      "Lu Qianchun"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.337": {
    "title": "VersaTune: An Efficient Data Composition Framework for Training Multi-Capability LLMs",
    "volume": "main",
    "abstract": "As demonstrated by the proprietary Large Language Models (LLMs) such as GPT and Claude series, LLMs have the potential to achieve remarkable proficiency across a wide range of domains, including law, medicine, finance, science, code, etc., all within a single model. These capabilities are further augmented during the Supervised Fine-Tuning (SFT) phase. Despite their potential, existing work mainly focuses on domain-specific enhancements during fine-tuning, the challenge of which lies in catastrophic forgetting of knowledge across other domains. In this study, we introduce **VersaTune**, a novel data composition framework designed for enhancing LLMs' overall multi-domain capabilities during training. We begin with detecting the distribution of domain-specific knowledge within the base model, followed by the training data composition that aligns with the model's existing knowledge distribution. During the subsequent training process, domain weights are dynamically adjusted based on their learnable potential and forgetting degree. Experimental results indicate that VersaTune is effective in multi-domain fostering, with an improvement of 29.77% in the overall multi-ability performances compared to uniform domain weights. Furthermore, we find that Qwen-2.5-32B + VersaTune even surpasses frontier models, including GPT-4o, Claude3.5-Sonnet and DeepSeek-V3 by 0.86%, 4.76% and 4.60%. Additionally, in scenarios where flexible expansion of a specific domain is required, VersaTune reduces the performance degradation in other domains by 38.77%, while preserving the training efficacy of the target domain",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keer Lu",
      "Keshi Zhao",
      "Zhuoran Zhang",
      "Zheng Liang",
      "Bin Cui",
      "Tengjiao Wang",
      "Wentao Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.338": {
    "title": "FlightGPT: Towards Generalizable and Interpretable UAV Vision-and-Language Navigation with Vision-Language Models",
    "volume": "main",
    "abstract": "Unmanned Aerial Vehicle (UAV) Vision-and-Language Navigation (VLN) is vital for applications such as disaster response, logistics delivery, and urban inspection. However, existing methods often struggle with insufficient multimodal fusion, weak generalization, and poor interpretability. To address these challenges, we propose FlightGPT, a novel UAV VLN framework built upon Vision-Language Models (VLMs) with powerful multimodal perception capabilities. We design a two-stage training pipeline: first, Supervised Fine-Tuning (SFT) using high-quality demonstrations to improve initialization and structured reasoning; then, Group Relative Policy Optimization (GRPO) algorithm, guided by a composite reward that considers goal accuracy, reasoning quality, and format compliance, to enhance generalization and adaptability. Furthermore, FlightGPT introduces a Chain-of-Thought (CoT)-based reasoning mechanism to improve decision interpretability. Extensive experiments on the city-scale dataset CityNav demonstrate that FlightGPT achieves state-of-the-art performance across all scenarios, with a 9.22% higher success rate than the strongest baseline in unseen environments. Our implementation is publicly available",
    "checked": true,
    "id": "def046b6591504dfe06108a794025291b166b98d",
    "semantic_title": "flightgpt: towards generalizable and interpretable uav vision-and-language navigation with vision-language models",
    "citation_count": 1,
    "authors": [
      "Hengxing Cai",
      "Jinhan Dong",
      "Jingjun Tan",
      "Jingcheng Deng",
      "Sihang Li",
      "Zhifeng Gao",
      "Haidong Wang",
      "Zicheng Su",
      "Agachai Sumalee",
      "Renxin Zhong"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.339": {
    "title": "Multimodal Language Models See Better When They Look Shallower",
    "volume": "main",
    "abstract": "Multimodal large language models (MLLMs) typically extract visual features from the final layers of a pretrained Vision Transformer (ViT). This widespread deep-layer bias, however, is largely driven by empirical convention rather than principled analysis. While prior studies suggest that different ViT layers capture different types of information—shallower layers focusing on fine visual details and deeper layers aligning more closely with textual semantics, the impact of this variation on MLLM performance remains underexplored. We present the first comprehensive study of visual layer selection for MLLMs, analyzing representation similarity across ViT layers to establish shallow, middle, and deep layer groupings. Through extensive evaluation of MLLMs (1.4B–7B parameters) across 10 benchmarks encompassing 60+ tasks, we find that while deep layers excel in semantic-rich tasks like OCR, shallow and middle layers significantly outperform them on fine-grained visual tasks including counting, positioning, and object localization. Building on these insights, we propose a lightweight feature fusion method that strategically incorporates shallower layers, achieving consistent improvements over both single-layer and specialized fusion baselines. Our work offers the first principled study of visual layer selection in MLLMs, showing that MLLMs can often see better when they look shallower",
    "checked": true,
    "id": "91349b900c24b1fb0f02a6f93df8b788891b6523",
    "semantic_title": "multimodal language models see better when they look shallower",
    "citation_count": 4,
    "authors": [
      "Haoran Chen",
      "Junyan Lin",
      "Xinghao Chen",
      "Yue Fan",
      "Jianfeng Dong",
      "Xin Jin",
      "Hui Su",
      "Jinlan Fu",
      "Xiaoyu Shen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.340": {
    "title": "LoSiA: Efficient High-Rank Fine-Tuning via Subnet Localization and Optimization",
    "volume": "main",
    "abstract": "Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA, significantly reduce the number of trainable parameters by introducing low-rank decomposition matrices. However, existing methods perform extensive matrix multiplications in domain specialization tasks, resulting in computational inefficiency and sub-optimal fine-tuning performance. Hence, we propose LoSiA (**Lo**w-Resources **S**ubnet **I**ntegration **A**daptation), an innovative method that dynamically localizes and optimizes critical parameters during the training process. Specifically, it identifies a sub-network using gradient sparsity analysis and optimizes it as the trainable target. This design enables effective high-rank adaptation by updating only the sub-network parameters, reducing the additional matrix multiplication. We also present LoSiA-Pro, a faster implementation of LoSiA, which reduces the training latency by about 27% compared to LoRA. Extensive evaluations show that our method achieves minimal performance drop compared to full fine-tuning, while requiring the least training time across domain specialization and common-sense reasoning tasks. Further analysis shows that LoSiA also reduces forgetting during continued training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xujia Wang",
      "Yunjia Qi",
      "Bin Xu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.341": {
    "title": "Invisible Entropy: Towards Safe and Efficient Low-Entropy LLM Watermarking",
    "volume": "main",
    "abstract": "Logit-based LLM watermarking traces and verifies AI-generated content by maintaining green and red token lists and increasing the likelihood of green tokens during generation. However, it struggles in low-entropy scenarios, where predictable outputs make green token selection difficult without disrupting natural text flow. Existing approaches address this by assuming access to the original LLM to calculate entropy and selectively watermark high-entropy tokens. However, these methods face two major challenges: (1) high computational costs and detection delays due to reliance on the original LLM, and (2) potential risks of model leakage. To address these limitations, we propose Invisible Entropy (IE), a watermarking paradigm designed to enhance both safety and efficiency. Instead of relying on the original LLM, IE introduces a lightweight feature extractor and an entropy tagger to predict whether the entropy of the next token is high or low. Furthermore, based on theoretical analysis, we developed a threshold navigator that adaptively sets entropy thresholds. It identifies a threshold where the watermark ratio decreases as the green token count increases, enhancing the naturalness of the watermarked text and improving detection robustness. Experiments on HumanEval and MBPP datasets demonstrate that IE reduces parameter size by 99% while achieving performance on par with state-of-the-art methods: https://anonymous.4open.science/r/IE-Official",
    "checked": true,
    "id": "ccbf55e59c372cebb8629e2695d5da20bc8bbf4a",
    "semantic_title": "invisible entropy: towards safe and efficient low-entropy llm watermarking",
    "citation_count": 1,
    "authors": [
      "Tianle Gu",
      "Zongqi Wang",
      "Kexin Huang",
      "Yuanqi Yao",
      "Xiangliang Zhang",
      "Yujiu Yang",
      "Xiuying Chen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.342": {
    "title": "Measuring Bias or Measuring the Task: Understanding the Brittle Nature of LLM Gender Biases",
    "volume": "main",
    "abstract": "As LLMs are increasingly applied in socially impactful settings, concerns about gender bias have prompted growing efforts both to measure and mitigate such bias. These efforts often rely on evaluation tasks that differ from natural language distributions, as they typically involve carefully constructed task prompts that overtly or covertly signal the presence of gender bias-related content. In this paper, we examine how signaling the evaluative purpose of a task impacts measured gender bias in LLMs.Concretely, we test models under prompt conditions that (1) make the testing context salient, and (2) make gender-focused content salient. We then assess prompt sensitivity across four task formats with both token-probability and discrete-choice metrics. We find that prompts that more clearly align with (gender bias) evaluation framing elicit distinct gender output distributions compared to less evaluation-framed prompts. Discrete-choice metrics further tend to amplify bias relative to probabilistic measures. These findings do not only highlight the brittleness of LLM gender bias evaluations but open a new puzzle for the NLP benchmarking and development community: To what extent can well-controlled testing designs trigger LLM testing mode performance, and what does this mean for the ecological validity of future benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bufan Gao",
      "Elisa Kreiss"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.343": {
    "title": "Alignment-Augmented Speculative Decoding with Alignment Sampling and Conditional Verification",
    "volume": "main",
    "abstract": "Recent works have revealed the great potential of speculative decoding in accelerating the autoregressive generation process of large language models. The success of these methods relies on the alignment between draft candidates and the sampled outputs of the target model. Existing methods mainly achieve draft-target alignment with training-based methods, e.g., EAGLE, Medusa, involving considerable training costs. In this paper, we present a training-free alignment-augmented speculative decoding algorithm. We propose alignment sampling, which leverages output distribution obtained in the prefilling phase to provide more aligned draft candidates. To further benefit from high-quality but non-aligned draft candidates, we also introduce a simple yet effective flexible verification strategy. Through an adaptive probability threshold, our approach can improve generation accuracy while further improving inference efficiency. Experiments on 8 datasets (including question answering, summarization and code completion tasks) show that our approach increases the average generation score by 3.3 points for the LLaMA3 model. Our method achieves a mean acceptance length up to 2.39 and speed up generation by 2.23×",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jikai Wang",
      "Zhenxu Tian",
      "Juntao Li",
      "Qingrong Xia",
      "Xinyu Duan",
      "Zhefeng Wang",
      "Baoxing Huai",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.344": {
    "title": "ViLBench: A Suite for Vision-Language Process Reward Modeling",
    "volume": "main",
    "abstract": "Process-supervised reward models serve as a fine-grained function that provides detailed step-wise feedback to model responses, facilitating effective selection of reasoning trajectories for complex tasks. Despite its advantages, evaluation on PRMs remains less explored, especially in the multimodal domain. To address this gap, this paper first benchmarks current vision large language models (VLLMs) as two types of reward models: output reward models (ORMs) and process reward models (PRMs) on multiple vision-language benchmarks, which reveal that neither ORM nor PRM consistently outperforms across all tasks, and superior VLLMs do not necessarily yield better rewarding performance. To further advance evaluation, we introduce ViLBench, a vision-language benchmark designed to require intensive process reward signals. Notably, OpenAI's GPT-4o with Chain-of-Thought (CoT) achieves only 27.3% accuracy, challenging current VLLMs. Lastly, we preliminarily showcase a promising pathway towards bridging the gap between general VLLMs and reward models—by collecting 73.6K vision-language process reward data using an enhanced tree-search algorithm, our 3B model is able to achieve an average improvement of 3.3% over standard CoT and up to 2.5% compared to its untrained counterpart on ViLBench by selecting OpenAI o1's generations. We will release our code, model, and data at https://ucsc-vlaa.github.io/ViLBench",
    "checked": true,
    "id": "2c3e41460bea99433b29aef2de54b9b9d3b71aac",
    "semantic_title": "vilbench: a suite for vision-language process reward modeling",
    "citation_count": 10,
    "authors": [
      "Haoqin Tu",
      "Weitao Feng",
      "Hardy Chen",
      "Hui Liu",
      "Xianfeng Tang",
      "Cihang Xie"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.345": {
    "title": "Keep Security! Benchmarking Security Policy Preservation in Large Language Model Contexts Against Indirect Attacks in Question Answering",
    "volume": "main",
    "abstract": "As Large Language Models (LLMs) are increasingly deployed in sensitive domains such as enterprise and government, ensuring that they adhere to **user-defined security policies** within context is critical-especially with respect to information non-disclosure. While prior LLM studies have focused on general safety and socially sensitive data, large-scale benchmarks for **contextual security** preservation against attacks remain lacking. To address this, we introduce a novel large-scale benchmark dataset, **CoPriva**, evaluating LLM adherence to contextual non-disclosure policies in question answering. Derived from realistic contexts, our dataset includes explicit policies and queries designed as direct and challenging indirect attacks seeking prohibited information. We evaluate 10 LLMs on our benchmark and reveal a significant vulnerability: many models violate user-defined policies and leak sensitive information. This failure is particularly severe against indirect attacks, highlighting a critical gap in current LLM safety alignment for sensitive applications. Our analysis reveals that while models can often identify the correct answer to a query, they struggle to incorporate policy constraints during generation. In contrast, they exhibit a partial ability to revise outputs when explicitly prompted. Our findings underscore the urgent need for more robust methods to guarantee contextual security",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hwan Chang",
      "Yumin Kim",
      "Yonghyun Jun",
      "Hwanhee Lee"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.346": {
    "title": "Route Sparse Autoencoder to Interpret Large Language Models",
    "volume": "main",
    "abstract": "Mechanistic interpretability of large language models (LLMs) aims to uncover the internal processes of information propagation and reasoning. Sparse autoencoders (SAEs) have demonstrated promise in this domain by extracting interpretable and monosemantic features. However, prior works primarily focus on feature extraction from a single layer, failing to effectively capture activations that span multiple layers. In this paper, we introduce Route Sparse Autoencoder (RouteSAE), a new framework that integrates a routing mechanism with a shared SAE to efficiently extract features from multiple layers. It dynamically assigns weights to activations from different layers, incurring minimal parameter overhead while achieving high interpretability and flexibility for targeted feature manipulation. We evaluate RouteSAE through extensive experiments on Llama-3.2-1B-Instruct. Specifically, under the same sparsity constraint of 64, RouteSAE extracts 22.5% more features than baseline SAEs while achieving a 22.3% higher interpretability score. These results underscore the potential of RouteSAE as a scalable and effective method for LLM interpretability, with applications in feature discovery and model intervention. Our codes are available at https://github.com/swei2001/RouteSAEs",
    "checked": true,
    "id": "2f85c93ab918aa4276cf6fad4638c83b228a8561",
    "semantic_title": "route sparse autoencoder to interpret large language models",
    "citation_count": 9,
    "authors": [
      "Wei Shi",
      "Sihang Li",
      "Tao Liang",
      "Mingyang Wan",
      "Guojun Ma",
      "Xiang Wang",
      "Xiangnan He"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.347": {
    "title": "BTS: Harmonizing Specialized Experts into a Generalist LLM",
    "volume": "main",
    "abstract": "We present Branch-Train-Stitch (BTS), an efficient and flexible training algorithm for combining independently trained large language model (LLM) experts into a single, capable generalist model. Following Li et al., we start with a single seed language model which is branched into domain-specific (e.g., coding or math) experts with continual pretraining. BTS combines experts into a generalist model using lightweight stitch layers, which are inserted between frozen experts and the seed LLM, and trained on a small datamix of the expert domains. Stitch layers enable the seed LLM to integrate representations from any number of experts during the forward pass, allowing it to generalize to new domains, despite remaining frozen. Because BTS does not alter the constituent LLMs, BTS provides a modular and flexible approach: experts can be easily removed and new experts can be added with only a small amount of training. Compared to alternative model merging approaches, BTS yields the best generalist performance on a variety of downstream tasks, retaining the specialized capabilities of each of the experts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qizhen Zhang",
      "Prajjwal Bhargava",
      "Chloe Bi",
      "Chris X. Cai",
      "Jakob Nicolaus Foerster",
      "Jeremy Fu",
      "Punit Singh Koura",
      "Ruan Silva",
      "Sheng Shen",
      "Emily Dinan",
      "Suchin Gururangan",
      "Mike Lewis"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.348": {
    "title": "CoCoA: Confidence- and Context-Aware Adaptive Decoding for Resolving Knowledge Conflicts in Large Language Models",
    "volume": "main",
    "abstract": "Faithful generation in large language models (LLMs) is challenged by knowledge conflicts between parametric memory and external context. Existing contrastive decoding methods tuned specifically to handle conflict often lack adaptability and can degrade performance in low conflict settings. We introduce CoCoA (Confidence- and Context-Aware Adaptive Decoding), a novel token-level algorithm for principled conflict resolution and enhanced faithfulness. CoCoA resolves conflict by utilizing confidence-aware measures (entropy gap and contextual peakedness) and the generalized divergence between the parametric and contextual distributions. Crucially, CoCoA maintains strong performance even in low conflict settings. Extensive experiments across multiple LLMs on diverse Question Answering (QA), Summarization, and Long-Form Question Answering (LFQA) benchmarks demonstrate CoCoA's state-of-the-art performance over strong baselines like AdaCAD. It yields significant gains in QA accuracy, up to 9.2 points on average compared to the strong baseline AdaCAD, and improves factuality in summarization and LFQA by up to 2.5 points on average across key benchmarks. Additionally, it demonstrates superior sensitivity to conflict variations. CoCoA enables more informed, context-aware, and ultimately more faithful token generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anant Khandelwal",
      "Manish Gupta",
      "Puneet Agrawal"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.349": {
    "title": "R-Bind: Unified Enhancement of Attribute and Relation Binding in Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "Text-to-image models frequently fail to achieve perfect alignment with textual prompts, particularly in maintaining proper semantic binding between semantic elements in the given prompt. Existing approaches typically require costly retraining or focus on only correctly generating the attributes of entities (entity-attribute binding), ignoring the cruciality of correctly generating the relations between entities (entity-relation-entity binding), resulting in unsatisfactory semantic binding performance. In this work, we propose a novel training-free method R-Bind that simultaneously improves both entity-attribute and entity-relation-entity binding. Our method introduces three inference-time optimization losses that adjust attention maps during generation. Comprehensive evaluations across multiple datasets demonstrate our approach's effectiveness, validity, and flexibility in enhancing semantic binding without additional training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huixuan Zhang",
      "Xiaojun Wan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.350": {
    "title": "Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning",
    "volume": "main",
    "abstract": "Supervised Fine-Tuning (SFT) Large Language Models (LLM) fundamentally rely on high-quality training data. While data selection and data synthesis are two common strategies to improve data quality, existing approaches often face limitations in static dataset curation that fail to adapt to evolving model capabilities. In this paper, we introduce **Middo**, a self-evolving **M**odel-**i**nformed **d**ynamic **d**ata **o**ptimization framework that uses model-aware data selection and context-preserving data refinement. Unlike conventional one-off filtering/synthesis methods, our framework establishes a closed-loop optimization system: (1) A self-referential diagnostic module proactively identifies suboptimal samples through tri-axial model signals - *loss patterns (complexity)*, *embedding cluster dynamics (diversity)*, and *self-alignment scores (quality)*; (2) An adaptive optimization engine then transforms suboptimal samples into pedagogically valuable training points while preserving semantic integrity; (3) This optimization process continuously evolves with model capability through dynamic learning principles. Experiments on multiple benchmarks demonstrate that our consistently enhances the quality of seed data and boosts LLM's performance with improving accuracy by 7.15% on average while maintaining the original dataset scale. This work establishes a new paradigm for sustainable LLM training through dynamic human-AI co-evolution of data and models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zinan Tang",
      "Xin Gao",
      "Qizhi Pei",
      "Zhuoshi Pan",
      "Mengzhang Cai",
      "Jiang Wu",
      "Conghui He",
      "Lijun Wu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.351": {
    "title": "Information Integration in Large Language Models is Gated by Linguistic Structural Markers",
    "volume": "main",
    "abstract": "Language comprehension relies on integrating information across both local words and broader context. We propose a method to quantify the information integration window of large language models (LLMs) and examine how sentence and clause boundaries constrain this window. Specifically, LLMs are required to predict a target word based on either a local window (local prediction) or the full context (global prediction), and we use Jensen-Shannon (JS) divergence to measure the information loss from relying solely on the local window, termed the local-prediction deficit. Results show that integration windows of both humans and LLMs are strongly modulated by sentence boundaries, and predictions primarily rely on words within the same sentence or clause: The local-prediction deficit follows a power-law decay as the window length increases and drops sharply at the sentence boundary. This boundary effect is primarily attributed to linguistic structural markers, e.g., punctuation, rather than implicit syntactic or semantic cues. Together, these results indicate that LLMs rely on explicit structural cues to guide their information integration strategy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Liu",
      "Nai Ding"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.352": {
    "title": "Why and How LLMs Benefit from Knowledge Introspection in Commonsense Reasoning",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) can improve commonsense reasoning through generating intermediate knowledge. However, the effectiveness of this knowledge introspection is not always guaranteed. This paper first systematically investigates and reveals an **introspection paradox**: while simple introspection tends to benefit weaker models, it often degrades the performance of stronger ones, particularly on simpler tasks. Our deep analysis indicates that this paradox arises from a complex interplay among model capability, task difficulty and the quality of generated knowledge. Further interpretability analysis reveals the origins of low-quality knowledge generation. To better employ introspected knowledge in LLM, this paper proposes a training-free **Adaptive Introspection Strategy** that operates in two stages using only the model's internal states: **Knowledge Detection**, which dynamically identifies and discards potentially low-quality knowledge, and **Knowledge Regeneration**, which employs attention smoothing to guide the model away from harmful failure modes during knowledge generation. Extensive experiments on five Llama models with different sizes and eight commonsense reasoning benchmarks demonstrate that our approach effectively mitigates the limitations of standard introspection and has consistent performance gains across almost all settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengfeng Zhao",
      "Shizhu He",
      "Shanshan Jiang",
      "Bin Dong",
      "Jun Zhao",
      "Kang Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.353": {
    "title": "GraDaSE: Graph-Based Dataset Search with Examples",
    "volume": "main",
    "abstract": "Dataset search is a specialized information retrieval task. In the emerging scenario of Dataset Search with Examples (DSE), the user submits a query and a few target datasets that are known to be relevant as examples. The retrieved datasets are expected to be relevant to the query and also similar to the target datasets. Distinguished from existing text-based retrievers, we propose a graph-based approach GraDaSE. Besides the textual metadata of the datasets, we identify their provenance-based and topic-based relationships to construct a graph, and jointly encode their structural and textual information for ranking candidate datasets. GraDaSE outperforms a variety of strong baselines on two test collections, including DataFinder-E that we construct",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing He",
      "Mingyang Lv",
      "Qing Shi",
      "Gong Cheng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.354": {
    "title": "Confidence-guided Refinement Reasoning for Zero-shot Question Answering",
    "volume": "main",
    "abstract": "We propose Confidence-guided Refinement Reasoning (C2R), a novel training-free framework applicable to question-answering (QA) tasks across text, image, and video domains. C2R strategically constructs and refines sub-questions and their answers (sub-QAs), deriving a better confidence score for the target answer. C2R first curates a subset of sub-QAs to explore diverse reasoning paths, then compares the confidence scores of the resulting answer candidates to select the most reliable final answer. Since C2R relies solely on confidence scores derived from the model itself, it can be seamlessly integrated with various existing QA models, demonstrating consistent performance improvements across diverse models and benchmarks. Furthermore, we provide essential yet underexplored insights into how leveraging sub-QAs affects model behavior, specifically analyzing the impact of both the quantity and quality of sub-QAs on achieving robust and reliable reasoning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youwon Jang",
      "Woo Suk Choi",
      "Minjoon Jung",
      "Minsu Lee",
      "Byoung-Tak Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.355": {
    "title": "DICE: Structured Reasoning in LLMs through SLM-Guided Chain-of-Thought Correction",
    "volume": "main",
    "abstract": "When performing reasoning tasks with user-specific requirements, such as strict output formats, large language models (LLMs) often prioritize reasoning over adherence to detailed instructions. Fine-tuning LLMs on supervised datasets to address this is impractical due to high computational costs and limited parameter access. To tackle this, we propose DICE, a lightweight framework that guides small language models (SLMs) to refine LLMs' outputs through chain-of-thought (CoT) correction. DICE decouples the process by first prompting LLMs to generate natural language responses, then using trained SLMs to analyze and refine these outputs to meet structured output specifications. This framework preserves LLMs' broad knowledge and reasoning capabilities while ensuring the outputs conform to user demands. Specifically, DICE first constructs structured CoT adaptation datasets via a two-stage method and subsequently applies a dual-tuning strategy to fine-tune SLMs for generating structured outputs in an analyze-then-answer pattern. Experiments demonstrate that DICE improves the average format accuracy and content correctness of LLM outputs by 35.4% and 29.4%, respectively, achieving state-of-the-art (SOTA) performance over other competitive baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiqi Li",
      "Yusheng Liao",
      "Zhe Chen",
      "Yanfeng Wang",
      "Yu Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.356": {
    "title": "CTCC: A Robust and Stealthy Fingerprinting Framework for Large Language Models via Cross-Turn Contextual Correlation Backdoor",
    "volume": "main",
    "abstract": "The widespread deployment of large language models (LLMs) has intensified concerns around intellectual property (IP) protection, as model theft and unauthorized redistribution become increasingly feasible. To address this, model fingerprinting aims to embed verifiable ownership traces into LLMs. However, existing methods face inherent trade-offs between stealthness, robustness, and generalizability—being either detectable via distributional shifts, vulnerable to adversarial modifications, or easily invalidated once the fingerprint is revealed. In this work, we introduce CTCC, a novel rule-driven fingerprinting framework that encodes contextual correlations across multiple dialogue turns—such as counterfactual—rather than relying on token-level or single-turn triggers. CTCC enables fingerprint verification under black-box access while mitigating false positives and fingerprint leakage, supporting continuous construction under a shared semantic rule even if partial triggers are exposed. Extensive experiments across multiple LLM architectures demonstrate that CTCC consistently achieves stronger stealth and robustness than prior work. Our findings position CTCC as a reliable and practical solution for ownership verification in real-world LLM deployment scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenhua Xu",
      "Xixiang Zhao",
      "Xubin Yue",
      "Shengwei Tian",
      "Changting Lin",
      "Meng Han"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.357": {
    "title": "Realistic Training Data Generation and Rule Enhanced Decoding in LLM for NameGuess",
    "volume": "main",
    "abstract": "The wide use of abbreviated column names (derived from English words or Chinese Pinyin) in database tables poses significant challenges for table-centric tasks in natural language processing and database management. Such a column name expansion task, referred to as the NameGuess task, has previously been addressed by fine-tuning Large Language Models (LLMs) on synthetically generated rule-based data. However, the current approaches yield suboptimal performance due to two fundamental limitations: 1) the rule-generated abbreviation data fails to reflect real-world distribution, and 2) the failure of LLMs to follow the rule-sensitive patterns in NameGuess persistently. For the data realism issue, we propose a novel approach that integrates a subsequence abbreviation generator trained on human-annotated data and collects non-subsequence abbreviations to improve the training set. For the rule violation issue, we propose a decoding system constrained on an automaton that represents the rules of abbreviation expansion. We extended the original English NameGuess test set to include non-subsequence and PinYin scenarios. Experimental results show that properly tuned 7/8B moderate-size LLMs with a refined decoding system can surpass the few-shot performance of state-of-the-art LLMs, such as the GPT-4 series. The code and data are presented in the supplementary material",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yikuan Xia",
      "Jiazun Chen",
      "Sujian Li",
      "Jun Gao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.358": {
    "title": "EverTracer: Hunting Stolen Large Language Models via Stealthy and Robust Probabilistic Fingerprint",
    "volume": "main",
    "abstract": "The proliferation of large language models (LLMs) has intensified concerns over model theft and license violations, necessitating robust and stealthy ownership verification. Existing fingerprinting methods either require impractical white-box access or introduce detectable statistical anomalies. We propose EverTracer, a novel gray-box fingerprinting framework that ensures stealthy and robust model provenance tracing. EverTracer is the first to repurpose Membership Inference Attacks (MIAs) for defensive use, embedding ownership signals via memorization instead of artificial trigger-output overfitting. It consists of Fingerprint Injection, which fine-tunes the model on any natural language data without detectable artifacts, and Verification, which leverages calibrated probability variation signal to distinguish fingerprinted models. This approach remains robust against adaptive adversaries, including input level modification, and model-level modifications. Extensive experiments across architectures demonstrate EverTracer's state-of-the-art effectiveness, stealthness, and resilience, establishing it as a practical solution for securing LLM intellectual property",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenhua Xu",
      "Meng Han",
      "Wenpeng Xing"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.359": {
    "title": "Selective Preference Optimization via Token-Level Reward Function Estimation",
    "volume": "main",
    "abstract": "Recent advancements in LLM alignment leverage token-level supervisions to perform fine-grained preference optimization. However, existing token-level alignment methods either optimize on all available tokens, which can be noisy and inefficient, or perform selective training with complex and expensive key token selection strategies. In this work, we propose Selective Preference Optimization (SePO), a novel selective alignment strategy that centers on efficient key token selection without requiring strong, fine-grained supervision signals. We theoretically prove the feasibility of Direct Preference Optimization (DPO) as token-level reward function estimators, which applies to any existing alignment datasets and enables cost-efficient token selection with small-scale model sizes and training data. We then train an oracle model with DPO on the target data and utilize the estimated reward function to score all tokens within the target dataset, where only the key tokens are selected to supervise the target policy model with a contrastive objective function. Extensive experiments on three public evaluation benchmarks show that SePO significantly outperforms competitive baseline methods by only optimizing on 30% key tokens with up to 60% reduction in GPU training hours. We also explore SePO as a new paradigm for weak-to-strong generalization, showing that weak oracle models effectively supervise strong policy models with up to 16.8 more parameters. SePO also selects useful supervision signals from out-of-distribution data, alleviating the over-optimization problem",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kailai Yang",
      "Zhiwei Liu",
      "Qianqian Xie",
      "Jimin Huang",
      "Erxue Min",
      "Sophia Ananiadou"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.360": {
    "title": "Arena-lite: Efficient and Reliable Large Language Model Evaluation via Tournament-Based Direct Comparisons",
    "volume": "main",
    "abstract": "As Large Language Models (LLMs) expand across domains, LLM judges have become essential for systems evaluation. Current benchmarks typically compare system outputs against baselines.This baseline-mediated approach, though convenient, yields lower reliability than direct comparison between systems.We propose Arena-Lite which integrates tournament structure on top of head-to-head comparison.The application of a tournament structure and direct comparison eliminates the need for baseline outputs, reduces the number of required comparisons, and allows higher reliability in system rankings.We conducted two experiments: (1) controlled stochastic modeling and (2) empirical validation with a real LLM judge. Those experiments collectively demonstrate that Arena-Lite consistently achieves higher reliability with fewer comparisons, even with smaller datasets or weaker judges.We release an easy-to-use web demonstration and code to foster adoption of Arena-Lite, streamlining model selection across research and industry communities. Arena-Lite demo and code are available on https://huggingface.co/spaces/NCSOFT/ArenaLite",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seonil Son",
      "Ju-Min Oh",
      "Heegon Jin",
      "Cheolhun Jang",
      "Jeongbeom Jeong",
      "KunTae Kim"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.361": {
    "title": "Addressing Tokenization Inconsistency in Steganography and Watermarking Based on Large Language Models",
    "volume": "main",
    "abstract": "Large language models have significantly enhanced the capacities and efficiency of text generation. On the one hand, they have improved the quality of text-based *steganography*. On the other hand, they have also underscored the importance of *watermarking* as a safeguard against malicious misuse. In this study, we focus on tokenization inconsistency (TI) between Alice and Bob in steganography and watermarking, where TI can undermine robustness. Our investigation reveals that the problematic tokens responsible for TI exhibit two key characteristics: **infrequency** and **temporariness**. Based on these findings, we propose two tailored solutions for TI elimination: *a stepwise verification* method for steganography and *a post-hoc rollback* method for watermarking. Experiments show that (1) compared to traditional disambiguation methods in steganography, directly addressing TI leads to improvements in fluency, imperceptibility, and anti-steganalysis capacity; (2) for watermarking, addressing TI enhances detectability and robustness against attacks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiyi Yan",
      "Yugo Murawaki"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.362": {
    "title": "ExeCoder: Empowering Large Language Models with Executability Representation for Code Translation",
    "volume": "main",
    "abstract": "Code translation is a crucial activity in the software development and maintenance process, and researchers have recently begun to focus on using pre-trained large language models (LLMs) for code translation. However, existing LLMs only learn the contextual semantics of code during pre-training, neglecting executability information closely related to the execution state of the code, which results in unguaranteed code executability and unreliable automated code translation. To address this issue, we propose ExeCoder, an LLM specifically designed for code translation, aimed at utilizing executability representations such as functional semantics, syntax structures, and variable dependencies to enhance the capabilities of LLMs in code translation. To evaluate the effectiveness of ExeCoder, we manually enhanced the widely used benchmark TransCoder-test, resulting in a benchmark called TransCoder-test-X that serves LLMs. Evaluation of TransCoder-test-X indicates that ExeCoder achieves state-of-the-art performance in code translation, surpassing existing open-source code LLMs by over 10.88% to 38.78% and over 27.44% to 42.97% on two metrics, and even outperforms the renowned closed-source LLM GPT-4o. Code is available at https://aka.ms/execoder",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minghua He",
      "Yue Chen",
      "Fangkai Yang",
      "Pu Zhao",
      "Wenjie Yin",
      "Yu Kang",
      "Qingwei Lin",
      "Saravan Rajmohan",
      "Dongmei Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.363": {
    "title": "TableEval: A Real-World Benchmark for Complex, Multilingual, and Multi-Structured Table Question Answering",
    "volume": "main",
    "abstract": "LLMs have shown impressive progress in natural language processing. However, they still face significant challenges in TableQA, where real-world complexities such as diverse table structures, multilingual data, and domain-specific reasoning are crucial. Existing TableQA benchmarks are often limited by their focus on simple flat tables and suffer from data leakage. Furthermore, most benchmarks are monolingual and fail to capture the cross-lingual and cross-domain variability in practical applications. To address these limitations, we introduce TableEval, a new benchmark designed to evaluate LLMs on realistic TableQA tasks. Specifically, TableEval includes tables with various structures (such as concise, hierarchical, and nested tables) collected from four domains (including government, finance, academia, and industry reports). Besides, TableEval features cross-lingual scenarios with tables in Simplified Chinese, Traditional Chinese, and English. To minimize the risk of data leakage, we collect all data from recent real-world documents. Considering that existing TableQA metrics fail to capture semantic accuracy, we further propose SEAT, a new evaluation framework that assesses the alignment between model responses and reference answers at the sub-question level. Experimental results have shown that SEAT achieves high agreement with human judgment. Extensive experiments on TableEval reveal critical gaps in the ability of state-of-the-art LLMs to handle these complex, real-world TableQA tasks, offering insights for future improvements",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junnan Zhu",
      "Jingyi Wang",
      "Bohan Yu",
      "Xiaoyu Wu",
      "Junbo Li",
      "Lei Wang",
      "Nan Xu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.364": {
    "title": "NOVA-63: Native Omni-lingual Versatile Assessments of 63 Disciplines",
    "volume": "main",
    "abstract": "The multilingual capabilities of large language models (LLMs) have attracted considerable attention over the past decade. Assessing the accuracy with which LLMs provide answers in multilingual contexts is essential for determining their level of multilingual proficiency. Nevertheless, existing multilingual benchmarks generally reveal severe drawbacks, such as overly translated content (translationese), the absence of difficulty control, constrained diversity, and disciplinary imbalance, making the benchmarking process unreliable and showing low convincingness. To alleviate those shortcomings, we introduce NOVA-63 (Native Omni-lingual Versatile Assessments of 63 Disciplines), a comprehensive, difficult multilingual benchmark featuring 93,536 questions sourced from native speakers across 14 languages and 63 academic disciplines. Leveraging a robust pipeline that integrates LLM-assisted formatting, expert quality verification, and multi-level difficulty screening, NOVA-63 is balanced on disciplines with consistent difficulty standards while maintaining authentic linguistic elements. Extensive experimentation with current LLMs has shown significant insights into cross-lingual consistency among language families, and exposed notable disparities in models' capabilities across various disciplines. This work provides valuable benchmarking data for the future development of multilingual models. Furthermore, our findings underscore the importance of moving beyond overall scores and instead conducting fine-grained analyses of model performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinyang Zhang",
      "Kexin Yang",
      "Yu Wan",
      "Muyang Ye",
      "Baosong Yang",
      "Fei Huang",
      "Junyang Lin",
      "Dayiheng Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.365": {
    "title": "InfoGain-RAG: Boosting Retrieval-Augmented Generation through Document Information Gain-based Reranking and Filtering",
    "volume": "main",
    "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a promising approach to address key limitations of Large Language Models (LLMs), such as hallucination, outdated knowledge, and lacking reliable reference. However, current RAG frameworks often struggle with identifying whether retrieved documents meaningfully contribute to answer generation. This shortcoming makes it difficult to filter out irrelevant or even misleading content, which notably impacts the final performance. In this paper, we propose Document Information Gain (DIG), a novel metric designed to quantify the contribution of retrieved documents to correct answer generation. DIG measures a document's value by computing the difference of LLM's generation confidence with and without the document augmented. Further, we introduce InfoGain-RAG, a framework that leverages DIG scores to train a specialized reranker, which prioritizes each retrieved document from exact distinguishing and accurate sorting perspectives. This approach can effectively filter out irrelevant documents and select the most valuable ones for better answer generation. Extensive experiments across various models and benchmarks demonstrate that InfoGain-RAG can significantly outperform existing approaches, on both single and multiple retrieval paradigm. Specifically on NaturalQA, it achieves the improvements of 17.9%, 4.5%, 12.5% in exact match accuracy against naive RAG, self-reflective RAG and modern ranking-based RAG respectively, and even an average of 15.3% increment on advanced proprietary models GPT-4o across all datasets. These results demonstrate the feasibility of InfoGain-RAG as it can offer a reliable solution for RAG in multiple applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihan Wang",
      "Zihan Liang",
      "Zhou Shao",
      "Yufei Ma",
      "Huangyu Dai",
      "Ben Chen",
      "Lingtao Mao",
      "Chenyi Lei",
      "Yuqing Ding",
      "Han Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.366": {
    "title": "SpecVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning",
    "volume": "main",
    "abstract": "Video large language models (Vid-LLMs) have shown strong capabilities in understanding video content. However, their reliance on dense video token representations introduces substantial memory and computational overhead in both prefilling and decoding. To mitigate the information loss of recent video token reduction methods and accelerate the decoding stage of Vid-LLMs losslessly, we introduce SpecVLM, a training-free speculative decoding (SD) framework tailored for Vid-LLMs that incorporates staged video token pruning.Building on our novel finding that the draft model's speculation exhibits low sensitivity to video token pruning, SpecVLM prunes up to 90% of video tokens to enable efficient speculation without sacrificing accuracy. To achieve this, we performs a two-stage pruning process: Stage I selects highly informative tokens guided by attention signals from the verifier (target model), while Stage II prunes remaining redundant ones in a spatially uniform manner.Extensive experiments on four video understanding benchmarks demonstrate the effectiveness and robustness of SpecVLM, which achieves up to 2.68× decoding speedup for LLaVA-OneVision-72B and 2.11× speedup for Qwen2.5-VL-32B. Code is available at https://github.com/zju-jiyicheng/SpecVLM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yicheng Ji",
      "Jun Zhang",
      "Heming Xia",
      "Jinpeng Chen",
      "Lidan Shou",
      "Gang Chen",
      "Huan Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.367": {
    "title": "What Do Indonesians Really Need from Language Technology? A Nationwide Survey",
    "volume": "main",
    "abstract": "Despite emerging efforts to develop NLP for Indonesia's 700+ local languages, progress remains costly due to the need for direct engagement with native speakers. However, it is unclear what these language communities truly need from language technology. To address this, we conduct a nationwide survey to assess the actual needs of native Indonesian speakers. Our findings indicate that addressing language barriers, particularly through machine translation and information retrieval, is the most critical priority. Although there is strong enthusiasm for advancements in language technology, concerns around privacy, bias, and the use of public data for AI training highlight the need for greater transparency and clear communication to support broader AI adoption",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Dehan Al Kautsar",
      "Lucky Susanto",
      "Derry Tanti Wijaya",
      "Fajri Koto"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.368": {
    "title": "LEO-MINI: An Efficient Multimodal Large Language Model using Conditional Token Reduction and Mixture of Multi-Modal Experts",
    "volume": "main",
    "abstract": "Redundancy of visual tokens in multi-modal large language models (MLLMs) significantly reduces their computational efficiency. Recent approaches, such as resamplers and summarizers, have sought to reduce the number of visual tokens, but at the cost of visual reasoning ability. To address this, we propose LEO-Mini, a novel MLLM that significantly reduces the number of visual tokens and simultaneously boosts visual reasoning capabilities. For efficiency, LEO-Mini incorporates CoTR, a novel token reduction module to consolidate a large number of visual tokens into a smaller set of tokens, using the similarity between visual tokens, text tokens, and a compact learnable query. For effectiveness, to scale up the model's ability with minimal computational overhead, LEO-Mini employs MMoE, a novel mixture of multi-modal experts module. MMoE employs a set of LoRA experts with a novel router to switch between them based on the input text and visual tokens instead of only using the input hidden state. MMoE also includes a general LoRA expert that is always activated to learn general knowledge for LLM reasoning. For extracting richer visual features, MMoE employs a set of vision experts trained on diverse domain-specific data. To demonstrate LEO-Mini's improved efficiency and performance, we evaluate it against existing efficient MLLMs on various benchmark vision-language tasks",
    "checked": true,
    "id": "92658eaf0eaf6665c1bfc2bb3c57755e82926bf8",
    "semantic_title": "leo-mini: an efficient multimodal large language model using conditional token reduction and mixture of multi-modal experts",
    "citation_count": 2,
    "authors": [
      "Yimu Wang",
      "Mozhgan Nasr Azadani",
      "Sean Sedwards",
      "Krzysztof Czarnecki"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.369": {
    "title": "Confounding Factors in Relating Model Performance to Morphology",
    "volume": "main",
    "abstract": "The extent to which individual language characteristics influence tokenization and language modeling is an open question. Differences in morphological systems have been suggested as both unimportant and crucial to consider (Cotterell et al., 2018; Gerz et al., 2018a; Park et al., 2021, inter alia). We argue this conflicting evidence is due to confounding factors in experimental setups, making it hard to compare results and draw conclusions. We identify confounding factors in analyses trying to answer the question of whether, and how, morphology relates to language modeling. Next, we re-assess three hypotheses by Arnett & Bergen (2025) for why modeling agglutinative languages results in higher perplexities than fusional languages: they look at morphological alignment of tokenization, tokenization efficiency, and dataset size. We show that each conclusion includes confounding factors. Finally, we introduce token bigram metrics as an intrinsic way to predict the difficulty of causal language modeling, and find that they are gradient proxies for morphological complexity that do not require expert annotation. Ultimately, we outline necessities to reliably answer whether, and how, morphology relates to language modeling",
    "checked": true,
    "id": "84ffe3f46d3571efb8d183e04a5f0749075bc9cf",
    "semantic_title": "confounding factors in relating model performance to morphology",
    "citation_count": 2,
    "authors": [
      "Wessel Poelman",
      "Thomas Bauwens",
      "Miryam de Lhoneux"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.370": {
    "title": "Context-Aware Membership Inference Attacks against Pre-trained Large Language Models",
    "volume": "main",
    "abstract": "Membership Inference Attacks (MIAs) on pre-trained Large Language Models (LLMs) aim at determining if a data point was part of the model's training set. Prior MIAs that are built for classification models fail at LLMs, due to ignoring the generative nature of LLMs across token sequences. In this paper, we present a novel attack on pre-trained LLMs that adapts MIA statistical tests to the perplexity dynamics of subsequences within a data point. Our method significantly outperforms prior approaches, revealing context-dependent memorization patterns in pre-trained LLMs",
    "checked": true,
    "id": "b1856118892a5e4258629a1ac420074d55393f5e",
    "semantic_title": "context-aware membership inference attacks against pre-trained large language models",
    "citation_count": 9,
    "authors": [
      "Hongyan Chang",
      "Ali Shahin Shamsabadi",
      "Kleomenis Katevas",
      "Hamed Haddadi",
      "Reza Shokri"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.371": {
    "title": "Formalizing Style in Personal Narratives",
    "volume": "main",
    "abstract": "Personal narratives are stories authors construct to make meaning of their experiences. Style, the distinctive way authors use language to express themselves, is fundamental to how these narratives convey subjective experiences. Yet there is a lack of a formal framework for systematically analyzing these stylistic choices. We present a novel approach that formalizes style in personal narratives as patterns in the linguistic choices authors make when communicating subjective experiences. Our framework integrates three domains: functional linguistics establishes language as a system of meaningful choices, computer science provides methods for automatically extracting and analyzing sequential patterns, and these patterns are linked to psychological observations. Using language models, we automatically extract linguistic features such as processes, participants, and circumstances. We apply our framework to hundreds of dream narratives, including a case study on a war veteran with post-traumatic stress disorder. Analysis of his narratives uncovers distinctive patterns, particularly how verbal processes dominate over mental ones, illustrating the relationship between linguistic choices and psychological states",
    "checked": true,
    "id": "986d5d5466bd1c109baca6ec7453359480c9dfcf",
    "semantic_title": "formalizing style in personal narratives",
    "citation_count": 0,
    "authors": [
      "Gustave Cortal",
      "Alain Finkel"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.372": {
    "title": "TopicAttack: An Indirect Prompt Injection Attack via Topic Transition",
    "volume": "main",
    "abstract": "Large language models (LLMs) have shown remarkable performance across a range of NLP tasks. However, their strong instruction-following capabilities and inability to distinguish instructions from data content make them vulnerable to indirect prompt injection attacks. In such attacks, instructions with malicious purposes are injected into external data sources, such as web documents. When LLMs retrieve this injected data through tools, such as a search engine and execute the injected instructions, they provide misled responses. Recent attack methods have demonstrated potential, but their abrupt instruction injection often undermines their effectiveness. Motivated by the limitations of existing attack methods, we propose **TopicAttack**, which prompts the LLM to generate a fabricated conversational transition prompt that gradually shifts the topic toward the injected instruction, making the injection smoother and enhancing the plausibility and success of the attack. Through comprehensive experiments, TopicAttack achieves state-of-the-art performance, with an attack success rate (ASR) over 90% in most cases, even when various defense methods are applied. We further analyze its effectiveness by examining attention scores. We find that a higher injected-to-original attention ratio leads to a greater success probability, and our method achieves a much higher ratio than the baseline methods",
    "checked": true,
    "id": "c87a76ebb3df5edcbd7a03699e798d2be1806a9b",
    "semantic_title": "topicattack: an indirect prompt injection attack via topic transition",
    "citation_count": 2,
    "authors": [
      "Yulin Chen",
      "Haoran Li",
      "Yuexin Li",
      "Yue Liu",
      "Yangqiu Song",
      "Bryan Hooi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.373": {
    "title": "PSET: a Phonetics-Semantics Evaluation Testbed",
    "volume": "main",
    "abstract": "We introduce the Phonetics-Semantics Evaluation Testbed (PSET), a new English-based testbed to evaluate phonetic embeddings. Our testbed is built on the assumption that phonetic embeddings should always prioritize phonetics over semantics, and it therefore leverages homophones and synonyms.We use PSET to test three phonetic embedding models: articulatory embeddings, Phoneme2Vec, and XPhoneBERT. The phonetic-based embeddings solve the task with varying degrees of success, with Phoneme2Vec performing the best.We also test five recent LLMs, GPT-4o, Gemini 2.5 Flash, Llama 3.1-8B, OLMo-7B and OLMo 2-7B. Gemini 2.5 Flash performs better than the other models. With this testbed, we hope to advance the development and evaluation of phonetic embedding models",
    "checked": true,
    "id": "d817d4ee9c55c9cb998449d09257003d3612444b",
    "semantic_title": "pset: a phonetics-semantics evaluation testbed",
    "citation_count": 0,
    "authors": [
      "Gianluca Sperduti",
      "Dong Nguyen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.374": {
    "title": "From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way Parallel Corpora",
    "volume": "main",
    "abstract": "Continued pretraining and instruction tuning on large-scale multilingual data have proven to be effective in scaling large language models (LLMs) to low-resource languages. However, the unaligned nature of such data limits its ability to effectively capture cross-lingual semantics. In contrast, multi-way parallel data, where identical content is aligned across multiple languages, provides stronger cross-lingual consistency and offers greater potential for improving multilingual performance. In this paper, we introduce a large-scale, high-quality multi-way parallel corpus, TED2025, based on TED Talks. The corpus spans 113 languages, with up to 50 languages aligned in parallel, ensuring extensive multilingual coverage. Using this dataset, we investigate best practices for leveraging multi-way parallel data to enhance LLMs, including strategies for continued pretraining, instruction tuning, and the analysis of key influencing factors. Experiments on six multilingual benchmarks show that models trained on multi-way parallel data consistently outperform those trained on unaligned multilingual data",
    "checked": true,
    "id": "ddfccebe9f87738cd1a1dff164cf8fec8223ee40",
    "semantic_title": "from unaligned to aligned: scaling multilingual llms with multi-way parallel corpora",
    "citation_count": 2,
    "authors": [
      "Yingli Shen",
      "Wen Lai",
      "Shuo Wang",
      "Ge Gao",
      "Kangyang Luo",
      "Alexander Fraser",
      "Maosong Sun"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.375": {
    "title": "GATEAU: Selecting Influential Samples for Long Context Alignment",
    "volume": "main",
    "abstract": "Aligning large language models to handle instructions with extremely long contexts has yet to be fully investigated. Previous studies have attempted to scale up the available data volume by synthesizing long instruction-following samples, as constructing such a dataset tends to be challenging for annotators. However, a lack of a well-defined strategy for ensuring data quality may introduce low-quality samples and restrict the model's performance. Thus, we propose GATEAU, a novel framework to address the unique challenge of long context alignment by identifying the influential samples enriched with long-range dependency relations. Specifically, GATEAU measures the long-range dependencies from two essential aspects: the difficulty of generating target responses due to the long-range dependencies, and the difficulty of understanding long inputs due to such dependencies. Comprehensive experiments indicate that GATEAU effectively identifies influential samples and the model trained on these selected samples exhibits better instruction-following and long-context understanding capabilities",
    "checked": true,
    "id": "17ed28d477169668d8591053d08a8bd4d410da59",
    "semantic_title": "gateau: selecting influential samples for long context alignment",
    "citation_count": 4,
    "authors": [
      "Shuzheng Si",
      "Haozhe Zhao",
      "Gang Chen",
      "Yunshui Li",
      "Kangyang Luo",
      "Chuancheng Lv",
      "Kaikai An",
      "Fanchao Qi",
      "Baobao Chang",
      "Maosong Sun"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.376": {
    "title": "Teach Small Models to Reason by Curriculum Distillation",
    "volume": "main",
    "abstract": "Large Reasoning Models (LRMs) show strong System-2-style reasoning, but at the cost of significant computational overhead. In contrast, efficient System-1-style Large Language Models (LLMs) often struggle on complex tasks. We identify a critical asymmetry between these two paradigms: LRMs can implicitly self-distill their own reasoning, solving hard problems with near System-1-style efficiency while retaining superior performance. LLMs, however, lack such deep internal modes and collapse when forced to rely on their own reasoning rather than imitating external traces. This asymmetry explains why direct distillation from strong LRMs to weaker LLMs often fails: student models struggle to learn from LRMs' overly complex explicit reasoning and gain little from their overly compact implicit solutions. To address this, we introduce a two-stage curriculum distillation framework, which first builds a robust internal problem-solving student model and then teaches the student model to externalize this latent knowledge as explicit reasoning. On challenging mathematical benchmarks, our method significantly outperforms single-stage baselines, creating compact models with strong reasoning ability",
    "checked": true,
    "id": "d149b409b4259f11e94211e48379b84ccfa0a751",
    "semantic_title": "teach small models to reason by curriculum distillation",
    "citation_count": 0,
    "authors": [
      "Wangyi Jiang",
      "Yaojie Lu",
      "Hongyu Lin",
      "Xianpei Han",
      "Le Sun"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.377": {
    "title": "Enhancing Reasoning Abilities of Small LLMs with Cognitive Alignment",
    "volume": "main",
    "abstract": "The reasoning capabilities of large language reasoning models (LRMs), such as OpenAI's o1 and DeepSeek-R1, have seen substantial advancements through deep thinking. However, these enhancements come with significant resource demands, underscoring the need for training effective small reasoning models. A critical challenge is that small models possess different reasoning capacities and cognitive trajectories compared with their larger counterparts. Hence, directly distilling chain-of-thought (CoT) results from large LRMs to smaller ones can sometimes be ineffective and often requires a substantial amount of annotated data. In this paper, we first introduce a novel Critique-Rethink-Verify (CRV) system, designed for training smaller yet powerful LRMs. Our CRV system consists of multiple LLM agents, each specializing in unique abilities: (i) critiquing the CoT qualities according to the cognitive capabilities of smaller models, (ii) rethinking and refining these CoTs based on the critiques, and (iii) verifying the correctness of the refined results. Based on the CRV system, we further propose the Cognitive Preference Optimization (CogPO) algorithm to continuously enhance the reasoning abilities of smaller models by aligning their reasoning processes with their cognitive capacities. Comprehensive evaluations on challenging reasoning benchmarks demonstrate the efficacy of our CRV+CogPO framework, which outperforms other methods by a large margin",
    "checked": true,
    "id": "71c1efe62996797d781490e68d353869112425ee",
    "semantic_title": "enhancing reasoning abilities of small llms with cognitive alignment",
    "citation_count": 2,
    "authors": [
      "Wenrui Cai",
      "Chengyu Wang",
      "Junbing Yan",
      "Jun Huang",
      "Xiangzhong Fang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.378": {
    "title": "NOVER: Incentive Training for Language Models via Verifier-Free Reinforcement Learning",
    "volume": "main",
    "abstract": "Recent advances, such as DeepSeek R1-Zero, highlight the effectiveness of incentive training, a reinforcement learning paradigm that computes rewards solely based on the final answer part of a language model's output, thereby encouraging the generation of intermediate reasoning steps. However, these methods fundamentally rely on external verifiers, which limits their applicability to domains like mathematics and coding, where such verifiers are readily available. Although reward models can serve as verifiers, they require high-quality annotated data and are costly to train.In this work, we propose NOVER, NO-VERifier Reinforcement Learning, a general reinforcement learning framework that requires only standard supervised fine-tuning data with no need for an external verifier. NOVER enables incentive training across a wide range of text-to-text tasks and outperforms the model of the same size distilled from large reasoning models such as DeepSeek R1 671B by 7.7%. Moreover, the flexibility of NOVER enables new possibilities for optimizing large language models, such as inverse incentive training",
    "checked": true,
    "id": "c5f39bcc5c9923f81047c804fb21ea0f761eab1b",
    "semantic_title": "nover: incentive training for language models via verifier-free reinforcement learning",
    "citation_count": 3,
    "authors": [
      "Wei Liu",
      "Siya Qi",
      "Xinyu Wang",
      "Chen Qian",
      "Yali Du",
      "Yulan He"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.379": {
    "title": "Genre Matters: How Text Types Interact with Decoding Strategies and Lexical Predictors in Shaping Reading Behavior",
    "volume": "main",
    "abstract": "The type of a text profoundly shapes reading behavior, yet little is known about how different text types interact with word-level features and the properties of machine-generated texts and how these interactions influence how readers process language. In this study, we investigate how different text types affect eye movements during reading, how neural decoding strategies used to generate texts interact with text type, and how text types modulate the influence of word-level psycholinguistic features such as surprisal, word length, and lexical frequency. Leveraging EMTeC (Bolliger et al., 2025), the first eye-tracking corpus of LLM-generated texts across six text types and multiple decoding algorithms, we show that text type strongly modulates cognitive effort during reading, that psycholinguistic effects induced by word-level features vary systematically across genres, and that decoding strategies interact with text types to shape reading behavior. These findings offer insights into genre-specific cognitive processing and have implications for the human-centric design of AI-generated texts. Our code is publicly available at https://github.com/DiLi-Lab/Genre-Matters",
    "checked": true,
    "id": "8a374b7c36c644dc1d80b60c89839debc7da9d37",
    "semantic_title": "genre matters: how text types interact with decoding strategies and lexical predictors in shaping reading behavior",
    "citation_count": 0,
    "authors": [
      "Lena Sophia Bolliger",
      "Lena Ann Jäger"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.380": {
    "title": "RTE-GMoE: A Model-agnostic Approach for Relation Triplet Extraction via Graph-based Mixture-of-Expert Mutual Learning",
    "volume": "main",
    "abstract": "Relation Triplet Extraction (RTE) is a fundamental while challenge task in knowledge acquisition, which identifies and extracts all triplets from unstructured text. Despite the recent advancements, the deep integration of the entity-, relation- and triplet-specific information remains a challenge. In this paper, we propose a Graph-based Mixture-of-Experts mutual learning framework for RTE, namely RTE-GMoE, to address this limitation. As a model-agnostic framework, RTE-GMoE distinguishes itself by including and modeling the mutual interactions among three vital task-specific experts: entity expert, RTE expert, and relation expert. RTE expert corresponds to the main RTE task and can be implemented by any model and the other two correspond to the two auxiliary tasks: entity recognition and relation extraction. We construct an expert graph and achieve comprehensive and adaptive graph-based MoE interactions with a novel mutual learning mechanism. In our framework, these experts perform knowledge extractions collaboratively via dynamic information exchange and knowledge sharing. We conduct extensive experiments on four state-of-the-art backbones and evaluate them on several widely-used benchmarks. The results demonstrate that our framework brings consistent and promising improvements on all backbones and benchmarks. Component study and model analysis further verify the effectiveness and advantages of our method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aziguli Wulamu",
      "Kaiyuan Gong",
      "Lyu Zhengyu",
      "Yu Han",
      "Zhihong Zhu",
      "Bowen Xing"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.381": {
    "title": "Avoidance Decoding for Diverse Multi-Branch Story Generation",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) often generate repetitive and monotonous outputs, especially in tasks like story generation, due to limited creative diversity when given the same input prompt. To address this challenge, we propose a novel decoding strategy, ***Avoidance Decoding***, that modifies token logits by penalizing similarity to previously generated outputs, thereby encouraging more diverse multi-branch stories. This penalty adaptively balances two similarity measures: (1) Concept-level Similarity Penalty, which is prioritized in early stages to diversify initial story concepts, and (2) Narrative-level Similarity Penalty, which is increasingly emphasized later to ensure natural yet diverse plot development. Notably, our method achieves up to **2.6** times higher output diversity and reduces repetition by an average of 30% compared to strong baselines, while effectively mitigating text degeneration. Furthermore, we reveal that our method activates a broader range of neurons, demonstrating that it leverages the model's intrinsic creative capacity",
    "checked": true,
    "id": "dab217bb6f5d52ce7cb764a13f238657ad054bf5",
    "semantic_title": "avoidance decoding for diverse multi-branch story generation",
    "citation_count": 0,
    "authors": [
      "Kyeongman Park",
      "Nakyeong Yang",
      "Kyomin Jung"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.382": {
    "title": "Probabilistic Soundness Guarantees in LLM Reasoning Chains",
    "volume": "main",
    "abstract": "In reasoning chains generated by large language models (LLMs), initial errors often propagate and undermine the reliability of the final conclusion. Current LLM-based error detection methods often fail to detect propagated errors because earlier errors can corrupt judgments of downstream reasoning. To better detect such errors, we introduce Autoregressive Reasoning Entailment Stability (ARES), a probabilistic framework that evaluates each reasoning step based solely on previously-verified premises. This inductive method yields a nuanced score for each step and provides certified statistical guarantees of its soundness, rather than a brittle binary label. ARES achieves state-of-the-art performance across four benchmarks (72.1% Macro-F1, +8.2 points) and demonstrates superior robustness on very long synthetic reasoning chains, where it excels at detecting propagated errors (90.3% F1, +27.6 points)",
    "checked": true,
    "id": "eb9edf778eeecf51e87cc31c5a35d33ca17c4162",
    "semantic_title": "probabilistic soundness guarantees in llm reasoning chains",
    "citation_count": 3,
    "authors": [
      "Weiqiu You",
      "Anton Xue",
      "Shreya Havaldar",
      "Delip Rao",
      "Helen Jin",
      "Chris Callison-Burch",
      "Eric Wong"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.383": {
    "title": "SQLWOZ: A Realistic Task-Oriented Dialogue Dataset with SQL-Based Dialogue State Representation for Complex User Requirements",
    "volume": "main",
    "abstract": "High-quality datasets are essential for building effective task-oriented dialogue (TOD) systems. The existing TOD datasets often present overly simplified interactions, where users incrementally express straightforward requests that can be managed with basic slot-value style dialogue states, such as \"hotel-area = east.\" However, this approach does not reflect real-life scenarios in which users may express complex constraints and preferences. To address this gap, in this paper, we propose SQLWOZ, a novel TOD dataset designed to capture complex, real-world user requirements. The user requirements in SQLWOZ include the four categories: 1) multiple values for a slot, 2) excluded values within a slot, 3) preferred or prioritized values, and 4) conditional values based on other conditions. We utilize SQL statements as a formalized and expressive representation of dialogue states within SQLWOZ. To evaluate the dataset, we adapt large language models as dialogue agents and conduct extensive experiments on the SQL-based dialogue state tracking, dialogue response generation and end-to-end TOD tasks. The experimental results demonstrate the complexity and quality of SQLWOZ, establishing it as a new benchmark for advancing TOD research",
    "checked": true,
    "id": "87e270280bbef4d48aadc14da3baaceb1b1d61c2",
    "semantic_title": "sqlwoz: a realistic task-oriented dialogue dataset with sql-based dialogue state representation for complex user requirements",
    "citation_count": 0,
    "authors": [
      "Heng-Da Xu",
      "Xian-Ling Mao",
      "Fanshu Sun",
      "Tian-Yi Che",
      "Cheng-Xin Xin",
      "Heyan Huang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.384": {
    "title": "SURE: Safety Understanding and Reasoning Enhancement for Multimodal Large Language Models",
    "volume": "main",
    "abstract": "Multimodal large language models (MLLMs) demonstrate impressive capabilities by integrating visual and textual information. However, the incorporation of visual modalities also introduces new and complex safety risks, rendering even the most advanced models vulnerable to sophisticated jailbreak attacks. This paper first analyzes the impact of inserting safety reasoning prompt on various aspects of the model. We find that this external method can help the model resist jailbreak attacks to some extent, but the model still fails to distinguish specific semantic scenarios, resulting in a significantly increased refusal rate for benign queries. Inspired by this, we propose a novel training framework, SURE (Safety Understanding and Reasoning Enhancement for Multimodal Large Language Models), designed to help models internalize chain-of-thought-based safety decision-making capabilities. Extensive experiments demonstrate that SURE significantly improves model safety while effectively avoiding over-defense, achieving a good balance between safety and generality. Finally, we create a large-scale multimodal safety reasoning dataset, MLLM-SCoT-Plus, to facilitate research on safety alignment in multimodal models.Our code and the dataset are publicly available at https://github.com/hfutml/SURE",
    "checked": true,
    "id": "03321d748b6aede91752f97f3d4d02d6f00e410b",
    "semantic_title": "sure: safety understanding and reasoning enhancement for multimodal large language models",
    "citation_count": 0,
    "authors": [
      "Yuxin Gou",
      "Xiaoning Dong",
      "Qin Li",
      "Shishen Gu",
      "Richang Hong",
      "Wenbo Hu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.385": {
    "title": "EMO: Embedding Model Distillation via Intra-Model Relation and Optimal Transport Alignments",
    "volume": "main",
    "abstract": "Knowledge distillation (KD) is crucial for compressing large text embedding models, but faces challenges when teacher and student models use different tokenizers (Cross-Tokenizer KD - CTKD). Vocabulary mismatches impede the transfer of relational knowledge encoded in deep representations, such as hidden states and attention matrices, which are vital for producing high-quality embeddings. Existing CTKD methods often focus on direct output alignment, neglecting this crucial structural information. We propose a novel framework tailored for CTKD embedding model distillation. We first map tokens one-to-one via Minimum Edit Distance (MinED). Then, we distill intra-model relational knowledge by aligning attention matrix patterns using Centered Kernel Alignment, focusing on the top-m most important tokens of the directly mapped tokens. Simultaneously, we align final hidden states via Optimal Transport with Importance-Scored Mass Assignment, which emphasizes semantically important token representations, based on importance scores derived from attention weights. We evaluate distillation from state-of-the-art embedding models (e.g., LLM2Vec, BGE) to a Bert-base-uncased model on embedding-reliant tasks such as text classification, sentence pair classification, and semantic textual similarity. Our proposed framework significantly outperforms existing CTKD baselines. By preserving attention structure and prioritizing key representations, our approach yields smaller, high-fidelity embedding models despite tokenizer differences",
    "checked": true,
    "id": "9b8c84400dea7667522c365aae5763888c834882",
    "semantic_title": "emo: embedding model distillation via intra-model relation and optimal transport alignments",
    "citation_count": 0,
    "authors": [
      "Minh-Phuc Truong",
      "Hai An Vu",
      "Tu Vu",
      "Nguyen Thi Ngoc Diep",
      "Linh Ngo Van",
      "Thien Huu Nguyen",
      "Trung Le"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.386": {
    "title": "AesBiasBench: Evaluating Bias and Alignment in Multimodal Language Models for Personalized Image Aesthetic Assessment",
    "volume": "main",
    "abstract": "Multimodal Large Language Models (MLLMs) are increasingly applied in Personalized Image Aesthetic Assessment (PIAA) as a scalable alternative to expert evaluations. However, their predictions may reflect subtle biases influenced by demographic factors such as gender, age, and education. In this work, we propose AesBiasBench, a benchmark designed to evaluate MLLMs along two complementary dimensions: (1) stereotype bias, quantified by measuring variations in aesthetic evaluations across demographic groups; and (2) alignment between model outputs and genuine human aesthetic preferences. Our benchmark covers three subtasks (Aesthetic Perception, Assessment, Empathy) and introduces structured metrics (IFD, NRD, AAS) to assess both bias and alignment. We evaluate 19 MLLMs, including proprietary models (e.g., GPT-4o, Claude-3.5-Sonnet) and open-source models (e.g., InternVL-2.5, Qwen2.5-VL). Results indicate that smaller models exhibit stronger stereotype biases, whereas larger models align more closely with human preferences. Incorporating identity information often exacerbates bias, particularly in emotional judgments. These findings underscore the importance of identity-aware evaluation frameworks in subjective vision-language tasks",
    "checked": true,
    "id": "c78c911fa95a2b0d5228590f5e3049542cbb32c8",
    "semantic_title": "aesbiasbench: evaluating bias and alignment in multimodal language models for personalized image aesthetic assessment",
    "citation_count": 0,
    "authors": [
      "Kun Li",
      "Lai Man Po",
      "Hongzheng Yang",
      "Xuyuan Xu",
      "Kangcheng Liu",
      "Yuzhi Zhao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.387": {
    "title": "DA-Pred: Performance Prediction for Text Summarization under Domain-Shift and Instruct-Tuning",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) often don't perform as expected under Domain Shift or after Instruct-tuning. A reliable indicator of LLM performance in these settings could assist in decision-making. We present a method that uses the known performance in high-resource domains and fine-tuning settings to predict performance in low-resource domains or base models, respectively. In our paper, we formulate the task of performance prediction, construct a dataset for it, and train regression models to predict the said change in performance. Our proposed methodology is lightweight and, in practice, can help researchers & practitioners decide if resources should be allocated for data labeling and LLM Instruct-tuning",
    "checked": true,
    "id": "ffa3ba7788088872c9b17457fb6d248e252e96fc",
    "semantic_title": "da-pred: performance prediction for text summarization under domain-shift and instruct-tuning",
    "citation_count": 0,
    "authors": [
      "Anum Afzal",
      "Florian Matthes",
      "Alexander Fabbri"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.388": {
    "title": "UnCo: Uncertainty-Driven Collaborative Framework of Large and Small Models for Grounded Multimodal NER",
    "volume": "main",
    "abstract": "Grounded Multimodal Named Entity Recognition (GMNER) is a new information extraction task. It requires models to extract named entities and ground them to real-world visual objects. Previous methods, relying on domain-specific fine-tuning, struggle with unseen multimodal entities due to limited knowledge and generalization. Recently, multimodal large language models (MLLMs) have demonstrated strong open-set abilities. However, their performance is hindered by the lack of in-domain knowledge due to costly training for GMNER datasets. To address these limitations, we propose **UnCo**, a two-stage Uncertainty-driven Collaborative framework that leverages the complementary strengths of small fine-tuned models and MLLMs. Specifically, **in stage one**, we equip the small model with a unified uncertainty estimation (UE) for multimodal entities. This enables the small model to express \"I do not know\" when recognizing unseen entities beyond its capabilities. Predictions with high uncertainty are then filtered and delegated to the MLLM. **In stage two**, an Uncertainty-aware Hierarchical Correction mechanism guides the MLLM to refine uncertain predictions using its open-domain knowledge. Ultimately, UnCo effectively retains the in-domain knowledge of small models while utilizing the capabilities of MLLMs to handle unseen samples. Extensive experiments demonstrate UnCo's effectiveness on two GMNER benchmarks",
    "checked": true,
    "id": "002f821f71f26c9ff724bfd1dcd848aad3fc678a",
    "semantic_title": "unco: uncertainty-driven collaborative framework of large and small models for grounded multimodal ner",
    "citation_count": 0,
    "authors": [
      "Jielong Tang",
      "Yang Yang",
      "Jianxing Yu",
      "Zhen-Xing Wang",
      "Haoyuan Liang",
      "Liang Yao",
      "Jian Yin"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.389": {
    "title": "An Empirical Study of LLM Reasoning Ability Under Strict Output Length Constraint",
    "volume": "main",
    "abstract": "Recent work has demonstrated the remarkable potential of Large Language Models (LLMs) in test-time scaling. By making models think before answering, they are able to achieve much higher accuracy with extra inference computation.However, in many real-world scenarios, models are used under time constraints, where an answer should be given within a certain output length. It is unclear whether and how the reasoning ability of different LLMs remain effective under strict constraints.We take a first look at this problem by conducting an in-depth empirical study. Specifically, we test 30 LLMs on common reasoning datasets under a wide range of output length budgets, and we analyze the correlation between the inference accuracy and various properties including model type, model size, prompt style, etc. We also consider the mappings between token budgets and actual on-device latency budgets.The results have demonstrated several interesting findings regarding the budget-aware LLM reasoning ability that differ from the unconstrained situation, e.g. the optimal choices of either model size or prompt style change under different budgets. These findings offer timely evaluation to this area and practical guidance for users to deploy LLMs under real-world latency constraints",
    "checked": true,
    "id": "6eec64f415c86800f1829afcad685cc0f4054fdd",
    "semantic_title": "an empirical study of llm reasoning ability under strict output length constraint",
    "citation_count": 5,
    "authors": [
      "Yi Sun",
      "Han Wang",
      "Jiaqiang Li",
      "Jiacheng Liu",
      "Xiangyu Li",
      "Hao Wen",
      "Yizhen Yuan",
      "Huiwen Zheng",
      "Yan Liang",
      "Yuanchun Li",
      "Yunxin Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.390": {
    "title": "Enrich-on-Graph: Query-Graph Alignment for Complex Reasoning with LLM Enriching",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) exhibit strong reasoning capabilities in complex tasks. However, they still struggle with hallucinations and factual errors in knowledge-intensive scenarios like knowledge graph question answering (KGQA). We attribute this to the semantic gap between structured knowledge graphs (KGs) and unstructured queries, caused by inherent differences in their focuses and structures. Existing methods usually employ resource-intensive, non-scalable workflows reasoning on vanilla KGs, but overlook this gap. To address this challenge, we propose a flexible framework, Enrich-on-Graph (EoG), which leverages LLMs' prior knowledge to enrich KGs, bridge the semantic gap between graphs and queries. EoG enables efficient evidence extraction from KGs for precise and robust reasoning, while ensuring low computational costs, scalability, and adaptability across different methods. Furthermore, we propose three graph quality evaluation metrics to analyze query-graph alignment in KGQA task, supported by theoretical validation of our optimization objectives. Extensive experiments on two KGQA benchmark datasets indicate that EoG can effectively generate high-quality KGs and achieve the state-of-the-art performance",
    "checked": true,
    "id": "f75f2e480328bf0d471025f7eec9696a0d3b92d0",
    "semantic_title": "enrich-on-graph: query-graph alignment for complex reasoning with llm enriching",
    "citation_count": 1,
    "authors": [
      "Songze Li",
      "Zhiqiang Liu",
      "Zhengke Gui",
      "Huajun Chen",
      "Wen Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.391": {
    "title": "Noise, Adaptation, and Strategy: Assessing LLM Fidelity in Decision-Making",
    "volume": "main",
    "abstract": "Large language models (LLMs) are increasingly used for social-science simulations, yet most evaluations target task optimality rather than the variability and adaptation characteristic of human decision-making. We propose a process-oriented evaluation framework with progressive interventions (Intrinsicality, Instruction, and Imitation), and apply it to two classic economics tasks: the second-price auction and the newsvendor inventory problem.By default, LLMs adopt stable, conservative strategies that diverge from observed human behavior. Giving LLMs risk-framed instructions makes them behave more like humans. However, this also causes complex irregularities. Incorporating human decision trajectories via in-context learning further narrows distributional gaps, indicating that models can absorb human patterns. However, across all interventions, LLMs underexpress round-to-round variability relative to humans, revealing a persistent alignment gap in behavioral fidelity. Future evaluations of LLM-based social simulations should prioritize process-level realism",
    "checked": true,
    "id": "8eb54765d7474badccb04bb44d675f191173f3b1",
    "semantic_title": "noise, adaptation, and strategy: assessing llm fidelity in decision-making",
    "citation_count": 0,
    "authors": [
      "Yuanjun Feng",
      "Vivek Choudhary",
      "Yash Raj Shrestha"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.392": {
    "title": "Structuring Radiology Reports: Challenging LLMs with Lightweight Models",
    "volume": "main",
    "abstract": "Radiology reports are critical for clinical decision-making but often lack a standardized format, limiting both human interpretability and machine learning (ML) applications. While large language models (LLMs) have shown strong capabilities in reformatting clinical text, their high computational requirements, lack of transparency, and data privacy concerns hinder practical deployment. To address these challenges, we explore lightweight encoder-decoder models (<300M parameters)—specifically T5 and BERT2BERT—for structuring radiology reports from the MIMIC-CXR and CheXpert Plus datasets. We benchmark these models against eight open-source LLMs (1B–70B parameters), adapted using prefix prompting, in-context learning (ICL), and low-rank adaptation (LoRA) finetuning. Our best-performing lightweight model outperforms all LLMs adapted using prompt-based techniques on a human-annotated test set. While some LoRA-finetuned LLMs achieve modest gains over the lightweight model on the Findings section (BLEU 6.4%, ROUGE-L 4.8%, BERTScore 3.6%, F1-RadGraph 1.1%, GREEN 3.6%, and F1-SRR-BERT 4.3%), these improvements come at the cost of substantially greater computational resources. For example, LLaMA-3-70B incurred more than 400 times the inference time, cost, and carbon emissions compared to the lightweight model. These results underscore the potential of lightweight, task-specific models as sustainable and privacy-preserving solutions for structuring clinical text in resource-constrained healthcare settings",
    "checked": true,
    "id": "3d7fc1824d6e6fde1b68c48015c9e3870ffd03db",
    "semantic_title": "structuring radiology reports: challenging llms with lightweight models",
    "citation_count": 0,
    "authors": [
      "Johannes Moll",
      "Louisa Fay",
      "Asfandyar Azhar",
      "Sophie Ostmeier",
      "Sergios Gatidis",
      "Tim C. Lueth",
      "Curtis Langlotz",
      "Jean-Benoit Delbrouck"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.393": {
    "title": "PricingLogic: Evaluating LLMs Reasoning on Complex Tourism Pricing Tasks",
    "volume": "main",
    "abstract": "We present PricingLogic, the first benchmarkthat probes whether Large Language Mod-els (LLMs) can reliably automate tourism-booking prices when multiple, overlapping farerules apply. Travel agencies are eager to of-fload this error-prone task to AI systems; how-ever, deploying LLMs without verified reliabil-ity could result in significant financial lossesand erode customer trust. PricingLogic com-prises 300 natural-language questions based onbooking requests derived from 42 real-worldpricing policies, spanning two levels of diffi-culty: (i) basic customer-type pricing and (ii)bundled-tour calculations involving interactingdiscounts. Evaluations of a line of LLMs re-veal a steep performance drop on the harder tier,exposing systematic failures in rule interpreta-tion and arithmetic reasoning. These resultshighlight that, despite their general capabilities,today's LLMs remain unreliable for revenue-critical applications without further safeguardsor domain adaptation. Our code and dataset areavaliable in https://github.com/EIT-NLP/PricingLogic",
    "checked": true,
    "id": "0562c0556f84897a02c79edc83462e912b361200",
    "semantic_title": "pricinglogic: evaluating llms reasoning on complex tourism pricing tasks",
    "citation_count": 0,
    "authors": [
      "Yunuo Liu",
      "Dawei Zhu",
      "Zena Al-Khalili",
      "Dai Cheng",
      "Yanjun Chen",
      "Dietrich Klakow",
      "Wei Zhang",
      "Xiaoyu Shen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.394": {
    "title": "EcoTune: Token-Efficient Multi-Fidelity Hyperparameter Optimization for Large Language Model Inference",
    "volume": "main",
    "abstract": "Tuning inference hyperparameters, such as temperature and maximum output tokens, on downstream tasks can enhance inference performance. However, directly applying hyperparameter optimization to these hyperparameters is token-expensive. Multi-fidelity optimization improves HPO efficiency with low-fidelity evaluations, but its static scheduling strategies ignore token consumption, leading to high costs. To address these limitations, we propose a token-efficient multi-fidelity optimization method, which enhances inference performance and minimizes token usage. Our method is empowered by (i) a token-based fidelity definition with explicit token cost modeling on configurations; (ii) a novel Token-Aware Expected Improvement acquisition function that selects configurations based on performance gain per token; and (iii) a dynamic fidelity scheduling mechanism that adapts to real-time budget status. We evaluate our method on LLaMA-2 and LLaMA-3 series across MMLU, Humaneval, MedQA, and OpenBookQA. Our method improves over the HELM leaderboard by 7.1%, 24.3%, 21.9%, and 4.6%, respectively. Compared to existing multi-fidelity HPO baselines, our method reduces token consumption by over 80% while maintaining or surpassing performance, demonstrating the state-of-the-art token efficiency for inference-time optimization",
    "checked": true,
    "id": "2a1428178e5de106213b0b770f5ac5b0330df506",
    "semantic_title": "ecotune: token-efficient multi-fidelity hyperparameter optimization for large language model inference",
    "citation_count": 0,
    "authors": [
      "Yuebin Xu",
      "Zhiyi Chen",
      "Zeyi Wen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.395": {
    "title": "Investigating Value-Reasoning Reliability in Small Large Language Models",
    "volume": "main",
    "abstract": "Although small Large Language models (sLLMs) have been widely deployed in practical applications, little attention has been paid to their value-reasoning abilities, particularly in terms of reasoning reliability. To address this gap, we propose a systematic evaluation framework for assessing the Value-Reasoning Reliability of sLLMs. We define Value-Reasoning Reliability as comprising: (1) Output consistency under identical prompts, (2) Output Robustness under semantically equivalent prompts, (3) Maintaining stable value reasoning in the face of attacks, and (4) Consistency of value reasoning in open-ended value expression tasks. Our framework includes three core tasks: Repetition Consistency task, Interaction Stability task, and Open-ended Expression Consistency task. We further incorporate self-reported confidence scores to evaluate the model's value reasoning reliability from two perspectives: the model's self-awareness of its values, and its value-based decision-making. Our findings show that models vary significantly in their stability when responding to value-related questions. Moreover, we observe considerable output randomness, which is not always correlated with the self-reported confidence or expressed value preferences. This suggests that current models lack a reliable internal mechanism for stable value reasoning when addressing value-sensitive queries",
    "checked": true,
    "id": "651ffde8f3781375f8edd6c2fb53e57d03a732b0",
    "semantic_title": "investigating value-reasoning reliability in small large language models",
    "citation_count": 0,
    "authors": [
      "Xia Du",
      "Shuhan Sun",
      "Pengyuan Liu",
      "Dong Yu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.396": {
    "title": "Can LLMs Explain Themselves Counterfactually?",
    "volume": "main",
    "abstract": "Explanations are an important tool for gaining insights into model behavior, calibrating user trust, and ensuring compliance.Past few years have seen a flurry of methods for generating explanations, many of which involve computing model gradients or solving specially designed optimization problems.Owing to the remarkable reasoning abilities of LLMs, *self-explanation*, i.e., prompting the model to explain its outputs has recently emerged as a new paradigm.We study a specific type of self-explanations, *self-generated counterfactual explanations* (SCEs).We test LLMs' ability to generate SCEs across families, sizes, temperatures, and datasets. We find that LLMs sometimes struggle to generate SCEs. When they do, their prediction often does not agree with their own counterfactual reasoning",
    "checked": true,
    "id": "961577c90243029b13ae2310b37455ba9423dc34",
    "semantic_title": "can llms explain themselves counterfactually?",
    "citation_count": 2,
    "authors": [
      "Zahra Dehghanighobadi",
      "Asja Fischer",
      "Muhammad Bilal Zafar"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.397": {
    "title": "Self-Adjust Softmax",
    "volume": "main",
    "abstract": "The softmax function is crucial in Transformer attention, which normalizes each row of the attention scores with summation to one. **Usually, tokens with larger attention scores are important for the final prediction.However, the softmax function can face a gradient vanishing issue for such important tokens (e.g., probabilities close to one), leading to optimization difficulties for the important tokens so that the performance may not be better.**In this paper, we propose Self-Adjust Softmax (SA-Softmax) to address this issue by modifying softmax(z) to z ⋅ softmax(z) and its normalized variant (z - min(z\\min,0))⁄max(0,zmax)-min(zmin,0) ⋅ softmax(z).We theoretically show that SA-Softmax provides enhanced gradient properties compared to the vanilla softmax function.Moreover, Attention can be seamlessly integrated into existing Transformer models to their attention mechanisms with minor adjustments.We conducted experiments to evaluate the empirical performance of Transformer models using compared to the vanilla softmax function. These experiments, involving models with up to 2.7 billion parameters, are conducted across diverse datasets, language tasks, and positional encoding methods",
    "checked": true,
    "id": "19feca8bb1cc18885d487a0c6347123f63e88c4a",
    "semantic_title": "self-adjust softmax",
    "citation_count": 3,
    "authors": [
      "Chuanyang Zheng",
      "Yihang Gao",
      "Guoxuan Chen",
      "Han Shi",
      "Jing Xiong",
      "Xiaozhe Ren",
      "Chao Huang",
      "Zhenguo Li",
      "Yu Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.398": {
    "title": "DiscoSG: Towards Discourse-Level Text Scene Graph Parsing through Iterative Graph Refinement",
    "volume": "main",
    "abstract": "Vision-Language Models (VLMs) generate discourse-level, multi-sentence visual descriptions, challenging text scene graph parsers built for single-sentence caption-to-graph mapping. Current approaches typically merge sentence-level parsing outputs for discourse input, often missing phenomena like cross-sentence coreference, resulting in fragmented graphs and degraded downstream VLM task performance. We introduce a new task, Discourse-level text Scene Graph parsing (DiscoSG), and release DiscoSG-DS, a dataset of 400 expert-annotated and 8,430 synthesised multi-sentence caption-graph pairs. Each caption averages 9 sentences, and each graph contains at least 3× more triples than those in existing datasets. Fine-tuning GPT-4o on DiscoSG-DS yields over 40% higher SPICE than the strongest sentence-merging baseline. However, its high inference cost and licensing restrict open-source use, and smaller fine-tuned open-source models (e.g., Flan-T5) perform poorly on dense graph generation. To bridge this gap, we propose DiscoSG-Refiner, which drafts a base graph using a seed parser and iteratively refines it with a second model, improving robustness for complex graph generation. Using two small fine-tuned Flan-T5-Base models, DiscoSG-Refiner improves SPICE by ~30% over the baseline while achieving 86× faster inference than GPT-4o. It also delivers consistent gains on downstream VLM tasks, including discourse-level caption evaluation and hallucination detection, outperforming alternative parsers. Code and data are available at https://github.com/ShaoqLin/DiscoSG",
    "checked": true,
    "id": "606884d939050acfd81542f23a4350173e81da51",
    "semantic_title": "discosg: towards discourse-level text scene graph parsing through iterative graph refinement",
    "citation_count": 0,
    "authors": [
      "Shaoqing Lin",
      "Chong Teng",
      "Fei Li",
      "Donghong Ji",
      "Lizhen Qu",
      "Zhuang Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.399": {
    "title": "XAutoLM: Efficient Fine-Tuning of Language Models via Meta-Learning and AutoML",
    "volume": "main",
    "abstract": "Experts in machine learning leverage domain knowledge to navigate decisions in model selection, hyperparameter optimization, and resource allocation. This is particularly critical for fine-tuning language models (LMs), where repeated trials incur substantial computational overhead and environmental impact. However, no existing automated framework simultaneously tackles the entire model selection and hyperparameter optimization (HPO) task for resource-efficient LM fine-tuning. We introduce XAutoLM, a meta-learning-augmented AutoML framework that reuses past experiences to optimize discriminative and generative LM fine-tuning pipelines efficiently. XAutoLM learns from stored successes and failures by extracting task- and system-level meta-features to bias its sampling toward valuable configurations and away from costly dead ends. On four text classification and two question-answering benchmarks, XAutoLM surpasses zero-shot optimizer's peak F1 on five of six tasks, cuts mean evaluation time of pipelines by up to 4.5x, reduces search error ratios by up to sevenfold, and uncovers up to 50% more pipelines above the zero-shot Pareto front. In contrast, simpler memory-based baselines suffer negative transfer. We release XAutoLM and our experience store to catalyze resource-efficient, Green AI fine-tuning in the NLP community",
    "checked": true,
    "id": "2fdcd01758082514d8d40e5d5d5cdd00968302de",
    "semantic_title": "xautolm: efficient fine-tuning of language models via meta-learning and automl",
    "citation_count": 0,
    "authors": [
      "Ernesto Luis Estevanell Valladares",
      "Suilan Estevez-Velarde",
      "Yoan Gutierrez",
      "Andrés Montoyo",
      "Ruslan Mitkov"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.400": {
    "title": "UNCERTAINTY-LINE: Length-Invariant Estimation of Uncertainty for Large Language Models",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have become indispensable tools across various applications, making it more important than ever to ensure the quality and the trustworthiness of their outputs. This has led to growing interest in uncertainty quantification (UQ) methods for assessing the reliability of LLM outputs. Many existing UQ techniques rely on token probabilities, which inadvertently introduces a bias with respect to the length of the output. While some methods attempt to account for this, we demonstrate that such biases persist even in length-normalized approaches. To address the problem, here we propose UNCERTAINTY-LINE (Length-INvariant Estimation), a simple debiasing procedure that regresses uncertainty scores on output length and uses the residuals as corrected, length-invariant estimates. Our method is post-hoc, model-agnostic, and applicable to a range of UQ measures. Through extensive evaluation on machine translation, summarization, and question-answering tasks, we demonstrate that UNCERTAINTY-LINE consistently improves over even nominally length-normalized UQ methods uncertainty estimates across multiple metrics and models. We release our code publicly at https://github.com/stat-ml/uncertainty-line",
    "checked": true,
    "id": "171183622ae115b23dfcf696abf15a38db7c9649",
    "semantic_title": "uncertainty-line: length-invariant estimation of uncertainty for large language models",
    "citation_count": 0,
    "authors": [
      "Roman Vashurin",
      "Maiya Goloburda",
      "Preslav Nakov",
      "Maxim Panov"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.401": {
    "title": "WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning",
    "volume": "main",
    "abstract": "While reinforcement learning (RL) has demonstrated remarkable success in enhancing large language models (LLMs), it has primarily focused on single-turn tasks such as solving math problems. Training effective web agents for multi-turn interactions remains challenging due to the complexity of long-horizon decision-making across dynamic web interfaces. In this work, we present WebAgent-R1, a simple yet effective end-to-end multi-turn RL framework for training web agents. It learns directly from online interactions with web environments by asynchronously generating diverse trajectories, entirely guided by binary rewards depending on task success. Experiments on the WebArena-Lite benchmark demonstrate the effectiveness of WebAgent-R1, boosting the task success rate of Qwen-2.5-3B from 6.1% to 33.9% and LLaMA-3.1-8B from 8.5% to 44.8%, significantly outperforming existing state-of-the-art methods and strong proprietary models such as OpenAI o3. In-depth analyses reveal the effectiveness of the thinking-based prompting strategy and test-time scaling through increased interactions for web tasks. We further investigate different RL initialization policies by introducing two variants, namely WebAgent-R1-Zero and WebAgent-R1-CoT, which highlight the importance of the warm-up training stage (i.e., behavior cloning) and provide insights on incorporating long chain-of-thought (CoT) reasoning in web agents",
    "checked": true,
    "id": "a343a24a0d47e0acf4d71bfd0215924bc76d0256",
    "semantic_title": "webagent-r1: training web agents via end-to-end multi-turn reinforcement learning",
    "citation_count": 46,
    "authors": [
      "Zhepei Wei",
      "Wenlin Yao",
      "Yao Liu",
      "Weizhi Zhang",
      "Qin Lu",
      "Liang Qiu",
      "Changlong Yu",
      "Puyang Xu",
      "Chao Zhang",
      "Bing Yin",
      "Hyokun Yun",
      "Lihong Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.402": {
    "title": "Same evaluation, more tokens: On the effect of input length for machine translation evaluation using Large Language Models",
    "volume": "main",
    "abstract": "Accurately evaluating machine-translated text remains a long-standing challenge, particularly for long documents. Recent work has shown that large language models (LLMs) can serve as reliable and interpretable sentence-level translation evaluators via MQM error span annotations. With modern LLMs supporting larger context windows, a natural question arises: can we feed entire document translations into an LLM for quality assessment? Ideally, evaluation should be invariant to text length, producing consistent error spans regardless of input granularity. However, our analysis shows that text length significantly impacts evaluation: longer texts lead to fewer error spans and reduced system ranking accuracy. To address this limitation, we evaluate several strategies, including granularity-aligned prompting, Focus Sentence Prompting (FSP), and a fine-tuning approach to better align LLMs with the evaluation task. The latter two methods largely mitigate this length bias, making LLMs more reliable for long-form translation evaluation",
    "checked": true,
    "id": "43b3740583c7b651e9f5e121b2def93b64f12d53",
    "semantic_title": "same evaluation, more tokens: on the effect of input length for machine translation evaluation using large language models",
    "citation_count": 1,
    "authors": [
      "Tobias Domhan",
      "Dawei Zhu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.403": {
    "title": "PAKTON: A Multi-Agent Framework for Question Answering in Long Legal Agreements",
    "volume": "main",
    "abstract": "Contract review is a complex and time-intensive task that typically demands specialized legal expertise, rendering it largely inaccessible to non-experts. Moreover, legal interpretation is rarely straightforward—ambiguity is pervasive, and judgments often hinge on subjective assessments. Compounding these challenges, contracts are usually confidential, restricting their use with proprietary models and necessitating reliance on open-source alternatives. To address these challenges, we introduce PAKTON: a fully open-source, end-to-end, multi-agent framework with plug-and-play capabilities. PAKTON is designed to handle the complexities of contract analysis through collaborative agent workflows and a novel retrieval-augmented generation (RAG) component, enabling automated legal document review that is more accessible, adaptable, and privacy-preserving. Experiments demonstrate that PAKTON outperforms both general-purpose and pretrained models in predictive accuracy, retrieval performance, explainability, completeness, and grounded justifications as evaluated through a human study and validated with automated metrics",
    "checked": true,
    "id": "278d8df9ce87555dabd41f7cf56784116232138a",
    "semantic_title": "pakton: a multi-agent framework for question answering in long legal agreements",
    "citation_count": 0,
    "authors": [
      "Raptopoulos Petros",
      "Giorgos Filandrianos",
      "Maria Lymperaiou",
      "Giorgos Stamou"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.404": {
    "title": "PoSum-Bench: Benchmarking Position Bias in LLM-based Conversational Summarization",
    "volume": "main",
    "abstract": "Large language models (LLMs) are increasingly used for zero-shot conversation summarization, but often exhibit positional bias—tending to overemphasize content from the beginning or end of a conversation while neglecting the middle. To address this issue, we introduce PoSum-Bench, a comprehensive benchmark for evaluating positional bias in conversational summarization, featuring diverse English and French conversational datasets spanning formal meetings, casual conversations, and customer service interactions. We propose a novel semantic similarity-based sentence-level metric to quantify the direction and magnitude of positional bias in model-generated summaries, enabling systematic and reference-free evaluation across conversation positions, languages, and conversational contexts.Our benchmark and methodology thus provide the first systematic, cross-lingual framework for reference-free evaluation of positional bias in conversational summarization, laying the groundwork for developing more balanced and unbiased summarization models",
    "checked": true,
    "id": "cd975c99226d55907406440f4f283bd002fba924",
    "semantic_title": "posum-bench: benchmarking position bias in llm-based conversational summarization",
    "citation_count": 0,
    "authors": [
      "Xu Sun",
      "Lionel Delphin-Poulat",
      "Christèle Tarnec",
      "Anastasia Shimorina"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.405": {
    "title": "ConCISE: Confidence-guided Compression in Step-by-step Efficient Reasoning",
    "volume": "main",
    "abstract": "Large Reasoning Models (LRMs) perform strongly in complex reasoning tasks via Chain-of-Thought (CoT) prompting, but often suffer from verbose outputs, increasing computational overhead. Existing fine-tuning-based compression methods either operate post-hoc pruning, risking disruption to reasoning coherence, or rely on sampling-based selection, which fails to remove redundant content thoroughly. To address these limitations, this work begins by framing two key patterns of redundant reflection in LRMs—Confidence Deficit, wherein the model reflects on correct intermediate steps, and Termination Delay, where reflection continues after a verified, confident answer—through a confidence-guided perspective. Based on this, we introduce ConCISE (Confidence-guided Compression In Step-by-step Efficient Reasoning), a framework designed to generate concise reasoning chains, integrating Confidence Injection to boost reasoning confidence, and Early Stopping to terminate reasoning when confidence is sufficient. Extensive experiments demonstrate that compared to baseline methods, fine-tuning LRMs on ConCISE-generated data yields a better balance between compression and task performance, reducing length by up to ～50% under SimPO, while maintaining high task accuracy",
    "checked": true,
    "id": "3ce1c4a05f06c18f7c3598c061cb6980253b17ba",
    "semantic_title": "concise: confidence-guided compression in step-by-step efficient reasoning",
    "citation_count": 18,
    "authors": [
      "Ziqing Qiao",
      "Yongheng Deng",
      "Jiali Zeng",
      "Dong Wang",
      "Lai Wei",
      "Guanbo Wang",
      "Fandong Meng",
      "Jie Zhou",
      "Ju Ren",
      "Yaoxue Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.406": {
    "title": "Layer-Aware Representation Filtering: Purifying Finetuning Data to Preserve LLM Safety Alignment",
    "volume": "main",
    "abstract": "With rapid advancement and increasing accessibility of LLMs, fine-tuning aligned models has become a critical step for adapting them to real-world applications, which makes the safety of this fine-tuning process more important than ever. However, recent studies have highlighted a critical challenge: even when fine-tuning with seemingly benign downstream datasets, the safety of aligned LLMs can be compromised, making them more susceptible to malicious instructions. In this paper, we show that fine-tuning datasets often contain samples with safety-degrading features that are not easily identifiable on the surface. These samples can significantly degrade the safety alignment of LLMs during fine-tuning. To address this issue, we propose LARF, a Layer-Aware Representation Filtering method. This method identifies safety-sensitive layers within the LLM and leverages their representations to detect which data samples in the post-training dataset contain safety-degrading features. Experimental results demonstrate that LARF can effectively identify benign data with safety-degrading features. After removing such data, the safety alignment degradation caused by fine-tuning is mitigated",
    "checked": true,
    "id": "100c56afc6f331d96f9dcca74fea9af795f820ea",
    "semantic_title": "layer-aware representation filtering: purifying finetuning data to preserve llm safety alignment",
    "citation_count": 5,
    "authors": [
      "Hao Li",
      "Lijun Li",
      "Zhenghao Lu",
      "Xianyi Wei",
      "Rui Li",
      "Jing Shao",
      "Lei Sha"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.407": {
    "title": "Cross-domain Rumor Detection via Test-Time Adaptation and Large Language Models",
    "volume": "main",
    "abstract": "Rumor detection on social media has become crucial due to the rapid spread of misinformation. Existing approaches primarily focus on within-domain tasks, resulting in suboptimal performance in cross-domain scenarios due to domain shift. To address this limitation, we draw inspiration from the strong generalization capabilities of Test-Time Adaptation (TTA) and propose a novel framework to enhance rumor detection performance across different domains. Specifically, we introduce Test-Time Adaptation for Rumor Detection (T2ARD), which incorporates both single-domain model and target graph adaptation strategies tailored to the unique requirements of cross-domain rumor detection. T2ARD utilizes a graph adaptation module that updates the graph structure and node attributes through multi-level self-supervised contrastive learning, aiming to derive invariant graph representations. To mitigate the impact of significant distribution shifts on self-supervised signals, T2ARD performs model adaptation by using annotations from Large Language Models (LLMs) on target graph to produce pseudo-labels as supervised signals. Experiments conducted on four widely used cross-domain datasets demonstrate that T2ARD achieves state-of-the-art performance, surpassing existing methods in rumor detection",
    "checked": true,
    "id": "e0ab41c064ca313a3f536933b7ad434fec38f28c",
    "semantic_title": "cross-domain rumor detection via test-time adaptation and large language models",
    "citation_count": 0,
    "authors": [
      "Yuxia Gong",
      "Shuguo Hu",
      "Huaiwen Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.408": {
    "title": "MLWQ: Efficient Small Language Model Deployment via Multi-Level Weight Quantization",
    "volume": "main",
    "abstract": "Small language models (SLMs) are gaining attention for their lower computational and memory needs while maintaining strong performance. However, efficiently deploying SLMs on resource-constrained devices remains a significant challenge. Post-training quantization(PTQ) is a widely used compression technique that reduces memory usage and inference computation, yet existing methods face challenges in inefficient bit-width allocation and insufficient fine-grained quantization adjustments, leading to suboptimal performance, particularly at lower bit-widths. To address these challenges, we propose multi-level weight quantization (MLWQ), which facilitates the efficient deployment of SLMs. Our method enables more effective bit-width allocation by jointly considering inter-layer loss and intra-layer salience. Furthermore, we propose a fine-grained partitioning of intra-layer salience to support the tweaking of quantization parameters within each group. Experimental results indicate that MLWQ achieves competitive performance compared to state-of-the-art methods, providing an effective approach for the efficient deployment of SLMs while maintaining model accuracy",
    "checked": true,
    "id": "5c5e5cc24d90701cba3cfa6b2496e5ed09a6c047",
    "semantic_title": "mlwq: efficient small language model deployment via multi-level weight quantization",
    "citation_count": 0,
    "authors": [
      "Chun Hu",
      "Junhui He",
      "Shangyu Wu",
      "Yuxin He",
      "Chun Jason Xue",
      "Qingan Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.409": {
    "title": "ToDi: Token-wise Distillation via Fine-Grained Divergence Control",
    "volume": "main",
    "abstract": "Large language models (LLMs) offer impressive performance but are impractical for resource-constrained deployment due to high latency and energy consumption. Knowledge distillation (KD) addresses this by transferring knowledge from a large teacher to a smaller student model. However, conventional KD, notably approaches like Forward KL (FKL) and Reverse KL (RKL), apply uniform divergence loss across the entire vocabulary, neglecting token-level prediction discrepancies. By investigating these representative divergences via gradient analysis, we reveal that FKL boosts underestimated tokens, while RKL suppresses overestimated ones, showing their complementary roles. Based on this observation, we propose Token-wise Distillation (ToDi), a novel method that adaptively combines FKL and RKL per token using a sigmoid-based weighting function derived from the teacher-student probability log-ratio. ToDi dynamically emphasizes the appropriate divergence for each token, enabling precise distribution alignment. We demonstrate that ToDi consistently outperforms recent distillation baselines using uniform or less granular strategies across instruction-following benchmarks. Extensive ablation studies and efficiency analysis further validate ToDi's effectiveness and practicality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seongryong Jung",
      "Suwan Yoon",
      "DongGeon Kim",
      "Hwanhee Lee"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.410": {
    "title": "RethinkMCTS: Refining Erroneous Thoughts in Monte Carlo Tree Search for Code Generation",
    "volume": "main",
    "abstract": "Tree search methods have demonstrated impressive performance in code generation. Previous methods combine tree search with reflection that summarizes past mistakes to achieve iterative improvement. However, these methods face significant challenges. First, they search directly within the code language space, neglecting the underlying reasoning process critical for effective code generation. Second, reflection-based approaches merely accumulate historical errors in memory without providing correct reasoning pathways, making it difficult for subsequent search iterations to identify optimal solutions, resulting in decreased search quality. In this work, we propose RethinkMCTS, a framework that systematically explores and refines the reasoning process for code generation. Specifically, we employ MCTS to search for thoughts before code generation and integrate MCTS with a refinement mechanism called rethink, which incorporates fine-grained code execution feedback to refine erroneous thoughts during the search. It ensures the search path aligns with better reasoning, improving overall search quality. Through extensive experiments, we demonstrate that RethinkMCTS outperforms previous search-based and feedback-enhanced code generation baselines",
    "checked": true,
    "id": "1cffb1289d048caecb939ebbbb143ed863d6712f",
    "semantic_title": "rethinkmcts: refining erroneous thoughts in monte carlo tree search for code generation",
    "citation_count": 20,
    "authors": [
      "Qingyao Li",
      "Wei Xia",
      "Xinyi Dai",
      "Kounianhua Du",
      "Weiwen Liu",
      "Yasheng Wang",
      "Ruiming Tang",
      "Yong Yu",
      "Weinan Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.411": {
    "title": "Probing for Arithmetic Errors in Language Models",
    "volume": "main",
    "abstract": "We investigate whether internal activations in language models can be used to detect arithmetic errors. Starting with a controlled setting of 3-digit addition, we show that simple probes can accurately decode both the model's predicted output and the correct answer from hidden states, regardless of whether the model's output is correct. Building on this, we train lightweight error detectors that predict model correctness with over 90% accuracy. We then extend our analysis to structured chain-of-thought traces on addition-only GSM8K problems and find that probes trained on simple arithmetic generalize well to this more complex setting, revealing consistent internal representations. Finally, we demonstrate that these probes can guide selective re-prompting of erroneous reasoning steps, improving task accuracy with minimal disruption to correct outputs. Our findings suggest that arithmetic errors can be anticipated from internal activations alone, and that simple probes offer a viable path toward lightweight model self-correction",
    "checked": true,
    "id": "c13d100ccb7a759f7c31d9f369b775e3b7de5652",
    "semantic_title": "probing for arithmetic errors in language models",
    "citation_count": 2,
    "authors": [
      "Yucheng Sun",
      "Alessandro Stolfo",
      "Mrinmaya Sachan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.412": {
    "title": "NILE: Internal Consistency Alignment in Large Language Models",
    "volume": "main",
    "abstract": "Recent advances show that the world knowledge in the Instruction Fine-Tuning (IFT) dataset, which is incompatible with LLMs' internal knowledge, can greatly hurt the IFT performance. However, the effective integration and balancing of the internal knowledge of LLMs, acquired during pre-training, with existing IFT datasets remains a largely underexplored area of research. To address this gap, this work introduces NILE, a novel framework to optimize the effectiveness of IFT by adjusting IFT datasets through carefully aligning the world and internal knowledge. NILE employs a three-stage pipeline to effectively quantify and adjust consistency with the internal knowledge of target LLMs. Our analysis provides compelling evidence that balancing such consistency with pre-trained internal knowledge is pivotal for unleashing LLM potential, and confirms that NILE can systematically contribute to these substantial performance improvements. Experimental results demonstrate that NILE-aligned IFT datasets sharply boost LLM performance across multiple LLM ability evaluation datasets, achieving up to 66.6% gain on Arena-Hard and 68.5% on Alpaca-Eval V2",
    "checked": true,
    "id": "208b18709a20bbe518233441f3b080e72f4d4ec7",
    "semantic_title": "nile: internal consistency alignment in large language models",
    "citation_count": 0,
    "authors": [
      "Minda Hu",
      "Qiyuan Zhang",
      "Yufei Wang",
      "Bowei He",
      "Hongru Wang",
      "Jingyan Zhou",
      "Liangyou Li",
      "Yasheng Wang",
      "Chen Ma",
      "Irwin King"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.413": {
    "title": "Mining the Past with Dual Criteria: Integrating Three types of Historical Information for Context-aware Event Forecasting",
    "volume": "main",
    "abstract": "Event forecasting requires modeling historical event data to predict future events, and achieving accurate predictions depends on effectively capturing the relevant historical information that aids forecasting. Most existing methods focus on entities and structural dependencies to capture historical clues but often overlook implicitly relevant information. This limitation arises from overlooking event semantics and deeper factual associations that are not explicitly connected in the graph structure but are nonetheless critical for accurate forecasting. To address this, we propose a dual-criteria constraint strategy that leverages event semantics for relevance modeling and incorporates a self-supervised semantic filter based on factual event associations to capture implicitly relevant historical information. Building on this strategy, our method, termed ITHI (Integrating Three types of Historical Information), combines sequential event information, periodically repeated event information, and relevant historical information to achieve context-aware event forecasting. We evaluated the proposed ITHI method on three public benchmark datasets, achieving state-of-the-art performance and significantly outperforming existing approaches. Additionally, we validated its effectiveness on two structured temporal knowledge graph forecasting dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rong Ma",
      "Lei Wang",
      "Yating Yang",
      "Bo Ma",
      "Rui Dong",
      "Fengyi Yang",
      "Ahtamjan Ahmat",
      "Kaiwen Lu",
      "Xinyue Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.414": {
    "title": "RAGferee: Building Contextual Reward Models for Retrieval-Augmented Generation",
    "volume": "main",
    "abstract": "Existing Reward Models (RMs), typically trained on general preference data, struggle in Retrieval Augmented Generation (RAG) settings, which require judging responses for faithfulness to retrieved context, relevance to the user query, appropriate refusals when context is insufficient, completeness and conciseness of information. To address the lack of publicly available RAG-centric preference datasets and specialised RMs, we introduce RAGferee, a methodology that repurposes question-answering (QA) datasets into preference pairs that prioritise groundedness over stylistic features, enabling the training of contextual RMs better suited to judging RAG responses. Using RAGferee, we curate a small preference dataset of 4K samples and fine-tune RMs ranging from 7B to 24B parameters. Our RAG-centric RMs achieve state-of-the-art performance on ContextualJudgeBench, surpassing existing 70B+ RMs trained on much larger (up to 2.4M samples) general corpora, with an absolute improvement of +15.5%",
    "checked": true,
    "id": "028dc666c1c1362e5b4d060d311af862764055a2",
    "semantic_title": "ragferee: building contextual reward models for retrieval-augmented generation",
    "citation_count": 0,
    "authors": [
      "Andrei Catalin Coman",
      "Ionut Teodor Sorodoc",
      "Leonardo F. R. Ribeiro",
      "Bill Byrne",
      "James Henderson",
      "Adrià de Gispert"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.415": {
    "title": "Large Language Models Discriminate Against Speakers of German Dialects",
    "volume": "main",
    "abstract": "Dialects represent a significant component of human culture and are found across all regions of the world. In Germany, more than 40% of the population speaks a regional dialect (Adler and Hansen, 2022). However, despite cultural importance, individuals speaking dialects often face negative societal stereotypes. We examine whether such stereotypes are mirrored by large language models (LLMs). We draw on the sociolinguistic literature on dialect perception to analyze traits commonly associated with dialect speakers. Based on these traits, we assess the dialect naming bias and dialect usage bias expressed by LLMs in two tasks: association task and decision task. To assess a model's dialect usage bias, we construct a novel evaluation corpus that pairs sentences from seven regional German dialects (e.g., Alemannic and Bavarian) with their standard German counterparts. We find that: (1) in the association task, all evaluated LLMs exhibit significant dialect naming and dialect usage bias against German dialect speakers, reflected in negative adjective associations; (2) all models reproduce these dialect naming and dialect usage biases in their decision making; and (3) contrary to prior work showing minimal bias with explicit demographic mentions, we find that explicitly labeling linguistic demographics—German dialect speakers—amplifies bias more than implicit cues like dialect usage",
    "checked": true,
    "id": "24f369c7f866332768fd346e03e5f2484e17b1b5",
    "semantic_title": "large language models discriminate against speakers of german dialects",
    "citation_count": 0,
    "authors": [
      "Minh Duc Bui",
      "Carolin Holtermann",
      "Valentin Hofmann",
      "Anne Lauscher",
      "Katharina von der Wense"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.416": {
    "title": "Uncovering Argumentative Flow: A Question-Focus Discourse Structuring Framework",
    "volume": "main",
    "abstract": "Understanding the underlying argumentative flow in analytic argumentative writing is essential for discourse comprehension, especially in complex argumentative discourse such as think-tank commentary. However, existing structure modeling approaches often rely on surface-level topic segmentation, failing to capture the author's rhetorical intent and reasoning process. To address this limitation, we propose a Question-Focus discourse structuring framework that explicitly models the underlying argumentative flow by anchoring each argumentative unit to a guiding question (reflecting the author's intent) and a set of attentional foci (highlighting analytical pathways). To assess its effectiveness, we introduce an argument reconstruction task in which the modeled discourse structure guides both evidence retrieval and argument generation. We construct a high-quality dataset comprising 600 authoritative Chinese think-tank articles for experimental analysis. To quantitatively evaluate performance, we propose two novel metrics: (1) Claim Coverage, measuring the proportion of original claims preserved or similarly expressed in reconstructions, and (2) Evidence Coverage, assessing the completeness of retrieved supporting evidences. Experimental results show that our framework uncovers the author's argumentative logic more effectively and offers better structural guidance for reconstruction, yielding up to a 10% gain in claim coverage and outperforming strong baselines across both curated and LLM-based metrics",
    "checked": true,
    "id": "8306bbe3e3f69d136279c3a559afb294cadf242d",
    "semantic_title": "uncovering argumentative flow: a question-focus discourse structuring framework",
    "citation_count": 0,
    "authors": [
      "Yini Wang",
      "Xian Zhou",
      "Shengan Zheng",
      "Linpeng Huang",
      "Zhunchen Luo",
      "Wei Luo",
      "Xiaoying Bai"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.417": {
    "title": "AbsVis – Benchmarking How Humans and Vision-Language Models \"See\" Abstract Concepts in Images",
    "volume": "main",
    "abstract": "Abstract concepts like mercy and peace often lack clear visual grounding, and thus challenge humans and models to provide suitable image representations. To address this challenge, we introduce AbsVis – a dataset of 675 images annotated with 14,175 concept–explanation attributions from humans and two Vision-Language Models (VLMs: Qwen and LLaVA), where each concept is accompanied by a textual explanation. We compare human and VLM attributions in terms of diversity, abstractness, and alignment, and find that humans attribute more varied concepts. AbsVis also includes 2,680 human preference judgments evaluating the quality of a subset of these annotations, showing that overlapping concepts (attributed by both humans and VLMs) are most preferred. Explanations clarify and strengthen the perceived attributions, both from humans and VLMs. Explanations clarify and strengthen the perceived attributions, both from human and VLMs. Finally, we show that VLMs can approximate human preferences and use them to fine-tune VLMs via Direct Preference Optimization (DPO), yielding improved alignments with preferred concept–explanation pairs",
    "checked": true,
    "id": "8b974b7750f1c94e813f8afdf2351ea1fe9015b2",
    "semantic_title": "absvis – benchmarking how humans and vision-language models \"see\" abstract concepts in images",
    "citation_count": 0,
    "authors": [
      "Tarun Tater",
      "Diego Frassinelli",
      "Sabine Schulte im Walde"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.418": {
    "title": "A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) are increasingly used to generate synthetic textual data for training smaller specialized models. However, a comparison of various generation strategies for low-resource language settings is lacking. While various prompting strategies have been proposed—such as demonstrations, label-based summaries, and self-revision—their comparative effectiveness remains unclear, especially for low-resource languages. In this paper, we systematically evaluate the performance of these generation strategies and their combinations across 11 typologically diverse languages, including several extremely low-resource ones. Using three NLP tasks and four open-source LLMs, we assess downstream model performance on generated versus gold-standard data. Our results show that strategic combinations of generation methods — particularly target-language demonstrations with LLM-based revisions — yield strong performance, narrowing the gap with real data to as little as 5% in some settings. We also find that smart prompting techniques can reduce the advantage of larger LLMs, highlighting efficient generation strategies for synthetic data generation in low-resource scenarios with smaller models",
    "checked": true,
    "id": "5f588b2c153f1b7682c3b0690b5b8d1ea29a1fd9",
    "semantic_title": "a rigorous evaluation of llm data generation strategies for low-resource languages",
    "citation_count": 0,
    "authors": [
      "Tatiana Anikina",
      "Jan Cegin",
      "Jakub Simko",
      "Simon Ostermann"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.419": {
    "title": "Alignment with Fill-In-the-Middle for Enhancing Code Generation",
    "volume": "main",
    "abstract": "The code generation capabilities of Large Language Models (LLMs) have advanced applications like tool invocation and problem-solving. However, improving performance in code-related tasks remains challenging due to limited training data that is verifiable with accurate test cases. While Direct Preference Optimization (DPO) has shown promise, existing methods for generating test cases still face limitations. In this paper, we propose a novel approach that splits code snippets into smaller, granular blocks, creating more diverse DPO pairs from the same test cases. Additionally, we introduce the Abstract Syntax Tree (AST) splitting and curriculum training method to enhance the DPO training. Our approach demonstrates significant improvements in code generation tasks, as validated by experiments on benchmark datasets such as HumanEval (+), MBPP (+), APPS, LiveCodeBench, and BigCodeBench. Code and data are available at https://github.com/SenseLLM/StructureCoder",
    "checked": true,
    "id": "8ed4a4d278a8898e19b31e2af9e6d7275a5ae888",
    "semantic_title": "alignment with fill-in-the-middle for enhancing code generation",
    "citation_count": 0,
    "authors": [
      "Houxing Ren",
      "Zimu Lu",
      "Weikang Shi",
      "Haotian Hou",
      "Yunqiao Yang",
      "Ke Wang",
      "Aojun Zhou",
      "Junting Pan",
      "Mingjie Zhan",
      "Hongsheng Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.420": {
    "title": "A Middle Path for On-Premises LLM Deployment: Preserving Privacy Without Sacrificing Model Confidentiality",
    "volume": "main",
    "abstract": "Privacy-sensitive users require deploying large language models (LLMs) within their own infrastructure (on-premises) to safeguard private data and enable customization. However, vulnerabilities in local environments can lead to unauthorized access and potential model theft. To address this, prior research on small models has explored securing only the output layer within hardware-secured devices to balance model confidentiality and customization. Yet this approach fails to protect LLMs effectively. In this paper, we discover that (1) query-based distillation attacks targeting the secured top layer can produce a functionally equivalent replica of the victim model; (2) securing the same number of layers, bottom layers before a transition layer provide stronger protection against distillation attacks than top layers, with comparable effects on customization performance; and (3) the number of secured layers creates a trade-off between protection and customization flexibility. Based on these insights, we propose SOLID, a novel deployment framework that secures a few bottom layers in a secure environment and introduces an efficient metric to optimize the trade-off by determining the ideal number of hidden layers. Extensive experiments on five models (1.3B to 70B parameters) demonstrate that SOLID outperforms baselines, achieving a better balance between protection and downstream customization",
    "checked": true,
    "id": "befa5df856721b10c50d034594fbc7194d96e386",
    "semantic_title": "a middle path for on-premises llm deployment: preserving privacy without sacrificing model confidentiality",
    "citation_count": 3,
    "authors": [
      "Hanbo Huang",
      "Yihan Li",
      "Bowen Jiang",
      "Bo Jiang",
      "Lin Liu",
      "Zhuotao Liu",
      "Ruoyu Sun",
      "Shiyu Liang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.421": {
    "title": "Variance Sensitivity Induces Attention Entropy Collapse and Instability in Transformers",
    "volume": "main",
    "abstract": "Attention-based language models commonly rely on the softmax function to convert attention logits into probability distributions. However, this softmax re-weighting can lead to *attention entropy collapse*, in which attention disproportionately concentrates on a single token, ultimately causing training instability. In this work, we identify the high *variance sensitivity* of softmax as a primary cause of this collapse. We show that *entropy-stable* attention methods, which either control or are insensitive to the variance of attention logits, can prevent entropy collapse and enable more stable training. We provide empirical evidence of this effect in both large language models (LLMs) and a small Transformer model composed solely of self-attention and support our findings with theoretical analysis. Moreover, we identify that the concentration of attention probabilities increases the probability matrix norm, leading to the gradient exploding",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonghyun Hong",
      "Sungyoon Lee"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.422": {
    "title": "X-FLoRA: Cross-modal Federated Learning with Modality-expert LoRA for Medical VQA",
    "volume": "main",
    "abstract": "Medical visual question answering (VQA) and federated learning (FL) have emerged as vital approaches for enabling privacy-preserving, collaborative learning across clinical institutions. However, both these approaches face significant challenges in cross-modal FL scenarios, where each client possesses unpaired images from only one modality. To address this limitation, we propose X-FLoRA, a cross-modal FL framework that uses modality-expert low-rank adaptation (LoRA) for medical VQA. Specifically, X-FLoRA enables the synthesis of images from one modality to another without requiring data sharing between clients. This is achieved by training a backward translation model within a federated asymmetric translation scheme that integrates clinical semantics from textual data. Additionally, X-FLoRA introduces modality-expert LoRA, which fine-tunes separate LoRA modules to strengthen modality-specific representations in the VQA task. The server aggregates the trained backward translation models and fine-tuned LoRA modules using discriminator quality scores and expert-aware weighting, which regulate the relative contributions from different clients. Experiments were conducted on VQA datasets encompassing different medical modalities, and the results demonstrate that X-FLoRA outperforms existing FL methods in terms of VQA performance",
    "checked": true,
    "id": "fb1b0bfa8601cbc07cacf91cf24751380d689715",
    "semantic_title": "x-flora: cross-modal federated learning with modality-expert lora for medical vqa",
    "citation_count": 0,
    "authors": [
      "Min Hyuk Kim",
      "Changheon Kim",
      "Seok Bong Yoo"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.423": {
    "title": "Robust Native Language Identification through Agentic Decomposition",
    "volume": "main",
    "abstract": "Large language models (LLMs) often achieve high performance in native language identification (NLI) benchmarks by leveraging superficial contextual clues such as names, locations, and cultural stereotypes, rather than the underlying linguistic patterns indicative of native language (L1) influence. To improve robustness, previous work has instructed LLMs to disregard such clues. In this work, we demonstrate that such a strategy is unreliable and model predictions can be easily altered by misleading hints. To address this problem, we introduce an agentic NLI pipeline inspired by forensic linguistics, where specialized agents accumulate and categorize diverse linguistic evidence before an independent final overall assessment. In this final assessment, a goal-aware coordinating agent synthesizes all evidence to make the NLI prediction. On two benchmark datasets, our approach significantly enhances NLI robustness against misleading contextual clues and performance consistency compared to standard prompting methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahmet Yavuz Uluslu",
      "Tannon Kew",
      "Tilia Ellendorff",
      "Gerold Schneider",
      "Rico Sennrich"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.424": {
    "title": "ConsistentChat: Building Skeleton-Guided Consistent Multi-Turn Dialogues for Large Language Models from Scratch",
    "volume": "main",
    "abstract": "Current instruction data synthesis methods primarily focus on single-turn instructions and often neglect cross-turn coherence, resulting in context drift and reduced task completion rates in extended conversations. To address this limitation, we propose Skeleton-Guided Multi-Turn Dialogue Generation, a framework that constrains multi-turn instruction synthesis by explicitly modeling human conversational intent. It operates in two stages: (1) Intent Modeling, which captures the global structure of human dialogues by assigning each conversation to one of nine well-defined intent trajectories, ensuring a coherent and goal-oriented information flow; and (2) Skeleton Generation, which constructs a structurally grounded sequence of user queries aligned with the modeled intent, thereby serving as a scaffold that constrains and guides the downstream instruction synthesis process. Based on this process, we construct ConsistentChat, a multi-turn instruction dataset with approximately 15,000 multi-turn conversations and 224,392 utterances. Experiments on the Light, Topdial, and MT-Eval benchmarks show that models fine-tuned on ConsistentChat achieve a 20–30% improvement in chat consistency and up to a 15% increase in task success rate, significantly outperforming models trained on existing single-turn and multi-turn instruction datasets",
    "checked": false,
    "id": "4fed632be84043ff9bba51d80ecbb4d520cc9981",
    "semantic_title": "consistentchat: building skeleton-guided consistent dialogues for large language models from scratch",
    "citation_count": 3,
    "authors": [
      "Jiawei Chen",
      "Xinyan Guan",
      "Qianhao Yuan",
      "Mo Guozhao",
      "Weixiang Zhou",
      "Yaojie Lu",
      "Hongyu Lin",
      "Ben He",
      "Le Sun",
      "Xianpei Han"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.425": {
    "title": "Does Acceleration Cause Hidden Instability in Vision Language Models? Uncovering Instance-Level Divergence Through a Large-Scale Empirical Study",
    "volume": "main",
    "abstract": "Vision-Language Models (VLMs) are powerful yet computationally intensive for widespread practical deployments. To address such challenge without costly re-training, post-training acceleration techniques like quantization and token reduction are extensively explored. However, current acceleration evaluations primarily target minimal overall performance degradation, overlooking a crucial question: does the accelerated model still give the same answers to the same questions as it did before acceleration? This is vital for stability-centered industrial applications where consistently correct answers for specific, known situations are paramount, such as in AI-based disease diagnosis. We systematically investigate this for accelerated VLMs, testing four leading models (LLaVA-1.5, LLaVA-Next, Qwen2-VL, Qwen2.5-VL) with eight acceleration methods on ten multi-modal benchmarks. Our findings are stark: despite minimal aggregate performance drops, accelerated models changed original answers up to 20% of the time. Critically, up to 6.5% of these changes converted correct answers to incorrect. Input perturbations magnified these inconsistencies, and the trend is confirmed by case studies with the medical VLM LLaVA-Med. This research reveals a significant oversight in VLM acceleration, stressing an urgent need for instance-level stability checks to ensure trustworthy real-world deployment",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yizheng Sun",
      "Hao Li",
      "Chang Xu",
      "Hongpeng Zhou",
      "Chenghua Lin",
      "Riza Batista-Navarro",
      "Jingyuan Sun"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.426": {
    "title": "When Annotators Disagree, Topology Explains: Mapper, a Topological Tool for Exploring Text Embedding Geometry and Ambiguity",
    "volume": "main",
    "abstract": "Language models are often evaluated with scalar metrics like accuracy, but such measures fail to capture how models internally represent ambiguity, especially when human annotators disagree. We propose a topological perspective to analyze how fine-tuned models encode ambiguity and more generally instances.Applied to RoBERTa-Large on the MD-Offense dataset, Mapper, a tool from topological data analysis, reveals that fine-tuning restructures embedding space into modular, non-convex regions aligned with model predictions, even for highly ambiguous cases. Over 98% of connected components exhibit ≥ 90% prediction purity, yet alignment with ground-truth labels drops in ambiguous data, surfacing a hidden tension between structural confidence and label uncertainty.Unlike traditional tool such as PCA or UMAP, Mapper captures this geometry directly uncovering decision regions, boundary collapses, and overconfident clusters. Our findings position Mapper as a powerful diagnostic tool for understanding how models resolve ambiguity. Beyond visualization, it also enables topological metrics that may inform proactive modeling strategies in subjective NLP tasks",
    "checked": true,
    "id": "1ad8a323761b8b61499a79043388ea54bf754e7e",
    "semantic_title": "when annotators disagree, topology explains: mapper, a topological tool for exploring text embedding geometry and ambiguity",
    "citation_count": 0,
    "authors": [
      "Nisrine Rair",
      "Alban Goupil",
      "Valeriu Vrabie",
      "Emmanuel Chochoy"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.427": {
    "title": "Self-Critique and Refinement for Faithful Natural Language Explanations",
    "volume": "main",
    "abstract": "With the rapid development of Large Language Models (LLMs), Natural Language Explanations (NLEs) have become increasingly important for understanding model predictions. However, these explanations often fail to faithfully represent the model's actual reasoning process. While existing work has demonstrated that LLMs can self-critique and refine their initial outputs for various tasks, this capability remains unexplored for improving explanation faithfulness. To address this gap, we introduce Self-critique and Refinement for Natural Language Explanations (SR-NLE), a framework that enables models to improve the faithfulness of their own explanations – specifically, post-hoc NLEs – through an iterative critique and refinement process without external supervision. Our framework leverages different feedback mechanisms to guide the refinement process, including natural language self-feedback and, notably, a novel feedback approach based on feature attribution that highlights important input words. Our experiments across three datasets and four state-of-the-art LLMs demonstrate that SR-NLE significantly reduces unfaithfulness rates, with our best method achieving an average unfaithfulness rate of 36.02%, compared to 54.81% for baseline – an absolute reduction of 18.79%. These findings reveal that the investigated LLMs can indeed refine their explanations to better reflect their actual reasoning process, requiring only appropriate guidance through feedback without additional training or fine-tuning",
    "checked": true,
    "id": "42027330117bc232ae21e1b00b3f417b12d7047c",
    "semantic_title": "self-critique and refinement for faithful natural language explanations",
    "citation_count": 1,
    "authors": [
      "Yingming Wang",
      "Pepa Atanasova"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.428": {
    "title": "The Psychology of Falsehood: A Human-Centric Survey of Misinformation Detection",
    "volume": "main",
    "abstract": "Misinformation remains one of the most significant issues in the digital age. While automated fact-checking has emerged as a viable solution, most current systems are limited to evaluating factual accuracy. However, the detrimental effect of misinformation transcends simple falsehoods; it takes advantage of how individuals perceive, interpret, and emotionally react to information. This underscores the need to move beyond factuality and adopt more human-centered detection frameworks. In this survey, we explore the evolving interplay between traditional fact-checking approaches and psychological concepts such as cognitive biases, social dynamics, and emotional responses. By analyzing state-of-the-art misinformation detection systems through the lens of human psychology and behavior, we reveal critical limitations of current methods and identify opportunities for improvement. Additionally, we outline future research directions aimed at creating more robust and adaptive frameworks, such as neuro-behavioural models that integrate technological factors with the complexities of human cognition and social influence. These approaches offer promising pathways to more effectively detect and mitigate the societal harms of misinformation",
    "checked": true,
    "id": "d2b9ab7ef9c8eb2c788f190beb1d591ba0d5e5f6",
    "semantic_title": "the psychology of falsehood: a human-centric survey of misinformation detection",
    "citation_count": 0,
    "authors": [
      "Arghodeep Nandi",
      "Megha Sundriyal",
      "Euna Mehnaz Khan",
      "Jikai Sun",
      "Emily K. Vraga",
      "Jaideep Srivastava",
      "Tanmoy Chakraborty"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.429": {
    "title": "SEAL: Structure and Element Aware Learning Improves Long Structured Document Retrieval",
    "volume": "main",
    "abstract": "In long structured document retrieval, existing methods typically fine-tune pre-trained language models (PLMs) using contrastive learning on datasets lacking explicit structural information. This practice suffers from two critical issues: 1) current methods fail to leverage structural features and element-level semantics effectively, and 2) the lack of datasets containing structural metadata. To bridge these gaps, we propose SEAL, a novel contrastive learning framework. It leverages structure-aware learning to preserve semantic hierarchies and masked element alignment for fine-grained semantic discrimination. Furthermore, we release StructDocRetrieval, a long structured document retrieval dataset with rich structural annotations. Extensive experiments on both the released and industrial datasets across various modern PLMs, and online A/B testing demonstrate consistent improvements, boosting NDCG@10 from 73.96% to 77.84% on BGE-M3. The resources are available at https://github.com/xinhaoH/SEAL",
    "checked": true,
    "id": "13a8fab6e2dd52d664e1490f40de5dd6ba2c5341",
    "semantic_title": "seal: structure and element aware learning improves long structured document retrieval",
    "citation_count": 0,
    "authors": [
      "Xinhao Huang",
      "Zhibo Ren",
      "Yipeng Yu",
      "Ying Zhou",
      "Zulong Chen",
      "Zeyi Wen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.430": {
    "title": "AnchorAttention: Difference-Aware Sparse Attention with Stripe Granularity",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) with extended context lengths face significant computational challenges during the pre-filling phase, primarily due to the quadratic complexity of self-attention. Existing methods typically employ dynamic pattern matching and block-sparse low-level implementations. However, their reliance on local information for pattern identification fails to capture global contexts, and the coarse granularity of blocks leads to persistent internal sparsity, resulting in suboptimal accuracy and efficiency. To address these limitations, we propose AnchorAttention, a difference-aware, dynamic sparse attention mechanism that efficiently identifies critical attention regions at a finer stripe granularity while adapting to global contextual information, achieving superior speed and accuracy. AnchorAttention comprises three key components: (1) Pattern-based Anchor Computation, leveraging the commonalities present across all inputs to rapidly compute a set of near-maximum scores as anchor; (2) Difference-aware Stripe Sparsity Identification, performing difference-aware comparisons with anchor to quickly obtain discrete coordinates of significant regions in a stripe-like sparsity pattern; (3) Fine-grained Sparse Computation, replacing the traditional contiguous loading strategy with a discrete key-value loading approach to maximize sparsity rates while preserving hardware computational potential. Additionally, we integrate the identification strategy into a single operator to maximize parallelization potential. With its finer-grained sparsity strategy, AnchorAttention achieves higher sparsity rates at the same recall level, significantly reducing computation time. Compared to previous state-of-the-art methods, at a text length of 128k, it achieves a speedup of 1.44× while maintaining higher recall rates",
    "checked": true,
    "id": "71855b38433e4c29c790e26cef1a33255e58f345",
    "semantic_title": "anchorattention: difference-aware sparse attention with stripe granularity",
    "citation_count": 1,
    "authors": [
      "Yu Zhang",
      "Dong Guo",
      "Fang Wu",
      "Guoliang Zhu",
      "Dian Ding",
      "Yiming Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.431": {
    "title": "Attacks by Content: Automated Fact-checking is an AI Security Issue",
    "volume": "main",
    "abstract": "When AI agents retrieve and reason over external documents, adversaries can manipulate the data they receive to subvert their behaviour. Previous research has studied indirect prompt injection, where the attacker injects malicious instructions. We argue that injection of instructions is not necessary to manipulate agents – attackers could instead supply biased, misleading, or false information. We term this an *attack by content*. Existing defenses, which focus on detecting hidden commands, are ineffective against attacks by content. To defend themselves and their users, agents must critically evaluate retrieved information, corroborating claims with external evidence and evaluating source trustworthiness. We argue that this is analogous to an existing NLP task, automated fact-checking, which we propose to repurpose as a cognitive self-defense tool for agents",
    "checked": true,
    "id": "3a476459f3c073225d2ef04f0350e74a0ffc28e3",
    "semantic_title": "attacks by content: automated fact-checking is an ai security issue",
    "citation_count": 0,
    "authors": [
      "Michael Sejr Schlichtkrull"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.432": {
    "title": "MUZO: Leveraging Multiple Queries and Momentum for Zeroth-Order Fine-Tuning of Large Language Models",
    "volume": "main",
    "abstract": "Fine-tuning pre-trained large language models (LLMs) on downstream tasks has achieved significant success across various domains. However, as model sizes grow, traditional first-order fine-tuning algorithms incur substantial memory overhead due to the need for activation storage for back-propagation (BP). The BP-free Memory-Efficient Zeroth-Order Optimization (MeZO) method estimates gradients through finite differences, avoiding the storage of activation values, and has been demonstrated as a viable approach for fine-tuning large language models. This work proposes the Multiple-query Memory Efficient Zeroth-Order (MUZO) method, which is based on variance-reduced multiple queries to obtain the average of gradient estimates. When combined with Adam optimizer, MUZO-Adam demonstrates superior performance in fine-tuning various LLMs. Furthermore, we provide theoretical guarantees for the convergence of the MUZO-Adam optimizer. Extensive experiments empirically demonstrate that MUZO-Adam converges better than MeZO-SGD and achieves near first-order optimizer performance on downstream classification, multiple-choice, and generation tasks",
    "checked": true,
    "id": "321a9cf1a4284e6004ad34875d2422cd22db566b",
    "semantic_title": "muzo: leveraging multiple queries and momentum for zeroth-order fine-tuning of large language models",
    "citation_count": 0,
    "authors": [
      "Yuezhang Peng",
      "Yuxin Liu",
      "Fei Wen",
      "Xie Chen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.433": {
    "title": "Your Language Model Can Secretly Write Like Humans: Contrastive Paraphrase Attacks on LLM-Generated Text Detectors",
    "volume": "main",
    "abstract": "The misuse of large language models (LLMs), such as academic plagiarism, has driven the development of detectors to identify LLM-generated texts. To bypass these detectors, paraphrase attacks have emerged to purposely rewrite these texts to evade detection. Despite the success, existing methods require substantial data and computational budgets to train a specialized paraphraser, and their attack efficacy greatly reduces when faced with advanced detection algorithms. To address this, we propose Contrastive Paraphrase Attack (CoPA), a training-free method that effectively deceives text detectors using off-the-shelf LLMs. The first step is to carefully craft instructions that encourage LLMs to produce more human-like texts. Nonetheless, we observe that the inherent statistical biases of LLMs can still result in some generated texts carrying certain machine-like attributes that can be captured by detectors. To overcome this, CoPA constructs an auxiliary machine-like word distribution as a contrast to the human-like distribution generated by the LLM. By subtracting the machine-like patterns from the human-like distribution during the decoding process, CoPA is able to produce sentences that are less discernible by text detectors. Our theoretical analysis suggests the superiority of the proposed attack. Extensive experiments validate the effectiveness of CoPA in fooling text detectors across various scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Fang",
      "Jiawei Kong",
      "Tianqu Zhuang",
      "Yixiang Qiu",
      "Kuofeng Gao",
      "Bin Chen",
      "Shu-Tao Xia",
      "Yaowei Wang",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.434": {
    "title": "Will It Still Be True Tomorrow? Multilingual Evergreen Question Classification to Improve Trustworthy QA",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) often hallucinate in question answering (QA) tasks. A key yet underexplored factor contributing to this is the temporality of questions – whether they are evergreen (answers remain stable over time) or mutable (answers change). In this work, we introduce EverGreenQA, the first multilingual QA dataset with evergreen labels, supporting both evaluation and training. Using EverGreenQA, we benchmark 12 modern LLMs to assess whether they encode question temporality explicitly (via verbalized judgments) or implicitly (via uncertainty signals). We also train EG-E5, a lightweight multilingual classifier that achieves SoTA performance on this task. Finally, we demonstrate the practical utility of evergreen classification across three applications: improving self-knowledge estimation, filtering QA datasets, and explaining GPT-4o's retrieval behavior",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sergey Pletenev",
      "Maria Marina",
      "Nikolay Ivanov",
      "Daria Galimzianova",
      "Nikita Krayko",
      "Mikhail Salnikov",
      "Vasily Konovalov",
      "Alexander Panchenko",
      "Viktor Moskvoretskii"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.435": {
    "title": "Steering Language Models in Multi-Token Generation: A Case Study on Tense and Aspect",
    "volume": "main",
    "abstract": "Large language models (LLMs) are able to generate grammatically well-formed text, but how do they encode their syntactic knowledge internally? While prior work has focused largely on binary grammatical contrasts, in this work, we study the representation and control of two multidimensional hierarchical grammar phenomena—verb tense and aspect—and for each, identify distinct, orthogonal directions in residual space using linear discriminant analysis. Next, we demonstrate causal control over both grammatical features through concept steering across three generation tasks. Then, we use these identified features in a case study to investigate factors influencing effective steering in multi-token generation. We find that steering strength, location, and duration are crucial parameters for reducing undesirable side effects such as topic shift and degeneration. Our findings suggest that models encode tense and aspect in structurally organized, human-like ways, but effective control of such features during generation is sensitive to multiple factors and requires manual tuning or automated optimization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alina Klerings",
      "Jannik Brinkmann",
      "Daniel Ruffinelli",
      "Simone Paolo Ponzetto"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.436": {
    "title": "DocReRank: Single-Page Hard Negative Query Generation for Training Multi-Modal RAG Rerankers",
    "volume": "main",
    "abstract": "Rerankers play a critical role in multimodal Retrieval-Augmented Generation (RAG) by refining ranking of an initial set of retrieved documents. Rerankers are typically trained using hard negative mining, whose goal is to select pages for each query which rank high, but are actually irrelevant. However, this selection process is typically passive and restricted to what the retriever can find in the available corpus, leading to several inherent limitations. These include: limited diversity, negative examples which are often not hard enough, low controllability, and frequent false negatives which harm training. Our paper proposes an alternative approach: Single-Page Hard Negative Query Generation, which goes the other way around. Instead of retrieving negative pages per query, we generate hard negative queries per page. Using an automated LLM-VLM pipeline, and given a page and its positive query, we create hard negatives by rephrasing the query to be as similar as possible in form and context, yet not answerable from the page. This paradigm enables fine-grained control over the generated queries, resulting in diverse, hard, and targeted negatives. It also supports efficient false negative verification. Our experiments show that rerankers trained with data generated using our approach outperform existing models and significantly improve retrieval performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Navve Wasserman",
      "Oliver Heinimann",
      "Yuval Golbari",
      "Tal Zimbalist",
      "Eli Schwartz",
      "Michal Irani"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.437": {
    "title": "Reason to Rote: Rethinking Memorization in Reasoning",
    "volume": "main",
    "abstract": "Large language models readily memorize arbitrary training instances, such as label noise, yet they perform strikingly well on reasoning tasks. In this work, we investigate how language models memorize label noise, and why such memorization in many cases does not heavily affect generalizable reasoning capabilities. Using two controllable synthetic reasoning tasks with noisy labels, four-digit addition (FDA) and two-hop relational reasoning (THR), we discover a reliance of memorization on generalizable reasoning mechanisms: models continue to compute intermediate reasoning outputs even when retrieving memorized noisy labels, and intervening reasoning adversely affects memorization. We further show that memorization operates through distributed encoding, i.e., aggregating various inputs and intermediate results, rather than building a look-up mechanism from inputs to noisy labels. Moreover, our FDA case study reveals memorization occurs via outlier heuristics, where existing neuron activation patterns are slightly shifted to fit noisy labels. Together, our findings suggest that memorization of label noise in language models builds on, rather than overrides, the underlying reasoning mechanisms, shedding lights on the intriguing phenomenon of benign memorization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yupei Du",
      "Philipp Mondorf",
      "Silvia Casola",
      "Yuekun Yao",
      "Robert Litschko",
      "Barbara Plank"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.438": {
    "title": "VELA: An LLM-Hybrid-as-a-Judge Approach for Evaluating Long Image Captions",
    "volume": "main",
    "abstract": "In this study, we focus on the automatic evaluation of long and detailed image captions generated by multimodal Large Language Models (MLLMs). Most existing automatic evaluation metrics for image captioning are primarily designed for short captions and are not suitable for evaluating long captions. Moreover, recent LLM-as-a-Judge approaches suffer from slow inference due to their reliance on autoregressive inference and early fusion of visual information. To address these limitations, we propose VELA, an automatic evaluation metric for long captions developed within a novel LLM-Hybrid-as-a-Judge framework. Furthermore, we propose LongCap-Arena, a benchmark specifically designed for evaluating metrics for long captions. This benchmark comprises 7,805 images, the corresponding human-provided long reference captions and long candidate captions, and 32,246 human judgments from three distinct perspectives: Descriptiveness, Relevance, and Fluency. We demonstrated that VELA outperformed existing metrics and achieved superhuman performance on LongCap-Arena",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kazuki Matsuda",
      "Yuiga Wada",
      "Shinnosuke Hirano",
      "Seitaro Otsuki",
      "Komei Sugiura"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.439": {
    "title": "LLM-Independent Adaptive RAG: Let the Question Speak for Itself",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) are prone to hallucinations, and Retrieval-Augmented Generation (RAG) helps mitigate this, but at a high computational cost while risking misinformation. Adaptive retrieval aims to retrieve only when necessary, but existing approaches rely on LLM-based uncertainty estimation, which remains inefficient and impractical.In this study, we introduce lightweight LLM-independent adaptive retrieval methods based on external information. We investigated 27 features, organized into 7 groups, and their hybrid combinations. We evaluated these methods on 6 QA datasets, assessing the QA performance and efficiency. The results show that our approach matches the performance of complex LLM-based methods while achieving significant efficiency gains, demonstrating the potential of external information for adaptive retrieval",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maria Marina",
      "Nikolay Ivanov",
      "Sergey Pletenev",
      "Mikhail Salnikov",
      "Daria Galimzianova",
      "Nikita Krayko",
      "Vasily Konovalov",
      "Alexander Panchenko",
      "Viktor Moskvoretskii"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.440": {
    "title": "TurnBack: A Geospatial Route Cognition Benchmark for Large Language Models through Reverse Route",
    "volume": "main",
    "abstract": "Humans can interpret geospatial information through natural language, while the geospatial cognition capabilities of Large Language Models (LLMs) remain underexplored. Prior research in this domain has been constrained by non-quantifiable metrics, limited evaluation datasets; unclear research hierarchies further compound these limitations. Therefore, we propose a scalable benchmark and conduct a comprehensive evaluation of the geospatial route cognition of LLMs. We create a large-scale evaluation dataset comprised of 36000 routes from 12 metropolises. Then, we introduce PathBuilder, a novel tool for converting natural language instructions into navigation routes, and vice versa, bridging the gap between geospatial information and natural language. Finally, we propose a new evaluation framework and metrics to rigorously assess 9 state-of-the-art (SOTA) LLMs, on the task of route reversal. The benchmark reveals that LLMs exhibit limited ability to reverse routes: most of the reverse routes neither return to the starting point nor are similar to the optimal route. Additionally, LLMs face challenges such as low robustness in route generation and high confidence for their incorrect answers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyi Luo",
      "Qing Cheng",
      "Daniel Matos",
      "Hari Krishna Gadi",
      "Yanfeng Zhang",
      "Lu Liu",
      "Yongliang Wang",
      "Niclas Zeller",
      "Daniel Cremers",
      "Liqiu Meng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.441": {
    "title": "Certainty in Uncertainty: Reasoning over Uncertain Knowledge Graphs with Statistical Guarantees",
    "volume": "main",
    "abstract": "Uncertain knowledge graph embedding (UnKGE) methods learn vector representations that capture both structural and uncertainty information to predict scores of unseen triples. However, existing methods produce only point estimates, without quantifying predictive uncertainty—limiting their reliability in high-stakes applications where understanding confidence in predictions is crucial. To address this limitation, we propose UnKGCP, a framework that generates prediction intervals guaranteed to contain the true score with a user-specified level of confidence. The length of the intervals reflects the model's predictive uncertainty. UnKGCP builds on the conformal prediction framework but introduces a novel nonconformity measure tailored to UnKGE methods and an efficient procedure for interval construction. We provide theoretical guarantees for the intervals and empirically verify these guarantees. Extensive experiments on standard UKG benchmarks across diverse UnKGE methods further demonstrate that the intervals are sharp and effectively capture predictive uncertainty",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuqicheng Zhu",
      "Jingcheng Wu",
      "Yizhen Wang",
      "Hongkuan Zhou",
      "Jiaoyan Chen",
      "Evgeny Kharlamov",
      "Steffen Staab"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.442": {
    "title": "Beyond Seen Data: Improving KBQA Generalization Through Schema-Guided Logical Form Generation",
    "volume": "main",
    "abstract": "Knowledge base question answering (KBQA) aims to answer user questions in natural language using rich human knowledge stored in large KBs. As current KBQA methods struggle with unseen knowledge base elements and their novel compositions at test time, we introduce SG-KBQA — a novel model that injects schema contexts into entity retrieval and logical form generation to tackle this issue. It exploits information about the semantics and structure of the knowledge base provided by schema contexts to enhance generalizability. We show that achieves strong generalizability, outperforming state-of-the-art models on two commonly used benchmark datasets across a variety of test settings. Our source code is available at https://github.com/gaosx2000/SG_KBQA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengxiang Gao",
      "Jey Han Lau",
      "Jianzhong Qi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.443": {
    "title": "A Training-Free Length Extrapolation Approach for LLMs: Greedy Attention Logit Interpolation",
    "volume": "main",
    "abstract": "Transformer-based Large Language Models (LLMs) struggle with inputs exceeding their training context window due to positional out-of-distribution (O.O.D.) issues that disrupt attention. Existing solutions, including fine-tuning and training-free methods, face challenges like inefficiency, redundant interpolation, logit outliers, or loss of local positional information. We propose Greedy Attention Logit Interpolation (GALI), a training-free method that improves length extrapolation by greedily reusing pretrained positional intervals and interpolating attention logits to eliminate outliers. GALI achieves stable and superior performance across a wide range of long-context tasks without requiring input-length-specific tuning. Our analysis further reveals that LLMs interpret positional intervals unevenly and that restricting interpolation to narrower ranges improves performance, even on short-context tasks. GALI represents a step toward more robust and generalizable long-text processing in LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Li",
      "Tianyi Zhang",
      "Zechuan Li",
      "Caren Han"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.444": {
    "title": "Taming Text-to-Image Synthesis for Novices: User-centric Prompt Generation via Multi-turn Guidance",
    "volume": "main",
    "abstract": "The emergence of text-to-image synthesis (TIS) models has significantly influenced digital image creation by producing high-quality visuals from written descriptions. Yet these models are sensitive on textual prompts, posing a challenge for novice users who may not be familiar with TIS prompt writing. Existing solutions relieve this via automatic prompt expansion or generation from a user query. However, this single-turn manner suffers from limited user-centricity in terms of result interpretability and user interactivity. Thus, we propose DialPrompt, a dialogue-based TIS prompt generation model that emphasizes user experience for novice users. DialPrompt is designed to follow a multi-turn workflow, where in each round of dialogue the model guides user to express their preferences on possible optimization dimensions before generating the final TIS prompt. To achieve this, we mined 15 essential dimensions for high-quality prompts from advanced users and curated a multi-turn dataset. Through training on this dataset, DialPrompt improves user-centricity by allowing users to perceive and control the creation process of TIS prompts. Experiments indicate that DialPrompt improves significantly in user-centricity score compared with existing approaches while maintaining a competitive quality of synthesized images. In our user evaluation, DialPrompt is highly rated by 19 human reviewers (especially novices)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilun Liu",
      "Minggui He",
      "Feiyu Yao",
      "Yuhe Ji",
      "Shimin Tao",
      "Jingzhou Du",
      "Justin Li",
      "Jian Gao",
      "Zhang Li",
      "Hao Yang",
      "Boxing Chen",
      "Osamu Yoshie"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.445": {
    "title": "We Need to Measure Data Diversity in NLP — Better and Broader",
    "volume": "main",
    "abstract": "Although diversity in NLP datasets has received growing attention, the question of how to measure it remains largely underexplored. This opinion paper examines the conceptual and methodological challenges of measuring data diversity and argues that interdisciplinary perspectives are essential for developing more fine-grained and valid measures",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong Nguyen",
      "Esther Ploeger"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.446": {
    "title": "Sheaf Discovery with Joint Computation Graph Pruning and Flexible Granularity",
    "volume": "main",
    "abstract": "In this paper, we introduce DiscoGP, a novel framework for extracting self-contained modular units, or sheaves, within neural language models (LMs). Sheaves extend the concept of functional circuits, a unit widely explored in interpretability research, by considering not only subsets of edges in an LM's computation graph but also the model's weight parameters. Our framework identifies sheaves through a gradient-based pruning algorithm that operates on both of these in such a way that reduces the original LM to a sparse skeleton that preserves certain core capabilities. Experimental results demonstrate that, across a range of linguistic and reasoning tasks, DiscoGP extracts sheaves that preserve 93-100% of a model's performance on the identified task while comprising only 1-7% of the original weights and connections. Furthermore, our analysis reveals that, compared to previously identified LM circuits, the sheaves discovered by DiscoGP exhibit superior modularity and functional fidelity. Extending our method to the neuron level also unveils novel insights into the inner workings of LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Yu",
      "Jingcheng Niu",
      "Zining Zhu",
      "Xi Chen",
      "Gerald Penn"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.447": {
    "title": "Hierarchical Bracketing Encodings Work for Dependency Graphs",
    "volume": "main",
    "abstract": "We revisit hierarchical bracketing encodings from a practical perspective in the context of dependency graph parsing. The approach encodes graphs as sequences, enabling linear-time parsing with n tagging actions, and still representing reentrancies, cycles, and empty nodes. Compared to existing graph linearizations, this representation substantially reduces the label space while preserving structural information. We evaluate it on a multilingual and multi-formalism benchmark, showing competitive results and consistent improvements over other methods in exact match accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ana Ezquerro",
      "Carlos Gómez-Rodríguez",
      "David Vilares"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.448": {
    "title": "Multimodal Fine-grained Context Interaction Graph Modeling for Conversational Speech Synthesis",
    "volume": "main",
    "abstract": "Conversational Speech Synthesis (CSS) aims to generate speech with natural prosody by understanding the multimodal dialogue history (MDH). The latest work predicts the accurate prosody expression of the target utterance by modeling the utterance-level interaction characteristics of MDH and the target utterance. However, MDH contains fine-grained semantic and prosody knowledge at the word level. Existing methods overlook the fine-grained semantic and prosodic interaction modeling. To address this gap, we propose MFCIG-CSS, a novel Multimodal Fine-grained Context Interaction Graph-based CSS system. Our approach constructs two specialized multimodal fine-grained dialogue interaction graphs: a semantic interaction graph and a prosody interaction graph. These two interaction graphs effectively encode interactions between word-level semantics, prosody, and their influence on subsequent utterances in MDH. The encoded interaction features are then leveraged to enhance synthesized speech with natural conversational prosody. Experiments on the DailyTalk dataset demonstrate that MFCIG-CSS outperforms all baseline models in terms of prosodic expressiveness. Code and speech samples are available at https://github.com/AI-S2-Lab/MFCIG-CSS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenqi Jia",
      "Rui Liu",
      "Berrak Sisman",
      "Haizhou Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.449": {
    "title": "Judging Quality Across Languages: A Multilingual Approach to Pretraining Data Filtering with Language Models",
    "volume": "main",
    "abstract": "High-quality multilingual training data is essential for effectively pretraining large language models (LLMs). Yet, the availability of suitable open-source multilingual datasets remains limited. Existing state-of-the-art datasets mostly rely on heuristic filtering methods, restricting both their cross-lingual transferability and scalability. Here, we introduce JQL, a systematic approach that efficiently curates diverse and high-quality multilingual data at scale while significantly reducing computational demands. JQL distills LLMs' annotation capabilities into lightweight annotators based on pretrained multilingual embeddings. These models exhibit robust multilingual and cross-lingual performance, even for languages and scripts unseen during training. Evaluated empirically across 35 languages, the resulting annotation pipeline substantially outperforms current heuristic filtering methods like Fineweb2. JQL notably enhances downstream model training quality and increases data retention rates. Our research provides practical insights and valuable resources for multilingual data curation, raising the standards of multilingual dataset development",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mehdi Ali",
      "Manuel Brack",
      "Max Lübbering",
      "Elias Wendt",
      "Abbas Goher Khan",
      "Richard Rutmann",
      "Alex Jude",
      "Maurice Kraus",
      "Alexander Arno Weber",
      "Felix Stollenwerk",
      "David Kaczér",
      "Florian Mai",
      "Lucie Flek",
      "Rafet Sifa",
      "Nicolas Flores-Herr",
      "Joachim Koehler",
      "Patrick Schramowski",
      "Michael Fromm",
      "Kristian Kersting"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.450": {
    "title": "Conditional [MASK] Discrete Diffusion Language Model",
    "volume": "main",
    "abstract": "Although auto-regressive models excel in natural language processing, they often struggle to generate diverse text and provide limited controllability. Non-auto-regressive methods could be an alternative but often produce degenerate outputs and exhibit shortcomings in conditional generation. To address these challenges, we propose Diffusion-EAGS, a novel framework that integrates conditional masked language models into diffusion language models through the theoretical lens of a conditional Markov Random Field. In doing so, we propose entropy-adaptive Gibbs sampling and entropy-based noise scheduling to counterbalance each model's shortcomings. Experimental results show that Diffusion-EAGS outperforms baselines and achieves the best quality-diversity tradeoff, demonstrating its effectiveness in non-autoregressive text generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyukhun Koh",
      "Minha Jhang",
      "Dohyung Kim",
      "Sangmook Lee",
      "Kyomin Jung"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.451": {
    "title": "Language-Guided Temporal Token Pruning for Efficient VideoLLM Processing",
    "volume": "main",
    "abstract": "Vision Language Models (VLMs) struggle with long-form videos due to the quadratic complexity of attention mechanisms. We propose Language-Guided Temporal Token Pruning (LGTTP), which leverages temporal cues from queries to adaptively prune video tokens, preserving contextual continuity while reducing computational overhead. Unlike uniform pruning or keyframe selection, LGTTP retains higher token density in temporally relevant segments. Our model-agnostic framework integrates with TimeChat and LLaVA-Video, achieving a 65% reduction in computation while preserving 97-99% of the original performance. On QVHighlights, LGTTP improves HIT@1 by +9.5%, and on Charades-STA, it retains 99.6% of R@1. It excels on queries with explicit temporal markers and remains effective across general video understanding tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yogesh Kumar"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.452": {
    "title": "A Fully Probabilistic Perspective on Large Language Model Unlearning: Evaluation and Optimization",
    "volume": "main",
    "abstract": "Large Language Model Unlearning (LLMU) is a promising way to remove private or sensitive information from large language models. However, the comprehensive evaluation of LLMU remains underexplored. The dominant deterministic evaluation can yield overly optimistic assessments of unlearning efficacy. To mitigate this, we propose a Fully Probabilistic Evaluation (FPE) framework that incorporates input and output distributions in LLMU evaluation. FPE obtains a probabilistic evaluation result by querying unlearned models with various semantically similar inputs and multiple sampling attempts. We introduce an Input Distribution Sampling method in FPE to select high-quality inputs, enabling a stricter measure of information leakage risks. Furthermore, we introduce a Contrastive Embedding Loss (CEL) to advance the performance of LLMU. CEL employs contrastive learning to distance latent representations of unlearned samples from adaptively clustered contrast samples while aligning them with random vectors, leading to improved efficacy and robustness for LLMU. Our experiments show that FPE uncovers more unlearned information leakage risks than prior evaluation methods, and CEL improves unlearning effectiveness by at least 50.1% and robustness by at least 37.2% on Llama-2-7B while retaining high model utility",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anda Cheng",
      "Wei Huang",
      "Yinggui Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.453": {
    "title": "IIET: Efficient Numerical Transformer via Implicit Iterative Euler Method",
    "volume": "main",
    "abstract": "High-order numerical methods enhance Transformer performance in tasks like NLP and CV, but introduce a performance-efficiency trade-off due to increased computational overhead. Our analysis reveals that conventional efficiency techniques, such as distillation, can be detrimental to the performance of these models, exemplified by PCformer. To explore more optimizable ODE-based Transformer architectures, we propose the Iterative Implicit Euler Transformer (IIET), which simplifies high-order methods using an iterative implicit Euler approach. This simplification not only leads to superior performance but also facilitates model compression compared to PCformer. To enhance inference efficiency, we introduce Iteration Influence-Aware Distillation (IIAD). Through a flexible threshold, IIAD allows users to effectively balance the performance-efficiency trade-off. On lm-evaluation-harness, IIET boosts average accuracy by 2.65% over vanilla Transformers and 0.8% over PCformer. Its efficient variant, E-IIET, significantly cuts inference overhead by 55% while retaining 99.4% of the original task accuracy. Moreover, the most efficient IIET variant achieves an average performance gain exceeding 1.6% over vanilla Transformer with comparable speed",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Liu",
      "Bei Li",
      "Jiahao Liu",
      "Junhao Ruan",
      "Kechen Jiao",
      "Hongyin Tang",
      "Jingang Wang",
      "Tong Xiao",
      "JingBo Zhu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.454": {
    "title": "WebEvolver: Enhancing Web Agent Self-Improvement with Co-evolving World Model",
    "volume": "main",
    "abstract": "Agent self-improvement, where agents autonomously train their underlying Large Language Model (LLM) on self-sampled trajectories, shows promising results but often stagnates in web environments due to limited exploration and under-utilization of pretrained web knowledge. To improve the performance of self-improvement, we propose a novel framework that introduces a co-evolving World Model LLM. This world model predicts the next observation based on the current observation and action within the web environment. The World Model serves dual roles: (1) as a virtual web server generating self-instructed training data to continuously refine the agent's policy, and (2) as an imagination engine during inference, enabling look-ahead simulation to guide action selection for the agent LLM. Experiments in real-world web environments (Mind2Web-Live, WebVoyager, and GAIA-web) show a 10% performance gain over existing self-evolving agents, demonstrating the efficacy and generalizability of our approach, without using any distillation from more powerful close-sourced models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianqing Fang",
      "Hongming Zhang",
      "Zhisong Zhang",
      "Kaixin Ma",
      "Wenhao Yu",
      "Haitao Mi",
      "Dong Yu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.455": {
    "title": "Leveraging Semantic Triples for Private Document Generation with Local Differential Privacy Guarantees",
    "volume": "main",
    "abstract": "Many works at the intersection of Differential Privacy (DP) in Natural Language Processing aim to protect privacy by transforming texts under DP guarantees. This can be performed in a variety of ways, from word perturbations to full document rewriting, and most often under *local* DP. Here, an input text must be made indistinguishable from any other potential text, within some bound governed by the privacy parameter 𝜀. Such a guarantee is quite demanding, and recent works show that privatizing texts under local DP can only be done reasonably under very high 𝜀 values. Addressing this challenge, we introduce **DP-ST**, which leverages semantic triples for neighborhood-aware private document generation under local DP guarantees. Through the evaluation of our method, we demonstrate the effectiveness of the *divide-and-conquer* paradigm, particularly when limiting the DP notion (and privacy guarantees) to that of a *privatization neighborhood*. When combined with LLM post-processing, our method allows for coherent text generation even at lower 𝜀 values, while still balancing privacy and utility. These findings highlight the importance of coherence in achieving balanced privatization outputs at reasonable 𝜀 levels",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stephen Meisenbacher",
      "Maulik Chevli",
      "Florian Matthes"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.456": {
    "title": "HVGuard: Utilizing Multimodal Large Language Models for Hateful Video Detection",
    "volume": "main",
    "abstract": "The rapid growth of video platforms has transformed information dissemination and led to an explosion of multimedia content. However, this widespread reach also introduces risks, as some users exploit these platforms to spread hate speech, which is often concealed through complex rhetoric, making hateful video detection a critical challenge. Existing detection methods rely heavily on unimodal analysis or simple feature fusion, struggling to capture cross-modal interactions and reason through implicit hate in sarcasm and metaphor. To address these limitations, we propose HVGuard, the first reasoning-based hateful video detection framework with multimodal large language models (MLLMs). Our approach integrates Chain-of-Thought (CoT) reasoning to enhance multimodal interaction modeling and implicit hate interpretation. Additionally, we design a Mixture-of-Experts (MoE) network for efficient multimodal fusion and final decision-making. The framework is modular and extensible, allowing flexible integration of different MLLMs and encoders. Experimental results demonstrate that HVGuard outperforms all existing advanced detection tools, achieving an improvement of 6.88% to 13.13% in accuracy and 9.21% to 34.37% in M-F1 on two public datasets covering both English and Chinese",
    "checked": true,
    "id": "db876c689dee5e4c2f7809593c0659a987039361",
    "semantic_title": "hvguard: utilizing multimodal large language models for hateful video detection",
    "citation_count": 0,
    "authors": [
      "Yiheng Jing",
      "Mingming Zhang",
      "Yong Zhuang",
      "Jiacheng Guo",
      "Juan Wang",
      "Xiaoyang Xu",
      "Wenzhe Yi",
      "Keyan Guo",
      "Hongxin Hu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.457": {
    "title": "Accelerate Parallelizable Reasoning via Parallel Decoding within One Sequence",
    "volume": "main",
    "abstract": "Recent advances in reasoning models have demonstrated significant improvements in accuracy by employing detailed and comprehensive reasoning processes. However, generating these lengthy reasoning sequences is computationally expensive and time-consuming. To address this inefficiency, we leverage the inherent parallelizability of certain tasks to accelerate the reasoning process. Specifically, when multiple parallel reasoning steps exist, we decode multiple tokens per forward pass via a tree-like attention mask within a single sequence, avoiding additional memory usage. Experimental results show that our method achieves up to nearly 100% speedup in decoding while basically maintaining the answer quality. Our code is available in https://github.com/yuyijiong/parallel-decoding-in-one-sequence",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yijiong Yu",
      "Wei Wang",
      "Ran Chen",
      "Ji Pei"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.458": {
    "title": "SlideCoder: Layout-aware RAG-enhanced Hierarchical Slide Generation from Design",
    "volume": "main",
    "abstract": "Manual slide creation is labor-intensive and requires expert prior knowledge. Existing natural language-based LLM generation methods struggle to capture the visual and structural nuances of slide designs. To address this, we formalize the Reference Image to Slide Generation task and propose Slide2Code, the first benchmark with difficulty-tiered samples based on a novel Slide Complexity Metric. We introduce SlideCoder, a layout-aware, retrieval-augmented framework for generating editable slides from reference images. SlideCoder integrates a Color Gradient-based Segmentation algorithm and a Hierarchical Retrieval-Augmented Generation method to decompose complex tasks and enhance code generation. We also release SlideMaster, a 7B open-source model fine-tuned with improved reverse-engineered data. Experiments show that SlideCoder outperforms state-of-the-art baselines by up to 40.5 points, demonstrating strong performance across layout fidelity, execution accuracy, and visual consistency. Our code is available at https://github.com/vinsontang1/SlideCoder",
    "checked": true,
    "id": "98379520e26e3ea62187849b00bec667ccdd247a",
    "semantic_title": "slidecoder: layout-aware rag-enhanced hierarchical slide generation from design",
    "citation_count": 6,
    "authors": [
      "Wenxin Tang",
      "Jingyu Xiao",
      "Wenxuan Jiang",
      "Xi Xiao",
      "Yuhang Wang",
      "Xuxin Tang",
      "Qing Li",
      "Yuehe Ma",
      "Junliang Liu",
      "Shisong Tang",
      "Michael R. Lyu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.459": {
    "title": "LLM-OREF: An Open Relation Extraction Framework Based on Large Language Models",
    "volume": "main",
    "abstract": "The goal of open relation extraction (OpenRE) is to develop an RE model that can generalize to new relations not encountered during training. Existing studies primarily formulate OpenRE as a clustering task. They first cluster all test instances based on the similarity between the instances, and then manually assign a new relation to each cluster. However, their reliance on human annotation limits their practicality. In this paper, we propose an OpenRE framework based on large language models (LLMs), which directly predicts new relations for test instances by leveraging their strong language understanding and generation abilities, without human intervention. Specifically, our framework consists of two core components: (1) a relation discoverer (RD), designed to predict new relations for test instances based on demonstrations formed by training instances with known relations; and (2) a relation predictor (RP), used to select the most likely relation for a test instance from n candidate relations, guided by demonstrations composed of their instances. To enhance the ability of our framework to predict new relations, we design a self-correcting inference strategy composed of three stages: relation discovery, relation denoising, and relation prediction. In the first stage, we use RD to preliminarily predict new relations for all test instances. Next, we apply RP to select some high-reliability test instances for each new relation from the prediction results of RD through a cross-validation method. During the third stage, we employ RP to re-predict the relations of all test instances based on the demonstrations constructed from these reliable test instances. Extensive experiments on three OpenRE datasets demonstrate the effectiveness of our framework. We release our code at https://github.com/XMUDeepLIT/LLM-OREF.git",
    "checked": true,
    "id": "cb8bdaaf512ce26f6eaf9aec3e4eb53ce0aeb52e",
    "semantic_title": "llm-oref: an open relation extraction framework based on large language models",
    "citation_count": 0,
    "authors": [
      "Hongyao Tu",
      "Liang Zhang",
      "Yujie Lin",
      "Xin Lin",
      "Haibo Zhang",
      "Long Zhang",
      "Jinsong Su"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.460": {
    "title": "Ambiguity Awareness Optimization: Towards Semantic Disambiguation for Direct Preference Optimization",
    "volume": "main",
    "abstract": "Direct Preference Optimization (DPO) is a widely used reinforcement learning from human feedback (RLHF) method across various domains. The study of token importance has attracted widespread attention in DPO. Researchers have found that token importance is crucial for improving the effectiveness of DPO. It is observed that identical or semantically similar content (defined as ambiguous content) frequently appears within the preference pairs. We hypothesize that the presence of ambiguous content during DPO training may introduce ambiguity, thereby limiting further improvements in alignment. Through mathematical analysis and proof-of-concept experiments, we reveal that ambiguous content may potentially introduce ambiguities, thereby degrading performance. To address this issue, we introduce Ambiguity Awareness Optimization (AAO), a simple yet effective approach that automatically re-weights ambiguous content to reduce ambiguities by calculating semantic similarity from preference pairs. Through extensive experiments, we demonstrate that AAO consistently and significantly surpasses state-of-the-art approaches in performance, without markedly increasing response length, across multiple model scales and widely adopted benchmark datasets, including AlpacaEval 2, MT-Bench, and Arena-Hard. Specifically, AAO outperforms DPO by up to 8.9 points on AlpacaEval 2 and achieves an improvement of by up to 15.0 points on Arena-Hard",
    "checked": true,
    "id": "aca39f338e5ff73300934fda9fffc00f18c904e6",
    "semantic_title": "ambiguity awareness optimization: towards semantic disambiguation for direct preference optimization",
    "citation_count": 0,
    "authors": [
      "Jian Li",
      "Shenglin Yin",
      "Yujia Zhang",
      "Alan Zhao",
      "Xi Chen",
      "Xiaohui Zhou",
      "Pengfei Xu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.461": {
    "title": "Improving Multilingual Retrieval-Augmented Language Models through Dialectic Reasoning Argumentations",
    "volume": "main",
    "abstract": "Retrieval-augmented generation (RAG) is key to improving large language models (LLMs) in systematically accessing richer factual knowledge. Yet, using RAG mechanisms brings intrinsic challenges, as LLMs must deal with conflicting knowledge, especially in multilingual retrieval, where the heterogeneity of knowledge retrieved may deliver different outlooks. To make RAG more analytical, critical and grounded, we introduce Dialectic-RAG (DRAG), a modular approach guided by Argumentative Explanations, i.e., structured reasoning process that systematically evaluates retrieved information by comparing, contrasting, and resolving conflicting perspectives. Given a query and a set of multilingual related documents, selects and exemplifies relevant knowledge for delivering dialectic explanations that, by critically weighing opposing arguments and filtering extraneous content, clearly determine the final response. We show the impact of our framework both as an in-context learning strategy and for constructing demonstrations to instruct smaller models. Our experiments demonstrate that significantly improves RAG approaches, requiring low-impact computational effort and providing robustness to knowledge perturbations",
    "checked": true,
    "id": "406eb631bbb8f499a93e6b73a3e49b295563f8d7",
    "semantic_title": "improving multilingual retrieval-augmented language models through dialectic reasoning argumentations",
    "citation_count": 1,
    "authors": [
      "Leonardo Ranaldi",
      "Federico Ranaldi",
      "Fabio Massimo Zanzotto",
      "Barry Haddow",
      "Alexandra Birch"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.462": {
    "title": "Predicate-Guided Generation for Mathematical Reasoning",
    "volume": "main",
    "abstract": "We present Prolog-MATH, a curated corpus designed to support mathematical reasoning in large language models (LLMs) through logic programming. Each verbal math problem in the dataset is paired with a chain-of-thought explanation to generate Prolog program via a two-stage automated pipeline. In the first stage, an LLM (e.g., Deepseek-V3) predicts a set of relevant mathematical predicates that could be useful in solving the problem. In the second stage, the LLM uses these suggested predicates along with the expected answer type to gen- erate a complete Prolog program. To improve coverage, we fine-tune an open-source LLM us- ing supervised fine-tuning, followed by GRPO (Group Relative Policy Optimization) training to address problems that Deepseek-V3 fails to solve. To support this training, we propose a predicate-aware reward function that evaluates how well the generated solution incorporates the suggested predicates, complementing the standard binary reward. Experimental results show that: 1) Our two-stage pipeline achieves 81.3% solution coverage on the MATH training set; 2) GRPO training with the predicate-aware reward function enables a series of base models to correctly solve additional problems missed by Deepseek-V3, further increasing solution coverage to 97.4%. Data and source code can be obtained at the Github repository",
    "checked": true,
    "id": "86a1e65a80a27be6dc142b929f74eb03c044db06",
    "semantic_title": "predicate-guided generation for mathematical reasoning",
    "citation_count": 0,
    "authors": [
      "Jiajun Chen",
      "Yik-Cheung Tam"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.463": {
    "title": "ComplexTempQA: A 100m Dataset for Complex Temporal Question Answering",
    "volume": "main",
    "abstract": "We introduce ComplexTempQA,a large-scale dataset consisting of over 100 million question-answer pairs designed to tackle the challenges in temporal question answering. ComplexTempQA significantly surpasses existing benchmarks in scale and scope. Utilizing Wikipedia and Wikidata, the dataset covers questions spanning over two decades and offers an unmatched scale. We introduce a new taxonomy that categorizes questions as attributes, comparisons, and counting questions, revolving around events, entities, and time periods, respectively. A standout feature of ComplexTempQA is the high complexity of its questions, which demand reasoning capabilities for answering such as across-time comparison, temporal aggregation, and multi-hop reasoning involving temporal event ordering and entity recognition. Additionally, each question is accompanied by detailed metadata, including specific time scopes, allowing for comprehensive evaluation of temporal reasoning abilities of large language models",
    "checked": true,
    "id": "7f4bc71a88e2aef8d2d9212c8e2c444a47dbed3a",
    "semantic_title": "complextempqa: a 100m dataset for complex temporal question answering",
    "citation_count": 0,
    "authors": [
      "Raphael Gruber",
      "Abdelrahman Abdallah",
      "Michael Färber",
      "Adam Jatowt"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.464": {
    "title": "ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic Iterative Reasoning Agents",
    "volume": "main",
    "abstract": "Understanding information from visually rich documents remains a significant challenge for traditional Retrieval-Augmented Generation (RAG) methods. Existing benchmarks predominantly focus on image-based question answering (QA), overlooking the fundamental challenges of efficient retrieval, comprehension, and reasoning within dense visual documents. To bridge this gap, we introduce ViDoSeek, a novel dataset designed to evaluate RAG performance on visually rich documents requiring complex reasoning. Based on it, we identify key limitations in current RAG approaches: (i) purely visual retrieval methods struggle to effectively integrate both textual and visual features, and (ii) previous approaches often allocate insufficient reasoning tokens, limiting their effectiveness. To address these challenges, we propose ViDoRAG, a novel multi-agent RAG framework tailored for complex reasoning across visual documents. ViDoRAG employs a Gaussian Mixture Model (GMM)-based hybrid strategy to effectively handle multi-modal retrieval. To further elicit the model's reasoning capabilities, we introduce an iterative agent workflow incorporating exploration, summarization, and reflection, providing a framework for investigating test-time scaling in RAG domains. Extensive experiments on ViDoSeek validate the effectiveness and generalization of our approach. Notably, ViDoRAG outperforms existing methods by over 10% on the competitive ViDoSeek benchmark. The code will be available",
    "checked": true,
    "id": "afa3685b3a2bd6986346330c9dc990b64407e4ca",
    "semantic_title": "vidorag: visual document retrieval-augmented generation via dynamic iterative reasoning agents",
    "citation_count": 24,
    "authors": [
      "Qiuchen Wang",
      "Ruixue Ding",
      "Zehui Chen",
      "Weiqi Wu",
      "Shihang Wang",
      "Pengjun Xie",
      "Feng Zhao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.465": {
    "title": "IndoSafety: Culturally Grounded Safety for LLMs in Indonesian Languages",
    "volume": "main",
    "abstract": "Although region-specific large language models (LLMs) are increasingly developed, their safety remains underexplored, particularly in culturally diverse settings like Indonesia, where sensitivity to local norms is essential and highly valued by the community. In this work, we present IndoSafety, the first high-quality, human-verified safety evaluation dataset tailored for the Indonesian context, covering five language varieties: formal and colloquial Indonesian, along with three major local languages: Javanese, Sundanese, and Minangkabau. IndoSafety is constructed by extending prior safety frameworks to develop a taxonomy that captures Indonesia's sociocultural context. We find that existing Indonesian-centric LLMs often generate unsafe outputs, particularly in colloquial and local language settings, while fine-tuning on IndoSafety significantly improves safety while preserving task performance. Our work highlights the critical need for culturally grounded safety evaluation and provides a concrete step toward responsible LLM deployment in multilingual settings. Warning: This paper contains example data that may be offensive, harmful, or biased",
    "checked": true,
    "id": "d2828a25d7fa85c880a6dc2ef996ff7912ed64d7",
    "semantic_title": "indosafety: culturally grounded safety for llms in indonesian languages",
    "citation_count": 1,
    "authors": [
      "Muhammad Falensi Azmi",
      "Muhammad Dehan Al Kautsar",
      "Alfan Farizki Wicaksono",
      "Fajri Koto"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.466": {
    "title": "Can LLMs Help You at Work? A Sandbox for Evaluating LLM Agents in Enterprise Environments",
    "volume": "main",
    "abstract": "Enterprise systems are crucial for enhancing productivity and decision-making among employees and customers. Integrating LLM based systems into enterprise systems enables intelligent automation, personalized experiences, and efficient information retrieval, driving operational efficiency and strategic growth. However, developing and evaluating such systems is challenging due to the inherent complexity of enterprise environments, where data is fragmented across multiple sources and governed by sophisticated access controls. We present EnterpriseBench, a comprehensive benchmark that simulates enterprise settings, featuring 500 diverse tasks across software engineering, HR, finance, and administrative domains. Our benchmark uniquely captures key enterprise characteristics including data source fragmentation, access control hierarchies, and cross-functional workflows. Additionally, we provide a novel data generation pipeline that creates internally consistent enterprise tasks from organizational metadata. Experiments with state-of-the-art LLM agents demonstrate that even the most capable models achieve only 41.8% task completion, highlighting significant opportunities for improvement in enterprise-focused AI systems",
    "checked": true,
    "id": "a0e9eda18a7cc95e6ea0f2dbcf1e948b7cb053c5",
    "semantic_title": "can llms help you at work? a sandbox for evaluating llm agents in enterprise environments",
    "citation_count": 0,
    "authors": [
      "Harsh Vishwakarma",
      "Ankush Agarwal",
      "Ojas Patil",
      "Chaitanya Devaguptapu",
      "Mahesh Chandran"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.467": {
    "title": "Steering LLM Reasoning Through Bias-Only Adaptation",
    "volume": "main",
    "abstract": "We show that training a single d-dimensional steering vector per layer with reinforcement learning, while freezing all base weights, matches the accuracy of fully RL-tuned reasoning models on mathematical-reasoning tasks.On an 8 billion-parameter model this adds only ≈ 0.0016% additional parameters and reproduces performance across a range of base models and mathematical-reasoning benchmarks.These results tighten the upper bound on the parameter budget required for high-level chain-of-thought reasoning, indicating that millions of adapter weights are unnecessary.The minimal trainable footprint reduces optimizer memory and inter-GPU communication, lowering the overall cost of fine-tuning.Moreover, a logit-lens analysis shows that the learned vectors amplify coherent token directions, providing clearer insight into the model's internal computations",
    "checked": true,
    "id": "ca3ad95b0dd34095473aa2417232b903fdedfe32",
    "semantic_title": "steering llm reasoning through bias-only adaptation",
    "citation_count": 1,
    "authors": [
      "Viacheslav Sinii",
      "Alexey Gorbatovski",
      "Artem Cherepanov",
      "Boris Shaposhnikov",
      "Nikita Balagansky",
      "Daniil Gavrilov"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.468": {
    "title": "VLASCD: A Visual Language Action Model for Simultaneous Chatting and Decision Making",
    "volume": "main",
    "abstract": "Recent large pretrained models such as LLMs (e.g., GPT series) and VLAs (e.g., OpenVLA) have achieved notable progress on multimodal tasks, yet they are built upon a multi-input single-output (MISO) paradigm. We show that this paradigm fundamentally limits performance in multi-input multi-output (MIMO) scenarios, where parallel task execution is required. In MISO architectures, tasks compete for a shared output channel, creating mutual exclusion effects that cause unbalanced optimization and degraded performance. To address this gap, we introduce MIMO-VLA (VLASCD), a unified training framework that enables concurrent multi-task outputs, exemplified by simultaneous dialogue generation and decision-making. Inspired by human cognition, MIMO-VLA eliminates interference between tasks and supports efficient parallel processing. Experiments on the CARLA autonomous driving platform demonstrate that MIMO-VLA substantially outperforms state-of-the-art MISO-based LLMs, reinforcement learning models, and VLAs in MIMO settings, establishing a new direction for multimodal and multitask learning",
    "checked": true,
    "id": "fb4a7c641b18ae6191719a9dd9b0bd5ff157b3d8",
    "semantic_title": "vlascd: a visual language action model for simultaneous chatting and decision making",
    "citation_count": 3,
    "authors": [
      "Zuojin Tang",
      "Bin Hu",
      "Chenyang Zhao",
      "De Ma",
      "Gang Pan",
      "Bin Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.469": {
    "title": "M-LongDoc: A Benchmark For Multimodal Super-Long Document Understanding And A Retrieval-Aware Tuning Framework",
    "volume": "main",
    "abstract": "The ability to understand and answer questions over documents can be useful in many business and practical applications. However, documents often contain lengthy and diverse multimodal contents such as texts, figures, and tables, which are very time-consuming for humans to read thoroughly. Hence, there is an urgent need to develop effective and automated methods to aid humans in this task. In this work, we introduce M-LongDoc, a benchmark of 851 samples, and an automated framework to evaluate the performance of large multimodal models. We further propose a retrieval-aware tuning approach for efficient and effective multimodal document reading. Compared to existing works, our benchmark consists of more recent and lengthy documents with hundreds of pages, while also requiring open-ended explanations and not just extractive answers. To our knowledge, our training framework is the first to directly address the retrieval setting for multimodal long documents. To enhance open models, we construct a training corpus in a fully automatic manner. Experiments show that our tuning approach significantly improves the correctness of model responses by 4.6%",
    "checked": true,
    "id": "92e23242aaa59fa4455f990eeeccef8ad080c18b",
    "semantic_title": "m-longdoc: a benchmark for multimodal super-long document understanding and a retrieval-aware tuning framework",
    "citation_count": 0,
    "authors": [
      "Yew Ken Chia",
      "Liying Cheng",
      "Hou Pong Chan",
      "Maojia Song",
      "Chaoqun Liu",
      "Mahani Aljunied",
      "Soujanya Poria",
      "Lidong Bing"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.470": {
    "title": "Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language Models",
    "volume": "main",
    "abstract": "Recent advances in text-only \"slow-thinking\" reasoning have prompted efforts to transfer this capability to vision-language models (VLMs), for training visual reasoning models (VRMs). However, such transfer faces critical challenges: Effective \"slow thinking\" in VRMs requires visual reflection, the ability to check the reasoning process based on visual information. Through quantitative analysis, we observe that current VRMs exhibit limited visual reflection, as their attention to visual information diminishes rapidly with longer generated responses. To address this challenge, we propose a new VRM Reflection-V, which enhances visual reflection based on reasoning data construction for cold-start and reward design for reinforcement learning (RL). Firstly, we construct vision-centered reasoning data by leveraging an agent that interacts between VLMs and reasoning LLMs, enabling cold-start learning of visual reflection patterns. Secondly, a visual attention based reward model is employed during RL to encourage reasoning based on visual information. Therefore, Reflection-V demonstrates significant improvements across multiple visual reasoning benchmarks. Furthermore, Reflection-V maintains a stronger and more consistent reliance on visual information during visual reasoning, indicating effective enhancement in visual reflection capabilities",
    "checked": true,
    "id": "e99667033b8ac8e31520fbe4fba4323ef21f5cb7",
    "semantic_title": "look again, think slowly: enhancing visual reflection in vision-language models",
    "citation_count": 4,
    "authors": [
      "Pu Jian",
      "Junhong Wu",
      "Wei Sun",
      "Chen Wang",
      "Shuo Ren",
      "Jiajun Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.471": {
    "title": "FB-Bench: A Fine-Grained Multi-Task Benchmark for Evaluating LLMs' Responsiveness to Human Feedback",
    "volume": "main",
    "abstract": "Human feedback is crucial in the interactions between humans and Large Language Models (LLMs). However, existing research primarily focuses on benchmarking LLMs in single-turn dialogues. Even in benchmarks designed for multi-turn dialogues, the user utterances are often independent, neglecting the nuanced and complex nature of human feedback within real-world usage scenarios. To fill this research gap, we introduce FB-Bench, a fine-grained, multi-task benchmark designed to evaluate LLMs' responsiveness to human feedback under real-world usage scenarios in Chinese. Drawing from the two main interaction scenarios, FB-Bench comprises 591 meticulously curated samples, encompassing eight task types, five deficiency types of response, and nine feedback types. We extensively evaluate a broad array of popular LLMs, revealing significant variations in their performance across different interaction scenarios. Further analysis indicates that task, human feedback, and deficiencies of previous responses can also significantly impact LLMs' responsiveness. Our findings underscore both the strengths and limitations of current models, providing valuable insights and directions for future research",
    "checked": true,
    "id": "f9c1a7de0e21a2c650b8d42a32d003047b4561c4",
    "semantic_title": "fb-bench: a fine-grained multi-task benchmark for evaluating llms' responsiveness to human feedback",
    "citation_count": 10,
    "authors": [
      "Youquan Li",
      "Miao Zheng",
      "Fan Yang",
      "Guosheng Dong",
      "Bin Cui",
      "Weipeng Chen",
      "Zenan Zhou",
      "Wentao Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.472": {
    "title": "HYDRA: A Multi-Head Encoder-only Architecture for Hierarchical Text Classification",
    "volume": "main",
    "abstract": "We introduce HYDRA, a simple yet effective multi-head encoder-only architecture for hierarchical text classification that treats each level in the hierarchy as a separate classification task with its own label space. State-of-the-art approaches rely on complex components like graph encoders, label semantics, and autoregressive decoders. We demonstrate that such complexity is often unnecessary. Through parameter sharing and level-specific parameterization, HYDRA enables flat models to incorporate hierarchical awareness without architectural complexity. Experiments on four benchmarks (NYT, RCV1-V2, BGC, and WOS) demonstrate that HYDRA always increases the performance over flat models and matches or exceeds the performance of complex state-of-the-art methods",
    "checked": true,
    "id": "e4520eb66586329e197f13484acba5d1cb866e2f",
    "semantic_title": "hydra: a multi-head encoder-only architecture for hierarchical text classification",
    "citation_count": 0,
    "authors": [
      "Fabian Karl",
      "Ansgar Scherp"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.473": {
    "title": "CARD: Cross-modal Agent Framework for Generative and Editable Residential Design",
    "volume": "main",
    "abstract": "In recent years, architectural design automation has made significant progress, but the complexity of open-world environments continues to make residential design a challenging task, often requiring experienced architects to perform multiple iterations and human-computer interactions. Therefore, assisting ordinary users in navigating these complex environments to generate and edit residential design is crucial. In this paper, we present the CARD framework, which leverages a system of specialized cross-modal agents to adapt to complex open-world environments. The framework includes a point-based cross-modal information representation (CMI-P) that encodes the geometry and spatial relationships of residential rooms, a cross-modal residential generation model, supported by our customized Text2FloorEdit model, that acts as the lead designer to create standardized floor plans, and an embedded expert knowledge base for evaluating whether the designs meet user requirements and residential codes, providing feedback accordingly. Finally, a 3D rendering module assists users in visualizing and understanding the layout. CARD enables cross-modal residential generation from free-text input, empowering users to adapt to complex environments without requiring specialized expertise",
    "checked": true,
    "id": "8cb1e3ab8da0699345be54882c92bd417cd68177",
    "semantic_title": "card: cross-modal agent framework for generative and editable residential design",
    "citation_count": 12,
    "authors": [
      "Pengyu Zeng",
      "Jun Yin",
      "Miao Zhang",
      "Yuqin Dai",
      "Jizhizi Li",
      "ZhanXiang Jin",
      "Shuai Lu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.474": {
    "title": "DrDiff: Dynamic Routing Diffusion with Hierarchical Attention for Breaking the Efficiency-Quality Trade-off",
    "volume": "main",
    "abstract": "This paper introduces DrDiff, a novel framework for long-text generation that overcomes the efficiency-quality trade-off through three core technologies. First, we design a dynamic expert scheduling mechanism that intelligently allocates computational resources during the diffusion process based on text complexity, enabling more efficient handling of text generation tasks of varying difficulty. Second, we introduce a Hierarchical Sparse Attention (HSA) mechanism that adaptively adjusts attention patterns according to a variety of input lengths, reducing computational complexity from O(n2) to O(n) while maintaining model performance. Finally, we propose a Semantic Anchor States (SAS) module that combines with DPM-solver++ to reduce diffusion steps, significantly improving generation speed. Comprehensive experiments on various long-text generation benchmarks demonstrate the superiority of our DrDiff over the existing SOTA methods",
    "checked": true,
    "id": "0135919988d65d434557ec60ca9f6d5bb6a1c325",
    "semantic_title": "drdiff: dynamic routing diffusion with hierarchical attention for breaking the efficiency-quality trade-off",
    "citation_count": 6,
    "authors": [
      "Jusheng Zhang",
      "Yijia Fan",
      "Kaitong Cai",
      "Zimeng Huang",
      "Xiaofei Sun",
      "Jian Wang",
      "Chengpei Tang",
      "Keze Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.475": {
    "title": "FaST: Feature-aware Sampling and Tuning for Personalized Preference Alignment with Limited Data",
    "volume": "main",
    "abstract": "LLM-powered conversational assistants are often deployed in a one-size-fits-all manner, which fails to accommodate individual user preferences. Recently, LLM personalization – tailoring models to align with specific user preferences – has gained increasing attention as a way to bridge this gap. In this work, we specifically focus on a practical yet challenging setting where only a small set of preference annotations can be collected per user – a problem we define as Personalized Preference Alignment with Limited Data (PPALLI). To support research in this area, we introduce two datasets – DnD and ELIP – and benchmark a variety of alignment techniques on them. We further propose FaST, a highly parameter-efficient approach that leverages high-level features automatically discovered from the data, achieving the best overall performance",
    "checked": true,
    "id": "0006ef1aaa7261189e6acbac85c031985347db75",
    "semantic_title": "fast: feature-aware sampling and tuning for personalized preference alignment with limited data",
    "citation_count": 1,
    "authors": [
      "Thibaut Thonet",
      "Germán Kruszewski",
      "Jos Rozen",
      "Pierre Erbacher",
      "Marc Dymetman"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.476": {
    "title": "On LLM-Based Scientific Inductive Reasoning Beyond Equations",
    "volume": "main",
    "abstract": "As large language models (LLMs) increasingly exhibit human-like capabilities, a fundamental question emerges: How can we enable LLMs to learn the underlying patterns from limited examples in entirely novel environments and apply them effectively? This question is central to the ability of LLMs in inductive reasoning. Existing research on LLM-based inductive reasoning can be broadly categorized based on whether the underlying rules are expressible via explicit mathematical equations. However, many recent studies in the beyond-equations category have emphasized rule design without grounding them in specific scenarios. Inspired by the parallels between inductive reasoning and human scientific discovery, we propose the task of LLM-Based Scientific Inductive Reasoning Beyond Equations and introduce a new benchmark, SIRBench-V1, to evaluate the inductive reasoning abilities of LLMs in scientific settings. Our experimental results show that current LLMs still struggle with this task, underscoring its difficulty and the need for further advancement in this area",
    "checked": true,
    "id": "762448b51173b3e4082408cde863d16baed51ee7",
    "semantic_title": "on llm-based scientific inductive reasoning beyond equations",
    "citation_count": 0,
    "authors": [
      "Brian S. Lin",
      "Jiaxin Yuan",
      "Zihan Zhou",
      "Shouli Wang",
      "Shuo Wang",
      "Cunliang Kong",
      "Qi Shi",
      "Yuxuan Li",
      "Liner Yang",
      "Zhiyuan Liu",
      "Maosong Sun"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.477": {
    "title": "SPECS: Specificity-Enhanced CLIP-Score for Long Image Caption Evaluation",
    "volume": "main",
    "abstract": "As interest grows in generating long, detailed image captions, standard evaluation metrics become increasingly unreliable. N-gram-based metrics though efficient, fail to capture semantic correctness. Representational Similarity (RS) metrics, designed to address this, initially saw limited use due to high computational costs, while today, despite advances in hardware, they remain unpopular due to low correlation to human judgments. Meanwhile, metrics based on large language models (LLMs) show strong correlation with human judgments, but remain too expensive for iterative use during model development.We introduce SPECS (Specificity-Enhanced CLIPScore), a reference-free RS metric tailored to long image captioning. SPECS modifies CLIP with a new objective that emphasizes specificity: rewarding correct details and penalizing incorrect ones. We show that SPECS matches the performance of open-source LLM-based metrics in correlation to human judgments, while being far more efficient. This makes it a practical alternative for iterative checkpoint evaluation during image captioning model development.Our code can be found at https://github.com/mbzuai-nlp/SPECS",
    "checked": true,
    "id": "78d89d358b42d14284d0635069da63dd4897b87e",
    "semantic_title": "specs: specificity-enhanced clip-score for long image caption evaluation",
    "citation_count": 1,
    "authors": [
      "Xiaofu Chen",
      "Israfel Salazar",
      "Yova Kementchedjhieva"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.478": {
    "title": "LM-Searcher: Cross-domain Neural Architecture Search with LLMs via Unified Numerical Encoding",
    "volume": "main",
    "abstract": "Recent progress in Large Language Models (LLMs) has opened new avenues for solving complex optimization problems, including Neural Architecture Search (NAS). However, existing LLM-driven NAS approaches rely heavily on prompt engineering and domain-specific tuning, limiting their practicality and scalability across diverse tasks. In this work, we propose LM-Searcher, a novel framework that leverages LLMs for cross-domain neural architecture optimization without the need for extensive domain-specific adaptation. Central to our approach is NCode, a universal numerical string representation for neural architectures, which enables cross-domain architecture encoding and search. We also reformulate the NAS problem as a ranking task, training LLMs to select high-performing architectures from candidate pools using instruction-tuning samples derived from a novel pruning-based subspace sampling strategy. Our curated dataset, encompassing a wide range of architecture-performance pairs, encourages robust and transferable learning. Comprehensive experiments demonstrate that LM-Searcher achieves competitive performance in both in-domain (e.g., CNNs for image classification) and out-of-domain (e.g., LoRA configurations for segmentation and generation) tasks, establishing a new paradigm for flexible and generalizable LLM-based architecture search",
    "checked": true,
    "id": "187dcae9d09a3d63fc88963478f6c27568316de9",
    "semantic_title": "lm-searcher: cross-domain neural architecture search with llms via unified numerical encoding",
    "citation_count": 1,
    "authors": [
      "Yuxuan Hu",
      "Jihao Liu",
      "Ke Wang",
      "Jinliang Zheng",
      "Weikang Shi",
      "Manyuan Zhang",
      "Qi Dou",
      "Rui Liu",
      "Aojun Zhou",
      "Hongsheng Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.479": {
    "title": "Does quantization affect models' performance on long-context tasks?",
    "volume": "main",
    "abstract": "Large language models (LLMs) now support context windows exceeding 128K tokens, but this comes with significant memory requirements and high inference latency. Quantization can mitigate these costs, but may degrade performance. In this work, we present the first systematic evaluation of quantized LLMs on tasks with long inputs (≥64K tokens) and long-form outputs. Our evaluation spans 9.7K test examples, five quantization methods (FP8, GPTQ-int8, AWQ-int4, GPTQ-int4, BNB-nf4), and five models (Llama-3.1 8B and 70B; Qwen-2.5 7B, 32B, and 72B). We find that, on average, 8-bit quantization preserves accuracy (~0.8% drop), whereas 4-bit methods lead to substantial losses, especially for tasks involving long-context inputs (drops of up to 59%). This degradation tends to worsen when the input is in a language other than English. Crucially, the effects of quantization depend heavily on the quantization method, model, and task. For instance, while Qwen-2.5 72B remains robust under BNB-nf4, Llama-3.1 70B experiences a 32% performance drop on the same task. These findings highlight the importance of a careful, task-specific evaluation before deploying quantized LLMs, particularly in long-context scenarios and for languages other than English",
    "checked": true,
    "id": "123ca6b3e6ae6735db4839eb6d8748c798deab63",
    "semantic_title": "does quantization affect models' performance on long-context tasks?",
    "citation_count": 1,
    "authors": [
      "Anmol Mekala",
      "Anirudh Atmakuru",
      "Yixiao Song",
      "Marzena Karpinska",
      "Mohit Iyyer"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.480": {
    "title": "Token-Aware Editing of Internal Activations for Large Language Model Alignment",
    "volume": "main",
    "abstract": "Intervening the internal activations of large language models (LLMs) provides an effective inference-time alignment approach to mitigate undesirable behaviors, such as generating erroneous or harmful content, thereby ensuring safe and reliable applications of LLMs. However, previous methods neglect the misalignment discrepancy among varied tokens, resulting in deviant alignment direction and inflexible editing strength. To address these issues, we propose a token-aware editing (TAE) approach to fully utilize token-level alignment information in the activation space, therefore realizing superior post-intervention performance. Specifically, a Mutual Information-guided Graph Aggregation (MIG) module first develops an MI-guided graph to exploit the tokens' informative interaction for activation enrichment, thus improving alignment probing and facilitating intervention. Subsequently, Misalignment-aware Adaptive Intervention (MAI) comprehensively perceives the token-level misalignment degree from token representation and prediction to guide the adaptive adjustment of editing strength, thereby enhancing final alignment performance. Extensive experiments on three alignment capabilities demonstrate the efficacy of TAE, notably surpassing baseline by 25.8% on the primary metric of truthfulness with minimal cost",
    "checked": true,
    "id": "c08561da2884f3415e375c7da0b776b21cdcd042",
    "semantic_title": "token-aware editing of internal activations for large language model alignment",
    "citation_count": 0,
    "authors": [
      "Tianbo Wang",
      "Yuqing Ma",
      "Kewei Liao",
      "Chengzhao Yang",
      "Zhange Zhang",
      "Jiakai Wang",
      "Xianglong Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.481": {
    "title": "Bitune: Leveraging Bidirectional Attention to Improve Decoder-Only LLMs",
    "volume": "main",
    "abstract": "Decoder-only large language models typically rely solely on masked causal attention, which limits their expressiveness by restricting information flow to one direction. We propose Bitune, a method that enhances pretrained decoder-only LLMs by incorporating bidirectional attention into prompt processing. We evaluate Bitune in instruction-tuning and question-answering settings, showing significant improvements in performance on commonsense reasoning, arithmetic, and language understanding tasks. Furthermore, extensive ablation studies validate the role of each component of the method, and demonstrate that Bitune is compatible with various parameter-efficient finetuning techniques and full model finetuning",
    "checked": true,
    "id": "cf1b2dd5e36f34f38249f8a2011fab038d4d59f3",
    "semantic_title": "bitune: leveraging bidirectional attention to improve decoder-only llms",
    "citation_count": 4,
    "authors": [
      "Dawid Jan Kopiczko",
      "Tijmen Blankevoort",
      "Yuki M Asano"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.482": {
    "title": "Disambiguation in Conversational Question Answering in the Era of LLMs and Agents: A Survey",
    "volume": "main",
    "abstract": "Ambiguity remains a fundamental challenge in Natural Language Processing (NLP) due to the inherent complexity and flexibility of human language. With the advent of Large Language Models (LLMs), addressing ambiguity has become even more critical due to their expanded capabilities and applications. In the context of Conversational Question Answering (CQA), this paper explores the definition, forms, and implications of ambiguity for language driven systems, particularly in the context of LLMs. We define key terms and concepts, categorize various disambiguation approaches enabled by LLMs, and provide a comparative analysis of their advantages and disadvantages. We also explore publicly available datasets for benchmarking ambiguity detection and resolution techniques and highlight their relevance for ongoing research. Finally, we identify open problems and future research directions, especially in agentic settings, proposing areas for further investigation. By offering a comprehensive review of current research on ambiguities and disambiguation with LLMs, we aim to contribute to the development of more robust and reliable LLM-based systems",
    "checked": true,
    "id": "5217d9a8dd96be0c93cc2add7ff961453594ba30",
    "semantic_title": "disambiguation in conversational question answering in the era of llms and agents: a survey",
    "citation_count": 0,
    "authors": [
      "Mehrab Tanjim",
      "Yeonjun In",
      "Xiang Chen",
      "Victor Bursztyn",
      "Ryan A. Rossi",
      "Sungchul Kim",
      "Guang-Jie Ren",
      "Vaishnavi Muppala",
      "Shun Jiang",
      "Yongsung Kim",
      "Chanyoung Park"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.483": {
    "title": "Plan Dynamically, Express Rhetorically: A Debate-Driven Rhetorical Framework for Argumentative Writing",
    "volume": "main",
    "abstract": "Argumentative essay generation (AEG) is a complex task that requires advanced semantic understanding, logical reasoning, and organized integration of perspectives. Despite showing a promising performance, current efforts often overlook the dynamical and hierarchical nature of structural argumentative planning, and struggle with flexible rhetorical expression, leading to limited argument divergence and rhetorical optimization. Inspired by human debate behavior and Bitzer's rhetorical situation theory, we propose a debate-driven rhetorical framework for argumentative writing. The uniqueness lies in three aspects: (1) dynamic assesses the divergence of viewpoints and progressively reveals the hierarchical outline of arguments based on a depth-then-breadth paradigm, improving the perspective divergence within argumentation; (2) simulates human debate through iterative defender-attacker interactions, improving the logical coherence of arguments; (3) incorporates Bitzer's rhetorical situation theory to flexibly select appropriate rhetorical techniques, enabling the rhetorical expression. Experiments on four benchmarks validate that our approach significantly improves logical depth, argumentative diversity, and rhetorical persuasiveness over existing state-of-the-art models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xueguan Zhao",
      "Wenpeng Lu",
      "Chaoqun Zheng",
      "Weiyu Zhang",
      "Jiasheng Si",
      "Deyu Zhou"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.484": {
    "title": "TCPO: Thought-Centric Preference Optimization for Effective Embodied Decision-making",
    "volume": "main",
    "abstract": "Using effective generalization capabilities of vision language models (VLMs) in context-specific dynamic tasks for embodied artificial intelligence remains a significant challenge. Although supervised fine-tuned models can better align with the real physical world, they still exhibit sluggish responses and hallucination issues in dynamically changing environments, necessitating further alignment. Existing post-SFT methods, reliant on reinforcement learning and chain-of-thought (CoT) approaches, are constrained by sparse rewards and action-only optimization, resulting in low sample efficiency, poor consistency, and model degradation. To address these issues, this paper proposes Thought-Centric Preference Optimization (TCPO) for effective embodied decision-making. Specifically, TCPO introduces a stepwise preference-based optimization approach, transforming sparse reward signals into richer step sample pairs. It emphasizes the alignment of the model's intermediate reasoning process, mitigating the problem of model degradation. Moreover, by incorporating Action Policy Consistency Constraint (APC), it further imposes consistency constraints on the model output. Experiments in the ALFWorld environment demonstrate an average success rate of **26.67%**, achieving a **6%** improvement over RL4VLM and validating the effectiveness of our approach in mitigating model degradation after fine-tuning. These results highlight the potential of integrating preference-based learning techniques with CoT processes to enhance the decision-making capabilities of vision-language models in embodied agents",
    "checked": true,
    "id": "abf03d59a76ce15934c0beea918af4007efc5de0",
    "semantic_title": "tcpo: thought-centric preference optimization for effective embodied decision-making",
    "citation_count": 0,
    "authors": [
      "Kechen Jiao",
      "Zhirui Fang",
      "Jiahao Liu",
      "Bei Li",
      "Qifan Wang",
      "Xinyu Liu",
      "Junhao Ruan",
      "Zhongjian Qiao",
      "Yifan Zhu",
      "Yaxin Xu",
      "Jingang Wang",
      "Xiu Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.485": {
    "title": "Reimagining Safety Alignment with An Image",
    "volume": "main",
    "abstract": "Large language models (LLMs) excel in diverse applications but face dual challenges: generating harmful content under jailbreak attacks and over-refusing benign queries due to rigid safety mechanisms. These issues severely affect the application of LLMs, especially in the medical and education fields. Existing approaches can be divided into three types: contrastive decoding, activation manipulation, and prompting strategies. However, all these approaches face challenges like inefficiency, fragility, or architectural constraints,ultimately failing to strike a balance between safety and usability. These problems are more obvious in multimodal large language models (MLLMs), especially in terms of heightened over-refusal in cross-modal tasks and new security risks arising from expanded attack surfaces. We propose Magic Image, an optimization-driven visual prompt framework that enhances security and reduces over-refusal at the same time. The Magic Image is optimized using gradients derived from harmful/benign training samples. Using the magic image can modify the model's original safety alignment, maintaining robust safety while reducing unnecessary denials. Experiments demonstrate its effectiveness in preserving model performance and improving safety-responsiveness balance across datasets, including unseen data, offering a practical solution for reliable MLLM deployment",
    "checked": true,
    "id": "9c4029dd2909bcc7bfdad459e6e599447f3fa6c3",
    "semantic_title": "reimagining safety alignment with an image",
    "citation_count": 0,
    "authors": [
      "Yifan Xia",
      "Guorui Chen",
      "Wenqian Yu",
      "Zhijiang Li",
      "Philip Torr",
      "Jindong Gu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.486": {
    "title": "Generative or Discriminative? Revisiting Text Classification in the Era of Transformers",
    "volume": "main",
    "abstract": "*The comparison between discriminative and generative classifiers has intrigued researchers since [Efron (1975)'s](https://www.jstor.org/stable/2285453) seminal analysis of logistic regression versus discriminant analysis. While early theoretical work established that generative classifiers exhibit lower sample complexity but higher asymptotic error in simple linear settings, these trade-offs remain unexplored in the transformer era. We present the first comprehensive evaluation of modern generative and discriminative architectures—Auto-regressive, Masked Language Modeling, Discrete Diffusion, and Encoders for text classification. Our study reveals that the classical \"two regimes\" phenomenon manifests distinctly across different architectures and training paradigms. Beyond accuracy, we analyze sample efficiency, calibration, noise robustness, and ordinality across diverse scenarios. Our findings offer practical guidance for selecting the most suitable modeling approach based on real-world constraints such as latency and data limitations.*",
    "checked": true,
    "id": "722b64349df6f0b0fedf1a480567c67a46e5a48e",
    "semantic_title": "generative or discriminative? revisiting text classification in the era of transformers",
    "citation_count": 2,
    "authors": [
      "Siva Rajesh Kasa",
      "Karan Gupta",
      "Sumegh Roychowdhury",
      "Ashutosh Kumar",
      "Yaswanth Biruduraju",
      "Santhosh Kumar Kasa",
      "Pattisapu Nikhil Priyatam",
      "Arindam Bhattacharya",
      "Shailendra Agarwal",
      "Vijay Huddar"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.487": {
    "title": "Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection",
    "volume": "main",
    "abstract": "With the emergence of strong vision language capabilities, multimodal large language models (MLLMs) have demonstrated tremendous potential for real-world applications. However, the security vulnerabilities exhibited by the visual modality pose significant challenges to deploying such models in open-world environments.Recent studies have successfully induced harmful responses from target MLLMs by encoding harmful textual semantics directly into visual inputs. However, in these approaches, the visual modality primarily serves as a trigger for unsafe behavior, often exhibiting semantic ambiguity and lacking grounding in realistic scenarios. In this work, we define a novel setting: vision-centric jailbreak, where visual information serves as a necessary component in constructing a complete and realistic jailbreak context. Building on this setting, we propose the VisCo (Visual Contextual) Attack.VisCo fabricates contextual dialogue using four distinct vision-focused strategies, dynamically generating auxiliary images when necessary to construct a vision-centric jailbreak scenario.To maximize attack effectiveness, it incorporates automatic toxicity obfuscation and semantic refinement to produce a final attack prompt that reliably triggers harmful responses from the target black-box MLLMs. Specifically, VisCo achieves a toxicity score of 4.78 and an Attack Success Rate (ASR) of 85% on MM-SafetyBench against GPT-4o, significantly outperforming the baseline, which achieves a toxicity score of 2.48 and an ASR of 22.2%. Code: https://github.com/Dtc7w3PQ/Visco-Attack",
    "checked": true,
    "id": "c97f5fa7bf306e74e1f1254dea99dd6e39fb602d",
    "semantic_title": "visual contextual attack: jailbreaking mllms with image-driven context injection",
    "citation_count": 5,
    "authors": [
      "Miao Ziqi",
      "Yi Ding",
      "Lijun Li",
      "Jing Shao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.488": {
    "title": "Can Large Language Models Win the International Mathematical Games?",
    "volume": "main",
    "abstract": "Recent advances in large language models (LLMs) have demonstrated strong mathematical reasoning abilities, even in visual contexts, with some models surpassing human performance on existing benchmarks. However, these benchmarks lack structured age categorization, clearly defined skill requirements, and—crucially—were not designed to assess human performance in international competitions. To address these limitations, we introduce MathGames, a new benchmark of 2,183 high-quality mathematical problems (both text-only and multimodal) in an open-ended format, sourced from an international mathematical games championships. Spanning seven age groups and a skill-based taxonomy, MathGames enables a structured evaluation of LLMs' mathematical and logical reasoning abilities. Our experiments reveal a substantial gap between state-of-the-art LLMs and human participants—even 11-year-olds consistently outperform some of the strongest models—highlighting the need for advancements. Further, our detailed error analysis offers valuable insights to guide future research. The data is publicly available at https://disi-unibo-nlp.github.io/math-games",
    "checked": true,
    "id": "bb46d3c1bc3fed042a869820fcf9bb88bac31e7c",
    "semantic_title": "can large language models win the international mathematical games?",
    "citation_count": 0,
    "authors": [
      "Alessio Cocchieri",
      "Luca Ragazzi",
      "Giuseppe Tagliavini",
      "Lorenzo Tordi",
      "Antonella Carbonaro",
      "Gianluca Moro"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.489": {
    "title": "CodeArena: Evaluating and Aligning CodeLLMs on Human Preference",
    "volume": "main",
    "abstract": "We present CodeArena to emulate the complexity/diversity of real-world coding tasks, spanning 40 categories and 44 PLs. A 20B diverse synthetic instruction corpus is created by scaling instructions to help Qwen2.5-SynCoder achieve SOTA performance. Abstract: Code large language models (codeLLMs) have made significant strides in code generation. Most previous code-related benchmarks, which consist of various programming exercises along with the corresponding test cases, are used as a common measure to evaluate the performance and capabilities of code LLMs. However, the current code LLMs focus on synthesizing the correct code snippet, ignoring the alignment with human preferences, where the query should be sampled from the practical application scenarios and the model-generated responses should satisfy the human preference. To bridge the gap between the model-generated response and human preference, we present a rigorous human-curated benchmark CodeArena to emulate the complexity and diversity of real-world coding tasks, where 397 high-quality samples spanning 40 categories and 44 programming languages, carefully curated from user queries. Further, we propose a diverse synthetic instruction corpus SynCode-Instruct (nearly 20B tokens) by scaling instructions from the website to verify the effectiveness of the large-scale synthetic instruction fine-tuning, where Qwen2.5-SynCoder totally trained on synthetic instruction data can achieve top-tier performance of open-source code LLMs. The results find performance differences between execution-based benchmarks and CodeArena. Our systematic experiments of CodeArena on 40+ LLMs reveal a notable performance gap between open SOTA code LLMs (e.g. Qwen2.5-Coder) and proprietary LLMs (e.g., OpenAI o1), underscoring the importance of the human preference alignment",
    "checked": true,
    "id": "1608b75ecb0f47d13574f903cffde3bda79daefb",
    "semantic_title": "codearena: evaluating and aligning codellms on human preference",
    "citation_count": 0,
    "authors": [
      "Jian Yang",
      "Jiaxi Yang",
      "Wei Zhang",
      "Jin Ke",
      "Yibo Miao",
      "Lei Zhang",
      "Liqun Yang",
      "Zeyu Cui",
      "Yichang Zhang",
      "Zhoujun Li",
      "Binyuan Hui",
      "Junyang Lin"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.490": {
    "title": "Language models can learn implicit multi-hop reasoning, but only if they have lots of training data",
    "volume": "main",
    "abstract": "Implicit reasoning is the ability of a language model to solve multi-hop reasoning tasks in a single forward pass, without chain of thought.We investigate this capability using GPT2-style language models trained from scratch on controlled k-hop reasoning datasets (k = 2, 3, 4). We show that while such models can indeed learn implicit k-hop reasoning,the required training data grows exponentially in k, and the requirednumber of transformer layers grows linearly in k.We offer a theoretical explanation for why this depth growth is necessary.We further find that the data requirement can be mitigated, but not eliminated,through curriculum learning",
    "checked": true,
    "id": "35e9c8a22e836e019ed023d07b714d7379acb9b3",
    "semantic_title": "language models can learn implicit multi-hop reasoning, but only if they have lots of training data",
    "citation_count": 1,
    "authors": [
      "Yuekun Yao",
      "Yupei Du",
      "Dawei Zhu",
      "Michael Hahn",
      "Alexander Koller"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.491": {
    "title": "UniversalCEFR: Enabling Open Multilingual Research on Language Proficiency Assessment",
    "volume": "main",
    "abstract": "We introduce UniversalCEFR, a large-scale multilingual multidimensional dataset of texts annotated according to the CEFR (Common European Framework of Reference) scale in 13 languages. To enable open research in both automated readability and language proficiency assessment, UniversalCEFR comprises 505,807 CEFR-labeled texts curated from educational and learner-oriented resources, standardized into a unified data format to support consistent processing, analysis, and modeling across tasks and languages. To demonstrate its utility, we conduct benchmark experiments using three modelling paradigms: a) linguistic feature-based classification, b) fine-tuning pre-trained LLMs, and c) descriptor-based prompting of instruction-tuned LLMs. Our results further support using linguistic features and fine-tuning pretrained models in multilingual CEFR level assessment. Overall, UniversalCEFR aims to establish best practices in data distribution in language proficiency research by standardising dataset formats and promoting their accessibility to the global research community",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joseph Marvin Imperial",
      "Abdullah Barayan",
      "Regina Stodden",
      "Rodrigo Wilkens",
      "Ricardo Muñoz Sánchez",
      "Lingyun Gao",
      "Melissa Torgbi",
      "Dawn Knight",
      "Gail Forey",
      "Reka R. Jablonkai",
      "Ekaterina Kochmar",
      "Robert Joshua Reynolds",
      "Eugénio Ribeiro",
      "Horacio Saggion",
      "Elena Volodina",
      "Sowmya Vajjala",
      "Thomas François",
      "Fernando Alva-Manchego",
      "Harish Tayyar Madabushi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.492": {
    "title": "CROP: Contextual Region-Oriented Visual Token Pruning",
    "volume": "main",
    "abstract": "Current VLM-based VQA methods often process entire images, leading to excessive visual tokens that include redundant information irrelevant to the posed question. This abundance of unnecessary image details creates numerous visual tokens, drastically increasing memory and computational requirements in VLMs. To address this, we propose Contextual Region-Oriented Visual Token Pruning (CROP), a novel framework to compress visual tokens through a two-step process: Localization and Pruning. Specifically, CROP first employs an efficient model to identify the contextual region relevant to the input query. Subsequently, two distinct strategies are introduced for pruning: (1) Pre-LLM Compression (PLC), which adaptively compresses different image regions with varying ratios, and (2) Inner-LLM Pruning (ILP), a training-free method that prunes tokens within early LLM layers guided by the identified contextual region. Extensive experiments on a wide range of VQA tasks demonstrate that CROP significantly outperforms existing visual token pruning methods and achieves state-of-the-art performance",
    "checked": true,
    "id": "94f049bde5239fc281374fcc964ff4a62f170e9b",
    "semantic_title": "crop: contextual region-oriented visual token pruning",
    "citation_count": 6,
    "authors": [
      "Jiawei Guo",
      "Feifei Zhai",
      "Pu Jian",
      "Qianrun Wei",
      "Yu Zhou"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.493": {
    "title": "CR4-NarrEmote: An Open Vocabulary Dataset of Narrative Emotions Derived Using Citizen Science",
    "volume": "main",
    "abstract": "We introduce \"Citizen Readers for Narrative Emotions\" (CR4-NarrEmote), a large-scale, open-vocabulary dataset of narrative emotions derived through a citizen science initiative. Over a four-month period, 3,738 volunteers contributed more than 200,000 emotion annotations across 43,000 passages from long-form fiction and non-fiction, spanning 150 years, twelve genres, and multiple Anglophone cultural contexts. To facilitate model training and comparability, we provide mappings to both dimensional (Valence-Arousal-Dominance) and categorical (NRC Emotion) frameworks. We evaluate annotation reliability using lexical, categorical, and semantic agreement measures, and find substantial alignment between citizen science annotations and expert-generated labels. As the first open-vocabulary resource focused on narrative emotions at scale, CR4-NarrEmote provides an important foundation for affective computing and narrative understanding",
    "checked": true,
    "id": "189dd050e256b042bd13d74ceb5f964227cf8574",
    "semantic_title": "cr4-narremote: an open vocabulary dataset of narrative emotions derived using citizen science",
    "citation_count": 0,
    "authors": [
      "Andrew Piper",
      "Robert Budac"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.494": {
    "title": "XQuant: Achieving Ultra-Low Bit KV Cache Quantization with Cross-Layer Compression",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse natural language processing tasks. However, their extensive memory requirements, particularly due to KV cache growth during long-text understanding and generation, present significant challenges for deployment in resource-constrained environments. Quantization has emerged as a promising solution to reduce memory consumption while preserving historical information. We propose XQuant, a training-free and plug-and-play framework that achieves ultra-low equivalent bit-width KV cache quantization. XQuant introduces two key innovations: a computationally negligible data-free calibration method and cross-layer KV cache compression, enabling quantization to sub-1.4 bits. Extensive experiments on TruthfulQA and LongBench demonstrate that XQuant outperforms state-of-the-art methods (e.g., KIVI-2bit and AsymKV-1.5bit) by achieving lower bit-width while maintaining superior performance, establishing a better trade-off between memory efficiency and model accuracy. The source code is available at https://github.com/brinenick511/XQuant",
    "checked": true,
    "id": "266e2be613997ae244a53669df9665730072bee5",
    "semantic_title": "xquant: achieving ultra-low bit kv cache quantization with cross-layer compression",
    "citation_count": 0,
    "authors": [
      "Haoqi Yang",
      "Yao Yao",
      "Zuchao Li",
      "Baoyuan Qi",
      "Liu Guoming",
      "Hai Zhao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.495": {
    "title": "DINT Transformer",
    "volume": "main",
    "abstract": "The DIFF Transformer mitigates interference from irrelevant contexts by introducing a differential attention mechanism, thereby enhancing focus on critical tokens. However, this architecture suffers from two major limitations: first, its use of two independent attention matrices leads to numerical instability, and second, it lacks global context modeling, which is essential for identifying globally significant tokens. To address these challenges, we propose the DINT Transformer, which extends the DIFF Transformer by incorporating an integral mechanism. By computing global importance scores and integrating them into the attention matrix, the DINT Transformer not only improves overall numerical stability but also significantly enhances its ability to capture global dependencies. Experimental results demonstrate that the DINT Transformer achieves superior accuracy and robustness across various practical applications, including long-context language modeling and key information retrieval. These advancements establish the DINT Transformer as a highly effective and promising architecture",
    "checked": true,
    "id": "5eb7dff0deef69bbf585cddfa0493dad93fa9307",
    "semantic_title": "dint transformer",
    "citation_count": 1,
    "authors": [
      "Yueyang Cang",
      "Yuhang Liu",
      "Xiaoteng Zhang",
      "Erlu Zhao",
      "Li Shi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.496": {
    "title": "ICR: Iterative Clarification and Rewriting for Conversational Search",
    "volume": "main",
    "abstract": "Most previous work on Conversational Query Rewriting employs an end-to-end rewriting paradigm. However, this approach is hindered by the issue of multiple fuzzy expressions within the query, which complicates the simultaneous identification and rewriting of multiple positions. To address this issue, we propose a novel framework ICR (Iterative Clarification and Rewriting), an iterative rewriting scheme that pivots on clarification questions. Within this framework, the model alternates between generating clarification questions and rewritten queries. The experimental results show that our ICR can continuously improve retrieval performance in the clarification-rewriting iterative process, thereby achieving state-of-the-art performance on two popular datasets",
    "checked": true,
    "id": "9399970808f0b1965a69f29a659fcd909af4301b",
    "semantic_title": "icr: iterative clarification and rewriting for conversational search",
    "citation_count": 0,
    "authors": [
      "Zhiyu Cao",
      "Peifeng Li",
      "Qiaoming Zhu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.497": {
    "title": "Pre-training CLIP against Data Poisoning with Optimal Transport-based Matching and Alignment",
    "volume": "main",
    "abstract": "Recent studies have shown that Contrastive Language-Image Pre-training (CLIP) models are threatened by targeted data poisoning and backdoor attacks due to massive training image-caption pairs crawled from the Internet. Previous defense methods correct poisoned image-caption pairs by matching a new caption for each image. However, the matching process solely relies on the global representations of images and captions, overlooking fine-grained features of visual and textual features. It may introduce incorrect image-caption pairs and detriment the CLIP pre-training. To address their limitations, we propose an Optimal Transport-based framework to reconstruct the image-caption pairs, named OTCCLIP. We involve a new optimal transport-based distance measure between fine-grained visual and textual feature sets and re-assign new captions based on the proposed optimal transport distance. Additionally, to further reduce the negative impact of mismatched pairs, we encourage the inter- and intra-modality fine-grained alignment by employing optimal transport-based objective functions. Our experiments demonstrate that OTCCLIP can successfully decrease the attack success rates of poisoning attacks to 0% in most cases. Also, compared to previous methods, OTCCLIPsignificantly improves CLIP's zero-shot and linear probing performance trained on poisoned datasets",
    "checked": true,
    "id": "0cb92f9ed5f45ee4fda4c0a0d3fb26a4ec05bc7a",
    "semantic_title": "pre-training clip against data poisoning with optimal transport-based matching and alignment",
    "citation_count": 1,
    "authors": [
      "Tong Zhang",
      "Kuofeng Gao",
      "Jiawang Bai",
      "Leo Yu Zhang",
      "Xin Yin",
      "Zonghui Wang",
      "Shouling Ji",
      "Wenzhi Chen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.498": {
    "title": "Similarity = Value? Consultation Value-Assessment and Alignment for Personalized Search",
    "volume": "main",
    "abstract": "Personalized search systems in e-commerce platforms increasingly involve user interactions with AI assistants, where users consult about products, usage scenarios, and more. Leveraging consultation to personalize search services is trending. Existing methods typically rely on semantic similarity to align historical consultations with current queries due to the absence of ‘value' labels, but we observe that semantic similarity alone often fails to capture the true value of consultation for personalization. To address this, we propose a consultation value assessment framework that evaluates historical consultations from three novel perspectives: (1) Scenario Scope Value, (2) Posterior Action Value, and (3) Time Decay Value. Based on this, we introduce VAPS, a value-aware personalized search model that selectively incorporates high-value consultations through a consultation–user action interaction module and an explicit objective that aligns consultations with user actions. Experiments on both public and commercial datasets show that VAPS consistently outperforms baselines in both retrieval and ranking tasks. Codes are available at https://github.com/E-qin/VAPS",
    "checked": false,
    "id": "4832ebaaa56419d5ff4a5a6db75c69c6971bf9b5",
    "semantic_title": "similarity = value? consultation value assessment and alignment for personalized search",
    "citation_count": 0,
    "authors": [
      "Weicong Qin",
      "Yi Xu",
      "Weijie Yu",
      "Teng Shi",
      "Chenglei Shen",
      "Ming He",
      "Jianping Fan",
      "Xiao Zhang",
      "Jun Xu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.499": {
    "title": "RTQA : Recursive Thinking for Complex Temporal Knowledge Graph Question Answering with Large Language Models",
    "volume": "main",
    "abstract": "Current temporal knowledge graph question answering (TKGQA) methods primarily focus on implicit temporal constraints, lacking the capability to handle more complex temporal queries, and struggle with limited reasoning abilities and error propagation in decomposition frameworks. We propose RTQA, a novel framework to address these challenges by enhancing reasoning over TKGs without requiring training. Following recursive thinking, RTQA recursively decomposes questions into sub-problems, solves them bottom-up using LLMs and TKG knowledge, and employs multi-path answer aggregation to improve fault tolerance. RTQA consists of three core components: the Temporal Question Decomposer, the Recursive Solver, and the Answer Aggregator. Experiments on MultiTQ and TimelineKGQA benchmarks demonstrate significant Hits@1 improvements in \"Multiple\" and \"Complex\" categories, outperforming state-of-the-art methods. Our code and data are available at https://github.com/zjukg/RTQA",
    "checked": true,
    "id": "e37b7928b8dcda4c6f1a3b18a372d651521c6d33",
    "semantic_title": "rtqa : recursive thinking for complex temporal knowledge graph question answering with large language models",
    "citation_count": 2,
    "authors": [
      "Zhaoyan Gong",
      "Juan Li",
      "Zhiqiang Liu",
      "Lei Liang",
      "Huajun Chen",
      "Wen Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.500": {
    "title": "Not All Parameters Are Created Equal: Smart Isolation Boosts Fine-Tuning Performance",
    "volume": "main",
    "abstract": "Supervised fine-tuning (SFT) is a pivotal approach to adapting large language models (LLMs) for downstream tasks; however, performance often suffers from the \"seesaw phenomenon\", where indiscriminate parameter updates yield progress on certain tasks at the expense of others. To address this challenge, we propose a novel Core Parameter Isolation Fine-Tuning (CPI-FT) framework. Specifically, we first independently fine-tune the LLM on each task to identify its core parameter regions by quantifying parameter update magnitudes. Tasks with similar core regions are then grouped based on region overlap, forming clusters for joint modeling. We further introduce a parameter fusion technique: for each task, core parameters from its individually fine-tuned model are directly transplanted into a unified backbone, while non-core parameters from different tasks are smoothly integrated via Spherical Linear Interpolation (SLERP), mitigating destructive interference. A lightweight, pipelined SFT training phase using mixed-task data is subsequently employed, while freezing core regions from prior tasks to prevent catastrophic forgetting. Extensive experiments on multiple public benchmarks demonstrate that our approach significantly alleviates task interference and forgetting, consistently outperforming vanilla multi-task and multi-stage fine-tuning baselines",
    "checked": true,
    "id": "124c0f891b29021d995135a129389066dc687887",
    "semantic_title": "not all parameters are created equal: smart isolation boosts fine-tuning performance",
    "citation_count": 3,
    "authors": [
      "Yao Wang",
      "Di Liang",
      "Minlong Peng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.501": {
    "title": "AI Knows Where You Are: Exposure, Bias, and Inference in Multimodal Geolocation with KoreaGEO",
    "volume": "main",
    "abstract": "Recent advances in vision-language models (VLMs) have enabled accurate image-based geolocation, raising serious concerns about location privacy risks in everyday social media posts. Yet, a systematic evaluation of such risks is still lacking: existing benchmarks show coarse granularity, linguistic bias, and a neglect of multimodal privacy risks. To address these gaps, we introduce KoreaGEO, the first fine-grained, multimodal, and privacy-aware benchmark for geolocation, built on Korean street views. The benchmark covers four socio-spatial clusters and nine place types with rich contextual annotations and two captioning styles that simulate real-world privacy exposure. To evaluate mainstream VLMs, we design a three-path protocol spanning image-only, functional-caption, and high-risk-caption inputs, enabling systematic analysis of localization accuracy, spatial bias, and reasoning behavior. Results show that input modality exerts a stronger influence on localization precision and privacy exposure than model scale or architecture, with high-risk captions substantially boosting accuracy. Moreover, they highlight structural prediction biases toward core cities",
    "checked": true,
    "id": "bc3449ece6da946793724a9e2232a2c86df23300",
    "semantic_title": "ai knows where you are: exposure, bias, and inference in multimodal geolocation with koreageo",
    "citation_count": 0,
    "authors": [
      "Xiaonan Wang",
      "Bo Shao",
      "Hansaem Kim"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.502": {
    "title": "CAT: Causal Attention Tuning For Injecting Fine-grained Causal Knowledge into Large Language Models",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have achieved remarkable success across various domains. However, a fundamental question remains: Can LLMs effectively utilize causal knowledge for prediction and generation? Through empirical studies, we find that LLMs trained directly on large-scale data often capture spurious correlations rather than true causal relationships, leading to suboptimal performance, especially in out-of-distribution (OOD) scenarios. To address this challenge, we propose Causal Attention Tuning (CAT), a novel approach that injects fine-grained causal knowledge into the attention mechanism. We propose an automated pipeline that leverages human priors to automatically generate token-level causal signals and introduce the Re-Attention mechanism to guide training, helping the model focus on causal structures while mitigating noise and biases in attention scores. Experimental results on our proposed Spurious Token Game (STG) benchmark and multiple downstream tasks demonstrate that our approach effectively leverages causal knowledge for prediction and remains robust in OOD scenarios. The CAT achieves an average improvement of 5.76% on the STG dataset and 1.56% on downstream tasks. Notably, the OOD performance of the Llama-3.1-8B model on STG_M increased from 64.5% to 90.5%, and Qwen's OOD performance on the STG_H dataset improved from 25.4% to 55.9%. Implementation details can be found at https://github.com/Kairong-Han/CAT",
    "checked": true,
    "id": "4a81a2e48ab7c2d94fee2ebd32365cf04f4beb8f",
    "semantic_title": "cat: causal attention tuning for injecting fine-grained causal knowledge into large language models",
    "citation_count": 1,
    "authors": [
      "Kairong Han",
      "Wenshuo Zhao",
      "Ziyu Zhao",
      "Ye Jun Jian",
      "Lujia Pan",
      "Kun Kuang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.503": {
    "title": "Enhancing LLM Text Detection with Retrieved Contexts and Logits Distribution Consistency",
    "volume": "main",
    "abstract": "Large language models (LLMs) can generate fluent text, raising concerns about misuse in online comments and academic writing, leading to issues like corpus pollution and copyright infringement. Existing LLM text detection methods often rely on features from the logit distribution of the input text. However, the distinction between the LLM-generated and human-written texts may rely on only a few tokens due to the short length or insufficient information in some texts, leading to minimal and hard-to-detect differences in logit distributions. To address this, we propose HALO, an LLM-based detection method that leverages external text corpora to evaluate the difference in the logit distribution of input text under retrieved human-written and LLM-rewritten contexts. HALO also complements basic detection features and can serve as a plug-and-play module to enhance existing detection methods. Extensive experiments on five public datasets with three widely-used source LLMs show that our proposed detection method achieves state-of-the-art performance in AUROC, both in cross-domain and domain-specific scenarios",
    "checked": true,
    "id": "a7cece4f63baaa0f6dc08fcc711952f0e1b4714f",
    "semantic_title": "enhancing llm text detection with retrieved contexts and logits distribution consistency",
    "citation_count": 0,
    "authors": [
      "Zhaoheng Huang",
      "Yutao Zhu",
      "Ji-Rong Wen",
      "Zhicheng Dou"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.504": {
    "title": "Measuring Chain of Thought Faithfulness by Unlearning Reasoning Steps",
    "volume": "main",
    "abstract": "When prompted to think step-by-step, language models (LMs) produce a chain of thought (CoT), a sequence of reasoning steps that the model supposedly used to produce its prediction. Despite much work on CoT prompting, it is unclear if reasoning verbalized in a CoT is faithful to the models' parametric beliefs. We introduce a framework for measuring parametric faithfulness of generated reasoning and propose Faithfulness by Unlearning Reasoning steps (FUR), an instance of this framework. FUR erases information contained in reasoning steps from model parameters and measures faithfulness as the resulting effect on the model's prediction. Our experiments with four LMs and five multi-choice question answering (MCQA) datasets show that FUR is frequently able to precisely change the underlying models' prediction for a given instance by unlearning key steps, indicating when a CoT is parametrically faithful. Further analysis shows that CoTs generated by models post-unlearning support different answers, hinting at a deeper effect of unlearning",
    "checked": true,
    "id": "aaaa04358e03fe5ef0d24a86e75fe6c1ee0f3ca7",
    "semantic_title": "measuring chain of thought faithfulness by unlearning reasoning steps",
    "citation_count": 4,
    "authors": [
      "Martin Tutek",
      "Fateme Hashemi Chaleshtori",
      "Ana Marasovic",
      "Yonatan Belinkov"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.505": {
    "title": "Stop Looking for \"Important Tokens\" in Multimodal Language Models: Duplication Matters More",
    "volume": "main",
    "abstract": "Vision tokens in multimodal large language models often dominate huge computational overhead due to their excessive length compared to linguistic modality. Abundant recent methods aim to solve this problem with token pruning, which first defines an importance criterion for tokens and then prunes the unimportant vision tokens during inference. However, in this paper, we show that the importance is not an ideal indicator to decide whether a token should be pruned. Surprisingly, it usually results in inferior performance than random token pruning and leading to incompatibility to efficient attention computation operators. Instead, we propose DART (Duplication-Aware Reduction of Tokens), which prunes tokens based on its duplication with other tokens, leading to significant and training-free acceleration. Concretely, DART selects a small subset of pivot tokens and then retains the tokens with low duplication to the pivots, ensuring minimal information loss during token pruning. Experiments demonstrate that DART can prune 88.9% vision tokens while maintaining comparable performance, leading to a 1.99× and 2.99× speed-up in total time and prefilling stage, respectively, with good compatibility to efficient attention operators",
    "checked": false,
    "id": "6567c0e4f7bfb009e05b29bdc4e5b4ef94b40469",
    "semantic_title": "stop looking for important tokens in multimodal language models: duplication matters more",
    "citation_count": 34,
    "authors": [
      "Zichen Wen",
      "Yifeng Gao",
      "Shaobo Wang",
      "Junyuan Zhang",
      "Qintong Zhang",
      "Weijia Li",
      "Conghui He",
      "Linfeng Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.506": {
    "title": "AgentPro: Enhancing LLM Agents with Automated Process Supervision",
    "volume": "main",
    "abstract": "Large language model (LLM) agents have demonstrated significant potential for addressing complex tasks through mechanisms such as chain-of-thought reasoning and tool invocation. However, current frameworks lack explicit supervision during the reasoning process, which may lead to error propagation across reasoning chains and hinder the optimization of intermediate decision-making stages. This paper introduces a novel framework, AgentPro, which enhances LLM agent performance by automated process supervision. AgentPro employs Monte Carlo Tree Search to automatically generate step-level annotations, and develops a process reward model based on these annotations to facilitate fine-grained quality assessment of reasoning. By employing a rejection sampling strategy, the LLM agent dynamically adjusts generation probability distributions to prevent the continuation of erroneous paths, thereby improving reasoning capabilities. Extensive experiments on four datasets indicate that our method significantly outperforms existing agent-based LLM methods (e.g., achieving a 6.32% increase in accuracy on the HotpotQA dataset), underscoring its proficiency in managing intricate reasoning chains",
    "checked": true,
    "id": "654e9418b45cee011619223d0198e7ffac296512",
    "semantic_title": "agentpro: enhancing llm agents with automated process supervision",
    "citation_count": 0,
    "authors": [
      "Yuchen Deng",
      "Shichen Fan",
      "Naibo Wang",
      "Xinkui Zhao",
      "See-Kiong Ng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.507": {
    "title": "PORTS: Preference-Optimized Retrievers for Tool Selection with Large Language Models",
    "volume": "main",
    "abstract": "Integrating external tools with Large Language Models (LLMs) has emerged as a promising paradigm for accomplishing complex tasks. Since LLMs still struggle to effectively manage large tool collections, researchers have begun exploring retrieval-based methods to pre-select the most relevant options, addressing input length and latency constraints. However, existing retrievers are often misaligned with tool-calling LLMs due to their separate training processes. This paper presents PORTS, a novel odds ratio preference optimization method for training retrievers aimed at tool selection. Using a perplexity-inspired preference signal from a frozen LLM, our approach fine-tunes a retriever to find helpful tools by optimizing the correlation between the selection probabilities and the downstream performances while jointly enforcing a contrastive semantic loss between documentation strings. The versatility of PORTS and its ability to significantly improve tool selection accuracy are demonstrated through extensive experiments on six datasets, two encoder models, and three LLMs with diverse prior knowledge. With low computational demands, our alignment process facilitates generalization to new queries and tools, proving valuable for practical applications with evolving toolsets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lorenzo Molfetta",
      "Giacomo Frisoni",
      "Nicolò Monaldini",
      "Gianluca Moro"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.508": {
    "title": "MusKGC: A Flexible Multi-source Knowledge Enhancement Framework for Open-World Knowledge Graph Completion",
    "volume": "main",
    "abstract": "Open-world knowledge graph completion (KGC) aims to infer novel facts by enriching existing graphs with external knowledge sources while maintaining semantic consistency under the open-world assumption (OWA). Generation-based KGC methods leverage the inherent strengths of large language models (LLMs) in language understanding and creative problem-solving, making them promising approaches. However, they face limitations: (1) The unreliable external knowledge from LLMs can lead to hallucinations and undermine KGC reliability. (2) The lack of an automated and rational evaluation strategy for new facts under OWA results in the exclusion of some new but correct entities. In the paper, we propose MusKGC, a novel multi-source knowledge enhancement framework based on an LLM for KGC under OWA. We induce relation templates with entity type constraints to link structured knowledge with natural language, improving the comprehension of the LLM. Next, we combine intrinsic KG facts with reliable external knowledge to guide the LLM in accurately generating missing entities with supporting evidence. Lastly, we introduce a new evaluation strategy for factuality and consistency to validate accurate inferences of new facts, including unknown entities. Extensive experiments show that our proposed model achieves SOTA performance across benchmarks, and our evaluation strategy effectively assesses new facts under OWA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Song",
      "Liu Haiyan",
      "Haiyang Wang",
      "Ye Wang",
      "Kai Chen",
      "Bin Zhou"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.509": {
    "title": "Towards Transferable Personality Representation Learning based on Triplet Comparisons and Its Applications",
    "volume": "main",
    "abstract": "Personality is an important concept in psychology that reflects individual differences in thinking and behavior, and has significant applications across various fields. Most existing personality analysis methods address this issue at the bag level, treating the entire corpus gathered from one individual as a single unit for classification. However, this paradigm presents several challenges. From the data perspective, collecting a large corpus for each individual and performing comprehensive annotations pose significant difficulties in both data collection and labeling. On the application side, concentrating on classifying the entire corpus limits its applicability in more common single-instance scenarios. To address these issues, we propose a new task paradigm in text-based personality representation learning. Specifically, we construct a triplet personality trend comparison dataset to learn single-sentence personality embeddings with desirable metric properties. This approach removes the traditional constraints on data sources, facilitating dataset expansion, and can leverage the transfer capabilities of embeddings to easily adapt to various downstream tasks. Our experiments show that the learned embeddings significantly boost performance by a relative 10% across various applications, including personality detection, personality retrieval, and emotion translation prediction. The code and dataset are available at https://github.com/zjutangk/PTCD",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Tang",
      "Rui Wang",
      "Renyu Zhu",
      "Minmin Lin",
      "Xiao Ding",
      "Tangjie Lv",
      "Changjie Fan",
      "Runze Wu",
      "Haobo Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.510": {
    "title": "Reshaping Representation Space to Balance the Safety and Over-rejection in Large Audio Language Models",
    "volume": "main",
    "abstract": "Large Audio Language Models (LALMs) have extended the capabilities of Large Language Models (LLMs) by enabling audio-based human interactions. However, recent research has revealed that LALMs remain vulnerable to harmful queries due to insufficient safety-alignment. Despite advances in defence measures for text and vision LLMs, effective safety-alignment strategies and audio-safety dataset specifically targeting LALMs are notably absent. Meanwhile defence measures based on Supervised Fine-tuning (SFT) struggle to address safety improvement while avoiding over-rejection issues, significantly compromising helpfulness. In this work, we propose an unsupervised safety-fine-tuning strategy as remedy that reshapes model's representation space to enhance existing LALMs safety-alignment while balancing the risk of over-rejection. Our experiments, conducted across three generations of Qwen LALMs, demonstrate that our approach significantly improves LALMs safety under three modality input conditions (audio-text, text-only, and audio-only) while increasing over-rejection rate by only 0.88% on average. Warning: this paper contains harmful examples",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Yang",
      "Lizhen Qu",
      "Ehsan Shareghi",
      "Gholamreza Haffari"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.511": {
    "title": "Benchmarking Large Language Models Under Data Contamination: A Survey from Static to Dynamic Evaluation",
    "volume": "main",
    "abstract": "In the era of evaluating large language models (LLMs), data contamination has become an increasingly prominent concern. To address this risk, LLM benchmarking has evolved from a *static* to a *dynamic* paradigm. In this work, we conduct an in-depth analysis of existing *static* and *dynamic* benchmarks for evaluating LLMs. We first examine methods that enhance *static* benchmarks and identify their inherent limitations. We then highlight a critical gap—the lack of standardized criteria for evaluating *dynamic* benchmarks. Based on this observation, we propose a series of optimal design principles for *dynamic* benchmarking and analyze the limitations of existing *dynamic* benchmarks.This survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts. We maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simin Chen",
      "Yiming Chen",
      "Zexin Li",
      "Yifan Jiang",
      "Zhongwei Wan",
      "Yixin He",
      "Dezhi Ran",
      "Tianle Gu",
      "Haizhou Li",
      "Tao Xie",
      "Baishakhi Ray"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.512": {
    "title": "FinTrust: A Comprehensive Benchmark of Trustworthiness Evaluation in Finance Domain",
    "volume": "main",
    "abstract": "Recent LLMs have demonstrated promising ability in solving finance related problems. However, applying LLMs in real-world finance application remains challenging due to its high risk and high stakes property. This paper introduces FinTrust, a comprehensive benchmark specifically designed for evaluating the trustworthiness of LLMs in finance applications. Our benchmark focuses on a wide range of alignment issues based on practical context and features fine-grained tasks for each dimension of trustworthiness evaluation. We assess eleven LLMs on FinTrust and find that proprietary models like o4-mini outperforms in most tasks such as safety while open-source models like DeepSeek-V3 have advantage in specific areas like industry-level fairness. For challenging task like fiduciary alignment and disclosure, all LLMs fall short, showing a significant gap in legal awareness. We believe that FinTrust can be a valuable benchmark for LLMs' trustworthiness evaluation in finance domain",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tiansheng Hu",
      "Tongyan Hu",
      "Liuyang Bai",
      "Yilun Zhao",
      "Arman Cohan",
      "Chen Zhao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.513": {
    "title": "RecGPT: A Foundation Model for Sequential Recommendation",
    "volume": "main",
    "abstract": "This work addresses a fundamental barrier in recommender systems: the inability to generalize across domains without extensive retraining. Traditional ID-based approaches fail entirely in cold-start and cross-domain scenarios where new users or items lack sufficient interaction history. Inspired by foundation models' cross-domain success, we develop a foundation model for sequential recommendation that achieves genuine zero-shot generalization capabilities. Our approach fundamentally departs from existing ID-based methods by deriving item representations exclusively from textual features. This enables immediate embedding of any new item without model retraining. We introduce unified item tokenization with Finite Scalar Quantization that transforms heterogeneous textual descriptions into standardized discrete tokens. This eliminates domain barriers that plague existing systems. Additionally, the framework features hybrid bidirectional-causal attention that captures both intra-item token coherence and inter-item sequential dependencies. An efficient catalog-aware beam search decoder enables real-time token-to-item mapping. Unlike conventional approaches confined to their training domains, RecGPT naturally bridges diverse recommendation contexts through its domain-invariant tokenization mechanism. Comprehensive evaluations across six datasets and industrial scenarios demonstrate consistent performance advantages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangqin Jiang",
      "Xubin Ren",
      "Lianghao Xia",
      "Da Luo",
      "Kangyi Lin",
      "Chao Huang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.514": {
    "title": "Towards Holistic Evaluation of Large Audio-Language Models: A Comprehensive Survey",
    "volume": "main",
    "abstract": "With advancements in large audio-language models (LALMs), which enhance large language models (LLMs) with auditory capabilities, these models are expected to demonstrate universal proficiency across various auditory tasks. While numerous benchmarks have emerged to assess LALMs' performance, they remain fragmented and lack a structured taxonomy. To bridge this gap, we conduct a comprehensive survey and propose a systematic taxonomy for LALM evaluations, categorizing them into four dimensions based on their objectives: (1) General Auditory Awareness and Processing, (2) Knowledge and Reasoning, (3) Dialogue-oriented Ability, and (4) Fairness, Safety, and Trustworthiness. We provide detailed overviews within each category and highlight challenges in this field, offering insights into promising future directions. To the best of our knowledge, this is the first survey specifically focused on the evaluations of LALMs, providing clear guidelines for the community",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chih-Kai Yang",
      "Neo S. Ho",
      "Hung-yi Lee"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.515": {
    "title": "Train One Sparse Autoencoder Across Multiple Sparsity Budgets to Preserve Interpretability and Accuracy",
    "volume": "main",
    "abstract": "Sparse Autoencoders (SAEs) have proven to be powerful tools for interpreting neural networks by decomposing hidden representations into disentangled, interpretable features via sparsity constraints. However, conventional SAEs are constrained by the fixed sparsity level chosen during training; meeting different sparsity requirements therefore demands separate models and increases the computational footprint during both training and evaluation. We introduce a novel training objective, HierarchicalTopK, which trains a single SAE to optimise reconstructions across multiple sparsity levels simultaneously. Experiments with Gemma-2 2B demonstrate that our approach achieves Pareto-optimal trade-offs between sparsity and explained variance, outperforming traditional SAEs trained at individual sparsity levels. Further analysis shows that HierarchicalTopK preserves high interpretability scores even at higher sparsity. The proposed objective thus closes an important gap between flexibility and interpretability in SAE design",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikita Balagansky",
      "Yaroslav Aksenov",
      "Daniil Laptev",
      "Vadim Kurochkin",
      "Gleb Gerasimov",
      "Nikita Koriagin",
      "Daniil Gavrilov"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.516": {
    "title": "Learn and Unlearn: Addressing Misinformation in Multilingual LLMs",
    "volume": "main",
    "abstract": "This paper investigates the propagation of information in multilingual large language models (LLMs) and evaluates the efficacy of various unlearning methods. We demonstrate that fake information, regardless of the language it is in, once introduced into these models through training data, can spread across different languages, compromising the integrity and reliability of the generated content. Our findings reveal that standard unlearning techniques, which typically focus on English data, are insufficient in mitigating the spread of harmful content in multilingual contexts and could inadvertently reinforce harmful content across languages. We show that only by addressing harmful responses in both English and the original language of the harmful data we can effectively eliminate it for all languages. This underscores the critical need for comprehensive unlearning strategies that consider the multilingual nature of modern LLMs to enhance their safety and reliability across landscapes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "TaiMing Lu",
      "Philipp Koehn"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.517": {
    "title": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs",
    "volume": "main",
    "abstract": "Long-range tasks demand reasoning over long inputs. However, existing solutions are limited, e.g., long-context models require large compute budgets, parameter-efficient fine-tuning (PEFT) needs training data, and retrieval-augmented generation (RAG) entails complex task-specific designs. Though in-context approaches overcome many of these issues, methods with short-context LLMs are inefficient, trading context for processing more tokens. We introduce **PRISM**, a highly token-efficient in-context method based on structured schemas that outperforms baselines on diverse tasks with **4x shorter contexts**. This approach produces concise outputs and efficiently leverages key-value (KV) caches to **reduce costs by up to 54%**. PRISM scales down to tiny contexts without increasing costs or sacrificing quality, and generalizes to new tasks with minimal effort by generating schemas from task descriptions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dulhan Jayalath",
      "James Bradley Wendt",
      "Nicholas Monath",
      "Sandeep Tata",
      "Beliz Gunel"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.518": {
    "title": "Augmenting Multi-Agent Communication with State Delta Trajectory",
    "volume": "main",
    "abstract": "Multi-agent techniques such as role playing or multi-turn debates have been shown to be effective in improving the performance of large language models (LLMs) in downstream tasks. Despite their differences in workflows, existing multi-agent systems constructed from a single base LLM mostly use natural language for agent communication.While this is appealing for its simplicity and interpretability, it also introduces inevitable information loss as one model must down sample its continuous state vectors to discrete tokens before transferring them to the other model.Such losses are particularly significant when the information to transfer is not simple facts, but reasoning logics or abstractive thoughts.To tackle this problem, we propose a new communication protocol that transfers both natural language tokens and token-wise state transition trajectory from one agent to another.Particularly, compared to the actual state value, we find that the sequence of state changes in LLMs after generating each token can better reflect the information hidden behind the inference process.We propose a State Delta Encoding (SDE) method to represent state transition trajectories.The experimental results show that multi-agent systems with SDE achieve SOTA performance compared to other communication protocols, particularly in tasks that involve complex reasoning. We have open-sourced all the code and data in https://github.com/LittleDinoC/StateDelta/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yichen Tang",
      "Weihang Su",
      "Yujia Zhou",
      "Yiqun Liu",
      "Min Zhang",
      "Shaoping Ma",
      "Qingyao Ai"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.519": {
    "title": "SAEs Are Good for Steering – If You Select the Right Features",
    "volume": "main",
    "abstract": "Sparse Autoencoders (SAEs) have been proposed as an unsupervised approach to learn a decomposition of a model's latent space. This enables useful applications, such as fine-grained steering of model outputs without requiring labeled data. Current steering methods identify SAE features to target by analyzing the input tokens that activate them. However, recent work has highlighted that activations alone do not fully describe the effect of a feature on the model's output. In this work we draw a distinction between two types of features: input features, which mainly capture patterns in the model's input, and output features, those that have a human-understandable effect on the model's output. We propose input and output scores to characterize and locate these types of features, and show that high values for both scores rarely co-occur in the same features. These findings have practical implications: After filtering out features with low output scores, steering with SAEs results in a 2–3x improvement, matching the performance of existing supervised methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dana Arad",
      "Aaron Mueller",
      "Yonatan Belinkov"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.520": {
    "title": "CoBA: Counterbias Text Augmentation for Mitigating Various Spurious Correlations via Semantic Triples",
    "volume": "main",
    "abstract": "Deep learning models often learn and exploit spurious correlations in training data, using these non-target features to inform their predictions. Such reliance leads to performance degradation and poor generalization on unseen data. To address these limitations, we introduce a more general form of counterfactual data augmentation, termed *counterbias* data augmentation, which simultaneously tackles multiple biases (e.g., gender bias, simplicity bias) and enhances out-of-distribution robustness. We present **CoBA**: **Co**unter**B**ias **A**ugmentation, a unified framework that operates at the semantic triple level: first decomposing text into subject-predicate-object triples, then selectively modifying these triples to disrupt spurious correlations. By reconstructing the text from these adjusted triples, **CoBA** generates *counterbias* data that mitigates spurious patterns. Through extensive experiments, we demonstrate that **CoBA** not only improves downstream task performance, but also effectively reduces biases and strengthens out-of-distribution resilience, offering a versatile and robust solution to the challenges posed by spurious correlations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyohoon Jin",
      "Juhwan Choi",
      "JungMin Yun",
      "Junho Lee",
      "Soojin Jang",
      "YoungBin Kim"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.521": {
    "title": "Layered Insights: Generalizable Analysis of Human Authorial Style by Leveraging All Transformer Layers",
    "volume": "main",
    "abstract": "We propose a new approach for the authorship attribution task that leverages the various linguistic representations learned at different layers of pre-trained transformer-based models. We evaluate our approach on two popular authorship attribution models and three evaluation datasets, in in-domain and out-of-domain scenarios. We found that utilizing various transformer layers improves the robustness of authorship attribution models when tested on out-of-domain data, resulting in a much stronger performance. Our analysis gives further insights into how our model's different layers get specialized in representing certain linguistic aspects that we believe benefit the model when tested out of the domain",
    "checked": true,
    "id": "376410456e8a40b2eedc4daa209103a7b892a6ca",
    "semantic_title": "layered insights: generalizable analysis of human authorial style by leveraging all transformer layers",
    "citation_count": 0,
    "authors": [
      "Milad Alshomary",
      "Nikhil Reddy Varimalla",
      "Vishal Anand",
      "Smaranda Muresan",
      "Kathleen McKeown"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.522": {
    "title": "When Long Helps Short: How Context Length in Supervised Fine-tuning Affects Behavior of Large Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) have achieved impressive performance across natural language processing (NLP) tasks. As real-world applications increasingly demand longer context windows, continued pretraining and supervised fine-tuning (SFT) on long-context data has become a common approach. While the effects of data length in continued pretraining have been extensively studied, their implications for SFT remain unclear. In this work, we systematically investigate how SFT data length influences LLM behavior on short-context tasks. Counterintuitively, we find that long-context SFT improves short-context performance, contrary to the commonly observed degradation from long-context pretraining. To uncover the underlying mechanisms of this phenomenon, we first decouple and analyze two key components, Multi-Head Attention (MHA) and Feed-Forward Network (FFN), and show that both independently benefit from long-context SFT. We further study their interaction and reveal a knowledge preference bias: long-context SFT promotes contextual knowledge, while short-context SFT favors parametric knowledge, making exclusive reliance on long-context SFT suboptimal. Finally, we demonstrate that hybrid training mitigates this bias, offering explainable guidance for fine-tuning LLMs",
    "checked": true,
    "id": "0c8d2b0fa29fe95e6675acdd2e12d5150127fb91",
    "semantic_title": "when long helps short: how context length in supervised fine-tuning affects behavior of large language models",
    "citation_count": 0,
    "authors": [
      "Yingming Zheng",
      "Hanqi Li",
      "Kai Yu",
      "Lu Chen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.523": {
    "title": "A Case Against Implicit Standards: Homophone Normalization in Machine Translation for Languages that use the Ge'ez Script",
    "volume": "main",
    "abstract": "Homophone normalization–where characters that have the same sound in a writing script are mapped to one character–is a pre-processing step applied in Amharic Natural Language Processing (NLP) literature. While this may improve performance reported by automatic metrics, it also results in models that are unable to effectively process different forms of writing in a single language. Further, there might be impacts in transfer learning, where models trained on normalized data do not generalize well to other languages. In this paper, we experiment with monolingual training and cross-lingual transfer to understand the impacts of normalization on languages that use the Ge'ez script. We then propose a post-inference intervention in which normalization is applied to model predictions instead of training data. With our simple scheme of post-inference normalization, we show that we can achieve an increase in BLEU score of up to 1.03 while preserving language features in training",
    "checked": true,
    "id": "e17a0832acbb11bb841c251405baa09086ae4ef2",
    "semantic_title": "a case against implicit standards: homophone normalization in machine translation for languages that use the ge'ez script",
    "citation_count": 0,
    "authors": [
      "Hellina Hailu Nigatu",
      "Atnafu Lambebo Tonja",
      "Henok Biadglign Ademtew",
      "Hizkiel Mitiku Alemayehu",
      "Negasi Haile Abadi",
      "Tadesse Destaw Belay",
      "Seid Muhie Yimam"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.524": {
    "title": "Evaluating Language Translation Models by Playing Telephone",
    "volume": "main",
    "abstract": "Our ability to efficiently and accurately evaluate the quality of machine translation systems has been outrun by the effectiveness of current language models—which limits the potential for further improving these models on more challenging tasks like long-form and literary translation. We propose an unsupervised method to generate training data for translation evaluation over different document lengths and application domains by repeated rounds of translation between source and target languages. We evaluate evaluation systems trained on texts mechanically generated using both model rotation and language translation approaches, demonstrating improved performance over a popular translation evaluation system (xCOMET) on two different tasks: (i) scoring the quality of a given translation against a human reference and (ii) selecting which of two translations is generationally closer to an original source document",
    "checked": true,
    "id": "ae6104a217aa929922e005d22c723ca411e9b35d",
    "semantic_title": "evaluating language translation models by playing telephone",
    "citation_count": 0,
    "authors": [
      "Syeda Jannatus Saba",
      "Steven Skiena"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.525": {
    "title": "Doubling Your Data in Minutes: Ultra-fast Tabular Data Generation via LLM-Induced Dependency Graphs",
    "volume": "main",
    "abstract": "Tabular data is critical across diverse domains, yet high-quality datasets remain scarce due to privacy concerns and the cost of collection. Contemporary approaches adopt large language models (LLMs) for tabular augmentation, but exhibit two major limitations: (1) dense dependency modeling among tabular features that can introduce bias, and (2) high computational overhead in sampling. To address these issues, we propose SPADA for SPArse Dependency-driven Augmentation, a lightweight generative framework that explicitly captures sparse dependencies via an LLM-induced graph. We treat each feature as a node and synthesize values by traversing the graph, conditioning each feature solely on its parent nodes. We explore two synthesis strategies: a non-parametric method using Gaussian kernel density estimation, and a conditional normalizing flow model that learns invertible mappings for conditional density estimation. Experiments on four datasets show that SPADA reduces constraint violations by 4% compared to diffusion-based methods and accelerates generation by nearly 9,500× over LLM-based baselines",
    "checked": true,
    "id": "1f18e8a3313924c25b7cbc9aabb883ada87c6df8",
    "semantic_title": "doubling your data in minutes: ultra-fast tabular data generation via llm-induced dependency graphs",
    "citation_count": 4,
    "authors": [
      "Shuo Yang",
      "Zheyu Zhang",
      "Bardh Prenkaj",
      "Gjergji Kasneci"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.526": {
    "title": "SPaRC: A Spatial Pathfinding Reasoning Challenge",
    "volume": "main",
    "abstract": "Existing reasoning datasets saturate and fail to test abstract, multi-step problems, especially pathfinding and complex rule constraint satisfaction. We introduce SPaRC (Spatial Pathfinding Reasoning Challenge), a dataset of 1,000 2D grid pathfinding puzzles to evaluate spatial and rule-based reasoning, requiring step-by-step planning with arithmetic and geometric rules. Humans achieve near-perfect accuracy (98.0%; 94.5% on hard puzzles), while the best reasoning models, such as o4-mini, struggle (15.8%; 1.1% on hard puzzles). Models often generate invalid paths (>50% of puzzles for o4-mini), and reasoning tokens reveal they make errors in navigation and spatial logic. Unlike humans, who take longer on hard puzzles, models fail to scale test-time compute with difficulty. Allowing models to make multiple solution attempts improves accuracy, suggesting potential for better spatial reasoning with improved training and efficient test-time scaling methods. SPaRC can be used as a window into models' spatial reasoning limitations and drive research toward new methods that excel in abstract, multi-step problem-solving",
    "checked": true,
    "id": "5c379bd58ddfaa9befab852fc9936f596b803398",
    "semantic_title": "sparc: a spatial pathfinding reasoning challenge",
    "citation_count": 1,
    "authors": [
      "Lars Benedikt Kaesberg",
      "Jan Philip Wahle",
      "Terry Ruas",
      "Bela Gipp"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.527": {
    "title": "Primus: A Pioneering Collection of Open-Source Datasets for Cybersecurity LLM Training",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have shown remarkable advancements in specialized fields such as finance, law, and medicine. However, in cybersecurity, we have noticed a lack of open-source datasets, with a particular lack of high-quality cybersecurity pretraining corpora, even though much research indicates that LLMs acquire their knowledge during pretraining. To address this, we present a comprehensive suite of datasets covering all major training stages, including pretraining, instruction fine-tuning, and reasoning distillation with cybersecurity-specific self-reflection data. Extensive ablation studies demonstrate their effectiveness on public cybersecurity benchmarks. In particular, continued pre-training on our dataset yields a **15.9%** improvement in the aggregate score, while reasoning distillation leads to a **15.8%** gain in security certification (CISSP). We will release all datasets and trained cybersecurity LLMs under the ODC-BY and MIT licenses to encourage further research in the community",
    "checked": true,
    "id": "208130591c949e50a463cd1ddf955e1942f63982",
    "semantic_title": "primus: a pioneering collection of open-source datasets for cybersecurity llm training",
    "citation_count": 11,
    "authors": [
      "Yao-Ching Yu",
      "Tsun-Han Chiang",
      "Cheng-Wei Tsai",
      "Chien-Ming Huang",
      "Wen-Kwang Tsao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.528": {
    "title": "Bit-Flip Error Resilience in LLMs: A Comprehensive Analysis and Defense Framework",
    "volume": "main",
    "abstract": "Bit-flip errors (BFEs) are hardware faults where individual bits in memory or processing units are unintentionally flipped. These errors pose a significant threat to neural network reliability because even small changes in model parameters can lead to large shifts in outputs. Large language models (LLMs) are particularly vulnerable on resource-constrained or outdated hardware. Such hardware often lacks error-correction mechanisms and faces aging issues, leading to instability under the vast parameter counts and heavy computational loads of LLMs. While the impact of BFEs on traditional networks like CNNs is relatively well-studied, their effect on the complex architecture of transformers remains largely unexplored. Firstly, this paper presents a comprehensive systematic analysis of BFE vulnerabilities in key LLM components, revealing distinct sensitivities across parameters, activations, and gradients during fine-tuning and inference. Secondly, based on our findings, we introduce a novel defense strategy FlipGuard: (i) exponent bit protection, and (ii) a self-correction based fine-tuning mechanism, to address BFE consequences. FlipGuard minimizes performance degradation while significantly enhancing robustness against BFEs. Experiments demonstrate a 9.27 reduction in accuracy drop under 1 BFEs on the SST-2 dataset using BERT, and a 36.35-point improvement in perplexity on the Wikitext-103 dataset using GPT-2, compared to unprotected models. These results show the potential of our approach in enabling reliable LLM deployment on diverse and less reliable hardware platforms",
    "checked": true,
    "id": "2837d9e3014bc8a4bbab7cdd4f8c7b74dc7a5877",
    "semantic_title": "bit-flip error resilience in llms: a comprehensive analysis and defense framework",
    "citation_count": 0,
    "authors": [
      "Yuhang Chen",
      "Zhen Tan",
      "Ajay Kumar Jaiswal",
      "Huaizhi Qu",
      "Xinyu Zhao",
      "Qi Lin",
      "Yu Cheng",
      "Andrew Kwong",
      "Zhichao Cao",
      "Tianlong Chen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.529": {
    "title": "Towards Faithful Natural Language Explanations: A Study Using Activation Patching in Large Language Models",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) are capable of generating persuasive Natural Language Explanations (NLEs) to justify their answers. However, the faithfulness of these explanations should not be readily trusted at face value. Recent studies have proposed various methods to measure the faithfulness of NLEs, typically by inserting perturbations at the explanation or feature level. We argue that these approaches are neither comprehensive nor correctly designed according to the established definition of faithfulness. Moreover, we highlight the risks of grounding faithfulness findings on out-of-distribution samples. In this work, we leverage a causal mediation technique called activation patching, to measure the faithfulness of an explanation towards supporting the explained answer. Our proposed metric, Causal Faithfulness quantifies the consistency of causal attributions between explanations and the corresponding model outputs as the indicator of faithfulness. We experimented across models varying from 2B to 27B parameters and found that models that underwent alignment-tuning tend to produce more faithful and plausible explanations. We find that Causal Faithfulness is a promising improvement over existing faithfulness tests by taking into account the model's internal computations and avoiding out-of-distribution concerns that could otherwise undermine the validity of faithfulness assessments",
    "checked": true,
    "id": "e42262c4b67a8003ca930de0ac6275725bb76332",
    "semantic_title": "towards faithful natural language explanations: a study using activation patching in large language models",
    "citation_count": 2,
    "authors": [
      "Wei Jie Yeo",
      "Ranjan Satapathy",
      "Erik Cambria"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.530": {
    "title": "Calibrating LLM Confidence by Probing Perturbed Representation Stability",
    "volume": "main",
    "abstract": "Miscalibration in Large Language Models (LLMs) undermines their reliability, highlighting the need for accurate confidence estimation. We introduce CCPS (Calibrating LLM Confidence by Probing Perturbed Representation Stability), a novel method analyzing internal representational stability in LLMs. CCPS applies targeted adversarial perturbations to final hidden states, extracts features reflecting the model's response to these perturbations, and uses a lightweight classifier to predict answer correctness. CCPS was evaluated on LLMs from 8B to 32B parameters (covering Llama, Qwen, and Mistral architectures) using MMLU and MMLU-Pro benchmarks in both multiple-choice and open-ended formats. Our results show that CCPS significantly outperforms current approaches. Across four LLMs and three MMLU variants, CCPS reduces Expected Calibration Error by approximately 55% and Brier score by 21%, while increasing accuracy by 5 percentage points, Area Under the Precision-Recall Curve by 4 percentage points, and Area Under the Receiver Operating Characteristic Curve by 6 percentage points, all relative to the strongest prior method. CCPS delivers an efficient, broadly applicable, and more accurate solution for estimating LLM confidence, thereby improving their trustworthiness",
    "checked": true,
    "id": "ab24ae5b6e6827ad5e0050d451e3770f966762f8",
    "semantic_title": "calibrating llm confidence by probing perturbed representation stability",
    "citation_count": 1,
    "authors": [
      "Reza Khanmohammadi",
      "Erfan Miahi",
      "Mehrsa Mardikoraem",
      "Simerjot Kaur",
      "Ivan Brugere",
      "Charese Smiley",
      "Kundan S Thind",
      "Mohammad M. Ghassemi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.531": {
    "title": "SATER: A Self-Aware and Token-Efficient Approach to Routing and Cascading",
    "volume": "main",
    "abstract": "Large language models (LLMs) demonstrate remarkable performance across diverse tasks, yet their effectiveness frequently depends on costly commercial APIs or cloud services. Model selection thus entails a critical trade-off between performance and cost: high-performing LLMs typically incur substantial expenses, whereas budget-friendly small language models (SLMs) are constrained by limited capabilities. Current research primarily proposes two routing strategies: pre-generation routing and cascade routing. Both approaches have distinct characteristics, with cascade routing typically offering superior cost-effectiveness and accuracy despite its higher latency. To further address the limitations of both approaches, we introduce SATER, a dual-mode compatible approach that fine-tunes models through shortest-response preference optimization and a confidence-aware rejection mechanism. SATER significantly reduces redundant outputs and response times, while improving both the performance of pre-generation routing and the efficiency of cascade routing. Experiments across three SLMs and six datasets, varying in type and complexity, demonstrate that SATER achieves comparable performance while consistently reducing computational costs by over 50% and cascade latency by over 80%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanzhe Shen",
      "Yide Liu",
      "Zisu Huang",
      "Ruicheng Yin",
      "Xiaoqing Zheng",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.532": {
    "title": "DSG-MCTS: A Dynamic Strategy-Guided Monte Carlo Tree Search for Diversified Reasoning in Large Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) have shown strong potential in complex reasoning tasks. However, as task complexity increases, their performance often degrades, resulting in hallucinations, errors, and logical inconsistencies. To enhance reasoning capabilities, Monte Carlo Tree Search (MCTS) has been introduced to guide the exploration of reasoning paths in a structured manner. Despite its advantages, traditional MCTS relies on fixed reasoning strategies, limiting the diversity of reasoning paths and the coverage of the solution space. To address these limitations, we propose Dynamic Strategy-Guided MCTS (DSG-MCTS), a novel framework that dynamically integrates multiple reasoning strategies, such as abductive and analogical reasoning, to expand the reasoning space. At the same time, DSG-MCTS enhances reasoning efficiency through a dynamic strategy selection mechanism that adapts to the task context. Experimental results on challenging reasoning benchmarks demonstrate that DSG-MCTS achieves improved accuracy and efficiency, outperforming existing state-of-the-art methods",
    "checked": true,
    "id": "9758601342497aabc935c4ce2afc1d013e44bb47",
    "semantic_title": "dsg-mcts: a dynamic strategy-guided monte carlo tree search for diversified reasoning in large language models",
    "citation_count": 0,
    "authors": [
      "Rui Ha",
      "Chaozhuo Li",
      "Rui Pu",
      "Litian Zhang",
      "Xi Zhang",
      "Sen Su"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.533": {
    "title": "CIFLEX: Contextual Instruction Flow for Sub-task Execution in Multi-Turn Interactions with a Single On-Device LLM",
    "volume": "main",
    "abstract": "We present CIFLEX (Contextual Instruction FLow with EXecution), a novel execution system for efficient sub-task handling in multi-turn interactions with a single on-device large language model (LLM). As LLMs become increasingly capable, a single model is expected to handle diverse sub-tasks that more effectively and comprehensively support answering user requests. Naive approach reprocesses the entire conversation context when switching between main and sub-tasks (e.g., query rewriting, summarization), incurring significant computational overhead. CIFLEX mitigates this overhead by reusing the key-value (KV) cache from the main task and injecting only task-specific instructions into isolated side paths. After sub-task execution, the model rolls back to the main path via cached context, thereby avoiding redundant prefill computation. To support sub-task selection, we also develop a hierarchical classification strategy tailored for small-scale models, decomposing multi-choice decisions into binary ones. Experiments show that CIFLEX significantly reduces computational costs without degrading task performance, enabling scalable and efficient multi-task dialogue on-device",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juntae Lee",
      "Jihwan Bang",
      "Seunghan Yang",
      "Simyung Chang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.534": {
    "title": "On the Role of Model Prior in Real-World Inductive Reasoning",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) show impressive inductive reasoning capabilities, enabling them to generate hypotheses that could generalize effectively to new instances when guided by in-context demonstrations. However, in real-world applications, LLMs' hypothesis generation is not solely determined by these demonstrations but is significantly shaped by task-specific model priors. Despite their critical influence, the distinct contributions of model priors versus demonstrations to hypothesis generation have been underexplored. This study bridges this gap by systematically evaluating three inductive reasoning strategies across five real-world tasks with three LLMs. Our empirical findings reveal that, hypothesis generation is primarily driven by the model's inherent priors; removing demonstrations results in minimal loss of hypothesis quality and downstream usage. Further analysis shows the result is consistent across various label formats with different label configurations, and prior is hard to override, even under flipped labeling. These insights advance our understanding of the dynamics of hypothesis generation in LLMs and highlight the potential for better utilizing model priors in real-world inductive reasoning tasks",
    "checked": true,
    "id": "a2161c412ef9cffb965e6c284885966f92c8e83f",
    "semantic_title": "on the role of model prior in real-world inductive reasoning",
    "citation_count": 0,
    "authors": [
      "Zhuo Liu",
      "Ding Yu",
      "Hangfeng He"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.535": {
    "title": "Viability of Machine Translation for Healthcare in Low-Resourced Languages",
    "volume": "main",
    "abstract": "Machine Translation errors in high-stakes settings like healthcare pose unique risks that could lead to clinical harm. The challenges are even more pronounced for low-resourced languages where human translators are scarce and MT tools perform poorly. In this work, we provide a taxonomy of Machine Translation errors for the healthcare domain using a publicly available MT system. Preparing an evaluation dataset from pre-existing medical datasets, we conduct our study focusing on two low-resourced languages: Amharic and Tigrinya. Based on our error analysis and findings from prior work, we test two pre-translation interventions–namely, paraphrasing the source sentence and pivoting with a related language– for their effectiveness in reducing clinical risk. We find that MT errors for healthcare most commonly happen when the source sentence includes medical terminology and procedure descriptions, synonyms, figurative language, and word order differences. We find that pre-translation interventions are not effective in reducing clinical risk if the base translation model performs poorly. Based on our findings, we provide recommendations for improving MT for healthcare",
    "checked": true,
    "id": "a782601061797e60728a0f7935e5b53ec8aa7e6a",
    "semantic_title": "viability of machine translation for healthcare in low-resourced languages",
    "citation_count": 0,
    "authors": [
      "Hellina Hailu Nigatu",
      "Nikita Mehandru",
      "Negasi Haile Abadi",
      "Blen Gebremeskel",
      "Ahmed Alaa",
      "Monojit Choudhury"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.536": {
    "title": "Latent Inter-User Difference Modeling for LLM Personalization",
    "volume": "main",
    "abstract": "Large language models (LLMs) are increasingly integrated into users' daily lives, leading to a growing demand for personalized outputs.Previous work focuses on leveraging a user's own history, overlooking inter-user differences that are crucial for effective personalization.While recent work has attempted to model such differences, the reliance on language-based prompts often hampers the effective extraction of meaningful distinctions.To address these issues, we propose Difference-aware Embedding-based Personalization (DEP), a framework that models inter-user differences in the latent space instead of relying on language prompts. DEP constructs soft prompts by contrasting a user's embedding with those of peers who engaged with similar content, highlighting relative behavioral signals.A sparse autoencoder then filters and compresses both user-specific and difference-aware embeddings, preserving only task-relevant features before injecting them into a frozen LLM.Experiments on personalized review generation show that DEP consistently outperforms baseline methods across multiple metrics.Our code is available at https://github.com/SnowCharmQ/DEP",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilun Qiu",
      "Tianhao Shi",
      "Xiaoyan Zhao",
      "Fengbin Zhu",
      "Yang Zhang",
      "Fuli Feng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.537": {
    "title": "IG-Pruning: Input-Guided Block Pruning for Large Language Models",
    "volume": "main",
    "abstract": "With the growing computational demands of large language models (LLMs), efficient inference has become increasingly critical for practical deployment. Depth pruning has emerged as a promising approach for reducing the computational costs of large language models by removing transformer layers. However, existing methods typically rely on fixed block masks, which can lead to suboptimal performance across different tasks and inputs. In this paper, we propose IG-Pruning, a novel input-aware block-wise pruning method that dynamically selects layer masks at inference time. Our approach consists of two stages: (1) Discovering diverse mask candidates through semantic clustering and L0 optimization, and (2) Implementing efficient dynamic pruning without the need for extensive training. Experimental results demonstrate that our method consistently outperforms state-of-the-art static depth pruning methods, making it particularly suitable for resource-constrained deployment scenarios",
    "checked": true,
    "id": "8c6e2ff5feccd62d750e3a92edd783f71fcd633d",
    "semantic_title": "ig-pruning: input-guided block pruning for large language models",
    "citation_count": 0,
    "authors": [
      "Kangyu Qiao",
      "Shaolei Zhang",
      "Yang Feng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.538": {
    "title": "Are Checklists Really Useful for Automatic Evaluation of Generative Tasks?",
    "volume": "main",
    "abstract": "Automatic evaluation of generative tasks using large language models faces challenges due to ambiguous criteria. Although automatic checklist generation is a potentially promising approach, its usefulness remains underexplored.We investigate whether checklists should be used for all questions or selectively, generate them using six methods, evaluate their effectiveness across eight model sizes, and identify checklist items that correlate with human evaluations.Through experiments on pairwise comparison and direct scoring tasks, we find that selective checklist use tends to improve evaluation performance in pairwise settings, while its benefits are less consistent in direct scoring.Our analysis also shows that even checklist items with low correlation to human scores often reflect human-written criteria, indicating potential inconsistencies in human evaluation. These findings highlight the need to more clearly define objective evaluation criteria to guide both human and automatic evaluations.Our code is available at https://github.com/momo0817/checklist-effectiveness-study",
    "checked": true,
    "id": "47140caf9064fd575e49d8c88f893d0c5fd648c3",
    "semantic_title": "are checklists really useful for automatic evaluation of generative tasks?",
    "citation_count": 1,
    "authors": [
      "Momoka Furuhashi",
      "Kouta Nakayama",
      "Takashi Kodama",
      "Saku Sugawara"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.539": {
    "title": "Measuring the Effect of Disfluency in Multilingual Knowledge Probing Benchmarks",
    "volume": "main",
    "abstract": "For multilingual factual knowledge assessment of LLMs, benchmarks such as MLAMA use template translations that do not take into account the grammatical and semantic information of the named entities inserted in the sentence. This leads to numerous instances of ungrammaticality or wrong wording of the final prompts, which complicates the interpretation of scores, especially for languages that have a rich morphological inventory. In this work, we sample 4 Slavic languages from the MLAMA dataset and compare the knowledge retrieval scores between the initial (templated) MLAMA dataset and its sentence-level translations made by Google Translate and ChatGPT. We observe a significant increase in knowledge retrieval scores, and provide a qualitative analysis for possible reasons behind it. We also make an additional analysis of 5 more languages from different families and see similar patterns. Therefore, we encourage the community to control the grammaticality of highly multilingual datasets for higher and more interpretable results, which is well approximated by whole sentence translation with neural MT or LLM systems",
    "checked": true,
    "id": "53af1832c365791dd502efb0a9f0e4532db2a0d0",
    "semantic_title": "measuring the effect of disfluency in multilingual knowledge probing benchmarks",
    "citation_count": 0,
    "authors": [
      "Kirill Semenov",
      "Rico Sennrich"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.540": {
    "title": "Knowledge Editing through Chain-of-Thought",
    "volume": "main",
    "abstract": "Knowledge Editing is a technique that updates large language models (LLMs) with new information to maintain their world knowledge. This approach avoids the need to rebuild the model from scratch, thereby addressing the high costs associated with frequent retraining. Among these, the in-context editing paradigm stands out for its effectiveness in integrating new knowledge while preserving the model's original capabilities. Despite its potential, existing in-context knowledge editing methods are often task-specific, focusing primarily on multi-hop QA tasks using structured knowledge triples. Moreover, their reliance on few-shot prompting for task decomposition makes them unstable and less effective in generalizing across diverse tasks. In response to these limitations, we propose EditCoT, a novel knowledge editing framework that flexibly and efficiently updates LLMs across various tasks without retraining. EditCoT works by generating a chain-of-thought (CoT) for a given input and then iteratively refining this CoT process using a CoT editor based on updated knowledge. We evaluate EditCoT across a diverse range of benchmarks, covering multiple languages and tasks. The results demonstrate that our approach achieves state-of-the-art performance while offering superior generalization, effectiveness, and stability compared to existing methods, marking a significant advancement in the field of knowledge updating",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changyue Wang",
      "Weihang Su",
      "Qingyao Ai",
      "Yichen Tang",
      "Yiqun Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.541": {
    "title": "SelfRACG: Enabling LLMs to Self-Express and Retrieve for Code Generation",
    "volume": "main",
    "abstract": "Existing retrieval-augmented code generation (RACG) methods typically use an external retrieval module to fetch semantically similar code snippets used for generating subsequent fragments. However, even for consecutive code fragments, the content often diverges due to logical progression, resulting in a content gap. This gap undermines the performance of current RACG methods, as external retrieval modules based on content matching fail to infer the specific information need of LLMs to generate the next code fragment. Therefore, we propose SelfRACG, a novel paradigm that enables large language models (LLMs) to Self-express their information needs to enhance RACG. Specifically, SelfRACG includes an information need expression module and a two-stage information need-guided training strategy, which encourages LLMs to express their information need. Extensive experiments demonstrate that SelfRACG can retrieve external knowledge that better aligns with the LLM's own information needs, resulting in superior generation performance compared to vanilla RACG. Moreover, both the training and deployment costs for retrieval in our framework are much lower than those of the strongest retrieval model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qian Dong",
      "Jia Chen",
      "Qingyao Ai",
      "Hongning Wang",
      "Haitao Li",
      "Yiwu",
      "Yao Hu",
      "Yiqun Liu",
      "Shaoping Ma"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.542": {
    "title": "Probing Logical Reasoning of MLLMs in Scientific Diagrams",
    "volume": "main",
    "abstract": "We examine how multimodal large language models (MLLMs) perform logical inference grounded in visual information. We first construct a dataset of food web/chain images, along with questions that follow seven structured templates with progressively more complex reasoning involved. We show that complex reasoning about entities in the images remains challenging (even with elaborate prompts) and that visual information is underutilized",
    "checked": true,
    "id": "580bbae8553e619dfc0d80f4664c841e341dc34e",
    "semantic_title": "probing logical reasoning of mllms in scientific diagrams",
    "citation_count": 0,
    "authors": [
      "Yufei Wang",
      "Adriana Kovashka"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.543": {
    "title": "AdamS: Momentum Itself Can Be A Normalizer for LLM Pretraining and Post-training",
    "volume": "main",
    "abstract": "We introduce AdamS, a simple yet effective alternative to Adam for large language model (LLM) pretraining and post-training. By leveraging a novel denominator, i.e., the root of weighted sum of squares of the momentum and the current gradient, AdamS eliminates the need for second-moment estimates. Hence, AdamS is efficient, matching the memory and compute footprint of SGD with momentum while delivering superior optimization performance. Moreover, AdamS is easy to adopt: it can directly inherit hyperparameters of AdamW, and is entirely model-agnostic, integrating seamlessly into existing pipelines without modifications to optimizer APIs or architectures. The motivation behind AdamS stems from the observed smoothness properties in transformer objectives, where local smoothness is governed by gradient magnitudes that can be further approximated by momentum magnitudes. We establish rigorous theoretical convergence guarantees and provide practical guidelines for hyperparameter selection. Empirically, AdamS demonstrates strong performance in various tasks, including pre-training runs on GPT-2 and Llama2 (up to 13B parameters) and reinforcement learning in post-training regimes. With its efficiency, simplicity, and theoretical grounding, AdamS stands as a compelling alternative to existing optimizers",
    "checked": true,
    "id": "46c7b2c0ee6d12f79d0547ee4e8f2c72ed753911",
    "semantic_title": "adams: momentum itself can be a normalizer for llm pretraining and post-training",
    "citation_count": 0,
    "authors": [
      "Huishuai Zhang",
      "Bohan Wang",
      "Luoxin Chen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.544": {
    "title": "Demystifying Synthetic Data in LLM Pre-training: A Systematic Study of Scaling Laws, Benefits, and Pitfalls",
    "volume": "main",
    "abstract": "Training data plays a crucial role in Large Language Models (LLM) scaling, yet high quality data is of limited supply. Synthetic data techniques offer a potential path toward sidestepping these limitations.We conduct a large-scale empirical investigation (>1000 LLMs with >100k GPU hours) using a unified protocol and scaling laws, comparing natural web data, diverse synthetic types (rephrased text, generated textbooks), and mixtures of natural and synthetic data. Specifically, we found pre-training on rephrased synthetic data alone is not faster than pre-training on natural web texts; while pre-training on 1/3 rephrased synthetic data mixed with 2/3 natural web texts can speed up 5-10x (to reach the same validation loss) at larger data budgets. Pre-training on textbook-style synthetic data alone results in notably higher loss on many downstream domains especially at small data budgets. \"Good\" ratios of synthetic data in training data mixtures depend on the model size and data budget, empirically converging to ~30% for rephrased synthetic data. Larger generator models do not necessarily yield better pre-training data than ~8B-param models. These results contribute mixed evidence on \"model collapse\" during large-scale single-round (n=1) model training on synthetic data–training on rephrased synthetic data shows no degradation in performance in foreseeable scales whereas training on mixtures of textbook-style pure-generated synthetic data shows patterns predicted by \"model collapse\". Our work demystifies synthetic data in pre-training, validates its conditional benefits, and offers practical guidance",
    "checked": true,
    "id": "cd31ee04059cf49054ee25bc79fe0a863b19063d",
    "semantic_title": "demystifying synthetic data in llm pre-training: a systematic study of scaling laws, benefits, and pitfalls",
    "citation_count": 0,
    "authors": [
      "Feiyang Kang",
      "Newsha Ardalani",
      "Michael Kuchnik",
      "Youssef Emad",
      "Mostafa Elhoushi",
      "Shubhabrata Sengupta",
      "Shang-Wen Li",
      "Ramya Raghavendra",
      "Ruoxi Jia",
      "Carole-Jean Wu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.545": {
    "title": "Static or Dynamic: Towards Query-Adaptive Token Selection for Video Question Answering",
    "volume": "main",
    "abstract": "Video question answering benefits from the rich information in videos, enabling various applications. However, the large volume of tokens generated from long videos presents challenges to memory efficiency and model performance. To alleviate this, existing works propose to compress video inputs, but often overlook the varying importance of static and dynamic information across different queries, leading to inefficient token usage within limited budgets. We propose a novel token selection strategy, explore-then-select, that adaptively adjusts static and dynamic information based on question requirements. Our framework first explores different token allocations between key frames, which preserve spatial details, and delta frames, which capture temporal changes. Then it employs a query-aware attention-based metric to select the optimal token combination without model updates. Our framework is plug-and-play and can be seamlessly integrated within diverse video language models. Extensive experiments show that our method achieves significant performance improvements (up to 5.8%) on multiple video question answering benchmarks. Our code is available at *https://github.com/ANDgate99/Explore-Then-Select*",
    "checked": true,
    "id": "d4dff97340197fb610bc1cf78cff31cedd6e5527",
    "semantic_title": "static or dynamic: towards query-adaptive token selection for video question answering",
    "citation_count": 0,
    "authors": [
      "Yumeng Shi",
      "Quanyu Long",
      "Wenya Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.546": {
    "title": "DischargeSim: A Simulation Benchmark for Educational Doctor–Patient Communication at Discharge",
    "volume": "main",
    "abstract": "Discharge communication is a critical yet underexplored component of patient care, where the goal shifts from diagnosis to education. While recent large language model (LLM) benchmarks emphasize in-visit diagnostic reasoning, they fail to evaluate models' ability to support patients after the visit. We introduce DischargeSim, a novel benchmark that evaluates LLMs on their ability to act as personalized discharge educators. DischargeSim simulates post-visit, multi-turn conversations between LLM-driven DoctorAgents and PatientAgents with diverse psychosocial profiles (e.g., health literacy, education, emotion). Interactions are structured across six clinically grounded discharge topics and assessed along three axes: (1) dialogue quality via automatic and LLM-as-judge evaluation, (2) personalized document generation including free-text summaries and structured AHRQ checklists, and (3) patient comprehension through a downstream multiple-choice exam. Experiments across 18 LLMs reveal significant gaps in discharge education capability, with performance varying widely across patient profiles. Notably, model size does not always yield better education outcomes, highlighting trade-offs in strategy use and content prioritization. DischargeSim offers a first step toward benchmarking LLMs in post-visit clinical education and promoting equitable, personalized patient support",
    "checked": false,
    "id": "62b7fa550c5af54211db695c00d64ed15dee556e",
    "semantic_title": "dischargesim: a simulation benchmark for educational doctor-patient communication at discharge",
    "citation_count": 0,
    "authors": [
      "Zonghai Yao",
      "Michael Sun",
      "Won Seok Jang",
      "Sunjae Kwon",
      "Soie Kwon",
      "Hong Yu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.547": {
    "title": "Can Vision-Language Models Solve Visual Math Equations?",
    "volume": "main",
    "abstract": "Despite strong performance in visual understanding and language-based reasoning, Vision-Language Models (VLMs) struggle with tasks requiring integrated perception and symbolic computation. We study this limitation through visual equation solving, where mathematical equations are embedded in images, variables are represented by object icons, and coefficients must be inferred by counting. While VLMs perform well on textual equations, they fail on visually grounded counterparts. To understand this gap, we decompose the task into coefficient counting and variable recognition, and find that counting is the primary bottleneck, even when recognition is accurate. We also observe that composing recognition and reasoning introduces additional errors, highlighting challenges in multi-step visual reasoning. Finally, as equation complexity increases, symbolic reasoning itself becomes a limiting factor. These findings reveal key weaknesses in current VLMs and point toward future improvements in visually grounded mathematical reasoning",
    "checked": true,
    "id": "2f17247df81844c8b1c761c9ee38e70807a1fa08",
    "semantic_title": "can vision-language models solve visual math equations?",
    "citation_count": 0,
    "authors": [
      "Monjoy Narayan Choudhury",
      "Junling Wang",
      "Yifan Hou",
      "Mrinmaya Sachan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.548": {
    "title": "From Scores to Steps: Diagnosing and Improving LLM Performance in Evidence-Based Medical Calculations",
    "volume": "main",
    "abstract": "Large language models (LLMs) have demonstrated promising performance on medical benchmarks; however, their ability to perform medical calculations, a crucial aspect of clinical decision-making, remains underexplored and poorly evaluated. Existing benchmarks often assess only the final answer with a wide numerical tolerance, overlooking systematic reasoning failures and potentially causing serious clinical misjudgments. In this work, we revisit medical calculation evaluation with a stronger focus on clinical trustworthiness. First, we clean and restructure the MedCalc-Bench dataset and propose a new step-by-step evaluation pipeline that independently assesses formula selection, entity extraction, and arithmetic computation. Under this granular framework, the accuracy of GPT-4o drops from 62.7% to 43.6%, revealing errors masked by prior evaluations. Second, we introduce an automatic error analysis framework that generates structured attribution for each failure mode. Human evaluation confirms its alignment with expert judgment, enabling scalable and explainable diagnostics. Finally, we propose a modular agentic pipeline, MedRaC, that combines retrieval-augmented generation and Python-based code execution. Without any fine-tuning, MedRaC improves the accuracy of different LLMs from 16.35% up to 53.19%. Our work highlights the limitations of current benchmark practices and proposes a more clinically faithful methodology. By enabling transparent and transferable reasoning evaluation, we move closer to making LLM-based systems trustworthy for real-world medical applications",
    "checked": true,
    "id": "f704d289a3a1075ded47e554f2dc08466dd64107",
    "semantic_title": "from scores to steps: diagnosing and improving llm performance in evidence-based medical calculations",
    "citation_count": 1,
    "authors": [
      "Benlu Wang",
      "Iris Xia",
      "Yifan Zhang",
      "Junda Wang",
      "Feiyun Ouyang",
      "Shuo Han",
      "Arman Cohan",
      "Hong Yu",
      "Zonghai Yao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.549": {
    "title": "Bridging External and Parametric Knowledge: Mitigating Hallucination of LLMs with Shared-Private Semantic Synergy in Dual-Stream Knowledge",
    "volume": "main",
    "abstract": "Retrieval-augmented generation (RAG) aims to mitigate the hallucination of Large Language Models (LLMs) by retrieving and incorporating relevant external knowledge into the generation process. However, the external knowledge may contain noise and conflict with the parametric knowledge of LLMs, leading to degraded performance. Current LLMs lack inherent mechanisms for resolving such conflicts. To fill this gap, we propose a Dual-Stream Knowledge-Augmented Framework for Shared-Private Semantic Synergy (DSSP-RAG). Central to it is the refinement of the traditional self-attention into a mixed-attention that distinguishes shared and private semantics for a controlled knowledge integration. An unsupervised hallucination detection method that captures the LLMs' intrinsic cognitive uncertainty ensures that external knowledge is introduced only when necessary. To reduce noise in external knowledge, an Energy Quotient (EQ), defined by attention difference matrices between task-aligned and task-misaligned layers, is proposed. Extensive experiments show that DSSP-RAG achieves a superior performance over strong baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Sui",
      "Chaozhuo Li",
      "Chen Zhang",
      "Dawei Song",
      "Qiuchi Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.550": {
    "title": "Deep Associations, High Creativity: A Simple yet Effective Metric for Evaluating Large Language Models",
    "volume": "main",
    "abstract": "The evaluation of LLMs' creativity represents a crucial research domain, though challenges such as data contamination and costly human assessments often impede progress. Drawing inspiration from human creativity assessment, we propose PACE, asking LLMs to generate Parallel Chains of Associations to Evaluate their creativity. PACE minimizes the risk of data contamination and offers a straightforward, highly efficient evaluation, as evidenced by its strong correlation with Arena Creative Writing (Spearman's 𝜌 = 0.739, p < 0.001) on various proprietary and open-source models. A comparative analysis of associative creativity between LLMs and humans reveals that while high-performing LLMs achieve scores comparable to average human performance, top-performing humans consistently outperform LLMs. Furthermore, linguistic analysis reveals that both humans and LLMs exhibit a trend of decreasing concreteness in their associations, and humans demonstrating a greater diversity of associative patterns",
    "checked": true,
    "id": "1b1432e9a68eb0b56de584428b352adb377a53bd",
    "semantic_title": "deep associations, high creativity: a simple yet effective metric for evaluating large language models",
    "citation_count": 0,
    "authors": [
      "Ziliang Qiu",
      "Renfen Hu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.551": {
    "title": "Identifying Unlearned Data in LLMs via Membership Inference Attacks",
    "volume": "main",
    "abstract": "Unlearning evaluation has traditionally followed the retrieval paradigm, where adversaries attempt to extract residual knowledge of an unlearning target by issuing queries to a language model. However, the absence of retrievable knowledge does not necessarily prevent an adversary from inferring which targets have been intentionally unlearned in the post-training optimization. Such inferences can still pose significant privacy risks, as they may reveal the sensitive data in the model's training set and the internal policies of model creators. To quantify such privacy risks, we propose a new evaluation framework **Forensic Unlearning Membership Attacks (FUMA)**, drawing on principles from membership inference attacks. FUMA assesses whether unlearning leaves behind detectable artifacts that can be exploited to infer membership in the forget set. Specifically, we evaluate four major optimization-based unlearning methods on 258 models across diverse unlearned settings and show that examples in the forget set can be identified up to 99% accuracy. This highlights privacy risks not covered in existing retrieval-based benchmarks. We conclude by discussing recommendations to mitigate these vulnerabilities",
    "checked": true,
    "id": "52d1097f847a3e413245fe79fb73ce039e7e4bc4",
    "semantic_title": "identifying unlearned data in llms via membership inference attacks",
    "citation_count": 0,
    "authors": [
      "Advit Deepak",
      "Megan Mou",
      "Jing Huang",
      "Diyi Yang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.552": {
    "title": "Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning in Language Models",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) demonstrate the ability to solve reasoning and mathematical problems using the Chain-of-Thought (CoT) technique. Expanding CoT length, as seen in models such as DeepSeek-R1, significantly enhances this reasoning for complex problems, but requires costly and high-quality long CoT data and fine-tuning. This work, inspired by the deep thinking paradigm of DeepSeek-R1, utilizes a steering technique to enhance the reasoning ability of an LLM without external datasets. Our method first employs Sparse Autoencoders (SAEs) to extract interpretable features from vanilla CoT. These features are then used to steer the LLM's internal states during generation. Recognizing that many LLMs do not have corresponding pre-trained SAEs, we further introduce a novel SAE-free steering algorithm, which directly computes steering directions from the residual activations of an LLM, obviating the need for an explicit SAE. Experimental results demonstrate that both our SAE-based and subsequent SAE-free steering algorithms significantly enhance the reasoning capabilities of LLMs",
    "checked": true,
    "id": "2a832b34583332cbaae8cf02ee5f823c43f5689e",
    "semantic_title": "feature extraction and steering for enhanced chain-of-thought reasoning in language models",
    "citation_count": 10,
    "authors": [
      "Zihao Li",
      "Xu Wang",
      "Yuzhe Yang",
      "Ziyu Yao",
      "Haoyi Xiong",
      "Mengnan Du"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.553": {
    "title": "LLMs cannot spot math errors, even when allowed to peek into the solution",
    "volume": "main",
    "abstract": "Large language models (LLMs) demonstrate remarkable performance on math word problems, yet they have been shown to struggle with meta-reasoning tasks such as identifying errors in student solutions. In this work, we investigate the challenge of locating the first error step in stepwise solutions using two error reasoning datasets: VtG and PRM800K. Our experiments show that state-of-the-art LLMs struggle to locate the first error step in student solutions even when given access to the reference solution. To that end, we propose an approach that generates an intermediate corrected student solution, aligning more closely with the original student's solution, which helps improve performance",
    "checked": true,
    "id": "04779a727759edd098a7e8e5ee844aef97e14e78",
    "semantic_title": "llms cannot spot math errors, even when allowed to peek into the solution",
    "citation_count": 0,
    "authors": [
      "Kv Aditya Srivatsa",
      "Kaushal Kumar Maurya",
      "Ekaterina Kochmar"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.554": {
    "title": "Can LLMs be Good Graph Judge for Knowledge Graph Construction?",
    "volume": "main",
    "abstract": "In real-world scenarios, most of the data obtained from the information retrieval (IR) system is unstructured. Converting natural language sentences into structured Knowledge Graphs (KGs) remains a critical challenge. We identified three limitations with respect to existing KG construction methods: (1) There could be a large amount of noise in real-world documents, which could result in extracting messy information. (2) Naive LLMs usually extract inaccurate knowledge from some domain-specific documents. (3) Hallucination phenomenon cannot be overlooked when directly using LLMs to construct KGs. In this paper, we propose GraphJudge, a KG construction framework to address the aforementioned challenges. In this framework, we designed an entity-centric strategy to eliminate the noise information in the documents. And we fine-tuned a LLM as a graph judge to finally enhance the quality of generated KGs. Experiments conducted on two general and one domain-specific text-graph pair datasets demonstrate state-of-the-art performance against various baseline methods with strong generalization abilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Huang",
      "Chong Chen",
      "Zeang Sheng",
      "Yang Li",
      "Wentao Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.555": {
    "title": "NeuroAda: Activating Each Neuron's Potential for Parameter-Efficient Fine-Tuning",
    "volume": "main",
    "abstract": "Existing parameter-efficient fine-tuning (PEFT) methods primarily fall into two categories: addition-based and selective in-situ adaptation. The former, such as LoRA, introduce additional modules to adapt the model to downstream tasks, offering strong memory efficiency. However, their representational capacity is often limited, making them less suitable for fine-grained adaptation. In contrast, the latter directly fine-tunes a carefully chosen subset of the original model parameters, allowing for more precise and effective adaptation, but at the cost of significantly increased memory consumption.To reconcile this trade-off, we propose NeuroAda, a novel PEFT method that enables fine-grained model finetuning while maintaining high memory efficiency. Our approach first identifies important parameters (i.e., connections within the network) as in selective adaptation, and then introduces bypass connections for these selected parameters. During finetuning, only the bypass connections are updated, leaving the original model parameters frozen.Empirical results on 23+ tasks spanning both natural language generation and understanding demonstrate that NeuroAda achieves state-of-the-art performance with as little as ≤ 0.02% trainable parameters, while reducing CUDA memory usage by up to 60%.We release our code here: https://github.com/FightingFighting/NeuroAda.git",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhi Zhang",
      "Yixian Shen",
      "Congfeng Cao",
      "Ekaterina Shutova"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.556": {
    "title": "NileChat: Towards Linguistically Diverse and Culturally Aware LLMs for Local Communities",
    "volume": "main",
    "abstract": "Enhancing the linguistic capabilities of Large Language Models (LLMs) to include low-resource languages is a critical research area. Current research directions predominantly rely on synthetic data generated by translating English corpora, which, while demonstrating promising linguistic understanding and translation abilities, often results in models aligned with source language culture. These models frequently fail to represent the cultural heritage and values of local communities. This work proposes a methodology to create both synthetic and retrieval-based pre-training data tailored to a specific community, considering its (i) language, (ii) cultural heritage, and (iii) cultural values. We demonstrate our methodology using Egyptian and Moroccan dialects as testbeds, chosen for their linguistic and cultural richness and current underrepresentation in LLMs. As a proof-of-concept, we develop NileChat, a 3B parameter Egyptian and Moroccan Arabic LLM adapted for Egyptian and Moroccan communities, incorporating their language, cultural heritage, and values. Our results on various understanding, translation, and cultural and values alignment benchmarks show that NileChat outperforms existing Arabic-aware LLMs of similar size and performs on par with larger models. This work addresses Arabic dialect in LLMs with a focus on cultural and values alignment via controlled synthetic data generation and retrieval-augmented pre-training for Moroccan Darija and Egyptian Arabic, including Arabizi variants, advancing Arabic NLP for low-resource communities.We share our methods, data, and models with the community to promote the inclusion and coverage of more diverse communities in cultural LLM development: https://github.com/UBC-NLP/nilechat",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abdellah El Mekki",
      "Houdaifa Atou",
      "Omer Nacar",
      "Shady Shehata",
      "Muhammad Abdul-Mageed"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.557": {
    "title": "A Computational Simulation of Language Production in First Language Acquisition",
    "volume": "main",
    "abstract": "We introduce a computational framework for modeling child language production, focusing on the acquisition of the competence to map meaning onto linguistic form. Our approach uses graphs to formalize meaning and Synchronous Hyperedge Replacement Grammar (SHRG) to formalize the syntax–semantics interface.The setup provides computationally-sound induction algorithms of statistical grammar knowledge. We induce SHRGs solely from semantic graphs, and the resulting interpretable grammars are evaluated by their ability to generate utterances—providing a novel controlled paradigm to simulate child language acquisition.A notable finding is that unsupervised statistical learning (analogous to children's implicit learning mechanisms) performs as well as the corresponding supervised oracle when a proper symbolic grammar is assumed (reflecting knowledge gained via comprehension)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Gao",
      "Weiwei Sun"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.558": {
    "title": "Long-Form Information Alignment Evaluation Beyond Atomic Facts",
    "volume": "main",
    "abstract": "Information alignment evaluators are vital for various NLG evaluation tasks and trustworthy LLM deployment, reducing hallucinations and enhancing user trust. Current fine-grained methods, like FactScore, verify facts individually but neglect inter-fact dependencies, enabling subtle vulnerabilities.In this work, we introduce MontageLie, a challenging benchmark that constructs deceptive narratives by \"montaging\" truthful statements without introducing explicit hallucinations.We demonstrate that both coarse-grained LLM-based evaluators and current fine-grained frameworks are susceptible to this attack, with AUC-ROC scores falling below 65%.To enable more robust fine-grained evaluation, we propose DoveScore, a novel framework that jointly verifies factual accuracy and event-order consistency. By modeling inter-fact relationships, DoveScore outperforms existing fine-grained methods by over 8%, providing a more robust solution for long-form text alignment evaluation. Our code and datasets are available at https://github.com/dannalily/DoveScore",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Danna Zheng",
      "Mirella Lapata",
      "Jeff Z. Pan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.559": {
    "title": "Voice of a Continent: Mapping Africa's Speech Technology Frontier",
    "volume": "main",
    "abstract": "Africa's rich linguistic diversity remains significantly underrepresented in speech technologies, creating barriers to digital inclusion. To alleviate this challenge, we systematically map the continent's speech space of datasets and technologies, leading to a new comprehensive benchmark SimbaBench for downstream African speech tasks. Using SimbaBench, we introduce the Simba family of models, achieving state-of-the-art performance across multiple African languages and speech tasks. Our benchmark analysis reveals critical patterns in resource availability, while our model evaluation demonstrates how dataset quality, domain diversity, and language family relationships influence performance across languages. Our work highlights the need for expanded speech technology resources that better reflect Africa's linguistic diversity and provides a solid foundation for future research and development efforts toward more inclusive speech technologies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "AbdelRahim A. Elmadany",
      "Sang Yun Kwon",
      "Hawau Olamide Toyin",
      "Alcides Alcoba Inciarte",
      "Hanan Aldarmaki",
      "Muhammad Abdul-Mageed"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.560": {
    "title": "Cache-Efficient Posterior Sampling for Reinforcement Learning with LLM-Derived Priors Across Discrete and Continuous Domains",
    "volume": "main",
    "abstract": "Integrating large language models (LLMs) as action proposers in reinforcement learning (RL) significantly boosts performance in text-based environments but incurs prohibitive computational costs. We introduce a cache-efficient framework for Bayesian RL that leverages LLM-derived action suggestions, drastically reducing these costs while maintaining near-optimal performance. Our approach features an adaptive caching mechanism, optimized via meta-learning based on policy performance, to enable efficient inference across text-based games (e.g., TextWorld, ALFWorld) and robotic control tasks (e.g., MuJoCo, MetaWorld). This framework achieves a 3.8×–4.7× reduction in LLM queries and 4.0×–12.0× lower median latencies (85–93ms on consumer hardware), while retaining 96–98% of the uncached policy's performance. We provide theoretical guarantees on the reliability of cached decisions with Kullback-Leibler (KL) divergence bounds, which are validated empirically by high success rates (90.4–95.6%) in complex text environments. For offline RL, our proposed CQL-Prior variant improves performance by 14–29% and reduces training time by 38–40%. Evaluations across eight diverse tasks demonstrate the framework's generalizability and practicality for resource-constrained settings, making LLM-guided RL a viable and accessible approach for both text-based and robotic applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ibne Farabi Shihab",
      "Sanjeda Akter",
      "Anuj Sharma"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.561": {
    "title": "Circuit Complexity Bounds for RoPE-based Transformer Architecture",
    "volume": "main",
    "abstract": "Characterizing the expressive power of the Transformer architecture is critical to understanding its capacity limits and scaling law. Recent works provide the circuit complexity bounds to Transformer-like architecture. On the other hand, position embedding has emerged as a crucial technique in modern large language models, offering superior performance in capturing positional information, which shows great performance for the long context scenario. In this work, we take a circuit complexity perspective and rigorously analyze Transformers augmented with widely adopted positional embeddings. We prove that, under standard complexity assumptions, such models remain incapable of efficiently solving canonical tasks such as arithmetic formula evaluation and Boolean formula value computation. Our results expose a fundamental expressivity limitation that persists despite the remarkable empirical success of positionally-enhanced Transformers. Beyond tightening known complexity bounds, our findings offer new theoretical insights for designing future architectures with provably stronger reasoning and compositional capabilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Chen",
      "Xiaoyu Li",
      "Yingyu Liang",
      "Jiangxuan Long",
      "Zhenmei Shi",
      "Zhao Song",
      "Jiahao Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.562": {
    "title": "Efficient Unstructured Pruning of Mamba State-Space Models for Resource-Constrained Environments",
    "volume": "main",
    "abstract": "As the deployment of AI models shifts towards edge devices, developing efficient sequence models has become critical. State-space models (SSMs), particularly Mamba, have emerged as strong rivals to Transformers due to their linear-time complexity and impressive performance across a range of tasks. However, their large parameter counts still hinder their use in resource-constrained environments. To address this, we propose a novel unstructured pruning framework specifically tailored for Mamba, achieving up to 70% parameter reduction with only a 3–9% drop in performance. Unlike pruning techniques designed for Transformers, our approach leverages Mamba's unique recurrent dynamics by incorporating pruning based on both weight and gradient importance to preserve critical parameters, a gradual pruning schedule to maintain model stability, and a global strategy to optimize parameter allocation across the model. Extensive experiments on the WikiText-103, Long Range Arena, and ETT benchmarks demonstrate significant efficiency gains, including 1.77× faster inference and a 46% reduction in memory usage. Our component analysis confirms Mamba's robustness to pruning, highlighting the framework's potential for enabling practical deployment while underscoring the need for careful evaluation to avoid introducing biases in sensitive applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ibne Farabi Shihab",
      "Sanjeda Akter",
      "Anuj Sharma"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.563": {
    "title": "Towards Infinite-Long Prefix in Transformer",
    "volume": "main",
    "abstract": "Prompting and context-based fine-tuning methods, which we call Prefix Learning, have been proposed to enhance the performance of language models on various downstream tasks. They are empirically efficient and effective, matching the performance of full parameter fine-tuning, but the theoretical understandings are limited. In this paper, we aim to address this limitation by studying their ability from the perspective of prefix length. In particular, we provide a convergence guarantee for training an ultra-long prefix in a stylized setting using the Neural Tangent Kernel (NTK) framework. Based on this strong theoretical guarantee, we design and implement an algorithm that only needs to introduce and fine-tune a few extra trainable parameters instead of an infinite-long prefix in each layer of a transformer, and can approximate the prefix attention to a guaranteed polynomial-small error.Preliminary experimental results on vision, natural language, and math data show that our method achieves superior or competitive performance compared to existing methods like full parameters fine-tuning, P-Tuning V2, and LoRA. This demonstrates our method is promising for parameter-efficient fine-tuning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingyu Liang",
      "Zhenmei Shi",
      "Zhao Song",
      "Chiwun Yang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.564": {
    "title": "LATTE: Learning to Think with Vision Specialists",
    "volume": "main",
    "abstract": "While open-source vision-language models perform well on simple question-answering, they still struggle with complex questions that require both perceptual and reasoning capabilities. We propose LATTE, a family of vision-language models that have LeArned to Think wiTh vision spEcialists. By offloading perception to state-of-the-art vision models, our approach enables vision-language models to focus solely on reasoning over high-quality perceptual information. To train LATTE, we synthesize and filter a large dataset of 293K multi-modal reasoning traces over perceptual outputs of vision specialists. LATTE trained on this data achieves significant 4-5% gains over baselines across 6 benchmarks covering both perception and reasoning abilities. Ablation studies reveal that the effectiveness of multi-modal reasoning traces depends on the data sources, formats, and quality of thoughts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixian Ma",
      "Jianguo Zhang",
      "Zhiwei Liu",
      "Jieyu Zhang",
      "Juntao Tan",
      "Manli Shu",
      "Juan Carlos Niebles",
      "Shelby Heinecke",
      "Huan Wang",
      "Caiming Xiong",
      "Ranjay Krishna",
      "Silvio Savarese"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.565": {
    "title": "SUA: Stealthy Multimodal Large Language Model Unlearning Attack",
    "volume": "main",
    "abstract": "Multimodal Large Language Models (MLLMs) trained on massive data may memorize sensitive personal information and photos, posing serious privacy risks. To mitigate this, MLLM unlearning methods are proposed, which fine-tune MLLMs to reduce the \"forget\" sensitive information. However, it remains unclear whether the knowledge has been truly forgotten or just hidden in the model. Therefore, we propose to study a novel problem of LLM unlearning attack, which aims to recover the unlearned knowledge of an unlearned LLM. To achieve the goal, we propose a novel framework Stealthy Unlearning Attack (SUA) framework that learns a universal noise pattern. When applied to input images, this noise can trigger the model to reveal unlearned content. While pixel-level perturbations may be visually subtle, they can be detected in the semantic embedding space, making such attacks vulnerable to potential defenses. To improve stealthiness, we introduce an embedding alignment loss that minimizes the difference between the perturbed and denoised image embeddings, ensuring the attack is semantically unnoticeable. Experimental results show that SUA can effectively recover unlearned information from MLLMs. Furthermore, the learned noise generalizes well: a single perturbation trained on a subset of samples can reveal forgotten content in unseen images. This indicates that knowledge reappearance is not an occasional failure, but a consistent behavior",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xianren Zhang",
      "Hui Liu",
      "Delvin Ce Zhang",
      "Xianfeng Tang",
      "Qi He",
      "Dongwon Lee",
      "Suhang Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.566": {
    "title": "ResFormer: All-Time Reservoir Memory for Long Sequence Classification",
    "volume": "main",
    "abstract": "Sequence classification is essential in NLP for understanding and categorizing language patterns in tasks like sentiment analysis, intent detection, and topic classification. Transformer-based models, despite achieving state-of-the-art performance, have inherent limitations due to quadratic time and memory complexity, restricting their input length. Although extensive efforts have aimed at reducing computational demands, processing extensive contexts remains challenging. To overcome these limitations, we propose ResFormer, a novel neural network architecture designed to model varying context lengths efficiently through a cascaded methodology. ResFormer integrates an reservoir computing network featuring a nonlinear readout to effectively capture long-term contextual dependencies in linear time. Concurrently, short-term dependencies within sentences are modeled using a conventional Transformer architecture with fixed-length inputs. Experiments demonstrate that ResFormer significantly outperforms baseline models of DeepSeek-Qwen and ModernBERT, delivering an accuracy improvement of up to +22.3% on the EmoryNLP dataset and consistent gains on MultiWOZ, MELD, and IEMOCAP. In addition, ResFormer exhibits reduced memory consumption, underscoring its effectiveness and efficiency in modeling extensive contextual information",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongbo Liu",
      "Jia Xu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.567": {
    "title": "Back Attention: Understanding and Enhancing Multi-Hop Reasoning in Large Language Models",
    "volume": "main",
    "abstract": "We investigate how large language models (LLMs) perform latent multi-hop reasoning in prompts like \"Wolfgang Amadeus Mozart's mother's spouse is\". To analyze this process, we introduce logit flow, an interpretability method that traces how logits propagate across layers and positions toward the final prediction. Using logit flow, we identify four distinct stages in single-hop knowledge prediction: (A) entity subject enrichment, (B) entity attribute extraction, (C) relation subject enrichment, and (D) relation attribute extraction. Extending this analysis to multi-hop reasoning, we find that failures often stem from the relation attribute extraction stage, where conflicting logits reduce prediction accuracy. To address this, we propose back attention, a novel mechanism that enables lower layers to leverage higher-layer hidden states from different positions during attention computation. With back attention, a 1-layer transformer achieves the performance of a 2-layer transformer. Applied to five LLMs, back attention improves accuracy on five reasoning datasets, demonstrating its effectiveness in enhancing latent multi-hop reasoning ability. Code and data is available at https://github.com/zepingyu0512/back-attention",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeping Yu",
      "Yonatan Belinkov",
      "Sophia Ananiadou"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.568": {
    "title": "Interdisciplinary Research in Conversation: A Case Study in Computational Morphology for Language Documentation",
    "volume": "main",
    "abstract": "Computational morphology has the potential to support language documentation through tasks like morphological segmentation and the generation of Interlinear Glossed Text (IGT). However, our research outputs have seen limited use in real-world language documentation settings. This position paper situates the disconnect between computational morphology and language documentation within a broader misalignment between research and practice in NLP and argues that the field risks becoming decontextualized and ineffectual without systematic integration of User-Centered Design (UCD). To demonstrate how principles from UCD can reshape the research agenda, we present a case study of GlossLM, a state-of-the-art multilingual IGT generation model. Through a small-scale user study with three documentary linguists, we find that despite strong metric-based performance, the system fails to meet core usability needs in real documentation contexts. These insights raise new research questions around model constraints, label standardization, segmentation, and personalization. We argue that centering users not only produces more effective tools, but surfaces richer, more relevant research directions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Enora Rice",
      "Katharina von der Wense",
      "Alexis Palmer"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.569": {
    "title": "Analyzing Uncertainty of LLM-as-a-Judge: Interval Evaluations with Conformal Prediction",
    "volume": "main",
    "abstract": "LLM-as-a-judge has become a promising paradigm for using large language models (LLMs) to evaluate natural language generation (NLG), but the uncertainty of its evaluation remains underexplored. This lack of reliability may limit its deployment in many applications. This work presents the first framework to analyze the uncertainty by offering a prediction interval of LLM-based scoring via conformal prediction. Conformal prediction constructs continuous prediction intervals from a single evaluation run, and we design an ordinal boundary adjustment for discrete rating tasks. We also suggest a midpoint-based score within the interval as a low-bias alternative to raw model score and weighted average. We perform extensive experiments and analysis, which show that conformal prediction can provide valid prediction interval with coverage guarantees. We also explore the usefulness of interval midpoint and judge reprompting for better judgment",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huanxin Sheng",
      "Xinyi Liu",
      "Hangfeng He",
      "Jieyu Zhao",
      "Jian Kang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.570": {
    "title": "AlphaOne: Reasoning Models Thinking Slow and Fast at Test Time",
    "volume": "main",
    "abstract": "This paper presents AlphaOne (𝛼1), a universal framework for modulating reasoning progress in large reasoning models (LRMs) at test time. 𝛼1 first introduces 𝛼 moment, which represents the scaled thinking phase with a universal parameter 𝛼.Within this scaled pre-𝛼 moment phase, it dynamically schedules slow thinking transitions by modeling the insertion of reasoning transition tokens as a Bernoulli stochastic process. After the 𝛼 moment, 𝛼1 deterministically terminates slow thinking with the end-of-thinking token, thereby fostering fast reasoning and efficient answer generation. This approach unifies and generalizes existing monotonic scaling methods by enabling flexible and dense slow-to-fast reasoning modulation. Extensive empirical studies on various challenging benchmarks across mathematical, coding, and scientific domains demonstrate 𝛼1‘s superior reasoning capability and efficiency. Project page: https://alphaone-project.github.io/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyu Zhang",
      "Runpei Dong",
      "Han Wang",
      "Xuying Ning",
      "Haoran Geng",
      "Peihao Li",
      "Xialin He",
      "Yutong Bai",
      "Jitendra Malik",
      "Saurabh Gupta",
      "Huan Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.571": {
    "title": "Dual-Path Dynamic Fusion with Learnable Query for Multimodal Sentiment Analysis",
    "volume": "main",
    "abstract": "Multimodal Sentiment Analysis (MSA) is the task of understanding human emotions by analyzing a combination of different data sources, such as text, audio, and visual inputs. Although recent advances have improved emotion modeling across modalities, existing methods still struggle with two fundamental challenges: balancing global and fine-grained sentiment contributions, and over-reliance on the text modality. To address these issues, we propose DPDF-LQ (Dual-Path Dynamic Fusion with Learnable Query), an architecture that processes inputs through two complementary paths: global and local. The global path is responsible for establishing cross-modal dependencies, while the local path captures fine-grained representations. Additionally, we introduce the key module Dynamic Global Learnable Query Attention (DGLQA) in the global path, which dynamically allocates weights to each modality to capture their relevant features and learn global representations. Extensive experiments on the CMU-MOSI and CMU-MOSEI benchmarks demonstrate that DPDF-LQ achieves state-of-the-art performance, particularly in fine-grained sentiment prediction by effectively combining global and local features. Our code will be released at https://github.com/ZhouMiaoGX/DPDF-LQ",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miao Zhou",
      "Lina Yang",
      "Thomas Wu",
      "Dongnan Yang",
      "Xinru Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.572": {
    "title": "CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners",
    "volume": "main",
    "abstract": "Knowledge Editing (KE) enables the modification of outdated or incorrect information in large language models (LLMs). While existing KE methods can update isolated facts, they often fail to generalize these updates to multi-hop reasoning tasks that rely on the modified knowledge. Through an analysis of reasoning circuits—the neural pathways LLMs use for knowledge-based inference, we find that current layer-localized KE approaches (e.g., MEMIT, WISE), which edit only single or a few model layers, inadequately integrate updated knowledge into these reasoning pathways. To address this limitation, we present CaKE (Circuit-aware Knowledge Editing), a novel method that enhances the effective integration of updated knowledge in LLMs. By only leveraging a few curated data samples guided by our circuit-based analysis, CaKE stimulates the model to develop appropriate reasoning circuits for newly incorporated knowledge. Experiments show that CaKE enables more accurate and consistent use of edited knowledge across related reasoning tasks, achieving an average improvement of 20% in multi-hop reasoning accuracy on the MQuAKE dataset while requiring less memory than existing KE methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunzhi Yao",
      "Jizhan Fang",
      "Jia-Chen Gu",
      "Ningyu Zhang",
      "Shumin Deng",
      "Huajun Chen",
      "Nanyun Peng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.573": {
    "title": "DEL-ToM: Inference-Time Scaling for Theory-of-Mind Reasoning via Dynamic Epistemic Logic",
    "volume": "main",
    "abstract": "Theory-of-Mind (ToM) tasks pose a unique challenge for large language models (LLMs), which often lack the capability for dynamic logical reasoning. In this work, we propose DEL-ToM, a framework that improves verifiable ToM reasoning through inference-time scaling rather than architectural changes. Our approach decomposes ToM tasks into a sequence of belief updates grounded in Dynamic Epistemic Logic (DEL), enabling structured and verifiable dynamic logical reasoning. We use data generated automatically via a DEL simulator to train a verifier, which we call the Process Belief Model (PBM), to score each belief update step. During inference, the PBM evaluates candidate belief traces from the LLM and selects the highest-scoring one. This allows LLMs to allocate extra inference-time compute to yield more transparent reasoning. Experiments across model scales and benchmarks show that DEL-ToM consistently improves performance, demonstrating that verifiable belief supervision significantly enhances LLMs' ToM capabilities without retraining. Code is available at https://github.com/joel-wu/DEL-ToM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuheng Wu",
      "Jianwen Xie",
      "Denghui Zhang",
      "Zhaozhuo Xu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.574": {
    "title": "Collaborative Beam Search: Enhancing LLM Reasoning via Collective Consensus",
    "volume": "main",
    "abstract": "Complex multi-step reasoning remains challenging for large language models (LLMs). While parallel inference-time scaling methods, such as step-level beam search, offer a promising solution, existing approaches typically depend on either domain-specific external verifiers, or self-evaluation which is brittle and prompt-sensitive. To address these issues, we propose Collaborative Beam Search (CBS), an iterative framework that harnesses the collective intelligence of multiple LLMs across both generation and verification stages. For generation, CBS leverages multiple LLMs to explore a broader search space, resulting in more diverse candidate steps. For verifications, CBS employs a perplexity-based collective consensus among these models, eliminating reliance on an external verifier or complex prompts. Between iterations, CBS leverages a dynamic quota allocation strategy that reassigns generation budget based on each model's past performance, striking a balance between candidate diversity and quality. Experimental results on six tasks across arithmetic, logical, and commonsense reasoning show that CBS outperforms single‐model scaling and multi-model ensemble baselines by over 4 percentage points in average accuracy, demonstrating its effectiveness and general applicability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangyifan Xu",
      "Shuo Ren",
      "Jiajun Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.575": {
    "title": "Deriving Strategic Market Insights with Large Language Models: A Benchmark for Forward Counterfactual Generation",
    "volume": "main",
    "abstract": "Counterfactual reasoning typically involves considering alternatives to actual events. While often applied to understand past events, a distinct form—forward counterfactual reasoning—focuses on anticipating plausible future developments. This type of reasoning is invaluable in dynamic financial markets, where anticipating market developments can powerfully unveil potential risks and opportunities for stakeholders, guiding their decision-making. However, performing this at scale is challenging due to the cognitive demands involved, underscoring the need for automated solutions. Large Language Models (LLMs) offer promise, but remain unexplored for this application. To address this gap, we introduce a novel benchmark, Fin-Force—**FIN**ancial **FOR**ward **C**ounterfactual **E**valuation. By curating financial news headlines and providing structured evaluation, Fin-Force supports LLM based forward counterfactual generation. This paves the way for scalable and automated solutions for exploring and anticipating future market developments, thereby providing structured insights for decision-making. Through experiments on Fin-Force, we evaluate state-of-the-art LLMs and counterfactual generation methods, analyzing their limitations and proposing insights for future research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keane Ong",
      "Rui Mao",
      "Deeksha Varshney",
      "Paul Pu Liang",
      "Erik Cambria",
      "Gianmarco Mengaldo"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.576": {
    "title": "Towards Statistical Factuality Guarantee for Large Vision-Language Models",
    "volume": "main",
    "abstract": "Advancements in Large Vision-Language Models (LVLMs) have demonstrated impressive performance in image-conditioned text generation; however, hallucinated outputs–text that misaligns with the visual input–pose a major barrier to their use in safety-critical applications. We introduce ConfLVLM, a conformal-prediction-based framework that achieves finite-sample distribution-free statistical guarantees to the factuality of LVLM output. Taking each generated detail as a hypothesis, ConfLVLM statistically tests factuality via efficient heuristic uncertainty measures to filter out unreliable claims. We conduct extensive experiments covering three representative application domains: general scene understanding, medical radiology report generation, and document understanding. Remarkably, ConfLVLM reduces the error rate of claims generated by LLaVa-1.5 for scene descriptions from 87.8% to 10.0% by filtering out erroneous claims with a 95.3% true positive rate. Our results further show that ConfLVLM is highly flexible, and can be applied to any black-box LVLMs paired with any uncertainty measure for any image-conditioned free-form text generation task while providing a rigorous guarantee on controlling hallucination risk",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuohang Li",
      "Chao Yan",
      "Nicholas J Jackson",
      "Wendi Cui",
      "Bo Li",
      "Jiaxin Zhang",
      "Bradley A. Malin"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.577": {
    "title": "Unlearning vs. Obfuscation: Are We Truly Removing Knowledge?",
    "volume": "main",
    "abstract": "Unlearning has emerged as a critical capability for large language models (LLMs) to support data privacy, regulatory compliance, and ethical AI deployment. Recent techniques often rely on obfuscation by injecting incorrect or irrelevant information to suppress knowledge. Such methods effectively constitute knowledge addition rather than true removal, often leaving models vulnerable to probing. In this paper, we formally distinguish unlearning from obfuscation and introduce a probing-based evaluation framework to assess whether existing approaches genuinely remove targeted information. Moreover, we propose DF-MCQ, a novel unlearning method that flattens the model predictive distribution over automatically generated multiple-choice questions using KL-divergence, effectively removing knowledge about target individuals and triggering appropriate refusal behaviour. Experimental results demonstrate that DF-MCQ achieves unlearning with over 90% refusal rate and a random choice-level uncertainty that is much higher than obfuscation on probing questions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangzhi Sun",
      "Potsawee Manakul",
      "Xiao Zhan",
      "Mark Gales"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.578": {
    "title": "Reward-Shifted Speculative Sampling Is An Efficient Test-Time Weak-to-Strong Aligner",
    "volume": "main",
    "abstract": "Aligning large language models (LLMs) with human preferences has become a critical step in their development. Recent research has increasingly focused on test-time alignment, where additional compute is allocated during inference to enhance LLM safety and reasoning capabilities. However, these test-time alignment techniques often incur substantial inference costs, limiting their practical application. We are inspired by the speculative sampling acceleration, which leverages a small draft model to efficiently predict future tokens, to address the efficiency bottleneck of test-time alignment. We introduce the reward-shifted speculative sampling (SSS) algorithm, in which the draft model is aligned with human preferences, while the target model remains unchanged. We theoretically demonstrate that the distributional shift between the aligned draft model and the unaligned target model can be exploited to recover the RLHF optimal solution without actually obtaining it, by modifying the acceptance criterion and bonus token distribution. Our algorithm achieves superior gold reward scores at a significantly reduced inference cost in test-time weak-to-strong alignment experiments, thereby validating both its effectiveness and efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bolian Li",
      "Yanran Wu",
      "Xinyu Luo",
      "Ruqi Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.579": {
    "title": "Stimulate the Critical Thinking of LLMs via Debiasing Discussion",
    "volume": "main",
    "abstract": "Large language models (LLMs) often succumb to users' viewpoints when faced with conflicting perspectives. We identify two key biases underlying this issue : stance homogeneity bias and human preference bias. To address these biases, we propose a novel two-stage training framework: Multi-stance Discussion Sampling and Truth Alignment Training (MDTA). First, we introduce an equal multi-stance discussion framework to automatically generate multi-model discussion datasets. Based on this framework, we construct the first and largest multi-model fair discussion dataset named Eq-Discussion for supervised fine-tuning, reducing stance homogeneity bias. Second, we optimize Reinforcement Learning from Human Feedback (RLHF) to align with discussion correctness, mitigating human preference bias. Extensive experimental results demonstrate that MDTA effectively reduces both biases and significantly enhances the performance of LLMs across a variety of downstream tasks, including reading comprehension, logical reasoning, and social question answering. Furthermore, we observe that MDTA improves the generalization capabilities of LLMs, leading to substantial performance improvements in non-discussion scenarios and on out-of-domain datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiyu Xiao",
      "Lei Wu",
      "Yuanxing Liu",
      "Weinan Zhang",
      "Ting Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.580": {
    "title": "Toward Multi-Session Personalized Conversation: A Large-Scale Dataset and Hierarchical Tree Framework for Implicit Reasoning",
    "volume": "main",
    "abstract": "There has been a surge in the use of large language models (LLM) conversational agents to generate responses based on long-term history from multiple sessions. However, existing long-term open-domain dialogue datasets lack complex, real-world personalization and fail to capture implicit reasoning—where relevant information is embedded in subtle, syntactic, or semantically distant connections rather than explicit statements. In such cases, traditional retrieval methods fail to capture relevant context, and long-context modeling also becomes inefficient due to numerous complicated persona-related details. To address this gap, we introduce ImplexConv, a large-scale long-term dataset with 2,500 examples, each containing approximately 100 conversation sessions, designed to study implicit reasoning in personalized dialogues. Additionally, we propose TaciTree, a novel hierarchical tree framework that structures conversation history into multiple levels of summarization. Instead of brute-force searching all data, TaciTree enables an efficient, level-based retrieval process where models refine their search by progressively selecting relevant details. Our experiments demonstrate that TaciTree significantly improves the ability of LLMs to reason over long-term conversations with implicit contextual dependencies",
    "checked": true,
    "id": "800d0e03ed4d8dd9e182e4d389733d84261a47ce",
    "semantic_title": "toward multi-session personalized conversation: a large-scale dataset and hierarchical tree framework for implicit reasoning",
    "citation_count": 3,
    "authors": [
      "Xintong Li",
      "Jalend Bantupalli",
      "Ria Dharmani",
      "Yuwei Zhang",
      "Jingbo Shang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.581": {
    "title": "Improving Instruct Models for Free: A Study on Partial Adaptation",
    "volume": "main",
    "abstract": "Instruct models, obtained from various instruction tuning or post-training steps, are commonly deemed superior and more usable than their base counterpart. While the model gains instruction following ability, instruction tun- ing may lead to forgetting the knowledge from pre-training or it may encourage the model being overly conversational or verbose. This, in turn, can lead to degradation of in-context few-shot learning performance. In this work, we study the performance trajectory between base and instruct models by scaling down the strength of instruction-tuning via the partial adaption method. We show that, across several model families and model sizes, reducing the strength of instruction-tuning results in material improvement on a few-shot in-context learning benchmark covering a variety of classic natural language tasks. This comes at the cost of losing some degree of instruction following ability as measured by AlpacaEval. Our study shines light on the potential trade-off between in-context learning and instruction following abilities that is worth considering in practice",
    "checked": true,
    "id": "4f93077585e492bc52844504fc9249388e16e11f",
    "semantic_title": "improving instruct models for free: a study on partial adaptation",
    "citation_count": 0,
    "authors": [
      "Ozan Irsoy",
      "Pengxiang Cheng",
      "Jennifer L Chen",
      "Daniel Preotiuc-Pietro",
      "Shiyue Zhang",
      "Duccio Pappadopulo"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.582": {
    "title": "CoMMIT: Coordinated Multimodal Instruction Tuning",
    "volume": "main",
    "abstract": "Instruction tuning in multimodal large language models (MLLMs) generally involves cooperative learning between a backbone LLM and a feature encoder of non-text input modalities. The major challenge is how to efficiently find the synergy between the two modules so that LLMs can adapt their reasoning abilities to downstream tasks while feature encoders can adjust to provide more task-specific information about its modality. In this paper, we analyze the MLLM instruction tuning from both theoretical and empirical perspectives, where we find the unbalanced learning between the feature encoder and the LLM can cause problems of oscillation and biased learning that lead to sub-optimal convergence. Inspired by our findings, we propose a Multimodal Balance Coefficient that enables quantitative measurement of the balance of learning. Based on this, we further design a dynamic learning scheduler that better coordinates the learning between the LLM and feature encoder, alleviating the problems of oscillation and biased learning. In addition, we introduce an auxiliary regularization on the gradient to promote updating with larger step sizes, which potentially allows for a more accurate estimation of the proposed MultiModal Balance Coefficient and further improves the training sufficiency. Our proposed approach is agnostic to the architecture of LLM and feature encoder, so it can be generically integrated with various MLLMs. We conduct experiments on multiple downstream tasks with various MLLMs, demonstrating that the proposed method is more effective than the baselines in MLLM instruction tuning",
    "checked": true,
    "id": "780827b3c1fb105b23476c0833838d4a633ae4a1",
    "semantic_title": "commit: coordinated multimodal instruction tuning",
    "citation_count": 4,
    "authors": [
      "Xintong Li",
      "Junda Wu",
      "Tong Yu",
      "Rui Wang",
      "Yu Wang",
      "Xiang Chen",
      "Jiuxiang Gu",
      "Lina Yao",
      "Julian McAuley",
      "Jingbo Shang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.583": {
    "title": "Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) are rapidly surpassing human knowledge in many domains. While improving these models traditionally relies on costly human data, recent self-rewarding mechanisms have shown that LLMs can improve by judging their own responses instead of relying on human labelers. However, existing methods have primarily focused on improving model responses rather than judgment capabilities, resulting in rapid saturation during iterative training. To address this issue, we introduce a novel Meta-Rewarding step to the self-improvement process, where the model judges its own judgements and uses that feedback to refine its judgment skills. Surprisingly, this unsupervised approach improves the model's ability to judge and follow instructions, as demonstrated by a win rate improvement of Llama-3-8B-Instruct from 22.9% to 39.4% on AlpacaEval 2, and 20.6% to 29.1% on Arena-Hard. These results strongly suggest the potential for self-improving models without human supervision",
    "checked": true,
    "id": "df90ee11ed6378635f22e6d0061cf67dd0bacd13",
    "semantic_title": "meta-rewarding language models: self-improving alignment with llm-as-a-meta-judge",
    "citation_count": 141,
    "authors": [
      "Tianhao Wu",
      "Weizhe Yuan",
      "Olga Golovneva",
      "Jing Xu",
      "Yuandong Tian",
      "Jiantao Jiao",
      "Jason E Weston",
      "Sainbayar Sukhbaatar"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.584": {
    "title": "AnyMAC: Cascading Flexible Multi-Agent Collaboration via Next-Agent Prediction",
    "volume": "main",
    "abstract": "Recent progress in large language model (LLM)-based multi-agent collaboration highlights the power of structured communication in enabling collective intelligence. However, existing methods largely rely on static or graph-based inter-agent topologies, lacking the potential adaptability and flexibility in communication. In this work, we propose a new framework that rethinks multi-agent coordination through a sequential structure rather than a graph structure, offering a significantly larger topology space for multi-agent communication. Our method focuses on two key directions: (1) Next-Agent Prediction, which selects the most suitable agent role at each step, and (2) Next-Context Selection (NCS), which enables each agent to selectively access relevant information from any previous step. Together, these components construct task-adaptive communication pipelines that support both role flexibility and global information flow. Extensive evaluations across multiple benchmarks demonstrate that our approach achieves superior performance while substantially reducing communication overhead",
    "checked": true,
    "id": "0af3aff3e0781dc32a706b722d4e1430702913f2",
    "semantic_title": "anymac: cascading flexible multi-agent collaboration via next-agent prediction",
    "citation_count": 5,
    "authors": [
      "Song Wang",
      "Zhen Tan",
      "Zihan Chen",
      "Shuang Zhou",
      "Tianlong Chen",
      "Jundong Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.585": {
    "title": "A Good Plan is Hard to Find: Aligning Models with Preferences is Misaligned with What Helps Users",
    "volume": "main",
    "abstract": "To assist users in complex tasks, LLMs generate plans: step-by-step instructions towards a goal. While alignment methods aim to ensure LLM plans are helpful, they train (RLHF) or evaluate (ChatbotArena) on what users prefer, assuming this reflects what helps them. We test this with Planorama: an interface where 126 users answer 300 multi-step questions with LLM plans. We get 4388 plan executions and 5584 comparisons to measure plan helpfulness (QA success) and user preferences on plans, and recreate the setup in agents and reward models to see if they simulate or prefer what helps users. We expose: 1) user/model preferences and agent success do not accurately predict which plans help users, so common alignment feedback can misalign with helpfulness; 2) this gap is not due to user-specific preferences, as users are similarly successful when using plans they prefer/disprefer; 3) surface-level cues like brevity and question similarity strongly link to preferences, but such biases fail to predict helpfulness. In all, we argue aligning helpful LLMs needs feedback from real user interactions—not just preferences of what looks helpful—so we discuss the plan NLP researchers can execute to solve this problem",
    "checked": true,
    "id": "dac3786fcf23a447babf1f20aa5b257792dbb3a4",
    "semantic_title": "a good plan is hard to find: aligning models with preferences is misaligned with what helps users",
    "citation_count": 0,
    "authors": [
      "Nishant Balepur",
      "Matthew Shu",
      "Yoo Yeon Sung",
      "Seraphina Goldfarb-Tarrant",
      "Shi Feng",
      "Fumeng Yang",
      "Rachel Rudinger",
      "Jordan Lee Boyd-Graber"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.586": {
    "title": "Words Like Knives: Backstory-Personalized Modeling and Detection of Violent Communication",
    "volume": "main",
    "abstract": "Conversational breakdowns in close relationships are deeply shaped by personal histories and emotional context, yet most NLP research treats conflict detection as a general task, overlooking the relational dynamics that influence how messages are perceived. In this work, we leverage nonviolent communication (NVC) theory to evaluate LLMs in detecting conversational breakdowns and assessing how relationship backstory influences both human and model perception of conflicts. Given the sensitivity and scarcity of real-world datasets featuring conflict between familiar social partners with rich personal backstories, we contribute the PersonaConflicts Corpus, a dataset of N=5,772 naturalistic simulated dialogues spanning diverse conflict scenarios between friends, family members, and romantic partners. Through a controlled human study, we annotate a subset of dialogues and obtain fine-grained labels of communication breakdown types on individual turns, and assess the impact of backstory on human and model perception of conflict in conversation. We find that the polarity of relationship backstories significantly shifted human perception of communication breakdowns and impressions of the social partners, yet models struggle to meaningfully leverage those backstories in the detection task. Additionally, we find that models consistently overestimate how positively a message will make a listener feel. Our findings underscore the critical role of personalization to relationship contexts in enabling LLMs to serve as effective mediators in human communication for authentic connection",
    "checked": true,
    "id": "25c80fc581d085429dfe17e88d7b97f0c8e077c6",
    "semantic_title": "words like knives: backstory-personalized modeling and detection of violent communication",
    "citation_count": 0,
    "authors": [
      "Jocelyn J Shen",
      "Akhila Yerukola",
      "Xuhui Zhou",
      "Cynthia Breazeal",
      "Maarten Sap",
      "Hae Won Park"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.587": {
    "title": "Separate the Wheat from the Chaff: Winnowing Down Divergent Views in Retrieval Augmented Generation",
    "volume": "main",
    "abstract": "Retrieval-augmented generation (RAG) addresses the limitation of large language models (LLMs) in achieving up-to-date information by integrating external knowledge sources, but it is hindered by noisy or irrelevant retrieved data, leading to reduced accuracy. Additionally, most RAG methods rely on task-specific supervision, reducing their adaptability across domains. To overcome these challenges, we propose WinnowRAG, a novel multi-agent debate-based RAG framework. WinnowRAG operates in two stages: in Stage I, query-aware clustering groups similar documents, with each cluster assigned to an LLM agent for generating personalized responses. A critic LLM then consolidates these answers, forming super-agents. In Stage II, the super-agents engage in a structured discussion to filter out incorrect or irrelevant information, ensuring only relevant knowledge is used for final response generation. Crucially, WinnowRAG is unsupervised and leverages pretrained LLMs without requiring fine-tuning, making it easily adaptable to various tasks. The experiments on various realistic datasets demonstrate the effectiveness of WinnowRAG over state-of-the-art baselines",
    "checked": true,
    "id": "f172884521714798aba7e4fa1599525afad08cc9",
    "semantic_title": "separate the wheat from the chaff: winnowing down divergent views in retrieval augmented generation",
    "citation_count": 1,
    "authors": [
      "Song Wang",
      "Zihan Chen",
      "Peng Wang",
      "Zhepei Wei",
      "Zhen Tan",
      "Yu Meng",
      "Cong Shen",
      "Jundong Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.588": {
    "title": "Cognitive Linguistic Identity Fusion Score (CLIFS): A Scalable Cognition‐Informed Approach to Quantifying Identity Fusion from Text",
    "volume": "main",
    "abstract": "Quantifying *identity fusion*—the psychological merging of self with another entity or abstract target (e.g., a religious group, political party, ideology, value, brand, belief, etc.)—is vital for understanding a wide range of group‐based human behaviors. We introduce the Cognitive Linguistic Identity Fusion Score ([CLIFS](https://github.com/DevinW-sudo/CLIFS)), a novel metric that integrates cognitive linguistics with large language models (LLMs), which builds on implicit metaphor detection. Unlike traditional pictorial and verbal scales, which require controlled surveys or direct field contact, CLIFS delivers fully automated, scalable assessments while maintaining strong alignment with the established verbal measure. In benchmarks, CLIFS outperforms both existing automated approaches and human annotation. As a proof of concept, we apply CLIFS to violence risk assessment to demonstrate that it can improve violence risk assessment by more than 240%. Building on our identification of a new NLP task and early success, we underscore the need to develop larger, more diverse datasets that encompass additional fusion-target domains and cultural backgrounds to enhance generalizability and further advance this emerging area. CLIFS models and code are public at [https://github.com/DevinW-sudo/CLIFS](https://github.com/DevinW-sudo/CLIFS)",
    "checked": false,
    "id": "e158849b3d253c5be0a79d60c524c86ee139c21b",
    "semantic_title": "cognitive linguistic identity fusion score (clifs): a scalable cognition-informed approach to quantifying identity fusion from text",
    "citation_count": 0,
    "authors": [
      "Devin R. Wright",
      "Jisun An",
      "Yong-Yeol Ahn"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.589": {
    "title": "SilVar: Speech-Driven Multimodal Model for Reasoning Visual Question Answering and Object Localization",
    "volume": "main",
    "abstract": "Visual Language Models have demonstrated remarkable capabilities across various tasks, including visual question answering and image captioning. However, most models rely on text-based instructions, limiting their effectiveness in natural human-machine interactions. Moreover, the quality of language models primarily depends on reasoning and prompting techniques, such as chain-of-thought, which remain underexplored when using speech instructions. To address these challenges, we propose SilVar, an end-to-end multimodal model that leverages speech instructions for reasoning-based visual question answering. Additionally, we investigate reasoning techniques at different levels, including conversational, simple, and complex speech instructions. SilVar is built upon CLIP, Whisper, and LLaMA 3.1-8B, enabling more intuitive interactions by allowing users to provide verbal or text-based instructions. To this end, we introduce a new dataset designed to challenge models with speech-based reasoning tasks for object localization. This dataset enhances the model's ability to process and explain visual scenes from spoken input, moving beyond simple object recognition to reasoning-based interactions. To our knowledge, SilVar is the first open-source, speech-driven VLM. We believe SilVar will inspire the next generation of multimodal reasoning models, advancing toward expert artificial general intelligence",
    "checked": false,
    "id": "7870df383916f80ed14015da13162aaf306aaa91",
    "semantic_title": "silvar: speech driven multimodal model for reasoning visual question answering and object localization",
    "citation_count": 1,
    "authors": [
      "Tan-Hanh Pham",
      "Le Hoang Nam",
      "Phu-Vinh Nguyen",
      "Chris Ngo",
      "Truong-Son Hy"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.590": {
    "title": "CEMTM: Contextual Embedding-based Multimodal Topic Modeling",
    "volume": "main",
    "abstract": "We introduce CEMTM, a context-enhanced multimodal topic model designed to infer coherent and interpretable topic structures from both short and long documents containing text and images. CEMTM builds on fine-tuned large vision language models (LVLMs) to obtain contextualized embeddings, and employs a distributional attention mechanism to weight token-level contributions to topic inference. A reconstruction objective aligns topic-based representations with the document embedding, encouraging semantic consistency across modalities. Unlike existing approaches, CEMTM can process multiple images per document without repeated encoding and maintains interpretability through explicit word-topic and document-topic distributions. Extensive experiments on six multimodal benchmarks show that CEMTM consistently outperforms unimodal and multimodal baselines, achieving a remarkable average LLM score of 2.61. Further analysis shows its effectiveness in downstream few-shot retrieval and its ability to capture visually grounded semantics in complex domains such as scientific articles",
    "checked": true,
    "id": "7ad8d9c1e18ddf519d9c5a99a82c41f9b52211d5",
    "semantic_title": "cemtm: contextual embedding-based multimodal topic modeling",
    "citation_count": 0,
    "authors": [
      "Amirhossein Abaskohi",
      "Raymond Li",
      "Chuyuan Li",
      "Shafiq Joty",
      "Giuseppe Carenini"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.591": {
    "title": "RedHerring Attack: Testing the Reliability of Attack Detection",
    "volume": "main",
    "abstract": "In response to adversarial text attacks, attack detection models have been proposed and shown to successfully identify text modified by adversaries. Attack detection models can be leveraged to provide an additional check for NLP models and give signals for human input. However, the reliability of these models has not yet been thoroughly explored. Thus, we propose and test a novel attack setting and attack, RedHerring. RedHerring aims to make attack detection models unreliable by modifying a text to cause the detection model to predict an attack, while keeping the classifier correct. This creates a tension between the classifier and detector. If a human sees that the detector is giving an \"incorrect\" prediction, but the classifier a correct one, then the human will see the detector as unreliable. We test this novel threat model on 4 datasets against 3 detectors defending 4 classifiers. We find that RedHerring is able to drop detection accuracy between 20 - 71 points, while maintaining (or improving) classifier accuracy. As an initial defense, we propose a simple confidence check which requires no retraining of the classifier or detector and increases detection accuracy greatly. This novel threat model offers new insights into how adversaries may target detection models",
    "checked": true,
    "id": "541fa986626065df19b42475296d77c08d66181c",
    "semantic_title": "redherring attack: testing the reliability of attack detection",
    "citation_count": 0,
    "authors": [
      "Jonathan Rusert"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.592": {
    "title": "Modeling Bottom-up Information Quality during Language Processing",
    "volume": "main",
    "abstract": "Contemporary theories model language processing as integrating both top-down expectations and bottom-up inputs. One major prediction of such models is that the quality of the bottom-up inputs modulates ease of processing—noisy inputs should lead to difficult and effortful comprehension. We test this prediction in the domain of reading. First, we propose an information-theoretic operationalization for the \"quality\" of bottom-up information as the mutual information (MI) between visual information and word identity. We formalize this prediction in a mathematical model of reading as a Bayesian update. Second, we test our operationalization by comparing participants' reading times in conditions where words' information quality has been reduced, either by occluding their top or bottom half, with full words. We collect data in English and Chinese. We then use multimodal language models to estimate the mutual information between visual inputs and words. We use these data to estimate the specific effect of reduced information quality on reading times. Finally, we compare how information is distributed across visual forms. In English and Chinese, the upper half contains more information about word identity than the lower half. However, the asymmetry is more pronounced in English, a pattern which is reflected in the reading times",
    "checked": true,
    "id": "81863ad203f4def05ca2eea93a02c3b16ef698ab",
    "semantic_title": "modeling bottom-up information quality during language processing",
    "citation_count": 0,
    "authors": [
      "Cui Ding",
      "Yanning Yin",
      "Lena Ann Jäger",
      "Ethan Wilcox"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.593": {
    "title": "Data Drives Unstable Hierarchical Generalization in LMs",
    "volume": "main",
    "abstract": "Early in training, LMs can behave like n-gram models, but eventually, they often learn tree-based syntactic rules and generalize hierarchically out of distribution (OOD). We study this shift using controlled grammar-learning tasks: question formation and tense inflection. We find a model learns to generalize hierarchically if its training data is *complex*–in particular, if it includes center-embedded clauses, a special syntactic structure. Under this definition, complex data drives hierarchical rules, while less complex data encourages shortcut learning in the form of n-gram-like linear rules. Furthermore, we find that a model uses rules to generalize, whether hierarchical or linear, if its training data is *diverse*–in particular, if it includes many distinct syntax trees in the training set. Under this definition, diverse data promotes stable rule learning, whereas less diverse data promotes memorization of individual syntactic sequences. Finally, intermediate diversity and intermediate complexity form an *unstable regime*, which is characterized by oscillatory learning dynamics and inconsistent behaviors across random seeds. These results highlight the central role of training data in shaping generalization and explain why competing strategies can lead to unstable outcomes",
    "checked": true,
    "id": "3f97155d3db634f08e130c85bd207e334ee6daf1",
    "semantic_title": "data drives unstable hierarchical generalization in lms",
    "citation_count": 0,
    "authors": [
      "Tian Qin",
      "Naomi Saphra",
      "David Alvarez-Melis"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.594": {
    "title": "EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety",
    "volume": "main",
    "abstract": "The rise of LLM-driven AI characters raises safety concerns, particularly for vulnerable human users with psychological disorders. To address these risks, we propose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate mental health hazards in human-AI interactions. EmoAgent comprises two components: **EmoEval** simulates virtual users, including those portraying mentally vulnerable individuals, to assess mental health changes before and after interactions with AI characters. It uses clinically proven psychological and psychiatric assessment tools (PHQ-9, PDI, PANSS) to evaluate mental risks induced by LLM. **EmoGuard** serves as an intermediary, monitoring users' mental status, predicting potential harm, and providing corrective feedback to mitigate risks. Experiments conducted in popular character-based chatbots show that emotionally engaging dialogues can lead to psychological deterioration in vulnerable users, with mental state deterioration in more than 34.4% of the simulations. EmoGuard significantly reduces these deterioration rates, underscoring its role in ensuring safer AI-human interactions",
    "checked": true,
    "id": "110ab0beb74ffb7ab1efe55ad36b4732835fa5c9",
    "semantic_title": "emoagent: assessing and safeguarding human-ai interaction for mental health safety",
    "citation_count": 8,
    "authors": [
      "Jiahao Qiu",
      "Yinghui He",
      "Xinzhe Juan",
      "Yimin Wang",
      "Yuhan Liu",
      "Zixin Yao",
      "Yue Wu",
      "Xun Jiang",
      "Ling Yang",
      "Mengdi Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.595": {
    "title": "Polysemantic Dropout: Conformal OOD Detection for Specialized LLMs",
    "volume": "main",
    "abstract": "We propose a novel inference-time out-of-domain (OOD) detection algorithm for specialized large language models (LLMs). Despite achieving state-of-the-art performance on in-domain tasks through fine-tuning, specialized LLMs remain vulnerable to incorrect or unreliable outputs when presented with OOD inputs, posing risks in critical applications. Our method leverages the Inductive Conformal Anomaly Detection (ICAD) framework, using a new non-conformity measure based on the model's dropout tolerance. Motivated by recent findings on polysemanticity and redundancy in LLMs, we hypothesize that in-domain inputs exhibit higher dropout tolerance than OOD inputs. We aggregate dropout tolerance across multiple layers via a valid ensemble approach, improving detection while maintaining theoretical false alarm bounds from ICAD. Experiments with medical-specialized LLMs show that our approach detects OOD inputs better than baseline methods, with AUROC improvements of 2% to 37% when treating OOD datapoints as positives and in-domain test datapoints as negatives",
    "checked": true,
    "id": "bf9ab2f5a067bccb4c3adc09d458fb16e7f3a272",
    "semantic_title": "polysemantic dropout: conformal ood detection for specialized llms",
    "citation_count": 0,
    "authors": [
      "Ayush Gupta",
      "Ramneet Kaur",
      "Anirban Roy",
      "Adam D. Cobb",
      "Rama Chellappa",
      "Susmit Jha"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.596": {
    "title": "Facilitating Cognitive Accessibility with LLMs: A Multi-Task Approach to Easy-to-Read Text Generation",
    "volume": "main",
    "abstract": "Simplifying complex texts is essential to ensure equitable access to information, particularly for individuals with cognitive impairments. The Easy-to-Read (ETR) initiative provides a framework to make content more accessible for these individuals. However, manually creating such texts remains time-consuming and resource-intensive. In this work, we investigate the potential of large language models (LLMs) to automate the generation of ETR content. To address the scarcity of aligned corpora and the specific constraints of ETR, we propose a multi-task learning (MTL) approach that trains models jointly on text summarization, text simplification, and ETR generation. We explore two complementary strategies: multi-task retrieval-augmented generation (RAG) for in-context learning (ICL), and MTL-LoRA for parameter-efficient fine-tuning (PEFT). Our experiments with Mistral-7B and LLaMA-3-8B, conducted on ETR-fr, a new high-quality dataset, show that MTL-LoRA consistently outperforms all other strategies in in-domain settings, while the MTL-RAG-based approach achieves better generalization in out-of-domain scenarios. Our code is publicly available at https://github.com/FrLdy/ETR-PEFT-Composition",
    "checked": true,
    "id": "715b70660d691708a6540aedde5c322d772dbe90",
    "semantic_title": "facilitating cognitive accessibility with llms: a multi-task approach to easy-to-read text generation",
    "citation_count": 0,
    "authors": [
      "François Ledoyen",
      "Gaël Dias",
      "Jeremie Pantin",
      "Alexis Lechervy",
      "Fabrice Maurel",
      "Youssef Chahir"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.597": {
    "title": "D-CoDe: Scaling Image-Pretrained VLMs to Video via Dynamic Compression and Question Decomposition",
    "volume": "main",
    "abstract": "Video large language models (Vid-LLMs), which excel in diverse video-language tasks, can be effectively constructed by adapting image-pretrained vision-language models (VLMs). However, this adaptation remains challenging, as it requires processing dense and temporally extended visual inputs that exceed the capacity of image-based models. This paper identifies the perception bottleneck and token overload as key challenges in extending image-based VLMs to the video domain. To address these issues, we propose D-CoDe, a training-free adaptation framework that incorporates dynamic compression and question decomposition. Specifically, dynamic compression alleviates the perception bottleneck through adaptive selection of representative frames and content-aware aggregation of spatial tokens, thereby reducing redundancy while preserving informative content. In parallel, question decomposition mitigates token overload by reformulating the original query into sub-questions, guiding the model to focus on distinct aspects of the video and enabling more comprehensive understanding. Experiments demonstrate that D-CoDe effectively improves video understanding across various benchmarks. Furthermore, strong performance on the challenging long-video benchmark highlights the potential of D-CoDe in handling complex video-language tasks. Code is available at https://github.com/hukcc/D-CoDe",
    "checked": true,
    "id": "72f8365eb7dd58dbf6c0d755815406475d29b71f",
    "semantic_title": "d-code: scaling image-pretrained vlms to video via dynamic compression and question decomposition",
    "citation_count": 0,
    "authors": [
      "Yiyang Huang",
      "Yizhou Wang",
      "Yun Fu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.598": {
    "title": "ReEvalMed: Rethinking Medical Report Evaluation by Aligning Metrics with Real-World Clinical Judgment",
    "volume": "main",
    "abstract": "Automatically generated radiology reports often receive high scores from existing evaluation metrics but fail to earn clinicians' trust. This gap reveals fundamental flaws in how current metrics assess the quality of generated reports. We rethink the design and evaluation of these metrics and propose a clinically grounded Meta-Evaluation framework. We define clinically grounded criteria spanning clinical alignment and key metric capabilities, including discrimination, robustness, and monotonicity. Using a fine-grained dataset of ground truth and rewritten report pairs annotated with error types, clinical significance labels, and explanations, we systematically evaluate existing metrics and reveal their limitations in interpreting clinical semantics, such as failing to distinguish clinically significant errors, over-penalizing harmless variations, and lacking consistency across error severity levels. Our framework offers guidance for building more clinically reliable evaluation methods",
    "checked": true,
    "id": "2198a46ff2110ae50aac4a8428642823188d260c",
    "semantic_title": "reevalmed: rethinking medical report evaluation by aligning metrics with real-world clinical judgment",
    "citation_count": 1,
    "authors": [
      "Ruochen Li",
      "Jun Li",
      "Bailiang Jian",
      "Kun Yuan",
      "Youxiang Zhu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.599": {
    "title": "MultiMed-ST: Large-scale Many-to-many Multilingual Medical Speech Translation",
    "volume": "main",
    "abstract": "Multilingual speech translation (ST) and machine translation (MT) in the medical domain enhances patient care by enabling efficient communication across language barriers, alleviating specialized workforce shortages, and facilitating improved diagnosis and treatment, particularly during pandemics. In this work, we present the first systematic study on medical ST, to our best knowledge, by releasing MultiMedST, a large-scale ST dataset for the medical domain, spanning all translation directions in five languages: Vietnamese, English, German, French, and Simplified/Traditional Chinese, together with the models. With 290,000 samples, this is the largest medical MT dataset and the largest many-to-many multilingual ST among all domains. Secondly, we present the most comprehensive ST analysis in the field's history, to our best knowledge, including: empirical baselines, bilingual-multilingual comparative study, end-to-end vs. cascaded comparative study, task-specific vs. multi-task sequence-to-sequence comparative study, code-switch analysis, and quantitative-qualitative error analysis. All code, data, and models are available online: https://github.com/leduckhai/MultiMed-ST",
    "checked": true,
    "id": "d3e6ae1a390b97f678c906cb6b964e065e20e815",
    "semantic_title": "multimed-st: large-scale many-to-many multilingual medical speech translation",
    "citation_count": 1,
    "authors": [
      "Khai Le-Duc",
      "Tuyen Tran",
      "Bach Phan Tat",
      "Nguyen Kim Hai Bui",
      "Quan Dang Anh",
      "Hung-Phong Tran",
      "Thanh Thuy Nguyen",
      "Ly Nguyen",
      "Tuan Minh Phan",
      "Thi Thu Phuong Tran",
      "Chris Ngo",
      "Khanh Xuan Nguyen",
      "Thanh Nguyen-Tang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.600": {
    "title": "Beyond Checkmate: Exploring the Creative Choke Points for AI Generated Texts",
    "volume": "main",
    "abstract": "The rapid advancement of Large Language Models (LLMs) has revolutionized text generation but also raised concerns about potential misuse, making detecting LLM-generated text (AI text) increasingly essential. While prior work has focused on identifying AI text and effectively checkmating it, our study investigates a less-explored territory: portraying the nuanced distinctions between human and AI texts across text segments (introduction, body, and conclusion). Whether LLMs excel or falter in incorporating linguistic ingenuity across text segments, the results will critically inform their viability and boundaries as effective creative assistants to humans. Through an analogy with the structure of chess games, comprising opening, middle, and end games, we analyze segment-specific patterns to reveal where the most striking differences lie. Although AI texts closely resemble human writing in the body segment due to its length, deeper analysis shows a higher divergence in features dependent on the continuous flow of language, making it the most informative segment for detection. Additionally, human texts exhibit greater stylistic variation across segments, offering a new lens for distinguishing them from AI. Overall, our findings provide fresh insights into human-AI text differences and pave the way for more effective and interpretable detection strategies. Codes available at https://github.com/tripto03/chess_inspired_human_ai_text_distinction",
    "checked": true,
    "id": "94eda7d4e78d9e96288bdb832f46ad0e596274d2",
    "semantic_title": "beyond checkmate: exploring the creative choke points for ai generated texts",
    "citation_count": 2,
    "authors": [
      "Nafis Irtiza Tripto",
      "Saranya Venkatraman",
      "Mahjabin Nahar",
      "Dongwon Lee"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.601": {
    "title": "MoR: Better Handling Diverse Queries with a Mixture of Sparse, Dense, and Human Retrievers",
    "volume": "main",
    "abstract": "Retrieval-augmented Generation (RAG) is powerful, but its effectiveness hinges on which retrievers we use and how. Different retrievers offer distinct, often complementary signals: BM25 captures lexical matches; dense retrievers, semantic similarity. Yet in practice, we typically fix a single retriever based on heuristics, which fails to generalize across diverse information needs. Can we dynamically select and integrate multiple retrievers for each individual query, without the need for manual selection? In our work, we validate this intuition with quantitative analysis and introduce a mixture of retrievers: a zero-shot, weighted combination of heterogeneous retrievers. Extensive experiments show that such mixtures are effective and efficient: Despite totaling just 0.8B parameters, this mixture outperforms every individual retriever and even larger 7B models—by +10.8% and +3.9% on average, respectively. Further analysis also shows that this mixture framework can help incorporate specialized non-oracle human information sources as retrievers to achieve good collaboration, with a 58.9% relative performance improvement over simulated humans alone",
    "checked": true,
    "id": "6a209270efac31f32f3c86a5a3315861fab1f1e2",
    "semantic_title": "mor: better handling diverse queries with a mixture of sparse, dense, and human retrievers",
    "citation_count": 0,
    "authors": [
      "Jushaan Singh Kalra",
      "Xinran Zhao",
      "To Eun Kim",
      "Fengyu Cai",
      "Fernando Diaz",
      "Tongshuang Wu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.602": {
    "title": "Learning Contextual Retrieval for Robust Conversational Search",
    "volume": "main",
    "abstract": "Effective conversational search demands a deep understanding of user intent across multiple dialogue turns. Users frequently use abbreviations and shift topics in the middle of conversations, posing challenges for conventional retrievers. While query rewriting techniques improve clarity, they often incur significant computational cost due to additional autoregressive steps. Moreover, although LLM-based retrievers demonstrate strong performance, they are not explicitly optimized to track user intent in multi-turn settings, often failing under topic drift or contextual ambiguity. To address these limitations, we propose ContextualRetriever, a novel LLM-based retriever that directly incorporates conversational context into the retrieval process. Our approach introduces: (1) a context-aware embedding mechanism that highlights the current query within the dialogue history; (2) intent-guided supervision based on high-quality rewritten queries; and (3) a training strategy that preserves the generative capabilities of the base LLM. Extensive evaluations across multiple conversational search benchmarks demonstrate that ContextualRetriever significantly outperforms existing methods while incurring no additional inference overhead",
    "checked": true,
    "id": "899f606f01e5307eff8e99fd681399910cbded8c",
    "semantic_title": "learning contextual retrieval for robust conversational search",
    "citation_count": 0,
    "authors": [
      "Seunghan Yang",
      "Juntae Lee",
      "Jihwan Bang",
      "Kyuhong Shim",
      "Minsoo Kim",
      "Simyung Chang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.603": {
    "title": "LIDDIA: Language-based Intelligent Drug Discovery Agent",
    "volume": "main",
    "abstract": "Drug discovery is a long, expensive, and complex process, relying heavily on human medicinal chemists, who can spend years searching the vast space of potential therapies. Recent advances in artificial intelligence for chemistry have sought to expedite individual drug discovery tasks; however, there remains a critical need for an intelligent agent that can navigate the drug discovery process. Towards this end, we introduce LIDDiA, an autonomous agent capable of intelligently navigating the drug discovery process in silico. By leveraging the reasoning capabilities of large language models, LIDDiA serves as a low-cost and highly-adaptable tool for autonomous drug discovery. We comprehensively examine LIDDiA, demonstrating that (1) it can generate molecules meeting key pharmaceutical criteria on over 70% of 30 clinically relevant targets, (2) it intelligently balances exploration and exploitation in the chemical space, and (3) it identifies one promising novel candidate on AR/NR3C4, a critical target for both prostate and breast cancers. Code and dataset are available at https://github.com/ninglab/LIDDiA",
    "checked": true,
    "id": "a7a2b307a05b8b4c5804e3c4601e0bd7381742c4",
    "semantic_title": "liddia: language-based intelligent drug discovery agent",
    "citation_count": 9,
    "authors": [
      "Reza Averly",
      "Frazier N. Baker",
      "Ian A Watson",
      "Xia Ning"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.604": {
    "title": "Agentic-R1: Distilled Dual-Strategy Reasoning",
    "volume": "main",
    "abstract": "Current long chain-of-thought (long-CoT) models excel at mathematical reasoning but rely on slow and error-prone natural language traces. Tool-augmented agents address arithmetic via code execution, but often falter on complex logical tasks. We introduce a fine-tuning framework, **DualDistill**, that distills complementary reasoning strategies from multiple teachers into a unified student model. Using this approach, we train **Agentic-R1**, which dynamically selects the optimal strategy for each query, invoking tools for arithmetic and algorithmic problems and using text-based reasoning for abstract ones. Our method improves accuracy on computation-intensive tasks and reduces inference latency on standard benchmarks, demonstrating the promise of multi-strategy distillation for robust and efficient reasoning",
    "checked": true,
    "id": "0844db6fcc148864aaf339febc83c09cbfdd6534",
    "semantic_title": "agentic-r1: distilled dual-strategy reasoning",
    "citation_count": 2,
    "authors": [
      "Weihua Du",
      "Pranjal Aggarwal",
      "Sean Welleck",
      "Yiming Yang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.605": {
    "title": "Proactive Assistant Dialogue Generation from Streaming Egocentric Videos",
    "volume": "main",
    "abstract": "Recent advances in conversational AI have been substantial, but developing real-time systems for perceptual task guidance remains challenging. These systems must provide interactive, proactive assistance based on streaming visual inputs, yet their development is constrained by the costly and labor-intensive process of data collection and system evaluation. To address these limitations, we present a comprehensive framework with three key contributions. First, we introduce a novel data curation pipeline that synthesizes dialogues from annotated egocentric videos, resulting in ProAssist, a large-scale synthetic dialogue dataset spanning multiple domains. Second, we develop a suite of automatic evaluation metrics, validated through extensive human studies. Third, we propose an end-to-end model that processes streaming video inputs to generate contextually appropriate responses, incorporating novel techniques for handling data imbalance and long-duration videos. This work lays the foundation for developing real-time, proactive AI assistants capable of guiding users through diverse tasks",
    "checked": true,
    "id": "7174281927ec932a1531a9e3275308d8f3aaae1e",
    "semantic_title": "proactive assistant dialogue generation from streaming egocentric videos",
    "citation_count": 0,
    "authors": [
      "Yichi Zhang",
      "Xin Luna Dong",
      "Zhaojiang Lin",
      "Andrea Madotto",
      "Anuj Kumar",
      "Babak Damavandi",
      "Joyce Chai",
      "Seungwhan Moon"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.606": {
    "title": "Should I Share this Translation? Evaluating Quality Feedback for User Reliance on Machine Translation",
    "volume": "main",
    "abstract": "As people increasingly use AI systems in work and daily life, feedback mechanisms that help them use AI responsibly are urgently needed, particularly in settings where users are not equipped to assess the quality of AI predictions. We study a realistic Machine Translation (MT) scenario where monolingual users decide whether to share an MT output, first without and then with quality feedback. We compare four types of quality feedback: explicit feedback that directly give users an assessment of translation quality using (1) error highlights and (2) LLM explanations, and implicit feedback that helps users compare MT inputs and outputs through (3) backtranslation and (4) question–answer (QA) tables. We find that all feedback types, except error highlights, significantly improve both decision accuracy and appropriate reliance. Notably, implicit feedback, especially QA tables, yields significantly greater gains than explicit feedback in terms of decision accuracy, appropriate reliance, and user perceptions – receiving the highest ratings for helpfulness and trust, and the lowest for mental burden",
    "checked": true,
    "id": "a912bacd191c73a23af3d909692f31036508a264",
    "semantic_title": "should i share this translation? evaluating quality feedback for user reliance on machine translation",
    "citation_count": 1,
    "authors": [
      "Dayeon Ki",
      "Kevin Duh",
      "Marine Carpuat"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.607": {
    "title": "ChartGaze: Enhancing Chart Understanding in LVLMs with Eye-Tracking Guided Attention Refinement",
    "volume": "main",
    "abstract": "Charts are a crucial visual medium for communicating and representing information. While Large Vision-Language Models (LVLMs) have made progress on chart question answering (CQA), the task remains challenging, particularly when models attend to irrelevant regions of the chart. In this work, we present ChartGaze, a new eye-tracking dataset that captures human gaze patterns during chart reasoning tasks. Through a systematic comparison of human and model attention, we find that LVLMs often diverge from human gaze, leading to reduced interpretability and accuracy. To address this, we propose a gaze-guided attention refinement that aligns image-text attention with human fixations. Our approach improves both answer accuracy and attention alignment, yielding gains of up to 2.56 percentage points across multiple models. These results demonstrate the promise of incorporating human gaze to enhance both the reasoning quality and interpretability of chart-focused LVLMs",
    "checked": true,
    "id": "28c41cc246a97db95ce2c3dd0724b13a7584c190",
    "semantic_title": "chartgaze: enhancing chart understanding in lvlms with eye-tracking guided attention refinement",
    "citation_count": 0,
    "authors": [
      "Ali Salamatian",
      "Amirhossein Abaskohi",
      "Wan-Cyuan Fan",
      "Mir Rayat Imtiaz Hossain",
      "Leonid Sigal",
      "Giuseppe Carenini"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.608": {
    "title": "LogiCoL: Logically-Informed Contrastive Learning for Set-based Dense Retrieval",
    "volume": "main",
    "abstract": "While significant progress has been made with dual- and bi-encoder dense retrievers, they often struggle on queries with logical connectives, a use case that is often overlooked yet important in downstream applications. Current dense retrievers struggle with such queries, such that the retrieved results do not respect the logical constraints implied in the queries. To address this challenge, we introduce LogiCoL, a logically-informed contrastive learning objective for dense retrievers. LogiCoL builds upon in-batch supervised contrastive learning, and learns dense retrievers to respect the subset and mutually-exclusive set relation between query results via two sets of soft constraints expressed via t-norm in the learning objective. We evaluate the effectiveness of LogiCoL on the task of entity retrieval, where the model is expected to retrieve a set of entities in Wikipedia that satisfy the implicit logical constraints in the query. We show that models trained with LogiCoL yield improvement both in terms of retrieval performance and logical consistency in the results. We provide detailed analysis and insights to uncover why queries with logical connectives are challenging for dense retrievers and why LogiCoL is most effective",
    "checked": true,
    "id": "b72cd679b53cfc453823327d7eed97867f994312",
    "semantic_title": "logicol: logically-informed contrastive learning for set-based dense retrieval",
    "citation_count": 0,
    "authors": [
      "Yanzhen Shen",
      "Sihao Chen",
      "Xueqiang Xu",
      "Yunyi Zhang",
      "Chaitanya Malaviya",
      "Dan Roth"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.609": {
    "title": "ModalPrompt: Towards Efficient Multimodal Continual Instruction Tuning with Dual-Modality Guided Prompt",
    "volume": "main",
    "abstract": "Large Multimodal Models (LMMs) exhibit remarkable multi-tasking ability by learning mixed instruction datasets. However, novel tasks would be encountered sequentially in dynamic world, which urges for equipping LMMs with multimodal continual instruction learning (MCIT) ability especially for diverse and challenging generative tasks. Existing MCIT methods do not fully exploit the unique attribute of LMMs and often gain performance at the expense of efficiency. In this paper, we propose a novel prompt learning framework for MCIT to effectively alleviate forgetting of previous knowledge while managing computational complexity with natural image-text supervision. Concretely, we learn prompts for each task and exploit efficient prompt fusion for knowledge transfer and prompt selection for complexity management with dual-modality guidance. Extensive experiments demonstrate that our approach achieves substantial +14.26% performance gain on MCIT benchmarks with remarkable x1.42 inference speed free from growing computation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fanhu Zeng",
      "Fei Zhu",
      "Haiyang Guo",
      "Xu-Yao Zhang",
      "Cheng-Lin Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.610": {
    "title": "Skip-Thinking: Chunk-wise Chain-of-Thought Distillation Enable Smaller Language Models to Reason Better and Faster",
    "volume": "main",
    "abstract": "Chain-of-thought (CoT) distillation allows a large language model (LLM) to guide a small language model (SLM) in reasoning tasks. Existing methods train the SLM to learn the long rationale in one iteration, resulting in two issues: 1) Long rationales lead to a large token-level batch size during training, making gradients of core reasoning tokens (i.e., the token will directly affect the correctness of subsequent reasoning) over-smoothed as they contribute a tiny fraction of the rationale. As a result, the SLM converges to sharp minima where it fails to grasp the reasoning logic. 2) The response is slow, as the SLM must generate a long rationale before reaching the answer. Therefore, we propose chunk-wise training (CWT), which uses a heuristic search to divide the rationale into internal semantically coherent chunks and focuses SLM on learning from only one chunk per iteration. In this way, CWT naturally isolates non-reasoning chunks that do not involve the core reasoning token (e.g., summary and transitional chunks) from the SLM learning for reasoning chunks, making the fraction of the core reasoning token increase in the corresponding iteration. Based on CWT, skip-thinking training (STT) is proposed. STT makes the SLM automatically skip non-reasoning medium chunks to reach the answer, improving reasoning speed while maintaining accuracy. We validate our approach on a variety of SLMs and multiple reasoning tasks",
    "checked": true,
    "id": "6980c678af771a4c50c597c9318bcd2cf3345585",
    "semantic_title": "skip-thinking: chunk-wise chain-of-thought distillation enable smaller language models to reason better and faster",
    "citation_count": 4,
    "authors": [
      "Xiaoshu Chen",
      "Sihang Zhou",
      "Ke Liang",
      "Xiaoyu Sun",
      "Xinwang Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.611": {
    "title": "Can an Individual Manipulate the Collective Decisions of Multi-Agents?",
    "volume": "main",
    "abstract": "Individual Large Language Models (LLMs) have demonstrated significant capabilities across various domains, such as healthcare and law. Recent studies also show that coordinated multi-agent systems exhibit enhanced decision-making and reasoning abilities through collaboration. However, due to the vulnerabilities of individual LLMs and the difficulty of accessing all agents in a multi-agent system, a key question arises: If attackers only know one agent, could they still generate adversarial samples capable of misleading the collective decision?To explore this question, we formulate it as a game with incomplete information, where attackers know only one target agent and lack knowledge of the other agents in the system. With this formulation, we propose M-Spoiler, a framework that simulates agent interactions within a multi-agent system to generate adversarial samples. These samples are then used to manipulate the target agent in the target system, misleading the system's collaborative decision-making process.More specifically, M-Spoiler introduces a stubborn agent that actively aids in optimizing adversarial samples by simulating potential stubborn responses from agents in the target system. This enhances the effectiveness of the generated adversarial samples in misleading the system.Through extensive experiments across various tasks, our findings confirm the risks posed by the knowledge of an individual agent in multi-agent systems and demonstrate the effectiveness of our framework.We also explore several defense mechanisms, showing that our proposed attack framework remains more potent than baselines, underscoring the need for further research into defensive strategies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fengyuan Liu",
      "Rui Zhao",
      "Shuo Chen",
      "Guohao Li",
      "Philip Torr",
      "Lei Han",
      "Jindong Gu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.612": {
    "title": "Toxicity Red-Teaming: Benchmarking LLM Safety in Singapore's Low-Resource Languages",
    "volume": "main",
    "abstract": "The advancement of Large Language Models (LLMs) has transformed natural language processing; however, their safety mechanisms remain under-explored in low-resource, multilingual settings. Here, we aim to bridge this gap. In particular, we introduce SGToxicGuard, a novel dataset and evaluation framework for benchmarking LLM safety in Singapore's diverse linguistic context, including Singlish, Chinese, Malay, and Tamil. SGToxicGuard adopts a red-teaming approach to systematically probe LLM vulnerabilities in three real-world scenarios: conversation, question-answering, and content composition. We conduct extensive experiments with state-of-the-art multilingual LLMs, and the results uncover critical gaps in their safety guardrails. By offering actionable insights into cultural sensitivity and toxicity mitigation, we lay the foundation for safer and more inclusive AI systems in linguistically diverse environments. Disclaimer: This paper contains sensitive content that may be disturbing to some readers",
    "checked": true,
    "id": "67f79d117aac1fc2b020435cd9c31130a9ed00de",
    "semantic_title": "toxicity red-teaming: benchmarking llm safety in singapore's low-resource languages",
    "citation_count": 0,
    "authors": [
      "Yujia Hu",
      "Ming Shan Hee",
      "Preslav Nakov",
      "Roy Ka-Wei Lee"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.613": {
    "title": "Improving Clustering with Positive Pairs Generated from LLM-Driven Labels",
    "volume": "main",
    "abstract": "Traditional unsupervised clustering methods, which often rely on contrastive training of embedders, suffer from a lack of label knowledge, resulting in suboptimal performance. Furthermore, the presence of potential false negatives can destabilize the training process. Hence, we propose to improve clustering with Positive Pairs generated from LLM-driven Labels (PPLL). In the proposed framework, LLM is initially employed to cluster the data and generate corresponding mini-cluster labels. Subsequently, positive pairs are constructed based on these labels, and an embedder is trained using BYOL to obviate the need for negative pairs. Following training, the acquired label knowledge is integrated into K-means clustering. This framework enables the integration of label information throughout the training and inference processes, while mitigating the reliance on negative pairs. Additionally, it generates interpretable labels for improved understanding of clustering results. Empirical evaluations on a range of datasets demonstrate that our proposed framework consistently surpasses state-of-the-art baselines, achieving superior performance, robustness, and computational efficiency for diverse text clustering applications",
    "checked": true,
    "id": "d6e36a3acbf313c6d483e8b87726e91a3e6df2e6",
    "semantic_title": "improving clustering with positive pairs generated from llm-driven labels",
    "citation_count": 0,
    "authors": [
      "Xiaotong Zhang",
      "Ying Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.614": {
    "title": "Gamma-Guard: Lightweight Residual Adapters for Robust Guardrails in Large Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) are widely deployed as zero-shot evaluators for answer grading, content moderation, and document ranking. Yet studies show that guard models (Guards)—LLMs fine-tuned for safety—remain vulnerable to \"jailbreak\" attacks, jeopardising downstream chatbots.We confirm this weakness on three public benchmarks (BeaverTails, XSTest, AdvBench) and trace it to representation shifts that arise in the embedding layer and cascade through the Transformer stack.To counteract the effect, we introduce Gamma-Guard: lightweight residual adapters inserted after the embeddings and at sparse intervals in the model. The adapters start with zero-scaled gates, so they retain the original behaviour; a brief adversarial fine-tuning phase then teaches them to denoise embeddings and refocus attention.With fewer than 0.1% extra parameters and only a 2% latency increase, Gamma-Guard lifts adversarial accuracy from <5% to 95% a 90 percentage-point gain while reducing clean-data accuracy by just 8 percentage points.Extensive ablations further show that robustness improvements persist across different layer placements and model sizes.To our knowledge, this is the first approach that directly augments large Guards with trainable adapters, providing a practical path toward safer large-scale LLM deployments",
    "checked": true,
    "id": "18ef14b5a407bc746b6a4f805411ab5dd97b76d5",
    "semantic_title": "gamma-guard: lightweight residual adapters for robust guardrails in large language models",
    "citation_count": 0,
    "authors": [
      "Lijia Lv",
      "Yuanshu Zhao",
      "Guan Wang",
      "Xuehai Tang",
      "Wen Jie",
      "Jizhong Han",
      "Songlin Hu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.615": {
    "title": "Facilitating Long Context Understanding via Supervised Chain-of-Thought Reasoning",
    "volume": "main",
    "abstract": "Recent advances in Large Language Models (LLMs) have enabled them to process increasingly longer sequences, ranging from 2K to 2M tokens and even beyond. However, simply extending the input sequence length does not necessarily lead to effective long-context understanding. In this study, we integrate Chain-of-Thought (CoT) reasoning into LLMs in a supervised manner to facilitate effective long-context understanding. To achieve this, we introduce LongFinanceQA, a synthetic dataset in the financial domain designed to improve long-context reasoning. Unlike existing long-context synthetic data, LongFinanceQA includes intermediate CoT reasoning before the final conclusion, which encourages LLMs to perform explicit reasoning, improving accuracy and interpretability in long-context understanding. To generate synthetic CoT reasoning, we propose Property-based Agentic Inference (PAI), an agentic framework that simulates human-like reasoning steps, including property extraction, retrieval, and summarization. We evaluate PAI's reasoning capabilities by assessing GPT-4o-mini w/ PAI on the Loong benchmark, outperforming standard GPT-4o-mini by 20.0%. Furthermore, we fine-tune LLaMA-3.1-8B-Instruct on LongFinanceQA, achieving a 28.0% gain on Loong's financial subset",
    "checked": true,
    "id": "3e815bd1929531ad60707783002574176ac35854",
    "semantic_title": "facilitating long context understanding via supervised chain-of-thought reasoning",
    "citation_count": 3,
    "authors": [
      "Jingyang Lin",
      "Andy Wong",
      "Tian Xia",
      "Shenghua He",
      "Hui Wei",
      "Mei Han",
      "Jiebo Luo"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.616": {
    "title": "Dynamic Energy-Based Contrastive Learning with Multi-Stage Knowledge Verification for Event Causality Identification",
    "volume": "main",
    "abstract": "Event Causal Identification (ECI) aims to identify fine-grained causal relationships between events from unstructured text. Contrastive learning has shown promise in enhancing ECI by optimizing representation distances between positive and negative samples. However, existing methods often rely on rule-based or random sampling strategies, which may introduce spurious causal positives. Moreover, static negative samples often fail to approximate actual decision boundaries, thus limiting discriminative performance. Therefore, we propose an ECI method enhanced by Dynamic Energy-based Contrastive Learning with multi-stage knowledge Verification (DECLV). Specifically, we integrate multi-source knowledge validation and LLM-driven causal inference to construct a multi-stage knowledge validation mechanism, which generates high-quality contrastive samples and effectively suppresses spurious causal disturbances. Meanwhile, we introduce the Stochastic Gradient Langevin Dynamics (SGLD) method to dynamically generate adversarial negative samples, and employ an energy-based function to model the causal boundary between positive and negative samples. The experimental results show that our method outperforms previous state-of-the-art methods on both benchmarks, EventStoryLine and Causal-TimeBank",
    "checked": true,
    "id": "c2b4f6cd721a15515b6bb782ca254f3966874453",
    "semantic_title": "dynamic energy-based contrastive learning with multi-stage knowledge verification for event causality identification",
    "citation_count": 0,
    "authors": [
      "Ya Su",
      "Hu Zhang",
      "Yue Fan",
      "Guangjun Zhang",
      "YuJie Wang",
      "Ru Li",
      "Hongye Tan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.617": {
    "title": "ICG: Improving Cover Image Generation via MLLM-based Prompting and Personalized Preference Alignment",
    "volume": "main",
    "abstract": "Recent advances in multimodal large language models (MLLMs) and diffusion models (DMs) have opened new possibilities for AI-generated content. Yet, personalized cover image generation remains underexplored, despite its critical role in boosting user engagement on digital platforms. We propose ICG, a novel framework that integrates MLLM-based prompting with personalized preference alignment to generate high-quality, contextually relevant covers. ICG extracts semantic features from item titles and reference images via meta tokens, refines them with user embeddings, and injects the resulting personalized context into the diffusion model. To address the lack of labeled supervision, we adopt a multi-reward learning strategy that combines public aesthetic and relevance rewards with a personalized preference model trained from user behavior. Unlike prior pipelines relying on handcrafted prompts and disjointed modules, ICG employs an adapter to bridge MLLMs and diffusion models for end-to-end training. Experiments demonstrate that ICG significantly improves image quality, semantic fidelity, and personalization, leading to stronger user appeal and offline recommendation accuracy in downstream tasks. As a plug-and-play adapter bridging MLLMs and diffusion models, ICG is compatible with common checkpoints and requires no ground-truth labels during optimization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhipeng Bian",
      "Jieming Zhu",
      "Qijiong Liu",
      "Wang Lin",
      "Guohao Cai",
      "Zhaocheng Du",
      "Jiacheng Sun",
      "Zhou Zhao",
      "Zhenhua Dong"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.618": {
    "title": "From Long to Lean: Performance-aware and Adaptive Chain-of-Thought Compression via Multi-round Refinement",
    "volume": "main",
    "abstract": "Chain-of-Thought (CoT) reasoning improves performance on complex tasks but introduces significant inference latency due to its verbosity. In this work, we propose Multiround Adaptive Chain-of-Thought Compression (MACC), a framework that leverages the token elasticity phenomenon—where overly small token budgets may paradoxically increase output length—to progressively compress CoTs via multiround refinement. This adaptive strategy allows MACC to dynamically determine the optimal compression depth for each input. Our method achieves an average accuracy improvement of 5.6% over state-of-the-art baselines, while also reducing CoT length by an average of 47 tokens and significantly lowering latency. Furthermore, we show that test-time performance—accuracy and token length—can be reliably predicted using interpretable features like perplexity and compression rate on training set. Evaluated across different models, our method enables efficient model selection and forecasting without repeated fine-tuning, demonstrating that CoT compression is both effective and predictable. Our code will be released in https://github.com/Leon221220/MACC",
    "checked": true,
    "id": "63df6cfb96c715b4e8eb28a1f3a6b4ec9e5d33bb",
    "semantic_title": "from long to lean: performance-aware and adaptive chain-of-thought compression via multi-round refinement",
    "citation_count": 0,
    "authors": [
      "JianZhi Yan",
      "Le Liu",
      "Youcheng Pan",
      "Shiwei Chen",
      "Zike Yuan",
      "Yang Xiang",
      "Buzhou Tang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.619": {
    "title": "A Symbolic Adversarial Learning Framework for Evolving Fake News Generation and Detection",
    "volume": "main",
    "abstract": "Rapid LLM advancements heighten fake news risks by enabling the automatic generation of increasingly sophisticated misinformation. Previous detection methods, including fine-tuned small models or LLM-based detectors, often struggle with its dynamically evolving nature. In this work, we propose a novel framework called the Symbolic Adversarial Learning Framework (SALF), which implements an adversarial training paradigm by an agent symbolic learning optimization process, rather than relying on numerical updates. SALF introduces a paradigm where the generation agent crafts deceptive narratives, and the detection agent uses structured debates to identify logical and factual flaws for detection, and they iteratively refine themselves through such adversarial interactions. Unlike traditional neural updates, we represent agents using agent symbolic learning, where learnable weights are defined by agent prompts, and simulate back-propagation and gradient descent by operating on natural language representations of weights, loss, and gradients. Experiments on two multilingual benchmark datasets demonstrate SALF's effectiveness, showing it generates sophisticated fake news that degrades state-of-the-art detection performance by up to 53.4% in Chinese and 34.2% in English on average. SALF also refines detectors, improving detection of refined content by up to 7.7%. We hope our work inspires further exploration into more robust, adaptable fake news detection systems",
    "checked": true,
    "id": "a361b83780ae05a0ef1e74e35cf310d5af9c50b6",
    "semantic_title": "a symbolic adversarial learning framework for evolving fake news generation and detection",
    "citation_count": 0,
    "authors": [
      "Chong Tian",
      "Qirong Ho",
      "Xiuying Chen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.620": {
    "title": "RareSyn: Health Record Synthesis for Rare Disease Diagnosis",
    "volume": "main",
    "abstract": "Diagnosis based on Electronic Health Records (EHRs) often struggles with data scarcity and privacy concerns. To address these issues, we introduce RareSyn, an innovative data synthesis approach designed to augment and de-identify EHRs, with a focus on rare diseases. The core insight of RareSyn involves using seed EHRs of rare diseases to recall similar records from both common and rare diseases, and then leveraging Large Language Models to substitute the key medical information (e.g., symptoms or examination details) in these records with information from the knowledge graph, thereby generating new EHRs. We first train a transformer Encoder with contrastive learning to integrate various types of medical knowledge. Then, RareSyn engages in iterative processes of recalling similar EHRs, structuring EHRs, revising EHRs, and generating new EHRs until the produced EHRs achieve extensive coverage of the rare disease knowledge. We assess RareSyn based on its utility for diagnosis modeling, the diversity of medical knowledge it incorporates, and the privacy of the synthesized EHRs. Extensive experiments demonstrate its effectiveness in improving disease diagnosis, enhancing diversity, and maintaining privacy",
    "checked": true,
    "id": "16933ee3e5f0de4567f13df1c8bcc9fd3980e84b",
    "semantic_title": "raresyn: health record synthesis for rare disease diagnosis",
    "citation_count": 0,
    "authors": [
      "Huimin Wang",
      "Yutian Zhao",
      "Yefeng Zheng",
      "Xian Wu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.621": {
    "title": "Sticker-TTS: Learn to Utilize Historical Experience with a Sticker-driven Test-Time Scaling Framework",
    "volume": "main",
    "abstract": "Large reasoning models (LRMs) have exhibited strong performance on complex reasoning tasks, with further gains achievable through increased computational budgets at inference. However, current test-time scaling methods predominantly rely on redundant sampling, ignoring the historical experience utilization, thereby limiting computational efficiency. To overcome this limitation, we propose Sticker-TTS, a novel test-time scaling framework that coordinates three collaborative LRMs to iteratively explore and refine solutions guided by historical attempts. At the core of our framework are distilled key conditions—termed stickers—which drive the extraction, refinement, and reuse of critical information across multiple rounds of reasoning. To further enhance the efficiency and performance of our framework, we introduce a two-stage optimization strategy that combines imitation learning with self-improvement, enabling progressive refinement. Extensive evaluations on three challenging mathematical reasoning benchmarks, including AIME-24, AIME-25, and OlymMATH, demonstrate that Sticker-TTS consistently surpasses strong baselines, including self-consistency and advanced reinforcement learning approaches, under comparable inference budgets. These results highlight the effectiveness of sticker-guided historical experience utilization. Our code and data are available at https://github.com/RUCAIBox/Sticker-TTS",
    "checked": true,
    "id": "53c33193b70419344033b79e27496db72c676f2c",
    "semantic_title": "sticker-tts: learn to utilize historical experience with a sticker-driven test-time scaling framework",
    "citation_count": 0,
    "authors": [
      "Jie Chen",
      "Jinhao Jiang",
      "Yingqian Min",
      "Zican Dong",
      "Shijie Wang",
      "Xin Zhao",
      "Ji-Rong Wen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.622": {
    "title": "CMHG: A Dataset and Benchmark for Headline Generation of Minority Languages in China",
    "volume": "main",
    "abstract": "Minority languages in China, such as Tibetan, Uyghur, and Traditional Mongolian, face significant challenges due to their unique writing systems, which differ from international standards. This discrepancy has led to a severe lack of relevant corpora, particularly for supervised tasks like headline generation. To address this gap, we introduce a novel dataset, Chinese Minority Headline Generation (CMHG), which includes 100,000 entries for Tibetan, and 50,000 entries each for Uyghur and Mongolian, specifically curated for headline generation tasks. Additionally, we propose a high-quality test set annotated by native speakers, designed to serve as a benchmark for future research in this domain. We hope this dataset will become a valuable resource for advancing headline generation in Chinese minority languages and contribute to the development of related benchmarks",
    "checked": true,
    "id": "93e5df65a1eb917c66a1577f12cf984691885c88",
    "semantic_title": "cmhg: a dataset and benchmark for headline generation of minority languages in china",
    "citation_count": 0,
    "authors": [
      "Guixian Xu",
      "Zeli Su",
      "Ziyin Zhang",
      "Jianing Liu",
      "Xu Han",
      "Ting Zhang",
      "Yushuang Dong"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.623": {
    "title": "Understanding the Information Propagation Effects of Communication Topologies in LLM-based Multi-Agent Systems",
    "volume": "main",
    "abstract": "The communication topology in large language model-based multi-agent systems fundamentally governs inter-agent collaboration patterns, critically shaping both the efficiency and effectiveness of collective decision-making. While recent studies for communication topology automated design tend to construct sparse structures for efficiency, they often overlook why and when sparse and dense topologies help or hinder collaboration. In this paper, we present a causal framework to analyze how agent outputs, whether correct or erroneous, propagate under topologies with varying sparsity. Our empirical studies reveal that moderately sparse topologies, which effectively suppress error propagation while preserving beneficial information diffusion, typically achieve optimal task performance. Guided by this insight, we propose a novel topology design approach, EIB-Learner, that balances error suppression and beneficial information propagation by fusing connectivity patterns from both dense and sparse graphs. Extensive experiments show the superior effectiveness, communication cost, and robustness of EIB-Learner",
    "checked": true,
    "id": "86fcbae58aa690dde38665fff09c700ae18a8386",
    "semantic_title": "understanding the information propagation effects of communication topologies in llm-based multi-agent systems",
    "citation_count": 4,
    "authors": [
      "Xu Shen",
      "Yixin Liu",
      "Yiwei Dai",
      "Yili Wang",
      "Rui Miao",
      "Yue Tan",
      "Shirui Pan",
      "Xin Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.624": {
    "title": "Boosting Data Utilization for Multilingual Dense Retrieval",
    "volume": "main",
    "abstract": "Multilingual dense retrieval aims to retrieve relevant documents across different languages based on a unified retriever model. The challenge lies in aligning representations of different languages in a shared vector space. The common practice is to fine-tune the dense retriever via contrastive learning, whose effectiveness highly relies on the quality of the negative sample and the efficacy of mini-batch data. Different from the existing studies that focus on developing sophisticated model architecture, we propose a method to boost data utilization for multilingual dense retrieval by obtaining high-quality hard negative samples and effective mini-batch data. The extensive experimental results on a multilingual retrieval benchmark, MIRACL, with 16 languages demonstrate the effectiveness of our method by outperforming several existing strong baselines",
    "checked": true,
    "id": "0af30cd7180bb34c55de0847fbc6bfa3ad4c9324",
    "semantic_title": "boosting data utilization for multilingual dense retrieval",
    "citation_count": 1,
    "authors": [
      "Chao Huang",
      "Fengran Mo",
      "Yufeng Chen",
      "Changhao Guan",
      "Zhenrui Yue",
      "Xinyu Wang",
      "Jinan Xu",
      "Kaiyu Huang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.625": {
    "title": "Self-Augmented Preference Alignment for Sycophancy Reduction in LLMs",
    "volume": "main",
    "abstract": "Sycophancy causes models to produce answers that cater to user expectations rather than providing truthful responses. Sycophantic behavior in models can erode user trust by creating a perception of dishonesty or bias. This lack of authenticity may lead users to question the reliability and objectivity of the system's responses. Although Reinforcement Learning from Human Feedback (RLHF) is effective in aligning models with human preferences, previous studies have observed that it can simultaneously amplify sycophantic behavior. However, these studies primarily focused on proprietary models and employed indirect analysis to demonstrate the influence of human feedback. Our study focuses on sycophancy in open-source models, which are more reproducible and transparent for research. We investigated the impact of human feedback on sycophancy by directly comparing models aligned with human feedback to those not aligned. To address sycophancy, we proposed assessing the user's expected answer rather than ignoring it. Consequently, we developed the Sycophancy Answer Assessment (SAA) dataset and introduced Self-Augmented Preference Alignment, demonstrating that these methods effectively enhance the model's assessment ability and significantly reduce sycophancy across tasks",
    "checked": true,
    "id": "e10626aaf9d0e236da83c39d4f937ff6a1c21b1f",
    "semantic_title": "self-augmented preference alignment for sycophancy reduction in llms",
    "citation_count": 0,
    "authors": [
      "Chien Hung Chen",
      "Hen-Hsen Huang",
      "Hsin-Hsi Chen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.626": {
    "title": "TP-RAG: Benchmarking Retrieval-Augmented Large Language Model Agents for Spatiotemporal-Aware Travel Planning",
    "volume": "main",
    "abstract": "Large language models (LLMs) have shown promise in automating travel planning, yet they often fall short in addressing nuanced spatiotemporal rationality. While existing benchmarks focus on basic plan validity, they neglect critical aspects such as route efficiency, POI appeal, and real-time adaptability. This paper introduces **TP-RAG**, the first benchmark tailored for retrieval-augmented, spatiotemporal-aware travel planning. Our dataset includes 2,348 real-world travel queries, 85,575 fine-grain annotated POIs, and 18,784 high-quality travel trajectory references sourced from online tourist documents, enabling dynamic and context-aware planning. Through extensive experiments, we reveal that integrating reference trajectories significantly improves spatial efficiency and POI rationality of the travel plan, while challenges persist in universality and robustness due to conflicting references and noisy data. To address these issues, we propose *EvoRAG*, an evolutionary framework that potently synergizes diverse retrieved trajectories with LLMs' intrinsic reasoning. *EvoRAG* achieves state-of-the-art performance, improving spatiotemporal compliance and reducing commonsense violation compared to ground-up and retrieval-augmented baselines. Our work underscores the potential of hybridizing Web knowledge with LLM-driven optimization, paving the way for more reliable and adaptive travel planning agents",
    "checked": true,
    "id": "2a919d50d2f757269615646ae40d4f2d33a273e8",
    "semantic_title": "tp-rag: benchmarking retrieval-augmented large language model agents for spatiotemporal-aware travel planning",
    "citation_count": 3,
    "authors": [
      "Hang Ni",
      "Fan Liu",
      "Xinyu Ma",
      "Lixin Su",
      "Shuaiqiang Wang",
      "Dawei Yin",
      "Hui Xiong",
      "Hao Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.627": {
    "title": "Recontextualizing Revitalization: A Mixed Media Approach to Reviving the Nüshu Language",
    "volume": "main",
    "abstract": "Nüshu is an endangered language from Jiangyong County, China, and the world's only known writing system created and used exclusively by women. Recent Natural Language Processing (NLP) work has digitized small Nüshu-Chinese corpora, but the script remains computationally inaccessible due to its handwritten, mixed-media form and dearth of multimodal resources. We address this gap with two novel datasets: NüshuVision, an image corpus of 500 rendered sentences in traditional vertical, right-to-left orthography, and NüshuStrokes, the first sequential handwriting recordings of all 397 Unicode Nüshu characters by an expert calligrapher. Evaluating five state-of-the-art Chinese Optical Character Recognition (OCR) systems on NüshuVision shows that all fail entirely, each yielding a Character Error Rate (CER) of 1.0. Fine-tuning Microsoft's TrOCR on NüshuVision lowers CER to 0.67, a modest yet meaningful improvement. These contributions establish the first multimodal foundation for Nüshu revitalization and offer a culturally grounded framework for language preservation",
    "checked": true,
    "id": "7b71269f8ab76083c752a3b347d3b460c829e335",
    "semantic_title": "recontextualizing revitalization: a mixed media approach to reviving the nüshu language",
    "citation_count": 0,
    "authors": [
      "Ivory Yang",
      "Xiaobo Guo",
      "Yuxin Wang",
      "Hefan Zhang",
      "Yaning Jia",
      "William Dinauer",
      "Soroush Vosoughi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.628": {
    "title": "Towards Advanced Mathematical Reasoning for LLMs via First-Order Logic Theorem Proving",
    "volume": "main",
    "abstract": "Large language models (LLMs) have shown promising first-order logic (FOL) reasoning capabilities with applications in various areas. However, their effectiveness in complex mathematical reasoning involving multi-step FOL deductions is still under-researched. While LLMs perform competitively on established mathematical reasoning benchmarks, they struggle with multi-step FOL tasks, as demonstrated by Deepseek-Prover-V2-7B's low accuracy (4.2%) on our proposed theorem proving dataset. This issue arises from the limited exploration of diverse proof strategies and the potential for early reasoning mistakes to undermine entire proofs. To address these issues, we propose DREAM, a self-adaptive solution that enhances the Diversity and REAsonability of LLMs' generation strategies. DREAM incorporates an Axiom-Driven Strategy Diversification mechanism to promote varied strategic outcomes and a Sub-Proposition Error Feedback to help LLMs reflect on and correct their proofs. Our contributions include pioneering advancements in LLMs' mathematical reasoning through FOL theorem proving, introducing a novel inference stage solution that improves performance by 0.6% to 6.4%, and providing a curated dataset of 447 mathematical theorems in Lean 4 format for evaluation",
    "checked": true,
    "id": "9f35da31770673df81e6bf10d950fcc4572b3c04",
    "semantic_title": "towards advanced mathematical reasoning for llms via first-order logic theorem proving",
    "citation_count": 2,
    "authors": [
      "Chuxue Cao",
      "Mengze Li",
      "Juntao Dai",
      "Jinluan Yang",
      "Zijian Zhao",
      "Shengyu Zhang",
      "Weijie Shi",
      "Chengzhong Liu",
      "Sirui Han",
      "Yike Guo"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.629": {
    "title": "From Tens of Hours to Tens of Thousands: Scaling Back-Translation for Speech Recognition",
    "volume": "main",
    "abstract": "Recent advances in Automatic Speech Recognition (ASR) have been largely fueled by massive speech corpora. However, extending coverage to diverse languages with limited resources remains a formidable challenge. This paper introduces Speech Back-Translation, a a scalable pipeline that improves multilingual ASR models by converting large-scale text corpora into synthetic speech via off-the-shelf text-to-speech (TTS) models. We demonstrate that just tens of hours of real transcribed speech can effectively train TTS models to generate synthetic speech at hundreds of times the original volume while maintaining high quality. To evaluate synthetic speech quality, we develop an intelligibility-based assessment framework and establish clear thresholds for when synthetic data benefits ASR training. Using Speech Back-Translation, we generate more than 500,000 hours of synthetic speech in ten languages and continue pre-training Whisper-large-v3, achieving average transcription error reductions of over 30%. These results highlight the scalability and effectiveness of Speech Back-Translation for enhancing multilingual ASR systems",
    "checked": true,
    "id": "b8f76b11b6efc0dac5373b306d8ac0486f9dbc01",
    "semantic_title": "from tens of hours to tens of thousands: scaling back-translation for speech recognition",
    "citation_count": 0,
    "authors": [
      "Tianduo Wang",
      "Lu Xu",
      "Wei Lu",
      "Shanbo Cheng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.630": {
    "title": "CityEQA: A Hierarchical LLM Agent on Embodied Question Answering Benchmark in City Space",
    "volume": "main",
    "abstract": "Embodied Question Answering (EQA) has primarily focused on indoor environments, leaving the complexities of urban settings—spanning environment, action, and perception—largely unexplored. To bridge this gap, we introduce CityEQA, a new task where an embodied agent answers open-vocabulary questions through active exploration in dynamic city spaces. To support this task, we present CityEQA-EC, the first benchmark dataset featuring 1,412 human-annotated tasks across six categories, grounded in a realistic 3D urban simulator. Moreover, we propose -Manager-Actor (PMA), a novel agent tailored for CityEQA. PMA enables long-horizon planning and hierarchical task execution: the Planner breaks down the question answering into sub-tasks, the Manager maintains an object-centric cognitive map for spatial reasoning during the process control, and the specialized Actors handle navigation, exploration, and collection sub-tasks. Experiments demonstrate that PMA achieves 60.7% of human-level answering accuracy, significantly outperforming frontier-based baselines. While promising, the performance gap compared to humans highlights the need for enhanced visual reasoning in CityEQA. This work paves the way for future advancements in urban spatial intelligence. Dataset and code are available at https://github.com/tsinghua-fib-lab/CityEQA.git",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yong Zhao",
      "Kai Xu",
      "Zhengqiu Zhu",
      "Yue Hu",
      "Zhiheng Zheng",
      "Yingfeng Chen",
      "Yatai Ji",
      "Chen Gao",
      "Yong Li",
      "Jincai Huang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.631": {
    "title": "Mitigating Hallucinations in Vision-Language Models through Image-Guided Head Suppression",
    "volume": "main",
    "abstract": "Despite their remarkable progress in multimodal understanding tasks, large vision language models (LVLMs) often suffer from \"hallucination\", generating texts misaligned with the visual context. Existing methods aimed at reducing hallucinations through inference time intervention incur a significant increase in latency. To mitigate this, we present **SPIN**, a task-agnostic attention-guided head suppression strategy that can be seamlessly integrated during inference **without incurring any significant compute or latency overhead**. We investigate whether hallucination in LVLMs can be linked to specific model components. Our analysis suggests that hallucinations can be attributed to a dynamic subset of attention heads in each layer. Leveraging this insight, for each text query token, we selectively suppress attention heads that exhibit low attention to image tokens, keeping the top-k attention heads intact. Extensive evaluations on visual question answering and image description tasks demonstrate the efficacy of SPIN in reducing hallucination scores up to **2.7x** while maintaining F1, and improving throughput by **1.8x** compared to existing alternatives",
    "checked": true,
    "id": "0f74aa18e83856d67a0d433007a73ebe62a5ae81",
    "semantic_title": "mitigating hallucinations in vision-language models through image-guided head suppression",
    "citation_count": 2,
    "authors": [
      "Sreetama Sarkar",
      "Yue Che",
      "Alex Gavin",
      "Peter Anthony Beerel",
      "Souvik Kundu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.632": {
    "title": "Examining False Positives under Inference Scaling for Mathematical Reasoning",
    "volume": "main",
    "abstract": "Recent advancements in language models have led to significant improvements in mathematical reasoning across various benchmarks. However, most of these benchmarks rely on automatic evaluation methods that only compare final answers using heuristics, without verifying the underlying reasoning steps. This limitation results in false positive solutions, where models may produce correct final answers but with flawed deduction paths. In this paper, we systematically examine the prevalence of false positive solutions in mathematical problem solving for language models. We analyze the characteristics and extent of this issue across different open-source models, datasets of varying difficulty levels, and decoding strategies. Specifically, we explore how false positives influence the inference time scaling behavior of language models. Our experimental results reveal that: (1) false positive solutions persist across different models, datasets, and decoding methods, (2) sampling-based inference time scaling methods do not alleviate the problem, and (3) the pass@N evaluation metric is more susceptible to false positives, suggesting a significantly lower scaling ceiling than what automatic evaluations indicate. Additionally, we analyze specific instances of false positives and discuss potential limitations in self-improvement techniques and synthetic data generation under such conditions. Our data and code are publicly available at https://github.com/Wloner0809/False-Positives-in-Math",
    "checked": true,
    "id": "4249144131e296aeb4ba708f13d65c38927c3e12",
    "semantic_title": "examining false positives under inference scaling for mathematical reasoning",
    "citation_count": 8,
    "authors": [
      "Yu Wang",
      "Nan Yang",
      "Liang Wang",
      "Furu Wei",
      "Fuli Feng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.633": {
    "title": "Translationese-index: Using Likelihood Ratios for Graded and Generalizable Measurement of Translationese",
    "volume": "main",
    "abstract": "Translationese refers to linguistic properties that usually occur in translated texts. Previous works study translationese by framing it as a binary classification between original texts and translated texts. In this paper, we argue that translationese should be graded instead of binary and propose the first measure for translationese—the translationese-index (T-index), computed from the likelihood ratios of two contrastively fine-tuned language models (LMs). We use synthesized translations and translations in the wild to evaluate T-index's generalizability in cross-domain settings and its validity against human judgments.Our results show that T-index can generalize to unseen genres, authors, and language pairs. Moreover, T-index computed using two 0.5B LMs fine-tuned on only 1-5k pairs of synthetic data can effectively capture translationese, as demonstrated by alignment with human pointwise ratings and pairwise judgments.Additionally, the correlation between T-index and existing machine translation (MT) quality estimation (QE) metrics such as BLEU and COMET is low, suggesting that T-index is not covered by these metrics andcan serve as a complementary metric in MT QE",
    "checked": true,
    "id": "bca3895efa330477c1a6a76f2a0642dff933e4bb",
    "semantic_title": "translationese-index: using likelihood ratios for graded and generalizable measurement of translationese",
    "citation_count": 0,
    "authors": [
      "Yikang Liu",
      "Wanyang Zhang",
      "Yiming Wang",
      "Jialong Tang",
      "Pei Zhang",
      "Baosong Yang",
      "Fei Huang",
      "Rui Wang",
      "Hai Hu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.634": {
    "title": "Exploring the Limitations of Mamba in COPY and CoT Reasoning",
    "volume": "main",
    "abstract": "Transformers have become the backbone of modern Large Language Models (LLMs); however, their inference overhead grows linearly with the sequence length, posing challenges for modeling long sequences. In light of this, Mamba has attracted attention for maintaining a constant inference size, with empirical evidence demonstrating that it can match Transformer performance in sequence modeling while significantly reducing computational costs. However, an open question remains: can Mamba always bring savings while achieving performance comparable to Transformers? In this paper, we focus on analyzing the expressive ability of Mamba to perform our defined COPY operation and Chain of Thought (CoT) reasoning. First, inspired by the connection between Mamba and linear attention, we show that constant-sized Mamba may struggle to perform COPY operations while Transformers can handle them more easily. However, when the size of Mamba grows linearly with the input sequence length, it can accurately perform COPY, but in this case, Mamba no longer provides overhead savings. Based on this observation, we further analyze Mamba's ability to tackle CoT tasks, which can be described by the Dynamic Programming (DP) problems. Our findings suggest that to solve arbitrary DP problems, the total cost of Mamba is still comparable to standard Transformers. However, similar to efficient Transformers, when facing DP problems with favorable properties such as locality, Mamba can provide savings in overhead. Our experiments on the copy and CoT tasks further demonstrate Mamba's limitations compared to Transformers in learning these tasks",
    "checked": true,
    "id": "c785c4144d3ab565e828f89057251bccbfa7839a",
    "semantic_title": "exploring the limitations of mamba in copy and cot reasoning",
    "citation_count": 2,
    "authors": [
      "Ruifeng Ren",
      "Zhicong Li",
      "Yong Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.635": {
    "title": "ProcWorld: Benchmarking Large Model Planning in Reachability-Constrained Environments",
    "volume": "main",
    "abstract": "We introduce ProcWorld, a large-scale benchmark for partially observable embodied spatial reasoning and long-term planning with large language models (LLM) and vision language models (VLM). ProcWorld features a wide range of challenging embodied navigation and object manipulation tasks, covering 16 task types, 5,000 rooms, and over 10 million evaluation trajectories with diverse data distribution. ProcWorld supports configurable observation modes, ranging from text-only descriptions to vision-only observations. It enables text-based actions to control the agent following language instructions. ProcWorld has presented significant challenges for LLMs and VLMs: (1) active information gathering given partial observations for disambiguation; (2) simultaneous localization and decision-making by tracking the spatio-temporal state-action distribution; (3) constrained reasoning with dynamic states subject to physical reachability. Our extensive evaluation of 15 foundation models and 5 reasoning algorithms (with over 1 million rollouts) indicates larger models perform better. However, ProcWorld remains highly challenging for existing state-of-the-art models and in-context learning methods due to constrained reachability and the need for combinatorial spatial reasoning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong Wang",
      "Xinghang Li",
      "Zhengshen Zhang",
      "Jirong Liu",
      "Xiao Ma",
      "Hanbo Zhang",
      "Tao Kong",
      "Huaping Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.636": {
    "title": "R2I-Bench: Benchmarking Reasoning-Driven Text-to-Image Generation",
    "volume": "main",
    "abstract": "Reasoning is a fundamental capability often required in real-world text-to-image (T2I) generation, e.g., generating \"a bitten apple that has been left in the air for more than a week\" necessitates understanding temporal decay and commonsense concepts. While recent T2I models have made impressive progress in producing photorealistic images, their reasoning capability remains underdeveloped and insufficiently evaluated. To bridge this gap, we introduce R2I-Bench, a comprehensive benchmark specifically designed to rigorously assess reasoning-driven T2I generation. R2I-Bench comprises 3068 meticulously curated data instances, spanning 7 core reasoning categories, including commonsense, mathematical, logical, compositional, numerical, causal, and concept mixing. To facilitate fine-grained evaluation, we design R2IScore, a QA-style metric based on instance-specific, reasoning-oriented evaluation questions that assess three critical dimensions: text-image alignment, reasoning accuracy, and image quality. Extensive experiments with 16 representative T2I models, including a strong pipeline-based framework that decouples reasoning and generation using the state-of-the-art language and image generation models, demonstrate consistently limited reasoning performance, highlighting the need for more robust, reasoning-aware architectures in the next generation of T2I systems",
    "checked": true,
    "id": "9dd7a12fc5b364c8944b1e57ea9775324dae6044",
    "semantic_title": "r2i-bench: benchmarking reasoning-driven text-to-image generation",
    "citation_count": 2,
    "authors": [
      "Kaijie Chen",
      "Zihao Lin",
      "Zhiyang Xu",
      "Ying Shen",
      "Yuguang Yao",
      "Joy Rimchala",
      "Jiaxin Zhang",
      "Lifu Huang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.637": {
    "title": "Can GRPO Boost Complex Multimodal Table Understanding?",
    "volume": "main",
    "abstract": "Existing table understanding methods face challenges due to complex table structures and intricate logical reasoning. While supervised finetuning (SFT) dominates existing research, reinforcement learning (RL), such as Group Relative Policy Optimization (GRPO), has shown promise but struggled with low initial policy accuracy and coarse rewards in tabular contexts. In this paper, we introduce Table-R1, a three-stage RL framework that enhances multimodal table understanding through: (1) Warm-up that prompts initial perception and reasoning capabilities, (2) Perception Alignment GRPO (PA-GRPO), which employs continuous Tree-Edit-Distance Similarity (TEDS) rewards for recognizing table structures and contents, and (3) Hint-Completion GRPO (HC-GRPO), which utilizes fine-grained rewards of residual steps based on the hint-guided question. Extensive experiments demonstrate that Table-R1 can boost the model's table reasoning performance obviously on both held-in and held-out datasets, outperforming SFT and GRPO largely. Notably, Qwen2-VL-7B with Table-R1 surpasses larger specific table understanding models (e.g., Table-LLaVA 13B), even achieving comparable performance to the closed-source model GPT-4o on held-in datasets, demonstrating the efficacy of each stage of Table-R1 in overcoming initialization bottlenecks and reward sparsity, thereby advancing robust multimodal table understanding",
    "checked": true,
    "id": "5fad7f432b969d4264a54df873edbcd45b076567",
    "semantic_title": "can grpo boost complex multimodal table understanding?",
    "citation_count": 0,
    "authors": [
      "Xiaoqiang Kang",
      "Shengen Wu",
      "Zimu Wang",
      "Yilin Liu",
      "Xiaobo Jin",
      "Kaizhu Huang",
      "Wei Wang",
      "Yutao Yue",
      "Xiaowei Huang",
      "Qiufeng Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.638": {
    "title": "MoMoE: Mixture of Moderation Experts Framework for AI-Assisted Online Governance",
    "volume": "main",
    "abstract": "Large language models (LLMs) have shown great potential in flagging harmful content in online communities. Yet, existing approaches for moderation require a separate model for every community and are opaque in their decision-making, limiting real-world adoption. We introduce Mixture of Moderation Experts (MoMoE), a modular, cross-community framework that adds post-hoc explanations to enable scalable content moderation. MoMoE orchestrates four operators—Allocate, Predict, Aggregate, Explain—and is instantiated as seven community-specialized experts (MoMoE-Community) and five norm-violation experts (MoMoE-NormVio). On 30 unseen subreddits, the best variants obtain Micro-F1 scores of 0.72 and 0.67, respectively, matching or surpassing strong fine-tuned baselines while consistently producing concise and reliable explanations. Although community-specialized experts deliver the highest peak accuracy, norm-violation experts provide steadier performance across domains. These findings show that MoMoE yields scalable, transparent moderation without needing per-community fine-tuning. More broadly, they suggest that lightweight, explainable expert ensembles can guide future NLP and HCI research on trustworthy human-AI governance of online communities",
    "checked": true,
    "id": "e8d12993400a20f7ff18fabe753c471363671acb",
    "semantic_title": "momoe: mixture of moderation experts framework for ai-assisted online governance",
    "citation_count": 4,
    "authors": [
      "Agam Goyal",
      "Xianyang Zhan",
      "Yilun Chen",
      "Koustuv Saha",
      "Eshwar Chandrasekharan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.639": {
    "title": "Following the Autoregressive Nature of LLM Embeddings via Compression and Alignment",
    "volume": "main",
    "abstract": "A new trend uses LLMs as dense text encoders via contrastive learning. However, since LLM embeddings predict the probability distribution of the next token, they are inherently generative and distributive, conflicting with contrastive learning, which requires embeddings to capture full-text semantics and align via cosine similarity. This discrepancy hinders the full utilization of LLMs' pre-training capabilities, resulting in inefficient learning. In response to this issue, we propose AutoRegEmbed, a new contrastive learning method built on embedding conditional probability distributions, which integrates two core tasks: information compression and conditional distribution alignment. The information compression task encodes text into the embedding space, ensuring that the embedding vectors capture global semantics. The conditional distribution alignment task focuses on aligning text embeddings with positive samples embeddings by leveraging the conditional distribution of embeddings while simultaneously reducing the likelihood of generating negative samples from text embeddings, thereby achieving embedding alignment and uniformity. Experimental results demonstrate that our method significantly outperforms traditional contrastive learning approaches and achieves performance comparable to state-of-the-art models when using the same amount of data",
    "checked": true,
    "id": "c1ea0972ed4ae06e8fa603e274591535a832f377",
    "semantic_title": "following the autoregressive nature of llm embeddings via compression and alignment",
    "citation_count": 7,
    "authors": [
      "Jingcheng Deng",
      "Zhongtao Jiang",
      "Liang Pang",
      "Zihao Wei",
      "Liwei Chen",
      "Kun Xu",
      "Yang Song",
      "Huawei Shen",
      "Xueqi Cheng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.640": {
    "title": "Evaluating LLM-Generated Diagrams as Graphs",
    "volume": "main",
    "abstract": "Diagrams play a central role in research papers for conveying ideas, yet they are often notoriously complex and labor-intensive to create. Although diagrams are presented as images, standard image generative models struggle to produce clear diagrams with well-defined structure. We argue that a promising direction is to generate demonstration diagrams directly in textual form as SVGs, which can leverage recent advances in large language models (LLMs). However, due to the complexity of components and the multimodal nature of diagrams, sufficiently discriminative and explainable metrics for evaluating the quality of LLM-generated diagrams remain lacking. In this paper, we propose DiagramEval, a novel evaluation metric designed to assess demonstration diagrams generated by LLMs. Specifically, DiagramEval conceptualizes diagrams as graphs, treating text elements as nodes and their connections as directed edges, and evaluates diagram quality using two new groups of metrics: node alignment and path alignment. For the first time, we effectively evaluate diagrams produced by state-of-the-art LLMs on recent research literature, quantitatively demonstrating the validity of our metrics. Furthermore, we show how the enhanced explainability of our proposed metrics offers valuable insights into the characteristics of LLM-generated diagrams",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chumeng Liang",
      "Jiaxuan You"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.641": {
    "title": "Breaking Bad Tokens: Detoxification of LLMs Using Sparse Autoencoders",
    "volume": "main",
    "abstract": "Large language models (LLMs) are now ubiquitous in user-facing applications, yet they still generate undesirable toxic outputs, including profanity, vulgarity, and derogatory remarks. Although numerous detoxification methods exist, most apply broad, surface-level fixes and can therefore easily be circumvented by jailbreak attacks. In this paper we leverage sparse autoencoders (SAEs) to identify toxicity-related directions in the residual stream of models and perform targeted activation steering using the corresponding decoder vectors. We introduce three tiers of steering aggressiveness and evaluate them on GPT-2 Small and Gemma-2-2B, revealing trade-offs between toxicity reduction and language fluency. At stronger steering strengths, these causal interventions surpass competitive baselines in reducing toxicity by up to 20%, though fluency can degrade noticeably on GPT-2 Small depending on the aggressiveness. Crucially, standard NLP benchmark scores upon steering remain stable, indicating that the model's knowledge and general abilities are preserved. We further show that feature-splitting in wider SAEs hampers safety interventions, underscoring the importance of disentangled feature learning. Our findings highlight both the promise and the current limitations of SAE-based causal interventions for LLM detoxification, further suggesting practical guidelines for safer language-model deployment",
    "checked": true,
    "id": "5b765db5e48a94f9e902dfed1847f132d5c3101b",
    "semantic_title": "breaking bad tokens: detoxification of llms using sparse autoencoders",
    "citation_count": 1,
    "authors": [
      "Agam Goyal",
      "Vedant Rathi",
      "William Yeh",
      "Yian Wang",
      "Yuen Chen",
      "Hari Sundaram"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.642": {
    "title": "VCSearch: Bridging the Gap Between Well-Defined and Ill-Defined Problems in Mathematical Reasoning",
    "volume": "main",
    "abstract": "Large language models (LLMs) have demonstrated impressive performance on reasoning tasks, including mathematical reasoning. However, the current evaluation mostly focuses on carefully constructed benchmarks and neglects the consideration of real-world reasoning problems that present missing or contradictory conditions, known as ill-defined problems. To further study this problem, we develop a large-scale benchmark called Problems with Missing and Contradictory conditions (PMC) containing over 5,000 validated ill-defined mathematical problems. Our preliminary experiments through PMC reveal two challenges about existing methods: (1) traditional methods exhibit a trade-off between solving accuracy and rejection capabilities, and (2) formal methods struggle with modeling complex problems. To address these challenges, We develop Variable-Constraint Search (VCSearch), a training-free framework that leverages formal language to detect ill-defined problems, where a variable-constraint pair search strategy is incorporated to improve the modeling capability of formal language. Extensive experiments demonstrate that VCSearch improves the accuracy of identifying unsolvable problems by at least 12% across different LLMs, thus achieving stronger robust mathematical reasoning ability",
    "checked": true,
    "id": "4b5fc5f372d8b43b666b37159062ad3e6f113551",
    "semantic_title": "vcsearch: bridging the gap between well-defined and ill-defined problems in mathematical reasoning",
    "citation_count": 4,
    "authors": [
      "Shi-Yu Tian",
      "Zhi Zhou",
      "Kun-Yang Yu",
      "Ming Yang",
      "Lin-Han Jia",
      "Lan-Zhe Guo",
      "Yu-Feng Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.643": {
    "title": "How do autoregressive transformers solve full addition?",
    "volume": "main",
    "abstract": "Large pre-trained language models have demonstrated impressive capabilities, but there is still much to learn about how they operate. In this study, we conduct an investigation of the autoregressive transformer's ability to perform basic addition operations. Specifically, by using causal analysis we found that a few different attention heads in the middle layers control the addition carry, with each head processing carries of different lengths. Due to the lack of global focus on the sequence within these attention heads, the model struggles to handle long-sequence addition tasks. By performing inference intervention on mistral-7B, partial task performance can be restored, with the accuracy on 20-digit long-sequence additions from 2% to 38%. Through fine-tuning, a new mechanism branches out for handling complex cases, yet it still faces challenges with length generalization. Our research reveals how the models perform basic arithmetic task, and further provides insights into the debate on whether these models are merely statistical",
    "checked": true,
    "id": "56b90c41b3360c9cfc5936c8d36415064589a638",
    "semantic_title": "how do autoregressive transformers solve full addition?",
    "citation_count": 0,
    "authors": [
      "Wang Peixu",
      "Chen Yu",
      "Yu Ming",
      "Cheng Xiang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.644": {
    "title": "MAIN: Mutual Alignment Is Necessary for instruction tuning",
    "volume": "main",
    "abstract": "Instruction tuning has empowered large language models (LLMs) to achieve remarkable performance, yet its success heavily depends on the availability of large-scale, high-quality instruction-response pairs. To meet this demand, various methods have been developed to synthesize data at scale. However, current methods for scaling up data generation often overlook a crucial aspect: the alignment between instructions and responses. We hypothesize that the quality of instruction-response pairs is determined not by the individual quality of each component, but by the degree of mutual alignment. To address this, we propose a Mutual Alignment Framework (MAIN) which enforces coherence between instructions and responses through mutual constraints. We demonstrate that MAIN generalizes well across model architectures and sizes, achieving state-of-the-art performance on LLaMA, Mistral, and Qwen models across diverse benchmarks. This work underscores the critical role of instruction-response alignment in enabling generalizable and high-quality instruction tuning for LLMs. All code is available from our repository",
    "checked": true,
    "id": "25d3bf42d53b9aa9f4ad43f371e035c1ccaab618",
    "semantic_title": "main: mutual alignment is necessary for instruction tuning",
    "citation_count": 0,
    "authors": [
      "Fanyi Yang",
      "Jianfeng Liu",
      "Xin Zhang",
      "Haoyu Liu",
      "Xixin Cao",
      "Yuefeng Zhan",
      "Hao Sun",
      "Weiwei Deng",
      "Feng Sun",
      "Qi Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.645": {
    "title": "Expanding before Inferring: Enhancing Factuality in Large Language Models through Premature Layers Interpolation",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) demonstrate remarkable capabilities in text understanding and generation. However, their tendency to produce factually inconsistent outputs—commonly referred to as \"hallucinations\"—remains a critical challenge. Existing approaches, such as retrieval-based and inference-time correction methods, primarily address this issue at the input or output level, often overlooking the intrinsic information refinement process and the role of premature layers. Meanwhile, alignment- and fine-tuning-based methods are resource-intensive. In this paper, we propose **PLI** (**P**remature **L**ayers **I**nterpolation), a novel, training-free, and plug-and-play intervention designed to enhance factuality. PLI mitigates hallucinations by inserting premature layers formed through mathematical interpolation with adjacent layers. Inspired by stable diffusion and sampling steps, PLI extends the depth of information processing and transmission in LLMs, improving factual coherence. Experiments on four publicly available datasets demonstrate that PLI effectively reduces hallucinations while outperforming existing baselines in most cases. Further analysis suggests that the success of layer interpolation is closely linked to LLMs' internal mechanisms. To promote reproducibility, we will release our code and data upon acceptance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dingwei Chen",
      "Ziqiang Liu",
      "Feiteng Fang",
      "Chak Tou Leong",
      "Shiwen Ni",
      "Ahmadreza Argha",
      "Hamid Alinejad-Rokny",
      "Min Yang",
      "Chengming Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.646": {
    "title": "DeepWell-Adol: A Scalable Expert-Based Dialogue Corpus for Adolescent Positive Mental Health and Wellbeing Promotion",
    "volume": "main",
    "abstract": "Promoting positive mental health and well-being, especially in adolescents, is a critical yet underexplored area in natural language processing (NLP). Most existing NLP research focuses on clinical therapy or psychological counseling for the general population, which does not adequately address the preventative and growth-oriented needs of adolescents. In this paper, we introduce DeepWell-Adol, a domain-specific Chinese dialogue corpus grounded in positive psychology and coaching, designed to foster adolescents' positive mental health and well-being. To balance the trade-offs between data quality, quantity, and scenario diversity, the corpus comprises two main components: human expert-written seed data (ensuring professional quality) and its mirrored expansion (automatically generated using a two-stage scenario-based augmentation framework). This approach enables large-scale data creation while maintaining domain relevance and reliability. Comprehensive evaluations demonstrate that the corpus meets general standards for psychological dialogue and emotional support, while also showing superior performance across multiple models in promoting positive psychological processes, character strengths, interpersonal relationships, and healthy behaviors. Moreover, the framework proposed for building and evaluating DeepWell-Adol offers a flexible and scalable method for developing domain-specific datasets. It significantly enhances automation and reduces development costs without compromising professional standards—an essential consideration in sensitive areas like adolescent and elderly mental health. We make our dataset publicly available",
    "checked": true,
    "id": "97ca7415153c50d6f7b98751ad8b1a14fe3b7ff1",
    "semantic_title": "deepwell-adol: a scalable expert-based dialogue corpus for adolescent positive mental health and wellbeing promotion",
    "citation_count": 0,
    "authors": [
      "Wenyu Qiu",
      "Yuxiong Wang",
      "Jiajun Tan",
      "Hanchao Hou",
      "Qinda Liu",
      "Wei Yao",
      "Shiguang Ni"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.647": {
    "title": "Data to Defense: The Role of Curation in Aligning Large Language Models Against Safety Compromise",
    "volume": "main",
    "abstract": "Large language models (LLMs) are widely adapted for downstream applications through fine-tuning, a process named customization. However, recent studies have identified a vulnerability during this process, where malicious samples can compromise the robustness of LLMs and amplify harmful behaviors. To address this challenge, we propose an adaptive data curation approach allowing any text to be curated to enhance its effectiveness in counteracting harmful samples during customization. To avoid the need for additional defensive modules, we further introduce a comprehensive mitigation framework spanning the lifecycle of the customization process: before customization to immunize LLMs against future compromise attempts, during customization to neutralize risks, and after customization to restore compromised models. Experimental results demonstrate a significant reduction in compromising effects, achieving up to a 100% success rate in generating safe responses. By combining adaptive data curation with lifecycle-based mitigation strategies, this work represents a solid step forward in mitigating compromising risks and ensuring the secure adaptation of LLMs",
    "checked": true,
    "id": "f6b24c5126b589236a690f68ede5b6e399ce71ca",
    "semantic_title": "data to defense: the role of curation in aligning large language models against safety compromise",
    "citation_count": 0,
    "authors": [
      "Xiaoqun Liu",
      "Jiacheng Liang",
      "Luoxi Tang",
      "Muchao Ye",
      "Weicheng Ma",
      "Zhaohan Xi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.648": {
    "title": "Speculative Safety-Aware Decoding",
    "volume": "main",
    "abstract": "Despite extensive efforts to align large language models (LLMs) with human values and safety rules, jailbreak attacks that exploit certain vulnerabilities continuously emerge, highlighting the need to strengthen existing LLMs with additional safety properties to defend against these attacks. However, tuning large models has become increasingly resource-intensive and may have difficulty ensuring consistent performance. We introduce Speculative Safety-Aware Decoding (SSD), a lightweight decoding-time approach that equips LLMs with the desired safety property while accelerating inference. We assume that there exists a small language model that possesses the desired safety property. SSD integrates speculative sampling during decoding and leverages the match ratio between the small and composite models to quantify jailbreak risks. This enables SSD to dynamically switch between decoding schemes to prioritize utility or safety, to handle the challenge of different model capacities. The output token is then sampled from a new distribution that combines the distributions of both models. Experimental results show that SSD successfully equips the large model with the desired safety property, and also allows the model to remain helpful to benign queries. Furthermore, SSD accelerates the inference time, thanks to the speculative sampling design",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuekang Wang",
      "Shengyu Zhu",
      "Xueqi Cheng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.649": {
    "title": "PanicToCalm: A Proactive Counseling Agent for Panic Attacks",
    "volume": "main",
    "abstract": "Panic attacks are acute episodes of fear and distress, in which timely, appropriate intervention can significantly help individuals regain stability. However, suitable datasets for training such models remain scarce due to ethical and logistical issues. To address this, we introduce Pace, which is a dataset that includes high-distress episodes constructed from first-person narratives, and structured around the principles of Psychological First Aid (PFA). Using this data, we train Pacer, a counseling model designed to provide both empathetic and directive support, which is optimized through supervised learning and simulated preference alignment. To assess its effectiveness, we propose PanicEval, a multi-dimensional framework covering general counseling quality and crisis-specific strategies. Experimental results show that Pacer outperforms strong baselines in both counselor-side metrics and client affect improvement. Human evaluations further confirm its practical value, with Pacer consistently preferred over general, CBT-based, and GPT-4-powered models in panic scenarios",
    "checked": true,
    "id": "aa1e48de26cf1461b2fab6d087778f16a5ebedcc",
    "semantic_title": "panictocalm: a proactive counseling agent for panic attacks",
    "citation_count": 0,
    "authors": [
      "Jihyun Lee",
      "Yejin Min",
      "San Kim",
      "Yejin Jeon",
      "Sung Jun Yang",
      "Hyounghun Kim",
      "Gary Lee"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.650": {
    "title": "CoPL: Collaborative Preference Learning for Personalizing LLMs",
    "volume": "main",
    "abstract": "Personalizing large language models (LLMs) is important for aligning outputs with diverse user preferences, yet existing methods struggle with flexibility and generalization. We propose CoPL (Collaborative Preference Learning), a graph-based collaborative filtering framework that models user-response relationships to enhance preference estimation, particularly in sparse annotation settings. By integrating a mixture of LoRA experts, CoPL efficiently fine-tunes LLMs while dynamically balancing shared and user-specific preferences. Additionally, an optimization-free adaptation strategy enables generalization to unseen users without fine-tuning. Experiments on TL;DR, UltraFeedback-P, and PersonalLLM datasets demonstrate that CoPL outperforms existing personalized reward models, effectively capturing both common and controversial preferences, making it a scalable solution for personalized LLM alignment",
    "checked": true,
    "id": "4d41946f2f8d1fafa12edd284fde140455ccffff",
    "semantic_title": "copl: collaborative preference learning for personalizing llms",
    "citation_count": 0,
    "authors": [
      "Youngbin Choi",
      "Seunghyuk Cho",
      "Minjong Lee",
      "MoonJeong Park",
      "Yesong Ko",
      "Jungseul Ok",
      "Dongwoo Kim"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.651": {
    "title": "Dynamic Collaboration of Multi-Language Models based on Minimal Complete Semantic Units",
    "volume": "main",
    "abstract": "This paper investigates the enhancement of reasoning capabilities in language models through token-level multi-model collaboration. Our approach selects the optimal tokens from the next token distributions provided by multiple models to perform autoregressive reasoning. Contrary to the assumption that more models yield better results, we introduce a distribution distance-based dynamic selection strategy (DDS) to optimize the multi-model collaboration process. To address the critical challenge of vocabulary misalignment in multi-model collaboration, we propose the concept of minimal complete semantic units (MCSU), which is simple yet enables multiple language models to achieve natural alignment within the linguistic space. Experimental results across various benchmarks demonstrate the superiority of our method. The codes will be released soon",
    "checked": true,
    "id": "c2d46058f0e918ef11c8a366e01dbcc5a89a77fd",
    "semantic_title": "dynamic collaboration of multi-language models based on minimal complete semantic units",
    "citation_count": 0,
    "authors": [
      "Chao Hao",
      "Zezheng Wang",
      "Yanhua Huang",
      "Ruiwen Xu",
      "Wenzhe Niu",
      "Xin Liu",
      "Zitong Yu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.652": {
    "title": "AI Chatbots as Professional Service Agents: Developing a Professional Identity",
    "volume": "main",
    "abstract": "With the rapid expansion of large language model (LLM) applications, there is an emerging shift in the role of LLM-based AI chatbots from serving merely as general inquiry tools to acting as professional service agents. However, current studies often overlook a critical aspect of professional service agents: the act of communicating in a manner consistent with their professional identities. This is of particular importance in the healthcare sector, where effective communication with patients is essential for achieving professional goals, such as promoting patient well-being by encouraging healthy behaviors. To bridge this gap, we propose LAPI (LLM-based Agent with a Professional Identity), a novel framework for designing professional service agent tailored for medical question-and-answer (Q&A) services, ensuring alignment with a specific professional identity. Our method includes a theory-guided task planning process that decomposes complex professional tasks into manageable subtasks aligned with professional objectives and a pragmatic entropy method designed to generate professional and ethical responses with low uncertainty. Experiments on various LLMs show that the proposed approach outperforms baseline methods, including few-shot prompting, chain-of-thought prompting, across key metrics such as fluency, naturalness, empathy, patient-centricity, and ROUGE-L scores. Additionally, the ablation study underscores the contribution of each component to the overall effectiveness of the approach",
    "checked": true,
    "id": "dfa8ac3354969fa701d501e5004baa33ec60e0e4",
    "semantic_title": "ai chatbots as professional service agents: developing a professional identity",
    "citation_count": 2,
    "authors": [
      "Wenwen Li",
      "Kangwei Shi",
      "Yidong Chai"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.653": {
    "title": "DeepResonance: Enhancing Multimodal Music Understanding via Music-centric Multi-way Instruction Tuning",
    "volume": "main",
    "abstract": "Recent advancements in music large language models (LLMs) have significantly improved music understanding tasks, which involve the model's ability to analyze and interpret various musical elements. These improvements primarily focused on integrating both music and text inputs. However, the potential of incorporating additional modalities such as images, videos and textual music features to enhance music understanding remains unexplored. To bridge this gap, we propose DeepResonance, a multimodal music understanding LLM fine-tuned via multi-way instruction tuning with multi-way aligned music, text, image, and video data. To this end, we construct Music4way-MI2T, Music4way-MV2T, and Music4way-Any2T, three 4-way training and evaluation datasets designed to enable DeepResonance to integrate both visual and textual music feature content. We also introduce multi-sampled ImageBind embeddings and a pre-LLM fusion Transformer to enhance modality fusion prior to input into text LLMs, tailoring for multi-way instruction tuning. Our model achieves state-of-the-art performances across six music understanding tasks, highlighting the benefits of the auxiliary modalities and the structural superiority of DeepResonance. We open-source the codes, models and datasets we constructed: https://github.com/sony/DeepResonance",
    "checked": true,
    "id": "fbeca92b7b136b3cc12ca1283a075fdf9d0ae8a2",
    "semantic_title": "deepresonance: enhancing multimodal music understanding via music-centric multi-way instruction tuning",
    "citation_count": 2,
    "authors": [
      "Zhuoyuan Mao",
      "Mengjie Zhao",
      "Qiyu Wu",
      "Hiromi Wakaki",
      "Yuki Mitsufuji"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.654": {
    "title": "Advancing Oversight Reasoning across Languages for Audit Sycophantic Behaviour via X-Agent",
    "volume": "main",
    "abstract": "Large language models (LLMs) have demonstrated capabilities that are highly satisfactory to a wide range of users by adapting to their culture and wisdom. Yet, this can translate into a propensity to produce responses that align with users' viewpoints, even when the latter are wrong. This behaviour is known as sycophancy, the tendency of LLMs to generate misleading responses as long as they align with the user's, inducing bias and reducing reliability. To make interactions consistent, reliable and safe, we introduce X-Agent, an Oversight Reasoning framework that audits human–LLM dialogues, reasons about them, captures sycophancy and corrects the final outputs. Concretely, X-Agent extends debate-based frameworks by (i) auditing human-LLM conversations, (ii) applying a defence layer that steers model behaviour and goes beyond user beliefs, and (iii) extracting reasoning traces from evaluations that serve as training signals for mitigating sycophancy. We evaluate X-Agent across diverse scenarios and languages, showing that it consistently detects sycophancy, reduces unwarranted agreement, and improves cross-turn consistency, advancing a reasoning-as-overview paradigm for safer LLM interaction. Our approach introduces a novel paradigm in which reasoning is not merely a means to solve problems, but as a mechanism for overseeing the problem-solving processes of other models",
    "checked": true,
    "id": "78cd29e79d84386d9c8ea3bf0a7d9b1b23bcb658",
    "semantic_title": "advancing oversight reasoning across languages for audit sycophantic behaviour via x-agent",
    "citation_count": 0,
    "authors": [
      "Giulia Pucci",
      "Leonardo Ranaldi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.655": {
    "title": "CAFE: Retrieval Head-based Coarse-to-Fine Information Seeking to Enhance Multi-Document QA Capability",
    "volume": "main",
    "abstract": "Advancements in Large Language Models (LLMs) have extended their input context length, yet they still struggle with retrieval and reasoning in long-context inputs. Existing methods propose to utilize the prompt strategy and Retrieval-Augmented Generation (RAG) to alleviate this limitation. However, they still face challenges in balancing retrieval precision and recall, impacting their efficacy in answering questions. To address this, we introduce **CAFE**, a two-stage coarse-to-fine method to enhance multi-document question-answering capacities. By gradually eliminating the negative impacts of background and distracting documents, CAFE makes the responses more reliant on the evidence documents. Initially, a coarse-grained filtering method leverages retrieval heads to identify and rank relevant documents. Then, a fine-grained steering method guides attention to the most relevant content. Experiments across benchmarks show that CAFE outperforms baselines, achieving an average SubEM improvement of up to 22.1% and 13.7% over SFT and RAG methods, respectively, across three different models. Our code is available at https://github.com/RUCAIBox/CAFE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Peng",
      "Jinhao Jiang",
      "Zican Dong",
      "Xin Zhao",
      "Lei Fang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.656": {
    "title": "SSA-COMET: Do LLMs Outperform Learned Metrics in Evaluating MT for Under-Resourced African Languages?",
    "volume": "main",
    "abstract": "Evaluating machine translation (MT) quality for under-resourced African languages remains a significant challenge, as existing metrics often suffer from limited language coverage and poor performance in low-resource settings. While recent efforts, such as AfriCOMET, have addressed some of the issues, they are still constrained by small evaluation sets, a lack of publicly available training data tailored to African languages, and inconsistent performance in extremely low-resource scenarios. In this work, we introduce SSA-MTE, a large-scale human-annotated MT evaluation (MTE) dataset covering 13 African language pairs from the News domain, with over 63,000 sentence-level annotations from a diverse set of MT systems. Based on this data, we develop SSA-COMET and SSA-COMET-QE, improved reference-based and reference-free evaluation metrics. We also benchmark prompting-based approaches using state-of-the-art LLMs like GPT-4o and Claude. Our experimental results show that SSA-COMET models significantly outperform AfriCOMET and are competitive with the strongest LLM (Gemini 2.5 Pro) evaluated in our study, particularly on low-resource languages such as Twi, Luo, and Yoruba. All resources are released under open licenses to support future research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Senyu Li",
      "Jiayi Wang",
      "Felermino D. M. A. Ali",
      "Colin Cherry",
      "Daniel Deutsch",
      "Eleftheria Briakou",
      "Rui Sousa-Silva",
      "Henrique Lopes Cardoso",
      "Pontus Stenetorp",
      "David Ifeoluwa Adelani"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.657": {
    "title": "FaithUn: Toward Faithful Forgetting in Language Models by Investigating the Interconnectedness of Knowledge",
    "volume": "main",
    "abstract": "Various studies have attempted to remove sensitive or private knowledge from a language model to prevent its unauthorized exposure. However, prior studies have overlooked the inherent complexity and interconnectedness of knowledge, which requires careful examination. To resolve this problem, we first define a new concept called superficial unlearning, which refers to the phenomenon where an unlearning method either fails to erase the interconnected knowledge it should remove or unintentionally erases irrelevant knowledge. Based on the definition, we introduce a novel benchmark, FaithUn, to analyze and evaluate the faithfulness of unlearning in real-world knowledge QA settings. Furthermore, we propose a novel unlearning method, KLUE, which updates only knowledge-related neurons to achieve faithful unlearning. KLUE leverages a regularized explainability method to localize contextual knowledge neurons, updating only these neurons using carefully selected unforgotten samples. Experimental results demonstrate that existing unlearning methods fail to ensure faithful unlearning, while our method shows significant effectiveness in real-world QA unlearning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nakyeong Yang",
      "Minsung Kim",
      "Seunghyun Yoon",
      "Joongbo Shin",
      "Kyomin Jung"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.658": {
    "title": "Calibrating Pseudo-Labeling with Class Distribution for Semi-supervised Text Classification",
    "volume": "main",
    "abstract": "Semi-supervised text classification (SSTC) aims to train text classification models with few labeled data and massive unlabeled data. Existing studies develop effective pseudo-labeling methods, but they can struggle with unlabeled data that have imbalanced classes mismatched with the labeled data, making the pseudo-labeling biased towards majority classes, resulting in catastrophic error propagation. We believe it is crucial to explicitly estimate the overall class distribution, and use it to calibrate pseudo-labeling to constrain majority classes. To this end, we formulate the pseudo-labeling as an optimal transport (OT) problem, which transports the unlabeled sample distribution to the class distribution. With a memory bank, we dynamically collect both the high-confidence pseudo-labeled data and true labeled data, thus deriving reliable (pseudo-) labels for class distribution estimation. Empirical results on 3 commonly used benchmarks demonstrate that our model is effective and outperforms previous state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiyi Yang",
      "Richong Zhang",
      "Junfan Chen",
      "Jiawei Sheng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.659": {
    "title": "Coarse-to-Fine Grounded Memory for LLM Agent Planning",
    "volume": "main",
    "abstract": "Recent advancements in Large Language Models (LLMs) have driven growing interest in LLM-based agents for complex planning tasks. To avoid costly agent training, many studies adopted memory mechanism that enhances LLM with offline experiences or online trajectory analysis. However, existing works focus on single-granularity memory derived from dynamic environmental interactions, which are inherently constrained by the quality of the collected experiences. This limitation, in turn, constrain the diversity of knowledge and the flexibility of planning. We propose Coarse-to-Fine Grounded Memory (CFGM), a novel framework that grounds coarse-to-fine memories with LLM, thereby fully leverage them for flexible adaptation to diverse scenarios. CFGM grounds environmental information into coarse-grained focus points to guide experience collection in training tasks, followed by grounding of actionable hybrid-grained tips from each experience. At inference, CFGM retrieves task-relevant experiences and tips to support planning. When facing environmental anomalies, the LLM grounds the current situation into fine-grained key information, enabling flexible self-QA reflection and plan correction. Extensive experiments on AlfWorld, Webshop and ScienceWorld demonstrate that CFGM significantly outperforms competitive baselines and comprehensively optimizes memory-enhanced LLM Agent system",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Yang",
      "Jinwei Xiao",
      "Hongming Zhang",
      "Qingyang Zhang",
      "Yanna Wang",
      "Bo Xu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.660": {
    "title": "From A and B to A+B: Can Large Language Models Solve Compositional Math Problems?",
    "volume": "main",
    "abstract": "Large language models (LLMs) have demonstrated strong performance in solving math problems, and there is growing research on evaluating their robustness. Unlike previous studies that create problem variants by adding perturbations to a single problem, this paper focuses on the interaction between problems. Specifically, we combine two original problems with a logical connection to get a new math problem, and measure the LLMs' performance on it to evaluate its compositional generalization, which is an important and essential reasoning capability in human intelligence. The result of experiments that cover 14 different LLMs shows that even when the mathematical essence remains unchanged, a simple form of combination can significantly reduce the performance of LLMs, revealing the limitation of their generalization ability. Additionally, we propose an automated pipeline with 98.2% accuracy to assist in annotating datasets (1 manual, 2 synthetic). The extensive experiments conducted on these datasets further verify the conclusion and obtain some important findings. Finally, we analyze the impact of factors such as difficulty and length on LLMs' performance, offering insights for future research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xisheng Xiao",
      "Hanlin Zhao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.661": {
    "title": "Sycophancy Mitigation Through Reinforcement Learning with Uncertainty-Aware Adaptive Reasoning Trajectories",
    "volume": "main",
    "abstract": "Despite the remarkable capabilities of large language models, current training paradigms inadvertently foster sycophancy—alignment with user-provided information, regardless of factual accuracy. In this paper, we introduce SMART (Sycophancy Mitigation through Adaptive Reasoning Trajectories), reconceptualizing sycophancy as a reasoning optimization problem rather than an output alignment issue. SMART employs a two-stage approach: (1) Uncertainty-Aware Adaptive Monte Carlo Tree Search (UA-MCTS), which dynamically adjusts exploration based on state-level uncertainty; and (2) progress-based reinforcement learning that distills these improved reasoning patterns into model adaptation. Through extensive experiments, we show that SMART significantly outperforms existing baselines in effectively reducing sycophancy while maintaining performance on out-of-distribution inputs. These findings demonstrate the importance of optimizing internal reasoning processes for developing aligned truthful AI assistant",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Beigi",
      "Ying Shen",
      "Parshin Shojaee",
      "Qifan Wang",
      "Zichao Wang",
      "Chandan K. Reddy",
      "Ming Jin",
      "Lifu Huang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.662": {
    "title": "SimVBG: Simulating Individual Values by Backstory Generation",
    "volume": "main",
    "abstract": "As Large Language Models (LLMs) demonstrate increasingly strong human-like capabilities, the need to align them with human values has become significant. Recent advanced techniques, such as prompt learning and reinforcement learning, are being employed to bring LLMs closer to aligning with human values. While these techniques address broad ethical and helpfulness concerns, they rarely consider simulating individualized human values. To bridge this gap, we propose SimVBG, a framework that simulates individual values based on individual backstories that reflect their past experience and demographic information. SimVBG transforms structured data on an individual to a backstory and utilizes a multi-module architecture inspired by the Cognitive–Affective Personality System to simulate individual value based on the backstories. We test SimVBG on a self-constructed benchmark derived from the World Values Survey and show that SimVBG improves top-1 accuracy by more than 10% over the retrieval-augmented generation method. Further analysis shows that performance increases as additional interaction user history becomes available, indicating that the model can refine its persona over time. Code, dataset, and complete experimental results are available at https://github.com/bangdedadi/SimVBG",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bangde Du",
      "Ziyi Ye",
      "Zhijing Wu",
      "Monika A. Jankowska",
      "Shuqi Zhu",
      "Qingyao Ai",
      "Yujia Zhou",
      "Yiqun Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.663": {
    "title": "EvolveSearch: An Iterative Self-Evolving Search Agent",
    "volume": "main",
    "abstract": "The rapid advancement of large language models (LLMs) has transformed the landscape of agentic information seeking capabilities through the integration of tools such as search engines and web browsers. However, current mainstream approaches for enabling LLM web search proficiency face significant challenges: supervised fine-tuning struggles with data production in open-search domains, while RL converges quickly, limiting their data utilization efficiency. To address these issues, we propose EvolveSearch, a novel iterative self-evolution framework that combines SFT and RL to enhance agentic web search capabilities without any external human-annotated reasoning data. Extensive experiments on seven multi-hop question-answering (MHQA) benchmarks demonstrate that EvolveSearch consistently improves performance across iterations, ultimately achieving an average improvement of 4.7% over the current state-of-the-art across seven benchmarks, opening the door to self-evolution agentic capabilities in open web search domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ding-Chu Zhang",
      "Yida Zhao",
      "Jialong Wu",
      "Liwen Zhang",
      "Baixuan Li",
      "Wenbiao Yin",
      "Yong Jiang",
      "Yu-Feng Li",
      "Kewei Tu",
      "Pengjun Xie",
      "Fei Huang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.664": {
    "title": "Syntax-Aware Retrieval Augmentation for Neural Symbolic Regression",
    "volume": "main",
    "abstract": "Symbolic regression is a powerful technique for discovering mathematical expressions that best fit observed data. While neural symbolic regression methods based on large-scale pre-trained models perform well on simple tasks, the reliance on fixed parametric knowledge typically limits their generalization to complex and diverse data distributions. To address this challenge, we propose a syntax-aware retrieval-augmented mechanism that leverages the syntactic structure of symbolic expressions to perform context-aware retrieval from a pre-constructed token datastore during inference. This mechanism enables the model to incorporate highly relevant non-parametric prior information to assist in expression generation. Additionally, we design an entropy-based confidence network that dynamically adjusts the fusion strength between neural and retrieved components by estimating predictive uncertainty. Extensive experiments on multiple symbolic regression benchmarks demonstrate that the proposed method significantly outperforms representative baselines, validating the effectiveness of retrieval augmentation in enhancing the generalization performance of neural symbolic regression models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Canmiao Zhou",
      "Han Huang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.665": {
    "title": "Merge then Realign: Simple and Effective Modality-Incremental Continual Learning for Multimodal LLMs",
    "volume": "main",
    "abstract": "Recent advances in Multimodal Large Language Models (MLLMs) have enhanced their versatility as they integrate a growing number of modalities. Considering the heavy cost of training MLLMs, it is efficient to reuse the existing ones and extend them to more modalities through Modality-incremental Continual Learning (MCL). The exploration of MCL is in its early stages. In this work, we dive into the causes of performance degradation in MCL. We uncover that it suffers not only from forgetting as in traditional continual learning, but also from misalignment between the modality-agnostic and modality-specific components. To this end, we propose an elegantly simple MCL paradigm called \"MErge then ReAlign\" (MERA) to address both forgetting and misalignment. MERA avoids introducing heavy model budgets or modifying model architectures, hence is easy to deploy and highly reusable in the MLLM community. Extensive experiments demonstrate the impressive performance of MERA, holding an average of 99.84% Backward Relative Gain when extending to four modalities, achieving nearly lossless MCL performance. Our findings underscore the misalignment issue in MCL. More broadly, our work showcases how to adjust different components of MLLMs during continual learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dingkun Zhang",
      "Shuhan Qi",
      "Xinyu Xiao",
      "Kehai Chen",
      "Xuan Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.666": {
    "title": "Graceful Forgetting in Generative Language Models",
    "volume": "main",
    "abstract": "Recently, the pretrain-finetune paradigm has become a cornerstone in various deep learning areas. While in general the pre-trained model would promote both effectiveness and efficiency of downstream tasks fine-tuning, studies have shown that not all knowledge acquired during pre-training is beneficial. Some of the knowledge may actually bring detrimental effects to the fine-tuning tasks, which is also known as negative transfer. To address this problem, graceful forgetting has emerged as a promising approach. The core principle of graceful forgetting is to enhance the learning plasticity of the target task by selectively discarding irrelevant knowledge. However, this approach remains underexplored in the context of generative language models, and it is often challenging to migrate existing forgetting algorithms to these models due to architecture incompatibility. To bridge this gap, in this paper we propose a novel framework, Learning With Forgetting (LWF), to achieve graceful forgetting in generative language models. With Fisher Information Matrix weighting the intended parameter updates, LWF computes forgetting confidence to evaluate self-generated knowledge regarding the forgetting task, and consequently, knowledge with high confidence is periodically unlearned during fine-tuning. Our experiments demonstrate that, although thoroughly uncovering the mechanisms of knowledge interaction remains challenging in pre-trained language models, applying graceful forgetting can contribute to enhanced fine-tuning performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chunyang Jiang",
      "Chi-Min Chan",
      "Yiyang Cai",
      "Yulong Liu",
      "Wei Xue",
      "Yike Guo"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.667": {
    "title": "Answering Narrative-Driven Recommendation Queries via a Retrieve–Rank Paradigm and the OCG-Agent",
    "volume": "main",
    "abstract": "Narrative-driven recommendation queries are common in question-answering platforms, AI search engines, social forums, and some domain-specific vertical applications. Users typically submit free-form text requests for recommendations, e.g., \"Any mind-bending thrillers like Shutter Island you'd recommend?\" Such special queries have traditionally been addressed as generic QA task under the RAG paradigm. This work formally introduces narrative recommendation as a distinct task and contends that the RAG paradigm is inherently ill-suited for it, owing to information loss in LLMs when retrieving information from from multiple long and fragmented contexts, and limitations in ranking effectiveness. To overcome these limitations, we propose a novel retrieve-rank paradigm by theoretically demonstrating its superiority over RAG paradigm. Central to this new paradigm, we specially focus on the information retrieval stage and introduce Open-domain Candidate Generation (OCG)-Agent that generatively retrieves structurally adaptive and semantically aligned candidates, ensuring both extensive candidate coverage and high-quality information. We validate effectiveness of new paradigm and OCG-Agent's retrieve mechanism under real-world datasets from Reddit and corporate education-consulting scenarios. Further extensive ablation studies confirming the rationality of each OCG-Agent component",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunxiao Shi",
      "Haoning Shang",
      "Xing Zi",
      "Wujiang Xu",
      "Yue Feng",
      "Min Xu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.668": {
    "title": "Direct Value Optimization: Improving Chain-of-Thought Reasoning in LLMs with Refined Values",
    "volume": "main",
    "abstract": "We introduce Direct Value Optimization (DVO), an innovative offline reinforcement learning framework for enhancing large language models in complex reasoning tasks. Unlike traditional methods relying on preference labels, DVO utilizes value signals at individual reasoning steps, optimizing models via a mean squared error loss. The key benefit of DVO lies in its fine-grained supervision, circumventing the need for labor-intensive human annotations. Target values within the DVO are estimated using either Monte Carlo Tree Search or an outcome value model. Our empirical analysis on 3 math reasoning, 4 commonsense reasoning, and 3 coding tasks shows that DVO consistently outperforms existing offline preference optimization techniques by a significant margin of 4% to 6%, and is competitive to online GRPO but with higher sample efficiency. These findings underscore the importance of value signals in advancing reasoning capabilities and highlight DVO as a superior methodology under scenarios lacking explicit human preference information",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongbo Zhang",
      "Han Cui",
      "Guangsheng Bao",
      "Linyi Yang",
      "Jun Wang",
      "Yue Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.669": {
    "title": "Jailbreak-Tuning: Models Efficiently Learn Jailbreak Susceptibility",
    "volume": "main",
    "abstract": "AI systems are rapidly advancing in capability, and frontier model developers broadly acknowledge the need for safeguards against serious misuse. However, this paper demonstrates that fine-tuning, whether via open weights or closed fine-tuning APIs, can produce helpful-only models with safeguards destroyed. In contrast to prior work which is blocked by modern moderation systems or achieved only partial removal of safeguards or degraded output quality, our jailbreak-tuning method teaches models to generate detailed, high-quality responses to arbitrary harmful requests. For example, OpenAI, Google, and Anthropic models will fully comply with requests for CBRN assistance, executing cyberattacks, and other criminal activity. We further show that backdoors can increase not only the stealth but also the severity of attacks. Stronger jailbreak prompts become even more effective in fine-tuning attacks, linking attacks and potentially defenses in the input and weight spaces. Not only are current models vulnerable, more recent ones also appear to be becoming even more vulnerable to these attacks, underscoring the urgent need for tamper-resistant safeguards. Until such safeguards are discovered, companies and policymakers should view the release of any fine-tunable model as simultaneously releasing its evil twin: equally capable as the original model, and usable for any malicious purpose within its capabilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brendan Murphy",
      "Dillon Bowen",
      "Shahrad Mohammadzadeh",
      "Tom Tseng",
      "Julius Broomfield",
      "Adam Gleave",
      "Kellin Pelrine"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.670": {
    "title": "Neural Topic Modeling via Contextual and Graph Information Fusion",
    "volume": "main",
    "abstract": "Topic modeling is a powerful unsupervised tool for knowledge discovery. However, existing work struggles with generating limited-quality topics that are uninformative and incoherent, which hindering interpretable insights from managing textual data. In this paper, we improve the original variational autoencoder framework by incorporating contextual and graph information to address the above issues. First, the encoder utilizes topic fusion techniques to combine contextual and bag-of-words information well, and meanwhile exploits the constraints of topic alignment and topic sharpening to generate informative topics. Second, we develop a simple word co-occurrence graph information fusion strategy that efficiently increases topic coherence. On three benchmark datasets, our new framework generates more coherent and diverse topics compared to various baselines, and achieves strong performance on both automatic and manual evaluations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiyuan Liu",
      "Jiaxing Yan",
      "Chunjiang Zhu",
      "Xingyu Liu",
      "Li Qing",
      "Yanghui Rao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.671": {
    "title": "CARE: A Disagreement Detection Framework with Concept Alignment and Reasoning Enhancement",
    "volume": "main",
    "abstract": "Disagreement detection is a crucial task in natural language processing (NLP), particularly in analyzing online discussions and social media content. Large language models (LLMs) have demonstrated significant advancements across various NLP tasks. However, the performance of LLM in disagreement detection is limited by two issues: *conceptual gap* and *reasoning gap*. In this paper, we propose a novel two-stage framework, Concept Alignment and Reasoning Enhancement (CARE), to tackle the issues. The first stage, Concept Alignment, addresses the gap between expert and model by performing **sub-concept taxonomy extraction**, aligning the model's comprehension with human experts. The second stage, Reasoning Enhancement, improves the model's reasoning capabilities by introducing curriculum learning workflow, which includes **rationale to critique** and **counterfactual to detection** for reducing spurious association. Extensive experiments on disagreement detection task demonstrate the effectiveness of our framework, showing superior performance in zero-shot and supervised learning settings, both within and across domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiyuan Liu",
      "Jielin Song",
      "Yunhe Pang",
      "Zhiyu Shen",
      "Yanghui Rao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.672": {
    "title": "Beyond Task-Oriented and Chitchat Dialogues: Proactive and Transition-Aware Conversational Agents",
    "volume": "main",
    "abstract": "Conversational agents have traditionally been developed for either task-oriented dialogue (TOD) or open-ended chitchat, with limited progress in unifying the two. Yet, real-world conversations naturally involve fluid transitions between these modes. To address this gap, we introduce TACT (TOD-And-Chitchat Transition), a dataset designed for transition-aware dialogue modeling that incorporates structurally diverse and integrated mode flows. TACT supports both user- and agent-driven mode switches, enabling robust modeling of complex conversational dynamics.To evaluate an agent's ability to initiate and recover from mode transitions, we propose two new metrics—Switch and Recovery.Models trained on TACT outperform baselines in both intent detection and mode transition handling. Moreover, applying Direct Preference Optimization (DPO) to TACT-trained models yields additionalgains, achieving 75.74% joint mode-intent accuracy and a 70.1% win rate against GPT-4o in human evaluation.These results demonstrate that pairing structurally diverse data with DPO enhances response quality and transition control, paving the way for more proactive and transition-aware conversational agents",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yejin Yoon",
      "Yuri Son",
      "Namyeong So",
      "Minseo Kim",
      "Minsoo Cho",
      "Chanhee Park",
      "Seungshin Lee",
      "Taeuk Kim"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.673": {
    "title": "LightThinker: Thinking Step-by-Step Compression",
    "volume": "main",
    "abstract": "Large language models (LLMs) have shown remarkable performance in complex reasoning tasks, but their efficiency is hindered by the substantial memory and computational costs associated with generating lengthy tokens. In this paper, we propose LightThinker, a novel method that enables LLMs to dynamically compress intermediate thoughts during reasoning. Inspired by human cognitive processes, LightThinker compresses verbose thought steps into compact representations and discards the original reasoning chains, thereby significantly reducing the number of tokens stored in the context window.This is achieved by training the model on when and how to perform compression through data construction, mapping hidden states to condensed gist tokens, and creating specialized attention masks. Additionally, we introduce the Dependency (Dep) metric to quantify the degree of compression by measuring the reliance on historical tokens during generation. Extensive experiments on four datasets and two models show that LightThinker reduces peak memory usage and inference time, while maintaining competitive accuracy. Our work provides a new direction for improving the efficiency of LLMs in complex reasoning tasks without sacrificing performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jintian Zhang",
      "Yuqi Zhu",
      "Mengshu Sun",
      "Yujie Luo",
      "Shuofei Qiao",
      "Lun Du",
      "Da Zheng",
      "Huajun Chen",
      "Ningyu Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.674": {
    "title": "How Is LLM Reasoning Distracted by Irrelevant Context? An Analysis Using a Controlled Benchmark",
    "volume": "main",
    "abstract": "We introduce Grade School Math with Distracting Context (GSM-DC), a synthetic benchmark to evaluate Large Language Models' (LLMs) reasoning robustness against systematically controlled irrelevant context (IC). GSM-DC constructs symbolic reasoning graphs with precise distractor injections, enabling rigorous, reproducible evaluation. Our experiments demonstrate that LLMs are significantly sensitive to IC, affecting both reasoning path selection and arithmetic accuracy. Additionally, training models with strong distractors improves performance in both in-distribution and out-of-distribution scenarios. We further propose a stepwise tree search guided by a process reward model, which notably enhances robustness in out-of-distribution conditions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minglai Yang",
      "Ethan Huang",
      "Liang Zhang",
      "Mihai Surdeanu",
      "William Yang Wang",
      "Liangming Pan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.675": {
    "title": "Investigating Pedagogical Teacher and Student LLM Agents: Genetic Adaptation Meets Retrieval-Augmented Generation Across Learning Styles",
    "volume": "main",
    "abstract": "Effective teaching necessitates adapting pedagogical strategies to the inherent diversity of students, encompassing variations in aptitude, learning styles, and personality, a critical challenge in education and teacher training. Large Language Models (LLMs) offer a powerful tool to simulate complex classroom dynamics, providing a controlled environment for exploring optimal teaching patterns. However, existing simulation frameworks often fall short by neglecting comprehensive student modeling beyond basic knowledge states and, more importantly, by lacking mechanisms for teachers to dynamically adapt their approach based on student feedback and collective performance. Addressing these limitations, we propose a simulation framework that integrates LLM-based diverse student agents with a self-evolving teacher agent. We use genetic algorithms to automatically tune and optimize the teacher's pedagogical parameters based on simulated student performance, enabling the teacher agent to discover and refine teaching patterns tailored to specific class characteristics. Complementing this, we introduce Persona-RAG, a novel Retrieval-Augmented Generation method specifically designed for personalized knowledge retrieval in pedagogical contexts, allowing students to retrieve information as per their learning styles. We show how Persona-RAG remains competitive with standard RAG baselines in accurately retrieving relevant information while adding a touch of personalization for students. Crucially, we perform extensive experiments and highlight the different patterns learnt by the teacher agent while optimizing over classes with students of various learning styles. Our work presents a significant step towards creating adaptive educational technologies and improving teacher training through realistic, data-driven simulation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Debdeep Sanyal",
      "Agniva Maiti",
      "Umakanta Maharana",
      "Dhruv Kumar",
      "Ankur Mali",
      "C. Lee Giles",
      "Murari Mandal"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.676": {
    "title": "GeoEdit: Geometric Knowledge Editing for Large Language Models",
    "volume": "main",
    "abstract": "Regular updates are essential for maintaining up-to-date knowledge in large language models (LLMs). However, existing training-based model editing methods often struggle to effectively incorporate new knowledge while preserving unrelated general knowledge. To address this challenge, we propose a novel framework called Geometric Knowledge Editing (GeoEdit). GeoEdit utilizes the geometric relationships of parameter updates from fine-tuning to differentiate between neurons associated with new knowledge updates and those related to general knowledge perturbations. By employing a direction-aware knowledge identification method, we avoid updating neurons with directions approximately orthogonal to existing knowledge, thus preserving the model's generalization ability. For the remaining neurons, we integrate both old and new knowledge for aligned directions and apply a \"forget-then-learn\" editing strategy for opposite directions. Additionally, we introduce an importance-guided task vector fusion technique that filters out redundant information and provides adaptive neuron-level weighting, further enhancing model editing performance. Extensive experiments on two publicly available datasets demonstrate the superiority of GeoEdit over existing state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujie Feng",
      "Li-Ming Zhan",
      "Zexin Lu",
      "Yongxin Xu",
      "Xu Chu",
      "Yasha Wang",
      "Jiannong Cao",
      "Philip S. Yu",
      "Xiao-Ming Wu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.677": {
    "title": "A Generative Pre-Trained Language Model for Channel Prediction in Wireless Communications Systems",
    "volume": "main",
    "abstract": "Channel prediction can greatly reduce the pilot overhead and is a critical technology in the fifth-generation (5G) and the coming 6G wireless communications systems. Conventional model-based channel prediction methods suffer from limited accuracy due to imperfect temporal modeling, while existing AI-based methods suffer from limited generalization due to inadequate training strategies. Recently, large language models (LLMs) have demonstrated remarkable generalization and generation capabilities across diverse domains such as computer vision, quantitative economics, and bioinformatics, which motivates us to apply LLMs in channel prediction. In this paper, we formulate the ‘channel sentence' based on channel correlation, where the channel is regarded as a ‘word'. Subsequently, we propose a generative pre-trained language model for channel prediction (CP-GPT). We collect 12M channel data according to the 3GPP 38.901 protocol and train CP-GPT based on the transformer decoder architecture. Moreover, we design two pre-training tasks based on the characteristics of wireless channels to enhance CP-GPT's understanding of communications channels. We further propose a comprehensive benchmark to rigorously evaluate the capabilities of CP-GPT across multiple dimensions. The simulation results demonstrate that CP-GPT has successfully learned various channel characteristics and exhibits impressive capabilities across numerous downstream tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Lin",
      "Huanming Zhang",
      "Yuhua Jiang",
      "Yucong Wang",
      "Tengyu Zhang",
      "Shaoqiang Yan",
      "Hongyao Li",
      "Yihong Liu",
      "Feifei Gao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.678": {
    "title": "AIMMerging: Adaptive Iterative Model Merging Using Training Trajectories for Language Model Continual Learning",
    "volume": "main",
    "abstract": "Continual learning (CL) is essential for deploying large language models (LLMs) in dynamic real-world environments without the need for costly retraining. Recent model merging-based methods have attracted significant attention, but they still struggle to effectively manage the trade-off between learning new knowledge and preventing forgetting, a challenge largely stemming from suboptimal number of merges and merging frequency. In this paper, we introduce Adaptive Iterative Model Merging (AimMerging), a novel CL framework that utilizes learning and forgetting signals from the training trajectory to dynamically monitor the model's training status. Guided by dynamic monitoring, the training trajectory-guided merge controller adaptively determines the timing and frequency of iterative fusion, while the rehearsal-based knowledge fusion module computes the merging weights and executes the fusion. Comprehensive experiments on three CL benchmarks with various model sizes (from 770M to 13B) demonstrate that AimMerging achieves significant performance improvements over existing state-of-the-art methods, with an average relative improvement of 80% and 59% on FWT and BWT, respectively. The source code is provided for reproducibility",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujie Feng",
      "Jian Li",
      "Xiaoyu Dong",
      "Pengfei Xu",
      "Xiaohui Zhou",
      "Yujia Zhang",
      "Zexin Lu",
      "Yasha Wang",
      "Alan Zhao",
      "Xu Chu",
      "Xiao-Ming Wu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.679": {
    "title": "R-PRM: Reasoning-Driven Process Reward Modeling",
    "volume": "main",
    "abstract": "Process Reward Models (PRMs) have emerged as a promising solution to address the reasoning mistakes of large language models (LLMs). However, existing PRMs typically output evaluation scores directly, limiting both learning efficiency and evaluation accuracy. This limitation is further compounded by the scarcity of annotated data. To address these issues, we propose Reasoning-Driven Process Reward Modeling (R-PRM), which activates inherent reasoning to enhance process-level evaluation. First, we leverage stronger LLMs to generate seed data from limited annotations, effectively activating reasoning capabilities and enabling comprehensive step-by-step evaluation. Second, we explore self-improvement of our PRM through preference optimization, without requiring additional annotated data. Third, we introduce inference time scaling to fully harness our model's reasoning potential. Extensive experiments demonstrate R-PRM's effectiveness: on ProcessBench and PRMBench, it surpasses strong baselines by 13.9 and 8.5 F1 scores. When applied to guide mathematical reasoning, R-PRM achieves consistent accuracy improvements of over 8.6 points across six challenging datasets. Further analysis reveals that R-PRM exhibits more comprehensive evaluation and robust generalization, indicating its broader potential",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuaijie She",
      "Junxiao Liu",
      "Yifeng Liu",
      "Jiajun Chen",
      "Xin Huang",
      "Shujian Huang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.680": {
    "title": "RLAE: Reinforcement Learning-Assisted Ensemble for LLMs",
    "volume": "main",
    "abstract": "Ensembling large language models (LLMs) can effectively combine diverse strengths of different models, offering a promising approach to enhance performance across various tasks. However, existing methods typically rely on fixed weighting strategies that fail to adapt to the dynamic, context-dependent characteristics of LLM capabilities. In this work, we propose **R**einforcement **L**earning-**A**ssisted **E**nsemble for LLMs (RLAE), a novel framework that reformulates LLM ensemble through the lens of a Markov Decision Process (MDP). Our approach introduces a RL agent that dynamically adjusts ensemble weights by considering both input context and intermediate generation states, with the agent being trained using rewards that directly correspond to the quality of final outputs. We implement RLAE using both single-agent and multi-agent reinforcement learning algorithms (RLAE_PPO and RLAE_MAPPO ), demonstrating substantial improvements over conventional ensemble methods. Extensive evaluations on a diverse set of tasks show that RLAE outperforms existing approaches by up to 3.3\\\\% accuracy points, offering a more effective framework for LLM ensembling. Furthermore, our method exhibits superior generalization capabilities across different tasks without the need for retraining, while simultaneously achieving lower time latency. The source code is available at here",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuqian Fu",
      "Yuanheng Zhu",
      "Jiajun Chai",
      "Guojun Yin",
      "Wei Lin",
      "Qichao Zhang",
      "Dongbin Zhao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.681": {
    "title": "Do Large Language Models Truly Grasp Addition? A Rule-Focused Diagnostic Using Two-Integer Arithmetic",
    "volume": "main",
    "abstract": "Large language models (LLMs) achieve impressive results on advanced mathematics benchmarks but sometimes fail on basic arithmetic tasks, raising the question of whether they have truly grasped fundamental arithmetic rules or are merely relying on pattern matching. To unravel this issue, we systematically probe LLMs' understanding of two-integer addition (0 to 264) by testing three crucial properties: commutativity (A+B=B+A), representation invariance via symbolic remapping (e.g., 7 ↦ Y), and consistent accuracy scaling with operand length. Our evaluation of 12 leading LLMs reveals a stark disconnect: while models achieve high numeric accuracy (73.8–99.8%), they systematically fail these diagnostics. Specifically, accuracy plummets to ≤ 7.5% with symbolic inputs, commutativity is violated in up to 20% of cases, and accuracy scaling is non-monotonic. Interventions further expose this pattern-matching reliance: explicitly providing rules degrades performance by 29.49%, while prompting for explanations before answering merely maintains baseline accuracy. These findings demonstrate that current LLMs address elementary addition via pattern matching, not robust rule induction, motivating new diagnostic benchmarks and innovations in model architecture and training to cultivate genuine mathematical reasoning. Our dataset and generating code are available at https://github.com/kuri-leo/llm-arithmetic-diagnostic",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Yan",
      "Yu Lu",
      "Renjun Xu",
      "Zhenzhong Lan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.682": {
    "title": "AskToAct: Enhancing LLMs Tool Use via Self-Correcting Clarification",
    "volume": "main",
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in tool learning. In real-world scenarios, user queries are often ambiguous and incomplete, requiring effective clarification. However, existing interactive clarification approaches face two critical limitations: reliance on manually constructed datasets, which inherently constrains training data scale and diversity, and lack of error correction mechanisms during multi-turn clarification, leading to error accumulation that compromises both accuracy and efficiency. We present AskToAct, which addresses these challenges by exploiting the structural mapping between queries and their tool invocation solutions. Our key insight is that tool parameters naturally represent explicit user intents. By systematically removing key parameters from queries while retaining them as ground truth, we enable automated construction of high-quality training data. We further enhance model robustness through error-correction pairs and selective masking, enabling dynamic error detection during clarification interactions. Comprehensive experiments demonstrate that AskToAct significantly outperforms existing approaches, achieving above 57% accuracy in recovering critical unspecified intents and enhancing clarification efficiency by an average of 10.46% while maintaining high accuracy in tool invocation. Our framework exhibits robust performance across different model architectures and successfully generalizes to entirely unseen APIs without additional training, achieving performance comparable to GPT-4o with substantially fewer computational resources",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Zhang",
      "Yongliang Shen",
      "Zhe Zheng",
      "Linjuan Wu",
      "Wenqi Zhang",
      "Yuchen Yan",
      "Qiuying Peng",
      "Jun Wang",
      "Weiming Lu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.683": {
    "title": "START: Self-taught Reasoner with Tools",
    "volume": "main",
    "abstract": "Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in complex reasoning through long chain-of-thought, yet they struggle with precise computations and algorithmic operations. Integrating computational tools with LRMs remains challenging, particularly in activating and enhancing models' tool-use capabilities without compromising their reasoning strengths. We address these challenges through START (Self-taught Reasoner with Tools), introducing two key innovations: (1) Hint-infer, a training-free approach that activates LRMs' latent tool-use capabilities through artificial hints, enabling test-time performance scaling; (2) Hint-RFT, a self-training framework that enables models to learn effective tool utilization through diverse hint patterns and rejection-based data synthesis. Experiments show that START significantly improves state-of-the-art LRMs across challenging benchmarks, including competition-level mathematics (AMC23: 95.0%, AIME24: 75.6%) and graduate-level science questions (GPQA: 64.6%). Our analysis reveals that START not only enhances accuracy but also improves reasoning efficiency through strategic tool utilization, demonstrating broad applicability in complex reasoning scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengpeng Li",
      "Mingfeng Xue",
      "Zhenru Zhang",
      "Jiaxi Yang",
      "Beichen Zhang",
      "Bowen Yu",
      "Binyuan Hui",
      "Junyang Lin",
      "Xiang Wang",
      "Dayiheng Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.684": {
    "title": "The Impact of Negated Text on Hallucination with Large Language Models",
    "volume": "main",
    "abstract": "Recent studies on hallucination in large language models (LLMs) have been actively progressing in natural language processing. However, the impact of negated text on hallucination with LLMs remains largely unexplored. In this paper, we set three important yet unanswered research questions and aim to address them. To derive the answers, we investigate whether LLMs can recognize contextual shifts caused by negation and still reliably distinguish hallucinations comparable to affirmative cases. We also design the NegHalu dataset by reconstructing existing hallucination detection datasets with negated expressions. Our experiments demonstrate that LLMs struggle to detect hallucinations in negated text effectively, often producing logically inconsistent or unfaithful judgments. Moreover, we trace the internal state of LLMs as they process negated inputs at the token level and reveal the challenges of mitigating their unintended effects",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaehyung Seo",
      "Hyeonseok Moon",
      "Heuiseok Lim"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.685": {
    "title": "A Probabilistic Inference Scaling Theory for LLM Self-Correction",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have demonstrated the capability to refine their generated answers through self-correction, enabling continuous performance improvement over multiple rounds. However, the mechanisms underlying how and why accuracy evolves during this iterative process remain unexplored. To fill this gap, we propose a probabilistic theory to model the dynamics of accuracy change and explain the performance improvements observed in multi-round self-correction. Through mathematical derivation, we establish that the accuracy after the tth round of self-correction is given by: Acct = Upp - 𝛼t(Upp - Acc0),where Acc0 denotes the initial accuracy, Upp represents the upper bound of accuracy convergence, and 𝛼 determines the rate of convergence. Based on our theory, these parameters can be calculated and the predicted accuracy curve then can be obtained through only a single round of self-correction. Extensive experiments across diverse models and datasets demonstrate that our theoretical predictions align closely with empirical accuracy curves, validating the effectiveness of the theory. Our work provides a theoretical foundation for understanding LLM self-correction, thus paving the way for further explorations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhe Yang",
      "Yichang Zhang",
      "Yudong Wang",
      "Ziyao Xu",
      "Junyang Lin",
      "Zhifang Sui"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.686": {
    "title": "MentalGLM Series: Explainable Large Language Models for Mental Health Analysis on Chinese Social Media",
    "volume": "main",
    "abstract": "With the rise of mental health challenges, social media has become a key platform for emotional expression. Deep learning offers a promising solution for analyzing mental health but lacks flexibility and interpretability. Large language models (LLMs) introduce greater adaptability and can explain their decisions, yet they still underperform deep learning in complex psychological analysis. We present C-IMHI, the first multi-task Chinese social media interpretable mental health instruction dataset (9K samples) with quality control and manual validation. Additionally, we introduce MentalGLM, the first open-source Chinese LLMs for explainable mental health analysis, trained on 50K instructions. The proposed models excelled in three mental health downstream tasks, outperforming or matching deep learning and LLMs. A portion of the generated decision explanations was validated by experts, demonstrating promising accuracy and reliability. We evaluated the proposed models on a clinical dataset, where they significantly outperformed other LLMs, demonstrating their potential for clinical applications. Our models show strong performance, validated across tasks and domains. The decision explanations enhance usability and facilitate better understanding and practical application of the models. Both the constructed dataset and the models are publicly available via: https://github.com/zwzzzQAQ/MentalGLM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Zhai",
      "Nan Bai",
      "Qing Zhao",
      "Jianqiang Li",
      "Fan Wang",
      "Hongzhi Qi",
      "Meng Jiang",
      "Xiaoqin Wang",
      "Bing Xiang Yang",
      "Guanghui Fu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.687": {
    "title": "Knowledge-Aware Co-Reasoning for Multidisciplinary Collaboration",
    "volume": "main",
    "abstract": "Large language models (LLMs) have shown significant potential to improve diagnostic performance for clinical professionals. Existing multi-agent paradigms rely mainly on prompt engineering, suffering from improper agent selection and insufficient knowledge integration. In this work, we propose a novel framework KACR (Knowledge-Aware Co-Reasoning) that integrates structured knowledge reasoning into multidisciplinary collaboration from two aspects: (1) a reinforcement learning-optimized agent that uses clinical knowledge graphs to guide dynamic discipline determination; (2) a multidisciplinary collaboration strategy that enables robust consensus through integration of domain-specific expertise and interdisciplinary persuasion mechanism. Extensive experiments conducted on both academic and real-world datasets demonstrate the effectiveness of our method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xurui Li",
      "Wanghaijiao",
      "Kaisong Song",
      "Rui Zhu",
      "Haixu Tang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.688": {
    "title": "Astra: Efficient Transformer Architecture and Contrastive Dynamics Learning for Embodied Instruction Following",
    "volume": "main",
    "abstract": "Vision-language-action models have gained significant attention for their ability to model multimodal sequences in embodied instruction following tasks. However, most existing models rely on causal attention, which we find suboptimal for processing sequences composed of interleaved segments from different modalities. In this paper, we introduce Astra, a novel Transformer architecture featuring trajectory attention and learnable action queries, designed to efficiently process segmented multimodal trajectories and predict actions for imitation learning. Furthermore, we propose a contrastive dynamics learning objective to enhance the model's understanding of environment dynamics and multimodal alignment, complementing the primary behavior cloning objective. Through extensive experiments on three large-scale robot manipulation benchmarks, Astra demonstrates substantial performance improvements over previous models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yueen Ma",
      "DaFeng Chi",
      "Shiguang Wu",
      "Yuecheng Liu",
      "Yuzheng Zhuang",
      "Irwin King"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.689": {
    "title": "MAVL: A Multilingual Audio-Video Lyrics Dataset for Animated Song Translation",
    "volume": "main",
    "abstract": "Lyrics translation requires both accurate semantic transfer and preservation of musical rhythm, syllabic structure, and poetic style. In animated musicals, the challenge intensifies due to alignment with visual and auditory cues. We introduce Multilingual Audio-Video Lyrics Benchmark for Animated Song Translation (MAVL), the first multilingual, multimodal benchmark for singable lyrics translation. By integrating text, audio, and video, MAVL enables richer and more expressive translations than text-only approaches. Building on this, we propose Syllable-Constrained Audio-Video LLM with Chain-of-Thought (SylAVL-CoT), which leverages audio-video cues and enforces syllabic constraints to produce natural-sounding lyrics. Experimental results demonstrate that SylAVL-CoT significantly outperforms text-based models in singability and contextual accuracy, emphasizing the value of multimodal, multilingual approaches for lyrics translation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Woohyun Cho",
      "Youngmin Kim",
      "Sunghyun Lee",
      "Youngjae Yu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.690": {
    "title": "MuTIS: Enhancing Reasoning Efficiency through Multi Turn Intervention Sampling in Reinforcement Learning",
    "volume": "main",
    "abstract": "Recently, large reasoning models (LRMs) have demonstrated state-of-the-art performance across a wide range of benchmarks. However, a common challenge for these models is the \"overthinking\" problem, which leads to excessive reasoning steps and significant computational overhead. Furthermore, the issues with long Chain-of-Thought (CoT) are especially pronounced in smaller models (≤ 3B parameters). Aside from producing excessively verbose \"reflection words\", they often exhibit repetition and get trapped in unproductive generation loops. Existing solutions typically involve either using flexible reasoning chains as training data or leveraging the model's latent space to bypass intermediate reasoning steps, but none of these methods have considered directly optimizing reasoning trajectories during the sampling phase of training. In our work, we introduce the Multi-Turn Intervention Sampling Framework (MuTIS). Our framework leverages multi-turn interventions to produce concise reasoning chains. It fine-tunes reasoning models through reinforcement learning, demonstrably breaking the accuracy-efficiency trade-off. It also demonstrates strong scalability, exhibiting excellent performance on 7B models. Code is available at https://github.com/Edric-Zhao/MuTIS/tree/main",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenshuo Zhao",
      "Haoxing Zhai",
      "Xinyu Qiu",
      "Zhenting Qi",
      "Shuhe Li",
      "Linchao Zhu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.691": {
    "title": "PRIM: Towards Practical In-Image Multilingual Machine Translation",
    "volume": "main",
    "abstract": "In-Image Machine Translation (IIMT) aims to translate images containing texts from one language to another. Current research of end-to-end IIMT mainly conducts on synthetic data, with simple background, single font, fixed text position, and bilingual translation, which can not fully reflect real world, causing a significant gap between the research and practical conditions. To facilitate research of IIMT in real-world scenarios, we explore Practical In-Image Multilingual Machine Translation (IIMMT). In order to convince the lack of publicly available data, we annotate the PRIM dataset, which contains real-world captured one-line text images with complex background, various fonts, diverse text positions, and supports multilingual translation directions. We propose an end-to-end model VisTrans to handle the challenge of practical conditions in PRIM, which processes visual text and background information in the image separately, ensuring the capability of multilingual translation while improving the visual quality. Experimental results indicate the VisTrans achieves a better translation quality and visual effect compared to other models. The code and dataset are available at: https://github.com/BITHLP/PRIM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanzhi Tian",
      "Zeming Liu",
      "Zhengyang Liu",
      "Chong Feng",
      "Xin Li",
      "Heyan Huang",
      "Yuhang Guo"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.692": {
    "title": "Mind the Inclusivity Gap: Multilingual Gender-Neutral Translation Evaluation with mGeNTE",
    "volume": "main",
    "abstract": "Avoiding the propagation of undue (binary) gender inferences and default masculine language remains a key challenge towards inclusive multilingual technologies, particularly when translating into languages with extensive gendered morphology. Gender-neutral translation (GNT) represents a linguistic strategy towards fairer communication across languages. However, research on GNT is limited to a few resources and language pairs. To address this gap, we introduce mGeNTE, an expert-curated resource, and use it to conduct the first systematic multilingual evaluation of inclusive translation with state-of-the-art instruction-following language models (LMs). Experiments on en-es/de/it/el reveal that while models can recognize when neutrality is appropriate, they cannot consistently produce neutral translations, limiting their usability. To probe this behavior, we enrich our evaluation with interpretability analyses that identify task-relevant features and offer initial insights into the internal dynamics of LM-based GNT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Beatrice Savoldi",
      "Giuseppe Attanasio",
      "Eleonora Cupin",
      "Eleni Gkovedarou",
      "Janiça Hackenbuchner",
      "Anne Lauscher",
      "Matteo Negri",
      "Andrea Piergentili",
      "Manjinder Thind",
      "Luisa Bentivogli"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.693": {
    "title": "DiplomacyAgent: Do LLMs Balance Interests and Ethical Principles in International Events?",
    "volume": "main",
    "abstract": "The widespread deployment of large language models (LLMs) across various domains has made their safety a critical priority. Inspired by think-tank decision-making philosophy, we propose DiplomacyAgent, an LLM-based multi-agent system for diplomatic position analysis. With DiplomacyAgent, we are able to systematically assess how LLMs balance \"interests\" against \"ethical principles\" when addressing various international events, hence understanding the safety implications of LLMs in diplomacy. Specifically, this will help to assess the consistency of LLM stance with widely recognized ethical standards, as well as the potential risks or ideological biases that may arise. Through integrated quantitative metrics, our research uncovers unexpected decision-making patterns in LLM responses to sensitive issues including human rights protection, environmental sustainability, regional conflicts, etc. It discloses that LLMs could exhibit a strong bias towards interests, leading to unsafe decisions that violate ethical and moral principles. Our experiment results suggest that deploying LLMs in high-stakes domains, particularly in the formulation of diplomatic policies, necessitates a comprehensive assessment of potential ethical and social implications, as well as the implementation of stringent safety protocols",
    "checked": true,
    "id": "042527cf922c53bcaa831699276205a110c42fbb",
    "semantic_title": "diplomacyagent: do llms balance interests and ethical principles in international events?",
    "citation_count": 0,
    "authors": [
      "Jianxiang Peng",
      "Ling Shi",
      "Xinwei Wu",
      "Hanwen Zhang",
      "Fujiang Liu",
      "Haocheng Lyu",
      "Deyi Xiong"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.694": {
    "title": "DisLoRA: Task-specific Low-Rank Adaptation via Orthogonal Basis from Singular Value Decomposition",
    "volume": "main",
    "abstract": "Parameter-efficient fine-tuning (PEFT) of large language models (LLMs) is critical for adapting to diverse downstream tasks with minimal computational cost. We propose **Di**rectional-**S**VD **Lo**w-**R**ank **A**daptation (DisLoRA), a novel PEFT framework that leverages singular value decomposition (SVD) to decompose pretrained weight matrices into orthogonal backbone and task-specific subspaces, enabling precise capture of task-specific directions (TSDs). By dynamically identifying TSDs and employing adaptive soft orthogonal regularization with mean-normalization mechanism, DisLoRA balances task-specific and orthogonal losses without manual tuning, ensuring robust training stability. Extensive experiments on GLUE and Commonsense Reasoning benchmarks demonstrate that DisLoRA surpasses established PEFT methods, including LoRA, PiSSA, DoRA, LoRA-Dash, and SORSA. DisLoRA achieves superior performance on multiple individual GLUE datasets, surpassing baselines by up to 10.28% on SST-2 and 3.28% on CoLA, and consistently attains higher average accuracy than baselines across Commonsense Reasoning Tasks, with a maximum gain of 3.1%. These results demonstrate DisLoRA's performance in efficient and high-performing LLM adaptation for domain-specific tasks while preserving generalization",
    "checked": true,
    "id": "5f7772fb48410ea7187a392a89ef9034129954bd",
    "semantic_title": "dislora: task-specific low-rank adaptation via orthogonal basis from singular value decomposition",
    "citation_count": 0,
    "authors": [
      "She Yifei",
      "Xinhao Wei",
      "Yulong Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.695": {
    "title": "Unmasking Deceptive Visuals: Benchmarking Multimodal Large Language Models on Misleading Chart Question Answering",
    "volume": "main",
    "abstract": "Misleading visualizations, which manipulate chart representations to support specific claims, can distort perception and lead to incorrect conclusions. Despite decades of research, they remain a widespread issue, posing risks to public understanding and raising safety concerns for AI systems involved in data-driven communication. While recent multimodal large language models (MLLMs) show strong chart comprehension abilities, their capacity to detect and interpret misleading charts remains unexplored. We introduce Misleading ChartQA benchmark, a large-scale multimodal dataset designed to evaluate MLLMs on misleading chart reasoning. It contains 3,026 curated examples spanning 21 misleader types and 10 chart types, each with standardized chart code, CSV data, multiple-choice questions, and labeled explanations, validated through iterative MLLM checks and exhausted expert human review. We benchmark 24 state-of-the-art MLLMs, analyze their performance across misleader types and chart formats, and propose a novel region-aware reasoning pipeline that enhances model accuracy. Our work lays the foundation for developing MLLMs that are robust, trustworthy, and aligned with the demands of responsible visual communication",
    "checked": true,
    "id": "d02f166a16f9cd635ed60e32d5e6c0e746a78dfc",
    "semantic_title": "unmasking deceptive visuals: benchmarking multimodal large language models on misleading chart question answering",
    "citation_count": 4,
    "authors": [
      "Zixin Chen",
      "Sicheng Song",
      "KaShun Shum",
      "Yanna Lin",
      "Rui Sheng",
      "Weiqi Wang",
      "Huamin Qu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.696": {
    "title": "Textual Aesthetics in Large Language Models",
    "volume": "main",
    "abstract": "Image aesthetics is a crucial metric in the field of image generation. However, textual aesthetics has not been sufficiently explored. With the widespread application of large language models (LLMs), previous work has primarily focused on the correctness of content and the helpfulness of responses. Nonetheless, providing responses with textual aesthetics is also an important factor for LLMs, which can offer a cleaner layout and ensure greater consistency and coherence in content. In this work, we introduce a pipeline for aesthetics polishing and help construct a textual aesthetics dataset named TEXAES. We propose a textual aesthetics-powered fine-tuning method based on direct preference optimization, termed TAPO, which leverages textual aesthetics without compromising content correctness. Additionally, we develop two evaluation methods for textual aesthetics based on text and image analysis, respectively.Our experiments demonstrate that using textual aesthetics data and employing the TAPO fine-tuning method not only improves aesthetic scores but also enhances performance on general evaluation datasets such as AlpacalEval and Arena-hard",
    "checked": true,
    "id": "aff9c734f236d517af67c4412e68952e48a68933",
    "semantic_title": "textual aesthetics in large language models",
    "citation_count": 0,
    "authors": [
      "Lingjie Jiang",
      "Shaohan Huang",
      "Xun Wu",
      "Furu Wei"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.697": {
    "title": "Section-Level Simplification of Biomedical Abstracts",
    "volume": "main",
    "abstract": "Cochrane produces systematic reviews whose abstracts are divided into seven standard sections. However, the plain language summaries (PLS) of Cochrane reviews do not adhere to the same structure, which has prevented researchers from training simplification models on paired abstract and PLS sections. In this work, we devise a two-step method to automatically divide PLS of Cochrane reviews into the same sections in which abstracts are divided. In the first step, we align each sentence in a PLS to a section in the parallel abstract if they cover similar content. In the second step, we classify the remaining sentences into sections based on the content of the PLS and what we learned from the first step. We manually divide 22 PLS into sections to evaluate our method. Upon execution of our method, we obtain the Cochrane-sections dataset, which consists of paired abstract and PLS sections in English for a total of 7.7K Cochrane reviews. Thus, our work yields references for the section-level simplification of biomedical abstracts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan Bakker",
      "Jaap Kamps"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.698": {
    "title": "PoseStitch-SLT: Linguistically Inspired Pose-Stitching for End-to-End Sign Language Translation",
    "volume": "main",
    "abstract": "Sign language translation remains a challenging task due to the scarcity of large-scale, sentence-aligned datasets. Prior arts have focused on various feature extraction and architectural changes to support neural machine translation for sign languages. We propose PoseStitch-SLT, a novel pre-training scheme that is inspired by linguistic-templates-based sentence generation technique. With translation comparison on two sign language datasets, How2Sign and iSign, we show that a simple transformer-based encoder-decoder architecture outperforms the prior art when considering template-generated sentence pairs in training. We achieve BLEU-4 score improvements from 1.97 to 4.56 on How2Sign and from 0.55 to 3.43 on iSign, surpassing prior state-of-the-art methods for pose-based gloss-free translation. The results demonstrate the effectiveness of template-driven synthetic supervision in low-resource sign language settings",
    "checked": true,
    "id": "eb7ea015570e832aeb2ef00b90a687bce4c07495",
    "semantic_title": "posestitch-slt: linguistically inspired pose-stitching for end-to-end sign language translation",
    "citation_count": 0,
    "authors": [
      "Abhinav Joshi",
      "Vaibhav Sharma",
      "Sanjeet Singh",
      "Ashutosh Modi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.699": {
    "title": "Few-Shot Open-Set Classification via Reasoning-Aware Decomposition",
    "volume": "main",
    "abstract": "Large language models (LLMs) excel at few-shot learning, but their ability to reject out-of-distribution examples remains under-explored. We study this challenge under the setting of few-shot open-set classification, where a model must not only classify examples from a small set of seen classes but also reject unseen ones at inference time. This setting is more realistic and challenging than traditional closed-set supervised learning, requiring both fine-grained classification and robust rejection. We show that, for small LLMs, neither chain-of-thought (CoT) prompting nor supervised fine-tuning (SFT) alone are sufficient to generalise reliably, particularly when class semantics are anonymised. We introduce Wasserstein GFN (W-GFN), a novel amortised Generative Flow Network framework that uses latent trajectories to approximate the Bayesian posterior. With as few as 4 examples per class, W-GFN substantially improves performance, enabling Llama 3.2 3B to achieve up to ≥80% of the performance of Llama 3.3 70B in complex datasets, despite being ∼ 23 times smaller, which highlights the importance of reasoning-aware approaches for robust open-set few-shot learning",
    "checked": true,
    "id": "faef872f37ee1e9e62c35ae7bf825ecf662cb70c",
    "semantic_title": "few-shot open-set classification via reasoning-aware decomposition",
    "citation_count": 0,
    "authors": [
      "Avyav Kumar Singh",
      "Helen Yannakoudakis"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.700": {
    "title": "Translation in the Hands of Many: Centering Lay Users in Machine Translation Interactions",
    "volume": "main",
    "abstract": "Converging societal and technical factors have transformed language technologies into user-facing applications used by the general public across languages. Machine Translation (MT) has become a global tool, with cross-lingual services now also supported by dialogue systems powered by multilingual Large Language Models (LLMs). Widespread accessibility has extended MT's reach to a vast base of *lay users*, many with little to no expertise in the languages or the technology itself. And yet, the understanding of MT consumed by such a diverse group of users—their needs, experiences, and interactions with multilingual systems—remains limited. In our position paper, we first trace the evolution of MT user profiles, focusing on non-experts and how their engagement with technology may shift with the rise of LLMs. Building on an interdisciplinary body of work, we identify three factors—usability, trust, and literacy—that are central to shaping user interactions and must be addressed to align MT with user needs. By examining these dimensions, we provide insights to guide the progress of more user-centered MT",
    "checked": false,
    "id": "1388e2fda16c7540251037175db2576ce27ab1a5",
    "semantic_title": "translation in the hands of many:centering lay users in machine translation interactions",
    "citation_count": 8,
    "authors": [
      "Beatrice Savoldi",
      "Alan Ramponi",
      "Matteo Negri",
      "Luisa Bentivogli"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.701": {
    "title": "iTool: Reinforced Fine-Tuning with Dynamic Deficiency Calibration for Advanced Tool Use",
    "volume": "main",
    "abstract": "Augmenting large language models (LLMs) with external tools is a promising approach to enhance their capabilities, especially for complex tasks. Synthesizing tool-use data through real-world simulations is an effective way to achieve this. However, our investigation reveals that training gains significantly decay as synthetic data increases. The model struggles to benefit from more synthetic data, and it can not equip the model with advanced tool-use capabilities in complex scenarios. Moreover, we discovered that the above limitation usually manifests as a fragment deficiency (i.e., parameter errors) in response. To this end, we propose an iterative reinforced fine-tuning strategy designed to alleviate this limitation. This strategy involves: (1) enhancing the diversity of response for synthetic data through path exploration of Monte Carlo Tree Search. (2) iteratively pinpointing the model's deficiency by constructing fine-grained preference pairs, and then improving it by preference optimization algorithms for targeted improvement. The experiments show that our method achieves 13.11% better performance than the same-size base model. It achieves an improvement of 6.5% in complex scenarios compared to the baseline, and it also outperforms larger open-source and closed-source models",
    "checked": true,
    "id": "dec2983a58e97c4e5e7aedfa4d42e688fae25473",
    "semantic_title": "itool: reinforced fine-tuning with dynamic deficiency calibration for advanced tool use",
    "citation_count": 5,
    "authors": [
      "Yirong Zeng",
      "Xiao Ding",
      "Yuxian Wang",
      "Weiwen Liu",
      "Yutai Hou",
      "Wu Ning",
      "Xu Huang",
      "Duyu Tang",
      "Dandan Tu",
      "Bing Qin",
      "Ting Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.702": {
    "title": "Transplant Then Regenerate: A New Paradigm for Text Data Augmentation",
    "volume": "main",
    "abstract": "Data augmentation is a critical technique in deep learning. Traditional methods like Back-translation typically focus on lexical-level rephrasing, which primarily produces variations with the same semantics. While large language models (LLMs) have enhanced text augmentation by their \"knowledge emergence\" capability, controlling the style and structure of these outputs remains challenging and requires meticulous prompt engineering. In this paper, we propose LMTransplant, a novel text augmentation paradigm leveraging LLMs. The core idea of LMTransplant is transplant-then-regenerate: incorporating seed text into a context expanded by LLM, and asking the LLM to regenerate a variant based on the expanded context. This strategy allows the model to create more diverse and creative content-level variants by fully leveraging the knowledge embedded in LLMs, while preserving the core attributes of the original text. We evaluate LMTransplant across various text-related tasks, demonstrating its superior performance over existing text augmentation methods. Moreover, LMTransplant demonstrates exceptional scalability as the size of augmented data grows",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangzhan Wang",
      "Hongyu Zhang",
      "Beijun Shen",
      "Xiaodong Gu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.703": {
    "title": "Compositional Generalisation for Explainable Hate Speech Detection",
    "volume": "main",
    "abstract": "Hate speech detection is key to online content moderation, but current models struggle to generalise beyond their training data. This has been linked to dataset biases and the use of sentence-level labels, which fail to teach models the underlying structure of hate speech. In this work, we show that even when models are trained with more fine-grained, span-level annotations (e.g., \"artists\" is labeled as target and \"are parasites\" as dehumanising comparison), they struggle to disentangle the meaning of these labels from the surrounding context. As a result, combinations of expressions that deviate from those seen during training remain particularly difficult for models to detect. We investigate whether training on a dataset where expressions occur with equal frequency across all contexts can improve generalisation. To this end, we create U-PLEAD, a dataset of ~364,000 synthetic posts, along with a novel compositional generalisation benchmark of ~8,000 manually validated posts. Training on a combination of U-PLEAD and real data improves compositional generalisation while achieving state-of-the-art performance on the human-sourced PLEAD",
    "checked": true,
    "id": "16622ae2d9dd18d011993e16e9b47950d062f0ba",
    "semantic_title": "compositional generalisation for explainable hate speech detection",
    "citation_count": 0,
    "authors": [
      "Agostina Calabrese",
      "Tom Sherborne",
      "Björn Ross",
      "Mirella Lapata"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.704": {
    "title": "CCQA: Generating Question from Solution Can Improve Inference-Time Reasoning in SLMs",
    "volume": "main",
    "abstract": "Recently, inference-time reasoning strategies have further improved the accuracy of large language models (LLMs), but their effectiveness on smaller models remains unclear. Based on the observation that conventional approaches often fail to improve performance in this context, we propose Cycle-Consistency in Question Answering (CCQA), a novel reasoning method that can be effectively applied to SLMs. Inspired by cycle consistency, CCQA generates a question from each reasoning path and answer, evaluates each by its similarity to the original question, and then selects the candidate solution with the highest similarity score as the final response. Since conventional SLMs struggle to generate accurate questions from their own reasoning paths and answers, we employ a lightweight Flan-T5 model specialized for question generation to support this process efficiently. From the experimental results, it is verified that CCQA consistently outperforms existing state-of-the-art (SOTA) methods across eight models on mathematical and commonsense reasoning benchmarks. Furthermore, our method establishes a new practical baseline for efficient reasoning in SLMs. Source code can be found at https://github.com/scai-research/ccqa_official",
    "checked": true,
    "id": "a99e3606f11173dd295025e5554ab875e86eabc9",
    "semantic_title": "ccqa: generating question from solution can improve inference-time reasoning in slms",
    "citation_count": 0,
    "authors": [
      "Jinyoung Kim",
      "Ji Won Yoon"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.705": {
    "title": "TVQACML: Benchmarking Text-Centric Visual Question Answering in Multilingual Chinese Minority Languages",
    "volume": "main",
    "abstract": "Text-Centric Visual Question Answering (TEC-VQA) is a critical research area that requires semantic interactions between objects and scene texts. However, most existing TEC-VQA benchmarks focus on high-resource languages like English and Chinese. Although few works expanding multilingual QA pairs in non-text-centric VQA datasets through translation, which encounters a substantial \"visual-textual misalignment\" problem when applied to TEC-VQA. Moreover, the open-source nature of these benchmarks and the broad sources of training data for MLLMs have inevitably led to benchmark contamination, resulting in unreliable evaluation results. To alleviate this issue, we propose a contamination-free and more challenging TEC-VQA benchmark called Text-Centric Visual Question Answering in Multilingual Chinese Minority Languages(TVQACML), which involves eight languages, including Standard Chinese, Korean, and six minority languages. TVQACML supports a wide range of tasks, such as Text Recognition, Scene Text-Centric VQA, Document-Oriented VQA, Key Information Extraction (KIE), and Handwritten Mathematical Expression Recognition (HMER), featuring 32,000 question-answer pairs across 8,000 images. Extensive experiments on TVQACML across multiple MLLMs demonstrate the effectiveness of evaluating the MLLMs and enhancing multilingual TEC-VQA performance with fine-tuning",
    "checked": true,
    "id": "1afaa843b22f1cb89e868f2d4c3c59f19e6c5411",
    "semantic_title": "tvqacml: benchmarking text-centric visual question answering in multilingual chinese minority languages",
    "citation_count": 0,
    "authors": [
      "Sha Jiu",
      "Yu Weng",
      "Mengxiao Zhu",
      "Chong Feng",
      "Zheng Liu",
      "Jialedongzhu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.706": {
    "title": "Transparent and Coherent Procedural Mistake Detection",
    "volume": "main",
    "abstract": "Procedural mistake detection (PMD) is a challenging problem of classifying whether a human user (observed through egocentric video) has successfully executed a task (specified by a procedural text). Despite significant recent efforts, machine performance in the wild remains nonviable, and the reasoning processes underlying this performance are opaque. As such, we extend PMD to require generating visual self-dialog rationales to inform decisions. Given the impressive, mature image understanding capabilities observed in recent vision-and-language models (VLMs), we curate a suitable benchmark dataset for PMD based on individual frames. As our reformulation enables unprecedented transparency, we leverage a natural language inference (NLI) model to formulate two automated metrics for the coherence of generated rationales. We establish baselines for this reframed task, showing that VLMs struggle off-the-shelf, but with some trade-offs, their accuracy, coherence, and efficiency can be improved by incorporating these metrics into common inference and fine-tuning methods. Lastly, our multi-faceted metrics visualize common outcomes, highlighting areas for further improvement",
    "checked": true,
    "id": "b1349bafd2294d0b8809b3f0a377efa283cd839b",
    "semantic_title": "transparent and coherent procedural mistake detection",
    "citation_count": 0,
    "authors": [
      "Shane Storks",
      "Itamar Bar-Yossef",
      "Yayuan Li",
      "Zheyuan Zhang",
      "Jason J Corso",
      "Joyce Chai"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.707": {
    "title": "Teaching Your Models to Understand Code via Focal Preference Alignment",
    "volume": "main",
    "abstract": "Preference learning extends the performance of Code LLMs beyond traditional supervised fine-tuning by leveraging relative quality comparisons. In existing approaches, a set of n candidate solutions is evaluated based on test case success rates, with the candidate demonstrating a higher pass rate being labeled as positive and its counterpart with a lower pass rate as negative. However, because this approach aligns entire failing code blocks rather than pinpointing specific errors, it lacks the granularity necessary to capture meaningful error-correction relationships. As a result, the model is unable to learn more informative error-correction patterns. To address these issues, we propose Target-DPO, a new preference alignment framework that mimics human iterative debugging to refine Code LLMs. Target-DPO explicitly locates error regions and aligns the corresponding tokens via a tailored DPO algorithm. To facilitate it, we introduce the CodeFlow dataset, where samples are iteratively refined until passing tests, with modifications capturing error corrections. Extensive experiments show that a diverse suite of Code LLMs equipped with Target-DPO achieves significant performance gains in code generation and improves on challenging tasks like BigCodeBench. In-depth analysis reveals that Target-DPO yields fewer errors. Code, model and datasets are in: https://github.com/JieWu02/Target-DPO",
    "checked": true,
    "id": "05ab61cb986ee3778f1a5628176af3976f097ef9",
    "semantic_title": "teaching your models to understand code via focal preference alignment",
    "citation_count": 2,
    "authors": [
      "Jie Wu",
      "Haoling Li",
      "Xin Zhang",
      "Xiao Liu",
      "Yangyu Huang",
      "Jianwen Luo",
      "Yizhen Zhang",
      "Zuchao Li",
      "Ruihang Chu",
      "Yujiu Yang",
      "Scarlett Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.708": {
    "title": "MoLoRAG: Bootstrapping Document Understanding via Multi-modal Logic-aware Retrieval",
    "volume": "main",
    "abstract": "Document Understanding is a foundational AI capability with broad applications, and Document Question Answering (DocQA) is a key evaluation task. Traditional methods convert the document into text for processing by Large Language Models (LLMs), but this process strips away critical multi-modal information like figures. While Large Vision-Language Models (LVLMs) address this limitation, their constrained input size makes multi-page document comprehension infeasible. Retrieval-augmented generation (RAG) methods mitigate this by selecting relevant pages, but they rely solely on semantic relevance, ignoring logical connections between pages and the query, which is essential for reasoning and accurate answers.To this end, we propose MoLoRAG, a logic-aware retrieval framework for multi-modal, multi-page document understanding. By constructing a page graph that captures contextual relationships between pages, a lightweight VLM performs graph traversal to retrieve relevant pages, including those with logical connections often overlooked. This approach combines semantic and logical relevance to deliver more accurate retrieval. After retrieval, the top-K pages are fed into arbitrary LVLMs for question answering. To enhance flexibility, MoLoRAG offers two variants: a training-free solution for easy deployment and a fine-tuned version to improve logical relevance checking. Experiments on four DocQA datasets demonstrate average improvements of 9.68% in accuracy over LVLM direct inference and 7.44% in retrieval precision over baselines. Codes and datasets are released at https://github.com/WxxShirley/MoLoRAG",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xixi Wu",
      "Yanchao Tan",
      "Nan Hou",
      "Ruiyang Zhang",
      "Hong Cheng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.709": {
    "title": "Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptions",
    "volume": "main",
    "abstract": "Contrastively-trained Vision-Language Models (VLMs), such as CLIP, have become the standard approach for learning discriminative vision-language representations. However, these models often exhibit shallow language understanding, manifesting bag-of-words behaviour. These limitations are reinforced by their dual-encoder design, which induces a modality gap. Additionally, the reliance on vast web-collected data corpora for training makes the process computationally expensive and introduces significant privacy concerns. To address these limitations, in this work, we challenge the necessity of vision encoders for retrieval tasks by introducing a vision-free, single-encoder retrieval pipeline. Departing from the traditional text-to-image retrieval paradigm, we migrate to a text-to-text paradigm with the assistance of VLLM-generated structured image descriptions. We demonstrate that this paradigm shift has significant advantages, including a substantial reduction of the modality gap, improved compositionality, and better performance on short and long caption queries, all attainable with only two hours of calibration on two GPUs. Additionally, substituting raw images with textual descriptions introduces a more privacy-friendly alternative for retrieval. To further assess generalisation and address some of the shortcomings of prior compositionality benchmarks, we release two benchmarks derived from Flickr30k and COCO, containing diverse compositional queries made of short captions, which we coin subFlickr and subCOCO. Our vision-free retriever matches and often surpasses traditional multimodal models. Importantly, our approach achieves state-of-the-art zero-shot performance on multiple retrieval and compositionality benchmarks, with models as small as 0.3B parameters",
    "checked": true,
    "id": "e3a425b832babd353674f24911a37293d8059aed",
    "semantic_title": "vision-free retrieval: rethinking multimodal search with textual scene descriptions",
    "citation_count": 0,
    "authors": [
      "Ioanna Ntinou",
      "Alexandros Xenos",
      "Yassine Ouali",
      "Adrian Bulat",
      "Georgios Tzimiropoulos"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.710": {
    "title": "TableRAG: A Retrieval Augmented Generation Framework for Heterogeneous Document Reasoning",
    "volume": "main",
    "abstract": "Retrieval-Augmented Generation (RAG) has demonstrated considerable effectiveness in open-domain question answering. However, when applied to heterogeneous documents, comprising both textual and tabular components, existing RAG approaches exhibit critical limitations. The prevailing practice of flattening tables and chunking strategies disrupts the intrinsic tabular structure, leads to information loss, and undermines the reasoning capabilities of LLMs in multi-hop, global queries. To address these challenges, we propose TableRAG, an SQL-based framework that unifies textual understanding and complex manipulations over tabular data. TableRAG iteratively operates in four steps: context-sensitive query decomposition, text retrieval, SQL programming and execution, and compositional intermediate answer generation. We also develop HeteQA, a novel benchmark designed to evaluate the multi-hop heterogeneous reasoning capabilities. Experimental results demonstrate that TableRAG consistently outperforms existing baselines on both public datasets and our HeteQA, establishing a new state-of-the-art for heterogeneous document question answering",
    "checked": true,
    "id": "8b0606d354d1452c9893b08f991a2da0f8ea4580",
    "semantic_title": "tablerag: a retrieval augmented generation framework for heterogeneous document reasoning",
    "citation_count": 9,
    "authors": [
      "Xiaohan Yu",
      "Pu Jian",
      "Chong Chen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.711": {
    "title": "Retrieval Enhanced Feedback via In-context Neural Error-book",
    "volume": "main",
    "abstract": "Recent advancements in Large Language Models (LLMs) have significantly improved reasoning capabilities, with in-context learning (ICL) emerging as a key technique for adaptation without retraining. While previous works have focused on leveraging correct examples, recent research highlights the importance of learning from errors to enhance performance. However, existing methods lack a structured framework for analyzing and mitigating errors, particularly in Multimodal Large Language Models (MLLMs), where integrating visual and textual inputs adds complexity. To address this issue, we propose REFINE: Retrieval-Enhanced Feedback via In-context Neural Error-book, a teacher-student framework that systematically structures errors and provides targeted feedback. REFINE introduces three systematic queries to construct structured feedback—Feed-Target, Feed-Check, and Feed-Path—to enhance multimodal reasoning by prioritizing relevant visual information, diagnosing critical failure points, and formulating corrective actions. Unlike prior approaches that rely on redundant retrievals, REFINE optimizes structured feedback retrieval, improving inference efficiency, token usage, and scalability. Our results demonstrate substantial speedup, reduced computational costs, and successful generalization, highlighting REFINE's potential for enhancing multimodal reasoning",
    "checked": true,
    "id": "2c959a61cd3bec5ac6f83ec78914cf8e11ef74df",
    "semantic_title": "retrieval enhanced feedback via in-context neural error-book",
    "citation_count": 0,
    "authors": [
      "Jongyeop Hyun",
      "Bumsoo Kim"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.712": {
    "title": "Improve LLM-as-a-Judge Ability as a General Ability",
    "volume": "main",
    "abstract": "LLM-as-a-Judge leverages the generative and reasoning capabilities of large language models (LLMs) to evaluate LLM responses across diverse scenarios, providing accurate preference signals. This approach plays a vital role in aligning LLMs with human values. Recent studies have raised many methods to train LLM as generative judges, but most of them are data consuming or lack accuracy, and only focus on LLM's judge ability. In this work, we conceptualize judging ability as a general capability of LLMs and adapt the two-stage SFT-DPO training framework—commonly used in traditional general model training—to the development of judge models. We introduce an efficient data synthesis method, which includes the automatic generation of various judge templates, dual verification for data accuracy and consistency. A difficulty-based data stratification strategy allows us to distribute more effective data to the SFT and DPO stages respectively. Experimental results demonstrate that our approach, utilizing only about 2% to 40% of the data required by other methods, achieves SOTA performance on RewardBench. Furthermore, our training method enhances the general capabilities of the model by constructing complicated judge task with CoT outputs. We further validate the effectiveness of our model by deploying it to provide reward signals in a real-world RLHF scenarios. We will open-source our model weights and training data to facilitate further research",
    "checked": true,
    "id": "350403f479a8c3b2983b576f1f56d8d19aac876f",
    "semantic_title": "improve llm-as-a-judge ability as a general ability",
    "citation_count": 18,
    "authors": [
      "Jiachen Yu",
      "Shaoning Sun",
      "Xiaohui Hu",
      "Jiaxu Yan",
      "Kaidong Yu",
      "Xuelong Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.713": {
    "title": "G2: Guided Generation for Enhanced Output Diversity in LLMs",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have demonstrated exceptional performance across diverse natural language processing tasks. However, these models exhibit a critical limitation in output diversity, often generating highly similar content across multiple attempts. This limitation significantly affects tasks requiring diverse outputs, from creative writing to reasoning. Existing solutions, like temperature scaling, enhance diversity by modifying probability distributions but compromise output quality. We propose Guide-to-Generation (G2), a training-free plug-and-play method that enhances output diversity while preserving generation quality. G2 employs a base generator alongside dual Guides, which guide the generation process through decoding-based interventions to encourage more diverse outputs conditioned on the original query. Comprehensive experiments demonstrate that G2 effectively improves output diversity while maintaining an optimal balance between diversity and quality",
    "checked": true,
    "id": "e7cf4f7d6d3579a65a89dccd46fe1624eb181041",
    "semantic_title": "g2: guided generation for enhanced output diversity in llms",
    "citation_count": 0,
    "authors": [
      "Zhiwen Ruan",
      "Yixia Li",
      "Yefeng Liu",
      "Yun Chen",
      "Weihua Luo",
      "Peng Li",
      "Yang Liu",
      "Guanhua Chen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.714": {
    "title": "ToolSafety: A Comprehensive Dataset for Enhancing Safety in LLM-Based Agent Tool Invocations",
    "volume": "main",
    "abstract": "LLMs are evolving into assistants that leverage tools, significantly expanding their capabilities but also introducing critical safety risks. Current models exhibit notable vulnerabilities, particularly in maintaining safety during multi-step tool interactions and in scenarios involving indirect harm. This paper introduces ToolSafety, a safety fine-tuning dataset designed to address these limitations. ToolSafety comprises 5,668 direct harm samples, 4,311 indirect harm samples, and 4,311 multi-step samples. Key features include support for multi-step safety through synthesized trajectories and realistic, context-aware sample generation. We fine-tuned LLaMA3.1-8B-Instruct and Qwen2.5-7B-Instruct using ToolSafety. Experimental results demonstrate that these models effectively maintain safety in multi-step and indirect harm scenarios. Further analysis into superficial alignment across different decoding strategies, languages, and jailbreak prompts indicates that while some risks persist, the issue is less severe than in multi-step settings. Overall, our approach significantly improves safety across various scenarios with small impact on helpfulness, positioning ToolSafety as a valuable resource for building safer tool-using AI systems",
    "checked": true,
    "id": "4ed0b9f8a24531121bd737f0dbaee1cf2959f2c0",
    "semantic_title": "toolsafety: a comprehensive dataset for enhancing safety in llm-based agent tool invocations",
    "citation_count": 0,
    "authors": [
      "Yuejin Xie",
      "Youliang Yuan",
      "Wenxuan Wang",
      "Fan Mo",
      "Jianmin Guo",
      "Pinjia He"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.715": {
    "title": "Learning to See through Sound: From VggCaps to Multi2Cap for Richer Automated Audio Captioning",
    "volume": "main",
    "abstract": "Automated Audio Captioning (AAC) aims to generate natural language descriptions of audio content, enabling machines to interpret and communicate complex acoustic scenes. However, current AAC datasets often suffer from short and simplistic captions, limiting model expressiveness and semantic depth. To address this, we introduce **VggCaps**, a new multi-modal dataset that pairs audio with corresponding video and leverages large language models (LLMs) to generate rich, descriptive captions. VggCaps significantly outperforms existing benchmarks in caption length, lexical diversity, and human-rated quality. Furthermore, we propose **Multi2Cap**, a novel AAC framework that learns audio-visual representations through a AV-grounding module during pre-training and reconstructs visual semantics using audio alone at inference. This enables visually grounded captioning in audio-only scenarios. Experimental results on Clotho and AudioCaps demonstrate that Multi2Cap achieves state-of-the-art performance across multiple metrics, validating the effectiveness of cross-modal supervision and LLM-based generation in advancing AAC",
    "checked": true,
    "id": "82262a2f180f0c26d4c946c672fc95617fc5cc16",
    "semantic_title": "learning to see through sound: from vggcaps to multi2cap for richer automated audio captioning",
    "citation_count": 0,
    "authors": [
      "Sangyeon Cho",
      "Mingi Kim",
      "Jinkwon Hwang",
      "Jaehoon Go",
      "Minuk Ma",
      "Sunjae Yoon",
      "Junyeong Kim"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.716": {
    "title": "Towards Optimal Evaluation Efficiency for Large Language Models",
    "volume": "main",
    "abstract": "Comprehensive evaluation of large language models (LLMs) typically requires large-scale benchmarks, which is costly in terms of both data annotation and computational resource needed for evaluation. To mitigate these challenges, we propose an efficient evaluation framework that selects a question subset based on pre-tested results, thereby reducing the costs. We formulate the subset selection problem as an optimization task, solved using optimal random sampling and simulated annealing algorithms. We compare our approach with prior clustering-based methods and assess their reliability in terms of score accuracy. Additionally, we perform semantic analysis and evaluate whether the selected subsets preserve the semantic information of the original benchmark using Wasserstein distance. Experimental results show that our method outperforms previous approaches in terms of reliability, as measured by L2 norm. Our study provides an optimized perspective for balancing evaluation efficiency and reliability in LLM assessments, while revealing the relationship between optimization methods and semantic retention",
    "checked": true,
    "id": "707fb15c1cf5176539b285cfa588cb3abdfc7d9e",
    "semantic_title": "towards optimal evaluation efficiency for large language models",
    "citation_count": 0,
    "authors": [
      "Guohong Li",
      "Deyi Xiong"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.717": {
    "title": "MMAPG: A Training-Free Framework for Multimodal Multi-hop Question Answering via Adaptive Planning Graphs",
    "volume": "main",
    "abstract": "Multimodal Multi-hop question answering requires integrating information from diverse sources, such as images and texts, to derive answers. Existing methods typically rely on sequential retrieval and reasoning, where each step builds on the previous output. However, this single-path paradigm makes them vulnerable to errors due to misleading intermediate steps. Moreover, developing multimodal models can be computationally expensive, often requiring extensive training. To address these limitations, we propose a training-free framework guided by an Adaptive Planning Graph, which consists of planning, retrieval and reasoning modules. The planning module analyzes the current state of the Adaptive Planning Graph, determines the next action and where to expand the graph, which enables dynamic and flexible exploration of reasoning paths. To handle retrieval of text to unspecified target modalities, we devise modality-specific strategies that dynamically adapt to distinct data types. Our approach preserves the characteristics of multimodal information without costly task-specific training, enabling seamless integration with up-to-date models. Finally, the experiments on MultimodalQA and WebQA show that our approach matches or outperforms existing models that rely on training",
    "checked": true,
    "id": "e3a9f690eaccde79853811d7b245b38dae27c3cc",
    "semantic_title": "mmapg: a training-free framework for multimodal multi-hop question answering via adaptive planning graphs",
    "citation_count": 1,
    "authors": [
      "Yiheng Hu",
      "Xiaoyang Wang",
      "Qing Liu",
      "Xiwei Xu",
      "Qian Fu",
      "Wenjie Zhang",
      "Liming Zhu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.718": {
    "title": "Mixture-of-Clustered-Experts: Advancing Expert Specialization and Generalization in Instruction Tuning",
    "volume": "main",
    "abstract": "A sparse Mixture-of-Experts (MoE) architecture has emerged as a highly scalable solution by conditionally activating sub-modules without a proportional increase in computational costs. However, improving expert specialization to enhance performance and generalization remains a challenge for MoE, especially in instruction tuning scenarios characterized by significant input heterogeneity. In this work, we propose the Mixture-of-Clustered-Experts (MoCE) to address this limitation through a dual-stage routing mechanism. The first stage in the mechanism performs expert group routing based on sequence-level features, while the second stage activates the top-k experts within the group at the token level. This approach enables the effective partitioning of heterogeneous inputs based on their knowledge requirements, encouraging expert group specialization while maintaining the advantages of token-level routing. We evaluate MoCE across a comprehensive set of benchmarks, demonstrating its consistent superiority over strong baselines and its enhanced generalization capabilities. Detailed analysis further highlights the robustness and effectiveness of MoCE",
    "checked": true,
    "id": "6a1874e3b732b39cff1454cb17e6fa583c2be8ba",
    "semantic_title": "mixture-of-clustered-experts: advancing expert specialization and generalization in instruction tuning",
    "citation_count": 0,
    "authors": [
      "Sugyeong Eo",
      "Jung Jun Lee",
      "Chanjun Park",
      "Heuiseok Lim"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.719": {
    "title": "Process-Supervised Reinforcement Learning for Code Generation",
    "volume": "main",
    "abstract": "Existing reinforcement learning (RL) strategies based on outcome supervision have proven effective in enhancing the performance of large language models (LLMs) for code generation. While reinforcement learning based on process supervision shows great potential in multi-step reasoning tasks, its effectiveness in the field of code generation still lacks sufficient exploration and verification. The primary obstacle stems from the resource-intensive nature of constructing a high-quality process-supervised reward dataset, which requires substantial human expertise and computational resources. To overcome this challenge, this paper proposes a \"mutation/refactoring-execution verification\" strategy. Specifically, the teacher model is used to mutate and refactor the statement lines or blocks, and the execution results of the compiler are used to automatically label them, thus generating a process-supervised reward dataset. Based on this dataset, we have carried out a series of RL experiments. The experimental results show that, compared with the method relying only on outcome supervision, reinforcement learning based on process supervision performs better in handling complex code generation tasks. In addition, this paper for the first time confirms the advantages of the Direct Preference Optimization (DPO) method in the RL task of code generation based on process supervision, providing new ideas and directions for code generation research",
    "checked": true,
    "id": "7ad25d4e9c2e60bde200bb730c83126bb85def14",
    "semantic_title": "process-supervised reinforcement learning for code generation",
    "citation_count": 9,
    "authors": [
      "Yufan Ye",
      "Ting Zhang",
      "Wenbin Jiang",
      "Hua Huang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.720": {
    "title": "MuCAL: Contrastive Alignment for Preference-Driven KG-to-Text Generation",
    "volume": "main",
    "abstract": "We propose MuCAL (Multilingual Contrastive Alignment Learning) to tackle the challenge of Knowledge Graphs (KG)-to-Text generation using preference learning, where reliable preference data is scarce. MuCAL is a multilingual KG/Text alignment model achieving robust cross-modal retrieval across multiple languages and difficulty levels. Building on MuCAL, we automatically create preference data by ranking candidate texts from three LLMs (Qwen2.5, DeepSeek-v3, Llama-3). We then apply Direct Preference Optimization (DPO) on these preference data, bypassing typical reward modelling steps to directly align generation outputs with graph semantics. Extensive experiments on KG-to-English Text generation show two main advantages: (1) Our KG/text similarity models provide a better signal for DPO than similar existing metrics, and (2) significantly better generalisation on out-of-domain datasets compared to standard instruction tuning. Our results highlight MuCAL's effectiveness in supporting preference learning for KG-to-English Text generation and lay the foundation for future multilingual extensions. Code and data are available at https://github.com/MeloS7/MuCAL_DPO/tree/main",
    "checked": true,
    "id": "e1f59f242f985ab7adfa3874d7939f711aa97625",
    "semantic_title": "mucal: contrastive alignment for preference-driven kg-to-text generation",
    "citation_count": 0,
    "authors": [
      "Yifei Song",
      "Claire Gardent"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.721": {
    "title": "Advancing Fine-Grained Visual Understanding with Multi-Scale Alignment in Multi-Modal Models",
    "volume": "main",
    "abstract": "Multi-modal large language models (MLLMs) have achieved remarkable success in fine-grained visual understanding across a range of tasks. However, they often encounter significant challenges due to inadequate alignment for fine-grained knowledge, which restricts their ability to accurately capture local details and attain a comprehensive global perception. While recent advancements have focused on aligning object expressions with grounding information, they typically lack explicit integration of object images, which contain affluent information beyond mere texts or coordinates. To bridge this gap, we introduce a novel fine-grained visual knowledge alignment method that effectively aligns and integrates multi-scale knowledge of objects, including texts, coordinates, and images. This innovative method is underpinned by our multi-scale fine-grained enhancement data synthesis pipeline, which provides over 300K essential training data to enhance alignment and improve overall performance. Furthermore, we present TinyGroundingGPT, a series of compact models optimized for high-level alignments. With a scale of approximately 3B parameters, TinyGroundingGPT achieves outstanding results in grounding tasks while delivering performance comparable to larger MLLMs in complex visual scenarios",
    "checked": true,
    "id": "f684aa1e48982f22d7f7bb831b06298582a63a48",
    "semantic_title": "advancing fine-grained visual understanding with multi-scale alignment in multi-modal models",
    "citation_count": 7,
    "authors": [
      "Wei Wang",
      "Zhaowei Li",
      "Qi Xu",
      "Linfeng Li",
      "YiQing Cai",
      "Botian Jiang",
      "Hang Song",
      "Xingcan Hu",
      "Pengyu Wang",
      "Li Xiao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.722": {
    "title": "Thought calibration: Efficient and confident test-time scaling",
    "volume": "main",
    "abstract": "Reasoning large language models achieve impressive test-time scaling by thinking for longer, but this performance gain comes at significant compute cost. Directly limiting test-time budget hurts overall performance, but not all problems are equally difficult. We propose thought calibration to decide dynamically when thinking can be terminated. To calibrate our decision rule, we view a language model's growing body of thoughts as a nested sequence of reasoning trees, where the goal is to identify the point at which novel reasoning plateaus. We realize this framework through lightweight probes that operate on top of the language model's hidden representations, which are informative of both the reasoning structure and overall consistency of response. Based on three reasoning language models and four datasets, thought calibration preserves model performance with up to a 60% reduction in thinking tokens on in-distribution data, and up to 20% in out-of-distribution data",
    "checked": true,
    "id": "c9b7ca0b54c05ddfbd2543e01c0aa02cf197b493",
    "semantic_title": "thought calibration: efficient and confident test-time scaling",
    "citation_count": 0,
    "authors": [
      "Menghua Wu",
      "Cai Zhou",
      "Stephen Bates",
      "Tommi Jaakkola"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.723": {
    "title": "Can LLMs Reason Abstractly Over Math Word Problems Without CoT? Disentangling Abstract Formulation From Arithmetic Computation",
    "volume": "main",
    "abstract": "Final-answer-based metrics are commonly used for evaluating large language models (LLMs) on math word problems, often taken as proxies for reasoning ability. However, such metrics conflate two distinct sub-skills: abstract formulation (capturing mathematical relationships using expressions) and arithmetic computation (executing the calculations). Through a disentangled evaluation on GSM8K and SVAMP, we find that the final-answer accuracy of Llama-3 and Qwen2.5 (1B-32B) without CoT is overwhelmingly bottlenecked by the arithmetic computation step and not by the abstract formulation step. Contrary to the common belief, we show that CoT primarily aids in computation, with limited impact on abstract formulation. Mechanistically, we show that these two skills are composed conjunctively even in a single forward pass without any reasoning steps via an abstract-then-compute mechanism: models first capture problem abstractions, then handle computation. Causal patching confirms these abstractions are present, transferable, composable, and precede computation. These behavioural and mechanistic findings highlight the need for disentangled evaluation to accurately assess LLM reasoning and to guide future improvements",
    "checked": true,
    "id": "56a63ff0b3505afccca63e07e99bedffea2a728c",
    "semantic_title": "can llms reason abstractly over math word problems without cot? disentangling abstract formulation from arithmetic computation",
    "citation_count": 2,
    "authors": [
      "Ziling Cheng",
      "Meng Cao",
      "Leila Pishdad",
      "Yanshuai Cao",
      "Jackie CK Cheung"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.724": {
    "title": "QCRD: Quality-guided Contrastive Rationale Distillation for Large Language Models",
    "volume": "main",
    "abstract": "The deployment of large language models (LLMs) faces considerable challenges concerning resource constraints and inference efficiency. Recent research has increasingly focused on smaller, task-specific models enhanced by distilling knowledge from LLMs. However, prior studies have often overlooked the diversity and quality of knowledge, especially the untapped potential of negative knowledge. Constructing effective negative knowledge remains severely understudied. In this paper, we introduce a novel framework called quality-guided contrastive rationale distillation aimed at enhancing reasoning capabilities through contrastive knowledge learning. For positive knowledge, we enrich its diversity through temperature sampling and employ self-consistency for further denoising and refinement. For negative knowledge, we propose an innovative self-adversarial approach that generates low-quality rationales by sampling previous iterations of smaller language models, embracing the idea that one can learn from one's own weaknesses. A contrastive loss is developed to distill both positive and negative knowledge into smaller language models, where an online-updating discriminator is integrated to assess qualities of rationales and assign them appropriate weights, optimizing the training process. Through extensive experiments across multiple reasoning tasks, we demonstrate that our method consistently outperforms existing distillation techniques, yielding higher-quality rationales",
    "checked": true,
    "id": "e47b4c1212ed22504531cb782adf49f4bc6fb8ba",
    "semantic_title": "qcrd: quality-guided contrastive rationale distillation for large language models",
    "citation_count": 2,
    "authors": [
      "Wei Wang",
      "Zhaowei Li",
      "Qi Xu",
      "YiQing Cai",
      "Hang Song",
      "Qi Qi",
      "Ran Zhou",
      "Zhida Huang",
      "Tao Wang",
      "Li Xiao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.725": {
    "title": "SHARP: Steering Hallucination in LVLMs via Representation Engineering",
    "volume": "main",
    "abstract": "Despite their impressive capabilities, Large Vision-Language Models (LVLMs) frequently generate responses that are plausible but incorrect or unsupported—commonly referred to as hallucinations. In this study, we investigate whether different types of hallucinations are reflected in the model's internal representations by probing their encoded features. We focus on two key causes of hallucination in multimodal reasoning: (1) over-reliance on textual priors and (2) preference for user prompts over conflicting visual evidence—factors identified in prior work as frequent and impactful. Our probing results reveal that hallucinations exhibit distinguishable representational patterns, suggesting the potential for a representation-level approach to characterize and mitigate them. Motivated by these findings, we propose Steering HAllucination via RePresentation Engineering (SHARP), a representation-level intervention framework that modulates hallucination-related features during inference. SHARP identifies functional representations responsible for prior-driven biases and visual-context conflicts, and jointly adjusts the model's internal activations in real time. We evaluate our approach extensively on three large vision-language models across multiple benchmarks. Experimental results demonstrate that SHARP effectively reduces hallucinations while preserving the performance and generalization capabilities of LVLMs",
    "checked": true,
    "id": "14061b23a2aec184f879a6c41a578b13638d4f25",
    "semantic_title": "sharp: steering hallucination in lvlms via representation engineering",
    "citation_count": 0,
    "authors": [
      "Junfei Wu",
      "Yue Ding",
      "Guofan Liu",
      "Tianze Xia",
      "Ziyue Huang",
      "Dianbo Sui",
      "Qiang Liu",
      "Shu Wu",
      "Liang Wang",
      "Tieniu Tan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.726": {
    "title": "Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech",
    "volume": "main",
    "abstract": "Spoken dialogue systems increasingly employ large language models (LLMs) to leverage their advanced reasoning capabilities. However, direct application of LLMs in spoken communication often yield suboptimal results due to mismatches between optimal textual and verbal delivery. While existing approaches adapt LLMs to produce speech-friendly outputs, their impact on reasoning performance remains underexplored. In this work, we propose **Think-Verbalize-Speak**, a framework that decouples reasoning from spoken delivery to preserve the full reasoning capacity of LLMs. Central to our method is *verbalizing*, an intermediate step that translates thoughts into natural, speech-ready text. We also introduce **ReVerT**, a latency-efficient verbalizer based on incremental and asynchronous summarization. Experiments across multiple benchmarks show that our method enhances speech naturalness and conciseness with minimal impact on reasoning. The project page with the dataset and the source code is available at https://yhytoto12.github.io/TVS-ReVerT",
    "checked": true,
    "id": "1204cc04cc7043d0700d4b724f01e053d8ef9e7d",
    "semantic_title": "think, verbalize, then speak: bridging complex thoughts and comprehensible speech",
    "citation_count": 0,
    "authors": [
      "Tony Woo",
      "Sehun Lee",
      "Kang-wook Kim",
      "Gunhee Kim"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.727": {
    "title": "Warm Up Before You Train: Unlocking General Reasoning in Resource-Constrained Settings",
    "volume": "main",
    "abstract": "Designing effective reasoning-capable LLMs typically requires training using Reinforcement Learning with Verifiable Rewards (RLVR) or distillation with carefully curated Long Chain of Thoughts (CoT), both of which depend heavily on extensive training data. This creates a major challenge when the amount of quality training data is scarce. We propose a sample-efficient, two-stage training strategy to develop reasoning LLMs under limited supervision. In the first stage, we \"warm up\" the model by distilling Long CoTs from a toy domain, namely, Knights & Knaves (K&K) logic puzzles to acquire general reasoning skills. In the second stage, we apply RLVR to the warmed-up model using a limited set of target-domain examples. Our experiments demonstrate that this two-phase approach offers several benefits: (i) the warmup phase alone facilitates generalized reasoning, leading to performance improvements across a range of tasks, including MATH, HumanEval+, and MMLU-Pro; (ii) When both the base model and the warmed-up model are RLVR trained on the same small dataset (≤100 examples), the warmed-up model consistently outperforms the base model;(iii) Warming up before RLVR training allows a model to maintain cross-domain generalizability even after training on a specific domain; (iv) Introducing warmup in the pipeline improves not only accuracy but also overall sample efficiency during RLVR training. The results in this paper highlight the promise of warmup for building robust reasoning LLMs in data-scarce environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Safal Shrestha",
      "Minwu Kim",
      "Aadim Nepal",
      "Anubhav Shrestha",
      "Keith W. Ross"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.728": {
    "title": "PPTAgent: Generating and Evaluating Presentations Beyond Text-to-Slides",
    "volume": "main",
    "abstract": "Automatically generating presentations from documents is a challenging task that requires accommodating content quality, visual appeal, and structural coherence. Existing methods primarily focus on improving and evaluating the content quality in isolation, overlooking visual appeal and structural coherence, which limits their practical applicability. To address these limitations, we propose PPTAgent, which comprehensively improves presentation generation through a two-stage, edit-based approach inspired by human workflows. PPTAgent first analyzes reference presentations to extract slide-level functional types and content schemas, then drafts an outline and iteratively generates editing actions based on selected reference slides to create new slides. To comprehensively evaluate the quality of generated presentations, we further introduce PPTEval, an evaluation framework that assesses presentations across three dimensions: Content, Design, and Coherence. Results demonstrate that PPTAgent significantly outperforms existing automatic presentation generation methods across all three dimensions",
    "checked": true,
    "id": "7abfda966aec59cf385810415334abd65d1f6a85",
    "semantic_title": "pptagent: generating and evaluating presentations beyond text-to-slides",
    "citation_count": 23,
    "authors": [
      "Hao Zheng",
      "Xinyan Guan",
      "Hao Kong",
      "Wenkai Zhang",
      "Jia Zheng",
      "Weixiang Zhou",
      "Hongyu Lin",
      "Yaojie Lu",
      "Xianpei Han",
      "Le Sun"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.729": {
    "title": "SWAM: Adaptive Sliding Window and Memory-Augmented Attention Model for Rumor Detection",
    "volume": "main",
    "abstract": "Detecting rumors on social media has become a critical task in combating misinformation. Existing propagation-based rumor detection methods often focus on the static propagation graph, overlooking that rumor propagation is inherently dynamic and incremental in the real world. Recently propagation-based rumor detection models attempt to use the dynamic graph that is associated with coarse-grained temporal information. However, these methods fail to capture the long-term time dependency and detailed temporal features of propagation. To address these issues, we propose a novel adaptive Sliding Window and memory-augmented Attention Model (SWAM) for rumor detection. The adaptive sliding window divides the sequence of posts into consecutive disjoint windows based on the propagation rate of nodes. We also propose a memory-augmented attention to capture the long-term dependency and the depth of nodes in the propagation graph. Multi-head attention mechanism is applied between nodes in the memorybank and incremental nodes to iteratively update the memorybank, and the depth information of nodes is also considered. Finally, the propagation features of nodes in the memorybank are utilized for rumor detection. Experimental results on two public real-world datasets demonstrate the effectiveness of our model compared with the state-of-the-art baselines",
    "checked": true,
    "id": "abb591e12f00d564cfe6324f69d27d68781e2c55",
    "semantic_title": "swam: adaptive sliding window and memory-augmented attention model for rumor detection",
    "citation_count": 0,
    "authors": [
      "Mei Guo",
      "Chen Chen",
      "Chunyan Hou",
      "Yike Wu",
      "Xiaojie Yuan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.730": {
    "title": "HydraRAG: Structured Cross-Source Enhanced Large Language Model Reasoning",
    "volume": "main",
    "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating external knowledge. Current hybrid RAG system retrieves evidence from both knowledge graphs (KGs) and text documents to support LLM reasoning. However, it faces challenges like handling multi-hop reasoning, multi-entity questions, multi-source verification, and effective graph utilization. To address these limitations, we present HydraRAG, a training-free framework that unifies graph topology, document semantics, and source reliability to support deep, faithful reasoning in LLMs. HydraRAG handles multi-hop and multi-entity problems through agent-driven exploration that combines structured and unstructured retrieval, increasing both diversity and precision of evidence. To tackle multi-source verification, HydraRAG uses a tri-factor cross-source verification (source trustworthiness assessment, cross-source corroboration, and entity-path alignment), to balance topic relevance with cross-modal agreement. By leveraging graph structure, HydraRAG fuses heterogeneous sources, guides efficient exploration, and prunes noise early. Comprehensive experiments on seven benchmark datasets show that HydraRAG achieves overall state-of-the-art results on all benchmarks with GPT-3.5-Turbo, outperforming the strong hybrid baseline ToG-2 by an average of 20.3% and up to 30.1%. Furthermore, HydraRAG enables smaller models (e.g., Llama-3.1-8B) to achieve reasoning performance comparable to that of GPT-4-Turbo. The source code is available on https://stevetantan.github.io/HydraRAG/",
    "checked": false,
    "id": "71f69e23ae66cf50382e63c3d707e609932bd5e6",
    "semantic_title": "hydra: structured cross-source enhanced large language model reasoning",
    "citation_count": 2,
    "authors": [
      "Xingyu Tan",
      "Xiaoyang Wang",
      "Qing Liu",
      "Xiwei Xu",
      "Xin Yuan",
      "Liming Zhu",
      "Wenjie Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.731": {
    "title": "VRoPE: Rotary Position Embedding for Video Large Language Models",
    "volume": "main",
    "abstract": "Rotary Position Embedding (RoPE) has shown strong performance in text-based Large Language Models (LLMs), but extending it to video remains a challenge due to the intricate spatiotemporal structure of video frames. Existing adaptations, such as RoPE-3D, attempt to encode spatial and temporal dimensions separately but suffer from two major limitations: positional bias in attention distribution and disruptions in video-text transitions. To overcome these issues, we propose Video Rotary Position Embedding (VRoPE), a novel positional encoding method tailored for Video-LLMs. Specifically, we introduce a more balanced encoding strategy that mitigates attention biases, ensuring a more uniform distribution of spatial focus. Additionally, our approach restructures positional indices to ensure a smooth transition between video and text tokens. Extensive experiments on different models demonstrate that VRoPE consistently outperforms previous RoPE variants, achieving significant improvements in video understanding, temporal reasoning, and retrieval tasks. Code is available at https://github.com/johncaged/VRoPE",
    "checked": true,
    "id": "0c0d935a560a766e0e77b146be6bc35a3458bc81",
    "semantic_title": "vrope: rotary position embedding for video large language models",
    "citation_count": 6,
    "authors": [
      "Zikang Liu",
      "Longteng Guo",
      "Yepeng Tang",
      "Tongtian Yue",
      "Junxian Cai",
      "Kai Ma",
      "Qingbin Liu",
      "Xi Chen",
      "Jing Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.732": {
    "title": "SciNLP: A Domain-Specific Benchmark for Full-Text Scientific Entity and Relation Extraction in NLP",
    "volume": "main",
    "abstract": "Structured information extraction from scientific literature is crucial for capturing core concepts and emerging trends in specialized fields. While existing datasets aid model development, most focus on specific publication sections due to domain complexity and the high cost of annotating scientific texts. To address this limitation, we introduce SciNLP—a specialized benchmark for full-text entity and relation extraction in the Natural Language Processing (NLP) domain. The dataset comprises 60 manually annotated full-text NLP publications, covering 7,072 entities and 1,826 relations. Compared to existing research, SciNLP is the first dataset providing full-text annotations of entities and their relationships in the NLP domain. To validate the effectiveness of SciNLP, we conducted comparative experiments with similar datasets and evaluated the performance of state-of-the-art supervised models on this dataset. Results reveal varying extraction capabilities of existing models across academic texts of different lengths. Cross-comparisons with existing datasets show that SciNLP achieves significant performance improvements on certain baseline models. Using models trained on SciNLP, we implemented automatic construction of a fine-grained knowledge graph for the NLP domain. Our KG has an average node degree of 3.2 per entity, indicating rich semantic topological information that enhances downstream applications. The dataset is publicly available at: https://github.com/AKADDC/SciNLP",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Decheng Duan",
      "Jitong Peng",
      "Yingyi Zhang",
      "Chengzhi Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.733": {
    "title": "Think and Recall: Layer-Level Prompting for Lifelong Model Editing",
    "volume": "main",
    "abstract": "Lifelong model editing aims to dynamically adjust a model's output with respect to specific facts, knowledge points, or behaviors, enabling the model to adapt to the ever-changing demands of the real world without requiring retraining. While some retrieval-based methods have demonstrated potential in lifelong editing scenarios by storing edited knowledge in external memory, they often suffer from limitations in usability, such as requiring additional training corpora or lacking support for reversible and detachable edits.To address these issues, we propose a plug-and-play method for knowledge retrieval and storage, i.e., Layer-Level Prompting (LLP), which enables seamless and efficient lifelong model editing. In our LLP framework, the reasoning process of LLMs is divided into two stages, respectively knowledge retrieval (Think) and knowledge injection(Recall). Specifically, the knowledge retrieval process is performed in the early layers of the model. Based on the retrieved information, the model is guided to access the updated knowledge stored in the subsequent layer to complete the knowledge editing process. Experimental results demonstrate that our method consistently outperforms existing techniques on lifelong model editing tasks, achieving superior performance on question answering and hallucination benchmarks across different LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinke Wang",
      "Zenan Ying",
      "Qi Liu",
      "Wei Chen",
      "Tong Xu",
      "Huijun Hou",
      "Zhi Zheng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.734": {
    "title": "SPIRIT: Patching Speech Language Models against Jailbreak Attacks",
    "volume": "main",
    "abstract": "Speech Language Models (SLMs) enable natural interactions via spoken instructions, which more effectively capture user intent by detecting nuances in speech. The richer speech signal introduces new security risks compared to text-based models, as adversaries can better bypass safety mechanisms by injecting imperceptible noise to speech. We analyze adversarial attacks under white-box access and find that SLMs are substantially more vulnerable to jailbreak attacks, which can achieve a perfect 100% attack success rate in some instances. To improve security, we propose post-hoc patching defenses used to intervene during inference by modifying the SLM's activations that improve robustness up to 99% with (i) negligible impact on utility and (ii) without any re-training. We conduct ablation studies to maximize the efficacy of our defenses and improve the utility/security trade-off, validated with large-scale benchmarks unique to SLMs",
    "checked": true,
    "id": "8acbe98a2728eff42e349209c4880d1423a74444",
    "semantic_title": "spirit: patching speech language models against jailbreak attacks",
    "citation_count": 1,
    "authors": [
      "Amirbek Djanibekov",
      "Nurdaulet Mukhituly",
      "Kentaro Inui",
      "Hanan Aldarmaki",
      "Nils Lukas"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.735": {
    "title": "FIRE: Flexible Integration of Data Quality Ratings for Effective Pretraining",
    "volume": "main",
    "abstract": "Selecting high-quality data can improve the pretraining efficiency of large language models (LLMs). Existing methods generally rely on heuristic techniques or single quality signals, limiting their ability to evaluate data quality comprehensively. In this work, we propose FIRE, a flexible and scalable framework for integrating multiple data quality raters, which allows for a comprehensive assessment of data quality across various dimensions. FIRE aligns multiple quality signals into a unified space, and integrates diverse data quality raters to provide a comprehensive quality signal for each data point. Further, we introduce a progressive data selection scheme based on FIRE that iteratively refines the selection of high-quality data points. Extensive experiments show that FIRE outperforms other data selection methods and significantly boosts pretrained model performance across a wide range of downstream tasks, while requiring less than 37.5% tokens needed by the Random baseline to reach the target performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xu Liangyu",
      "Xuemiao Zhang",
      "Feiyu Duan",
      "Sirui Wang",
      "Rongxiang Weng",
      "Jingang Wang",
      "Xunliang Cai"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.736": {
    "title": "Multi-Domain Explainability of Preferences",
    "volume": "main",
    "abstract": "Preference mechanisms, such as human preference, LLM-as-a-Judge (LaaJ), and reward models, are central to aligning and evaluating large language models (LLMs). Yet, the underlying concepts that drive these preferences remain poorly understood. In this work, we propose a fully automated method for generating local and global concept-based explanations of preferences across multiple domains. Our method utilizes an LLM to identify concepts (rubrics) that distinguish between chosen and rejected responses, and to represent them with concept-based vectors. To model the relationships between concepts and preferences, we propose a white-box Hierarchical Multi-Domain Regression model that captures both domain-general and domain-specific effects. To evaluate our method, we curate a dataset spanning eight diverse domains and explain twelve mechanisms. Our method achieves strong preference prediction performance, outperforming baselines while also being explainable. Additionally, we assess explanations in two application-driven settings. First, guiding LLM outputs with concepts from LaaJ explanations yields responses that those judges consistently prefer. Second, prompting LaaJs with concepts explaining humans improves their preference predictions. Together, our work establishes a new paradigm for explainability in the era of LLMs",
    "checked": true,
    "id": "9db4ecfb5f2e78b43ff388892ce3bf0f122fa0d3",
    "semantic_title": "multi-domain explainability of preferences",
    "citation_count": 0,
    "authors": [
      "Nitay Calderon",
      "Liat Ein-Dor",
      "Roi Reichart"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.737": {
    "title": "Tuning Less, Prompting More: In-Context Preference Learning Pipeline for Natural Language Transformation",
    "volume": "main",
    "abstract": "Natural language transformation (NLT) tasks, such as machine translation (MT) and text style transfer (TST), require models to generate accurate and contextually appropriate outputs. However, existing approaches face significant challenges, including the computational costs of leveraging large pre-trained models and the limited generalization ability of fine-tuned smaller models. In this paper, we propose a novel framework that combines the flexibility of prompting with the cost-effectiveness of fine-tuning. Our method enhances smaller models by integrating In-Context Examples (ICE) from retrieval, enabling the model to better capture contextual information and align with user-level preferences. We further improve performance through hierarchical contrastive learning and dynamic preference inference mechanisms. Experimental results demonstrate that our approach outperforms existing methods, such as Supervised Fine Tuning (SFT), Direct Preference Optimization (DPO), and Contrastive Preference Optimization (CPO), across both MT and TST tasks, providing a more efficient solution for resource-constrained environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuyun Yang",
      "Yan Zhang",
      "Zhengmao Ye",
      "Lei Duan",
      "Mingjie Tang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.738": {
    "title": "IL-PCSR: Legal Corpus for Prior Case and Statute Retrieval",
    "volume": "main",
    "abstract": "Identifying/retrieving relevant statutes and prior cases/precedents for a given legal situation are common tasks exercised by law practitioners. Researchers till date have addressed the two tasks independently, thus developing completely different datasets and models for each task; however, both retrieval tasks are inherently related, e.g., similar cases tend to cite similar statutes (due to similar factual situation). In this paper, we address this gap. We propose IL-PCSR (Indian Legal corpus for Prior Case and Statute Retrieval), which is a unique corpus that provides a common testbed for developing models for both the tasks (Statute Retrieval and Precedent Retrieval) that can exploit the dependence between the two. We experiment extensively with several baseline models on the tasks, including lexical models, semantic models and ensemble based on GNNs. Further, to exploit the dependence between the two tasks, we develop an LLM based re-ranking approach that gives the best performance",
    "checked": true,
    "id": "df270cc50db0bb8c6375075d72cd3c52f512e405",
    "semantic_title": "il-pcsr: legal corpus for prior case and statute retrieval",
    "citation_count": 0,
    "authors": [
      "Shounak Paul",
      "Dhananjay Ghumare",
      "Pawan Goyal",
      "Saptarshi Ghosh",
      "Ashutosh Modi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.739": {
    "title": "ESGenius: Benchmarking LLMs on Environmental, Social, and Governance (ESG) and Sustainability Knowledge",
    "volume": "main",
    "abstract": "We introduce ESGenius, a comprehensive benchmark for evaluating and enhancing the proficiency of Large Language Models (LLMs) in Environmental, Social, and Governance (ESG) and sustainability-focused question answering. ESGenius comprises two key components: (i) ESGenius-QA, a collection of 1,136 Multiple-Choice Questions (MCQs) generated by LLMs and rigorously validated by domain experts, covering a broad range of ESG pillars and sustainability topics. Each question is systematically linked to its corresponding source text, enabling transparent evaluation and supporting Retrieval-Augmented Generation (RAG) methods; and (ii) ESGenius-Corpus, a meticulously curated repository of 231 foundational frameworks, standards, reports, and recommendation documents from 7 authoritative sources. Moreover, to fully assess the capabilities and adaptation potential of LLMs, we implement a rigorous two-stage evaluation protocol—Zero-Shot and RAG. Extensive experiments across 50 LLMs (0.5B to 671B) demonstrate that state-of-the-art models achieve only moderate performance in zero-shot settings, with accuracies around 55–70%, highlighting a significant knowledge gap for LLMs in this specialized, interdisciplinary domain. However, models employing RAG demonstrate significant performance improvements, particularly for smaller models. For example, DeepSeek-R1-Distill-Qwen-14B improves from 63.82% (zero-shot) to 80.46% with RAG. These results demonstrate the necessity of grounding responses in authoritative sources for enhanced ESG understanding. To the best of our knowledge, ESGenius is the first comprehensive QA benchmark designed to rigorously evaluate LLMs on ESG and sustainability knowledge, providing a critical tool to advance trustworthy AI in this vital domain",
    "checked": true,
    "id": "7a6911ceac3c4ef985a4bb16507315316df0603e",
    "semantic_title": "esgenius: benchmarking llms on environmental, social, and governance (esg) and sustainability knowledge",
    "citation_count": 3,
    "authors": [
      "Chaoyue He",
      "Xin Zhou",
      "Yi Wu",
      "Xinjia Yu",
      "Yan Zhang",
      "Lei Zhang",
      "Di Wang",
      "Shengfei Lyu",
      "Hong Xu",
      "Wang Xiaoqiao",
      "Wei Liu",
      "Chunyan Miao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.740": {
    "title": "How Sememic Components Can Benefit Link Prediction for Lexico-Semantic Knowledge Graphs?",
    "volume": "main",
    "abstract": "Link Prediction (LP) aims to predict missing triple information within a Knowledge Graph (KG). Existing LP methods have sought to improve the performance by integrating structural and textual information. However, for lexico-semantic KGs designed to document fine-grained sense distinctions, these types of information may not be sufficient to support effective LP. From a linguistic perspective, word senses within lexico-semantic relations usually show systematic differences in their sememic components. In light of this, we are motivated to enhance LP with sememe knowledge. We first construct a Sememe Prediction (SP) dataset, SememeDef, for learning such knowledge, and two Chinese datasets, HN7 and CWN5, for LP evaluation; Then, we propose a method, SememeLP, to leverage this knowledge for LP fully. It consistently and significantly improves the LP performance in both English and Chinese, achieving SOTA MRR of 75.1%, 80.5%, and 77.1% on WN18RR, HN7, and CWN5, respectively; Finally, an in-depth analysis is conducted, making clear how sememic components can benefit LP for lexico-semantic KGs, which provides promising progress for the completion of them",
    "checked": true,
    "id": "9a94212e55e20b23bfcd5333ec8b8bab698a322f",
    "semantic_title": "how sememic components can benefit link prediction for lexico-semantic knowledge graphs?",
    "citation_count": 0,
    "authors": [
      "Hansi Wang",
      "Yue Wang",
      "Qiliang Liang",
      "Yang Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.741": {
    "title": "WISE: Weak-Supervision-Guided Step-by-Step Explanations for Multimodal LLMs in Image Classification",
    "volume": "main",
    "abstract": "Multimodal Large Language Models (MLLMs) have shown promise in visual-textual reasoning, with Multimodal Chain-of-Thought (MCoT) prompting significantly enhancing interpretability. However, existing MCoT methods rely on rationale-rich datasets and largely focus on inter-object reasoning, overlooking the intra-object understanding crucial for image classification. To address this gap, we propose WISE, a Weak-supervision-guided Step-by-step Explanation method that augments any image classification dataset with MCoTs by reformulating the concept-based representations from Concept Bottleneck Models (CBMs) into concise, interpretable reasoning chains under weak supervision. Experiments across ten datasets show that our generated MCoTs not only improve interpretability by 37% but also lead to gains in classification accuracy when used to fine-tune MLLMs. Our work bridges concept-based interpretability and generative MCoT reasoning, providing a generalizable framework for enhancing MLLMs in fine-grained visual understanding",
    "checked": true,
    "id": "b9d29f68934730cad4fd92bf310d0615f5e78c70",
    "semantic_title": "wise: weak-supervision-guided step-by-step explanations for multimodal llms in image classification",
    "citation_count": 1,
    "authors": [
      "Yiwen Jiang",
      "Deval Mehta",
      "Siyuan Yan",
      "Yaling Shen",
      "Zimu Wang",
      "Zongyuan Ge"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.742": {
    "title": "Calibration Across Layers: Understanding Calibration Evolution in LLMs",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have demonstrated inherent calibration capabilities, where predicted probabilities align well with correctness, despite prior findings that deep neural networks are often overconfident. Recent studies have linked this behavior to specific components in the final layer, such as entropy neurons and the unembedding matrix's null space. In this work, we provide a complementary perspective by investigating how calibration evolves throughout the network's depth. Analyzing multiple open-weight models on the MMLU benchmark, we uncover a distinct confidence correction phase in the upper/later layers, where model confidence is actively recalibrated after decision certainty has been reached. Furthermore, we identify a low-dimensional calibration direction in the residual stream whose perturbation significantly improves calibration metrics (ECE and MCE) without harming accuracy. Our findings suggest that calibration is a distributed phenomenon, shaped throughout the network's forward pass, not just in its final projection, providing new insights into how confidence-regulating mechanisms operate within LLMs",
    "checked": true,
    "id": "1acca0c4b5f9963f7d262f793bf6a7cfda188a96",
    "semantic_title": "calibration across layers: understanding calibration evolution in llms",
    "citation_count": 0,
    "authors": [
      "Abhinav Joshi",
      "Areeb Ahmad",
      "Ashutosh Modi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.743": {
    "title": "The discordance between embedded ethics and cultural inference in large language models",
    "volume": "main",
    "abstract": "Effective interactions between artificial intelligence (AI) and humans require an equitable and accurate representation of diverse cultures. It is known that current AI, particularly large language models (LLMs), possesses some degrees of cultural knowledge but not without limitations. We present a framework aimed at understanding the origin of these limitations. We hypothesize that there is a fundamental discordance between embedded ethics—how LLMs represent right versus wrong, and cultural inference—how LLMs infer cultural knowledge, specifically cultural norms. We demonstrate this by extracting low-dimensional subspaces that embed ethical principles of LLMs based on established benchmarks. We then show that how LLMs make errors in culturally distinctive scenarios significantly correlates with how they represent cultural norms with respect to these embedded ethics subspaces. Furthermore, we show that coercing cultural norms to be more aligned with the embedded ethics increases LLM performance in cultural inference. Our analyses of 12 language models, two large-scale cultural benchmarks spanning 75 countries and two ethical datasets indicate that 1) the ethics-culture discordance tends to be exacerbated in instruct-tuned models, and 2) how current LLMs represent ethics can impose limitations on their adaptation to diverse cultures particularly pertaining to non-Western and low-income regions",
    "checked": true,
    "id": "484629423d847881ec5021d0ad6a437cc2a7b369",
    "semantic_title": "the discordance between embedded ethics and cultural inference in large language models",
    "citation_count": 0,
    "authors": [
      "Aida Ramezani",
      "Yang Xu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.744": {
    "title": "SSA: Semantic Contamination of LLM-Driven Fake News Detection",
    "volume": "main",
    "abstract": "Benchmark data contamination (BDC) silently inflate the evaluation performance of large language models (LLMs), yet current work on BDC has centered on direct token overlap (data/label level), leaving the subtler and equally harmful semantic level BDC largely unexplored. This gap is critical in fake news detection task, where prior exposure to semantic BDC lets a model \"remember\" the answer instead of reasoning. In this work, (1) we are the first to formally define semantic contamination for this task and (2) introduce the Semantic Sensitivity Amplifier (SSA), a lightweight, model-agnostic framework that detects BDC risks across semantic to label level via an entity shift perturbation and a comprehensive interpretable metric, the SSA Factor. Evaluating 45 variants of nine LLMs (0.5B–72B parameters) across four BDC levels, we find LIAR2 accuracy climbs monotonically with injected contamination, while the SSA Factor escalates in near-perfect lock-step (r≥.97, for models ≥3B, p<.05; 𝜌 ≥.9 overall, p<.05). These results show that SSA provides a sensitive and scalable audit of comprehensive BDC risk and paves the way for a more integrity evaluation of the LLM-driven fake news detection task",
    "checked": true,
    "id": "ac2260fece4793b010715a38a5d162237c1ca7a0",
    "semantic_title": "ssa: semantic contamination of llm-driven fake news detection",
    "citation_count": 0,
    "authors": [
      "Cheng Xu",
      "Nan Yan",
      "Shuhao Guan",
      "Yuke Mei",
      "Tahar Kechadi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.745": {
    "title": "Logits-Based Finetuning",
    "volume": "main",
    "abstract": "In recent years, developing compact and efficient large language models (LLMs) has emerged as a thriving area of research. However, traditional Supervised Fine-Tuning (SFT), which relies on singular ground truth labels, often fails to capture token-level dependencies and linguistic diversity. To address these limitations, we propose a logits-based fine-tuning framework that integrates the strengths of supervised learning and knowledge distillation. Our approach constructs enriched training targets by combining teacher logits with ground truth labels, preserving both correctness and linguistic diversity. This ensures more reliable and effective training. To validate our approach, we constructed a large-scale 1.2M logits dataset and trained a series of science-focused models. Experimental results demonstrate that our method achieves significant improvements over current SOTA, with accuracy gains of 18% on Mawps and 22.7% on TabMWP. Across nine widely used mathematical benchmarks, our method consistently outperforms prior SFT models, achieving an average improvement of 7.28%. All code and datasets will be open-sourced",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyao Li",
      "Senqiao Yang",
      "Sitong Wu",
      "Han Shi",
      "Chuanyang Zheng",
      "Hong Xu",
      "Jiaya Jia"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.746": {
    "title": "STARE at the Structure: Steering ICL Exemplar Selection with Structural Alignment",
    "volume": "main",
    "abstract": "In-Context Learning (ICL) has become a powerful paradigm that enables LLMs to perform a wide range of tasks without task-specific fine-tuning. However, the effectiveness of ICL heavily depends on the quality of exemplar selection. In particular, for structured prediction tasks such as semantic parsing, existing ICL selection strategies often overlook structural alignment, leading to suboptimal performance and poor generalization. To address this issue, we propose a novel two-stage exemplar selection strategy that achieves a strong balance between efficiency, generalizability, and performance. First, we fine-tune a BERT-based retriever using structure-aware supervision, guiding it to select exemplars that are both semantically relevant and structurally aligned. Then, we enhance the retriever with a plug-in module, which amplifies syntactically meaningful information in the hidden representations. This plug-in is model-agnostic, requires minimal overhead, and can be seamlessly integrated into existing pipelines. Experiments on four benchmarks spanning three semantic parsing tasks demonstrate that our method consistently outperforms existing baselines with multiple recent LLMs as inference-time models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqian Li",
      "Qisheng Hu",
      "Jing Li",
      "Wenya Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.747": {
    "title": "PPC-GPT: Federated Task-Specific Compression of Large Language Models via Pruning and Chain-of-Thought Distillation",
    "volume": "main",
    "abstract": "Compressing Large Language Models (LLMs) into task-specific Small Language Models (SLMs) encounters two significant challenges: safeguarding domain-specific knowledge privacy and managing limited resources. To tackle these challenges, we propose PPC-GPT, a novel unified framework that systematically addresses both privacy preservation and model compression in federated settings. PPC-GPT works on a server-client federated architecture, where the client sends differentially private (DP) perturbed task-specific data to the server's LLM. The LLM then generates synthetic data along with their corresponding rationales. This synthetic data is subsequently used for both LLM pruning and retraining processes. Our framework's key innovation lies in its holistic integration of privacy-preserving mechanisms, synthetic data generation, and task-specific compression techniques, creating unique benefits through component interaction. Our experiments across diverse text generation tasks demonstrate that PPC-GPT successfully achieves dual objectives: maintaining competitive performance comparable to full-sized LLMs while ensuring robust privacy protection through its federated architecture. Our code has been contributed to the FATE open-source project and is now publicly accessible at https://github.com/FederatedAI/FATE-LLM/tree/main/python/fate_llm/algo/ppc-gpt",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Fan",
      "Guoqiang Ma",
      "Yuanfeng Song",
      "Lixin Fan",
      "Qiang Yang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.748": {
    "title": "Efficient Beam Search for Large Language Models Using Trie-Based Decoding",
    "volume": "main",
    "abstract": "This work presents a novel trie (prefix-tree)-based parallel decoding method that addresses the memory inefficiency of batch-based beam search. By sharing a single KV cache across beams with common prefixes, our approach dramatically reduces memory usage and enables efficient decoding. We evaluated our method across three attention architectures, Multi-Head Attention (Phi-3.5-mini-instruct), Grouped Query Attention (Llama-3.1-8B-Instruct), and Sliding Window Attention (Mistral-Small-24B-Instruct-2501), using CNN/DailyMail for abstractive summarization and HumanEval for code generation. Our experiments demonstrate substantial memory savings (4–8×) and up to 2.4× faster decoding, without compromising generation quality. These results highlight our method's suitability for memory-constrained environments and large-scale deployments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brian J Chan",
      "Mao-xun Huang",
      "Jui-Hung Cheng",
      "Chao-Ting Chen",
      "Hen-Hsen Huang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.749": {
    "title": "Power doesn't reside in size: A Low Parameter Hybrid Language Model (HLM) for Sentiment Analysis in Code-mixed data",
    "volume": "main",
    "abstract": "Code-mixed text—where multiple languages are used within the same utterance—is increasingly common in both spoken and written communication. However, it presents significant challenges for machine learning models due to the interplay of distinct grammatical structures, effectively forming a hybrid language. While fine-tuning large language models (LLMs) such as GPT-3, or Llama-3 on code-mixed data has led to performance improvements, these models still lag behind their monolingual counterparts and incur high computational costs due to the large number of trainable parameters.In this paper, we focus on the task of sentiment detection in code-mixed text and propose a Hybrid Language Model (HLM) that combines a multilingual encoder (e.g., mBERT) with a lightweight decoder (e.g., Sarvam-1) (3B parameters). Despite having significantly fewer trainable parameters, HLM achieves sentiment classification performance comparable to that of fine-tuned Large Language Models (LLMs) (> 7B parameters). Furthermore, our results demonstrate that HLM significantly outperforms models trained individually, underscoring its effectiveness for low-resource, code-mixed sentiment analysis",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pavan Sai Balaga",
      "Nagasamudram Karthik",
      "Challa Vishwanath",
      "Raksha Sharma",
      "Rudra Murthy",
      "Ashish Mittal"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.750": {
    "title": "Evaluating Taxonomy Free Character Role Labeling (TF-CRL) in News Stories using Large Language Models",
    "volume": "main",
    "abstract": "We introduce Taxonomy-Free Character Role Labeling (TF-CRL); a novel task that assigns open-ended narrative role labels to characters in news stories based on their functional role in the narrative. Unlike fixed taxonomies, TF-CRL enables more nuanced and comparative analysis by generating compositional labels (e.g., Resilient Leader, Scapegoated Visionary). We evaluate several large language models (LLMs) on this task using human preference rankings and ratings across four criteria: faithfulness, relevance, informativeness, and generalizability. LLMs almost uniformly outperform human annotators across all dimensions. We further show how TF-CRL supports rich narrative analysis by revealing novel latent taxonomies and enabling cross-domain narrative comparisons. Our approach offers new tools for studying media portrayals, character framing, and the socio-political impacts of narrative roles at-scale",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David G Hobson",
      "Derek Ruths",
      "Andrew Piper"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.751": {
    "title": "MIRROR: Multimodal Cognitive Reframing Therapy for Rolling with Resistance",
    "volume": "main",
    "abstract": "Recent studies have explored the use of large language models (LLMs) in psychotherapy; however, text-based cognitive behavioral therapy (CBT) models often struggle with client resistance, which can weaken therapeutic alliance. To address this, we propose a multimodal approach that incorporates nonverbal cues, which allows the AI therapist to better align its responses with the client's negative emotional state.Specifically, we introduce a new synthetic dataset, Mirror (Multimodal Interactive Rolling with Resistance), which is a novel synthetic dataset that pairs each client's statements with corresponding facial images. Using this dataset, we train baseline vision language models (VLMs) so that they can analyze facial cues, infer emotions, and generate empathetic responses to effectively manage client resistance.These models are then evaluated in terms of both their counseling skills as a therapist, and the strength of therapeutic alliance in the presence of client resistance. Our results demonstrate that Mirror significantly enhances the AI therapist's ability to handle resistance, which outperforms existing text-based CBT approaches.Human expert evaluations further confirm the effectiveness of our approach in managing client resistance and fostering therapeutic alliance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Subin Kim",
      "Hoonrae Kim",
      "Jihyun Lee",
      "Yejin Jeon",
      "Gary Lee"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.752": {
    "title": "RETAIL: Towards Real-world Travel Planning for Large Language Models",
    "volume": "main",
    "abstract": "Although large language models have enhanced automated travel planning abilities, current systems remain misaligned with real-world scenarios. First, they assume users provide explicit queries, while in reality requirements are often implicit. Second, existing solutions ignore diverse environmental factors and user preferences, limiting the feasibility of plans. Third, systems can only generate plans with basic POI arrangements, failing to provide all-in-one plans with rich details. To mitigate these challenges, we construct a novel dataset RETAIL, which supports decision-making for implicit queries while covering explicit queries, both with and without revision needs. It also enables environmental awareness to ensure plan feasibility under real-world scenarios, while incorporating detailed POI information for all-in-one travel plans. Furthermore, we propose a topic-guided multi-agent framework, termed TGMA. Our experiments reveal that even the strongest existing model achieves merely a 1.0% pass rate, indicating real-world travel planning remains extremely challenging. In contrast, TGMA demonstrates substantially improved performance 2.72%, offering promising directions for real-world travel planning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bin Deng",
      "Yizhe Feng",
      "Zeming Liu",
      "Qing Wei",
      "Xiangrong Zhu",
      "Shuai Chen",
      "Yuanfang Guo",
      "Yunhong Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.753": {
    "title": "Unraveling Interwoven Roles of Large Language Models in Authorship Privacy: Obfuscation, Mimicking, and Verification",
    "volume": "main",
    "abstract": "Recent advancements in large language models (LLMs) have been fueled by large-scale training corpora drawn from diverse sources such as websites, news articles, and books. These datasets often contain explicit user information, such as person names, addresses, that LLMs may unintentionally reproduce in their generated outputs. Beyond such explicit content, LLMs can also leak identity-revealing cues through implicit signals such as distinctive writing styles, raising significant concerns about authorship privacy. There are three major automated tasks in authorship privacy, namely authorship obfuscation (AO), authorship mimicking (AM), and authorship verification (AV). Prior research has studied AO, AM, and AV independently. However, their interplays remain under-explored, which leaves a major research gap, especially in the era of LLMs, where they are profoundly shaping how we curate and share user-generated content, and the distinction between machine‐generated and human‐authored text is also increasingly blurred. This work then presents the first unified framework for analyzing the dynamic relationships among LLM-enabled AO, AM, and AV in the context of authorship privacy. We quantify how they interact with each other to transform human‐authored text, examining effects at a single point in time and iteratively over time. We also examine the role of demographic metadata, such as gender, academic background, in modulating their performances, inter-task dynamics, and privacy risks. The code is available at https://github.com/nguyentuc/authorship_privacy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tuc Nguyen",
      "Yifan Hu",
      "Thai Le"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.754": {
    "title": "Reward Model Perspectives: Whose Opinions Do Reward Models Reward?",
    "volume": "main",
    "abstract": "Reward models (RMs) are central to the alignment of language models (LMs). An RM often serves as a proxy for human preferences to guide downstream LM behavior. However, our understanding of RM behavior is limited. Our work (i) formalizes a framework for measuring the alignment of opinions captured by RMs, (ii) investigates the extent to which RMs demonstrate sociodemographic biases, and (iii) explores the effects of prompting to steer rewards towards the preferences of a target group. We study the subjective and diverse perspectives on controversial topics, which allows us to quantify RM perspectives in terms of their opinions, attitudes, and values. We show that RMs are poorly aligned with several demographic groups and can systematically reward harmful stereotypes, and steering alone is not enough to overcome these limitations. Our findings underscore the need for more careful consideration of RM behavior in model alignment during preference learning to prevent the propagation of unwanted social biases in the language technologies that we use",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elle"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.755": {
    "title": "FLRC: Fine-grained Low-Rank Compressor for Efficient LLM Inference",
    "volume": "main",
    "abstract": "Although large language models (LLM) have achieved remarkable performance, their enormous parameter counts hinder deployment on resource-constrained hardware. Low-rank compression can reduce both memory usage and computational demand, but applying a uniform compression ratio across all layers often leads to significant performance degradation, and previous methods perform poorly during decoding. To address these issues, we propose the Fine-grained Low-Rank Compressor (FLRC), which efficiently determines an optimal rank allocation for each layer, and incorporates progressive low-rank decoding to maintain text generation quality. Comprehensive experiments on diverse benchmarks demonstrate the superiority of FLRC, achieving up to a 17% improvement in ROUGE-L on summarization tasks compared to state-of-the-art low-rank compression methods, establishing a more robust and efficient framework to improve LLM inference",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu-Chen Lu",
      "Chong-Yan Chen",
      "Chi-Chih Chang",
      "Yu-Fang Hu",
      "Kai-Chiang Wu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.756": {
    "title": "Do You Know About My Nation? Investigating Multilingual Language Models' Cultural Literacy Through Factual Knowledge",
    "volume": "main",
    "abstract": "Most multilingual question-answering benchmarks, while covering a diverse pool of languages, do not factor in regional diversity in the information they capture and tend to be Western-centric. This introduces a significant gap in fairly evaluating multilingual models' comprehension of factual information from diverse geographical locations. To address this, we introduce XNationQA for investigating the cultural literacy of multilingual LLMs. XNationQA encompasses a total of 49,280 questions on the geography, culture, and history of nine countries, presented in seven languages. We benchmark eight standard multilingual LLMs on XNationQA and evaluate them using two novel transference metrics. Our analyses uncover a considerable discrepancy in the models' accessibility to culturally specific facts across languages. Notably, we often find that a model demonstrates greater knowledge of cultural information in English than in the dominant language of the respective culture. The models exhibit better performance in Western languages, although this does not necessarily translate to being more literate for Western countries, which is counterintuitive. Furthermore, we observe that models have a very limited ability to transfer knowledge across languages, particularly evident in open-source models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eshaan Tanwar",
      "Anwoy Chatterjee",
      "Michael Saxon",
      "Alon Albalak",
      "William Yang Wang",
      "Tanmoy Chakraborty"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.757": {
    "title": "CoEvo: Coevolution of LLM and Retrieval Model for Domain-Specific Information Retrieval",
    "volume": "main",
    "abstract": "Information retrieval in specialized domains (e.g., legal and medical) faces challenges in aligning user queries, often expressed in colloquial language, with highly structured, terminology-rich documents. This discrepancy creates a distribution gap in the text representation. Recent methods aim to enhance queries by generating intermediary elements (e.g., keywords, pseudo-documents) before performing retrieval with large language models (LLMs). However, by treating LLMs and retrievers separately, these approaches risk producing unreliable or irrelevant intermediaries, which can significantly degrade retrieval performance. To address this issue, we propose CoEvo, an alternating optimization framework that facilitates the coevolution of LLMs and retrieval models. CoEvo operates through two key steps: L-step directs the LLM in generating intermediaries by leveraging an archive of historical examples known to enhance retrieval. R-step trains the retriever using contrastive learning on the intermediaries produced by the LLM. Finally, we evaluate and flexibly leverage content generated by the LLM to amplify the effectiveness of coevolution. Experimental results demonstrate significant improvements in retrieval performance across both legal and medical domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ang Li",
      "Yiquan Wu",
      "Yinghao Hu",
      "Lizhi Qing",
      "Shihang Wang",
      "Chengyuan Liu",
      "Tao Wu",
      "Adam Jatowt",
      "Ming Cai",
      "Fei Wu",
      "Kun Kuang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.758": {
    "title": "Conan-Embedding-v2: Training an LLM from Scratch for Text Embeddings",
    "volume": "main",
    "abstract": "Large language models (LLMs) have recently demonstrated excellent performance in text embedding tasks. Previous work usually use LoRA to fine-tune existing LLMs, which are limited by the data and training gap between LLMs and embedding models. In this work, we introduce Conan-embedding-v2, a new 1.4B-parameter LLM trained from scratch and fine-tuned as a text embedder. First, we add news data and multilingual pairs for LLM pretraining to bridge the data gap. Based on this, we propose a cross-lingual retrieval dataset that enables the LLM to better integrate embeddings across different languages. Second, whereas LLMs use a causal mask with token-level loss, embedding models use a bidirectional mask with sentence-level loss. This training gap makes full fine-tuning less effective than LoRA. We introduce a soft-masking mechanism to gradually transition between these two types of masks, enabling the model to learn more comprehensive representations. Based on this, we propose a dynamic hard negative mining method that exposes the model to more difficult negative examples throughout the training process. Being intuitive and effective, with only approximately 1.4B parameters, Conan-embedding-v2 achieves SOTA performance on both the Massive Text Embedding Benchmark (MTEB) and Chinese MTEB (May 19, 2025)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiyu Li",
      "Yang Tang",
      "Ruijie Liu",
      "Shi-Zhe Chen",
      "Xi Chen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.759": {
    "title": "Vision-and-Language Navigation with Analogical Textual Descriptions in LLMs",
    "volume": "main",
    "abstract": "Integrating large language models (LLMs) into embodied AI models is becoming increasingly prevalent. However, existing zero-shot LLM-based Vision-and-Language Navigation (VLN) agents either encode images as textual scene descriptions, potentially oversimplifying visual details, or process raw image inputs, which can fail to capture abstract semantics required for high-level reasoning. In this paper, we improve the navigation agent's contextual understanding by incorporating textual descriptions that facilitate analogical reasoning across images from multiple perspectives. By leveraging text-based analogical reasoning, the agent enhances its global scene understanding and spatial reasoning, leading to more accurate action decisions. We evaluate our approach on the R2R dataset, where our experiments demonstrate significant improvements in navigation performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Zhang",
      "Tianyi Ma",
      "Zun Wang",
      "Yanyuan Qiao",
      "Parisa Kordjamshidi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.760": {
    "title": "MUCAR: Benchmarking Multilingual Cross-Modal Ambiguity Resolution for Multimodal Large Language Models",
    "volume": "main",
    "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated significant advances across numerous vision-language tasks. Due to their strong performance in image-text alignment, MLLMs can effectively understand image-text pairs with clear meanings. However, effectively resolving the inherent ambiguities in natural language and visual contexts remains challenging. Existing multimodal benchmarks typically overlook linguistic and visual ambiguities, relying mainly on unimodal context for disambiguation and thus failing to exploit the mutual clarification potential between modalities. To bridge this gap, we introduce MUCAR, a novel and challenging benchmark designed explicitly for evaluating multimodal ambiguity resolution across multilingual and cross-modal scenarios. MUCAR includes: (1) a multilingual dataset where ambiguous textual expressions are uniquely resolved by corresponding visual contexts, and (2) a dual-ambiguity dataset that systematically pairs ambiguous images with ambiguous textual contexts, with each combination carefully constructed to yield a single, clear interpretation through mutual disambiguation. Extensive evaluations involving 19 state-of-the-art multimodal models—encompassing both open-source and proprietary architectures—reveal substantial gaps compared to human-level performance, highlighting the need for future research into more sophisticated cross-modal ambiguity comprehension methods, further pushing the boundaries of multimodal reasoning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaolong Wang",
      "Zhaolu Kang",
      "Wangyuxuan Zhai",
      "Xinyue Lou",
      "Yunghwei Lai",
      "Ziyue Wang",
      "Yawen Wang",
      "Kaiyu Huang",
      "Yile Wang",
      "Peng Li",
      "Yang Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.761": {
    "title": "Mind the Gap: How BabyLMs Learn Filler-Gap Dependencies",
    "volume": "main",
    "abstract": "Humans acquire syntactic constructions like filler-gap dependencies from limited and often noisy input. Can neural language models do the same? We investigate this question by evaluating GPT-2 models trained on child-oriented input from the BabyLM Challenge. Our experiments focus on whether these \"baby\" language models acquire filler-gap dependencies, generalize across constructions, and respect structural constraints such as island effects. We apply a suite of syntactic constructions to four models trained on child language, including two base models (trained on 10M and 100M tokens) and two well-performing models from the BabyLM Challenge (ConcreteGPT and BabbleGPT). We evaluate model behavior using wh-licensing scores, flip tests, and grammaticality contrasts across four constructions. Results show that BabyLM-scale models partially acquire filler-gap dependencies but often fail to generalize or fully capture island constraints",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chi-Yun Chang",
      "Xueyang Huang",
      "Humaira Nasir",
      "Shane Storks",
      "Olawale Akingbade",
      "Huteng Dai"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.762": {
    "title": "Paths Not Taken: Understanding and Mending the Multilingual Factual Recall Pipeline",
    "volume": "main",
    "abstract": "Multilingual large language models (LLMs) often exhibit factual inconsistencies across languages, usually with better performance in factual recall tasks in high-resource languages than in other languages. The causes of these failures, however, remain poorly understood. Using mechanistic analysis techniques, we uncover the underlying pipeline that LLMs employ, which involves using the English-centric factual recall mechanism to process multilingual queries and then translating English answers back into the target language. We identify two primary sources of error: insufficient engagement of the reliable English-centric mechanism for factual recall, and incorrect translation from English back into the target language for the final answer. To address these vulnerabilities, we introduce two vector interventions, both independent of languages and datasets, to redirect the model toward better internal paths for higher factual consistency. Our interventions combined increase the recall accuracy by over 35 percent for the lowest-performing language. Our findings demonstrate how mechanistic insights can be used to unlock latent multilingual capabilities in LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meng Lu",
      "Ruochen Zhang",
      "Carsten Eickhoff",
      "Ellie Pavlick"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.763": {
    "title": "BTC-SAM: Leveraging LLMs for Generation of Bias Test Cases for Sentiment Analysis Models",
    "volume": "main",
    "abstract": "Sentiment Analysis (SA) models harbor inherent social biases that can be harmful in real-world applications. These biases are identified by examining the output of SA models for sentences that only vary in the identity groups of the subjects.Constructing natural, linguistically rich, relevant, and diverse sets of sentences that provide sufficient coverage over the domain is expensive, especially when addressing a wide range of biases: it requires domain experts and/or crowd-sourcing. In this paper, we present a novel bias testing framework, BTC-SAM, which generates high-quality test cases for bias testing in SA models with minimal specification using Large Language Models (LLMs) for the controllable generation of test sentences. Our experiments show that relying on LLMs can provide high linguistic variation and diversity in the test sentences, thereby offering better test coverage compared to base prompting methods even for previously unseen biases",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zsolt T. Kardkovács",
      "Lynda Djennane",
      "Anna Field",
      "Boualem Benatallah",
      "Yacine Gaci",
      "Fabio Casati",
      "Walid Gaaloul"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.764": {
    "title": "Debate-to-Detect: Reformulating Misinformation Detection as a Real-World Debate with Large Language Models",
    "volume": "main",
    "abstract": "The proliferation of misinformation in digital platforms reveals the limitations of traditional detection methods, which mostly rely on static classification and fail to capture the intricate process of real-world fact-checking. Despite advancements in Large Language Models (LLMs) that enhance automated reasoning, their application to misinformation detection remains hindered by issues of logical inconsistency and superficial verification. Inspired by the idea that \"Truth Becomes Clearer Through Debate\", we introduce Debate-to-Detect (D2D), a novel Multi-Agent Debate (MAD) framework that reformulates misinformation detection as a structured adversarial debate. Based on fact-checking workflows, D2D assigns domain-specific profiles to each agent and orchestrates a five-stage debate process, including Opening Statement, Rebuttal, Free Debate, Closing Statement, and Judgment. To transcend traditional binary classification, D2D introduces a multi-dimensional evaluation mechanism that assesses each claim across five distinct dimensions: Factuality, Source Reliability, Reasoning Quality, Clarity, and Ethics. Experiments with GPT-4o on two fakenews datasets demonstrate significant improvements over baseline methods, and the case study highlight D2D's capability to iteratively refine evidence while improving decision transparency, representing a substantial advancement towards robust and interpretable misinformation detection. Our code is available at https://github.com/hanshenmesen/Debate-to-Detect",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Han",
      "Wenzhen Zheng",
      "Xijin Tang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.765": {
    "title": "Controllable Memorization in LLMs via Weight Pruning",
    "volume": "main",
    "abstract": "The evolution of pre-trained large language models (LLMs) has significantly transformed natural language processing. However, these advancements pose challenges, particularly the unintended memorization of training data, which raises ethical and privacy concerns. While prior research has largely focused on mitigating memorization or extracting memorized information, the deliberate control of memorization has been underexplored. This study addresses this gap by introducing a novel and unified gradient-based weight pruning framework to freely control memorization rates in LLMs. Our method enables fine-grained control over pruning parameters, allowing models to suppress or enhance memorization based on application-specific requirements. Experimental results demonstrate that our approach effectively balances the trade-offs between memorization and generalization, with an increase of up to 89.3% in Fractional ER suppression and 40.9% in Exact ER amplification compared to the original models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenjie Ni",
      "Zhepeng Wang",
      "Runxue Bao",
      "Shangqian Gao",
      "Yanfu Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.766": {
    "title": "Tracing L1 Interference in English Learner Writing: A Longitudinal Corpus with Error Annotations",
    "volume": "main",
    "abstract": "Language transfer is an important topic of research in second language acquisition and computational linguistics. The availability of suitable learner corpora is paramount for the study of second language acquisition (SLA) and language transfer. However, curating learner corpora is a challenging endeavor as high quality learner data is rarely publicly available. This results in only a few such corpora available to the community. To address this important gap, in this paper we present LENS, a novel English learner corpus with longitudinal data which enables researchers to investigate language learning over time. LENS contains 687 instances written by speakers of 15 different L1s. We use LENS two perform two important tasks at the intersection of SLA and Computational Linguistics: (1) Native Language Identification (NLI); and (2) an evaluation of large language models as a tool for high-precision, semi-automated annotation of L1 interference features",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Poorvi Acharya",
      "J. Elizabeth Liebl",
      "Dhiman Goswami",
      "Kai North",
      "Marcos Zampieri",
      "Antonios Anastasopoulos"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.767": {
    "title": "DCIS: Efficient Length Extrapolation of LLMs via Divide-and-Conquer Scaling Factor Search",
    "volume": "main",
    "abstract": "Large language models (LLMs) based on the Transformer architecture usually have their context length limited due to the high training cost. Recent advancements extend the context window by adjusting the scaling factors of RoPE and fine-tuning. However, suboptimal initialization of these factors results in increased fine-tuning costs and reduced performance at target length. To address these challenges, we propose a novel RoPE-based fine-tuning framework that diverges from conventional scaling factors search. Specifically, we present a Divide-and-Conquer Incremental Search (DCIS) algorithm that strategically determines the better scaling factors. Further fine-tuning with the identified scaling factors effectively extends the context window of LLMs. Empirical results demonstrate that our methodology not only mitigates performance decay at extended target lengths but also allows the model to fine-tune on short contexts and generalize to long contexts, thereby reducing the cost of fine-tuning. The scaling factors obtained through DCIS can even perform effectively without fine-tuning. Further analysis of the search space reveals that DCIS achieves twice the search efficiency compared to other methods. We also examine the impact of the non-strictly increasing scaling factors utilized in DCIS and evaluate the general capabilities of LLMs across various context lengths",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Yang",
      "Shaoyang Xu",
      "Jianxiang Peng",
      "Shaolin Zhu",
      "Deyi Xiong"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.768": {
    "title": "Who is in the Spotlight: The Hidden Bias Undermining Multimodal Retrieval-Augmented Generation",
    "volume": "main",
    "abstract": "Multimodal Retrieval-Augmented Generation (RAG) systems have become essential in knowledge-intensive and open-domain tasks. As retrieval complexity increases, ensuring the robustness of these systems is critical. However, current RAG models are highly sensitive to the order in which evidence is presented, often resulting in unstable performance and biased reasoning, particularly as the number of retrieved items or modality diversity grows. This raises a central question: How does the position of retrieved evidence affect multimodal RAG performance? To answer this, we present the first comprehensive study of position bias in multimodal RAG systems. Through controlled experiments across text-only, image-only, and mixed-modality tasks, we observe a consistent U-shaped accuracy curve with respect to evidence position. To quantify this bias, we introduce the Position Sensitivity Index (PSIp) and develop a visualization framework to trace attention allocation patterns across decoder layers. Our results reveal that multimodal interactions intensify position bias compared to unimodal settings, and that this bias increases logarithmically with retrieval range. These findings offer both theoretical and empirical foundations for position-aware analysis in RAG, highlighting the need for evidence reordering or debiasing strategies to build more reliable and equitable generation systems. Our code and experimental resources are available at https://github.com/Theodyy/Multimodal-Rag-Position-Bias",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayu Yao",
      "Shenghua Liu",
      "Yiwei Wang",
      "Lingrui Mei",
      "Baolong Bi",
      "Yuyao Ge",
      "Zhecheng Li",
      "Xueqi Cheng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.769": {
    "title": "Let's Play Across Cultures: A Large Multilingual, Multicultural Benchmark for Assessing Language Models' Understanding of Sports",
    "volume": "main",
    "abstract": "Language Models (LMs) are primarily evaluated on globally popular sports, often overlooking regional and indigenous sporting traditions. To address this gap, we introduce CultSportQA, a benchmark designed to assess LMs' understanding of traditional sports across 60 countries and 6 continents, encompassing four distinct cultural categories. The dataset features 33,000 multiple-choice questions (MCQs) across text and image modalities, categorized into primarily three key types: history-based, rule-based, and scenario-based. To evaluate model performance, we employ zero-shot, few-shot, and chain-of-thought (CoT) prompting across a diverse set of Large Language Models (LLMs), Small Language Models (SLMs), and Multimodal Large Language Models (MLMs). By providing a comprehensive multilingual and multicultural sports benchmark, CultSportQA establishes a new standard for assessing AI's ability to understand and reason about traditional sports. The dataset will be publicly available, fostering research in culturally aware AI systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Punit Kumar Singh",
      "Nishant Kumar",
      "Akash Ghosh",
      "Kunal Pasad",
      "Khushi Soni",
      "Manisha Jaishwal",
      "Sriparna Saha",
      "Syukron Abu Ishaq Alfarozi",
      "Asres Temam Abagissa",
      "Kitsuchart Pasupa",
      "Haiqin Yang",
      "Jose G Moreno"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.770": {
    "title": "Multilingual Federated Low-Rank Adaptation for Collaborative Content Anomaly Detection across Multilingual Social Media Participants",
    "volume": "main",
    "abstract": "Recently, the rapid development of multilingual social media platforms (SNS) exacerbates new challenges in SNS content anomaly detection due to data islands and linguistic imbalance. While federated learning (FL) and parameter-efficient fine-tuning (PEFT) offer potential solutions in most cases, when every client is multilingual, existing solutions struggle with multilingual heterogeneity: 1) entangled language-specific knowledge during aggregation, 2) noise from minority languages, and 3) unstable cross-platform collaboration. Based on the asymmetric nature of LoRA, we propose MuLA-F, a multilingual Federated LoRA introducing SVD-based language-specific disentanglement of LoRA blocks and a local orthogonal tuning strategy. Evaluations across three SNS content anomaly detection tasks demonstrate MuLA-F's superiority in multilingual performance while reducing multilingual knowledge conflicts and communication rounds",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxin Li",
      "Geng Zhao",
      "Xiaoci Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.771": {
    "title": "M3Retrieve: Benchmarking Multimodal Retrieval for Medicine",
    "volume": "main",
    "abstract": "With the increasing use of Retrieval-Augmented Generation (RAG), strong retrieval models have become more important than ever. In healthcare, multimodal retrieval models that combine information from both text and images offer major advantages for many downstream tasks such as question answering, cross-modal retrieval, and multimodal summarization, since medical data often includes both formats. However, there is currently no standard benchmark to evaluate how well these models perform in medical settings. To address this gap, we introduce M3Retrieve, a Multimodal Medical Retrieval Benchmark. M3Retrieve spans 5 domains,16 medical fields, and 4 distinct tasks, with over 1.2 Million text documents and 164K multimodal queries, all collected under approved licenses. We evaluate leading multimodal retrieval models on this benchmark to explore the challenges specific to different medical specialities and to understand their impact on retrieval performance. By releasing M3Retrieve, we aim to enable systematic evaluation, foster model innovation, and accelerate research toward building more capable and reliable multimodal retrieval systems for medical applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arkadeep Acharya",
      "Akash Ghosh",
      "Pradeepika Verma",
      "Kitsuchart Pasupa",
      "Sriparna Saha",
      "Dr Priti Singh"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.772": {
    "title": "The Hidden Strength of Disagreement: Unraveling the Consensus-Diversity Tradeoff in Adaptive Multi-Agent Systems",
    "volume": "main",
    "abstract": "Consensus formation is pivotal in multi-agent systems (MAS), balancing collective coherence with individual diversity. Conventional LLM-based MAS primarily rely on explicit coordination, e.g., prompts or voting, risking premature homogenization. We argue that implicit consensus, where agents exchange information yet independently form decisions via in-context learning, can be more effective in dynamic environments that require long-horizon adaptability. By retaining partial diversity, systems can better explore novel strategies and cope with external shocks. We formalize a consensus-diversity tradeoff, showing conditions where implicit methods outperform explicit ones. Experiments on three scenarios – Dynamic Disaster Response, Information Spread and Manipulation, and Dynamic Public-Goods Provision – confirm partial deviation from group norms boosts exploration, robustness, and performance. We highlight emergent coordination via in-context learning, underscoring the value of preserving diversity for resilient decision-making",
    "checked": true,
    "id": "7ed28f269e12a453ba4abc9df7b4dda03d7cd9f0",
    "semantic_title": "the hidden strength of disagreement: unraveling the consensus-diversity tradeoff in adaptive multi-agent systems",
    "citation_count": 3,
    "authors": [
      "Zengqing Wu",
      "Takayuki Ito"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.773": {
    "title": "Friend or Foe? A Computational Investigation of Semantic False Friends across Romance Languages",
    "volume": "main",
    "abstract": "In this paper we present a comprehensive analysis of lexical semantic divergence between cognate words and borrowings in the Romance languages. We experiment with different algorithms for false friend detection including deceptive cognate and deceptive borrowings and correction and evaluate them systematically on cognate and borrowing pairs in the five Romance languages. We use the most complete and reliable dataset of cognate words based on etymological dictionaries for the five main Romance languages (Italian, Spanish, Portuguese, French and Romanian) to extract deceptive cognates and borrowings automatically based on usage, and freely publish the lexicon of obtained true and deceptive cognate and borrowings in every Romance language pair",
    "checked": true,
    "id": "49957984d0b4f14794ef4e7987297e1031a1777c",
    "semantic_title": "friend or foe? a computational investigation of semantic false friends across romance languages",
    "citation_count": 0,
    "authors": [
      "Ana Sabina Uban",
      "Liviu P Dinu",
      "Ioan-Bogdan Iordache",
      "Simona Georgescu",
      "Claudia Vlad"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.774": {
    "title": "KLAAD: Refining Attention Mechanisms to Reduce Societal Bias in Generative Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) often exhibit societal biases in their outputs, prompting ethical concerns regarding fairness and harm. In this work, we propose KLAAD (KL-Attention Alignment Debiasing), an attention-based debiasing framework that implicitly aligns attention distributions between stereotypical and anti-stereotypical sentence pairs without directly modifying model weights. KLAAD introduces a composite training objective combining Cross-Entropy, KL divergence, and Triplet losses, guiding the model to consistently attend across biased and unbiased contexts while preserving fluency and coherence. Experimental evaluation of KLAAD demonstrates improved bias mitigation on both the BBQ and BOLD benchmarks, with minimal impact on language modeling quality. The results indicate that attention-level alignment offers a principled solution for mitigating bias in generative language models",
    "checked": true,
    "id": "a9068dcc4f0b71fadf0405cc34cae69ade0e9b7d",
    "semantic_title": "klaad: refining attention mechanisms to reduce societal bias in generative language models",
    "citation_count": 0,
    "authors": [
      "Seorin Kim",
      "Dongyoung Lee",
      "Jaejin Lee"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.775": {
    "title": "SeMob: Semantic Synthesis for Dynamic Urban Mobility Prediction",
    "volume": "main",
    "abstract": "Human mobility prediction is vital for urban services, but often fails to account for abrupt changes from external events. Existing spatiotemporal models struggle to leverage textual descriptions detailing these events. We propose SeMob, an LLM-powered semantic synthesis pipeline for dynamic mobility prediction. Specifically, SeMob employs a multi-agent framework where LLM-based agents automatically extract and reason about spatiotemporally related text from complex online texts. Fine-grained relevant contexts are then incorporated with spatiotemporal data through our proposed innovative progressive fusion architecture. The rich pre-trained event prior contributes enriched insights about event-driven prediction, and hence results in a more aligned forecasting model. Evaluated on a dataset constructed through our pipeline, SeMob achieves maximal reductions of 13.92% in MAE and 11.12% in RMSE compared to the spatiotemporal model. Notably, the framework exhibits pronounced superiority especially within spatiotemporal regions close to an event's location and time of occurrence",
    "checked": true,
    "id": "916162944628baeb443ba5763621a2a236ea4ea4",
    "semantic_title": "semob: semantic synthesis for dynamic urban mobility prediction",
    "citation_count": 0,
    "authors": [
      "Runfei Chen",
      "Shuyang Jiang",
      "Wei Huang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.776": {
    "title": "DyePack: Provably Flagging Test Set Contamination in LLMs Using Backdoors",
    "volume": "main",
    "abstract": "Open benchmarks are essential for evaluating and advancing large language models, offering reproducibility and transparency. However, their accessibility makes them likely targets of test set contamination. In this work, we introduce **DyePack**, a framework that leverages backdoor attacks to identify models that used benchmark test sets during training, **without requiring access to the loss, logits, or any internal details of the model.** Like how banks mix dye packs with their money to mark robbers, DyePack mixes backdoor samples with the test data to flag models that trained on it. We propose a principled design incorporating multiple backdoors with stochastic targets, **enabling exact false positive rate (FPR) computation when flagging every model.** This provably prevents false accusations while providing strong evidence for every detected case of contamination. We evaluate DyePack on five models across three datasets, covering both multiple-choice and open-ended generation tasks. For multiple-choice questions, it successfully detects all contaminated models with guaranteed FPRs as low as 0.000073% on MMLU-Pro and 0.000017% on Big-Bench-Hard using eight backdoors. For open-ended generation tasks, it generalizes well and identifies all contaminated models on Alpaca with a guaranteed false positive rate of just 0.127% using six backdoors",
    "checked": true,
    "id": "1b21c8c25ad41381c4cf6ca700d1fbe729598546",
    "semantic_title": "dyepack: provably flagging test set contamination in llms using backdoors",
    "citation_count": 0,
    "authors": [
      "Yize Cheng",
      "Wenxiao Wang",
      "Mazda Moayeri",
      "Soheil Feizi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.777": {
    "title": "Minimal, Local, and Robust: Embedding-Only Edits for Implicit Bias in T2I Models",
    "volume": "main",
    "abstract": "Implicit assumptions and priors are often necessary in text-to-image generation tasks, especially when textual prompts lack sufficient context. However, these assumptions can sometimes reflect societal biases, low variance, or outdated concepts in the training data. We present Embedding-only Editing (EmbEdit), a method designed to efficiently edit implicit assumptions and priors in the text-to-image model without affecting unrelated objects or degrading overall performance. Given a \"source\" prompt (e.g., \"nurse\") that elicits an assumption (e.g., a female nurse) and a \"destination\" prompt or distribution (e.g. equal gender chance), EmbEdit only fine-tunes the word token embedding (WTE) of the target object (i.e. token \"nurse\"'s WTE). Our method prevents unintended effects on other objects in the model's knowledge base, as the WTEs for unrelated objects and the model weights remain unchanged. Further, our method can be applied to any text-to-image model with a text encoder. It is highly efficient, modifying only 768, 2048, and 4864 parameters for Stable Diffusion 1.4, Stable Diffusion XL, and FLUX, respectively, matching each model's WTE dimension. Additionally, changes could be easily reversed by restoring the original WTE layers. The results show that EmbEdit outperforms previous methods in various models, tasks, and editing scenarios (both single and sequential multiple edits), achieving at least a 6.01% improvement (from 87.17% to 93.18%)",
    "checked": true,
    "id": "f91ddcfddfddcd5ffbad8292245f58c848191db0",
    "semantic_title": "minimal, local, and robust: embedding-only edits for implicit bias in t2i models",
    "citation_count": 0,
    "authors": [
      "Feng He",
      "Chao Zhang",
      "Zhixue Zhao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.778": {
    "title": "Journalism-Guided Agentic In-context Learning for News Stance Detection",
    "volume": "main",
    "abstract": "As online news consumption grows, personalized recommendation systems have become integral to digital journalism. However, these systems risk reinforcing filter bubbles and political polarization by failing to incorporate diverse perspectives. Stance detection—identifying a text's position on a target—can help mitigate this by enabling viewpoint-aware recommendations and data-driven analyses of media bias. Yet, existing stance detection research remains largely limited to short texts and high-resource languages. To address these gaps, we introduce K-News-Stance, the first Korean dataset for article-level stance detection, comprising 2,000 news articles with article-level and 21,650 segment-level stance annotations across 47 societal issues. We also propose JoA-ICL, a Journalism-guided Agentic In-Context Learning framework that employs a language model agent to predict the stances of key structural segments (e.g., leads, quotes), which are then aggregated to infer the overall article stance. Experiments showed that JoA-ICL outperforms existing stance detection methods, highlighting the benefits of segment-level agency in capturing the overall position of long-form news articles. Two case studies further demonstrate its broader utility in promoting viewpoint diversity in news recommendations and uncovering patterns of media bias",
    "checked": true,
    "id": "a235b19c2abf2782d09ec042f657db3fd779f4f7",
    "semantic_title": "journalism-guided agentic in-context learning for news stance detection",
    "citation_count": 0,
    "authors": [
      "Dahyun Lee",
      "Jonghyeon Choi",
      "Jiyoung Han",
      "Kunwoo Park"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.779": {
    "title": "Less Is MuRE: Revisiting Shallow Knowledge Graph Embeddings",
    "volume": "main",
    "abstract": "In recent years, the field of knowledge graph completion has focused on increasingly sophisticated models, which perform well on link prediction tasks, but are less scalable than earlier methods and are not suitable for learning entity embeddings. As a result, shallow models such as TransE and ComplEx remain the most popular choice in many settings. However, the strengths and limitations of such models remain poorly understood. In this paper, we present a unifying framework and systematically analyze a number of variants and extensions of existing shallow models, empirically showing that MuRE and its extension, ExpressivE, are highly competitive. Motivated by the strong empirical results of MuRE, we also theoretically analyze the expressivity of its associated scoring function, surprisingly finding that it can capture the same class of rule bases as state-of-the-art region-based embedding models",
    "checked": true,
    "id": "3443257d154cc455e2b9762fa879323adb902652",
    "semantic_title": "less is mure: revisiting shallow knowledge graph embeddings",
    "citation_count": 0,
    "authors": [
      "Victor Charpenay",
      "Steven Schockaert"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.780": {
    "title": "Jailbreak LLMs through Internal Stance Manipulation",
    "volume": "main",
    "abstract": "To confront the ever-evolving safety risks of LLMs, automated jailbreak attacks have proven effective for proactively identifying security vulnerabilities at scale. Existing approaches, including GCG and AutoDAN, modify adversarial prompts to induce LLMs to generate responses that strictly follow a fixed affirmative template. However, we observed that the reliance on the rigid output template is ineffective for certain malicious requests, leading to suboptimal jailbreak performance. In this work, we aim to develop a method that is universally effective across all hostile requests. To achieve this, we explore LLMs' intrinsic safety mechanism: a refusal stance towards the adversarial prompt is formed in a confined region and ultimately leads to a rejective response. In light of this, we propose Stance Manipulation (SM), a novel automated jailbreak approach that generates jailbreak prompts to suppress the refusal stance and induce affirmative responses. Our experiments across four mainstream open-source LLMs demonstrate the superiority of SM's performance. Under commenly used setting, SM achieves success rates over 77.1% across all models on Advbench. Specifically, for Llama-2-7b-chat, SM outperforms the best baseline by 25.4%. In further experiments with extended iterations in a speedup setup, SM achieves over 92.2% attack success rate across all models. Our code is publicly available at https://github.com/Zed630/Stance-Manipulation",
    "checked": true,
    "id": "8e2552db44d29ec963bb730ff590fd02e7889456",
    "semantic_title": "jailbreak llms through internal stance manipulation",
    "citation_count": 0,
    "authors": [
      "Shuangjie Fu",
      "Du Su",
      "Beining Huang",
      "Fei Sun",
      "Jingang Wang",
      "Wei Chen",
      "Huawei Shen",
      "Xueqi Cheng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.781": {
    "title": "Pierce the Mists, Greet the Sky: Decipher Knowledge Overshadowing via Knowledge Circuit Analysis",
    "volume": "main",
    "abstract": "Large Language Models (LLMs), despite their remarkable capabilities, are hampered by hallucinations. A particularly challenging variant, knowledge overshadowing, occurs when one piece of activated knowledge inadvertently masks another relevant piece, leading to erroneous outputs even with high-quality training data. Current understanding of overshadowing is largely confined to inference-time observations, lacking deep insights into its origins and internal mechanisms during model training. Therefore, we introduce **PhantomCircuit, a novel framework designed to comprehensively analyze and detect knowledge overshadowing.** By innovatively employing knowledge circuit analysis, PhantomCircuit dissects the function of key components in the circuit and how the attention pattern dynamics contribute to the overshadowing phenomenon and its evolution throughout the training process. Extensive experiments demonstrate PhantomCircuit's effectiveness in identifying such instances, offering novel insights into this elusive hallucination and providing the research community with a new methodological lens for its potential mitigation. Our code can be found in https://github.com/halfmorepiece/PhantomCircuit",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoming Huang",
      "Yibo Yan",
      "Jiahao Huo",
      "Xin Zou",
      "Xinfeng Li",
      "Kun Wang",
      "Xuming Hu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.782": {
    "title": "Complex Numerical Reasoning with Numerical Semantic Pre-training Framework",
    "volume": "main",
    "abstract": "Multi-hop complex reasoning over incomplete knowledge graphs (KGs) has been extensively studied, but research on numerical knowledge graphs (NKGs) remains relatively limited. Recent approaches focus on separately encoding entities and numerical values, using neural networks to process query encodings for reasoning. However, in complex multi-hop reasoning tasks, numerical values are not merely symbols, and they carry specific semantics and logical relationships that must be accurately represented. The CNR-NST framework can perform binary operations on numerical attributes in NKGs, enabling it to infer new numerical attributes from existing knowledge. Our approach effectively handles up to 102 types of complex numerical reasoning queries. On three public datasets, CNR-NST demonstrates SOTA performance in complex numerical queries, achieving an average improvement of over 40% compared to existing methods. Notably, this work expands the query types for complex multi-hop numerical reasoning and introduces a new evaluation metric for numerical answers, which has been validated through comprehensive experiments",
    "checked": true,
    "id": "c1b8927b5b8211949404a54464103d1ce25a2cfb",
    "semantic_title": "complex numerical reasoning with numerical semantic pre-training framework",
    "citation_count": 0,
    "authors": [
      "Jun Zhang",
      "Haihong E",
      "Tianyi Hu",
      "Yifan Zhu",
      "Meina Song",
      "Haoran Luo"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.783": {
    "title": "Automated Knowledge Graph Construction using Large Language Models and Sentence Complexity Modelling",
    "volume": "main",
    "abstract": "We introduce CoDe-KG, an open-source, end-to-end pipeline for extracting sentence-level knowledge graphs by combining robust coreference resolution with syntactic sentence decomposition. Using our model, we contribute a dataset of over 150 000 knowledge triples, which is open source. We also contribute a training corpus of 7248 rows for sentence complexity, 200 rows of gold human annotations for coreference resolution using lung-cancer abstracts from PubMed, 900 rows of gold human annotations for sentence conversion policies from sentences in the abstract, and 398 triples of gold human annotations. We systematically select optimal prompt-model pairs across five complexity categories, showing that hybrid chain-of-thought and few-shot prompting yields up to 99.8% exact-match accuracy on sentence simplification. On relation extraction (RE), our pipeline achieves 65.8% macro-F1 on REBEL, an 8-point gain over the prior state of the art, and 75.7% micro-F1 on WebNLG2, while matching or exceeding performance on Wiki-NRE and CaRB. Ablation studies demonstrate that integrating coreference and decomposition increases recall on rare relations by over 20%",
    "checked": true,
    "id": "404ad47118151062064c79d4fd4302e19b71a385",
    "semantic_title": "automated knowledge graph construction using large language models and sentence complexity modelling",
    "citation_count": 0,
    "authors": [
      "Sydney Anuyah",
      "Mehedi Mahmud Kaushik",
      "Sri Rama Krishna Reddy Dwarampudi",
      "Rakesh Shiradkar",
      "Arjan Durresi",
      "Sunandan Chakraborty"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.784": {
    "title": "OntologyRAG-Q: Resource Development and Benchmarking for Retrieval-Augmented Question Answering in Qur'anic Tafsir",
    "volume": "main",
    "abstract": "This paper introduces essential resources for Qur'anic studies: an annotated Tafsir ontology, a dataset of approximately 4,200 question-answer pairs, and a collection of 15 structured Tafsir books available in two formats. We present a comprehensive framework for handling sensitive Qur'anic Tafsir data that spans the entire pipeline from dataset construction through evaluation and error analysis. Our work establishes new benchmarks for retrieval and question-answering tasks on Qur'anic content, comparing performance across state-of-the-art embedding models and large language models (LLMs).We introduce OntologyRAG-Q, a novel retrieval-augmented generation approach featuring our custom Ayat-Ontology chunking method that segments Tafsir content at the verse level using ontology-driven structure. Benchmarking reveals strong performance across various LLMs, with GPT-4 achieving the highest results, followed closely by ALLaM. Expert evaluations show our system achieves 69.52% accuracy and 74.36% correctness overall, though multi-hop and context-dependent questions remain challenging. Our analysis demonstrates that answer position within documents significantly impacts retrieval performance, and among the evaluation metrics tested, BERT-recall and BERT-F1 correlate most strongly with expert assessments. The resources developed in this study are publicly available at https://github.com/sazani/OntologyRAG-Q.git",
    "checked": true,
    "id": "e70be209afbdf667562c9baf38ef997d96dc50bc",
    "semantic_title": "ontologyrag-q: resource development and benchmarking for retrieval-augmented question answering in qur'anic tafsir",
    "citation_count": 0,
    "authors": [
      "Sadam Al-Azani",
      "Maad Alowaifeer",
      "Alhanoof Alhunief",
      "Ahmed Abdelali"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.785": {
    "title": "The Practical Impacts of Theoretical Constructs on Empathy Modeling",
    "volume": "main",
    "abstract": "Conceptual operationalizations of empathy in NLP are varied, with some having specific behaviors and properties, while others are more abstract. How these variations relate to one another and capture properties of empathy observable in text remains unclear. To provide insight into this, we analyze the transfer performance of empathy models adapted to empathy tasks with different theoretical groundings. We study (1) the dimensionality of empathy definitions, (2) the correspondence between the defined dimensions and measured/observed properties, and (3) the conduciveness of the data to represent them, finding they have a significant impact to performance compared to other transfer setting features. Characterizing the theoretical grounding of empathy tasks as direct, abstract, or adjacent further indicates that tasks that directly predict specified empathy components have higher transferability. Our work provides empirical evidence for the need for precise and multidimensional empathy operationalizations",
    "checked": true,
    "id": "9b512a3d7c0fdd56bf338defb35f96e5857ce538",
    "semantic_title": "the practical impacts of theoretical constructs on empathy modeling",
    "citation_count": 0,
    "authors": [
      "Allison Lahnala",
      "Charles Welch",
      "David Jurgens",
      "Lucie Flek"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.786": {
    "title": "RecBase: Generative Foundation Model Pretraining for Zero-Shot Recommendation",
    "volume": "main",
    "abstract": "Recent advances in LLM-based recommendation have shown promise, yet their cross-domain generalization is hindered by a fundamental mismatch between language-centric pretraining and the recommendation task. Existing methods, relying on language-level knowledge, fail to capture dynamic, item-level user interests across domains. To bridge this gap, we propose RecBase, a domain-agnostic foundational model pretrained with a recommendation-oriented objective. RecBase leverages a large-scale, heterogeneous, cross-domain corpus with unified textual representations and feature mappings to enhance cross-domain generalization. To further align item semantics across domains, we introduce a unified item tokenizer that encodes items into hierarchical concept identifiers, enabling structured representation and efficient vocabulary sharing. The model is trained using an autoregressive objective to capture complex item-level sequential patterns. On eight real-world datasets, our 1.5B-parameter model matches or surpasses the performance of LLM baselines up to 7B parameters in zero-shot and cross-domain recommendation tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sashuai Zhou",
      "Weinan Gan",
      "Qijiong Liu",
      "Ke Lei",
      "Jieming Zhu",
      "Hai Huang",
      "Yan Xia",
      "Ruiming Tang",
      "Zhenhua Dong",
      "Zhou Zhao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.787": {
    "title": "Grouping Entities with Shared Properties using Multi-Facet Prompting and Property Embeddings",
    "volume": "main",
    "abstract": "Methods for learning taxonomies from data have been widely studied. We study a specific version of this task, called commonality identification, where only the set of entities is given and we need to find meaningful ways to group those entities. While LLMs should intuitively excel at this task, it is difficult to directly use such models in large domains. In this paper, we instead use LLMs to describe the different properties that are satisfied by each of the entities individually. We then use pre-trained embeddings to cluster these properties, and finally group entities that have properties which belong to the same cluster. To achieve good results, it is paramount that the properties predicted by the LLM are sufficiently diverse. We find that this diversity can be improved by prompting the LLM to structure the predicted properties into different facets of knowledge",
    "checked": true,
    "id": "1786a098dab4c4dc8d5dd4ef7e4db13db71d815e",
    "semantic_title": "grouping entities with shared properties using multi-facet prompting and property embeddings",
    "citation_count": 0,
    "authors": [
      "Amit Gajbhiye",
      "Thomas Bailleux",
      "Zied Bouraoui",
      "Luis Espinosa-Anke",
      "Steven Schockaert"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.788": {
    "title": "Context-Aware Hierarchical Taxonomy Generation for Scientific Papers via LLM-Guided Multi-Aspect Clustering",
    "volume": "main",
    "abstract": "The rapid growth of scientific literature demands efficient methods to organize and synthesize research findings. Existing taxonomy construction methods, leveraging unsupervised clustering or direct prompting of large language models (LLMs), often lack coherence and granularity. We propose a novel context-aware hierarchical taxonomy generation framework that integrates LLM-guided multi-aspect encoding with dynamic clustering. Our method leverages LLMs to identify key aspects of each paper (e.g., methodology, dataset, evaluation) and generates aspect-specific paper summaries, which are then encoded and clustered along each aspect to form a coherent hierarchy. In addition, we introduce a new evaluation benchmark of 156 expert-crafted taxonomies encompassing 11.6k papers, providing the first naturally annotated dataset for this task. Experimental results demonstrate that our method significantly outperforms prior approaches, achieving state-of-the-art performance in taxonomy coherence, granularity, and interpretability",
    "checked": true,
    "id": "c57aa5217ca845d8d9502cbafc694c6b044c9121",
    "semantic_title": "context-aware hierarchical taxonomy generation for scientific papers via llm-guided multi-aspect clustering",
    "citation_count": 1,
    "authors": [
      "Kun Zhu",
      "Lizi Liao",
      "Yuxuan Gu",
      "Lei Huang",
      "Xiaocheng Feng",
      "Bing Qin"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.789": {
    "title": "Benchmark Profiling: Mechanistic Diagnosis of LLM Benchmarks",
    "volume": "main",
    "abstract": "Large Language Models are commonly judged by their scores on standard benchmarks, yet such scores often overstate real capability since they mask the mix of skills a task actually demands. For example, ARC is assumed to test reasoning, while HellaSwag is designed to evaluate commonsense. However, we lack a systematic way to verify if these benchmarks actually measure these labels. We introduce **BENCHMARK PROFILING**, a diagnostic framework that decomposes benchmark performance into ten cognitively grounded abilities. The method combines gradient-based importance scoring with targeted parameter ablation to compute an Ability Impact Score (AIS) that quantifies how much each ability contributes to a model's success on a given benchmark. Profiling three instruction-tuned models across ten widely used benchmarks yields four key findings: (i) most benchmarks draw on several abilities rather than one, (ii) datasets with similar labels rely on distinct ability mixtures, (iii) code-generation benchmarks reward broad, multi-skill improvement and thus show only modest gains from narrow domain-specific fine-tuning, and (iv) abilities irrelevant to the task could negatively affect performance. **BENCHMARK PROFILING** therefore explains why performance gains do not always translate into user-perceived competence and offer a transparent tool for benchmark audit and model interpretability",
    "checked": true,
    "id": "39218fa01febf0a5c9b9a3023c1005889458cc81",
    "semantic_title": "benchmark profiling: mechanistic diagnosis of llm benchmarks",
    "citation_count": 0,
    "authors": [
      "Dongjun Kim",
      "Gyuho Shim",
      "Yongchan Chun",
      "Minhyuk Kim",
      "Chanjun Park",
      "Heuiseok Lim"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.790": {
    "title": "TreeReview: A Dynamic Tree of Questions Framework for Deep and Efficient LLM-based Scientific Peer Review",
    "volume": "main",
    "abstract": "While Large Language Models (LLMs) have shown significant potential in assisting peer review, current methods often struggle to generate thorough and insightful reviews while maintaining efficiency. In this paper, we propose TreeReview, a novel framework that models paper review as a hierarchical and bidirectional question-answering process. TreeReview first constructs a tree of review questions by recursively decomposing high-level questions into fine-grained sub-questions and then resolves the question tree by iteratively aggregating answers from leaf to root to get the final review. Crucially, we incorporate a dynamic question expansion mechanism to enable deeper probing by generating follow-up questions when needed. We construct a benchmark derived from ICLR and NeurIPS venues to evaluate our method on full review generation and actionable feedback comments generation tasks. Experimental results of both LLM-based and human evaluation show that TreeReview outperforms strong baselines in providing comprehensive, in-depth, and expert-aligned review feedback, while reducing LLM token usage by up to 80% compared to computationally intensive approaches",
    "checked": true,
    "id": "fccde18f25977d129af241996a769bdb23d1148d",
    "semantic_title": "treereview: a dynamic tree of questions framework for deep and efficient llm-based scientific peer review",
    "citation_count": 7,
    "authors": [
      "Yuan Chang",
      "Ziyue Li",
      "Hengyuan Zhang",
      "Yuanbo Kong",
      "Yanru Wu",
      "Hayden Kwok-Hay So",
      "Zhijiang Guo",
      "Liya Zhu",
      "Ngai Wong"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.791": {
    "title": "Improving Chemical Understanding of LLMs via SMILES Parsing",
    "volume": "main",
    "abstract": "Large language models (LLMs) are increasingly recognized as powerful tools for scientific discovery, particularly in molecular science. A fundamental requirement for these models is the ability to accurately understand molecular structures, commonly encoded in the SMILES representation. However, current LLMs struggle to interpret SMILES, even failing to carry out basic tasks such as counting molecular rings. To address this limitation, we introduce CLEANMOL, a novel framework that formulates SMILES parsing into a suite of clean and deterministic tasks explicitly designed to promote graph-level molecular comprehension. These tasks span from subgraph matching to global graph matching, providing structured supervision aligned with molecular structural properties. We construct a molecular pretraining dataset with adaptive difficulty scoring and pre-train open-source LLMs on these tasks. Our results show that CLEANMOL not only enhances structural comprehension but also achieves the best or competes with the baseline on the Mol-Instructions benchmark",
    "checked": true,
    "id": "103382eeda379d1f5c5c5772ce9a95a47bde0115",
    "semantic_title": "improving chemical understanding of llms via smiles parsing",
    "citation_count": 2,
    "authors": [
      "Yunhui Jang",
      "Jaehyung Kim",
      "Sungsoo Ahn"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.792": {
    "title": "Can Large Language Models Tackle Graph Partitioning?",
    "volume": "main",
    "abstract": "Large language models (LLMs) demonstrate remarkable capabilities in understanding complex tasks and have achieved commendable performance in graph-related tasks, such as node classification, link prediction, and subgraph classification. These tasks primarily depend on the local reasoning capabilities of the graph structure. However, research has yet to address the graph partitioning task that requires global perception abilities. Our preliminary findings reveal that vanilla LLMs can only handle graph partitioning on extremely small-scale graphs. To overcome this limitation, we propose a three-phase pipeline to empower LLMs for large-scale graph partitioning: coarsening, reasoning, and refining. The coarsening phase reduces graph complexity. The reasoning phase captures both global and local patterns to generate a coarse partition. The refining phase ensures topological consistency by projecting the coarse-grained partitioning results back to the original graph structure. Extensive experiments demonstrate that our framework enables LLMs to perform graph partitioning across varying graph scales, validating both the effectiveness of LLMs for partitioning tasks and the practical utility of our proposed methodology",
    "checked": true,
    "id": "d6861421a094842f24bb2a4d6c1785891e7631f0",
    "semantic_title": "can large language models tackle graph partitioning?",
    "citation_count": 0,
    "authors": [
      "Yiheng Wu",
      "Ningchao Ge",
      "Yanmin Li",
      "Liwei Qian",
      "Mengna Zhu",
      "Haoyu Yang",
      "Haiwen Chen",
      "Jibing Wu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.793": {
    "title": "To See a World in a Spark of Neuron: Disentangling Multi-Task Interference for Training-Free Model Merging",
    "volume": "main",
    "abstract": "Fine-tuning pre-trained models on targeted datasets enhances task-specific performance but often comes at the expense of generalization. Model merging techniques, which integrate multiple fine-tuned models into a single multi-task model through task arithmetic, offer a promising solution. However, task interference remains a fundamental challenge, leading to performance degradation and suboptimal merged models. Existing approaches largely overlooked the fundamental roles of neurons, their connectivity, and activation, resulting in a merging process and a merged model that does not consider how neurons relay and process information. In this work, we present the first study that relies on neuronal mechanisms for model merging. Specifically, we decomposed task-specific representations into two complementary neuronal subspaces that regulate input sensitivity and task adaptability. Leveraging this decomposition, we introduced NeuroMerging, a novel merging framework developed to mitigate task interference within neuronal subspaces, enabling training-free model fusion across diverse tasks. Through extensive experiments, we demonstrated that NeuroMerging achieved superior performance compared to existing methods on multi-task benchmarks across both natural language and vision domains. Our findings highlighted the importance of aligning neuronal mechanisms in model merging, offering new insights into mitigating task interference and improving knowledge fusion. Our project is available at [this http URL](https://ZzzitaoFang.github.io/projects/NeuroMerging/)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zitao Fang",
      "Guodong Du",
      "Shuyang Yu",
      "Yifei Guo",
      "Yiwei Zhang",
      "Yiyao Cao",
      "Jing Li",
      "Ho-Kin Tang",
      "Sim Kuan Goh"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.794": {
    "title": "What You Read Isn't What You Hear: Linguistic Sensitivity in Deepfake Speech Detection",
    "volume": "main",
    "abstract": "Recent advances in text-to-speech technology have enabled highly realistic voice generation, fueling audio-based deepfake attacks such as fraud and impersonation. While audio anti-spoofing systems are critical for detecting such threats, prior research has predominantly focused on acoustic-level perturbations, leaving **the impact of linguistic variation largely unexplored**. In this paper, we investigate the linguistic sensitivity of both open-source and commercial anti-spoofing detectors by introducing **TAPAS** (Transcript-to-Audio Perturbation Anti-Spoofing), a novel framework for transcript-level adversarial attacks. Our extensive evaluation shows that even minor linguistic perturbations can significantly degrade detection accuracy: attack success rates exceed **60%** on several open-source detector–voice pairs, and the accuracy of one commercial detector drops from **100%** on synthetic audio to just **32%**. Through a comprehensive feature attribution analysis, we find that linguistic complexity and model-level audio embedding similarity are key factors contributing to detector vulnerabilities. To illustrate the real-world risks, we replicate a recent Brad Pitt audio deepfake scam and demonstrate that TAPAS can bypass commercial detectors. These findings underscore the **need to move beyond purely acoustic defenses** and incorporate linguistic variation into the design of robust anti-spoofing systems. Our source code is available at https://github.com/nqbinh17/audio_linguistic_adversarial",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Binh Nguyen",
      "Shuju Shi",
      "Ryan Ofman",
      "Thai Le"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.795": {
    "title": "Task-Aware Resolution Optimization for Visual Large Language Models",
    "volume": "main",
    "abstract": "Real-world vision-language applications demand varying levels of perceptual granularity. However, most existing visual large language models (VLLMs), such as LLaVA, pre-assume a fixed resolution for downstream tasks, which leads to subpar performance. To address this problem, we first conduct a comprehensive and pioneering investigation into the resolution preferences of different vision-language tasks, revealing a correlation between resolution preferences with (1) image complexity, and (2) uncertainty variance of the VLLM at different image input resolutions. Building on this insight, we propose an empirical formula to determine the optimal resolution for a given vision-language task, accounting for these two factors as the zeroth-order and first-order terms in the Taylor expansion on a given image input. Second, based on rigorous experiments, we propose a novel parameter-efficient fine-tuning technique to extend the visual input resolution of pre-trained VLLMs to the identified optimal resolution. Extensive experiments on various vision-language tasks validate the effectiveness of our method",
    "checked": true,
    "id": "01b709db1ddcf017ea5acd5b97da88be87edfea0",
    "semantic_title": "task-aware resolution optimization for visual large language models",
    "citation_count": 0,
    "authors": [
      "Weiqing Luo",
      "Zhen Tan",
      "Yifan Li",
      "Xinyu Zhao",
      "Kwonjoon Lee",
      "Behzad Dariush",
      "Tianlong Chen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.796": {
    "title": "CheckEval: A reliable LLM-as-a-Judge framework for evaluating text generation using checklists",
    "volume": "main",
    "abstract": "Existing LLM-as-a-Judge approaches for evaluating text generation suffer from rating inconsistencies, with low agreement and high rating variance across different evaluator models. We attribute this to subjective evaluation criteria combined with Likert scale scoring in existing protocols. To address this issue, we introduce CheckEval, a checklist-based evaluation framework that improves rating reliability via decomposed binary questions. Through experiments with 12 evaluator models across multiple datasets, we first demonstrate that CheckEval strongly correlates with human judgments. More importantly, CheckEval dramatically improves the average agreement across evaluator models by 0.45 and reduces the score variance. CheckEval scores furthermore have the benefit of being more interpretable because it decomposes evaluation criteria into traceable binary decisions, allowing analyses of specific attributes driving quality judgments",
    "checked": true,
    "id": "175060cb1e90cbd0f2036bc587ed8f76217f7deb",
    "semantic_title": "checkeval: a reliable llm-as-a-judge framework for evaluating text generation using checklists",
    "citation_count": 15,
    "authors": [
      "Yukyung Lee",
      "JoongHoon Kim",
      "Jaehee Kim",
      "Hyowon Cho",
      "Jaewook Kang",
      "Pilsung Kang",
      "Najoung Kim"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.797": {
    "title": "A Necessary Step toward Faithfulness: Measuring and Improving Consistency in Free-Text Explanations",
    "volume": "main",
    "abstract": "Faithful free-text explanations are important to ensure transparency in high-stakes AI decision-making contexts, but they are challenging to generate by language models and assess by humans. In this paper, we present a measure for Prediction-EXplanation (PEX) consistency, by extending the concept of weight of evidence. This measure quantifies how much a free-text explanation supports or opposes a prediction, serving as an important aspect of explanation faithfulness. Our analysis reveals that more than 62% explanations generated by large language models lack this consistency. We show that applying direct preference optimization improves the consistency of generated explanations across three model families, with improvement ranging from 43.1% to 292.3%. Furthermore, we demonstrate that optimizing this consistency measure can improve explanation faithfulness by up to 9.7%",
    "checked": true,
    "id": "c8ff536af85e5ee4b3543cf403dd518fdbe78248",
    "semantic_title": "a necessary step toward faithfulness: measuring and improving consistency in free-text explanations",
    "citation_count": 0,
    "authors": [
      "Lingjun Zhao",
      "Hal Daumé Iii"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.798": {
    "title": "Boosting Multi-modal Keyphrase Prediction with Dynamic Chain-of-Thought in Vision-Language Models",
    "volume": "main",
    "abstract": "Multi-modal keyphrase prediction (MMKP) aims to advance beyond text-only methods by incorporating multiple modalities of input information to produce a set of conclusive phrases. Traditional multi-modal approaches have been proven to have significant limitations in handling the challenging absence and unseen scenarios. Additionally, we identify shortcomings in existing benchmarks that overestimate model capability due to significant overlap in training tests. In this work, we propose leveraging vision-language models (VLMs) for the MMKP task. Firstly, we use two widely-used strategies, e.g., zero-shot and supervised fine-tuning (SFT) to assess the lower bound performance of VLMs. Next, to improve the complex reasoning capabilities of VLMs, we adopt Fine-tune-CoT, which leverages high-quality CoT reasoning data generated by a teacher model to finetune smaller models. Finally, to address the \"overthinking\" phenomenon, we propose a dynamic CoT strategy which adaptively injects CoT data during training, allowing the model to flexibly leverage its reasoning capabilities during the inference stage. We evaluate the proposed strategies on various datasets and the experimental results demonstrate the effectiveness of the proposed approaches. The code is available at https://github.com/bytedance/DynamicCoT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qihang Ma",
      "Shengyu Li",
      "Jie Tang",
      "Dingkang Yang",
      "Chenshaodong",
      "Yingyi Zhang",
      "Chao Feng",
      "Ran Jiao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.799": {
    "title": "Chart2Code53: A Large-Scale Diverse and Complex Dataset for Enhancing Chart-to-Code Generation",
    "volume": "main",
    "abstract": "Chart2code has recently received significant attention in the multimodal community due to its potential to reduce the burden of visualization and promote a more detailed understanding of charts. However, existing Chart2code-related training datasets suffer from at least one of the following issues: (1) limited scale, (2) limited type coverage, and (3) inadequate complexity. To address these challenges, we seek more diverse sources that better align with real-world user distributions and propose dual data synthesis pipelines: (1) synthesize based on online plotting code. (2) synthesize based on chart images in the academic paper. We create a large-scale Chart2code training dataset Chart2code53, including 53 chart types, 130K Chart-code pairs based on the pipeline. Experimental results demonstrate that even with few parameters, the model finetuned on Chart2code53 achieves state-of-the-art performance on multiple Chart2code benchmarks within open-source models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianhao Niu",
      "Yiming Cui",
      "Baoxin Wang",
      "Xiao Xu",
      "Xin Yao",
      "Qingfu Zhu",
      "Dayong Wu",
      "Shijin Wang",
      "Wanxiang Che"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.800": {
    "title": "The State of Multilingual LLM Safety Research: From Measuring The Language Gap To Mitigating It",
    "volume": "main",
    "abstract": "This paper presents a comprehensive analysis of the linguistic diversity of LLM safety research, highlighting the English-centric nature of the field. Through a systematic review of nearly 300 publications from 2020–2024 across major NLP conferences and workshops at ACL, we identify a significant and growing language gap in LLM safety research, with even high-resource non-English languages receiving minimal attention. We further observe that non-English languages are rarely studied as a standalone language and that English safety research exhibits poor language documentation practice. To motivate future research into multilingual safety, we make several recommendations based on our survey, and we then pose three concrete future directions on safety evaluation, training data generation, and crosslingual safety generalization. Based on our survey and proposed directions, the field can develop more robust, inclusive AI safety practices for diverse global populations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheng Xin Yong",
      "Beyza Ermis",
      "Marzieh Fadaee",
      "Stephen Bach",
      "Julia Kreutzer"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.801": {
    "title": "AIP: Subverting Retrieval-Augmented Generation via Adversarial Instructional Prompt",
    "volume": "main",
    "abstract": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by retrieving relevant documents from external sources to improve factual accuracy and verifiability. However, this reliance introduces new attack surfaces within the retrieval pipeline, beyond the LLM itself. While prior RAG attacks have exposed such vulnerabilities, they largely rely on manipulating user queries, which is often infeasible in practice due to fixed or protected user inputs. This narrow focus overlooks a more realistic and stealthy vector: instructional prompts, which are widely reused, publicly shared, and rarely audited. Their implicit trust makes them a compelling target for adversaries to manipulate RAG behavior covertly.We introduce a novel attack for Adversarial Instructional Prompt (AIP) that exploits adversarial instructional prompts to manipulate RAG outputs by subtly altering retrieval behavior. By shifting the attack surface to the instructional prompts, AIP reveals how trusted yet seemingly benign interface components can be weaponized to degrade system integrity. The attack is crafted to achieve three goals: (1) naturalness, to evade user detection; (2) utility, to encourage use of prompts; and (3) robustness, to remain effective across diverse query variations. We propose a diverse query generation strategy that simulates realistic linguistic variation in user queries, enabling the discovery of prompts that generalize across paraphrases and rephrasings. Building on this, a genetic algorithm-based joint optimization is developed to evolve adversarial prompts by balancing attack success, clean-task utility, and stealthiness. Experimental results show that AIP achieves up to 95.23% attack success rate while preserving benign functionality. These findings uncover a critical and previously overlooked vulnerability in RAG systems, emphasizing the need to reassess the shared instructional prompts",
    "checked": true,
    "id": "7424e85aa9249f032151b2519bf0217a68344cd2",
    "semantic_title": "aip: subverting retrieval-augmented generation via adversarial instructional prompt",
    "citation_count": 0,
    "authors": [
      "Saket Sanjeev Chaturvedi",
      "Gaurav Bagwe",
      "Lan Emily Zhang",
      "Xiaoyong Yuan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.802": {
    "title": "From Capabilities to Performance: Evaluating Key Functional Properties of LLM Architectures in Penetration Testing",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have been explored for automating or enhancing penetration testing tasks, but their effectiveness and reliability across diverse attack phases remain open questions. This study presents a comprehensive evaluation of multiple LLM-based agents, ranging from singular to modular designs, across realistic penetration testing scenarios, analyzing their empirical performance and recurring failure patterns. We further investigate the impact of core functional capabilities on agent success, operationalized through five targeted augmentations: Global Context Memory (GCM), Inter-Agent Messaging (IAM), Context-Conditioned Invocation (CCI), Adaptive Planning (AP), and Real-Time Monitoring (RTM). These interventions respectively support the capabilities of Context Coherence & Retention, Inter-Component Coordination & State Management, Tool Usage Accuracy & Selective Execution, Multi-Step Strategic Planning & Error Detection & Recovery, and Real-Time Dynamic Responsiveness. Our findings reveal that while some architectures natively exhibit select properties, targeted augmentations significantly enhance modular agent performance—particularly in complex, multi-step, and real-time penetration testing scenarios",
    "checked": true,
    "id": "ca515c68f3e867c38ac6531328b0852e6a660140",
    "semantic_title": "from capabilities to performance: evaluating key functional properties of llm architectures in penetration testing",
    "citation_count": 0,
    "authors": [
      "Lanxiao Huang",
      "Daksh Dave",
      "Tyler Cody",
      "Peter A. Beling",
      "Ming Jin"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.803": {
    "title": "Editing Across Languages: A Survey of Multilingual Knowledge Editing",
    "volume": "main",
    "abstract": "While Knowledge Editing has been extensively studied in monolingual settings, it remains underexplored in multilingual contexts. This survey systematizes recent research on Multilingual Knowledge Editing (MKE), a growing subdomain of model editing focused on ensuring factual edits generalize reliably across languages. We present a comprehensive taxonomy of MKE methods, covering parameter-based, memory-based, fine-tuning, and hypernetwork approaches. We survey available benchmarks, summarize key findings on method effectiveness and transfer patterns, and identify persistent challenges such as cross-lingual propagation, language anisotropy, and limited evaluation for low-resource and culturally specific languages. We also discuss broader concerns such as stability and scalability of multilingual edits. Our analysis consolidates a rapidly evolving area and lays the groundwork for future progress in editable language-aware LLMs",
    "checked": true,
    "id": "ddbda8eb9fc8cee46dea65affb302aabc50a3b3d",
    "semantic_title": "editing across languages: a survey of multilingual knowledge editing",
    "citation_count": 1,
    "authors": [
      "Nadir Durrani",
      "Basel Mousi",
      "Fahim Dalvi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.804": {
    "title": "Your RAG is Unfair: Exposing Fairness Vulnerabilities in Retrieval-Augmented Generation via Backdoor Attacks",
    "volume": "main",
    "abstract": "Retrieval-augmented generation (RAG) enhances factual grounding by integrating retrieval mechanisms with generative models but introduces new attack surfaces, particularly through backdoor attacks. While prior research has largely focused on disinformation threats, fairness vulnerabilities remain underexplored. Unlike conventional backdoors that rely on direct trigger-to-target mappings, fairness-driven attacks exploit the interaction between retrieval and generation models, manipulating semantic relationships between target groups and social biases to establish a persistent and covert influence on content generation.This paper introduces BiasRAG , a systematic framework that exposes fairness vulnerabilities in RAG through a two-phase backdoor attack. During the pre-training phase, the query encoder is compromised to align the target group with the intended social bias, ensuring long-term persistence. In the post-deployment phase, adversarial documents are injected into knowledge bases to reinforce the backdoor, subtly influencing retrieved content while remaining undetectable under standard fairness evaluations. Together, BiasRAG ensures precise target alignment over sensitive attributes, stealthy execution, and resilience. Empirical evaluations demonstrate that BiasRAG achieves high attack success rates while preserving contextual relevance and utility, establishing a persistent and evolving threat to fairness in RAG",
    "checked": true,
    "id": "3083bee50b6260e0fbd763e27d1ff2916d1eeb77",
    "semantic_title": "your rag is unfair: exposing fairness vulnerabilities in retrieval-augmented generation via backdoor attacks",
    "citation_count": 1,
    "authors": [
      "Gaurav Bagwe",
      "Saket Sanjeev Chaturvedi",
      "Xiaolong Ma",
      "Xiaoyong Yuan",
      "Kuang-Ching Wang",
      "Lan Emily Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.805": {
    "title": "Drift-Adapter: A Practical Approach to Near Zero-Downtime Embedding Model Upgrades in Vector Databases",
    "volume": "main",
    "abstract": "Upgrading embedding models in production vector databases typically necessitates re-encoding the entire corpus and rebuilding the Approximate Nearest Neighbor (ANN) index, leading to significant operational disruption and computational cost. This paper presents Drift-Adapter, a lightweight, learnable transformation layer designed to bridge embedding spaces between model versions. By mapping new queries into the legacy embedding space, Drift-Adapter enables the continued use of the existing ANN index, effectively deferring full re-computation. We systematically evaluate three adapter parameterizations: Orthogonal Procrustes, Low-Rank Affine, and a compact Residual MLP, trained on a small sample of paired old/new embeddings. Experiments on MTEB text corpora and a CLIP image model upgrade (1M items) show that Drift-Adapter recovers 95–99% of the retrieval recall (Recall@10, MRR) of a full re-embedding, adding less than 10,𝜇s query latency. Compared to operational strategies like full re-indexing or dual-index serving, Drift-Adapter dramatically reduces recompute costs (by over 100 times) and facilitates upgrades with near-zero operational interruption. We analyze robustness to varied model drift, training data size, scalability to billion-item systems, and the impact of design choices like diagonal scaling, demonstrating Drift-Adapter's viability as a pragmatic solution for agile model deployment",
    "checked": true,
    "id": "7f8182f67ccb496c14b62eae715046823949d692",
    "semantic_title": "drift-adapter: a practical approach to near zero-downtime embedding model upgrades in vector databases",
    "citation_count": 0,
    "authors": [
      "Harshil Vejendla"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.806": {
    "title": "The Staircase of Ethics: Probing LLM Value Priorities through Multi-Step Induction to Complex Moral Dilemmas",
    "volume": "main",
    "abstract": "Ethical decision-making is a critical aspect of human judgment, and the growing use of LLMs in decision-support systems necessitates a rigorous evaluation of their moral reasoning capabilities. However, existing assessments primarily rely on single-step evaluations, failing to capture how models adapt to evolving ethical challenges. Addressing this gap, we introduce the Multi-step Moral Dilemmas (MMDs), the first dataset specifically constructed to evaluate the evolving moral judgments of LLMs across 3,302 five-stage dilemmas. This framework enables a fine-grained, dynamic analysis of how LLMs adjust their moral reasoning across escalating dilemmas. Our evaluation of nine widely used LLMs reveals that their value preferences shift significantly as dilemmas progress, indicating that models recalibrate moral judgments based on scenario complexity. Furthermore, pairwise value comparisons demonstrate that while LLMs often prioritize the value of care, this value can sometimes be superseded by fairness in certain contexts, highlighting the dynamic and context-dependent nature of LLM ethical reasoning. Our findings call for a shift toward dynamic, context-aware evaluation paradigms, paving the way for more human-aligned and value-sensitive development of LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ya Wu",
      "Qiang Sheng",
      "Danding Wang",
      "Guang Yang",
      "Yifan Sun",
      "Zhengjia Wang",
      "Yuyan Bu",
      "Juan Cao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.807": {
    "title": "SliceMoE: Routing Embedding Slices Instead of Tokens for Fine-Grained and Balanced Transformer Scaling",
    "volume": "main",
    "abstract": "Mixture-of-Experts (MoE) layers scale transformers by routing tokens to a sparse subset of feed-forward experts. Token-level routing, however, assigns an entire semantic spectrum to each expert, creating capacity bottlenecks, load-balancing pathologies, and limited specialisation. We introduce SliceMoE, an architecture that routes contiguous slices of a token's hidden vector. A d-dimensional embedding is partitioned into S slices, and for each slice, a lightweight shared router predicts the top-k experts. Experts operate on their assigned slices independently, and outputs are re-assembled, maintaining per-token FLOP efficiency. Because slices from different tokens interleave within an expert, utilisation is naturally smoother. We propose a slice-level capacity loss, cross-slice dropout, and efficient fused batched-GEMM kernels. Experiments on WikiText-103 language modelling, WMT En–De translation, and three text-classification datasets show SliceMoE attains up to 1.7x faster inference than dense baselines, 12–18% lower perplexity than parameter-matched token-MoE, and improved expert balance, with interpretable expertise over syntactic versus semantic sub-spaces",
    "checked": true,
    "id": "0209fa1ae694ed1ec49a179c4e851cb7cf067183",
    "semantic_title": "slicemoe: routing embedding slices instead of tokens for fine-grained and balanced transformer scaling",
    "citation_count": 0,
    "authors": [
      "Harshil Vejendla"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.808": {
    "title": "ReSo: A Reward-driven Self-organizing LLM-based Multi-Agent System for Reasoning Tasks",
    "volume": "main",
    "abstract": "Multi-agent systems have emerged as a promising approach for enhancing the reasoning capabilities of large language models in complex problem-solving. However, current MAS frameworks are limited by poor flexibility and scalability, with underdeveloped optimization strategies. To address these challenges, we propose ReSo, which integrates task graph generation with a reward-driven two-stage agent selection process. The core of ReSo is the proposed Collaborative Reward Model, which can provide fine-grained reward signals for MAS cooperation for optimization. We also introduce an automated data synthesis framework for generating MAS benchmarks, without human annotations. Experimentally, ReSo matches or outperforms existing methods. ReSo achieves 33.7% and 32.3% accuracy on Math-MAS and SciBench-MAS SciBench, while other methods completely fail. The code and data are available at [Reso](https://github.com/hengzzzhou/ReSo)",
    "checked": true,
    "id": "499979b1347c53cc3cadedf66575623cc0d6a727",
    "semantic_title": "reso: a reward-driven self-organizing llm-based multi-agent system for reasoning tasks",
    "citation_count": 14,
    "authors": [
      "Heng Zhou",
      "Hejia Geng",
      "Xiangyuan Xue",
      "Li Kang",
      "Yiran Qin",
      "Zhiyong Wang",
      "Zhenfei Yin",
      "Lei Bai"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.809": {
    "title": "ConstraintLLM: A Neuro-Symbolic Framework for Industrial-Level Constraint Programming",
    "volume": "main",
    "abstract": "Constraint programming (CP) is a crucial technology for solving real-world constraint optimization problems (COPs), with the advantages of rich modeling semantics and high solving efficiency. Using large language models (LLMs) to generate formal modeling automatically for COPs is becoming a promising approach, which aims to build trustworthy neuro-symbolic AI with the help of symbolic solvers. However, CP has received less attention compared to works based on operations research (OR) models. We introduce ConstraintLLM, the first LLM specifically designed for CP modeling, which is trained on an open-source LLM with multi-instruction supervised fine-tuning. We propose the Constraint-Aware Retrieval Module (CARM) to increase the in-context learning capabilities, which is integrated in a Tree-of-Thoughts (ToT) framework with guided self-correction mechanism. Moreover, we construct and release IndusCP, the first industrial-level benchmark for CP modeling, which contains 140 challenging tasks from various domains. Our experiments demonstrate that ConstraintLLM achieves state-of-the-art solving accuracy across multiple benchmarks and outperforms the baselines by 2x on the new IndusCP benchmark. Code and data are available at: https://github.com/william4s/ConstraintLLM",
    "checked": true,
    "id": "75f67c14136c7f74a32b1397e64c04d4751a7c31",
    "semantic_title": "constraintllm: a neuro-symbolic framework for industrial-level constraint programming",
    "citation_count": 0,
    "authors": [
      "Weichun Shi",
      "Minghao Liu",
      "Wanting Zhang",
      "Langchen Shi",
      "Fuqi Jia",
      "Feifei Ma",
      "Jian Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.810": {
    "title": "VisEscape: A Benchmark for Evaluating Exploration-driven Decision-making in Virtual Escape Rooms",
    "volume": "main",
    "abstract": "Escape rooms present a unique cognitive challenge that demands exploration-driven planning: with the sole instruction to escape the room, players must actively search their environment, collecting information, and finding solutions through repeated trial and error. Motivated by this, we introduce VisEscape, a benchmark of 20 virtual escape rooms specifically designed to evaluate AI models under these challenging conditions, where success depends not only on solving isolated puzzles but also on iteratively constructing and refining spatial-temporal knowledge of a dynamically changing environment. On VisEscape, we observe that even state-of-the-art multi-modal models generally fail to escape the rooms, showing considerable variation in their progress and problem-solving approaches. We find that integrating memory management and reasoning contributes to efficient exploration and enables successive hypothesis formulation and testing, thereby leading to significant improvements in dynamic and exploration-driven environments",
    "checked": true,
    "id": "12b45109e246b410787bfc31c8b864b62e3c2baf",
    "semantic_title": "visescape: a benchmark for evaluating exploration-driven decision-making in virtual escape rooms",
    "citation_count": 2,
    "authors": [
      "Seungwon Lim",
      "Sungwoong Kim",
      "Jihwan Yu",
      "Sungjae Lee",
      "Jiwan Chung",
      "Youngjae Yu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.811": {
    "title": "ESC-Judge: A Framework for Comparing Emotional Support Conversational Agents",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) increasingly power mental-health chatbots, yet the field still lacks a scalable, theory-grounded way to decide which model is more effective to deploy. We present ESC-Judge, the first end-to-end evaluation framework that (i) grounds head-to-head comparison of Emotional-Support LLMs (ES-LLMs) in an established psychological theory—Clara Hill's Exploration–Insight–Action (E-I-A) counselling model—thereby delivering a structured, interpretable lens on performance, and (ii) fully automates the pipeline at scale. ESC-Judge proceeds in three stages: (1) it synthesizes realistic help-seeker roles by sampling empirically salient attributes (stressors, personality, life history); (2) it has two candidate ES-Agents conduct separate sessions with the same role, isolating model-specific strategies; and (3) it asks a specialised judge LLM to issue pairwise preferences across rubric-anchored skills that exhaustively cover the E-I-A spectrum. In our empirical study, ESC-Judge matches PhD-level annotators in 85% of Exploration, 83% of Insight, and 86% of Action decisions, demonstrating human-level reliability at a fraction of the cost. We release all code, prompts, synthetic roles, transcripts, and judgment scripts to catalyze transparent progress in emotionally supportive AI",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Navid Madani",
      "Rohini Srihari"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.812": {
    "title": "Neuron-Level Differentiation of Memorization and Generalization in Large Language Models",
    "volume": "main",
    "abstract": "We investigate how Large Language Models (LLMs) distinguish between memorization and generalization at the neuron level. Through carefully designed tasks, we identify distinct neuron subsets responsible for each behavior. Experiments on both a GPT-2 model trained from scratch and a pretrained LLaMA-3.2 model fine-tuned with LoRA show consistent neuron-level specialization. We further demonstrate that inference-time interventions on these neurons can steer the model's behavior toward memorization or generalization. To assess robustness, we evaluate intra-task and inter-task consistency, confirming that these neuron-behavior associations reflect generalizable patterns rather than dataset-specific artifacts. Our findings reveal modular structure in LLMs and enable controlling memorization and generalization behaviors at inference time",
    "checked": true,
    "id": "62f1ffb32b614afbe369f9012e7f0d2de080a989",
    "semantic_title": "neuron-level differentiation of memorization and generalization in large language models",
    "citation_count": 1,
    "authors": [
      "Ko-Wei Huang",
      "Yi-Fu Fu",
      "Ching-Yu Tsai",
      "Yu-Chieh Tu",
      "Tzu-ling Cheng",
      "Cheng-Yu Lin",
      "Yi-Ting Yang",
      "Heng-Yi Liu",
      "Keng-Te Liao",
      "Da-Cheng Juan",
      "Shou-De Lin"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.813": {
    "title": "Sparse Neurons Carry Strong Signals of Question Ambiguity in LLMs",
    "volume": "main",
    "abstract": "Ambiguity is pervasive in real-world questions, yet large language models (LLMs) often respond with confident answers rather than seeking clarification. In this work, we show that question ambiguity is linearly encoded in the internal representations of LLMs and can be both detected and controlled at the neuron level. During the model's pre-filling stage, we identify that a small number of neurons, as few as one, encode question ambiguity information. Probes trained on these Ambiguity-Encoding Neurons (AENs) achieve strong performance on ambiguity detection and generalize across datasets, outperforming prompting-based and representation-based baselines. Layerwise analysis reveals that AENs emerge from shallow layers, suggesting early encoding of ambiguity signals in the model's processing pipeline. Finally, we show that through manipulating AENs, we can control LLM's behavior from direct answering to abstention. Our findings reveal that LLMs form compact internal representations of question ambiguity, enabling interpretable and controllable behavior",
    "checked": true,
    "id": "a19c0c0299f7cd194e4dd3058412a83ef27795c5",
    "semantic_title": "sparse neurons carry strong signals of question ambiguity in llms",
    "citation_count": 0,
    "authors": [
      "Zhuoxuan Zhang",
      "Jinhao Duan",
      "Edward Kim",
      "Kaidi Xu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.814": {
    "title": "Do Slides Help? Multi-modal Context for Automatic Transcription of Conference Talks",
    "volume": "main",
    "abstract": "State-of-the-art (SOTA) Automatic Speech Recognition (ASR) systems primarily rely on acoustic information while disregarding additional multi-modal context. However, visual information are essential in disambiguation and adaptation. While most work focus on speaker images to handle noise conditions, this work also focuses on integrating presentation slides for the use cases of scientific presentation.In a first step, we create a benchmark for multi-modal presentation including an automatic analysis of transcribing domain-specific terminology. Next, we explore methods for augmenting speech models with multi-modal information. We mitigate the lack of datasets with accompanying slides by a suitable approach of data augmentation.Finally, we train a model using the augmented dataset, resulting in a relative reduction in word error rate of approximately 34%, across all words and 35%, for domain-specific terms compared to the baseline model",
    "checked": true,
    "id": "b1e67b8793f370d0655904f4f9b1f85961dd6538",
    "semantic_title": "do slides help? multi-modal context for automatic transcription of conference talks",
    "citation_count": 0,
    "authors": [
      "Supriti Sinhamahapatra",
      "Jan Niehues"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.815": {
    "title": "Promote, Suppress, Iterate: How Language Models Answer One-to-Many Factual Queries",
    "volume": "main",
    "abstract": "To answer one-to-many factual queries (e.g., listing cities of a country), a language model (LM) must simultaneously recall knowledge and avoid repeating previous answers. How are these two subtasks implemented and integrated internally? Across multiple datasets, models, and prompt templates, we identify a promote-then-suppress mechanism: the model first recalls all answers, and then suppresses previously generated ones. Specifically, LMs use both the subject and previous answer tokens to perform knowledge recall, with attention propagating subject information and MLPs promoting the answers. Then, attention attends to and suppresses previous answer tokens, while MLPs amplify the suppression signal. Our mechanism is corroborated by extensive experimental evidence: in addition to using early decoding and causal tracing, we analyze how components use different tokens by introducing both Token Lens, which decodes aggregated attention updates from specified tokens, and a knockout method that analyzes changes in MLP outputs after removing attention to specified tokens. Overall, we provide new insights into how LMs' internal components interact with different input tokens to support complex factual recall",
    "checked": true,
    "id": "090d99d05f031c578e633588d8b963b9b1fa8edd",
    "semantic_title": "promote, suppress, iterate: how language models answer one-to-many factual queries",
    "citation_count": 0,
    "authors": [
      "Tianyi Lorena Yan",
      "Robin Jia"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.816": {
    "title": "Out of Sight, Not Out of Context? Egocentric Spatial Reasoning in VLMs Across Disjoint Frames",
    "volume": "main",
    "abstract": "An embodied AI assistant operating on egocentric video must integrate spatial cues across time - for instance, determining where an object A, glimpsed a few moments ago lies relative to an object B encountered later. We introduce Disjoint-3DQA , a generative QA benchmark that evaluates this ability of VLMs by posing questions about object pairs that are not co-visible in the same frame. We evaluated seven state-of-the-art VLMs and found that models lag behind human performance by 28%, with steeper declines in accuracy (60% → 30 %) as the temporal gap widens. Our analysis further reveals that providing trajectories or bird's-eye-view projections to VLMs results in only marginal improvements, whereas providing oracle 3D coordinates leads to a substantial 20% performance increase. This highlights a core bottleneck of multi-frame VLMs in constructing and maintaining 3D scene representations over time from visual signals. Disjoint-3DQA therefore sets a clear, measurable challenge for long-horizon spatial reasoning and aims to catalyze future research at the intersection of vision, language, and embodied AI",
    "checked": true,
    "id": "49d9e3155672ed88452b92b0d10fcae0018843e5",
    "semantic_title": "out of sight, not out of context? egocentric spatial reasoning in vlms across disjoint frames",
    "citation_count": 1,
    "authors": [
      "Sahithya Ravi",
      "Gabriel Herbert Sarch",
      "Vibhav Vineet",
      "Andrew D Wilson",
      "Balasaravanan Thoravi Kumaravel"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.817": {
    "title": "Enhancing Chain-of-Thought Reasoning via Neuron Activation Differential Analysis",
    "volume": "main",
    "abstract": "Despite the impressive chain-of-thought(CoT) reasoning ability of large language models (LLMs), its underlying mechanisms remains unclear. In this paper, we explore the inner workings of LLM's CoT ability via the lens of neurons in the feed-forward layers. We propose an efficient method to identify reasoning-critical neurons by analyzing their activation patterns under reasoning chains of varying quality. Based on it, we devise a rather simple intervention method that directly stimulates these reasoning-critical neurons, to guide the generation of high-quality reasoning chains. Extended experiments validate the effectiveness of our method and demonstrate the critical role these identified neurons play in CoT reasoning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiru Tang",
      "Kun Zhou",
      "Yingqian Min",
      "Xin Zhao",
      "Jing Sha",
      "Zhichao Sheng",
      "Shijin Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.818": {
    "title": "PakBBQ: A Culturally Adapted Bias Benchmark for QA",
    "volume": "main",
    "abstract": "With the widespread adoption of Large Language Models (LLMs) across various applications, it is imperative to ensure their fairness across all user communities. However, most LLMs are trained and evaluated on Western centric data, with little attention paid to low-resource languages and regional contexts. To address this gap, we introduce PakBBQ, a culturally and regionally adapted extension of the original Bias Benchmark for Question Answering (BBQ) dataset. PakBBQ comprises over 214 templates, 17180 QA pairs across 8 categories in both English and Urdu, covering eight bias dimensions including age, disability, appearance, gender, socio-economic status, religious, regional affiliation, and language formality that are relevant in Pakistan. We evaluate multiple multilingual LLMs under both ambiguous and explicitly disambiguated contexts, as well as negative versus non negative question framings. Our experiments reveal (i) an average accuracy gain of 12% with disambiguation, (ii) consistently stronger counter bias behaviors in Urdu than in English, and (iii) marked framing effects that reduce stereotypical responses when questions are posed negatively. These findings highlight the importance of contextualized benchmarks and simple prompt engineering strategies for bias mitigation in low resource settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abdullah Hashmat",
      "Muhammad Arham Mirza",
      "Agha Ali Raza"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.819": {
    "title": "MULTIGUARD: An Efficient Approach for AI Safety Moderation Across Languages and Modalities",
    "volume": "main",
    "abstract": "The emerging capabilities of large language models (LLMs) have sparked concerns about their immediate potential for harmful misuse. The core approach to mitigate these concerns is the detection of harmful queries to the model. Current detection approaches are fallible, and are particularly susceptible to attacks that exploit mismatched generalization of model capabilities (e.g., prompts in low-resource languages or prompts provided in non-text modalities such as image and audio). To tackle this challenge, we propose OMNIGUARD, an approach for detecting harmful prompts across languages and modalities. Our approach (i) identifies internal representations of an LLM/MLLM that are aligned across languages or modalities and then (ii) uses them to build a language-agnostic or modality-agnostic classifier for detecting harmful prompts. OMNIGUARD improves harmful prompt classification accuracy by 11.57% over the strongest baseline in a multilingual setting, by 20.44% for image-based prompts, and sets a new SOTA for audio-based prompts. By repurposing embeddings computed during generation, OMNIGUARD is also very efficient (≈ 120× faster than the next fastest baseline). Code and data are available at https://github.com/vsahil/OmniGuard",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sahil Verma",
      "Keegan Hines",
      "Jeff Bilmes",
      "Charlotte Siska",
      "Luke Zettlemoyer",
      "Hila Gonen",
      "Chandan Singh"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.820": {
    "title": "Comparing human and LLM politeness strategies in free production",
    "volume": "main",
    "abstract": "Polite speech poses a fundamental alignment challenge for large language models (LLMs). Humans deploy a rich repertoire of linguistic strategies to balance informational and social goals – from positive approaches that build rapport (compliments, expressions of interest) to negative strategies that minimize imposition (hedging, indirectness).We investigate whether LLMs employ a similarly context-sensitive repertoire by comparing human and LLM responses to English-language scenarios in both constrained and open-ended production tasks.We find that larger models (≥70B parameters) successfully replicate key effects from the computational pragmatics literature, and human evaluators prefer LLM-generated responses in open-ended contexts. However, further linguistic analyses reveal that models disproportionately rely on negative politeness strategies to create distance even in positive contexts, potentially leading to misinterpretations. While LLMs thus demonstrate an impressive command of politeness strategies, these systematic differences provide important groundwork for making intentional choices about pragmatic behavior in human-AI communication",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoran Zhao",
      "Robert D. Hawkins"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.821": {
    "title": "ASTRA: A Negotiation Agent with Adaptive and Strategic Reasoning via Tool-integrated Action for Dynamic Offer Optimization",
    "volume": "main",
    "abstract": "Negotiation requires dynamically balancing self-interest and cooperation within the flow of conversation to maximize one's own utility. Yet, existing agents struggle due to bounded rationality in human data, low adaptability to counterpart behavior, and limited strategic reasoning. To address this, we introduce principle-driven negotiation agents, powered by ASTRA, a novel framework for turn-level offer optimization grounded in two core principles: opponent modeling and Tit-for-Tat reciprocity. ASTRA operates in three stages: (1) interpreting counterpart behavior, (2) optimizing counteroffers via a tool-integrated action with a linear programming (LP) solver, and (3) selecting offers based on strategy assessment and the partner's acceptance probability. Through simulations and human evaluations, our agent effectively adapts to an opponent's shifting stance and achieves favorable outcomes through enhanced adaptability and strategic reasoning. Beyond enhancing negotiation performance, it also serves as a powerful coaching tool, offering interpretable strategic feedback and optimal offer recommendations beyond human bounded rationality, with its potential further validated through human evaluation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Deuksin Kwon",
      "Jiwon Hae",
      "Emma Clift",
      "Daniel Shamsoddini",
      "Jonathan Gratch",
      "Gale Lucas"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.822": {
    "title": "CARMA: Enhanced Compositionality in LLMs via Advanced Regularisation and Mutual Information Alignment",
    "volume": "main",
    "abstract": "Large language models (LLMs) struggle with compositional generalisation, limiting their ability to systematically combine learned components to interpret novel inputs. While architectural modifications, fine-tuning, and data augmentation improve compositionality, they often have limited adaptability, face scalability constraints, or yield diminishing returns on real data. To address this, we propose CARMA, an intervention that enhances the stability and robustness of compositional reasoning in LLMs while preserving fine-tuned performance. CARMA employs mutual information regularisation and layer-wise stability constraints to mitigate feature fragmentation, ensuring structured representations persist across and within layers. We evaluate CARMA on inverse dictionary modelling and sentiment classification, measuring its impact on semantic consistency, performance stability, and robustness to lexical perturbations. Results show that CARMA reduces the variability introduced by fine-tuning, stabilises token representations, and improves compositional reasoning. While its effectiveness varies across architectures, CARMA's key strength lies in reinforcing learned structures rather than introducing new capabilities, making it a scalable auxiliary method. These findings suggest that integrating CARMA with fine-tuning can improve compositional generalisation while maintaining task-specific performance in LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nura Aljaafari",
      "Danilo Carvalho",
      "Andre Freitas"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.823": {
    "title": "MEPT: Mixture of Expert Prompt Tuning as a Manifold Mapper",
    "volume": "main",
    "abstract": "Considering deep neural networks as manifold mappers, the pretrain-then-fine-tune paradigm can be interpreted as a two-stage process: pretrain establishes a broad knowledge base, and fine-tune adjusts the model parameters to activate specific neural pathways to align with the target manifold. Although prior fine-tuning approaches demonstrate success, their rigid parameter space limits their ability to dynamically activate appropriate neural pathways, rendering them ill-equipped to adapt flexibly to the diverse and evolving data distributions. In light of this view, we propose a novel approach, Mixture of Expert Prompt Tuning (MEPT), as an effective and efficient manifold-mapping framework. MEPT leverages the Mixture of Experts architecture by integrating multiple prompt experts to adaptively learn diverse and non-stationary data distributions. Empirical evaluations demonstrate that MEPT outperforms several state-of-the-art parameter efficient baselines on SuperGLUE, achieving notable improvements in mean accuracy (e.g., 1.94%) while significantly reducing activated prompts by 79.25%. The effectiveness of MEPT is further supported by theoretical insights from manifold learning and validated through neural activation pathway visualization results. Our code is avaliable at https://runjia.tech/emnlp_mept/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runjia Zeng",
      "Guangyan Sun",
      "Qifan Wang",
      "Tong Geng",
      "Sohail Dianat",
      "Xiaotian Han",
      "Raghuveer Rao",
      "Xueling Zhang",
      "Cheng Han",
      "Lifu Huang",
      "Dongfang Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.824": {
    "title": "KG-CQR: Leveraging Structured Relation Representations in Knowledge Graphs for Contextual Query Retrieval",
    "volume": "main",
    "abstract": "The integration of knowledge graphs (KGs) with large language models (LLMs) offers significant potential to enhance the retrieval stage in retrieval-augmented generation (RAG) systems. In this study, we propose KG-CQR, a novel framework for Contextual Query Retrieval (CQR) that enhances the retrieval phase by enriching complex input queries with contextual representations derived from a corpus-centric KG. Unlike existing methods that primarily address corpus-level context loss, KG-CQR focuses on query enrichment through structured relation representations, extracting and completing relevant KG subgraphs to generate semantically rich query contexts. Comprising subgraph extraction, completion, and contextual generation modules, KG-CQR operates as a model-agnostic pipeline, ensuring scalability across LLMs of varying sizes without additional training. Experimental results on the RAGBench and MultiHop-RAG datasets demonstrate that KG-CQR outperforms strong baselines, achieving improvements of up to 4–6% in mAP and approximately 2–3% in Recall@25. Furthermore, evaluations on challenging RAG tasks such as multi-hop question answering show that, by incorporating KG-CQR, the performance outperforms the existing baseline in terms of retrieval effectiveness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chi Minh Bui",
      "Ngoc Mai Thieu",
      "Vinh Van Nguyen",
      "Jason J. Jung",
      "Khac-Hoai Nam Bui"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.825": {
    "title": "SABER: Uncovering Vulnerabilities in Safety Alignment via Cross-Layer Residual Connection",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) with safe-alignment training are powerful instruments with robust language comprehension capability. Typically LLMs undergo careful alignment training involving human feedback to ensure the acceptance of safe inputs while rejection of harmful or unsafe ones. However, these humongous models are still vulnerable to jailbreak attacks, in which malicious users attempt to generate harmful outputs that safety-aligned LLMs are trained to avoid. In this study, we find that the safety mechanisms in LLMs are predominantly prevalent in the middle-to-late layers. Based on this observation, we introduce a novel white-box jailbreak method SABER (Safety Alignment Bypass via Extra Residuals) that connects two intermediate layer s and e such that s<e with a residual connection, achieving an improvement of 51% over the best performing baseline GCG on HarmBench test set. Moreover, model demonstrates only a marginal shift in perplexity when evaluated on the validation set of HarmBench",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maithili Joshi",
      "Palash Nandi",
      "Tanmoy Chakraborty"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.826": {
    "title": "When Truthful Representations Flip Under Deceptive Instructions?",
    "volume": "main",
    "abstract": "Large language models (LLMs) tend to follow maliciously crafted instructions to generate deceptive responses, posing safety challenges. How deceptive instructions alter the internal representations of LLM compared to truthful ones remains poorly understood beyond output analysis. To bridge this gap, we investigate when and how these representations \"flip\", such as from truthful to deceptive, under deceptive versus truthful/neutral instructions. Analyzing the internal representations of Llama-3.1-8B-Instruct and Gemma-2-9B-Instruct on a factual verification task, we find the model's instructed True/False output is predictable via linear probes across all conditions based on the internal representation. Further, we use Sparse Autoencoders (SAEs) to show that the Deceptive instructions induce significant representational shifts compared to Truthful/Neutral representations (which are similar), concentrated in early-to-mid layers and detectable even on complex datasets. We also identify specific SAE features highly sensitive to deceptive instruction and use targeted visualizations to confirm distinct truthful/deceptive representational subspaces",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xianxuan Long",
      "Yao Fu",
      "Runchao Li",
      "Mu Sheng",
      "Haotian Yu",
      "Xiaotian Han",
      "Pan Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.827": {
    "title": "Can LLMs simulate the same correct solutions to free-response math problems as real students?",
    "volume": "main",
    "abstract": "Large language models (LLMs) have emerged as powerful tools for developing educational systems. While previous studies have explored modeling student mistakes, a critical gap remains in understanding whether LLMs can generate correct solutions that represent student responses to free-response problems. In this paper, we compare the distribution of solutions produced by four LLMs (one proprietary, two open-sourced general, and one open-sourced math models) with various sampling and prompting techniques and those generated by students, using conversations where students teach math problems to a conversational robot. Our study reveals discrepancies between the correct solutions produced by LLMs and by students. We discuss the practical implications of these findings for the design and evaluation of LLM-supported educational systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuya Asano",
      "Diane Litman",
      "Erin Walker"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.828": {
    "title": "Evaluating Behavioral Alignment in Conflict Dialogue: A Multi-Dimensional Comparison of LLM Agents and Humans",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) are increasingly deployed in socially complex, interaction-driven tasks, yet their ability to mirror human behavior in emotionally and strategically complex contexts remains underexplored. This study assesses the behavioral alignment of personality-prompted LLMs in adversarial dispute resolution by simulating multi-turn conflict dialogues that incorporate negotiation. Each LLM is guided by a matched Five-Factor personality profile to control for individual variation and enhance realism. We evaluate alignment across three dimensions: linguistic style, emotional expression (e.g., anger dynamics), and strategic behavior. GPT-4.1 achieves the closest alignment with humans in linguistic style and emotional dynamics, while Claude-3.7-Sonnet best reflects strategic behavior. Nonetheless, substantial alignment gaps persist. Our findings establish a benchmark for alignment between LLMs and humans in socially complex interactions, underscoring both the promise and the limitations of personality conditioning in dialogue modeling",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Deuksin Kwon",
      "Kaleen Shrestha",
      "Bin Han",
      "Elena Hayoung Lee",
      "Gale Lucas"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.829": {
    "title": "RECALL: REpresentation-aligned Catastrophic-forgetting ALLeviation via Hierarchical Model Merging",
    "volume": "main",
    "abstract": "We unveil that internal representations in large language models (LLMs) serve as reliable proxies of learned knowledge, and propose **RECALL**, a novel representation-aware model merging framework for continual learning without access to historical data. RECALL computes inter-model similarity from layer-wise hidden representations over clustered typical samples, and performs adaptive, hierarchical parameter fusion to align knowledge across models. This design enables the preservation of domain-general features in shallow layers while allowing task-specific adaptation in deeper layers. Unlike prior methods that require task labels or incur performance trade-offs, RECALL achieves seamless multi-domain integration and strong resistance to catastrophic forgetting. Extensive experiments across five NLP tasks and multiple continual learning scenarios show that RECALL outperforms baselines in both knowledge retention and generalization, providing a scalable and data-free solution for evolving LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowen Wang",
      "Haiyuan Wan",
      "Liwen Shi",
      "Chen Yang",
      "Peng He",
      "Yue Ma",
      "Haochen Han",
      "Wenhao Li",
      "Tiao Tan",
      "Yongjian Li",
      "Fangming Liu",
      "Gong Yifan",
      "Sheng Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.830": {
    "title": "Not-Just-Scaling Laws: Towards a Better Understanding of the Downstream Impact of Language Model Design Decisions",
    "volume": "main",
    "abstract": "Improvements in language model capabilities are often attributed to increasing model size or training data, but in some cases smaller models trained on curated data or with different architectural decisions can outperform larger ones trained on more tokens. What accounts for this? To quantify the impact of these design choices, we meta-analyze 92 open-source pretrained models across a wide array of scales, including state-of-the-art open-weights models as well as less performant models and those with less conventional design decisions. We find that by incorporating features besides model size and number of training tokens, we can achieve a relative 3-28% increase in ability to predict downstream performance compared with using scale alone. Analysis of model design decisions reveal insights into data composition, such as the trade-off between language and code tasks at 15-25% code, as well as the negative impact of web data on truthfulness. Broadly, our framework lays a foundation for more systematic investigation of how model development choices shape final capabilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emmy Liu",
      "Amanda Bertsch",
      "Lintang Sutawika",
      "Lindia Tjuatja",
      "Patrick Fernandes",
      "Lara Marinov",
      "Michael Chen",
      "Shreya Singhal",
      "Carolin Lawrence",
      "Aditi Raghunathan",
      "Kiril Gashteovski",
      "Graham Neubig"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.831": {
    "title": "Synthetic Socratic Debates: Examining Persona Effects on Moral Decision and Persuasion Dynamics",
    "volume": "main",
    "abstract": "As large language models (LLMs) are increasingly used in morally sensitive domains, it is crucial to understand how persona traits affect their moral reasoning and persuasive behavior. We present the first large-scale study of multi-dimensional persona effects in AI-AI debates over real-world moral dilemmas. Using a 6-dimensional persona space (age, gender, country, social class, ideology, and personality), we simulate structured debates between AI agents over 131 relationship-based cases. Our results show that personas affect initial moral stances and debate outcomes, with political ideology and personality traits exerting the strongest influence. Persuasive success varies across traits, with liberal and open personalities reaching higher consensus. While logit-based confidence grows during debates, emotional and credibility-based appeals diminish, indicating more tempered argumentation over time. These trends mirror findings from psychology and cultural studies, reinforcing the need for persona-aware evaluation frameworks for AI moral reasoning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiarui Liu",
      "Yueqi Song",
      "Yunze Xiao",
      "Mingqian Zheng",
      "Lindia Tjuatja",
      "Jana Schaich Borg",
      "Mona T. Diab",
      "Maarten Sap"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.832": {
    "title": "Linear-Time Demonstration Selection for In-Context Learning via Gradient Estimation",
    "volume": "main",
    "abstract": "This paper introduces an algorithm to select demonstration examples for in-context learning of a query set. Given a set of n examples, how can we quickly select k out of n to best serve as the conditioning for downstream inference? This problem has broad applications in prompt tuning and chain-of-thought reasoning. Since model weights remain fixed during in-context learning, previous work has sought to design methods based on the similarity of token embeddings. This work proposes a new approach based on gradients of the output taken in the input embedding space. Our approach estimates model outputs through a first-order approximation using the gradients. Then, we apply this estimation to multiple randomly sampled subsets. Finally, we aggregate the sampled subset outcomes to form an influence score for each demonstration, and select k most relevant examples. This procedure only requires pre-computing model outputs and gradients once, resulting in a linear-time algorithm relative to model and training set sizes. Extensive experiments across various models and datasets validate the efficiency of our approach. We show that the gradient estimation procedure yields approximations of full inference with less than 𝟏% error across six datasets. This allows us to scale up subset selection that would otherwise run full inference by up to 37.7× on models with up to 34 billion parameters, and outperform existing selection methods based on input embeddings by 11% on average",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziniu Zhang",
      "Zhenshuo Zhang",
      "Dongyue Li",
      "Lu Wang",
      "Jennifer Dy",
      "Hongyang R. Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.833": {
    "title": "Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents",
    "volume": "main",
    "abstract": "We present Speech Vecalign, a parallel speech document alignment method that monotonically aligns speech segment embeddings and does not depend on text transcriptions. Compared to the baseline method Global Mining, a variant of speech mining, Speech Vecalign produces longer speech-to-speech alignments. It also demonstrates greater robustness than Local Mining, another speech mining variant, as it produces less noise. We applied Speech Vecalign to 3,000 hours of unlabeled parallel English-German (En-De) speech documents from VoxPopuli, yielding about 1,000 hours of high-quality alignments. We then trained En-De speech-to-speech translation models on the aligned data. Speech Vecalign improves the En-to-De and De-to-En performance over Global Mining by 0.37 and 0.18 ASR-BLEU, respectively. Moreover, our models match or outperform SpeechMatrix model performance, despite using 8 times fewer raw speech documents",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chutong Meng",
      "Philipp Koehn"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.834": {
    "title": "TurBLiMP: A Turkish Benchmark of Linguistic Minimal Pairs",
    "volume": "main",
    "abstract": "We introduce TurBLiMP, the first Turkish benchmark of linguistic minimal pairs, designed to evaluate the linguistic abilities of monolingual and multilingual language models (LMs). Covering 16 linguistic phenomena with 1000 minimal pairs each, TurBLiMP fills an important gap in linguistic evaluation resources for Turkish. In designing the benchmark, we give extra attention to two properties of Turkish that remain understudied in current syntactic evaluations of LMs, namely word order flexibility and subordination through morphological processes. Our experiments on a wide range of LMs and a newly collected set of human acceptability judgments reveal that even cutting-edge Large LMs still struggle with grammatical phenomena that are not challenging for humans, and may also exhibit different sensitivities to word order and morphological complexity compared to humans",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ezgi Başar",
      "Francesca Padovani",
      "Jaap Jumelet",
      "Arianna Bisazza"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.835": {
    "title": "DynamicNER: A Dynamic, Multilingual, and Fine-Grained Dataset for LLM-based Named Entity Recognition",
    "volume": "main",
    "abstract": "The advancements of Large Language Models (LLMs) have spurred a growing interest in their application to Named Entity Recognition (NER) methods. However, existing datasets are primarily designed for traditional machine learning methods and are inadequate for LLM-based methods, in terms of corpus selection and overall dataset design logic. Moreover, the prevalent fixed and relatively coarse-grained entity categorization in existing datasets fails to adequately assess the superior generalization and contextual understanding capabilities of LLM-based methods, thereby hindering a comprehensive demonstration of their broad application prospects. To address these limitations, we propose DynamicNER, the first NER dataset designed for LLM-based methods with dynamic categorization, introducing various entity types and entity type lists for the same entity in different context, leveraging the generalization of LLM-based NER better. The dataset is also multilingual and multi-granular, covering 8 languages and 155 entity types, with corpora spanning a diverse range of domains. Furthermore, we introduce CascadeNER, a novel NER method based on a two-stage strategy and lightweight LLMs, achieving higher accuracy on fine-grained tasks while requiring fewer computational resources. Experiments show that DynamicNER serves as a robust and effective benchmark for LLM-based NER methods. Furthermore, we also conduct analysis for traditional methods and LLM-based methods on our dataset. Our code and dataset are openly available at https://github.com/Astarojth/DynamicNER",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanjun Luo",
      "Yingbin Jin",
      "Yiran Wang",
      "Xinfeng Li",
      "Tong Shang",
      "Xuecheng Liu",
      "Ruizhe Chen",
      "Kun Wang",
      "Hanan Salam",
      "Qingsong Wen",
      "Zuozhu Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.836": {
    "title": "Reliable and Cost-Effective Exploratory Data Analysis via Graph-Guided RAG",
    "volume": "main",
    "abstract": "Automating Exploratory Data Analysis (EDA) is critical for accelerating the workflow of data scientists. While Large Language Models (LLMs) offer a promising solution, current LLM-only approaches often exhibit limited accuracy and code reliability on less-studied or private datasets. Moreover, their effectiveness significantly diminishes with open-source LLMs compared to proprietary ones, limiting their usability in enterprises that prefer local models for privacy and cost. To address these limitations, we introduce RAGvis: a novel two-stage graph-guided Retrieval-Augmented Generation (RAG) framework. RAGvis first builds a base knowledge graph (KG) of EDA notebooks and enriches it with structured EDA operation semantics. These semantics are extracted by an LLM guided by our empirically-developed EDA operations taxonomy. Second, in the online generation stage for new datasets, RAGvis retrieves relevant operations from the KG, aligns them to the dataset's structure, refines them with LLM reasoning, and then employs a self-correcting agent to generate executable Python code. Experiments on two benchmarks demonstrate that RAGvis significantly improves code executability (pass rate), semantic accuracy, and visual quality in generated operations. This enhanced performance is achieved with substantially lower token usage compared to LLM-only baselines. Notably, our approach enables smaller, open-source LLMs to match the performance of proprietary models, presenting a reliable and cost-effective pathway for automated EDA code generation",
    "checked": true,
    "id": "43560a5e9bdfbef39cfe5add8171cde79771b9a8",
    "semantic_title": "reliable and cost-effective exploratory data analysis via graph-guided rag",
    "citation_count": 0,
    "authors": [
      "Mossad Helali",
      "Yutai Luo",
      "Tae Jun Ham",
      "Jim Plotts",
      "Ashwin Chaugule",
      "Jichuan Chang",
      "Parthasarathy Ranganathan",
      "Essam Mansour"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.837": {
    "title": "Med-PRM: Medical Reasoning Models with Stepwise, Guideline-verified Process Rewards",
    "volume": "main",
    "abstract": "Large language models have shown promise in clinical decision making, but current approaches struggle to localize and correct errors at specific steps of the reasoning process. This limitation is critical in medicine, where identifying and addressing reasoning errors is essential for accurate diagnosis and effective patient care. We introduce Med-PRM, a process reward modeling framework that leverages retrieval-augmented generation to verify each reasoning step against established medical knowledge bases. By verifying intermediate reasoning steps with evidence retrieved from clinical guidelines and literature, our model can precisely assess the reasoning quality in a fine-grained manner. Evaluations on five medical QA benchmarks and two open-ended diagnostic tasks demonstrate that Med-PRM achieves state-of-the-art performance, with improving the performance of base models by up to 13.50% using Med-PRM. Moreover, we demonstrate the generality of Med-PRM by integrating it in a plug-and-play fashion with strong policy models such as Meerkat, achieving over 80% accuracy on MedQA for the first time using small-scale models of 8 billion parameters",
    "checked": true,
    "id": "1585c43b9c584854bcb42ce4e56416c6b3bc54f3",
    "semantic_title": "med-prm: medical reasoning models with stepwise, guideline-verified process rewards",
    "citation_count": 2,
    "authors": [
      "Jaehoon Yun",
      "Jiwoong Sohn",
      "Jungwoo Park",
      "Hyunjae Kim",
      "Xiangru Tang",
      "Daniel Shao",
      "Yong Hoe Koo",
      "Ko Minhyeok",
      "Qingyu Chen",
      "Mark Gerstein",
      "Michael Moor",
      "Jaewoo Kang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.838": {
    "title": "Graders Should Cheat: Privileged Information Enables Expert-Level Automated Evaluations",
    "volume": "main",
    "abstract": "Auto-evaluating language models (LMs), *i.e*., using a grader LM to evaluate the candidate LM, is an appealing way to accelerate the evaluation process and the cost associated with it. But this presents a paradox: how can we trust the grader LM, which is presumably weaker than the candidate LM, to assess problems that are beyond the frontier of the capabilities of either model or both? For instance, today's LMs struggle on graduate-level physics and Olympiad-level math, making them unreliable graders in these domains. We show that providing *privileged information* – such as ground-truth solutions or problem-specific guidelines – improves automated evaluations on such frontier problems. This approach offers two key advantages. First, it expands the range of problems where LMs graders apply. Specifically, weaker models can now rate the predictions of stronger models. Second, privileged information can be used to devise easier variations of challenging problems which improves the separability of different LMs on tasks where their performance is generally low. With this approach, general-purpose LM graders match the state of the art performance on *RewardBench*, surpassing almost all the specially-tuned models. LM graders also outperform individual human raters on *Vibe-Eval*, and approach human expert graders on Olympiad-level math problems",
    "checked": true,
    "id": "4a86316eae73dfb3bd835d08281909bc6fb9bbfd",
    "semantic_title": "graders should cheat: privileged information enables expert-level automated evaluations",
    "citation_count": 2,
    "authors": [
      "Jin Peng Zhou",
      "Séb Arnold",
      "Nan Ding",
      "Kilian Q Weinberger",
      "Nan Hua",
      "Fei Sha"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.839": {
    "title": "SAMULE: Self-Learning Agents Enhanced by Multi-level Reflection",
    "volume": "main",
    "abstract": "Despite the rapid advancements in LLM agents, they still face the challenge of generating meaningful reflections due to inadequate error analysis and a reliance on rare successful trajectories, especially in complex tasks. In this work, we propose SAMULE, a new framework for self-learning agents powered by a retrospective language model that is trained based on Multi-Level Reflection Synthesis. It first synthesizes high-quality reflections across three complementary levels: Single-Trajectory Learning (micro-level) for detailed error correction; Intra-Task Learning (meso-level) to build error taxonomies across multiple trials of the same task, and Inter-Task Learning (macro-level) to extract transferable insights based on same typed errors from diverse task failures. Then we fine-tune a language model serving as the retrospective model to generate reflections during inference. We further extend our framework to interactive settings through a foresight-based reflection mechanism, enabling agents to proactively reflect and adapt during user interactions by comparing predicted and actual responses. Extensive experiments on three challenging benchmarks—TravelPlanner, NATURAL PLAN, and Tau-bench—demonstrate that our approach significantly outperforms reflection-based baselines. Our results highlight the critical role of well-designed reflection synthesis and failure-centric learning in building self-improving LLM agents",
    "checked": true,
    "id": "7c9ff4a4360ad562e9bfff00998b2232752e3dbc",
    "semantic_title": "samule: self-learning agents enhanced by multi-level reflection",
    "citation_count": 0,
    "authors": [
      "Yubin Ge",
      "Salvatore Romeo",
      "Jason Cai",
      "Monica Sunkara",
      "Yi Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.840": {
    "title": "Database-Augmented Query Representation for Information Retrieval",
    "volume": "main",
    "abstract": "Information retrieval models that aim to search for documents relevant to a query have shown multiple successes, which have been applied to diverse tasks. Yet, the query from the user is oftentimes short, which challenges the retrievers to correctly fetch relevant documents. To tackle this, previous studies have proposed expanding the query with a couple of additional (user-related) features related to it. However, they may be suboptimal to effectively augment the query, and there is plenty of other information available to augment it in a relational database. Motivated by this fact, we present a novel retrieval framework called Database-Augmented Query representation (DAQu), which augments the original query with various (query-related) metadata across multiple tables. In addition, as the number of features in the metadata can be very large and there is no order among them, we encode them with the graph-based set-encoding strategy, which considers hierarchies of features in the database without order. We validate our DAQu in diverse retrieval scenarios, demonstrating that it significantly enhances overall retrieval performance over relevant baselines",
    "checked": true,
    "id": "e0a3b67665dfd660d0adf30137e15d315ecca906",
    "semantic_title": "database-augmented query representation for information retrieval",
    "citation_count": 5,
    "authors": [
      "Soyeong Jeong",
      "Jinheon Baek",
      "Sukmin Cho",
      "Sung Ju Hwang",
      "Jong C. Park"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.841": {
    "title": "The Enemy from Within: A Study of Political Delegitimization Discourse in Israeli Political Speech",
    "volume": "main",
    "abstract": "We present the first large-scale computational study of political delegitimization discourse (PDD), defined as symbolic attacks on the normative validity of political entities. We curate and manually annotate a novel Hebrew-language corpus of 10,410 sentences drawn from parliamentary speeches (1993-2023), Facebook posts, and leading news outlets (2018-2021), of which 1,812 instances (17.4%) exhibit PDD and 642 carry additional annotations for intensity, incivility, target type, and affective framing. We introduce a two-stage classification pipeline, and benchmark finetuned encoder models and decoder LLMs. Our best model (DictaLM 2.0) attains an F1 of 0.74 for binary PDD detection and a macro-F1 of 0.67 for classification of delegitimization characteristics. Applying this classifier to longitudinal and cross-platform data, we see a marked rise in PDD over three decades, higher prevalence on social media versus parliamentary debate, greater use by male politicians than by their female counterparts, and stronger tendencies among right-leaning actors, with pronounced spikes during election campaigns and major political events. Our findings demonstrate the feasibility and value of automated PDD analysis for analyzing democratic discourse",
    "checked": true,
    "id": "796ccc365e2bfb38dd2278ee4607183d96e45563",
    "semantic_title": "the enemy from within: a study of political delegitimization discourse in israeli political speech",
    "citation_count": 0,
    "authors": [
      "Naama Rivlin-Angert",
      "Guy Mor-Lan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.842": {
    "title": "Attention Eclipse: Manipulating Attention to Bypass LLM Safety-Alignment",
    "volume": "main",
    "abstract": "Recent research has shown that carefully crafted jailbreak inputs can induce large language models to produce harmful outputs, despite safety measures such as alignment. It is important to anticipate the range of potential Jailbreak attacks to guide effective defenses and accurate assessment of model safety. In this paper, we present a new approach for generating highly effective Jailbreak attacks that manipulate the attention of the model to selectively strengthen or weaken attention among different parts of the prompt. By harnessing attention loss, we develop more effective jailbreak attacks, that are also transferrable. The attacks amplify the success rate of existing Jailbreak algorithms, including GCG, AutoDAN, and ReNeLLM, while lowering their generation cost (for example, the amplified GCG attack achieves 91.2% ASR, vs. 67.9% for the original attack on Llama2-7B-chat/AdvBench, using less than a third of the generation time)",
    "checked": true,
    "id": "911bf2f4066036a4dfa1de22888526e704f72c47",
    "semantic_title": "attention eclipse: manipulating attention to bypass llm safety-alignment",
    "citation_count": 1,
    "authors": [
      "Pedram Zaree",
      "Md Abdullah Al Mamun",
      "Quazi Mishkatul Alam",
      "Yue Dong",
      "Ihsen Alouani",
      "Nael Abu-Ghazaleh"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.843": {
    "title": "Representation Potentials of Foundation Models for Multimodal Alignment: A Survey",
    "volume": "main",
    "abstract": "Foundation models learn highly transferable representations through large-scale pretraining on diverse data. An increasing body of research indicates that these representations exhibit a remarkable degree of similarity across architectures and modalities. In this survey, we investigate the representation potentials of foundation models, defined as the latent capacity of their learned representations to capture task-specific information within a single modality while also providing a transferable basis for alignment and unification across modalities. We begin by reviewing representative foundation models and the key metrics that make alignment measurable. We then synthesize empirical evidence of representation potentials from studies in vision, language, speech, multimodality, and neuroscience. The evidence suggests that foundation models often exhibit structural regularities and semantic consistencies in their representation spaces, positioning them as strong candidates for cross-modal transfer and alignment. We further analyze the key factors that foster representation potentials, discuss open questions, and highlight potential challenges",
    "checked": true,
    "id": "fe860304efe129ee389725e986bdeb9eab9fd90f",
    "semantic_title": "representation potentials of foundation models for multimodal alignment: a survey",
    "citation_count": 0,
    "authors": [
      "Jianglin Lu",
      "Hailing Wang",
      "Yi Xu",
      "Yizhou Wang",
      "Kuo Yang",
      "Yun Fu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.844": {
    "title": "Draft Model Knows When to Stop: Self-Verification Speculative Decoding for Long-Form Generation",
    "volume": "main",
    "abstract": "Conventional speculative decoding (SD) methods utilize a predefined length policy for proposing drafts, which implies the premise that the target model smoothly accepts the proposed draft tokens. However, reality deviates from this assumption: the oracle draft length varies significantly, and the fixed-length policy hardly satisfies such a requirement. Moreover, such discrepancy is further exacerbated in scenarios involving complex reasoning and long-form generation, particularly under test-time scaling for reasoning-specialized models. Through both theoretical and empirical estimation, we establish that the discrepancy between the draft and target models can be approximated by the draft model's prediction entropy: a high entropy indicates a low acceptance rate of draft tokens, and vice versa. Based on this insight, we propose SVIP: Self-Verification Length Policy for Long-Context Speculative Decoding, which is a training-free dynamic length policy for speculative decoding systems that adaptively determines the lengths of draft sequences by referring to the draft entropy. Experimental results on mainstream SD benchmarks as well as reasoning-heavy benchmarks demonstrate the superior performance of SVIP, achieving up to 17% speedup on MT-Bench at 8K context compared with fixed draft lengths, and 22% speedup for QwQ in long-form reasoning",
    "checked": true,
    "id": "1154ef4ba126a2adae3a8568f93fa72031dc693f",
    "semantic_title": "draft model knows when to stop: self-verification speculative decoding for long-form generation",
    "citation_count": 2,
    "authors": [
      "Ziyin Zhang",
      "Jiahao Xu",
      "Tian Liang",
      "Xingyu Chen",
      "Zhiwei He",
      "Rui Wang",
      "Zhaopeng Tu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.845": {
    "title": "Visual-Aware Speech Recognition for Noisy Scenarios",
    "volume": "main",
    "abstract": "Humans have the ability to utilize visual cues, such as lip movements and visual scenes, to enhance auditory perception, particularly in noisy environments. However, current Automatic Speech Recognition (ASR) or Audio-Visual Speech Recognition (AVSR) models often struggle in noisy scenarios. To solve this task, we propose a model that improves transcription by correlating noise sources to visual cues. Unlike works that rely on lip motion and require the speaker's visibility, we exploit broader visual information from the environment. This allows our model to naturally filter speech from noise and improve transcription, much like humans do in noisy scenarios. Our method re-purposes pretrained speech and visual encoders, linking them with multi-headed attention. This approach enables the transcription of speech and the prediction of noise labels in video inputs. We introduce a scalable pipeline to develop audio-visual datasets, where visual cues correlate to noise in the audio. We show significant improvements over existing audio-only models in noisy scenarios. Results also highlight that visual cues play a vital role in improved transcription accuracy",
    "checked": true,
    "id": "3d1a867278c654c9eea4062ed2a198cd13e80955",
    "semantic_title": "visual-aware speech recognition for noisy scenarios",
    "citation_count": 0,
    "authors": [
      "Balaji Darur",
      "Karan Singla"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.846": {
    "title": "Advancing Arabic Diacritization: Improved Datasets, Benchmarking, and State-of-the-Art Models",
    "volume": "main",
    "abstract": "Arabic diacritics, similar to short vowels in English, provide phonetic and grammatical information but are typically omitted in written Arabic, leading to ambiguity. Diacritization (aka diacritic restoration or vowelization) is essential for natural language processing. This paper advances Arabic diacritization through the following contributions: first, we propose a methodology to analyze and refine a large diacritized corpus to improve training quality. Second, we introduce WikiNews-2024, a multi-reference evaluation methodology with an updated version of the standard benchmark \"WikiNews-2014\". In addition, we explore various model architectures and propose a BiLSTM-based model that achieves state-of-the-art results with 3.12% and 2.70% WER on WikiNews-2014 and WikiNews-2024, respectively. Moreover, we develop a model that preserves user-specified diacritics while maintaining accuracy. Lastly, we demonstrate that augmenting training data enhances performance in low-resource settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abubakr Mohamed",
      "Hamdy Mubarak"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.847": {
    "title": "Implicit Values Embedded in How Humans and LLMs Complete Subjective Everyday Tasks",
    "volume": "main",
    "abstract": "Large language models (LLMs) can underpin AI assistants that help users with everyday tasks, such as by making recommendations or performing basic computation. Despite AI assistants' promise, little is known about the implicit values these assistants display while completing subjective everyday tasks. Humans may consider values like environmentalism, charity, and diversity. To what extent do LLMs exhibit these values in completing everyday tasks? How do they compare with humans? We answer these questions by auditing how six popular LLMs complete 30 everyday tasks, comparing LLMs to each other and to 100 human crowdworkers from the US. We find LLMs often do not align with humans, nor with other LLMs, in the implicit values exhibited",
    "checked": true,
    "id": "7c8a7c8d9c72b8fbd2336be0aecbf5a09ac264b7",
    "semantic_title": "implicit values embedded in how humans and llms complete subjective everyday tasks",
    "citation_count": 0,
    "authors": [
      "Arjun Arunasalam",
      "Madison Pickering",
      "Z. Berkay Celik",
      "Blase Ur"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.848": {
    "title": "Dynamic Retriever for In-Context Knowledge Editing via Policy Optimization",
    "volume": "main",
    "abstract": "Large language models (LLMs) excel at factual recall yet still propagate stale or incorrect knowledge. In‐context knowledge editing offers a gradient-free remedy suitable for black-box APIs, but current editors rely on static demonstration sets chosen by surface-level similarity, leading to two persistent obstacles: (i) a quantity–quality trade-off, and (ii) lack of adaptivity to task difficulty. We address these issues by dynamically selecting supporting demonstrations according to their utility for the edit. We propose **D**ynamic **R**etriever for **I**n-Context **K**nowledge **E**diting (DR-IKE), a lightweight framework that (1) trains a BERT retriever with REINFORCE to rank demonstrations by editing reward, and (2) employs a *learnable threshold σ* to prune low-value examples, shortening the prompt when the edit is easy and expanding it when the task is hard. DR-IKE performs editing without modifying model weights, relying solely on forward passes for compatibility with black-box LLMs. On the CounterFact benchmark, it improves edit success by up to 17.1%, reduces latency by 41.6%, and preserves accuracy on unrelated queries—demonstrating scalable and adaptive knowledge editing",
    "checked": true,
    "id": "0c9ecac68bd587ec11c0662d116220171b547a30",
    "semantic_title": "dynamic retriever for in-context knowledge editing via policy optimization",
    "citation_count": 0,
    "authors": [
      "Mahmud Wasif Nafee",
      "Maiqi Jiang",
      "Haipeng Chen",
      "Yanfu Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.849": {
    "title": "LVLMs are Bad at Overhearing Human Referential Communication",
    "volume": "main",
    "abstract": "During spontaneous conversations, speakers collaborate on novel referring expressions, which they can then re-use in subsequent conversations. Understanding such referring expressions is an important ability for an embodied agent, so that it can carry out tasks in the real world. This requires integrating and understanding language, vision, and conversational interaction. We study the capabilities of seven state-of-the-art Large Vision Language Models (LVLMs) as overhearers to a corpus of spontaneous conversations between pairs of human discourse participants engaged in a collaborative object-matching task. We find that such a task remains challenging for current LVLMs and they all fail to show a consistent performance improvement as they overhear more conversations from the same discourse participants repeating the same task for multiple rounds. We release our corpus and code for reproducibility and to facilitate future research",
    "checked": true,
    "id": "bc242bb2087192ce72730e4990193762335e7a97",
    "semantic_title": "lvlms are bad at overhearing human referential communication",
    "citation_count": 1,
    "authors": [
      "Zhengxiang Wang",
      "Weiling Li",
      "Panagiotis Kaliosis",
      "Owen Rambow",
      "Susan Brennan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.850": {
    "title": "Let's Reason Formally: Natural-Formal Hybrid Reasoning Enhances LLM's Math Capability",
    "volume": "main",
    "abstract": "Enhancing the mathematical reasoning capabilities of LLMs has garnered significant attention in both the mathematical and computer science communities. Recent works have made substantial progress in both Natural Language (NL) reasoning and Formal Language (FL) reasoning by leveraging the potential of pure Reinforcement Learning (RL) methods on base models. However, RL approaches struggle to impart new capabilities not presented in the base model, highlighting the need to integrate more knowledge like FL into NL math reasoning effectively. Yet, this integration is challenging due to inherent disparities in problem structure and reasoning format between NL and FL. To address these challenges, we introduce **NL-FL HybridReasoning (NFL-HR)**, an end-to-end framework designed to incorporate the FL expert into NL math problem-solving. To bridge the NL and FL input format gap, we propose the *NL-FL Problem Alignment* method, which reformulates the Question-Answering (QA) problems in NL as existence theorems in FL. Subsequently, the *Mixed Problem Input* technique we provide enables the FL reasoner to handle both QA and existence problems concurrently. Lastly, we mitigate the NL and FL output format gap in reasoning through an LLM-based *Answer Extraction* mechanism. Comprehensive experiments demonstrate that the **NFL-HR** framework achieves **89.80%** and **84.34%** accuracy rates on the MATH-500 and the AMC benchmarks, surpassing the NL baseline by 4.60% and 4.82%, respectively. Notably, some problems resolved by our framework remain unsolved by the NL baseline model even under a larger number of trials",
    "checked": true,
    "id": "8791d0fd351f9b8fa294087bbdba817cba862dc8",
    "semantic_title": "let's reason formally: natural-formal hybrid reasoning enhances llm's math capability",
    "citation_count": 6,
    "authors": [
      "Ruida Wang",
      "Yuxin Li",
      "Yi R. Fung",
      "Tong Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.851": {
    "title": "TORSO: Template-Oriented Reasoning Towards General Tasks",
    "volume": "main",
    "abstract": "The approaches that guide Large Language Models (LLMs) to emulate human reasoning during response generation have emerged as an effective method for enabling them to solve complex problems in a step-by-step manner, thereby achieving superior performance. However, most existing approaches using few-shot prompts to generate responses heavily depend on the provided examples, limiting the utilization of the model's inherent reasoning capabilities. Moreover, constructing task-specific few-shot prompts is often costly and may lead to inconsistencies across different tasks. In this work, we introduce Template Oriented Reasoning (TORSO), which elicits the model to utilize internal reasoning abilities to generate proper responses across various tasks without the need for manually crafted few-shot examples. Our experimental results demonstrate that TORSO achieves strong performance on diverse LLMs benchmarks with reasonable rationales",
    "checked": true,
    "id": "114492f6c13930f7c5f264b883dc176bcc7b3b35",
    "semantic_title": "torso: template-oriented reasoning towards general tasks",
    "citation_count": 0,
    "authors": [
      "Minhyuk Kim",
      "Seungyoon Lee",
      "Heuiseok Lim"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.852": {
    "title": "Prototypical Human-AI Collaboration Behaviors from LLM-Assisted Writing in the Wild",
    "volume": "main",
    "abstract": "As large language models (LLMs) are used in complex writing workflows, users engage in multi-turn interactions to steer generations to better fit their needs. Rather than passively accepting output, users actively refine, explore, and co-construct text. We conduct a large scale analysis of this collaborative behavior for users engaged in writing tasks in the wild with two popular AI assistants, Bing Copilot and WildChat. Our analysis goes beyond simple task classification or satisfaction estimation common in prior work and instead characterizes how users interact with LLMs through the course of a session. We identify prototypical behaviors in how users interact with LLMs in prompts following their original request. We refer to these as Prototypical Human AI Collaboration Behaviors (PATHs) and find that a small group of PATHs explain a majority of the variation seen in user-LLM interaction. These PATHs span users revising intents, exploring texts, posing questions, adjusting style or injecting new content. Next, we find statistically significant correlations between specific writing intents and PATHs, revealing how users' intents shape their collaboration behaviors. We conclude by discussing the implications of our findings on LLM alignment",
    "checked": true,
    "id": "eb7b2be98e20ebcc08643aedd68e68aa890899c2",
    "semantic_title": "prototypical human-ai collaboration behaviors from llm-assisted writing in the wild",
    "citation_count": 3,
    "authors": [
      "Sheshera Mysore",
      "Debarati Das",
      "Hancheng Cao",
      "Bahareh Sarrafzadeh"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.853": {
    "title": "WildScore: Benchmarking MLLMs in-the-Wild Symbolic Music Reasoning",
    "volume": "main",
    "abstract": "Recent advances in Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities across various vision-language tasks. However, their reasoning abilities in the multimodal symbolic music domain remain largely unexplored.We introduce WildScore, the first in-the-wild multimodal symbolic music reasoning and analysis benchmark, designed to evaluate MLLMs' capacity to interpret real-world music scores and answer complex musicological queries. Each instance in WildScore is sourced from genuine musical compositions and accompanied by authentic user-generated questions and discussions, capturing the intricacies of practical music analysis. To facilitate a comprehensive evaluation, we propose a systematic taxonomy,comprising both high-level and fine-grained musicological ontologies. Furthermore, we frame complex music reasoning as multiple-choice question answering,enabling controlled and scalable assessment of MLLMs' symbolic music understanding. Empirical benchmarking of state-of-the-art MLLMs on WildScore reveals intriguing patterns in their visual-symbolic reasoning, uncovering both promising directions and persistent challenges for MLLMs in symbolic music reasoning and analysis.We release the dataset and code",
    "checked": true,
    "id": "016f06a97e7e3370e98aae26225f6c222696601d",
    "semantic_title": "wildscore: benchmarking mllms in-the-wild symbolic music reasoning",
    "citation_count": 2,
    "authors": [
      "Gagan Mundada",
      "Yash Vishe",
      "Amit Namburi",
      "Xin Xu",
      "Zachary Novack",
      "Julian McAuley",
      "Junda Wu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.854": {
    "title": "TRIAL: Token Relations and Importance Aware Late-interaction for Accurate Text Retrieval",
    "volume": "main",
    "abstract": "Late-interaction based multi-vector retrieval systems have greatly advanced the field of information retrieval by enabling fast and accurate search over millions of documents. However, these systems rely on a naive summation of token-level similarity scores which often leads to inaccurate relevance estimation caused by the tokenization of semantic units (e.g., words and phrases) and the influence of low-content words (e.g., articles and prepositions). To address these challenges, we propose **TRIAL**: **T**oken **R**elations and **I**mportance **A**ware **L**ate-interaction, which enhances late interaction by explicitly modeling token relations and token importance in relevance scoring. Extensive experiments on three widely used benchmarks show that TRIAL achieves state-of-the-art accuracy, with an nDCG@10 of 46.3 on MSMARCO (in-domain), and average nDCG@10 scores of 51.09 and 72.15 on BEIR and LoTTE Search (out-of-domain), respectively. With superior accuracy, TRIAL maintains competitive retrieval speed compared to existing late-interaction methods, making it a practical solution for large-scale text retrieval",
    "checked": true,
    "id": "b9f368a49139fd999020de60b8338f8751f28b43",
    "semantic_title": "trial: token relations and importance aware late-interaction for accurate text retrieval",
    "citation_count": 0,
    "authors": [
      "Hyukkyu Kang",
      "Injung Kim",
      "Wook-Shin Han"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.855": {
    "title": "Do Large Language Models excel in Complex Logical Reasoning with Formal Language?",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have been shown to achieve breakthrough performances on complex logical reasoning tasks. Nevertheless, most existing research focuses on employing formal language to guide LLMs for deriving reliable reasoning paths, with systematic evaluations of these capabilities still being limited. In this paper, we aim to conduct a comprehensive evaluation of LLMs across various logical reasoning problems utilizing formal languages. From the perspective of three dimensions, i.e., spectrum of LLMs, taxonomy of tasks, and format of trajectories, our key findings are: 1) Thinking models significantly outperform Instruct models, especially when formal language is employed; 2). All LLMs exhibit limitations in inductive reasoning capability, irrespective of whether they use a formal language; 3). Data with PoT format achieves the best generalization performance across other languages. Additionally, we also curate the formal-relative training data to further enhance the small language models, and the experimental results indicate that a simple rejected fine-tuning method can better enable LLMs to generalize across formal languages and achieve the best overall performance",
    "checked": true,
    "id": "a1274b0042bafb716e8b5f86f04f010c9ff038be",
    "semantic_title": "do large language models excel in complex logical reasoning with formal language?",
    "citation_count": 3,
    "authors": [
      "Jin Jiang",
      "Jianing Wang",
      "Yuchen Yan",
      "Yang Liu",
      "Jianhua Zhu",
      "Mengdi Zhang",
      "Liangcai Gao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.856": {
    "title": "Fair or Framed? Political Bias in News Articles Generated by LLMs",
    "volume": "main",
    "abstract": "Despite biases in Large Language Models (LLMs) being widely researched, systematic explorations of political biases in news article generation tasks remain underexplored. This study evaluates political bias across seven LLMs by leveraging our PublicViews dataset-extracted from the TwinViews-13K corpus-comprising 31 topics and 31,692 statements. We analyze 10,850 articles, finding left-leaning bias persists in generation tasks, with neutral content remaining rare even under balanced opinion settings. Models exhibit asymmetric behavior in minority opinion scenarios, amplifying preferred viewpoints when in minority while conforming to majority opinions otherwise. Notably, all models employ ‘stance-flipping quotations\" (altering supporters' statements to express opposite viewpoints) in 33-38% of quotations despite explicit instructions against distortion. Consistent with prior research, increased model size failed to enhance neutrality. This research measures political bias in LLM-generated news, analyzes its mechanisms, and reveals how opinion distribution and explicitness affect bias expression. Our results highlight how LLMs can introduce unintended political bias in generative contexts. We publicly release our PublicViews corpus and code at https://anonymous.4open.science/r/Fair-or-Framed-46F1",
    "checked": true,
    "id": "4da8a79e0939b9fa1abca1f443749c19cea3a264",
    "semantic_title": "fair or framed? political bias in news articles generated by llms",
    "citation_count": 0,
    "authors": [
      "Junho Yoo",
      "Youhyun Shin"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.857": {
    "title": "ReviewRL: Towards Automated Scientific Review with RL",
    "volume": "main",
    "abstract": "Peer review is essential for scientific progress but faces growing challenges due to increasing submission volumes and reviewer fatigue. Existing automated review approaches struggle with factual accuracy, rating consistency, and analytical depth, often generating superficial or generic feedback lacking the insights characteristic of high-quality human reviews. We introduce ReviewRL, a reinforcement learning framework for generating comprehensive and factually grounded scientific paper reviews. Our approach combines: (1) an ArXiv-MCP retrieval-augmented context generation pipeline that incorporates relevant scientific literature, (2) supervised fine-tuning that establishes foundational reviewing capabilities, and (3) a reinforcement learning procedure with a composite reward function that jointly enhances review quality and rating accuracy. Experiments on ICLR 2025 papers demonstrate that ReviewRL significantly outperforms existing methods across both rule-based metrics and model-based quality assessments. ReviewRL establishes a foundational framework for RL-driven automatic critique generation in scientific discovery, demonstrating promising potential for future development in this domain. The implementation of ReviewRL will be released at GitHub",
    "checked": true,
    "id": "278d12fdeb3fe3e5ad1e50fffb9ec8839de324c0",
    "semantic_title": "reviewrl: towards automated scientific review with rl",
    "citation_count": 0,
    "authors": [
      "Sihang Zeng",
      "Kai Tian",
      "Kaiyan Zhang",
      "Yuru Wang",
      "Junqi Gao",
      "Runze Liu",
      "Sa Yang",
      "Jingxuan Li",
      "Xinwei Long",
      "Jiaheng Ma",
      "Biqing Qi",
      "Bowen Zhou"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.858": {
    "title": "Grammar Pruning: Enabling Low-Latency Zero-Shot Task-Oriented Language Models for Edge AI",
    "volume": "main",
    "abstract": "Edge deployment of task-oriented semantic parsers demands high accuracy under tight latency and memory budgets. We present Grammar Pruning, a lightweight zero-shot framework that begins with a user-defined schema of API calls and couples a rule-based entity extractor with an iterative grammar-constrained decoder: extracted items dynamically prune the context-free grammar, limiting generation to only those intents, slots, and values that remain plausible at each step. This aggressive search-space reduction both reduces hallucinations and slashes decoding time. On the adapted FoodOrdering, APIMixSNIPS, and APIMixATIS benchmarks, Grammar Pruning with small language models achieves an average execution accuracy of over 90%—rivaling State-of-the-Art, cloud-based solutions—while sustaining at least 2x lower end-to-end latency than existing methods. By requiring nothing beyond the domain's full API schema values yet delivering precise, real-time natural-language understanding, Grammar Pruning positions itself as a practical building block for future edge-AI applications that cannot rely on large models or cloud offloading",
    "checked": true,
    "id": "8be9eb3f892671c82de3204aca7824e1fb3fd10d",
    "semantic_title": "grammar pruning: enabling low-latency zero-shot task-oriented language models for edge ai",
    "citation_count": 0,
    "authors": [
      "Octavian Alexandru Trifan",
      "Jason Lee Weber",
      "Marc Titus Trifan",
      "Alexandru Nicolau",
      "Alexander Veidenbaum"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.859": {
    "title": "Calibrating LLMs for Text-to-SQL Parsing by Leveraging Sub-clause Frequencies",
    "volume": "main",
    "abstract": "While large language models (LLMs) achieve strong performance on text-to-SQL parsing, they sometimes exhibit unexpected failures in which they are confidently incorrect. Building trustworthy text-to-SQL systems thus requires eliciting reliable uncertainty measures from the LLM. In this paper, we study the problem of providing a calibrated confidence score that conveys the likelihood of an output query being correct. Our work is the first to establish a benchmark for post-hoc calibration of LLM-based text-to-SQL parsing. In particular, we show that Platt scaling, a canonical method for calibration, provides substantial improvements over directly using raw model output probabilities as confidence scores. Furthermore, we propose a method for text-to-SQL calibration that leverages the structured nature of SQL queries to provide more granular signals of correctness, named \"sub-clause frequency\" (SCF) scores. Using multivariate Platt scaling (MPS), our extension of the canonical Platt scaling technique, we combine individual SCF scores into an overall accurate and calibrated score. Empirical evaluation on two popular text-to-SQL datasets shows that our approach of combining MPS and SCF yields further improvements in calibration and the related task of error detection over traditional Platt scaling",
    "checked": true,
    "id": "fabbd13bedcc025a08a31c340fd91b2b32cb52d6",
    "semantic_title": "calibrating llms for text-to-sql parsing by leveraging sub-clause frequencies",
    "citation_count": 1,
    "authors": [
      "Terrance Liu",
      "Shuyi Wang",
      "Daniel Preotiuc-Pietro",
      "Yash Chandarana",
      "Chirag Gupta"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.860": {
    "title": "REACT: Representation Extraction And Controllable Tuning to Overcome Overfitting in LLM Knowledge Editing",
    "volume": "main",
    "abstract": "Large language model editing methods frequently suffer from overfitting, wherein factual updates can propagate beyond their intended scope, overemphasizing the edited target even when it's contextually inappropriate. To address this challenge, we introduce REACT (Representation Extraction And Controllable Tuning), a unified two-phase framework designed for precise and controllable knowledge editing. In the initial phase, we utilize tailored stimuli to extract latent factual representations and apply Principal Component Analysis with a simple learnbale linear transformation to compute a directional \"belief shift\" vector for each instance. In the second phase, we apply controllable perturbations to hidden states using the obtained vector with a magnitude scalar, gated by a pre-trained classifier that permits edits only when contextually necessary. Relevant experiments on EVOKE benchmarks demonstrate that REACT significantly reduces overfitting across nearly all evaluation metrics, and experiments on COUNTERFACT and MQuAKE shows that our method preserves balanced basic editing performance (reliability, locality, and generality) under diverse editing scenarios",
    "checked": true,
    "id": "7f069a3d98f33316eabd015dc0f7616b37b19e33",
    "semantic_title": "react: representation extraction and controllable tuning to overcome overfitting in llm knowledge editing",
    "citation_count": 0,
    "authors": [
      "Haitian Zhong",
      "Yuhuan Liu",
      "Ziyang Xu",
      "Guofan Liu",
      "Qiang Liu",
      "Shu Wu",
      "Zhe Zhao",
      "Liang Wang",
      "Tieniu Tan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.861": {
    "title": "ThinkEdit: Interpretable Weight Editing to Mitigate Overly Short Thinking in Reasoning Models",
    "volume": "main",
    "abstract": "Recent studies have shown that Large Language Models (LLMs) augmented with chain-of-thought (CoT) reasoning demonstrate impressive problem-solving abilities. However, in this work, we identify a recurring issue where these models occasionally generate overly short reasoning, leading to degraded performance on even simple mathematical problems. Specifically, we investigate how reasoning length is embedded in the hidden representations of reasoning models and its impact on accuracy. Our analysis reveals that reasoning length is governed by a linear direction in the representation space, allowing us to induce overly short reasoning by steering the model along this direction. Building on this insight, we introduce ThinkEdit, a simple yet effective weight-editing approach to mitigate the issue of overly short reasoning. We first identify a small subset of attention heads (approximately 4%) that predominantly drive short reasoning behavior. We then edit the output projection weights of these heads to remove the short reasoning direction. With changes to only 0.2% of the model's parameters, ThinkEdit effectively reduces overly short reasoning and yields notable accuracy gains for short reasoning outputs (+6.39%), along with an overall improvement across multiple math benchmarks (+3.34%). Our findings provide new mechanistic insights into how reasoning length is controlled within LLMs and highlight the potential of fine-grained model interventions to improve reasoning quality",
    "checked": true,
    "id": "855e01ddb58ce23a8960c17cc775a2e9936b02e7",
    "semantic_title": "thinkedit: interpretable weight editing to mitigate overly short thinking in reasoning models",
    "citation_count": 6,
    "authors": [
      "Chung-En Sun",
      "Ge Yan",
      "Tsui-Wei Weng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.862": {
    "title": "Incorporating Diverse Perspectives in Cultural Alignment: Survey of Evaluation Benchmarks Through A Three-Dimensional Framework",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) increasingly serve diverse global audiences, making it critical for responsible AI deployment across cultures. While recent works have proposed various approaches to enhance cultural alignment in LLMs, a systematic analysis of their evaluation benchmarks remains needed. We propose a novel framework that conceptualizes alignment along three dimensions: Cultural Group (who to align with), Cultural Elements (what to align), and Awareness Scope (how to align: majority-focused vs. diversity-aware). Through this framework, we analyze 105 cultural alignment evaluation benchmarks, revealing significant imbalances: Region (37.9%) and Language (28.9%) dominate Cultural Group representation; Social and Political Relations (25.1%) and Speech and Language (20.9%) concentrate Cultural Elements coverage; and an overwhelming majority (97.1%) of datasets adopt majority-focused Awareness Scope approaches. In a case study examining AI safety evaluation across nine Asian countries (Section 5), we demonstrate how our framework reveals critical gaps between existing benchmarks and real-world cultural biases identified in the study, providing actionable guidance for developing more comprehensive evaluation resources tailored to specific deployment contexts",
    "checked": true,
    "id": "e7da33be896d6fc05cf0a5a55fa3731ec967f833",
    "semantic_title": "incorporating diverse perspectives in cultural alignment: survey of evaluation benchmarks through a three-dimensional framework",
    "citation_count": 0,
    "authors": [
      "Meng-Chen Wu",
      "Si-Chi Chin",
      "Tess Wood",
      "Ayush Goyal",
      "Narayanan Sadagopan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.863": {
    "title": "Are Large Language Models Chronically Online Surfers? A Dataset for Chinese Internet Meme Explanation",
    "volume": "main",
    "abstract": "Large language models (LLMs) are trained on vast amounts of text from the Internet, but do they truly understand the viral content that rapidly spreads online—commonly known as memes? In this paper, we introduce CHIME, a dataset for CHinese Internet Meme Explanation. The dataset comprises popular phrase-based memes from the Chinese Internet, annotated with detailed information on their meaning, origin, example sentences, types, etc. To evaluate whether LLMs understand these memes, we designed two tasks. In the first task, we assessed the models' ability to explain a given meme, identify its origin, and generate appropriate example sentences. The results show that while LLMs can explain the meanings of some memes, their performance declines significantly for culturally and linguistically nuanced meme types. Additionally, they consistently struggle to provide accurate origins for the memes. In the second task, we created a set of multiple-choice questions (MCQs) requiring LLMs to select the most appropriate meme to fill in a blank within a contextual sentence. While the evaluated models were able to provide correct answers, their performance remains noticeably below human levels. We have made CHIME public and hope it will facilitate future research on computational meme understanding",
    "checked": true,
    "id": "b894d37632a483c9013467ae48b9396e5115963e",
    "semantic_title": "are large language models chronically online surfers? a dataset for chinese internet meme explanation",
    "citation_count": 0,
    "authors": [
      "Yubo Xie",
      "Chenkai Wang",
      "Zongyang Ma",
      "Fahui Miao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.864": {
    "title": "RoDEval: A Robust Word Sense Disambiguation Evaluation Framework for Large Language Models",
    "volume": "main",
    "abstract": "Accurately evaluating the word sense disambiguation (WSD) capabilities of large language models (LLMs) remains challenging, as existing studies primarily rely on single-task evaluations and classification-based metrics that overlook the fundamental differences between generative LLMs and traditional classification models. To bridge this gap, we proposeRoDEval, the first comprehensive evaluation framework specifically tailored for assessing LLM-based WSD methods. RoDEval introduces four novel metrics: Disambiguation Scope, Disambiguation Robustness, Disambiguation Reliability, and Definition Generation Quality Score, enabling a multifaceted evaluation of LLMs' WSD capabilities. Experimental results using RoDEval across five mainstream LLMs uncover significant limitations in their WSD performance. Specifically, incorrect definition selections in multiple-choice WSD tasks stem not from simple neglect or forget of correct options, but rather from incomplete acquisition of the all senses for polysemous words. Instead, disambiguation reliability is often compromised by the models' persistent overconfidence. In addition, inherent biases continue to affect performance, and scaling up model parameters alone fails to meaningfully enhance their ability to generate accurate sense definitions. These findings provide actionable insights for enhancing LLMs' WSD capabilities. The source code and evaluation scripts are open-sourced at https://github.com/DayDream405/RoDEval",
    "checked": true,
    "id": "d3cb92623ab6c2e9dd434dda961d040619dd7367",
    "semantic_title": "rodeval: a robust word sense disambiguation evaluation framework for large language models",
    "citation_count": 0,
    "authors": [
      "Luyang Zhang",
      "Shuaimin Li",
      "Yishuo Li",
      "Kunpeng Kang",
      "Kaiyuan Zhang",
      "Cong Wang",
      "Wenpeng Lu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.865": {
    "title": "PychoAgent: Psychology-driven LLM Agents for Explainable Panic Prediction on Social Media during Sudden Disaster Events",
    "volume": "main",
    "abstract": "Accurately predicting public panic sentiment on social media is crucial for proactive governance and crisis management. Current efforts on this problem face three main challenges: lack of finely annotated data hinders emotion prediction studies, unmodeled risk perception causes prediction inaccuracies, and insufficient interpretability of panic formation mechanisms limits mechanistic insight. We address these issues by proposing a Psychology-driven generative Agent framework (PsychoAgent) for explainable panic prediction based on emotion arousal theory. Specifically, we first construct a fine-grained panic emotion dataset (namely COPE) via human-AI (Large Language Models, LLMs) collaboration, combining scalable LLM-based labeling with human annotators to ensure accuracy for panic emotion and to mitigate biases from linguistic variations. Then, we construct PsychoAgent integrating cross-domain heterogeneous data grounded in psychological mechanisms to model risk perception and cognitive differences in emotion generation. To enhance interpretability, we design an LLM-based role-playing agent that simulates individual psychological chains through dedicatedly designed prompts. Experimental results on our annotated dataset show that PsychoAgent improves panic emotion prediction performance by 13% to 21% compared to baseline models. Furthermore, the explainability and generalization of our approach is validated. Crucially, this represents a paradigm shift from opaque \"data-driven fitting\" to transparent \"role-based simulation with mechanistic interpretation\" for panic emotion prediction during emergencies. Our implementation is publicly available at: https://github.com/supersonic0919/PsychoAgent",
    "checked": true,
    "id": "c2dc2d405f1b8eee9ae535cda598787a81064688",
    "semantic_title": "pychoagent: psychology-driven llm agents for explainable panic prediction on social media during sudden disaster events",
    "citation_count": 0,
    "authors": [
      "Mengzhu Liu",
      "Zhengqiu Zhu",
      "Chuan Ai",
      "Chen Gao",
      "Xinghong Li",
      "Lingnan He",
      "Kaisheng Lai",
      "Yingfeng Chen",
      "Xin Lu",
      "Yong Li",
      "Quanjun Yin"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.866": {
    "title": "Stepwise Reasoning Checkpoint Analysis: A Test Time Scaling Method to Enhance LLMs' Reasoning",
    "volume": "main",
    "abstract": "Mathematical reasoning through Chain-of-Thought (CoT) has emerged as a powerful capability of Large Language Models (LLMs), which can be further enhanced through Test-Time Scaling (TTS) methods like Beam Search and DVTS. However, these methods, despite improving accuracy by allocating more computational resources during inference, often suffer from path homogenization and inefficient use of intermediate results. To address these limitations, we propose Stepwise Reasoning Checkpoint Analysis (SRCA), a framework that introduces checkpoints between reasoning steps. It incorporates two key strategies: (1) Answer-Clustered Search, which groups reasoning paths by their intermediate checkpoint answers to maintain diversity while ensuring quality, and (2) Checkpoint Candidate Augmentation, which leverages all intermediate answers for final decision-making. Our approach effectively reduces path homogenization and creates a fault-tolerant mechanism by utilizing high-quality intermediate results. Experimental results show that SRCA improves reasoning accuracy compared to existing TTS methods across various mathematical datasets",
    "checked": true,
    "id": "65e0a25a4fe7a88bdefad4a20aea072a1864e56f",
    "semantic_title": "stepwise reasoning checkpoint analysis: a test time scaling method to enhance llms' reasoning",
    "citation_count": 0,
    "authors": [
      "Zezhong Wang",
      "Xingshan Zeng",
      "Weiwen Liu",
      "Yufei Wang",
      "Liangyou Li",
      "Yasheng Wang",
      "Lifeng Shang",
      "Xin Jiang",
      "Qun Liu",
      "Kam-Fai Wong"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.867": {
    "title": "Inter-sentence Context Modeling and Structure-aware Representation Enhancement for Conversational Sentiment Quadruple Extraction",
    "volume": "main",
    "abstract": "Conversational aspect-based sentiment quadruple analysis (DiaASQ) is a newly-emergent task aiming to extract quadruples of target-aspect-opinion-sentiment from a conversation text. Existing studies struggle to capture complete dialogue semantics, largely due to inadequate inter-utterance modeling and the underutilization of dialogue structure. To address these issues, we propose an Inter-sentence Context Modeling and Structure-aware Representation Enhancement model (ICMSR) to extract dialogue aspect sentiment quadruple. We design the Dialog Inter-sentence Contextual Enhancer (DICE) module after the sentence-by-sentence encoding phase to enhance inter-sentence interactions and mitigate contextual fragmentation caused by traditional sequential encoding. Moreover, to fully exploit structural information within dialogues, we propose the Dialog Feature Amplifier (DFA), which consists of two submodules: STREAM and SMM. The STREAM module integrates diverse structural dialogue information to generate structure-aware sentence representations, effectively improving the modeling of intra-dialogue structural relations. Furthermore, the Structural Multi-scale Mechanism (SMM) employs a multi-scale modeling approach, simulating varying extents of contextual awareness, thereby enhancing the model's ability to capture cross-sentence structural dependencies. We extensively evaluate our method on benchmark datasets, and the empirical results consistently confirm its effectiveness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Zhang",
      "Zhaoman Zhong",
      "Huihui Lv"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.868": {
    "title": "Igniting Creative Writing in Small Language Models: LLM-as-a-Judge versus Multi-Agent Refined Rewards",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable creative writing capabilities, yet their substantial computational demands hinder widespread use. Enhancing Small Language Models (SLMs) offers a promising alternative, but current methods like Supervised Fine-Tuning (SFT) struggle with novelty, and Reinforcement Learning from Human Feedback (RLHF) is costly. This paper explores two distinct AI-driven reward strategies within a Reinforcement Learning from AI Feedback (RLAIF) framework to ignite the creative writing of a 7B-parameter SLM, specifically for generating Chinese greetings. The first strategy employs a Reward Model (RM) trained on high-quality preference data curated by a novel multi-agent rejection sampling framework designed for creative tasks. The second, more novel, strategy utilizes a principle-guided LLM-as-a-Judge, whose reward function is optimized via an adversarial training scheme with a reflection mechanism, to directly provide reward signals. Comprehensive experiments reveal that while both approaches significantly enhance creative output over baselines, the principle-guided LLM-as-a-Judge demonstrably yields superior generation quality. Furthermore, it offers notable advantages in training efficiency and reduced dependency on human-annotated data, presenting a more scalable and effective path towards creative SLMs. Our automated evaluation methods also exhibit strong alignment with human judgments",
    "checked": true,
    "id": "d8f8557e5fb3bb90acf576b36ac5165af0746c6c",
    "semantic_title": "igniting creative writing in small language models: llm-as-a-judge versus multi-agent refined rewards",
    "citation_count": 0,
    "authors": [
      "Xiaolong Wei",
      "Bo Lu",
      "Xingyu Zhang",
      "Zhejun Zhao",
      "Dongdong Shen",
      "Long Xia",
      "Dawei Yin"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.869": {
    "title": "Governance in Motion: Co-evolution of Constitutions and AI models for Scalable Safety",
    "volume": "main",
    "abstract": "Aligning large language models (LLMs) with human preferences is a central challenge for building reliable AI systems. Most existing alignment approaches rely on static signals, such as predefined principles or offline human annotations to guide model behavior toward a fixed approximation of human preferences. However, LLMs can exhibit distributional drift during training, and static alignment mechanisms lack the capacity to adaptively correct misaligned behaviors as they emerge. To address this limitation, we develop a two-stage framework that enables dynamic and continuous alignment. In the first stage, a constitution is continually revised based on observed model behaviors, and models are trained to comply with these evolving principles. In the second stage, this learned constitution is used to guide reinforcement learning, encouraging the model to align with the updated normative signals. We refer to this framework as COCOA: Co-evolution of Constitutions and AI Models. We show that COCOA enables a 7B model to greatly improve safety—raising StrongReject score from 0.741 to 0.935 and Safe-RLHF accuracy from 77.76% to 90.64% without human annotations, reaching performance close to much larger state-of-the-art models",
    "checked": true,
    "id": "6e3c22058a90ad6a530cce5bebdfd0ea7bda9352",
    "semantic_title": "governance in motion: co-evolution of constitutions and ai models for scalable safety",
    "citation_count": 0,
    "authors": [
      "Chenhao Huang",
      "Ziyu Shen",
      "Yicong Ren",
      "Huiyuan Zheng",
      "Jiazheng Zhang",
      "Mingxu Chai",
      "Ming Zhang",
      "Shihan Dou",
      "Fan Mo",
      "Jie Shi",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.870": {
    "title": "Web Intellectual Property at Risk: Preventing Unauthorized Real-Time Retrieval by Large Language Models",
    "volume": "main",
    "abstract": "The protection of cyber Intellectual Property (IP) such as web content is an increasingly critical concern. The rise of large language models (LLMs) with online retrieval capabilities enables convenient access to information but often undermines the rights of original content creators. As users increasingly rely on LLM-generated responses, they gradually diminish direct engagement with original information sources, which will significantly reduce the incentives for IP creators to contribute, and lead to a saturating cyberspace with more AI-generated content. In response, we propose a novel defense framework that empowers web content creators to safeguard their web-based IP from unauthorized LLM real-time extraction and redistribution by leveraging the semantic understanding capability of LLMs themselves. Our method follows principled motivations and effectively addresses an intractable black-box optimization problem. Real-world experiments demonstrated that our methods improve defense success rates from 2.5% to 88.6% on different LLMs, outperforming traditional defenses such as configuration-based restrictions",
    "checked": true,
    "id": "65f1e770f271409e32ef55c53b0740c7d552c7f1",
    "semantic_title": "web intellectual property at risk: preventing unauthorized real-time retrieval by large language models",
    "citation_count": 0,
    "authors": [
      "Yisheng Zhong",
      "Yizhu Wen",
      "Junfeng Guo",
      "Mehran Kafai",
      "Heng Huang",
      "Hanqing Guo",
      "Zhuangdi Zhu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.871": {
    "title": "SciEvent: Benchmarking Multi-domain Scientific Event Extraction",
    "volume": "main",
    "abstract": "Scientific information extraction (SciIE) has primarily relied on entity-relation extraction in narrow domains, limiting its applicability to interdisciplinary research and struggling to capture the necessary context of scientific information, often resulting in fragmented or conflicting statements. In this paper, we introduce SciEvent, a novel multi-domain benchmark of scientific abstracts annotated via a unified event extraction (EE) schema designed to enable structured and context-aware understanding of scientific content. It includes 500 abstracts across five research domains, with manual annotations of event segments, triggers, and fine-grained arguments. We define SciIE as a multi-stage EE pipeline: (1) segmenting abstracts into core scientific activities—Background, Method, Result, and Conclusion; and (2) extracting the corresponding triggers and arguments. Experiments with fine-tuned EE models, large language models (LLMs), and human annotators reveal a performance gap, with current models struggling in domains such as sociology and humanities. SciEvent serves as a challenging benchmark and a step toward generalizable, multi-domain SciIE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bofu Dong",
      "Pritesh Shah",
      "Sumedh Sonawane",
      "Tiyasha Banerjee",
      "Erin Brady",
      "Xinya Du",
      "Ming Jiang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.872": {
    "title": "Media Source Matters More Than Content: Unveiling Political Bias in LLM-Generated Citations",
    "volume": "main",
    "abstract": "Unlike traditional search engines that present ranked lists of webpages, generative search engines rely solely on in-line citations as the key gateway to original real-world webpages, making it crucial to examine whether LLM-generated citations have biases—particularly for politically sensitive queries. To investigate this, we first construct AllSides-2024, a new dataset comprising the latest real-world news articles (Jan. 2024 - Dec. 2024) labeled with left- or right-leaning stances. Through systematic evaluations, we find that LLMs exhibit a consistent tendency to cite left-leaning sources at notably higher rates compared to traditional retrieval systems (e.g., BM25 and dense retrievers). Controlled experiments further reveal that this bias arises from a preference for media outlets identified as left-leaning, rather than for left-oriented content itself. Meanwhile, our findings show that while LLMs struggle to infer political bias from news content alone, they can almost perfectly recognize the political orientation of media outlets based on their names. These insights highlight the risk that, in the era of generative search engines, information exposure may be disproportionately shaped by specific media outlets, potentially shaping public perception and decision-making",
    "checked": true,
    "id": "8be09e69ebbcfbcfa93c912146ed8ea9cb665862",
    "semantic_title": "media source matters more than content: unveiling political bias in llm-generated citations",
    "citation_count": 0,
    "authors": [
      "Sunhao Dai",
      "Zhanshuo Cao",
      "Wenjie Wang",
      "Liang Pang",
      "Jun Xu",
      "See-Kiong Ng",
      "Tat-Seng Chua"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.873": {
    "title": "RJE: A Retrieval-Judgment-Exploration Framework for Efficient Knowledge Graph Question Answering with LLMs",
    "volume": "main",
    "abstract": "Knowledge graph question answering (KGQA) aims to answer natural language questions using knowledge graphs.Recent research leverages large language models (LLMs) to enhance KGQA reasoning, but faces limitations: retrieval-based methods are constrained by the quality of retrieved information, while agent-based methods rely heavily on proprietary LLMs.To address these limitations, we propose Retrieval-Judgment-Exploration (RJE), a framework that retrieves refined reasoning paths, evaluates their sufficiency, and conditionally explores additional evidence. Moreover, RJE introduces specialized auxiliary modules enabling small-sized LLMs to perform effectively: Reasoning Path Ranking, Question Decomposition, and Retriever-assisted Exploration. Experiments show that our approach with proprietary LLMs (such as GPT-4o-mini) outperforms existing baselines while enabling small open-source LLMs (such as 3B and 8B parameters) to achieve competitive results without fine-tuning LLMs.Additionally, RJE substantially reduces the number of LLM calls and token usage compared to agent-based methods, yielding significant efficiency improvements",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Can Lin",
      "Zhengwang Jiang",
      "Ling Zheng",
      "Qi Zhao",
      "Yuhang Zhang",
      "Qi Song",
      "Wangqiu Zhou"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.874": {
    "title": "Bias Mitigation or Cultural Commonsense? Evaluating LLMs with a Japanese Dataset",
    "volume": "main",
    "abstract": "Large language models (LLMs) exhibit social biases, prompting the development of various debiasing methods. However, debiasing methods may degrade the capabilities of LLMs. Previous research has evaluated the impact of bias mitigation primarily through tasks measuring general language understanding, which are often unrelated to social biases. In contrast, cultural commonsense is closely related to social biases, as both are rooted in social norms and values. The impact of bias mitigation on cultural commonsense in LLMs has not been well investigated. Considering this gap, we propose SOBACO (SOcial BiAs and Cultural cOmmonsense benchmark), a Japanese benchmark designed to evaluate social biases and cultural commonsense in LLMs in a unified format. We evaluate several LLMs on SOBACO to examine how debiasing methods affect cultural commonsense in LLMs. Our results reveal that the debiasing methods degrade the performance of the LLMs on the cultural commonsense task (up to 75% accuracy deterioration). These results highlight the importance of developing debiasing methods that consider the trade-off with cultural commonsense to improve fairness and utility of LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taisei Yamamoto",
      "Ryoma Kumon",
      "Danushka Bollegala",
      "Hitomi Yanaka"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.875": {
    "title": "Chameleon LLMs: User Personas Influence Chatbot Personality Shifts",
    "volume": "main",
    "abstract": "As large language models (LLMs) integrate into society, their ability to adapt to users is as critical as their accuracy. While prior work has used personality tests to examine the perceived personalities of LLMs, little research has explored whether LLMs adapt their perceived personalities in response to user interactions. We investigate whether and how LLMs exhibit conversational adaptations over prolonged interactions. Using a controlled simulations where a user and chatbot engage in dialogue, we measure the chatbot's personality shift before and after the conversation. Across multiple models, we find that traits such as Agreeableness, Extraversion, and Conscientiousness are highly susceptible to user influence, whereas Emotional Stability and Intellect remain relatively more stable. Our results suggest that LLMs dynamically adjust their conversational style in response to user personas, raising important implications for AI alignment, trust, and safety",
    "checked": true,
    "id": "a09633dcd55e39b2847079595b38d6753f611222",
    "semantic_title": "chameleon llms: user personas influence chatbot personality shifts",
    "citation_count": 0,
    "authors": [
      "Jane Xing",
      "Tianyi Niu",
      "Shashank Srivastava"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.876": {
    "title": "GuessingGame: Measuring the Informativeness of Open-Ended Questions in Large Language Models",
    "volume": "main",
    "abstract": "We introduce GuessingGame, a protocol for evaluating large language models (LLMs) as strategic question-askers in open-ended, open-domain settings. A Guesser LLM identifies a hidden object by posing free-form questions to an Oracle—without predefined choices or candidate lists. To measure question quality, we propose two information gain (IG) metrics: a Bayesian method that tracks belief updates over semantic concepts using LLM-scored relevance, and an entropy-based method that filters candidates via ConceptNet. Both metrics are model-agnostic and support post hoc analysis. Across 858 games with multiple models and prompting strategies, higher IG strongly predicts efficiency: a one-standard-deviation IG increase reduces expected game length by 43%. Prompting constraints guided by IG—such as enforcing question diversity—enable weaker models to match GPT-4o. These results show that question-asking in LLMs is both measurable and improvable, and crucial for interactive reasoning",
    "checked": true,
    "id": "1d043a56defdc6cc0b945578a90b3ab318012ed0",
    "semantic_title": "guessinggame: measuring the informativeness of open-ended questions in large language models",
    "citation_count": 0,
    "authors": [
      "Dylan Hutson",
      "Daniel Vennemeyer",
      "Aneesh Deshmukh",
      "Justin Zhan",
      "Tianyu Jiang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.877": {
    "title": "SynC-LLM: Generation of Large-Scale Synthetic Circuit Code with Hierarchical Language Models",
    "volume": "main",
    "abstract": "In recent years, AI-assisted integrated circuit (IC) design methods have shown great potential in boosting IC design efficiency. However, this emerging technique is fundamentally limited by the serious scarcity of publicly accessible large-scale circuit design data, which are mostly private IPs owned by semiconductor companies. In this work, we propose SynC-LLM, the first technique that exploits LLM's ability to generate new large-scale synthetic digital circuits. In our hierarchical circuit generation process, we first design a directed graph diffusion model to learn and generate the skeleton of large circuits with sequential cells. Then we propose a cone function retrieval technique to annotate each sequential node in the skeleton with a function description. Finally, we apply a level-by-level customized prompting technique utilizing LLM to complete the code at every skeleton cone. Experiments show that our generated circuits are not only valid and fully functional, but also closely resemble realistic large-scale designs and can significantly improve AI models' performance in multiple IC design tasks. The code and data are open-sourced in https://github.com/hkust-zhiyao/SynCircuitData",
    "checked": true,
    "id": "f5e0509d8f58650d99dc1311229355adcb5a2ce9",
    "semantic_title": "sync-llm: generation of large-scale synthetic circuit code with hierarchical language models",
    "citation_count": 1,
    "authors": [
      "Shang Liu",
      "Yao Lu",
      "Wenji Fang",
      "Jing Wang",
      "Zhiyao Xie"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.878": {
    "title": "Why Stop at One Error? Benchmarking LLMs as Data Science Code Debuggers for Multi-Hop and Multi-Bug Errors",
    "volume": "main",
    "abstract": "LLMs are transforming software development, yet current code generation and code repair benchmarks mainly assess syntactic and functional correctness in simple, single-error cases. LLMs' capabilities to autonomously find and fix runtime logical errors in complex data science code remain largely unexplored. To address this gap, we introduce DSDBench: the Data Science Debugging Benchmark, the first benchmark for systematic evaluation of LLMs on multi-hop error tracing and multi-bug detection in data science code debugging. DSDBench adapts datasets from existing data science task benchmarks, such as DABench and MatPlotBench, featuring realistic data science debugging tasks with automatically synthesized multi-hop, multi-bug code snippets. DSDBench includes 1,117 annotated samples with 741 cause-effect error pairs and runtime error messages. Evaluations of state-of-the-art LLMs on DSDBench show significant performance gaps, highlighting challenges in debugging logical runtime errors in data science code. DSDBench offers a crucial resource to evaluate and improve LLMs' debugging and reasoning capabilities, enabling more reliable AI-assisted data science in the future",
    "checked": true,
    "id": "248cdfd7c0e46c1cf5532026579069dc6d8ea451",
    "semantic_title": "why stop at one error? benchmarking llms as data science code debuggers for multi-hop and multi-bug errors",
    "citation_count": 0,
    "authors": [
      "Zhiyu Yang",
      "Shuo Wang",
      "Yukun Yan",
      "Yang Deng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.879": {
    "title": "Dovetail: A CPU/GPU Heterogeneous Speculative Decoding for LLM inference",
    "volume": "main",
    "abstract": "With the continuous advancement in the performance of large language models (LLMs), their demand for computational resources and memory has significantly increased, which poses major challenges for efficient inference on consumer-grade devices and legacy servers. These devices typically feature relatively weaker GPUs and stronger CPUs. Although techniques such as parameter offloading and partial offloading can alleviate GPU memory pressure to some extent, their effectiveness is limited due to communication latency and suboptimal hardware resource utilization. To address this issue, we propose Dovetail—a lossless inference acceleration method that leverages the complementary characteristics of heterogeneous devices and the advantages of speculative decoding. Dovetail deploys a draft model on the GPU to perform preliminary predictions, while a target model running on the CPU validates these outputs. By reducing the granularity of data transfer, Dovetail significantly minimizes communication overhead. To further improve efficiency, we optimize the draft model specifically for heterogeneous hardware environments by reducing the number of draft tokens to lower parallel verification latency, increasing model depth to enhance predictive capabilities, and introducing a Dynamic Gating Fusion (DGF) mechanism to improve the integration of feature and embedding information. We conduct comprehensive evaluations of Dovetail across various consumer-grade GPUs, covering multiple tasks and mainstream models. Experimental results on 13B models demonstrate that Dovetail achieves inference speedups ranging from 1.79× to 10.1× across different devices, while maintaining consistency and stability in the distribution of generated texts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Libo Zhang",
      "Zhaoning Zhang",
      "Xubaizhou",
      "Rui Li",
      "Zhiliang Tian",
      "Songzhu Mei",
      "Dongsheng Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.880": {
    "title": "V-SEAM: Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models",
    "volume": "main",
    "abstract": "Recent advances in causal interpretability have extended from language models to vision-language models (VLMs), seeking to reveal their internal mechanisms through input interventions. While textual interventions often target semantics, visual interventions typically rely on coarse pixel-level perturbations, limiting semantic insights on multimodal integration. In this study, we introduce V-SEAM, a novel framework that combines **V**isual **S**emantic **E**diting and **A**ttention **M**odulating for causal interpretation of VLMs. V-SEAM enables concept-level visual manipulations and identifies attention heads with positive or negative contributions to predictions across three semantic levels: objects, attributes, and relationships. We observe that positive heads are often shared within the same semantic level but vary across levels, while negative heads tend to generalize broadly. Finally, we introduce an automatic method to modulate key head embeddings, demonstrating enhanced performance for both LLAVA and InstructBLIP across three diverse VQA benchmarks. Our data and code are released at: https://github.com/petergit1/V-SEAM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qidong Wang",
      "Junjie Hu",
      "Ming Jiang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.881": {
    "title": "LORAXBENCH: A Multitask, Multilingual Benchmark Suite for 20 Indonesian Languages",
    "volume": "main",
    "abstract": "As one of the world's most populous countries, with 700 languages spoken, Indonesia is behind in terms of NLP progress. We introduce LORAXBENCH, a benchmark that focuses on low-resource languages of Indonesia and covers 6 diverse tasks: reading comprehension, open-domain QA, language inference, causal reasoning, translation, and cultural QA. Our dataset cover 20 languages, with the addition of two formality registers for three languages. We evaluate a diverse set of multilingual and region-focused LLMs and found that this benchmark is challenging. We note a visible discrepancy between performance in Indonesian and other languages, especially the low-resource ones. There is no clear lead when using a region-specific model as opposed to the general multilingual model. Lastly, we show that a change in register affects model performance, especially with registers not commonly found in social media, such as high-level politeness ‘Krama' Javanese",
    "checked": true,
    "id": "9315408acb4dfe305097dfc416038cdef98b2b63",
    "semantic_title": "loraxbench: a multitask, multilingual benchmark suite for 20 indonesian languages",
    "citation_count": 0,
    "authors": [
      "Alham Fikri Aji",
      "Trevor Cohn"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.882": {
    "title": "MiCRo: Mixture Modeling and Context-aware Routing for Personalized Preference Learning",
    "volume": "main",
    "abstract": "Reward modeling is a key step in building safe foundation models when applying reinforcement learning from human feedback (RLHF) to align Large Language Models (LLMs). However, reward modeling based on the Bradley-Terry (BT) model assumes a global reward function, failing to capture the inherently diverse and heterogeneous human preferences. Hence, such oversimplification limits LLMs from supporting personalization and pluralistic alignment. Theoretically, we show that when human preferences follow a mixture distribution of diverse subgroups, a single BT model has an irreducible error. While existing solutions, such as fine-grained annotations via prompting or structured preference elicitation, help address this issue, they are costly and constrained by predefined attributes, failing to fully capture the richness of human values. In this work, we introduce MiCRo, a two-stage framework that enhances personalized preference learning by leveraging large-scale binary preference datasets without requiring explicit fine-grained annotations. In the first stage, MiCRo employs a mixture of preferences to model diverse human preferences, enabling a flexible representation of diverse value systems. In the second stage, MiCRo integrates an online routing strategy that dynamically adapts mixture weights based on specific context to resolve ambiguity, allowing for efficient and scalable preference adaptation with minimal additional supervision. Experiments on multiple preference datasets demonstrate that MiCRo effectively captures diverse human preferences and significantly improves personalized preference learning on downstream tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyan Shen",
      "Jiarui Yao",
      "Rui Yang",
      "Yifan Sun",
      "Feng Luo",
      "Rui Pan",
      "Tong Zhang",
      "Han Zhao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.883": {
    "title": "SAFE: Schema-Driven Approximate Distance Join for Efficient Knowledge Graph Querying",
    "volume": "main",
    "abstract": "To reduce hallucinations in large language models (LLMs), researchers are increasingly investigating reasoning methods that integrate LLMs with external knowledge graphs (KGs). Existing approaches either map an LLM-generated query graph onto the KG or let the LLM traverse the entire graph; the former is fragile because noisy query graphs derail retrieval, whereas the latter is inefficient due to entity-level reasoning over large graphs. In order to tackle these problems, we propose **SAFE** (**S**chema-Driven **A**pproximate Distance Join **F**or **E**fficient Knowledge Graph Querying), a framework that leverages schema graphs for robust query graph generation and efficient KG retrieval. SAFE introduces two key ideas: (1) an Approximate Distance Join (ADJ) algorithm that refines LLM-generated pseudo query graphs by flexibly aligning them with the KG's structure; and (2) exploiting a compact schema graph to perform ADJ efficiently, reducing overhead and improving retrieval accuracy. Extensive experiments on WebQSP, CWQ and GrailQA demonstrate that SAFE outperforms state-of-the-art methods in both accuracy and efficiency, providing a robust and scalable solution to overcome the inherent limitations of LLM-based knowledge retrieval",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sangoh Lee",
      "Sungho Park",
      "Wook-Shin Han"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.884": {
    "title": "Structured Preference Optimization for Vision-Language Long-Horizon Task Planning",
    "volume": "main",
    "abstract": "Existing vision-language planning methods perform well on short-horizon tasks but struggle with long-horizon reasoning in dynamic environments due to the difficulty of training models to generate high-quality reasoning processes. To address this, we propose Structured Preference Optimization (SPO), a framework that enhances reasoning and action selection for long-horizon task planning through structured evaluation and optimized training. SPO introduces: 1) Structured Preference Evaluation and Optimization, which evaluates reasoning chains across task relevance, historical consistency (as part of textual coherence), and image awareness (alignment with visual observations) to construct high-quality preference pairs; and 2) Curriculum-Guided Progressive Learning, enabling the model to adapt from simple to complex tasks, thereby improving generalization and robustness. To advance research in vision-language long-horizon task planning, we introduce ExtendaBench, a comprehensive benchmark covering 1,509 tasks across VirtualHome and Habitat 2.0, categorized into ultra-short, short, medium, and long tasks. Experimental results demonstrate that SPO significantly improves reasoning quality and final decision accuracy, outperforming prior methods on long-horizon tasks and underscoring the effectiveness of preference-driven optimization in vision-language task planning. Specifically, SPO achieves a +5.98% GCR and +4.68% SR improvement in VirtualHome and a +3.30% GCR and +2.11% SR improvement in Habitat over the best-performing baselines",
    "checked": true,
    "id": "7fb7dc542eebce34205d2ab8e6e96e059ac932c9",
    "semantic_title": "structured preference optimization for vision-language long-horizon task planning",
    "citation_count": 4,
    "authors": [
      "Xiwen Liang",
      "Min Lin",
      "Weiqi Ruan",
      "Rongtao Xu",
      "Yuecheng Liu",
      "Jiaqi Chen",
      "Bingqian Lin",
      "Yuzheng Zhuang",
      "Xiaodan Liang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.885": {
    "title": "Position: LLMs Can be Good Tutors in English Education",
    "volume": "main",
    "abstract": "While recent efforts have begun integrating large language models (LLMs) into English education, they often rely on traditional approaches to learning tasks without fully embracing educational methodologies, thus lacking adaptability to language learning. To address this gap, we argue that **LLMs have the potential to serve as effective tutors in English Education**. Specifically, LLMs can play three critical roles: (1) as data enhancers, improving the creation of learning materials or serving as student simulations; (2) as task predictors, serving as learner assessment or optimizing learning pathway; and (3) as agents, enabling personalized and inclusive education. We encourage interdisciplinary research to explore these roles, fostering innovation while addressing challenges and risks, ultimately advancing English Education through the thoughtful integration of LLMs",
    "checked": true,
    "id": "243c0a6ff221adcb947c2e4c75fdc70d2f221b38",
    "semantic_title": "position: llms can be good tutors in english education",
    "citation_count": 7,
    "authors": [
      "Jingheng Ye",
      "Shen Wang",
      "Deqing Zou",
      "Yibo Yan",
      "Kun Wang",
      "Hai-Tao Zheng",
      "Ruitong Liu",
      "Zenglin Xu",
      "Irwin King",
      "Philip S. Yu",
      "Qingsong Wen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.886": {
    "title": "CLLMate: A Multimodal Benchmark for Weather and Climate Events Forecasting",
    "volume": "main",
    "abstract": "Forecasting weather and climate events is crucial for making appropriate measures to mitigate environmental hazards and minimize losses. However, existing environmental forecasting research focuses narrowly on predicting numerical meteorological variables (e.g., temperature), neglecting the translation of these variables into actionable textual narratives of events and their consequences. To bridge this gap, we proposed Weather and Climate Event Forecasting (WCEF), a new task that leverages numerical meteorological raster data and textual event data to predict weather and climate events. This task is challenging to accomplish due to difficulties in aligning multimodal data and the lack of supervised datasets. To address these challenges, we present CLLMate, the first multimodal dataset for WCEF, using 26,156 environmental news articles aligned with ERA5 reanalysis data. We systematically benchmark 32 existing models on CLLMate, including closed-source, open-source, and our fine-tuned models. Our experiments reveal the advantages and limitations of existing MLLMs and the value of CLLMate for the training and benchmarking of the WCEF task. The dataset is available at https://github.com/hobolee/CLLMate",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haobo Li",
      "Zhaowei Wang",
      "Jiachen Wang",
      "Yueya Wang",
      "Alexis Kai Hon Lau",
      "Huamin Qu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.887": {
    "title": "Extracting and Combining Abilities For Building Multi-lingual Ability-enhanced Large Language Models",
    "volume": "main",
    "abstract": "Multi-lingual ability transfer has become increasingly important for the broad application of large language models (LLMs). Existing work highly relies on training with the multi-lingual ability-related data, which may not be available for low-resource languages. To solve it, we propose a **M**ulti-lingual **A**bilities **E**xtraction and **C**ombination approach, named as **MAEC**. Our key idea is to decompose and extract language-agnostic ability-related weights from LLMs, and combine them across different languages by simple addition and subtraction operations without training. Specifically, our MAEC consists of the extraction and combination stages. In the extraction stage, we firstly locate key neurons that are highly related to specific abilities, and then employ them to extract the transferable ability-related weights. In the combination stage, we further select the ability-related tensors that mitigate the linguistic effects, and design a combining strategy based on them and the language-specific weights, to build the multi-lingual ability-enhanced LLM. To assess the effectiveness of our approach, we conduct extensive experiments on LLaMA-3 8B on mathematical and scientific tasks in both high-resource and low-resource lingual scenarios. Experiment results have shown that MAEC can effectively and efficiently extract and combine the advanced abilities, achieving **comparable performance with PaLM**. We will publicly release our code and data",
    "checked": true,
    "id": "362c9be18cca8d89b903f69e3a3aabeabe091a79",
    "semantic_title": "extracting and combining abilities for building multi-lingual ability-enhanced large language models",
    "citation_count": 0,
    "authors": [
      "Zhipeng Chen",
      "Kun Zhou",
      "Liang Song",
      "Xin Zhao",
      "Bingning Wang",
      "Weipeng Chen",
      "Ji-Rong Wen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.888": {
    "title": "Evaluating the Effectiveness and Scalability of LLM-Based Data Augmentation for Retrieval",
    "volume": "main",
    "abstract": "Compact dual-encoder models are widely used for retrieval owing to their efficiency and scalability. However, such models often underperform compared to their Large Language Model (LLM)-based retrieval counterparts, likely due to their limited world knowledge. While LLM-based data augmentation has been proposed as a strategy to bridge this performance gap, there is insufficient understanding of its effectiveness and scalability to real-world retrieval problems. Existing research does not systematically explore key factors such as the optimal augmentation scale, the necessity of using large augmentation models, and whether diverse augmentations improve generalization, particularly in out-of-distribution (OOD) settings. This work presents a comprehensive study of the effectiveness of LLM augmentation for retrieval, comprising over 100 distinct experimental settings of retrieval models, augmentation models and augmentation strategies. We find that, while augmentation enhances retrieval performance, its benefits diminish beyond a certain scale, even with diverse augmentation strategies. Surprisingly, we observe that augmentation with smaller LLMs can achieve performance competitive with larger augmentation models. Moreover, we examine how augmentation effectiveness varies with retrieval model pre-training, revealing that augmentation provides the most benefit to models which are not well pre-trained. Our insights pave the way for more judicious and efficient augmentation strategies, thus enabling informed decisions and maximizing retrieval performance while being more cost-effective",
    "checked": true,
    "id": "b50b19aa5dd555f59c133dbaf34f00122e14bf8f",
    "semantic_title": "evaluating the effectiveness and scalability of llm-based data augmentation for retrieval",
    "citation_count": 1,
    "authors": [
      "Pranjal A Chitale",
      "Bishal Santra",
      "Yashoteja Prabhu",
      "Amit Sharma"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.889": {
    "title": "Temporal Referential Consistency: Do LLMs Favor Sequences Over Absolute Time References?",
    "volume": "main",
    "abstract": "The increasing acceptance of large language models (LLMs) as an alternative to knowledge sources marks a significant paradigm shift across various domains, including time-sensitive fields such as law, healthcare, and finance. To fulfill this expanded role, LLMs must not only be factually accurate but also demonstrate consistency across temporal dimensions, necessitating robust temporal reasoning capabilities. Despite this critical requirement, efforts to ensure temporal consistency in LLMs remain scarce including noticeable absence of endeavors aimed at evaluating or augmenting LLMs across temporal references in time-sensitive inquiries. In this paper, we seek to address this gap by introducing a novel benchmark entitled temporal referential consistency, accompanied by a resource TEMP-ReCon designed to benchmark a wide range of both open-source and closed-source LLMs with various linguistic contexts characterized by differing resource richness (including English, French, and Romanian). The findings emphasis that LLMs do exhibit insufficient temporal referent consistency. To address this, we propose , a reasoning path alignment-based model that aims to enhance the temporal referential consistency of LLMs. Our empirical experiments substantiate the efficacy of UnTRaP compared to several baseline models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashutosh Bajpai",
      "Tanmoy Chakraborty"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.890": {
    "title": "MemeArena: Automating Context-Aware Unbiased Evaluation of Harmfulness Understanding for Multimodal Large Language Models",
    "volume": "main",
    "abstract": "The proliferation of memes on social media necessitates the capabilities of multimodal Large Language Models (mLLMs) to effectively understand multimodal harmfulness. Existing evaluation approaches predominantly focus on mLLMs' detection accuracy for binary classification tasks, which often fail to reflect the in-depth interpretive nuance of harmfulness across diverse contexts. In this paper, we propose MemeArena, an agent-based arena-style evaluation framework that provides a context-aware and unbiased assessment for mLLMs' understanding of multimodal harmfulness. Specifically, MemeArena simulates diverse interpretive contexts to formulate evaluation tasks that elicit perspective-specific analyses from mLLMs. By integrating varied viewpoints and reaching consensus among evaluators, it enables fair and unbiased comparisons of mLLMs' abilities to interpret multimodal harmfulness. Extensive experiments demonstrate that our framework effectively reduces the evaluation biases of judge agents, with judgment results closely aligning with human preferences, offering valuable insights into reliable and comprehensive mLLM evaluations in multimodal harmfulness understanding. Our code and data are publicly available at https://github.com/Lbotirx/MemeArena",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixin Chen",
      "Hongzhan Lin",
      "Kaixin Li",
      "Ziyang Luo",
      "Yayue Deng",
      "Jing Ma"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.891": {
    "title": "Multi-perspective Analysis of Large Language Model Domain Specialization: An Experiment in Accounting Audit Procedures Generation",
    "volume": "main",
    "abstract": "Two major domain specialization approaches for Large Language Models (LLMs), fine-tuning and In-Context Learning (ICL), have been compared across various domains.While prior research has examined the similarities and differences between these approaches in task-specific capabilities, less is known about how they affect the feature of the generated text itself.To address this research gap, we conducted an experimental study using Accounting Audit Procedures Generation (AAPG) task, a highly specialized task requiring expert accounting knowledge.This task provides a practical testbed for a multi-perspective analysis of domain specialization due to its technical complexity and the large gap between general and domain expert knowledge.The results show consistent differences in output characteristics across models when comparing fine-tuning, ICL, and their combined approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yusuke Noro"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.892": {
    "title": "Generator-Assistant Stepwise Rollback Framework for Large Language Model Agent",
    "volume": "main",
    "abstract": "Large language model (LLM) agents typically adopt a step-by-step reasoning framework, in which they interleave the processes of thinking and acting to accomplish the given task. However, this paradigm faces a deep-rooted one-pass issue whereby each generated intermediate thought is plugged into the trajectory regardless of its correctness, which can cause irreversible error propagation. To address the issue, this paper proposes a novel framework called Generator-Assistant Stepwise Rollback (GA-Rollback) to induce better decision-making for LLM agents. Particularly, GA-Rollback utilizes a generator to interact with the environment and an assistant to examine each action produced by the generator, where the assistant triggers a rollback operation upon detection of incorrect actions. Moreover, we introduce two additional strategies tailored for the rollback scenario to further improve its effectiveness. Extensive experiments show that GA-Rollback achieves significant improvements over several strong baselines on three widely used benchmarks. Our analysis further reveals that GA-Rollback can function as a robust plug-and-play module, integrating seamlessly with other methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingzuo Li",
      "Kehai Chen",
      "Yunfei Long",
      "Xuefeng Bai",
      "Yong Xu",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.893": {
    "title": "DocAgent: An Agentic Framework for Multi-Modal Long-Context Document Understanding",
    "volume": "main",
    "abstract": "Recent advances in large language models (LLMs) have demonstrated significant promise in document understanding and question-answering. Despite the progress, existing approaches can only process short documents due to limited context length or fail to fully leverage multi-modal information. In this work, we introduce DocAgent, a multi-agent framework for long-context document understanding that imitates human reading practice. Specifically, we first extract a structured, tree-formatted outline from documents to help agents identify relevant sections efficiently. Further, we develop an interactive reading interface that enables agents to query and retrieve various types of content dynamically. To ensure answer reliability, we introduce a reviewer agent that cross-checks responses using complementary sources and maintains a task-agnostic memory bank to facilitate knowledge sharing across tasks. We evaluate our method on two long-context document understanding benchmarks, where it bridges the gap to human-level performance by surpassing competitive baselines, while maintaining a short context length. Our code is available at https://github.com/lisun-ai/DocAgent",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li Sun",
      "Liu He",
      "Shuyue Jia",
      "Yangfan He",
      "Chenyu You"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.894": {
    "title": "EasyRec: Simple yet Effective Language Models for Recommendation",
    "volume": "main",
    "abstract": "Deep neural networks have emerged as a powerful technique for learning representations from user-item interaction data in collaborative filtering (CF) for recommender systems. However, many existing methods heavily rely on unique user and item IDs, which restricts their performance in zero-shot learning scenarios. Inspired by the success of language models (LMs) and their robust generalization capabilities, we pose the question: How can we leverage language models to enhance recommender systems? We propose EasyRec, an effective approach that integrates text-based semantic understanding with collaborative signals. EasyRec employs a text-behavior alignment framework that combines contrastive learning with collaborative language model tuning. This ensures strong alignment between text-enhanced semantic representations and collaborative behavior information. Extensive evaluations across diverse datasets show EasyRec significantly outperforms state-of-the-art models, particularly in text-based zero-shot recommendation. EasyRec functions as a plug-and-play component that integrates seamlessly into collaborative filtering frameworks. This empowers existing systems with improved performance and adaptability to user preferences. Implementation codes are publicly available at: https://github.com/HKUDS/EasyRec",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xubin Ren",
      "Chao Huang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.895": {
    "title": "From Automation to Autonomy: A Survey on Large Language Models in Scientific Discovery",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) are catalyzing a paradigm shift in scientific discovery, evolving from task-specific automation tools into increasingly autonomous agents and fundamentally redefining research processes and human-AI collaboration. This survey systematically charts this burgeoning field, placing a central focus on the changing roles and escalating capabilities of LLMs in science. Through the lens of the scientific method, we introduce a foundational three-level taxonomy—Tool, Analyst, and Scientist—to delineate their escalating autonomy and evolving responsibilities within the research lifecycle. We further identify pivotal challenges and future research trajectories such as robotic automation, self-improvement, and ethical governance. Overall, this survey provides a conceptual architecture and strategic foresight to navigate and shape the future of AI-driven scientific discovery, fostering both rapid innovation and responsible advancement",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianshi Zheng",
      "Zheye Deng",
      "Hong Ting Tsang",
      "Weiqi Wang",
      "Jiaxin Bai",
      "Zihao Wang",
      "Yangqiu Song"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.896": {
    "title": "Mapping the Minds of LLMs: A Graph-Based Analysis of Reasoning LLMs",
    "volume": "main",
    "abstract": "Recent advances in test-time scaling have enabled Large Language Models (LLMs) to display sophisticated reasoning abilities via extended Chain-of-Thought (CoT) generation. Despite their impressive reasoning abilities, Large Reasoning Models (LRMs) frequently display unstable behaviors, e.g., hallucinating unsupported premises, overthinking simple tasks, and displaying higher sensitivity to prompt variations. This raises a deeper research question: How can we represent the reasoning process of LRMs to map their minds? To address this, we propose a unified graph-based analytical framework for fine-grained modeling and quantitative analysis of LRM reasoning dynamics. Our method first clusters long, verbose CoT outputs into semantically coherent reasoning steps, then constructs directed reasoning graphs to capture contextual and logical dependencies among these steps. Through a comprehensive analysis of derived reasoning graphs, we also reveal that key structural properties, such as exploration density, branching, and convergence ratios, strongly correlate with models' performance. The proposed framework enables quantitative evaluation of internal reasoning structure and quality beyond conventional metrics and also provides practical insights for prompt engineering and cognitive analysis of LLMs. Code and resources will be released to facilitate future research in this direction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Xiong",
      "Yujun Cai",
      "Zhecheng Li",
      "Yiwei Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.897": {
    "title": "ViPE: Visual Perception in Parameter Space for Efficient Video-Language Understanding",
    "volume": "main",
    "abstract": "Existing video-language models (Video-LLMs) typically rely on concatenating visual tokens with textual inputs for joint modeling. However, this token-level alignment leads to significant inefficiency, especially when scaling to long videos with dense visual inputs. In this work, we propose a video-to-parameter efficiency paradigm named ViPE that eliminates redundant visual tokens by transforming video content into visual perceptual weights, which are directly injected into the LLM's parameters. ViPE consists of a visual injection module that compresses video features into a small set of perceptual queries using a hierarchical merge strategy, and a visual perception module that integrates the resulting representations into the LLM through a lightweight LoRA-like mechanism. ViPE achieves performance comparable to token-based baselines such as LLaVA, while reducing FLOPs by 85% and inference time by up to 65%, demonstrating a highly efficient and scalable solution for video understanding",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shichen Lu",
      "Tongtian Yue",
      "Longteng Guo",
      "Handong Li",
      "Xingjian He",
      "Si Liu",
      "Jing Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.898": {
    "title": "Alignment for Efficient Tool Calling of Large Language Models",
    "volume": "main",
    "abstract": "Recent advancements in tool learning have enabled large language models (LLMs) to integrate external tools, enhancing their task performance by expanding their knowledge boundaries. However, relying on tools often introduces trade-offs between performance, speed, and cost, with LLMs sometimes exhibiting overreliance and overconfidence in tool usage. This paper addresses the challenge of aligning LLMs with their knowledge boundaries to make more intelligent decisions about tool invocation. We propose a multi-objective alignment framework that combines probabilistic knowledge boundary estimation with dynamic decision-making, allowing LLMs to better assess when to invoke tools based on their confidence. Our framework includes two methods for knowledge boundary estimation—consistency-based and absolute estimation—and two training strategies for integrating these estimates into the model's decision-making process. Experimental results on various tool invocation scenarios demonstrate the effectiveness of our framework, showing significant improvements in tool efficiency by reducing unnecessary tool usage",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongshen Xu",
      "Zihan Wang",
      "Zichen Zhu",
      "Lei Pan",
      "Xingyu Chen",
      "Shuai Fan",
      "Lu Chen",
      "Kai Yu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.899": {
    "title": "ToM: Leveraging Tree-oriented MapReduce for Long-Context Reasoning in Large Language Models",
    "volume": "main",
    "abstract": "Large Language Models (LLMs), constrained by limited context windows, often face significant performance degradation when reasoning over long contexts. To address this, Retrieval-Augmented Generation (RAG) retrieves and reasons over chunks but frequently sacrifices logical coherence due to its reliance on similarity-based rankings. Similarly, divide-and-conquer frameworks (DCF) split documents into small chunks for independent reasoning and aggregation. While effective for local reasoning, DCF struggles to capture long-range dependencies and risks inducing conflicts by processing chunks in isolation. To overcome these limitations, we propose ToM, a novel Tree-oriented MapReduce framework for long-context reasoning. ToM leverages the inherent hierarchical structure of long documents (e.g., main headings and subheadings) by constructing a DocTree through hierarchical semantic parsing and performing bottom-up aggregation. Using a Tree MapReduce approach, ToM enables recursive reasoning: in the Map step, rationales are generated at child nodes; in the Reduce step, these rationales are aggregated across sibling nodes to resolve conflicts or reach consensus at parent nodes. Experimental results on 70B+ LLMs show that ToM significantly outperforms existing divide-and-conquer frameworks and retrieval-augmented generation methods, achieving better logical coherence and long-context reasoning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiani Guo",
      "Zuchao Li",
      "Jie Wu",
      "Qianren Wang",
      "Yun Li",
      "Lefei Zhang",
      "Hai Zhao",
      "Yujiu Yang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.900": {
    "title": "BANMIME : Misogyny Detection with Metaphor Explanation on Bangla Memes",
    "volume": "main",
    "abstract": "Detecting misogyny in multimodal content remains a notable challenge, particularly in culturally conservative and low-resource contexts like Bangladesh. While existing research has explored hate speech and general meme classification, the nuanced identification of misogyny in Bangla memes, rich in metaphor, humor, and visual-textual interplay, remains severely underexplored. To address this gap, we introduce BanMiMe, the first comprehensive Bangla misogynistic meme dataset comprising 2,000 culturally grounded samples where each meme includes misogyny labels, humor categories, metaphor localization, and detailed human-written explanations. We benchmark the various performance of open and closed-source vision-language models (VLMs) under zero-shot and prompt-based settings and evaluate their capacity for both classification and explanation generation. Furthermore, we systematically explore multiple fine-tuning strategies, including standard, data-augmented, and Chain-of-Thought (CoT) supervision. Our results demonstrate that CoT-based fine-tuning consistently enhances model performance, both in terms of accuracy and in generating meaningful explanations. We envision BanMiMe as a foundational resource for advancing explainable multimodal moderation systems in low-resource and culturally sensitive settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Ayon Mia",
      "Akm Moshiur Rahman Mazumder",
      "Khadiza Sultana Sayma",
      "Md Fahim",
      "Md Tahmid Hasan Fuad",
      "Muhammad Ibrahim Khan",
      "Akmmahbubur Rahman"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.901": {
    "title": "Phi: Preference Hijacking in Multi-modal Large Language Models at Inference Time",
    "volume": "main",
    "abstract": "Recently, Multimodal Large Language Models (MLLMs) have gained significant attention across various domains. However, their widespread adoption has also raised serious safety concerns.In this paper, we uncover a new safety risk of MLLMs: the output preference of MLLMs can be arbitrarily manipulated by carefully optimized images. Such attacks often generate contextually relevant yet biased responses that are neither overtly harmful nor unethical, making them difficult to detect. Specifically, we introduce a novel method, **P**reference **Hi**jacking (**Phi**), for manipulating the MLLM response preferences using a preference hijacked image. Our method works at inference time and requires no model modifications. Additionally, we introduce a universal hijacking perturbation – a transferable component that can be embedded into different images to hijack MLLM responses toward any attacker-specified preferences. Experimental results across various tasks demonstrate the effectiveness of our approach. The code for Phi is accessible at https://github.com/Yifan-Lan/Phi",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Lan",
      "Yuanpu Cao",
      "Weitong Zhang",
      "Lu Lin",
      "Jinghui Chen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.902": {
    "title": "Retrieval-augmented GUI Agents with Generative Guidelines",
    "volume": "main",
    "abstract": "GUI agents powered by vision-language models (VLMs) show promise in automating complex digital tasks. However, their effectiveness in real-world applications is often limited by scarce training data and the inherent complexity of these tasks, which frequently require long-tailed knowledge covering rare, unseen scenarios. We propose RAG-GUI , a lightweight VLM that leverages web tutorials at inferencetime. RAG-GUI is first warm-started via supervised finetuning (SFT) and further refined through self-guided rejection sampling fine-tuning (RSF). Designed to be model-agnostic, RAG-GUI functions as a generic plug-in that enhances any VLM-based agent. Evaluatedacross three distinct tasks, it consistently outperforms baseline agents and surpasses other inference baselines by 2.6% to 13.3% acrosstwo model sizes, demonstrating strong generalization and practical plug-and-play capabilities in real-world scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ran Xu",
      "Kaixin Ma",
      "Wenhao Yu",
      "Hongming Zhang",
      "Joyce C. Ho",
      "Carl Yang",
      "Dong Yu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.903": {
    "title": "COAS2W: A Chinese Older-Adults Spoken-to-Written Transformation Corpus with Context Awareness",
    "volume": "main",
    "abstract": "Spoken language from older adults often deviates from written norms due to omission, disordered syntax, constituent errors, and redundancy, limiting the usefulness of automatic transcripts in downstream tasks. We present COAS2W, a Chinese spoken-to-written corpus of 10,004 utterances from older adults, each paired with a written version, fine-grained error labels, and four-sentence context. Fine-tuned lightweight open-source models on COAS2W outperform larger closed-source models. Context ablation shows the value of multi-sentence input, and normalization improves performance on downstream translation tasks. COAS2W supports the development of inclusive, context-aware language technologies for older speakers. Our annotation convention, data, and code are publicly available at https://github.com/Springrx/COAS2W",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chun Kang",
      "Zhigu Qian",
      "Zhen Fu",
      "Jiaojiao Fu",
      "Yangfan Zhou"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.904": {
    "title": "Answer Convergence as a Signal for Early Stopping in Reasoning",
    "volume": "main",
    "abstract": "Chain-of-thought (CoT) prompting enhances reasoning in large language models (LLMs) but often leads to verbose and redundant outputs, thus increasing inference cost. We hypothesize that many reasoning steps are unnecessary for producing correct answers. To investigate this, we start with a systematic study to investigate what is the minimum reasoning required for a model to reach a stable decision. Based on the insights, we propose three inference-time strategies to improve efficiency: (1) early stopping via answer consistency, (2) boosting the probability of generating end-of-reasoning signals, and (3) a supervised method that learns when to stop based on internal activations. Experiments across five benchmarks and five open-weights LLMs show that our methods largely reduce token usage with little or no accuracy drop. In particular, on NaturalQuestions, Answer Consistency reduces tokens by over 40% while further improving accuracy. Our work underscores the importance of cost-effective reasoning methods that operate at inference time, offering practical benefits for real-world applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Liu",
      "Lu Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.905": {
    "title": "VeriFact: Enhancing Long-Form Factuality Evaluation with Refined Fact Extraction and Reference Facts",
    "volume": "main",
    "abstract": "Large language models (LLMs) excel at generating long-form responses, but evaluating their factuality remains challenging due to complex inter-sentence dependencies within the generated facts. Prior solutions predominantly follow a decompose-decontextualize-verify pipeline but often fail to capture essential context and miss key relational facts. In this paper, we introduce VeriFact, a factuality evaluation framework designed to enhance fact extraction by identifying and resolving incomplete and missing facts to support more accurate verification results. Moreover, we introduce FactRBench , a benchmark that evaluates both precision and recall in long-form model responses, whereas prior work primarily focuses on precision. FactRBench provides reference fact sets from advanced LLMs and human-written answers, enabling recall assessment. Empirical evaluations show that VeriFact significantly enhances fact completeness and preserves complex facts with critical relational information, resulting in more accurate factuality evaluation. Benchmarking various open- and close-weight LLMs on FactRBench indicate that larger models within same model family improve precision and recall, but high precision does not always correlate with high recall, underscoring the importance of comprehensive factuality assessment",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Liu",
      "Lechen Zhang",
      "Sheza Munir",
      "Yiyang Gu",
      "Lu Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.906": {
    "title": "SQUAB: Evaluating LLM robustness to Ambiguous and Unanswerable Questions in Semantic Parsing",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have demonstrated robust performance in Semantic Parsing (SP) for well-defined queries with unambiguous intent and answerable responses. However, practical user questions frequently deviate from these ideal conditions, challenging the applicability of existing benchmarks. To address this issue, we introduce SQUAB, an automatic dataset generator of Ambiguous and Unanswerable questions. SQUAB generates complex, annotated SP tests using a blend of SQL and LLM capabilities. Results show that SQUAB reduces test generation costs by up to 99% compared to human-based solutions while aligning with real-world question patterns. Furthermore, these tests challenge LLM performance while revealing disparities between public and proprietary datasets. This highlights the need for a dynamic, automatic dataset generator as SQUAB. The code is designed for user extension to accommodate new ambiguous and unanswerable patterns and is available at https://anonymous.4open.science/r/squab-8716/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simone Papicchio",
      "Luca Cagliero",
      "Paolo Papotti"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.907": {
    "title": "Reliable Evaluation and Benchmarks for Statement Autoformalization",
    "volume": "main",
    "abstract": "Evaluating statement autoformalization, translating natural language mathematics into formal languages like Lean 4, remains a significant challenge, with few metrics, datasets, and standards to robustly measure progress. In this work, we present a comprehensive approach combining improved metrics, robust benchmarks, and systematic evaluation, to fill this gap. First, we introduce BEq+, an automated metric that correlates strongly with human judgment, along with ProofNetVerif, a new dataset for assessing the quality of evaluation metrics, containing 3,752 annotated examples. Second, we develop two new autoformalization benchmarks: ProofNet#, a corrected version of ProofNet, and RLM25, with 619 new pairs of research-level mathematics from six formalization projects. Through systematic experimentation across these benchmarks, we find that current techniques can achieve up to 45.1% accuracy on undergraduate mathematics but struggle with research-level content without proper context. Our work establishes a reliable foundation for evaluating and advancing autoformalization systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Auguste Poiroux",
      "Gail Weiss",
      "Viktor Kunčak",
      "Antoine Bosselut"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.908": {
    "title": "VisBias: Measuring Explicit and Implicit Social Biases in Vision Language Models",
    "volume": "main",
    "abstract": "This research investigates both explicit and implicit social biases exhibited by Vision-Language Models (VLMs). The key distinction between these bias types lies in the level of awareness: explicit bias refers to conscious, intentional biases, while implicit bias operates subconsciously. To analyze explicit bias, we directly pose questions to VLMs related to gender and racial differences: (1) Multiple-choice questions based on a given image (e.g., \"What is the education level of the person in the image?\") (2) Yes-No comparisons using two images (e.g., \"Is the person in the first image more educated than the person in the second image?\") For implicit bias, we design tasks where VLMs assist users but reveal biases through their responses: (1) Image description tasks: Models are asked to describe individuals in images, and we analyze disparities in textual cues across demographic groups. (2) Form completion tasks: Models draft a personal information collection form with 20 attributes, and we examine correlations among selected attributes for potential biases. We evaluate Gemini-1.5, GPT-4V, GPT-4o, LLaMA-3.2-Vision and LLaVA-v1.6. Our code and data are publicly available at https://github.com/uscnlp-lime/VisBias",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jen-tse Huang",
      "Jiantong Qin",
      "Jianping Zhang",
      "Youliang Yuan",
      "Wenxuan Wang",
      "Jieyu Zhao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.909": {
    "title": "Less Is More? Examining Fairness in Pruned Large Language Models for Summarising Opinions",
    "volume": "main",
    "abstract": "Model compression through post-training pruning offers a way to reduce model size and computational requirements without significantly impacting model performance. However, the effect of pruning on the fairness of LLM-generated summaries remains unexplored, particularly for opinion summarisation where biased outputs could influence public views. In this paper, we present a comprehensive empirical analysis of opinion summarisation, examining three state-of-the-art pruning methods and various calibration sets across three open-source LLMs using four fairness metrics. Our systematic analysis reveals that pruning methods have larger impact on fairness than calibration sets. Building on these insights, we propose High Gradient Low Activation (HGLA) pruning, which identifies and removes parameters that are redundant for input processing but influential in output generation. Our experiments demonstrate that HGLA can better maintain or even improve fairness compared to existing methods, showing promise across models and tasks where traditional methods have limitations. Our human evaluation shows HGLA-generated outputs are fairer than existing state-of-the-art pruning methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nannan Huang",
      "Haytham M. Fayek",
      "Xiuzhen Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.910": {
    "title": "AI Sees Your Location—But With A Bias Toward The Wealthy World",
    "volume": "main",
    "abstract": "Visual-Language Models (VLMs) have shown remarkable performance across various tasks, particularly in recognizing geographic information from images. However, VLMs still show regional biases in this task. To systematically evaluate these issues, we introduce a benchmark consisting of 1,200 images paired with detailed geographic metadata. Evaluating four VLMs, we find that while these models demonstrate the ability to recognize geographic information from images, achieving up to 53.8% accuracy in city prediction, they exhibit significant biases. Specifically, performance is substantially higher for economically developed and densely populated regions compared to less developed (-12.5%) and sparsely populated (-17.0%) areas. Moreover, regional biases of frequently over-predicting certain locations remain. For instance, they consistently predict Sydney for images taken in Australia, shown by the low entropy scores for these countries. The strong performance of VLMs also raises privacy concerns, particularly for users who share images online without the intent of being identified. Our code and dataset are publicly available at https://github.com/uscnlp-lime/FairLocator",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyuan Huang",
      "Jen-tse Huang",
      "Ziyi Liu",
      "Xiaoyuan Liu",
      "Wenxuan Wang",
      "Jieyu Zhao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.911": {
    "title": "Faster In-Context Learning for LLMs via N-Gram Trie Speculative Decoding",
    "volume": "main",
    "abstract": "As a crucial method in prompt engineering, In-Context Learning (ICL) enhances the generalization and knowledge utilization capabilities of Large Language Models (LLMs) (Dong et al., 2024). However, the lengthy retrieved contexts and limited token throughput in autoregressive models significantly constrain reasoning speed. To address this challenge, we propose N-Gram Trie Speculative Decoding, a novel approach that leverages the overlap between context and model output. This method constructs an n-gram trie from the context to generate drafts, accelerating token generation for LLMs. We evaluate our approach on summarization, Retrieval-Augmented Generation (RAG), and context-based Question Answering (QA) tasks. Experimental results on Vicuna-7B, Llama2-7B-Chat, and Llama3-8B-Instruct demonstrate substantial speed improvements without compromising accuracy. Compared with various strong baselines, our method achieves the highest mean speedup, showcasing its effectiveness and efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinglin Chen",
      "Qiwei Li",
      "Zuchao Li",
      "Baoyuan Qi",
      "Liu Guoming",
      "Haojun Ai",
      "Hai Zhao",
      "Ping Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.912": {
    "title": "From Surveys to Narratives: Rethinking Cultural Value Adaptation in LLMs",
    "volume": "main",
    "abstract": "Adapting cultural values in Large Language Models (LLMs) presents significant challenges, particularly due to biases and data limitations. Previous work aligns LLMs with different cultures using survey data, primarily from the World Values Survey (WVS). However, it remains unclear whether this approach effectively captures cultural nuances or produces distinct cultural representations for tasks like offensiveness classification. In this paper, we systematically investigate WVS-based training for cultural value adaptation and find that relying solely on survey data can homogenize cultural norms and interfere with factual knowledge. To address these issues, we propose augmenting WVS with encyclopedic and scenario-based cultural narratives from Wikipedia and NormAd. Our experiments across multiple cultures show that this approach captures more enhances differentiated cultural values and improves downstream classification performances",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Farid Adilazuarda",
      "Chen Cecilia Liu",
      "Iryna Gurevych",
      "Alham Fikri Aji"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.913": {
    "title": "Iterative Prompt Refinement for Safer Text-to-Image Generation",
    "volume": "main",
    "abstract": "Text-to-Image (T2I) models have made remarkable progress in generating images from text prompts, but their output quality and safety still depend heavily on how prompts are phrased. Existing safety methods typically refine prompts using large language models (LLMs), but they overlook the images produced, which can result in unsafe outputs or unnecessary changes to already safe prompts. To address this, we propose an iterative prompt refinement algorithm that uses Vision Language Models (VLMs) to analyze both the input prompts and the generated images. By leveraging visual feedback, our method refines prompts more effectively, improving safety while maintaining user intent and reliability comparable to existing LLM-based approaches. Additionally, we introduce a new dataset labeled with both textual and visual safety signals using off-the-shelf multi-modal LLM, enabling supervised fine-tuning. Experimental results demonstrate that our approach produces safer outputs without compromising alignment with user intent, offering a practical solution for generating safer T2I content. \\textcolor{red}{WARNING: This paper contains examples of harmful or inappropriate images generated by models.}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinwoo Jeon",
      "JunHyeok Oh",
      "Hayeong Lee",
      "Byung-Jun Lee"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.914": {
    "title": "Language Models as Continuous Self-Evolving Data Engineers",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities, yet their further evolution is often hampered by the scarcity of high-quality training data and the heavy reliance of traditional methods on expert-labeled data. This reliance sets a ceiling on LLM performance and is particularly challenging in low data resource scenarios where extensive supervision is unavailable. To address this issue, we propose a novel paradigm named LANCE (**LAN**guage models as **C**ontinuous self-**E**volving data engineers) that enables LLMs to train themselves by autonomously generating, cleaning, reviewing, and annotating data with preference information. Our approach demonstrates that LLMs can serve as continuous self-evolving data engineers, significantly reducing the time and cost of post-training data construction. Through iterative fine-tuning on Qwen2 series models, we validate the effectiveness of LANCE across various tasks, showing that it can maintain high-quality data generation and continuously improve model performance. Across multiple benchmark dimensions, LANCE results in an average score enhancement of **3.64** for Qwen2-7B and **1.75** for Qwen2-7B-Instruct. This autonomous data construction paradigm not only lessens reliance on human experts or external models but also ensures data aligns with human preferences, offering a scalable path for LLM self-improvement, especially in contexts with limited supervisory data. Code is available at: https://github.com/Control-derek/LANCE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peidong Wang",
      "Ming Wang",
      "Zhiming Ma",
      "Xiaocui Yang",
      "Shi Feng",
      "Daling Wang",
      "Yifei Zhang",
      "Kaisong Song"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.915": {
    "title": "Unilaw-R1: A Large Language Model for Legal Reasoning with Reinforcement Learning and Iterative Inference",
    "volume": "main",
    "abstract": "Reasoning-focused large language models (LLMs) are rapidly evolving across various domains, yet their capabilities in handling complex legal problems remains underexplored. In this paper, we introduce Unilaw-R1, a large language model tailored for legal reasoning. With a lightweight 7-billion parameter scale, Unilaw-R1 significantly reduces deployment cost while effectively tackling three core challenges in the legal domain: insufficient legal knowledge, unreliable reasoning logic, and weak business generalization. To address these issues, we first construct Unilaw-R1-Data, a high-quality dataset containing ~17K distilled and screened chain-of-thought (CoT) samples. Based on this, we adopt a two-stage training strategy combining Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), which significantly boosts the model's performance on complex legal reasoning tasks and supports interpretable decision-making in legal AI applications. To assess legal reasoning ability, we also introduce Unilaw-R1-Eval, a dedicated benchmark designed to evaluate models across single- and multi-choice legal tasks. Unilaw-R1 demonstrates strong results on authoritative benchmarks, outperforming all models of similar scale and achieving performance on par with the much larger DeepSeek-R1-Distill-Qwen-32B (54.9%). Following domain-specific training, it also showed significant gains on LawBench and LexEval, exceeding Qwen-2.5-7B-Instruct (46.6%) by an average margin of 6.6%. Code is available at: https://github.com/Hanscal/Unilaw-R1",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hua Cai",
      "Shuang Zhao",
      "Liang Zhang",
      "Xuli Shen",
      "Qing Xu",
      "Weilin Shen",
      "Zihao Wen",
      "Tianke Ban"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.916": {
    "title": "Exploring Response Uncertainty in MLLMs: An Empirical Evaluation under Misleading Scenarios",
    "volume": "main",
    "abstract": "Multimodal large language models (MLLMs) have recently achieved state-of-the-art performance on tasks ranging from visual question answering to video understanding. However, existing studies have concentrated mainly on visual–textual misalignment, leaving largely unexplored the MLLMs' ability to preserve an originally correct answer when confronted with misleading information. We reveal a response uncertainty phenomenon: across nine standard datasets, twelve state-of-the-art open-source MLLMs overturn a previously correct answer in 65% of cases after receiving a single deceptive cue. To systematically quantify this vulnerability, we propose a two-stage evaluation pipeline: (1) elicit each model's original response on unperturbed inputs; (2) inject explicit (false-answer hints) and implicit (contextual contradictions) misleading instructions, and compute the misleading rate—the fraction of correct-to-incorrect flips. Leveraging the most susceptible examples, we curate the Multimodal Uncertainty Benchmark (MUB), a collection of image–question pairs stratified into low, medium, and high difficulty based on how many of twelve state-of-the-art MLLMs they mislead. Extensive evaluation on twelve open-source and five closed-source models reveals a high uncertainty: average misleading rates exceed 86%, with explicit cues over 67.19% and implicit cues over 80.67%. To reduce the misleading rate, we then fine-tune all open-source MLLMs on a compact 2,000-sample mixed-instruction dataset, reducing misleading rates to 6.97% (explicit) and 32.77% (implicit), boosting consistency by nearly 29.37% on highly deceptive inputs, and slightly improving accuracy on standard benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunkai Dang",
      "Mengxi Gao",
      "Yibo Yan",
      "Xin Zou",
      "Yanggan Gu",
      "Jungang Li",
      "Jingyu Wang",
      "Peijie Jiang",
      "Aiwei Liu",
      "Jia Liu",
      "Xuming Hu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.917": {
    "title": "Evaluating and Aligning Human Economic Risk Preferences in LLMs",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) are increasingly used in decision-making scenarios that involve risk assessment, yet their alignment with human economic rationality remains unclear. In this study, we investigate whether LLMs exhibit risk preferences consistent with human expectations across different personas. Specifically, we propose an evaluation metric called Risk Disparity Score (RDS) and assess whether LLM-generated responses reflect appropriate levels of risk aversion or risk-seeking behavior based on individual's persona. Our results reveal that while LLMs make reasonable decisions in simplified, personalized risk contexts, their performance declines in more complex economic decision-making tasks. To address this, we test whether current state-of-art alignment methods such as Direct Preference Optimization(DPO) and In Context Learning(ICL) can enhance LLM adherence to persona-specific risk preferences. We find DPO can improve the economic rationality of LLMs in loss-related parameters, offering a step toward more human-aligned AI decision-making",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxin Liu",
      "Yixuan Tang",
      "Yi Yang",
      "Kar Yan Tam"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.918": {
    "title": "Ensembling Prompting Strategies for Zero-Shot Hierarchical Text Classification with Large Language Models",
    "volume": "main",
    "abstract": "Hierarchical text classification aims to classify documents into multiple labels within a hierarchical taxonomy, making it an essential yet challenging task in natural language processing. Recently, using Large Language Models (LLM) to tackle hierarchical text classification in a zero-shot manner has attracted increasing attention due to their cost-efficiency and flexibility. Given the challenges of understanding the hierarchy, various HTC prompting strategies have been explored to elicit the best performance from LLMs.However, our empirical study reveals that LLMs are highly sensitive to these prompting strategies—(i) within a task, different strategies yield substantially different results, and (ii) across various tasks, the relative effectiveness of a given strategy varies significantly. To address this, we propose a novel ensemble method, HiEPS, which integrates the results of diverse prompting strategies to promote LLMs' reliability. We also introduce a path-valid voting mechanism for ensembling, which selects a valid result with the highest path frequency score. Extensive experiments on three benchmark datasets show that HiEPS boosts the performance of single prompting strategies and achieves SOTA results. The source code is available at https://github.com/MingxuanXia/HiEPS",
    "checked": true,
    "id": "559088881b2d8210732397134bc280102676513b",
    "semantic_title": "ensembling prompting strategies for zero-shot hierarchical text classification with large language models",
    "citation_count": 0,
    "authors": [
      "Mingxuan Xia",
      "Zhijie Jiang",
      "Haobo Wang",
      "Junbo Zhao",
      "Tianlei Hu",
      "Gang Chen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.919": {
    "title": "Improbable Bigrams Expose Vulnerabilities of Incomplete Tokens in Byte-Level Tokenizers",
    "volume": "main",
    "abstract": "Tokenization is a crucial step that bridges human-readable text with model-readable discrete tokens. However, recent studies have revealed that tokenizers can be exploited to elicit unwanted model behaviors. In this work, we investigate incomplete tokens, i.e., undecodable tokens with stray bytes resulting from byte-level byte-pair encoding (BPE) tokenization. We hypothesize that such tokens are heavily reliant on their adjacent tokens and are fragile when paired with unfamiliar tokens. To demonstrate this vulnerability, we introduce improbable bigrams: out-of-distribution combinations of incomplete tokens designed to exploit their dependency. Our experiments show that improbable bigrams are significantly prone to hallucinatory behaviors. Surprisingly, the same phrases have drastically lower rates of hallucination (90% reduction in Llama3.1) when an alternative tokenization is used. We caution against the potential vulnerabilities introduced by byte-level BPE tokenizers, which may introduce blind spots to language models",
    "checked": true,
    "id": "5c80027e793f64dd02cb73cde821aab02f13a736",
    "semantic_title": "improbable bigrams expose vulnerabilities of incomplete tokens in byte-level tokenizers",
    "citation_count": 1,
    "authors": [
      "Eugene Jang",
      "Kimin Lee",
      "Jin-Woo Chung",
      "Keuntae Park",
      "Seungwon Shin"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.920": {
    "title": "UI-Hawk: Unleashing the Screen Stream Understanding for Mobile GUI Agents",
    "volume": "main",
    "abstract": "Graphical User Interface (GUI) agents are expected to precisely operate on the screens of digital devices. Existing GUI agents merely depend on current visual observations and plain-text action history, ignoring the significance of history screens. To mitigate this issue, we propose **UI-Hawk**, a multi-modal GUI agent specially designed to process screen streams encountered during GUI navigation. UI-Hawk incorporates a history-aware visual encoder to handle the screen sequences. To acquire a better understanding of screen streams, we select four fundamental tasks—UI grounding, UI referring, screen question answering, and screen summarization. We further propose a curriculum learning strategy to subsequently guide the model from fundamental tasks to advanced screen-stream comprehension.Along with the efforts above, we have also created a benchmark FunUI to quantitatively evaluate the fundamental screen understanding ability of MLLMs. Extensive experiments on FunUI and GUI navigation benchmarks consistently validate that screen stream understanding is essential for GUI tasks.Our code and data are now available at https://github.com/IMNearth/UIHawk",
    "checked": true,
    "id": "d22e4b16b9c3a6cb57742a2b04bd24cc732a83df",
    "semantic_title": "ui-hawk: unleashing the screen stream understanding for mobile gui agents",
    "citation_count": 0,
    "authors": [
      "Jiwen Zhang",
      "Ya-Qi Yu",
      "Minghui Liao",
      "WenTao Li",
      "Jihao Wu",
      "Zhongyu Wei"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.921": {
    "title": "UniDebugger: Hierarchical Multi-Agent Framework for Unified Software Debugging",
    "volume": "main",
    "abstract": "Software debugging is a time-consuming endeavor involving a series of steps, such as fault localization and patch generation, each requiring thorough analysis and a deep understanding of the underlying logic. While large language models (LLMs) demonstrate promising potential in coding tasks, their performance in debugging remains limited. Current LLM-based methods often focus on isolated steps and struggle with complex bugs. In this paper, we propose the first end-to-end framework, UniDebugger, for unified debugging through multi-agent synergy. It mimics the entire cognitive processes of developers, with each agent specialized as a particular component of this process rather than mirroring the actions of an independent expert as in previous multi-agent systems. Agents are coordinated through a three-level design, following a cognitive model of debugging, allowing adaptive handling of bugs with varying complexities. Experiments on extensive benchmarks demonstrate that UniDebugger significantly outperforms state-of-the-art repair methods, fixing 1.25x to 2.56x bugs on the repo-level benchmark, Defects4J. This performance is achieved without requiring ground-truth root-cause code statements, unlike the baselines. Our source code is available on an anonymous link: https://github.com/BEbillionaireUSD/UniDebugger",
    "checked": true,
    "id": "ce7954bd7eeafea8cda3139d8e79ae9cef00c19c",
    "semantic_title": "unidebugger: hierarchical multi-agent framework for unified software debugging",
    "citation_count": 0,
    "authors": [
      "Cheryl Lee",
      "Chunqiu Steven Xia",
      "Longji Yang",
      "Jen-tse Huang",
      "Zhouruixing Zhu",
      "Lingming Zhang",
      "Michael R. Lyu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.922": {
    "title": "Understanding the Thinking Process of Reasoning Models: A Perspective from Schoenfeld's Episode Theory",
    "volume": "main",
    "abstract": "While Large Reasoning Models (LRMs) generate extensive chain-of-thought reasoning, we lack a principled framework for understanding how these thoughts are structured. In this paper, we introduce a novel approach by applying Schoenfeld's Episode Theory, a classic cognitive framework for human mathematical problem-solving, to analyze the reasoning traces of LRMs. We annotated thousands of sentences and paragraphs from model-generated solutions to math problems using seven cognitive labels (e.g., Plan, Implement, Verify). The result is the first publicly available benchmark for the fine-grained analysis of machine reasoning, including a large annotated corpus and detailed annotation guidebooks. Our preliminary analysis reveals distinct patterns in LRM reasoning, such as the transition dynamics between cognitive states. This framework provides a theoretically grounded methodology for interpreting LRM cognition and enables future work on more controllable and transparent reasoning systems",
    "checked": true,
    "id": "bf7b61e3761a228dbd151241c1eefe1d324283d8",
    "semantic_title": "understanding the thinking process of reasoning models: a perspective from schoenfeld's episode theory",
    "citation_count": 3,
    "authors": [
      "Ming Li",
      "Nan Zhang",
      "Chenrui Fan",
      "Hong Jiao",
      "Yanbin Fu",
      "Sydney Peters",
      "Qingshu Xu",
      "Robert Lissitz",
      "Tianyi Zhou"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.923": {
    "title": "Thread: A Logic-Based Data Organization Paradigm for How-To Question Answering with Retrieval Augmented Generation",
    "volume": "main",
    "abstract": "Recent advances in retrieval-augmented generation (RAG) have substantially improved question-answering systems, particularly for factoid ‘5Ws' questions. However, significant challenges remain when addressing ‘1H' questions, specifically how-to questions, which are integral for decision-making and require dynamic, step-by-step responses. The key limitation lies in the prevalent data organization paradigm, chunk, which commonly divides documents into fixed-size segments, and disrupts the logical coherence and connections within the context. To address this, we propose THREAD, a novel data organization paradigm enabling systems to handle how-to questions more effectively. Specifically, we introduce a new knowledge granularity, ‘logic unit' (LU), where large language models transform documents into more structured and loosely interconnected LUs. Extensive experiments across both open-domain and industrial settings show that THREAD outperforms existing paradigms significantly, improving the success rate of handling how-to questions by 21% to 33%. Additionally, THREAD demonstrates high adaptability across diverse document formats, reducing retrieval information by up to 75% compared to chunk, and also shows better generalizability to ‘5Ws' questions, such as multi-hop questions, outperforming other paradigms",
    "checked": true,
    "id": "35adc58a2d9d3affa1a8d211bd1536d129658a35",
    "semantic_title": "thread: a logic-based data organization paradigm for how-to question answering with retrieval augmented generation",
    "citation_count": 2,
    "authors": [
      "Kaikai An",
      "Fangkai Yang",
      "Liqun Li",
      "Junting Lu",
      "Sitao Cheng",
      "Shuzheng Si",
      "Lu Wang",
      "Pu Zhao",
      "Lele Cao",
      "Qingwei Lin",
      "Saravan Rajmohan",
      "Dongmei Zhang",
      "Baobao Chang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.924": {
    "title": "Unsupervised Word-level Quality Estimation for Machine Translation Through the Lens of Annotators (Dis)agreement",
    "volume": "main",
    "abstract": "Word-level quality estimation (WQE) aims to automatically identify fine-grained error spans in machine-translated outputs and has found many uses, including assisting translators during post-editing. Modern WQE techniques are often expensive, involving prompting of large language models or ad-hoc training on large amounts of human-labeled data. In this work, we investigate efficient alternatives exploiting recent advances in language model interpretability and uncertainty quantification to identify translation errors from the inner workings of translation models. In our evaluation spanning 14 metrics across 12 translation directions, we quantify the impact of human label variation on metric performance by using multiple sets of human labels. Our results highlight the untapped potential of unsupervised metrics, the shortcomings of supervised methods when faced with label uncertainty, and the brittleness of single-annotator evaluation practices",
    "checked": true,
    "id": "533b4bcbbfa05c205b8ecf95aae51d044c33d87d",
    "semantic_title": "unsupervised word-level quality estimation for machine translation through the lens of annotators (dis)agreement",
    "citation_count": 0,
    "authors": [
      "Gabriele Sarti",
      "Vilém Zouhar",
      "Malvina Nissim",
      "Arianna Bisazza"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.925": {
    "title": "STEER-BENCH: A Benchmark for Evaluating the Steerability of Large Language Models",
    "volume": "main",
    "abstract": "Steerability, or the ability of large language models (LLMs) to adapt outputs to align with diverse community-specific norms, perspectives, and communication styles, is critical for real-world applications but remains under-evaluated. We introduce STEER-BENCH, a benchmark for assessing population-specific steering using contrasting Reddit communities. Covering 30 contrasting subreddit pairs across 19 domains, STEER-BENCH includes over 10,000 instruction-response pairs and validated 5,500 multiple-choice questions with corresponding silver labels to test alignment with diverse community norms. It systematically assesses how effectively LLMs understand community-specific instructions, their resilience to adversarial steering attempts, and their ability to accurately represent diverse cultural and ideological perspectives. Our evaluation of 13 popular LLMs using STEER-BENCH reveals that while human experts achieve an accuracy of 81% with silver labels, the best-performing models reach only around 65% accuracy depending on the domain and configuration. Some models lag behind human-level alignment by over 15 percentage points, highlighting significant gaps in community-sensitive steerability",
    "checked": true,
    "id": "e5cdf9f00cfbc8e68ae57b51e040d16276d7115a",
    "semantic_title": "steer-bench: a benchmark for evaluating the steerability of large language models",
    "citation_count": 2,
    "authors": [
      "Kai Chen",
      "Zihao He",
      "Taiwei Shi",
      "Kristina Lerman"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.926": {
    "title": "Combining Constrained and Unconstrained Decoding via Boosting: BoostCD and Its Application to Information Extraction",
    "volume": "main",
    "abstract": "Many recent approaches to structured NLP tasks use an autoregressive language model M to map unstructured input text x to output text y representing structured objects (such as tuples, lists, trees, code, etc.), where the desired output structure is enforced via constrained decoding. During training, these approaches do not require the model to be aware of the constraints, which are merely implicit in the training outputs y. This is advantageous as it allows for dynamic constraints without requiring retraining, but can lead to low-quality output during constrained decoding at test time. We overcome this problem with Boosted Constrained Decoding (BoostCD) which combines constrained and unconstrained decoding in two phases: Phase 1 decodes from the base model M twice, in constrained and unconstrained mode, obtaining two weak predictions. In phase 2, a learned autoregressive boosted model combines the two weak predictions into one final prediction. The mistakes made by the base model with vs. without constraints tend to be complementary, which the boosted model learns to exploit for improved performance. We demonstrate the power of BoostCD by applying it to closed information extraction. Our model, BoostIE, outperforms prior approaches both in and out of distribution, addressing several common errors identified in those approaches",
    "checked": true,
    "id": "923f22edea4d93a215c0c2113ae39e05422bcfa5",
    "semantic_title": "combining constrained and unconstrained decoding via boosting: boostcd and its application to information extraction",
    "citation_count": 0,
    "authors": [
      "Marija Sakota",
      "Robert West"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.927": {
    "title": "MultiLogicNMR(er): A Benchmark and Neural-Symbolic Framework for Non-monotonic Reasoning with Multiple Extensions",
    "volume": "main",
    "abstract": "Non-monotonic reasoning (NMR) refers to the fact that conclusions may be invalidated by new information. It is widely used in daily life and legal reasoning. An NMR task usually has multiple extensions, which are sets of plausible conclusions. There are two reasoning modes – skeptical and credulous reasoning, depending on whether to believe facts in all extensions or any one extension. Despite some preliminary work exploring the NMR abilities of LLMs, the multi-extension NMR capabilities of LLMs remain underexplored. In this paper, we synthesize a multi-extension NMR dataset MultiLogicNMR, and construct two variants of the dataset with more extensions or text diversity. We propose a neural-symbolic framework MultiLogicNMRer for multi-extension NMR. Experimental evaluation with the datasets shows that LLMs still face significant challenges in NMR abilities, and reveal the effectiveness of our neural-symbolic framework, with an average accuracy gain of about 15% compared to prompt-based methods, and even outperforming some fine-tuning methods. All code and data are publicly available",
    "checked": true,
    "id": "1784859b843633df14f69bc607e131116cb1ce61",
    "semantic_title": "multilogicnmr(er): a benchmark and neural-symbolic framework for non-monotonic reasoning with multiple extensions",
    "citation_count": 0,
    "authors": [
      "Yeliang Xiu",
      "Yongmei Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.928": {
    "title": "Beyond Demographics: Enhancing Cultural Value Survey Simulation with Multi-Stage Personality-Driven Cognitive Reasoning",
    "volume": "main",
    "abstract": "Introducing **MARK**, the **M**ulti-st**A**ge **R**easoning framewor**K** for cultural value survey response simulation, designed to enhance the accuracy, steerability, and interpretability of large language models in this task. The system is inspired by the type dynamics theory in the MBTI psychological framework for personality research. It effectively predicts and utilizes human demographic information for simulation: life-situational stress analysis, group-level personality prediction, and self-weighted cognitive imitation. Experiments on the World Values Survey show that MARK outperforms existing baselines by 10% accuracy and reduces the divergence between model predictions and human preferences. This highlights the potential of our framework to improve zero-shot personalization and help social scientists interpret model predictions",
    "checked": true,
    "id": "86d53ea8913b9fe4ca1f4bfd8a828350e36b5547",
    "semantic_title": "beyond demographics: enhancing cultural value survey simulation with multi-stage personality-driven cognitive reasoning",
    "citation_count": 2,
    "authors": [
      "Haijiang Liu",
      "Qiyuan Li",
      "Chao Gao",
      "Yong Cao",
      "Xiangyu Xu",
      "Xun Wu",
      "Daniel Hershcovich",
      "Jinguang Gu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.929": {
    "title": "CrystalICL: Enabling In-Context Learning for Crystal Generation",
    "volume": "main",
    "abstract": "Designing crystal materials with desired physicochemical properties remains a fundamental challenge in materials science. While large language models (LLMs) have demonstrated strong in-context learning (ICL) capabilities, existing LLM-based crystal generation approaches are limited to zero-shot scenarios and are unable to benefit from few-shot scenarios. In contrast, human experts typically design new materials by modifying relevant known structures which aligns closely with the few-shot ICL paradigm. Motivated by this, we propose CrystalICL, a novel model designed for few-shot crystal generation. Specifically, we introduce a space-group based crystal tokenization method, which effectively reduces the complexity of modeling crystal symmetry in LLMs. We further introduce a condition-structure aware hybrid instruction tuning framework and a multi-task instruction tuning strategy, enabling the model to better exploit ICL by capturing structure-property relationships from limited data. Extensive experiments on four crystal generation benchmarks demonstrate the superiority of CrystalICL over the leading baseline methods on conditional and unconditional generation tasks",
    "checked": true,
    "id": "6bc290a51240828d2f652610ab222f0b129bddcc",
    "semantic_title": "crystalicl: enabling in-context learning for crystal generation",
    "citation_count": 1,
    "authors": [
      "Ruobing Wang",
      "Qiaoyu Tan",
      "Yili Wang",
      "Ying Wang",
      "Xin Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.930": {
    "title": "Towards a Unified Paradigm of Concept Editing in Large Language Models",
    "volume": "main",
    "abstract": "Concept editing aims to control specific concepts in large language models (LLMs) and is an emerging subfield of model editing. Despite the emergence of various editing methods in recent years, there remains a lack of rigorous theoretical analysis and a unified perspective to systematically understand and compare these methods. To address this gap, we propose a unified paradigm for concept editing methods, in which all forms of conceptual injection are aligned at the neuron level. We study four representative concept editing methods: Neuron Editing (NE), Supervised Fine-tuning (SFT), Sparse Autoencoder (SAE), and Steering Vector (SV). Then we categorize them into two classes based on their mode of conceptual information injection: indirect (NE, SFT) and direct (SAE, SV). We evaluate above methods along four dimensions: editing reliability, output generalization, neuron level consistency, and mathematical formalization. Experiments show that SAE achieves the best editing reliability. In output generalization, SAE captures features closer to human-understood concepts, while NE tends to locate text patterns rather than true semantics. Neuron-level analysis reveals that direct methods share high neuron overlap, as do indirect methods, indicating methodological commonality within each category. Our unified paradigm offers a clear framework and valuable insights for advancing interpretability and controlled generation in LLMs",
    "checked": true,
    "id": "9294223b1f9f9d97eb74739ecda722c6978bac37",
    "semantic_title": "towards a unified paradigm of concept editing in large language models",
    "citation_count": 0,
    "authors": [
      "Zhuowen Han",
      "Xinwei Wu",
      "Dan Shi",
      "Renren Jin",
      "Deyi Xiong"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.931": {
    "title": "Step-level Verifier-guided Hybrid Test-Time Scaling for Large Language Models",
    "volume": "main",
    "abstract": "Test-Time Scaling (TTS) is a promising approach to progressively elicit the model's intelligence during inference. Recently, training-based TTS methods, such as continued reinforcement learning (RL), have further surged in popularity, while training-free TTS methods are gradually fading from prominence. However, the additional computation overhead of training amplifies the burden on test-time scaling.In this paper, we focus on training-free TTS methods for reasoning. We first design Conditional Step-level Self-refinement, a fine-grained sequential scaling method guided by process verification. On top of its effectiveness, we further combine it with other classical parallel scaling methods at the step level, to introduce a novel inference paradigm called Hybrid Test-Time Scaling. Extensive experiments on five instruction-tuned LLMs across different scales (3B-14B) and families demonstrate that hybrid strategy incorporating various training-free TTS methods at a fine granularity has considerable potential for expanding the reasoning performance boundaries of LLMs",
    "checked": true,
    "id": "11f48e072f72ae84dea668bb965a63cde3fd7276",
    "semantic_title": "step-level verifier-guided hybrid test-time scaling for large language models",
    "citation_count": 1,
    "authors": [
      "Kaiyan Chang",
      "Yonghao Shi",
      "Chenglong Wang",
      "Hang Zhou",
      "Chi Hu",
      "Xiaoqian Liu",
      "Yingfeng Luo",
      "Yuan Ge",
      "Tong Xiao",
      "JingBo Zhu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.932": {
    "title": "Dynamic Expert Specialization: Towards Catastrophic Forgetting-Free Multi-Domain MoE Adaptation",
    "volume": "main",
    "abstract": "Mixture-of-Experts (MoE) models offer immense capacity via sparsely gated expert subnetworks, yet adapting them to multiple domains without catastrophic forgetting remains an open challenge. Existing approaches either incur prohibitive computation, suffer cross-domain interference, or require separate runs per domain. We propose DES-MoE, a dynamic expert specialization framework for multi-domain adaptation of Mixture-of-Experts models. DES-MoE addresses catastrophic forgetting through three innovations: (1) an adaptive router balancing pre-trained knowledge retention and task-specific updates via distillation, (2) real-time expert-domain correlation mapping to isolate domain-specific gradients, and (3) a three-phase adaptive fine-tuning schedule that progressively freezes non-specialized parameters. Evaluated on six domains (math, code, law, etc.), DES-MoE matches single-domain ESFT performance while training one unified model, reduces forgetting by 89% compared to full fine-tuning as domains scale from 2 to 6, and achieves 68% faster convergence than conventional methods. Our work establishes dynamic expert isolation as a scalable paradigm for multi-task MoE adaptation",
    "checked": true,
    "id": "4fd2798710ae7e9ffe8ef96666891f43885c79db",
    "semantic_title": "dynamic expert specialization: towards catastrophic forgetting-free multi-domain moe adaptation",
    "citation_count": 0,
    "authors": [
      "Junzhuo Li",
      "Bo Wang",
      "Xiuze Zhou",
      "Xuming Hu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.933": {
    "title": "RRInf: Efficient Influence Function Estimation via Ridge Regression for Large Language Models and Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "The quality of data plays a vital role in the development of Large-scale Generative Models. Understanding how important a data point is for a generative model is essential for explaining its behavior and improving the performance. The influence function provides a framework for quantifying the impact of individual training data on model predictions. However, the high computational cost has hindered their applicability in large-scale applications. In this work, we present RRInf, a novel and principled method for estimating influence function in large-scale generative AI models. We show that influence function estimation can be transformed into a ridge regression problem. Based on this insight, we develop an algorithm that is efficient and scalable to large models. Experiments on noisy data detection and influential data identification tasks demonstrate that RRInf outperforms existing methods in terms of both efficiency and effectiveness for commonly used large models: RoBERTa-large, Llama-2-13B-chat, Llama-3-8B and stable-diffusion-v1.5",
    "checked": true,
    "id": "dd4328d12254d9bb80883633357e832b90613895",
    "semantic_title": "rrinf: efficient influence function estimation via ridge regression for large language models and text-to-image diffusion models",
    "citation_count": 0,
    "authors": [
      "Zhuozhuo Tu",
      "Cheng Chen",
      "Yuxuan Du"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.934": {
    "title": "Evaluating Spatiotemporal Consistency in Automatically Generated Sewing Instructions",
    "volume": "main",
    "abstract": "In this paper, we propose a novel, automatic tree-based evaluation metric for LLM-generated step-by-step assembly instructions, that more accurately reflects spatiotemporal aspects of construction than traditional metrics such as BLEU and BERT similarity scores. We apply our proposed metric to the domain of sewing instructions, and show that our metric better correlates with manually-annotated error counts, demonstrating our metric's superiority for evaluating the spatiotemporal soundness of sewing instructions. Further experiments show that our metric is more robust than traditional approaches against artificially-constructed counterfactual examples that are specifically constructed to confound metrics that rely on textual similarity",
    "checked": true,
    "id": "533cce87104de51c8ba6616f68bdc9a787319585",
    "semantic_title": "evaluating spatiotemporal consistency in automatically generated sewing instructions",
    "citation_count": 0,
    "authors": [
      "Luisa Geiger",
      "Mareike Hartmann",
      "Michael Sullivan",
      "Alexander Koller"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.935": {
    "title": "MaZO: Masked Zeroth-Order Optimization for Multi-Task Fine-Tuning of Large Language Models",
    "volume": "main",
    "abstract": "Large language models have demonstrated exceptional capabilities across diverse tasks, but their fine-tuning demands significant memory, posing challenges for resource-constrained environments. Zeroth-order (ZO) optimization provides a memory-efficient alternative by eliminating the need for backpropagation. However, ZO optimization suffers from high gradient variance, and prior research has largely focused on single-task learning, leaving its application to multi-task learning unexplored. Multi-task learning is crucial for leveraging shared knowledge across tasks to improve generalization, yet it introduces unique challenges under ZO settings, such as amplified gradient variance and collinearity. In this paper, we present MaZO, the first framework specifically designed for multi-task LLM fine-tuning under ZO optimization. MaZO tackles these challenges at the parameter level through two key innovations: a weight importance metric to identify critical parameters and a multi-task weight update mask to selectively update these parameters, reducing the dimensionality of the parameter space and mitigating task conflicts. Experiments demonstrate that MaZO achieves state-of-the-art performance, surpassing even multi-task learning methods designed for first-order optimization",
    "checked": true,
    "id": "24c85fcb76b8c4e30314f3fb6818adb05a740158",
    "semantic_title": "mazo: masked zeroth-order optimization for multi-task fine-tuning of large language models",
    "citation_count": 2,
    "authors": [
      "Zhen Zhang",
      "Yifan Yang",
      "Kai Zhen",
      "Nathan Susanj",
      "Athanasios Mouchtaris",
      "Siegfried Kunzmann",
      "Zheng Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.936": {
    "title": "Procedural Environment Generation for Tool-Use Agents",
    "volume": "main",
    "abstract": "Although the power of LLM tool-use agents has ignited a flurry of recent research in this area, the curation of tool-use training data remains an open problem\\textemdashespecially for online RL training. Existing approaches to synthetic tool-use data generation tend to be non-interactive and/or non-compositional. We introduce RandomWorld, a pipeline for the procedural generation of interactive tools and compositional tool-use data. We show that models tuned via SFT and RL on synthetic RandomWorld data improve on a range of tool-use benchmarks, and set the new SoTA for two metrics on the NESTFUL dataset. Further experiments show that downstream performance scales with the amount of RandomWorld-generated training data, opening up the possibility of further improvement through the use of entirely synthetic data",
    "checked": true,
    "id": "3f7d5f2953576350297f25f100228237d0e3726b",
    "semantic_title": "procedural environment generation for tool-use agents",
    "citation_count": 3,
    "authors": [
      "Michael Sullivan",
      "Mareike Hartmann",
      "Alexander Koller"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.937": {
    "title": "FacLens: Transferable Probe for Foreseeing Non-Factuality in Fact-Seeking Question Answering of Large Language Models",
    "volume": "main",
    "abstract": "Despite advancements in large language models (LLMs), non-factual responses still persist in fact-seeking question answering. Unlike extensive studies on post-hoc detection of these responses, this work studies non-factuality prediction (NFP), predicting whether an LLM will generate a non-factual response prior to the response generation. Previous NFP methods have shown LLMs' awareness of their knowledge, but they face challenges in terms of efficiency and transferability. In this work, we propose a lightweight model named Factuality Lens (FacLens), which effectively probes hidden representations of fact-seeking questions for the NFP task. Moreover, we discover that hidden question representations sourced from different LLMs exhibit similar NFP patterns, enabling the transferability of FacLens across different LLMs to reduce development costs. Extensive experiments highlight FacLens's superiority in both effectiveness and efficiency",
    "checked": true,
    "id": "3d31607b5cb48ec042f4fa54d019b10ccf607d35",
    "semantic_title": "faclens: transferable probe for foreseeing non-factuality in fact-seeking question answering of large language models",
    "citation_count": 0,
    "authors": [
      "Yanling Wang",
      "Haoyang Li",
      "Hao Zou",
      "Jing Zhang",
      "Xinlei He",
      "Qi Li",
      "Ke Xu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.938": {
    "title": "OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation via LLM Agent",
    "volume": "main",
    "abstract": "Keyword decision in Sponsored Search Advertising is critical to the success of ad campaigns. While LLM-based methods offer automated keyword generation, they face three major limitations: reliance on large-scale query–keyword pair data, lack of online multi-objective performance monitoring and optimization, and weak quality control in keyword selection. These issues hinder the agentic use of LLMs in fully automating keyword decisions by monitoring and reasoning over key performance indicators such as impressions, clicks, conversions, and CTA effectiveness. To overcome these challenges, we propose OMS, a keyword generation framework that is On-the-fly (requires no training data, monitors online performance, and adapts accordingly), Multi-objective (employs agentic reasoning to optimize keywords based on multiple performance metrics) and Self-reflective (agentically evaluates keyword quality). Experiments on benchmarks and real-world ad campaigns show that OMS outperforms existing methods; Ablation and human evaluations confirm the effectiveness of each component and the quality of generated keywords",
    "checked": true,
    "id": "ac9a0d265f8a18ff2bdbb906be5af21ead1ca599",
    "semantic_title": "oms: on-the-fly, multi-objective, self-reflective ad keyword generation via llm agent",
    "citation_count": 1,
    "authors": [
      "Bowen Chen",
      "Zhao Wang",
      "Shingo Takamatsu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.939": {
    "title": "Med-VRAgent: A Framework for Medical Visual Reasoning-Enhanced Agents",
    "volume": "main",
    "abstract": "Vision-language models (VLMs) achieve promising results in medical reasoning but struggle with hallucinations, vague descriptions, Inconsistent logic and poor localization. To address this, we propose a agent framework named Medical Visual Reasoning Agent (Med-VRAgent). The approach is based on Visual Guidance and Self-Reward paradigms and Monte Carlo Tree Search (MCTS). By combining the Visual Guidance with tree search, Med-VRAgent improves the medical visual reasoning capabilities of VLMs. We use the trajectories collected by Med-RAgent as feedback to further improve the performance by fine-tuning the VLMs with the proximal policy optimization (PPO) objective. Experiments on multiple medical VQA benchmarks demonstrate that our method outperforms existing approaches",
    "checked": true,
    "id": "53b9e6311e9c497d827030be2ea371877c7c7814",
    "semantic_title": "med-vragent: a framework for medical visual reasoning-enhanced agents",
    "citation_count": 0,
    "authors": [
      "Guangfu Guo",
      "Xiaoqian Lu",
      "Yue Feng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.940": {
    "title": "TrojanWave: Exploiting Prompt Learning for Stealthy Backdoor Attacks on Large Audio-Language Models",
    "volume": "main",
    "abstract": "Prompt learning has emerged as an efficient alternative to full fine-tuning for adapting large audio-language models (ALMs) to downstream tasks. While this paradigm enables scalable deployment via Prompt-as-a-Service frameworks, it also introduces a critical yet underexplored security risk of backdoor attacks. In this work, we present TrojanWave, the first backdoor attack tailored to the prompt-learning setting in frozen ALMs. Unlike prior audio backdoor methods that require training from scratch on full datasets, TrojanWave injects backdoors solely through learnable prompts, making it highly scalable and effective in few-shot settings. TrojanWave injects imperceptible audio triggers in both time and spectral domains to effectively induce targeted misclassification during inference. To mitigate this threat, we further propose TrojanWave-Defense, a lightweight prompt purification method that neutralizes malicious prompts without hampering the clean performance. Extensive experiments across 11 diverse audio classification benchmarks demonstrate the robustness and practicality of both the attack and defense. Our code is publicly available at https://asif-hanif.github.io/trojanwave/",
    "checked": true,
    "id": "e10689a33713a8dbbcc3a25252e3428d495f7054",
    "semantic_title": "trojanwave: exploiting prompt learning for stealthy backdoor attacks on large audio-language models",
    "citation_count": 0,
    "authors": [
      "Asif Hanif",
      "Maha Tufail Agro",
      "Fahad Shamshad",
      "Karthik Nandakumar"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.941": {
    "title": "Can LLMs be Literary Companions?: Analysing LLMs on Bengali Figures of Speech Identification",
    "volume": "main",
    "abstract": "Despite Bengali being among the most spoken languages bearing cultural importance and richness, the NLP endeavors on it, remain relatively limited. Figures of Speech (FoS) not only contribute to the phonetic and semantic nuances of a language, but they also exhibit aesthetics, expression, and creativity in literature. To our knowledge, in this paper, we present the first ever Bengali figures of speech classification dataset, **BengFoS**, on works of six renowned poets of Bengali literature. We deploy state-of-the-art Large Language Models (LLMs) to this dataset in the zero-shot setup, thereafter fine-tuning the best performing models, and finally dissect them for language model probing. This reveals novel insights on the intrinsic behavior of two open-source LLMs (Llama and DeepSeek) in FoS detection. **Though we have limited ourselves to Bengali, the experimental framework can be reproduced for English as well as for other low-resource languages**",
    "checked": true,
    "id": "23f2dd46eec596241d0fd9866acaf660c98dd16e",
    "semantic_title": "can llms be literary companions?: analysing llms on bengali figures of speech identification",
    "citation_count": 0,
    "authors": [
      "Sourav Das",
      "Kripabandhu Ghosh"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.942": {
    "title": "Group-SAE: Efficient Training of Sparse Autoencoders for Large Language Models via Layer Groups",
    "volume": "main",
    "abstract": "Sparse AutoEncoders (SAEs) have recently been employed as a promising unsupervised approach for understanding the representations of layers of Large Language Models (LLMs). However, with the growth in model size and complexity, training SAEs is computationally intensive, as typically one SAE is trained for each model layer. To address such limitation, we propose Group-SAE, a novel strategy to train SAEs. Our method considers the similarity of the residual stream representations between contiguous layers to group similar layers and train a single SAE per group. To balance the trade-off between efficiency and performance, we further introduce AMAD (Average Maximum Angular Distance), an empirical metric that guides the selection of an optimal number of groups based on representational similarity across layers. Experiments on models from the Pythia family show that our approach significantly accelerates training with minimal impact on reconstruction quality and comparable downstream task performance and interpretability over baseline SAEs trained layer by layer. This method provides an efficient and scalable strategy for training SAEs in modern LLMs",
    "checked": true,
    "id": "dd039186ad0fd661314ace27c85d354410896155",
    "semantic_title": "group-sae: efficient training of sparse autoencoders for large language models via layer groups",
    "citation_count": 9,
    "authors": [
      "Davide Ghilardi",
      "Federico Belotti",
      "Marco Molinari",
      "Tao Ma",
      "Matteo Palmonari"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.943": {
    "title": "Retrieval over Classification: Integrating Relation Semantics for Multimodal Relation Extraction",
    "volume": "main",
    "abstract": "Relation extraction (RE) aims to identify semantic relations between entities in unstructured text. Although recent work extends traditional RE to multimodal scenarios, most approaches still adopt classification-based paradigms with fused multimodal features, representing relations as discrete labels. This paradigm has two significant limitations: (1) it overlooks structural constraints like entity types and positional cues, and (2) it lacks semantic expressiveness for fine-grained relation understanding. We propose **R**etrieval **O**ver **C**lassification (ROC), a novel framework that reformulates multimodal RE as a retrieval task driven by relation semantics. ROC integrates entity type and positional information through a multimodal encoder, expands relation labels into natural language descriptions using a large language model, and aligns entity-relation pairs via semantic similarity-based contrastive learning. Experiments show that our method achieves state-of-the-art performance on the benchmark datasets MNRE and MORE and exhibits stronger robustness and interpretability",
    "checked": true,
    "id": "8f469c24ec84b4edffbaba5fbf7b0389158d18b8",
    "semantic_title": "retrieval over classification: integrating relation semantics for multimodal relation extraction",
    "citation_count": 0,
    "authors": [
      "Lei Hei",
      "Tingjing Liao",
      "Peiyingxin",
      "Yiyang Qi",
      "Jiaqi Wang",
      "Ruiting Li",
      "Feiliang Ren"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.944": {
    "title": "PunMemeCN: A Benchmark to Explore Vision-Language Models' Understanding of Chinese Pun Memes",
    "volume": "main",
    "abstract": "Pun memes, which combine wordplay with visual elements, represent a popular form of humor in Chinese online communications. Despite their prevalence, current Vision-Language Models (VLMs) lack systematic evaluation in understanding and applying these culturally-specific multimodal expressions. In this paper, we introduce PunMemeCN, a novel benchmark designed to assess VLMs' capabilities in processing Chinese pun memes across three progressive tasks: pun meme detection, sentiment analysis, and chat-driven meme response. PunMemeCN consists of 1,959 Chinese memes (653 pun memes and 1,306 non-pun memes) with comprehensive annotations of punchlines, sentiments, and explanations, alongside 2,008 multi-turn chat conversations incorporating these memes. Our experiments indicate that state-of-the-art VLMs struggle with Chinese pun memes, particularly with homophone wordplay, even with Chain-of-Thought prompting. Notably, punchlines in memes can effectively conceal potentially harmful content from AI detection. These findings underscore the challenges in cross-cultural multimodal understanding and highlight the need for culture-specific approaches to humor comprehension in AI systems",
    "checked": true,
    "id": "367100d6d40c5a55c32f574e854d4e7c7e7fa455",
    "semantic_title": "punmemecn: a benchmark to explore vision-language models' understanding of chinese pun memes",
    "citation_count": 0,
    "authors": [
      "Zhijun Xu",
      "Siyu Yuan",
      "Yiqiao Zhang",
      "Jingyu Sun",
      "Tong Zheng",
      "Deqing Yang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.945": {
    "title": "UltraIF: Advancing Instruction Following from the Wild",
    "volume": "main",
    "abstract": "Instruction-following made modern large language models (LLMs) helpful assistants. However, the key to taming LLMs on complex instructions remains mysterious, for that there are huge gaps between models trained by open-source community and those trained by leading companies. To bridge the gap, we propose a simple and scalable approach UltraIF for building LLMs that can follow complex instructions with open-source data. UltraIF first decomposes real-world user prompts into simpler queries, constraints, and corresponding evaluation questions for the constraints. Then, we train an UltraComposer to compose constraint-associated prompts with evaluation questions. This prompt composer allows us to synthesize complicated instructions as well as filter responses with evaluation questions. In our experiment, for the first time, we successfully align LLaMA-3.1-8B-Base to catch up with its instruct version on 5 instruction-following benchmarks without any benchmark information, using only 8B model as response generator and evaluator. The aligned model also achieved competitive scores on other benchmarks. Moreover, we also show that UltraIF could further improve LLaMA-3.1-8B-Instruct through self-alignment, motivating broader use cases for the method",
    "checked": true,
    "id": "ebfc796f637a9b8083055897c429556853cd0551",
    "semantic_title": "ultraif: advancing instruction following from the wild",
    "citation_count": 8,
    "authors": [
      "Kaikai An",
      "Li Sheng",
      "Ganqu Cui",
      "Shuzheng Si",
      "Ning Ding",
      "Yu Cheng",
      "Baobao Chang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.946": {
    "title": "Identifying Pre-training Data in LLMs: A Neuron Activation-Based Detection Framework",
    "volume": "main",
    "abstract": "The performance of large language models (LLMs) is closely tied to their training data, which can include copyrighted material or private information, raising legal and ethical concerns. Additionally, LLMs face criticism for dataset contamination and internalizing biases. To address these issues, the Pre-Training Data Detection (PDD) task was proposed to identify if specific data was included in an LLM's pre-training corpus. However, existing PDD methods often rely on superficial features like prediction confidence and loss, resulting in mediocre performance. To improve this, we introduce NA-PDD, a novel algorithm analyzing differential neuron activation patterns between training and non-training data in LLMs. This is based on the observation that these data types activate different neurons during LLM inference. We also introduce CCNewsPDD, a temporally unbiased benchmark employing rigorous data transformations to ensure consistent time distributions between training and non-training data. Our experiments demonstrate that NA-PDD significantly outperforms existing methods across three benchmarks and multiple LLMs",
    "checked": true,
    "id": "9cf9078ca90b4ad4b3c1d2c2fc92ef7e90810f2b",
    "semantic_title": "identifying pre-training data in llms: a neuron activation-based detection framework",
    "citation_count": 0,
    "authors": [
      "Hongyi Tang",
      "Zhihao Zhu",
      "Yi Yang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.947": {
    "title": "TreeRare: Syntax Tree-Guided Retrieval and Reasoning for Knowledge-Intensive Question Answering",
    "volume": "main",
    "abstract": "In real practice, questions are typically complex and knowledge-intensive, requiring Large Language Models (LLMs) to recognize the multifaceted nature of the question and reason across multiple information sources. Iterative and adaptive retrieval, where LLMs decide when and what to retrieve based on their reasoning, has been shown to be a promising approach to resolve complex, knowledge-intensive questions. However, the performance of such retrieval frameworks is limited by the accumulation of reasoning errors and misaligned retrieval results. To overcome these limitations, we propose TreeRare (Syntax Tree-Guided Retrieval and Reasoning, a framework that utilizes syntax trees to guide information retrieval and reasoning for question answering. Following the principle of compositionality, TreeRare traverses the syntax tree in a bottom-up fashion, and in each node, it generates subcomponent-based queries and retrieves relevant passages to resolve localized uncertainty. A subcomponent question answering module then synthesizes these passages into concise, context-aware evidence. Finally, TreeRare aggregates the evidence across the tree to form a final answer. Experiments across five question answering datasets involving ambiguous or multi-hop reasoning demonstrate that TreeRare achieves substantial improvements over existing state-of-the-art methods",
    "checked": true,
    "id": "f9e7a7463c2dea1c0db410d2405724e427ca2c74",
    "semantic_title": "treerare: syntax tree-guided retrieval and reasoning for knowledge-intensive question answering",
    "citation_count": 0,
    "authors": [
      "Boyi Zhang",
      "Zhuo Liu",
      "Hangfeng He"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.948": {
    "title": "Mapping Toxic Comments Across Demographics: A Dataset from German Public Broadcasting",
    "volume": "main",
    "abstract": "A lack of demographic context in existing toxic speech datasets limits our understanding of how different age groups communicate online. In collaboration with funk, a German public service content network, this research introduces the first large-scale German dataset annotated for toxicity and enriched with platform-provided age estimates. The dataset includes 3,024 human-annotated and 30,024 LLM-annotated anonymized comments from Instagram, TikTok, and YouTube. To ensure relevance, comments were consolidated using predefined toxic keywords, resulting in 16.7% labeled as problematic. The annotation pipeline combined human expertise with state-of-the-art language models, identifying key categories such as insults, disinformation, and criticism of broadcasting fees. The dataset reveals age-based differences in toxic speech patterns, with younger users favoring expressive language and older users more often engaging in disinformation and devaluation. This resource provides new opportunities for studying linguistic variation across demographics and supports the development of more equitable and age-aware content moderation systems",
    "checked": true,
    "id": "173d33f36248625fe300d500d2b90c9ba321a458",
    "semantic_title": "mapping toxic comments across demographics: a dataset from german public broadcasting",
    "citation_count": 0,
    "authors": [
      "Jan Fillies",
      "Michael Peter Hoffmann",
      "Rebecca Reichel",
      "Roman Salzwedel",
      "Sven Bodemer",
      "Adrian Paschke"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.949": {
    "title": "Small Models, Big Results: Achieving Superior Intent Extraction through Decomposition",
    "volume": "main",
    "abstract": "Understanding user intents from UI interaction trajectories remains a challenging, yet crucial, frontier in intelligent agent development. While massive, datacenter-based, multi-modal large language models (MLLMs) possess greater capacity to handle the complexities of such sequences, smaller models which can run on-device to provide a privacy-preserving, low-cost, and low-latency user experience, struggle with accurate intent inference. We address these limitations by introducing a novel decomposed approach: first, we perform structured interaction summarization, capturing key information from each user action. Second, we perform intent extraction using a fine-tuned model operating on the aggregated summaries. This method improves intent understanding in resource-constrained models, even surpassing the base performance of large MLLMs",
    "checked": true,
    "id": "7b8f09ee46825b5ab4857de0a90769daf465a61f",
    "semantic_title": "small models, big results: achieving superior intent extraction through decomposition",
    "citation_count": 0,
    "authors": [
      "Danielle Cohen",
      "Yoni Halpern",
      "Noam Kahlon",
      "Joel Oren",
      "Omri Berkovitch",
      "Sapir Caduri",
      "Ido Dagan",
      "Anatoly Efros"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.950": {
    "title": "On Pruning State-Space LLMs",
    "volume": "main",
    "abstract": "Recent work proposed state-space models (SSMs) as an efficient alternative to transformer-based LLMs. Can these models be pruned to further reduce their computation costs? We adapt several pruning methods to the SSM structure, and apply them to four SSM-based LLMs across multiple tasks. We find that such models are quite robust to some pruning methods (e.g., WANDA), while using other methods lead to fast performance degradation",
    "checked": true,
    "id": "8744333a8b6eb0ce5de563af2099b1858ff3419b",
    "semantic_title": "on pruning state-space llms",
    "citation_count": 4,
    "authors": [
      "Tamer Ghattas",
      "Michael Hassid",
      "Roy Schwartz"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.951": {
    "title": "An Orthogonal High-Rank Adaptation for Large Language Models",
    "volume": "main",
    "abstract": "Low-rank adaptation (LoRA) efficiently adapts LLMs to downstream tasks by decomposing LLMs' weight update into trainable low-rank matrices for fine-tuning. However, the random low-rank matrices may introduce massive task-irrelevant information, while their recomposed form suffer from limited representation spaces under low-rank operations. Such dense and choked adaptation in LoRA impairs the adaptation performance of LLMs on downstream tasks. To address these challenges, this paper proposes OHoRA, an orthogonal high-rank adaptation for parameter-efficient fine-tuning on LLMs. According to the information bottleneck theory, OHoRA decomposes LLMs' pre-trained weight matrices into orthogonal basis vectors via QR decomposition and splits them into two low-redundancy high-rank components to suppress task-irrelevant information. It then performs dynamic rank-elevated recomposition through Kronecker product to generate expansive task-tailored representation spaces, enabling precise LLM adaptation and enhanced generalization. OHoRA effectively operationalizes the information bottleneck theory to decompose LLMs' weight matrices into low-redundancy high-rank components and recompose them in rank-elevated manner for more task-tailored representation spaces and precise LLM adaptation. Empirical evaluation shows OHoRA's effectiveness by outperforming LoRA and its variants and achieving comparable performance to full fine-tuning with only 0.0371% trainable parameters",
    "checked": true,
    "id": "b0b89066c53e59bcb79b3f6b7dffc1a04999c318",
    "semantic_title": "an orthogonal high-rank adaptation for large language models",
    "citation_count": 0,
    "authors": [
      "Xin Zhang",
      "Guang-Ze Chen",
      "Shuzhen Li",
      "Zhulin Liu",
      "C.L.Philip Chen",
      "Tong Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.952": {
    "title": "BSFA: Leveraging the Subspace Dichotomy to Accelerate Neural Network Training",
    "volume": "main",
    "abstract": "Recent studies (CITATION) highlight a fundamental dichotomy in deep learning optimization: Although parameter updates along the top eigendirections of the loss Hessian (Dom-space) capture most of the update magnitude, they often contribute minimally to loss reduction. In contrast, updates in the orthogonal component (Bulk-space) have smaller magnitudes but drive most learning progress.In this work, we further advance the understanding of this phenomenon and introduce the Bulk-Space-Filtration-Accelerator (BSFA), a novel plug-and-play framework. BSFA accelerates training by differentially scaling update components projected onto these distinct subspaces, simultaneously enhancing stability by moderating updates in the dominant subspace and boosting convergence speed by amplifying those in the bulk-space.To ensure BSFA is both practical and scalable for contemporary large models, we introduce two key innovations: an efficient estimator using Principal Component Analysis (PCA) on historical updates for fast subspace estimation, and a block-wise strategy that applies this estimation on a per-parameter-block basis. These designs make BSFA computationally tractable and highly effective.We demonstrate BSFA's acceleration across various tasks, notably achieving approximately 2× speedup when pre-training LLaMA-72M on WikiText-103 and LLaMA-134M on OpenWebText compared to vanilla AdamW",
    "checked": true,
    "id": "34799d1e51ac0961b927a9459e8e9d5f1140bc38",
    "semantic_title": "bsfa: leveraging the subspace dichotomy to accelerate neural network training",
    "citation_count": 0,
    "authors": [
      "WenJie Zhou",
      "Bohan Wang",
      "Wei Chen",
      "Xueqi Cheng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.953": {
    "title": "Debatable Intelligence: Benchmarking LLM Judges via Debate Speech Evaluation",
    "volume": "main",
    "abstract": "We introduce Debate Speech Evaluation as a novel and challenging benchmark for assessing LLM judges. Evaluating debate speeches requires a deep understanding of the speech at multiple levels, including argument strength and relevance, the coherence and organization of the speech, the appropriateness of its style and tone, and so on. This task involves a unique set of cognitive abilities that previously received limited attention in systematic LLM benchmarking. To explore such skills, we leverage a dataset of over 600 meticulously annotated debate speeches and present the first in-depth analysis of how state-of-the-art LLMs compare to human judges on this task. Our findings reveal a nuanced picture: while larger models can approximate individual human judgments in some respects, they differ substantially in their overall judgment behavior. We also investigate the ability of frontier LLMs to generate persuasive, opinionated speeches, showing that models may perform at a human level on this task",
    "checked": true,
    "id": "aba395a88e9c60ceb3f1adedc0d295785783522c",
    "semantic_title": "debatable intelligence: benchmarking llm judges via debate speech evaluation",
    "citation_count": 0,
    "authors": [
      "Noy Sternlicht",
      "Ariel Gera",
      "Roy Bar-Haim",
      "Tom Hope",
      "Noam Slonim"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.954": {
    "title": "METok: Multi-Stage Event-based Token Compression for Efficient Long Video Understanding",
    "volume": "main",
    "abstract": "Recent advances in Video Large Language Models (VLLMs) have significantly enhanced their ability to understand video content. Nonetheless, processing long videos remains challenging due to high computational demands and the redundancy present in the visual data. In this work, we propose METok, a training-free, Multi-stage Event-based Token compression framework designed to accelerate VLLMs' inference while preserving accuracy. METok progressively eliminates redundant visual tokens across three critical stages: (1) event-aware compression during vision encoding, (2) hierarchical token pruning in the prefilling stage based on semantic alignment and event importance, and (3) a decoding-stage KV Cache optimization that further reduces memory consumption. Our experiments on diverse video benchmarks demonstrate that METok achieves an optimal trade-off between efficiency and accuracy by dynamically selecting informative visual tokens. For instance, equipping LongVA-7B with METok realizes an 80.6% FLOPs reduction and 93.5% KV Cache memory savings, all while maintaining comparable or even superior accuracy",
    "checked": true,
    "id": "c7a53a7ff08f5edf0fdae4056d84fee5b1c1f709",
    "semantic_title": "metok: multi-stage event-based token compression for efficient long video understanding",
    "citation_count": 1,
    "authors": [
      "Mengyue Wang",
      "Shuo Chen",
      "Kristian Kersting",
      "Volker Tresp",
      "Yunpu Ma"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.955": {
    "title": "VisiPruner: Decoding Discontinuous Cross-Modal Dynamics for Efficient Multimodal LLMs",
    "volume": "main",
    "abstract": "Multimodal Large Language Models (MLLMs) have achieved strong performance across vision-language tasks, but suffer from significant computational overhead due to the quadratic growth of attention computations with the number of multimodal tokens. Though efforts have been made to prune tokens in MLLMs, *they lack a fundamental understanding of how MLLMs process and fuse multimodal information*. Through systematic analysis, we uncover a three-stage cross-modal interaction process: (1) Shallow layers recognize task intent, with visual tokens acting as passive attention sinks; (2) Cross-modal fusion occurs abruptly in middle layers, driven by a few critical visual tokens; (3) Deep layers discard vision tokens, focusing solely on linguistic refinement. Based on these findings, we propose *VisiPruner*, a training-free pruning framework that reduces **99.9%** of vision-related attention computations and **62.8%** of FLOPs while maintaining performance. It significantly outperforms existing token pruning methods and generalizes across diverse MLLMs. Beyond pruning, our insights further provide actionable guidelines for training efficient MLLMs by aligning model architecture with its intrinsic layer-wise processing dynamics",
    "checked": true,
    "id": "7062031864d2db3a36cc599bfbce9f50651f8468",
    "semantic_title": "visipruner: decoding discontinuous cross-modal dynamics for efficient multimodal llms",
    "citation_count": 1,
    "authors": [
      "Yingqi Fan",
      "Anhao Zhao",
      "Jinlan Fu",
      "Junlong Tong",
      "Hui Su",
      "Yijie Pan",
      "Wei Zhang",
      "Xiaoyu Shen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.956": {
    "title": "Beyond Static Testbeds: An Interaction-Centric Agent Simulation Platform for Dynamic Recommender Systems",
    "volume": "main",
    "abstract": "Evaluating and iterating upon recommender systems is crucial, yet traditional A/B testing is resource-intensive, and offline methods struggle with dynamic user-platform interactions. While agent-based simulation is promising, existing platforms often lack a mechanism for user actions to dynamically reshape the environment. To bridge this gap, we introduce RecInter , a novel agent-based simulation platform for recommender systems featuring a robust interaction mechanism. In RecInter platform, simulated user actions (e.g., likes, reviews, purchases) dynamically update item attributes in real-time, and introduced Merchant Agents can reply, fostering a more realistic and evolving ecosystem. High-fidelity simulation is ensured through Multidimensional User Profiling module, Advanced Agent Architecture, and LLM fine-tuned on Chain-of-Thought (CoT) enriched interaction data. Our platform achieves significantly improved simulation credibility and successfully replicates emergent phenomena like Brand Loyalty and the Matthew Effect. Experiments demonstrate that this interaction mechanism is pivotal for simulating realistic system evolution, establishing our platform as a credible testbed for recommender systems research. All codes are released in https://github.com/jinsong8/RecInter",
    "checked": true,
    "id": "faa910fdeddfc0676c966a8e2d745fb6247ed116",
    "semantic_title": "beyond static testbeds: an interaction-centric agent simulation platform for dynamic recommender systems",
    "citation_count": 1,
    "authors": [
      "Song Jin",
      "Juntian Zhang",
      "Yuhan Liu",
      "Xun Zhang",
      "Yufei Zhang",
      "Guojun Yin",
      "Fei Jiang",
      "Wei Lin",
      "Rui Yan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.957": {
    "title": "SheetDesigner: MLLM-Powered Spreadsheet Layout Generation with Rule-Based and Vision-Based Reflection",
    "volume": "main",
    "abstract": "Spreadsheets are critical to data-centric tasks, with rich, structured layouts that enable efficient information transmission. Given the time and expertise required for manual spreadsheet layout design, there is an urgent need for automated solutions.However, existing automated layout models are ill-suited to spreadsheets, as they often (1) treat components as axis-aligned rectangles with continuous coordinates, overlooking the inherently discrete, grid-based structure of spreadsheets; and (2) neglect interrelated semantics, such as data dependencies and contextual links, unique to spreadsheets. In this paper, we first formalize the spreadsheet layout generation task, supported by a seven-criterion evaluation protocol and a dataset of 3,326 spreadsheets. We then introduce SheetDesigner, a zero-shot and training-free framework using Multimodal Large Language Models (MLLMs) that combines rule and vision reflection for component placement and content population. SheetDesigner outperforms five baselines by at least 22.6%. We further find that through vision modality, MLLMs handle overlap and balance well but struggle with alignment, necessitates hybrid rule and visual reflection strategies. Our codes and data is available at Github",
    "checked": true,
    "id": "1cca8e09c9286bdcbd13736c51184bdc709c5845",
    "semantic_title": "sheetdesigner: mllm-powered spreadsheet layout generation with rule-based and vision-based reflection",
    "citation_count": 0,
    "authors": [
      "Qin Chen",
      "Yuanyi Ren",
      "Xiaojun Ma",
      "Mugeng Liu",
      "Shi Han",
      "Dongmei Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.958": {
    "title": "CAIR: Counterfactual-based Agent Influence Ranker for Agentic AI Workflows",
    "volume": "main",
    "abstract": "An Agentic AI Workflow (AAW), also known as an LLM-based multi-agent system, is an autonomous system that assembles several LLM-based agents to work collaboratively towards a shared goal. The high autonomy, widespread adoption, and growing interest in such AAWs highlight the need for a deeper understanding of their operations, from both quality and security aspects. To this day, there are no existing methods to assess the influence of each agent on the AAW's final output. Adopting techniques from related fields is not feasible since existing methods perform only static structural analysis, which is unsuitable for inference time execution. We present Counterfactual-based Agent Influence Ranker (CAIR) - the first method for assessing the influence level of each agent on the AAW's output and determining which agents are the most influential. By performing counterfactual analysis, CAIR provides a task-agnostic analysis that can be used both offline and at inference time. We evaluate CAIR using an AAWs dataset of our creation, containing 30 different use cases with 230 different functionalities. Our evaluation showed that CAIR produces consistent rankings, outperforms baseline methods, and can easily enhance the effectiveness and relevancy of downstream tasks",
    "checked": true,
    "id": "3d8e3567e7c26a4927efbc1e47c5e053eabaa977",
    "semantic_title": "cair: counterfactual-based agent influence ranker for agentic ai workflows",
    "citation_count": 0,
    "authors": [
      "Amit Giloni",
      "Chiara Picardi",
      "Roy Betser",
      "Shamik Bose",
      "Aishvariya Priya Rathina Sabapathy",
      "Roman Vainshtein"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.959": {
    "title": "ReSURE: Regularizing Supervision Unreliability for Multi-turn Dialogue Fine-tuning",
    "volume": "main",
    "abstract": "Fine-tuning multi-turn dialogue systems requires high-quality supervision but often suffers from degraded performance when exposed to low-quality data. Supervision errors in early turns can propagate across subsequent turns, undermining coherence and response quality. Existing methods typically address data quality via static prefiltering, which decouples quality control from training and fails to mitigate turn-level error propagation. In this context, we propose **ReSURE** (REgularizing Supervision UnREliability), an adaptive learning method that dynamically down-weights unreliable supervision without explicit filtering. ReSURE estimates per-turn loss distributions using Welford's online statistics and reweights sample losses on the fly accordingly. Experiments on both single-source and mixed-quality datasets show improved stability and response quality. Notably, ReSURE enjoys positive Spearman correlations (0.21 ~ 1.0 across multiple benchmarks) between response scores and number of samples regardless of data quality, which potentially paves the way for utilizing large-scale data effectively",
    "checked": true,
    "id": "27855928029f30b91167f79c07ef533c59a480f3",
    "semantic_title": "resure: regularizing supervision unreliability for multi-turn dialogue fine-tuning",
    "citation_count": 1,
    "authors": [
      "Yiming Du",
      "Yifan Xiang",
      "Bin Liang",
      "Dahua Lin",
      "Kam-Fai Wong",
      "Fei Tan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.960": {
    "title": "Precise In-Parameter Concept Erasure in Large Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) often acquire knowledge during pretraining that is undesirable in downstream deployments, e.g., sensitive information or copyrighted content. Existing approaches for removing such knowledge rely on fine-tuning, training low-rank adapters or fact-level editing, but these are either too coarse, too shallow, or ineffective. In this work, we propose PISCES, a novel framework for precisely erasing entire concepts from model parameters by directly editing directions that encode them in parameter space. PISCES uses a disentangler model to decompose MLP vectors into interpretable features, identifies those associated with a target concept using automated interpretability techniques, and removes them from model parameters. Experiments on Gemma 2 and Llama 3.1 over various concepts show that PISCES achieves modest gains in efficacy over leading erasure methods, reducing accuracy on the target concept to as low as 7.7%, while dramatically improving erasure specificity (by up to 31%) and robustness (by up to 41%). Overall, these results demonstrate that feature-based in-parameter editing enables a more precise and reliable approach for removing conceptual knowledge in language models",
    "checked": true,
    "id": "68cf8d5284b88b9e8ef10916599bfdb00c3f238d",
    "semantic_title": "precise in-parameter concept erasure in large language models",
    "citation_count": 2,
    "authors": [
      "Yoav Gur-Arieh",
      "Clara Haya Suslik",
      "Yihuai Hong",
      "Fazl Barez",
      "Mor Geva"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.961": {
    "title": "PhonoThink: Improving Large Language Models' Reasoning on Chinese Phonological Ambiguities",
    "volume": "main",
    "abstract": "Effectively resolving phonological ambiguities is crucial for robust natural language processing, as these ambiguities are pervasive in tasks ranging from speech-to-text, spelling correction, to offensive language detection. However, current Large Language Models (LLMs) frequently struggle to resolve such ambiguities.To address this challenge, we present a framework to enhances LLMs' phonological capability through a multiple-stage training approach. Our method begins with supervised fine-tuning on well-constructed datasets, including three subtask datasets designed to enhance the model's foundational phonological knowledge, along with a synthetic dataset of step-by-step reasoning chains. Following this, we apply reinforcement learning to incentivize and stabilize its reasoning.Results show that our framework enables the base model to achieve relatively comparable performance to a much larger model. Our ablation studies reveal that subtask datasets and the synthetic dataset can simultaneously impact as complementary modular enhancers to strengthen LLMs' integrated application",
    "checked": true,
    "id": "3a117be97046775e6b21ffc19a33df8f98faab94",
    "semantic_title": "phonothink: improving large language models' reasoning on chinese phonological ambiguities",
    "citation_count": 0,
    "authors": [
      "Jianfei Ma",
      "Zhaoxin Feng",
      "Emmanuele Chersoni",
      "Huacheng Song",
      "Ziqi Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.962": {
    "title": "SAFE-SQL: Self-Augmented In-Context Learning with Fine-grained Example Selection for Text-to-SQL",
    "volume": "main",
    "abstract": "Text-to-SQL aims to convert natural language questions into executable SQL queries. While previous approaches, such as skeleton-masked selection, have demonstrated strong performance by retrieving similar training examples to guide large language models (LLMs), they struggle in real-world scenarios where such examples are unavailable. To overcome this limitation, we propose Fine-grained Self-Augmentation in-context learning for Text-to-SQL (SAFE-SQL), a novel framework that improves SQL generation by generating and filtering self-augmented examples. SAFE-SQL first prompts an LLM to generate multiple Text-to-SQL examples relevant to the test input. Then SAFE-SQL filters these examples through three relevance assessments, constructing high-quality in-context learning examples. Using self-generated examples, SAFE-SQL surpasses the previous zero-shot, and few-shot Text-to-SQL frameworks, achieving higher execution accuracy. Notably, our approach provides additional performance gains in extra hard and unseen scenarios, where conventional methods often fail",
    "checked": true,
    "id": "87c622d3f24ec5ac344c99c2a0318fe50ba09c98",
    "semantic_title": "safe-sql: self-augmented in-context learning with fine-grained example selection for text-to-sql",
    "citation_count": 3,
    "authors": [
      "Jimin Lee",
      "Ingeol Baek",
      "Byeongjeong Kim",
      "Hyunkyung Bae",
      "Hwanhee Lee"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.963": {
    "title": "ExpandR: Teaching Dense Retrievers Beyond Queries with LLM Guidance",
    "volume": "main",
    "abstract": "Large language models (LLMs) have demonstrated significant potential in enhancing dense retrieval through query augmentation. However, most existing methods treat the LLM and the retriever as separate modules, overlooking the alignment between generation and ranking objectives. In this work, we propose ExpandR, a unified LLM-augmented dense retrieval framework that jointly optimizes both the LLM and the retriever. ExpandR employs the LLM to generate semantically rich query expansions, which are leveraged to enhance the retriever's training. Simultaneously, the LLM is trained using Direct Preference Optimization (DPO), guided by a carefully designed reward function that balances retrieval effectiveness and generation consistency. This joint optimization paradigm enables mutual adaptation between the LLM and the retriever, resulting in query expansions that are both informative and well-suited for retrieval. Experimental results on multiple benchmarks show that ExpandR consistently outperforms strong baselines, achieving more than a 5% improvement in retrieval performance. All codes are available at https://github.com/NEUIR/ExpandR",
    "checked": true,
    "id": "6d85ceb896efd90abf218a0f6d0c88081ed32394",
    "semantic_title": "expandr: teaching dense retrievers beyond queries with llm guidance",
    "citation_count": 2,
    "authors": [
      "Sijia Yao",
      "Pengcheng Huang",
      "Zhenghao Liu",
      "Yu Gu",
      "Yukun Yan",
      "Shi Yu",
      "Ge Yu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.964": {
    "title": "Anecdoctoring: Automated Red-Teaming Across Language and Place",
    "volume": "main",
    "abstract": "Disinformation is among the top risks of generative artificial intelligence (AI) misuse. Global adoption of generative AI necessitates red-teaming evaluations (i.e., systematic adversarial probing) that are robust across diverse languages and cultures, but red-teaming datasets are commonly US- and English-centric. To address this gap, we propose \"anecdoctoring\", a novel red-teaming approach that automatically generates adversarial prompts across languages and cultures. We collect misinformation claims from fact-checking websites in three languages (English, Spanish, and Hindi) and two geographies (US and India). We then cluster individual claims into broader narratives and characterize the resulting clusters with knowledge graphs, with which we augment an attacker LLM. Our method produces higher attack success rates and offers interpretability benefits relative to few-shot prompting. Results underscore the need for disinformation mitigations that scale globally and are grounded in real-world adversarial misuse",
    "checked": true,
    "id": "90866eb832c3b2cc69081e6eea1734586b82609f",
    "semantic_title": "anecdoctoring: automated red-teaming across language and place",
    "citation_count": 1,
    "authors": [
      "Alejandro Cuevas",
      "Saloni Dash",
      "Bharat Kumar Nayak",
      "Dan Vann",
      "Madeleine I. G. Daepp"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.965": {
    "title": "ACING: Actor-Critic for Instruction Learning in Black-Box LLMs",
    "volume": "main",
    "abstract": "The effectiveness of Large Language Models (LLMs) in solving tasks depends significantly on the quality of their instructions, which often require substantial human effort to craft. This underscores the need for automated instruction optimization. However, optimizing instructions is particularly challenging when working with black-box LLMs, where model parameters and gradients are inaccessible. We introduce ACING, an actor-critic reinforcement learning framework that formulates instruction optimization as a stateless, continuous-action problem, enabling exploration of infinite instruction spaces using only black-box feedback. ACING automatically discovers prompts that outperform human-written prompts in 76% of instruction-induction tasks, with gains of up to 33 points and a 10-point median improvement over the best automatic baseline in 33 tasks spanning instruction-induction, summarization, and chain-of-thought reasoning. Extensive ablations highlight its robustness and efficiency. An implementation of ACING is available at https://github.com/salmakh1/ACING",
    "checked": true,
    "id": "8a6edeca3a104654657c66641c4a77d69b63f0a7",
    "semantic_title": "acing: actor-critic for instruction learning in black-box llms",
    "citation_count": 0,
    "authors": [
      "Salma Kharrat",
      "Fares Fourati",
      "Marco Canini"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.966": {
    "title": "Women, Infamous, and Exotic Beings: A Comparative Study of Honorific Usages in Wikipedia and LLMs for Bengali and Hindi",
    "volume": "main",
    "abstract": "The obligatory use of third-person honorifics is a distinctive feature of several South Asian languages, encoding nuanced socio-pragmatic cues such as power, age, gender, fame, and social distance.In this work, (i) We present the first large-scale study of third-person honorific pronoun and verb usage across 10,000 Hindi and Bengali Wikipedia articles with annotations linked to key socio-demographic attributes of the subjects, including gender, age group, fame, and cultural origin.(ii) Our analysis uncovers systematic intra-language regularities but notable cross-linguistic differences: honorifics are more prevalent in Bengali than in Hindi, while non-honorifics dominate while referring to infamous, juvenile, and culturally \"exotic\" entities. Notably, in both languages, and more prominently in Hindi, men are more frequently addressed with honorifics than women.(iii) To examine whether large language models (LLMs) internalize similar socio-pragmatic norms, we probe six LLMs using controlled generation and translation tasks over 1,000 culturally balanced entities. We find that LLMs diverge from Wikipedia usage, exhibiting alternative preferences in honorific selection across tasks, languages, and socio-demographic attributes. These discrepancies highlight gaps in the socio-cultural alignment of LLMs and open new directions for studying how LLMs acquire, adapt, or distort social-linguistic norms. Our code and data are publicly available at https://github.com/souro/honorific-wiki-llm",
    "checked": true,
    "id": "9152727f28f78d571f48e592161f1386cbba1826",
    "semantic_title": "women, infamous, and exotic beings: a comparative study of honorific usages in wikipedia and llms for bengali and hindi",
    "citation_count": 0,
    "authors": [
      "Sourabrata Mukherjee",
      "Atharva Mehta",
      "Sougata Saha",
      "Akhil Arora",
      "Monojit Choudhury"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.967": {
    "title": "Process-Supervised Reward Models for Verifying Clinical Note Generation: A Scalable Approach Guided by Domain Expertise",
    "volume": "main",
    "abstract": "Process-supervised reward models (PRMs) excel at providing step-by-step verification for large language model (LLM) outputs in domains like mathematics and coding. However, their application to fields lacking ground-truth answers, such as clinical note generation, poses significant challenges. We introduce a novel framework for training PRMs to deliver step-level reward signals for LLM-generated clinical notes. By precisely defining meaningful \"steps,\" injecting realistic \"errors\" informed by domain expertise, and leveraging LLMs to generate process supervision data at scale, we overcome previous limitations. Our PRM, built on LLaMA-3.1 8B, consistently outperforms proprietary reasoning and non-reasoning models, achieving state-of-the-art performance on two key evaluations: (1) distinguishing gold-standard from error-containing samples with 98.8% accuracy, and (2) selecting physician-preferred clinical notes with 56.2% accuracy. We investigate critical components for effective PRM training, including optimal loss functions and data selection strategies, and present a comprehensive physician reader study identifying predictors of downstream Best-of-N performance. Our study sheds light on unlocking the potential of PRMs for diverse generative tasks across domains",
    "checked": true,
    "id": "b7e7d274d22dc8f7dbd6d431e0c41c0e5a241fa3",
    "semantic_title": "process-supervised reward models for verifying clinical note generation: a scalable approach guided by domain expertise",
    "citation_count": 2,
    "authors": [
      "Hanyin Wang",
      "Chufan Gao",
      "Qiping Xu",
      "Bolun Liu",
      "Guleid Hussein",
      "Hariprasad Reddy Korsapati",
      "Mohamad El Labban",
      "Kingsley Iheasirim",
      "Mohamed Hassan",
      "Gokhan Anil",
      "Brian Bartlett",
      "Jimeng Sun"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.968": {
    "title": "GCML: Gradient Coherence Guided Meta-Learning for Cross-Domain Emerging Topic Rumor Detection",
    "volume": "main",
    "abstract": "With the emergence of new topics on social media as sources of rumor propagation, addressing the domain shift between the source and target domain and the target domain samples scarcity remains a crucial task in cross-domain rumor detection. Traditional deep learning-based methods and LLM-based methods are mostly focused on the in-domain condition, thus having poor performance in cross-domain setting. Existing domain adaptation rumor detection approaches ignore the data generalization differences and rely on a large amount of unlabeled target domain samples to achieve domain adaptation, resulting in less effective on emerging topic rumor detection. In this paper, we propose a Gradient Coherence guided Meta-Learning approach (GCML) for emerging topics rumor detection. Firstly, we calculate the task generalization score of each source task (sampled from source domain) from a gradient coherence perspective, and selectively learn more \"generalizable\" tasks that are more beneficial in adapting to the target domain. Secondly, we leverage meta-learning to alleviate the target domain samples scarcity, which utilizes task generalization scores to re-weight meta-test gradients and adaptively updates learning rate. Extensive experimental results on real-world datasets show that our method substantially outperforms SOTA baselines",
    "checked": true,
    "id": "d6d637137ae3d32cba4d127fe25ebf6ae6b971c4",
    "semantic_title": "gcml: gradient coherence guided meta-learning for cross-domain emerging topic rumor detection",
    "citation_count": 0,
    "authors": [
      "Zejiang He",
      "Jingyuan Huang",
      "Menglong Lu",
      "Zhen Huang",
      "Shanshan Liu",
      "Zhiliang Tian",
      "Dongsheng Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.969": {
    "title": "Can LLMs Generate and Solve Linguistic Olympiad Puzzles?",
    "volume": "main",
    "abstract": "In this paper, we introduce a combination of novel and exciting tasks: the solution and generation of linguistic puzzles. We focus on puzzles used in Linguistic Olympiads for high school students. We first extend the existing benchmark for the task of solving linguistic puzzles. We explore the use of Large Language Models (LLMs), including recent state-of-the-art models such as OpenAI's o1, for solving linguistic puzzles, analyzing their performance across various linguistic topics. We demonstrate that LLMs outperform humans on most puzzles types, except for those centered on writing systems, and for the understudied languages. We use the insights from puzzle-solving experiments to direct the novel task of puzzle generation. We believe that automating puzzle generation, even for relatively simple puzzles, holds promise for expanding interest in linguistics and introducing the field to a broader audience. This finding highlights the importance of linguistic puzzle generation as a research task: such puzzles can not only promote linguistics but also support the dissemination of knowledge about rare and understudied languages",
    "checked": true,
    "id": "8666d5b03df09c5f4c1da7447fa77ede56958841",
    "semantic_title": "can llms generate and solve linguistic olympiad puzzles?",
    "citation_count": 0,
    "authors": [
      "Neh Majmudar",
      "Elena Filatova"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.970": {
    "title": "E2LLM: Encoder Elongated Large Language Models for Long-Context Understanding and Reasoning",
    "volume": "main",
    "abstract": "Processing long contexts is increasingly important for Large Language Models (LLMs) in tasks like multi-turn dialogues, code generation, and document summarization. This paper addresses the challenges of achieving high long-context performance, low computational complexity, and compatibility with pretrained models – collectively termed the \"impossible triangle\". We introduce E2LLM (Encoder Elongated Large Language Models), a novel approach that effectively navigates this paradox. E2LLM divides long contexts into chunks, compresses each into soft prompts using a pretrained text encoder, and aligns these representations with a decoder-only LLM via an adapter. To enhance the LLM's reasoning with these soft prompts, we employ two training objectives: encoder output reconstruction and long-context instruction fine-tuning. Extensive experiments reveal that E2LLM not only outperforms 8 state-of-the-art (SOTA) methods in effectiveness and efficiency for document summarization and question answering, but also achieves the best performance on LongBench v2 among models of comparable size. The source code is available at https://github.com/codefuse-ai/E2LLM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihan Liao",
      "Jun Wang",
      "Hang Yu",
      "Lingxiao Wei",
      "Jianguo Li",
      "Jun Wang",
      "Wei Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.971": {
    "title": "DivScore: Zero-Shot Detection of LLM-Generated Text in Specialized Domains",
    "volume": "main",
    "abstract": "Detecting LLM-generated text in specialized and high-stakes domains like medicine and law is crucial for combating misinformation and ensuring authenticity. However, current zero-shot detectors, while effective on general text, often fail when applied to specialized content due to domain shift. We provide a theoretical analysis showing this failure is fundamentally linked to the KL divergence between human, detector, and source text distributions. To address this, we propose DivScore, a zero-shot detection framework using normalized entropy-based scoring and domain knowledge distillation to robustly identify LLM-generated text in specialized domains. Experiments on medical and legal datasets show that DivScore consistently outperforms state-of-the-art detectors, with 14.4% higher AUROC and 64.0% higher recall at 0.1% false positive rate threshold. In adversarial settings, DivScore demonstrates superior robustness to other baselines, achieving on average 22.8% advantage in AUROC and 29.5% in recall",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihui Chen",
      "Kai He",
      "Yucheng Huang",
      "Yunxiao Zhu",
      "Mengling Feng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.972": {
    "title": "Multi-Document Event Extraction Using Large and Small Language Models",
    "volume": "main",
    "abstract": "Multi-document event extraction aims to aggregate event information from diverse sources for a comprehensive understanding of complex events. Despite its practical significance, this task has received limited attention in existing research. The inherent challenges include handling complex reasoning over long contexts and intricate event structures. In this paper, we propose a novel collaborative framework that integrates large language models for multi-step reasoning and fine-tuned small language models to handle key subtasks, guiding the overall reasoning process. We introduce a new benchmark for multi-document event extraction and propose an evaluation metric designed for comprehensive assessment of multiple aggregated events. Experimental results demonstrate that our approach significantly outperforms existing methods, providing new insights into collaborative reasoning to tackle the complexities of multi-document event extraction",
    "checked": true,
    "id": "66cd33a40440ea1993ce00bdaf48c37a94321511",
    "semantic_title": "multi-document event extraction using large and small language models",
    "citation_count": 0,
    "authors": [
      "Qingkai Min",
      "Zitian Qu",
      "Qipeng Guo",
      "Xiangkun Hu",
      "Zheng Zhang",
      "Yue Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.973": {
    "title": "MA-GTS: A Multi-Agent Framework for Solving Complex Graph Problems in Real-World Applications",
    "volume": "main",
    "abstract": "Graph-theoretic problems arise in real-world applications like logistics, communication networks, and traffic optimization. These problems are often complex, noisy, and irregular, posing challenges for traditional algorithms. Large language models offer potential solutions but face several challenges, including limited accuracy, input length constraints, and suboptimal algorithm selection. To address these challenges, we propose MA-GTS(Multi-Agent Graph Theory Solver), a multi-agent framework that decomposes these complex problems through agent collaboration. MA-GTS maps the implicitly expressed text-based graph data into clear, structured graph representations and dynamically selects the most suitable algorithm based on problem constraints and graph structure scale. We validate MA-GTS using the G-REAL dataset, a real-world-inspired graph theory dataset we created. Experimental results show that MA-GTS outperforms state-of-the-art methods in cost-effectiveness, accuracy, and scalability, achieving strong results on multiple benchmarks (G-REAL 93.6%, GraCoRe 96.9% ,NLGraph 98.4%) with robust performance on both closed- and open-source models",
    "checked": true,
    "id": "3a0fffe62fb7d87ef953b73a4cfcaa36d79dc243",
    "semantic_title": "ma-gts: a multi-agent framework for solving complex graph problems in real-world applications",
    "citation_count": 2,
    "authors": [
      "Zike Yuan",
      "Ming Liu",
      "Hui Wang",
      "Bing Qin"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.974": {
    "title": "Enhancing Speech Large Language Models with Prompt-Aware Mixture of Audio Encoders",
    "volume": "main",
    "abstract": "Connecting audio encoders with large language models (LLMs) allows the LLM to perform various audio understanding tasks, such as automatic speech recognition (ASR) and audio captioning (AC). Most research focuses on training an adapter layer to generate a unified audio feature for the LLM. However, different tasks may require distinct features that emphasize either semantic or acoustic aspects, making task-specific audio features more desirable. In this paper, we propose Prompt-aware Mixture (PaM) to enhance the Speech LLM that uses multiple audio encoders. Our approach involves using different experts to extract different features based on the prompt that indicates different tasks. Experiments demonstrate that with PaM, only one Speech LLM surpasses the best performances achieved by all single-encoder Speech LLMs on ASR, speaker number verification, and AC tasks. PaM also outperforms other feature fusion baselines, such as concatenation and averaging",
    "checked": true,
    "id": "ac8a504a56ab92696e5f4f32845b6fccdba0e6cf",
    "semantic_title": "enhancing speech large language models with prompt-aware mixture of audio encoders",
    "citation_count": 2,
    "authors": [
      "Weiqiao Shan",
      "Yuang Li",
      "Yuhao Zhang",
      "Yingfeng Luo",
      "Chen Xu",
      "Xiaofeng Zhao",
      "Long Meng",
      "Yunfei Lu",
      "Min Zhang",
      "Hao Yang",
      "Tong Xiao",
      "JingBo Zhu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.975": {
    "title": "CIKT: A Collaborative and Iterative Knowledge Tracing Framework with Large Language Models",
    "volume": "main",
    "abstract": "Knowledge Tracing (KT) aims to model a student's learning state over time and predict their future performance. However, traditional KT methods often face challenges in explainability, scalability, and effective modeling of complex knowledge dependencies. While Large Language Models (LLMs) present new avenues for KT, their direct application often struggles with generating structured, explainable student representations and lacks mechanisms for continuous, task-specific refinement. To address these gaps, we propose Collaborative Iterative Knowledge Tracing (CIKT), a framework that harnesses LLMs to enhance both prediction accuracy and explainability. CIKT employs a dual-component architecture: an Analyst generates dynamic, explainable user profiles from student historical responses, and a Predictor utilizes these profiles to forecast future performance. The core of CIKT is a synergistic optimization loop. In this loop, the Analyst is iteratively refined based on the predictive accuracy of the Predictor, which conditions on the generated profiles, and the Predictor is subsequently retrained using these enhanced profiles. Evaluated on multiple educational datasets, CIKT demonstrates significant improvements in prediction accuracy, offers enhanced explainability through its dynamically updated user profiles, and exhibits improved scalability. Our work presents a robust and explainable solution for advancing knowledge tracing systems, effectively bridging the gap between predictive performance and model transparency",
    "checked": true,
    "id": "1afac959f51cb6c9674e2a77b2ae046a9571c5d6",
    "semantic_title": "cikt: a collaborative and iterative knowledge tracing framework with large language models",
    "citation_count": 1,
    "authors": [
      "Runze Li",
      "Siyu Wu",
      "Jun Wang",
      "Wei Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.976": {
    "title": "Mitigating Hallucinations in LM-Based TTS Models via Distribution Alignment Using GFlowNets",
    "volume": "main",
    "abstract": "Language Model (LM)-based Text-to-Speech (TTS) systems often generate hallucinated speech that deviates from input text. Existing mitigation strategies either demand excessive training resources or introduce significant inference latency. In this paper, we propose GFlOwNet-guided distribution AlignmenT (GOAT) for LM-based TTS, a post-training framework that mitigates hallucinations without relying on massive resources or inference cost. Specifically, we first conduct an uncertainty analysis, revealing a strong positive correlation between hallucination and model uncertainty. Based on this, we reformulate TTS generation as a trajectory flow optimization problem and introduce an enhanced Subtrajectory Balance objective together with a sharpened internal reward as target distribution. We further integrate reward temperature decay and learning rate optimization for stability and performance balance. Extensive experiments show that GOAT reduce over 50% character error rates on challenging test cases and lowering uncertainty by up to 58%, demonstrating its strong generalization ability and effectiveness",
    "checked": true,
    "id": "ac959d9039829796843c643a53991c9906dc26d4",
    "semantic_title": "mitigating hallucinations in lm-based tts models via distribution alignment using gflownets",
    "citation_count": 1,
    "authors": [
      "Chenlin Liu",
      "Minghui Fang",
      "Patrick Zhang",
      "Wei Zhou",
      "Jie Gao",
      "Jiqing Han"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.977": {
    "title": "MolErr2Fix: Benchmarking LLM Trustworthiness in Chemistry via Modular Error Detection, Localization, Explanation, and Correction",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have shown growing potential in molecular sciences, but they often produce chemically inaccurate descriptions and struggle to recognize or justify potential errors. This raises important concerns about their robustness and reliability in scientific applications. To support more rigorous evaluation of LLMs in chemical reasoning, we present the MolErr2Fix benchmark, designed to assess LLMs on error detection and correction in molecular descriptions. Unlike existing benchmarks focused on molecule-to-text generation or property prediction, MolErr2Fix emphasizes fine-grained chemical understanding. It tasks LLMs with identifying, localizing, explaining, and revising potential structural and semantic errors in molecular descriptions. Specifically, MolErr2Fix consists of 1,193 fine-grained annotated error instances. Each instance contains quadruple annotations, i.e., (error type, span location, the explanation, and the correction). These tasks are intended to reflect the types of reasoning and verification required in real-world chemical communication. Evaluations of current state-of-the-art LLMs reveal notable performance gaps, underscoring the need for more robust chemical reasoning capabilities. MolErr2Fix provides a focused benchmark for evaluating such capabilities and aims to support progress toward more reliable and chemically informed language models. All annotations and an accompanying evaluation API will be publicly released to facilitate future research",
    "checked": true,
    "id": "8b3e6686fdf24258e6495a85b92bbe804333b705",
    "semantic_title": "molerr2fix: benchmarking llm trustworthiness in chemistry via modular error detection, localization, explanation, and correction",
    "citation_count": 0,
    "authors": [
      "Yuyang Wu",
      "Jinhui Ye",
      "Shuhao Zhang",
      "Lu Dai",
      "Yonatan Bisk",
      "Olexandr Isayev"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.978": {
    "title": "Shared Path: Unraveling Memorization in Multilingual LLMs through Language Similarities",
    "volume": "main",
    "abstract": "We present the first comprehensive study of Memorization in Multilingual Large Language Models (MLLMs), analyzing 95 languages using models across diverse model scales, architectures, and memorization definitions. As MLLMs are increasingly deployed, understanding their memorization behavior has become critical. Yet prior work has focused primarily on monolingual models, leaving multilingual memorization underexplored, despite the inherently long-tailed nature of training corpora. We find that the prevailing assumption, that memorization is highly correlated with training data availability, fails to fully explain memorization patterns in MLLMs. We hypothesize that treating languages in isolation — ignoring their similarities — obscures the true patterns of memorization. To address this, we propose a novel graph-based correlation metric that incorporates language similarity to analyze cross-lingual memorization. Our analysis reveals that among similar languages, those with fewer training tokens tend to exhibit higher memorization, a trend that only emerges when cross-lingual relationships are explicitly modeled. These findings underscore the importance of a language-aware perspective in evaluating and mitigating memorization vulnerabilities in MLLMs. This also constitutes empirical evidence that language similarity both explains Memorization in MLLMs and underpins Cross-lingual Transferability, with broad implications for multilingual NLP",
    "checked": true,
    "id": "03cc9c8a39f1f80e921bea8dee319ba493a5a190",
    "semantic_title": "shared path: unraveling memorization in multilingual llms through language similarities",
    "citation_count": 1,
    "authors": [
      "Xiaoyu Luo",
      "Yiyi Chen",
      "Johannes Bjerva",
      "Qiongxiu Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.979": {
    "title": "Embedding Domain Knowledge for Large Language Models via Reinforcement Learning from Augmented Generation",
    "volume": "main",
    "abstract": "Large language models (LLMs) often exhibit limited performance on domain-specific tasks due to the natural disproportionate representation of specialized information in their training data and the static nature of these datasets. Knowledge scarcity and temporal lag create knowledge gaps for domain applications. While post-training on domain datasets can embed knowledge into models, existing approaches have some limitations. Continual Pre-Training (CPT) treats all tokens in domain documents with equal importance, failing to prioritize critical knowledge points, while supervised fine-tuning (SFT) with question-answer pairs struggles to develop the coherent knowledge structures necessary for complex reasoning tasks. To address these challenges, we propose Reinforcement Learning from Augmented Generation (RLAG). Our approach iteratively cycles between sampling generations and optimizing the model through calculated rewards, effectively embedding critical and contextually coherent domain knowledge. We select generated outputs with the highest log probabilities as the sampling result, then compute three tailored reward metrics to guide the optimization process. To comprehensively evaluate domain expertise, we assess answer accuracy and the rationality of explanations generated for correctly answered questions. Experimental results across medical, legal, astronomy, and current events datasets demonstrate that our proposed method significantly outperforms baseline approaches. Our code and data are open sourced at https://github.com/ChaojunNie/RLAG",
    "checked": true,
    "id": "5bfdc996e4c01e94924dfb8f79178dfbc10c2475",
    "semantic_title": "embedding domain knowledge for large language models via reinforcement learning from augmented generation",
    "citation_count": 0,
    "authors": [
      "Chaojun Nie",
      "Jun Zhou",
      "Guanxiang Wang",
      "Shisong Wu",
      "Zichen Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.980": {
    "title": "LLM-Driven Completeness and Consistency Evaluation for Cultural Heritage Data Augmentation in Cross-Modal Retrieval",
    "volume": "main",
    "abstract": "Cross-modal retrieval is essential for interpreting cultural heritage data, but its effectiveness is often limited by incomplete or inconsistent textual descriptions, caused by historical data loss and the high cost of expert annotation. While large language models (LLMs) offer a promising solution by enriching textual descriptions, their outputs frequently suffer from hallucinations or miss visually grounded details. To address these challenges, we propose C^3, a data augmentation framework that enhances cross-modal retrieval performance by improving the completeness and consistency of LLM-generated descriptions. C^3 introduces a bidirectional validation mechanism to assess semantic coverage using both visual cues and language-model outputs. Furthermore, to mitigate factual inconsistencies, we formulate a Markov Decision Process to supervise Chain-of-Thought reasoning, guiding consistency verification through adaptive query control. Experiments on the cultural heritage dataset CulTi and general benchmarks MSCOCO and Flickr30K demonstrate that C^3 achieves state-of-the-art performance in both fine-tuned and zero-shot settings",
    "checked": true,
    "id": "2fc291718f8f9fbafee7a0b6ed715a9bc018421e",
    "semantic_title": "llm-driven completeness and consistency evaluation for cultural heritage data augmentation in cross-modal retrieval",
    "citation_count": 0,
    "authors": [
      "Jian Zhang",
      "Junyi Guo",
      "Junyi Yuan",
      "Huanda Lu",
      "Yanlin Zhou",
      "Fangyu Wu",
      "Qiufeng Wang",
      "Dongming Lu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.981": {
    "title": "Artificial Impressions: Evaluating Large Language Model Behavior Through the Lens of Trait Impressions",
    "volume": "main",
    "abstract": "We introduce and study artificial impressions–patterns in LLMs' internal representations of prompts that resemble human impressions and stereotypes based on language. We fit linear probes on generated prompts to predict impressions according to the two-dimensional Stereotype Content Model (SCM). Using these probes, we study the relationship between impressions and downstream model behavior as well as prompt features that may inform such impressions. We find that LLMs inconsistently report impressions when prompted, but also that impressions are more consistently linearly decodable from their hidden representations. Additionally, we show that artificial impressions of prompts are predictive of the quality and use of hedging in model responses. We also investigate how particular content, stylistic, and dialectal features in prompts impact LLM impressions",
    "checked": true,
    "id": "4a5404318d26402d95b09421d8fe5ef4290e5a9e",
    "semantic_title": "artificial impressions: evaluating large language model behavior through the lens of trait impressions",
    "citation_count": 0,
    "authors": [
      "Nicholas Deas",
      "Kathleen McKeown"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.982": {
    "title": "Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization",
    "volume": "main",
    "abstract": "Large Visual Language Models (LVLMs) have demonstrated impressive capabilities across multiple tasks. However, their trustworthiness is often challenged by hallucinations, which can be attributed to the modality misalignment and the inherent hallucinations of their underlying Large Language Models (LLMs) backbone. Existing preference alignment methods focus on aligning model responses with human preferences while neglecting image-text modality alignment, resulting in over-reliance on LLMs and hallucinations. In this paper, we propose Entity-centric Multimodal Preference Optimization (EMPO), which achieves enhanced modality alignment than existing human preference alignment methods. Besides, to overcome the scarcity of high-quality multimodal preference data, we utilize open-source instruction datasets to automatically construct high-quality preference data across three aspects: image, instruction, and response. Experiments on two human preference datasets and five multimodal hallucination benchmarks demonstrate the effectiveness of EMPO, e.g., reducing hallucination rates by 80.4% on Object HalBench and 52.6% on MM HalBench, thereby enhancing the trustworthiness of LVLMs. The code and dataset will be made publicly available",
    "checked": true,
    "id": "6193797531e3d685eec1c4ee05fb317a9678301d",
    "semantic_title": "mitigating hallucinations in large vision-language models via entity-centric multimodal preference optimization",
    "citation_count": 0,
    "authors": [
      "Jiulong Wu",
      "Zhengliang Shi",
      "Shuaiqiang Wang",
      "Jizhou Huang",
      "Dawei Yin",
      "Lingyong Yan",
      "Min Cao",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.983": {
    "title": "3DS: Medical Domain Adaptation of LLMs via Decomposed Difficulty-based Data Selection",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) excel in general language tasks, motivating their adaptation to specialized domains such as healthcare. Effective domain adaptation typically involves supervised fine-tuning (SFT) on carefully selected instruction-tuning data. Current data selection methods adopt a data-centric approach, relying on external annotations and heuristics to identify externally defined high-quality or challenging data. Our exploratory experiments highlight this approach fails to improve the model's domain performance, due to misalignment between selected data and the model's knowledge distribution. To tackle this, we propose Decomposed Difficulty-based Data Selection (3DS), a two-stage model-centric data selection framework that aligns data selection with the model's distribution. 3DS employs Prompt-Driven Data Selection to filter out noise based on the model's knowledge via explicit alignment in Stage#1, then adopts Decomposed Difficulty-based Data Selection to guide selection via three novel data difficulty metrics, including Instruction Understanding, Response Confidence, and Response Correctness in Stage#2, enhanced by an attention-based importance weighting mechanism for accurate calibration.Extensive experiments in the healthcare domain show 3DS outperforms existing methods by up to 2.97% accuracy, with additional validation in law and general domains, confirming its generalization ability. Our dataset and code are open-sourced at https://github.com/PuppyKnightUniversity/3DS",
    "checked": true,
    "id": "2cf0b00130c5a6b1a97be56bd9a78e8300e27a08",
    "semantic_title": "3ds: medical domain adaptation of llms via decomposed difficulty-based data selection",
    "citation_count": 4,
    "authors": [
      "Hongxin Ding",
      "Yue Fang",
      "Runchuan Zhu",
      "Xinke Jiang",
      "Jinyang Zhang",
      "Yongxin Xu",
      "Weibin Liao",
      "Xu Chu",
      "Junfeng Zhao",
      "Yasha Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.984": {
    "title": "InfiniBench: A Benchmark for Large Multi-Modal Models in Long-Form Movies and TV Shows",
    "volume": "main",
    "abstract": "Understanding long-form videos, such as movies and TV episodes ranging from tens of minutes to two hours, remains a significant challenge for multi-modal models. Existing benchmarks often fail to test the full range of cognitive skills needed to process these temporally rich and narratively complex inputs. Therefore, we introduce InfiniBench, a comprehensive benchmark designed to evaluate the capabilities of models in long video understanding rigorously.InfiniBench offers:(1) Over 1,000 hours of video content, with an average video length of 53 minutes.(2) The largest set of question-answer pairs for long video comprehension, totaling around 87.7 K.(3) Eight diverse skills that span both grounding-based (e.g., scene transitions, character actions) and reasoning-based (e.g., deep context understanding, multi-event linking).(4) Rich annotation formats, including both multiple-choice and open-ended questions.We conducted an in-depth evaluation across both commercial (GPT-4o, Gemini 2.0 Flash) and most recent open-source vision-language models, such as Qwen2.5-VL, InternVL3.0). Results reveal that:(1) Models struggle across the board: Even the best model, GPT-4o, achieves only 47.1% on grounding-based skills, with most models performing near or just above random chance.(2) Strong reliance on world knowledge: Models achieve surprisingly high scores using only metadata (e.g., video titles), highlighting a tendency to rely on pre-trained knowledge rather than actual visual or temporal understanding.(3) Multi-Modal Importance: When provided with full video and subtitle context, however, models show substantial improvements, confirming the critical role of multimodal input in video understanding.Our findings underscore the inherent challenges in long-video comprehension and point to the need for substantial advancements in both grounding and reasoning capabilities in MLLMs",
    "checked": true,
    "id": "411a8f329a229d6c834a78d2609a903c9e884a42",
    "semantic_title": "infinibench: a benchmark for large multi-modal models in long-form movies and tv shows",
    "citation_count": 1,
    "authors": [
      "Kirolos Ataallah",
      "Eslam Mohamed Bakr",
      "Mahmoud Ahmed",
      "Chenhui Gou",
      "Khushbu Pahwa",
      "Jian Ding",
      "Mohamed Elhoseiny"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.985": {
    "title": "Intrinsic Test of Unlearning Using Parametric Knowledge Traces",
    "volume": "main",
    "abstract": "The task of \"unlearning\" certain concepts in large language models (LLMs) has gained attention for its role in mitigating harmful, private, or incorrect outputs. Current evaluations mostly rely on behavioral tests, without monitoring residual knowledge in model parameters, which can be adversarially exploited to recover erased information. We argue that unlearning should also be assessed internally by tracking changes in the parametric traces of unlearned concepts. To this end, we propose a general evaluation methodology that uses vocabulary projections to inspect concepts encoded in model parameters. We apply this approach to localize \"concept vectors\" — parameter vectors encoding concrete concepts — and construct ConceptVectors, a benchmark of hundreds of such concepts and their parametric traces in two open-source LLMs. Evaluation on ConceptVectors shows that existing methods minimally alter concept vectors, mostly suppressing them at inference time, while direct ablation of these vectors removes the associated knowledge and reduces adversarial susceptibility. Our findings reveal limitations of behavior-only evaluations and advocate for parameter-based assessments. We release our code and benchmark at https://github.com/yihuaihong/ConceptVectors",
    "checked": true,
    "id": "71af2709d4493bf8d592b7063af2626be43d5ee4",
    "semantic_title": "intrinsic test of unlearning using parametric knowledge traces",
    "citation_count": 21,
    "authors": [
      "Yihuai Hong",
      "Lei Yu",
      "Haiqin Yang",
      "Shauli Ravfogel",
      "Mor Geva"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.986": {
    "title": "Speculative Streaming: Efficient and Scalable Speculative Decoding with Multi-Stream Attention",
    "volume": "main",
    "abstract": "Speculative decoding is a prominent technique for accelerating LLM inference by leveraging an auxiliary draft model, but its effectiveness is limited by the autoregressive nature of draft generation, where acceptance rates depend on the draft model's size. Scaling the draft model improves acceptance but also increases speculation latency, limiting overall speedup. Furthermore, fine-tuning both the draft and target models is often necessary to achieve high acceptance rates, adding complexity to inference systems as the number of downstream tasks grows. Single-model approaches like Medusa generate speculative tokens non-autoregressively but lack token dependencies, limiting effectiveness. Alternatives like Hydra and Eagle incorporate token dependencies but rely on dedicated heads, making speculation independent of the base model and limiting the extent to which stronger base models can improve speculation.We introduce a novel speculative decoding method that integrates speculative draft generation directly within the target model using multi-stream attention. This improves acceptance rates by introducing interdependencies between speculative tokens while ensuring non-autoregressive draft generation with minimal overhead. As target models scale in size and quality, speculative generation improves naturally with our method, unlike prior approaches. Furthermore, our approach is both parameter- and FLOP-efficient, requiring over 1000X fewer additional parameters than Medusa, making it highly suitable for resource-constrained devices. We design our method to operate in two modes: (1) Lossless mode, a plug-and-play method that preserves the output of any pre-trained model; and (2) Shared mode, optimizing both speedup and downstream performance. We demonstrate a 2–3.5X speedup across diverse tasks, including summarization, translation, question answering, mathematical reasoning, SQL generation, and retrieval-augmented generation (RAG)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikhil Bhendawade",
      "Irina Belousova",
      "Qichen Fu",
      "Henry Mason",
      "Antonie Lin",
      "Mohammad Rastegari",
      "Mahyar Najibi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.987": {
    "title": "Evaluating Cognitive-Behavioral Fixation via Multimodal User Viewing Patterns on Social Media",
    "volume": "main",
    "abstract": "Digital social media platforms frequently contribute to cognitive-behavioral fixation, a phenomenon in which users exhibit sustained and repetitive engagement with narrow content domains. While cognitive-behavioral fixation has been extensively studied in psychology, methods for computationally detecting and evaluating such fixation remain underexplored. To address this gap, we propose a novel framework for assessing cognitive-behavioral fixation by analyzing users' multimodal social media engagement patterns. Specifically, we introduce a multimodal topic extraction module and a cognitive-behavioral fixation quantification module that collaboratively enable adaptive, hierarchical, and interpretable assessment of user behavior. Experiments on existing benchmarks and a newly curated multimodal dataset demonstrate the effectiveness of our approach, laying the groundwork for scalable computational analysis of cognitive fixation. All code in this project is publicly available for research purposes at https://github.com/Liskie/cognitive-fixation-evaluation",
    "checked": true,
    "id": "e8c03be7d94b0264149b124dc2eea10e8d4afbd1",
    "semantic_title": "evaluating cognitive-behavioral fixation via multimodal user viewing patterns on social media",
    "citation_count": 0,
    "authors": [
      "Yujie Wang",
      "Yunwei Zhao",
      "Jing Yang",
      "Han Han",
      "Shiguang Shan",
      "Jie Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.988": {
    "title": "Mind the Gap: A Closer Look at Tokenization for Multiple-Choice Question Answering with LLMs",
    "volume": "main",
    "abstract": "When evaluating large language models (LLMs) with multiple-choice question answering (MCQA), it is common to end the prompt with the string \"Answer:\" to facilitate automated answer extraction via next-token probabilities. However, there is no consensus on how to tokenize the space following the colon, often overlooked as a trivial choice. In this paper, we uncover accuracy differences of up to 11% due to this (seemingly irrelevant) tokenization variation as well as reshuffled model rankings, raising concerns about the reliability of LLM comparisons in prior work. Surprisingly, we are able to recommend one specific strategy – tokenizing the space together with the answer letter – as we observe consistent and statistically significant performance improvements. Additionally, it improves model calibration, enhancing the reliability of the model's confidence estimates. Our findings underscore the importance of careful evaluation design and highlight the need for standardized, transparent evaluation protocols to ensure reliable and comparable results",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mario Sanz-Guerrero",
      "Minh Duc Bui",
      "Katharina von der Wense"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.989": {
    "title": "VocalNet: Speech LLMs with Multi-Token Prediction for Faster and High-Quality Generation",
    "volume": "main",
    "abstract": "Speech large language models (LLMs) have emerged as a prominent research focus in speech processing. In this work, we introduce VocalNet, a series of high-performance speech LLMs featuring a scalable and model-agnostic training framework as well as a novel multi-token prediction (MTP) paradigm for speech generation. We first propose an efficient two-stage training framework that enables LLMs to acquire real-time speech interaction capabilities. Through extensive experiments on various training configurations, we ensure both simplicity and effectiveness in the training strategy. Furthermore, inspired by advances in language modeling, we introduce MTP into the domain of speech LLMs—an alternative to traditional next-token prediction (NTP)—which enables the model to predict multiple future tokens at each step. Through systematic analysis and improved implementation, we show that MTP not only accelerates inference speed but also significantly enhances speech quality. Experimental results demonstrate that VocalNet achieves performance comparable to state-of-the-art Omni LLMs while outperforming existing open-source speech LLMs, despite using limited training data",
    "checked": true,
    "id": "68d60dcd353e69079f3068bb0863fcbea2bcf093",
    "semantic_title": "vocalnet: speech llms with multi-token prediction for faster and high-quality generation",
    "citation_count": 0,
    "authors": [
      "Yuhao Wang",
      "Heyang Liu",
      "Ziyang Cheng",
      "Ronghua Wu",
      "Qunshan Gu",
      "Yanfeng Wang",
      "Yu Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.990": {
    "title": "Path Drift in Large Reasoning Models: How First-Person Commitments Override Safety",
    "volume": "main",
    "abstract": "As large language models (LLMs) are increasingly deployed for complex reasoning tasks, Long Chain-of-Thought (Long-CoT) prompting has emerged as a key paradigm for structured inference. Despite early-stage safeguards enabled by alignment techniques such as RLHF, we identify a previously underexplored vulnerability: reasoning trajectories in Long-CoT models can drift from aligned paths, resulting in content that violates safety constraints. We term this phenomenon Path Drift. Through empirical analysis, we uncover three behavioral triggers of Path Drift: (1) first-person commitments that induce goal-driven reasoning that delays refusal signals; (2) ethical evaporation, where surface-level disclaimers bypass alignment checkpoints; (3) condition chain escalation, where layered cues progressively steer models toward unsafe completions. Building on these insights, we introduce a three-stage Path Drift Induction Framework comprising cognitive load amplification, self-role priming, and condition chain hijacking. Each stage independently reduces refusal rates, while their combination further compounds the effect. To mitigate these risks, we propose a path-level defense strategy incorporating role attribution correction and metacognitive reflection (reflective safety cues). Our findings highlight the need for trajectory-level alignment oversight in long-form reasoning beyond token-level alignment",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuyi Huang",
      "Runzhe Zhan",
      "Lidia S. Chao",
      "Ailin Tao",
      "Derek F. Wong"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.991": {
    "title": "CBP-Tuning: Efficient Local Customization for Black-box Large Language Models",
    "volume": "main",
    "abstract": "The high costs of customizing large language models (LLMs) fundamentally limit their adaptability to user-specific needs. Consequently, LLMs are increasingly offered as cloud-based services, a paradigm that introduces critical limitations: providers struggle to support personalized customization at scale, while users face privacy risks when exposing sensitive data. To address this dual challenge, we propose Customized Black-box Prompt Tuning (CBP-Tuning), a novel framework that facilitates efficient local customization while preserving bidirectional privacy. Specifically, we design a two-stage framework: (1) a prompt generator trained on the server-side to capture domain-specific and task-agnostic capabilities, and (2) user-side gradient-free optimization that tailors soft prompts for individual tasks. This approach eliminates the need for users to access model weights or upload private data, requiring only a single customized vector per task while achieving effective adaptation. Furthermore, the evaluation of CBP-Tuning in the commonsense reasoning, medical and financial domain settings demonstrates superior performance compared to baselines, showcasing its advantages in task-agnostic processing and privacy preservation",
    "checked": true,
    "id": "889275a8df9e2a63620ab7cc3b3ec04e8b039ed3",
    "semantic_title": "cbp-tuning: efficient local customization for black-box large language models",
    "citation_count": 0,
    "authors": [
      "Jiaxuan Zhao",
      "Naibin Gu",
      "Yuchen Feng",
      "Xiyu Liu",
      "Peng Fu",
      "Zheng Lin",
      "Weiping Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.992": {
    "title": "Beyond the Score: Uncertainty-Calibrated LLMs for Automated Essay Assessment",
    "volume": "main",
    "abstract": "Automated Essay Scoring (AES) systems now attain near–human agreement on some public benchmarks, yet real-world adoption—especially in high-stakes examinations—remains limited. A principal obstacle is that most models output a single score without any accompanying measure of confidence or explanation. We address this gap with conformal prediction, a distribution-free wrapper that equips any classifier with set-valued outputs enjoying formal coverage guarantees. Two open-source Large Language Models—Llama-3 8B and Qwen-2.5 3B—are fine-tuned on three diverse corpora (ASAP, TOEFL11, Cambridge-FCE) and calibrated at a 90% risk level. Reliability is assessed with UAcc, an uncertainty-aware accuracy that rewards models for being both correct and concise. To our knowledge, this is the first work to combine conformal prediction and UAcc for essay scoring. The calibrated models consistently meet the coverage target while keeping prediction sets compact, indicating that open-source, mid-sized LLMs can already support teacher-in-the-loop AES; we discuss scaling and broader user studies as future work",
    "checked": true,
    "id": "f2cacde2fdcd107b879fa78e3bf0503231301cf4",
    "semantic_title": "beyond the score: uncertainty-calibrated llms for automated essay assessment",
    "citation_count": 0,
    "authors": [
      "Ahmed Karim",
      "Qiao Wang",
      "Zheng Yuan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.993": {
    "title": "Humans Hallucinate Too: Language Models Identify and Correct Subjective Annotation Errors With Label-in-a-Haystack Prompts",
    "volume": "main",
    "abstract": "Modeling complex subjective tasks in Natural Language Processing, such as recognizing emotion and morality, is considerably challenging due to significant variation in human annotations. This variation often reflects reasonable differences in semantic interpretations rather than mere noise, necessitating methods to distinguish between legitimate subjectivity and error.We address this challenge by exploring label verification in these contexts using Large Language Models (LLMs). First, we propose a simple In-Context Learning binary filtering baseline that estimates the reasonableness of a document-label pair. We then introduce the Label-in-a-Haystack setting: the query and its label(s) are included in the demonstrations shown to LLMs, which are prompted to predict the label(s) again, while receiving task-specific instructions (e.g., emotion recognition) rather than label copying.We show how the failure to copy the label(s) to the output of the LLM are task-relevant and informative. Building on this, we propose the Label-in-a-Haystack Rectification (LiaHR) framework for subjective label correction: when the model outputs diverge from the reference gold labels, we assign the generated labels to the example instead of discarding it. This approach can be integrated into annotation pipelines to enhance signal-to-noise ratios. Comprehensive analyses, human evaluations, and ecological validity studies verify the utility of LiaHR for label correction. Code is available at https://github.com/gchochla/liahr",
    "checked": true,
    "id": "bd5baab37c0a752266f2cd7bd77a37187ea4daa9",
    "semantic_title": "humans hallucinate too: language models identify and correct subjective annotation errors with label-in-a-haystack prompts",
    "citation_count": 0,
    "authors": [
      "Georgios Chochlakis",
      "Peter Wu",
      "Tikka Arjun Singh Bedi",
      "Marcus Ma",
      "Kristina Lerman",
      "Shrikanth Narayanan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.994": {
    "title": "Do It Yourself (DIY): Modifying Images for Poems in a Zero-Shot Setting Using Weighted Prompt Manipulation",
    "volume": "main",
    "abstract": "Poetry is an expressive form of art that invites multiple interpretations, as readers often bring their own emotions, experiences, and cultural backgrounds into their understanding of a poem. Recognizing this, we aim to generate images for poems and improve these images in a zero-shot setting, enabling audiences to modify images as per their requirements. To achieve this, we introduce a novel Weighted Prompt Manipulation (WPM) technique, which systematically modifies attention weights and text embeddings within diffusion models. By dynamically adjusting the importance of specific words, WPM enhances or suppresses their influence in the final generated image, leading to semantically richer and more contextually accurate visualizations. Our approach exploits diffusion models and large language models (LLMs) such as GPT in conjunction with existing poetry datasets, ensuring a comprehensive and structured methodology for improved image generation in the literary domain. To the best of our knowledge, this is the first attempt at integrating weighted prompt manipulation for enhancing imagery in poetic language",
    "checked": true,
    "id": "33591373d7f21a107fea8f9260e2ade5df9f08e7",
    "semantic_title": "do it yourself (diy): modifying images for poems in a zero-shot setting using weighted prompt manipulation",
    "citation_count": 0,
    "authors": [
      "Sofia Jamil",
      "Kotla Sai Charan",
      "Sriparna Saha",
      "Koustava Goswami",
      "Joseph K J"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.995": {
    "title": "Looking Beyond Text: Reducing Language Bias in Large Vision-Language Models via Multimodal Dual-Attention and Soft-Image Guidance",
    "volume": "main",
    "abstract": "Large vision-language models (LVLMs) have achieved impressive results in vision-language tasks. However, Therefore, we propose LACING, designed to address such bias with Mu ̲Ltimodal Du ̲Al-attention Me ̲Chan ̲Ism (MDA) a ̲Nd Soft-Image ̲Guidance (SIG). Specifically, MDA adopts a parallel dual-attention mechanism that constructs separate attention for visual and text inputs to enhance integration of visual inputs across model. SIG uses a learnable soft visual prompt during training and inference to replace visual inputs, designed to compel LVLMs to prioritize text inputs during inference. Experiments across different model architectures and scales demonstrate that LACING effectively debiases LVLMs from their language bias, enhancing visual comprehension and reducing hallucinations without additional resources",
    "checked": true,
    "id": "f1940f9b856e2167c700da8b6d58ed8b75a35b41",
    "semantic_title": "looking beyond text: reducing language bias in large vision-language models via multimodal dual-attention and soft-image guidance",
    "citation_count": 0,
    "authors": [
      "Haozhe Zhao",
      "Shuzheng Si",
      "Liang Chen",
      "Yichi Zhang",
      "Maosong Sun",
      "Baobao Chang",
      "Minjia Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.996": {
    "title": "Who Holds the Pen? Caricature and Perspective in LLM Retellings of History",
    "volume": "main",
    "abstract": "Large language models (LLMs) are no longer just language generators—they are increasingly used to simulate human behavior, perspectives, and demographic variation across social domains, from public opinion surveys to experimental research. Amid this shift, the use of LLMs to simulate historical narratives has emerged as a timely frontier. It is crucial to scrutinize the asymmetries these models embed when framing, interpreting, and retelling the past. Building on prior work that defines caricature as the combination of individuation and exaggeration, we analyze LLM-generated responses across 197 historically significant events—each featuring a directly and an indirectly affected persona. We find that LLMs reliably distinguish persona-based responses from neutral baselines, and that directly affected personas consistently exhibit higher exaggeration—amplifying identity-specific portrayals. Beyond lexical patterns, personas often frame the same event in conflicting ways—especially in military, political, and morally charged contexts. Grammatical analysis further reveals that direct personas adopt more passive constructions in institutional contexts, but shift to active framing when emotional immediacy is foregrounded. Our findings show how subtle asymmetries in tone, stance, and emphasis—not overt toxicity—can quietly, yet systematically, distort how history is told and remembered",
    "checked": true,
    "id": "4781175c562ab9201bc3878f201b90cb710f87d3",
    "semantic_title": "who holds the pen? caricature and perspective in llm retellings of history",
    "citation_count": 0,
    "authors": [
      "Lubna Zahan Lamia",
      "Mabsur Fatin Bin Hossain",
      "Md Mosaddek Khan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.997": {
    "title": "DSMoE: Matrix-Partitioned Experts with Dynamic Routing for Computation-Efficient Dense LLMs",
    "volume": "main",
    "abstract": "As large language models continue to scale, computational costs and resource consumption have emerged as significant challenges. While existing sparsification methods like pruning reduce computational overhead, they risk losing model knowledge through parameter removal. This paper proposes DSMoE (Dynamic Sparse Mixture-of-Experts), a novel approach that achieves sparsification by partitioning pre-trained FFN layers into computational blocks. We implement adaptive expert routing using sigmoid activation and straight-through estimators, enabling tokens to flexibly access different aspects of model knowledge based on input complexity. Additionally, we introduce a sparsity loss term to balance performance and computational efficiency. Extensive experiments on LLaMA models demonstrate that under equivalent computational constraints, DSMoE achieves superior performance compared to existing pruning and MoE approaches across language modeling and downstream tasks, particularly excelling in generation tasks. Analysis reveals that DSMoE learns distinctive layerwise activation patterns, providing new insights for future MoE architecture design",
    "checked": true,
    "id": "500b4c76d991e619c71a02a7ce3d946b64d02c7b",
    "semantic_title": "dsmoe: matrix-partitioned experts with dynamic routing for computation-efficient dense llms",
    "citation_count": 1,
    "authors": [
      "Minxuan Lv",
      "Zhenpeng Su",
      "Leiyu Pan",
      "Yizhe Xiong",
      "Zijia Lin",
      "Hui Chen",
      "Wei Zhou",
      "Jungong Han",
      "Guiguang Ding",
      "Wenwu Ou",
      "Di Zhang",
      "Kun Gai",
      "Songlin Hu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.998": {
    "title": "Search Wisely: Mitigating Sub-optimal Agentic Searches By Reducing Uncertainty",
    "volume": "main",
    "abstract": "Agentic Retrieval-Augmented Generation (RAG) systems enhance Large Language Models (LLMs) by enabling dynamic, multi-step reasoning and information retrieval. However, these systems often exhibit sub-optimal search behaviors like over-search (retrieving redundant information) and under-search (failing to retrieve necessary information), which hinder efficiency and reliability. This work formally defines and quantifies these behaviors, revealing their prevalence across multiple QA datasets and agentic RAG systems (e.g., one model could have avoided searching in 27.7% of its search steps). Furthermore, we demonstrate a crucial link between these inefficiencies and the models' uncertainty regarding their own knowledge boundaries, where response accuracy correlates with model's uncertainty in its search decisions. To address this, we propose β-GRPO, a reinforcement learning-based training method that incorporates confidence threshold to reward high-certainty search decisions. Experiments on seven QA benchmarks show that β-GRPO enable a 3B model with better agentic RAG ability, outperforming other strong baselines with a 4% higher average exact match score",
    "checked": true,
    "id": "89a5223e14896e7404d8c2fbd246f989ab3bcaac",
    "semantic_title": "search wisely: mitigating sub-optimal agentic searches by reducing uncertainty",
    "citation_count": 5,
    "authors": [
      "Peilin Wu",
      "Mian Zhang",
      "Xinlu Zhang",
      "Xinya Du",
      "Zhiyu Chen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.999": {
    "title": "Child-Directed Language Does Not Consistently Boost Syntax Learning in Language Models",
    "volume": "main",
    "abstract": "Seminal work by Huebner et al. (2021) showed that language models (LMs) trained on English Child-Directed Language (CDL) can outperform LMs trained on an equal amount of adult-directed text like Wikipedia. However, it remains unclear whether these results generalize across languages, architectures, and evaluation settings. We test this by comparing models trained on CDL vs. Wikipedia across two LM objectives (masked and causal), three languages (English, French, German), and three syntactic minimal pair benchmarks. Our results on these benchmarks show inconsistent benefits of CDL, which in most cases is outperformed by Wikipedia models. We then identify various shortcomings in these benchmarks, and introduce a novel testing methodology, FIT-CLAMS, which uses a frequency-controlled design to enable balanced comparisons across training corpora. Through minimal pair evaluations and regression analysis we show that training on CDL does not yield stronger generalizations for acquiring syntax and highlight the importance of controlling for frequency effects when evaluating syntactic ability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesca Padovani",
      "Jaap Jumelet",
      "Yevgen Matusevych",
      "Arianna Bisazza"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1000": {
    "title": "Benchmarking Debiasing Methods for LLM-based Parameter Estimates",
    "volume": "main",
    "abstract": "Large language models (LLMs) offer an inexpensive yet powerful way to annotate text, but are often inconsistent when compared with experts. These errors can bias downstream estimates of population parameters such as regression coefficients and causal effects. To mitigate this bias, researchers have developed debiasing methods such as Design-based Supervised Learning (DSL) and Prediction-Powered Inference (PPI), which promise valid estimation by combining LLM annotations with a limited number of expensive expert annotations.Although these methods produce consistent estimates under theoretical assumptions, it is unknown how they compare in finite samples of sizes encountered in applied research. We make two contributions: First, we study how each method's performance scales with the number of expert annotations, highlighting regimes where LLM bias or limited expert labels significantly affect results. Second, we compare DSL and PPI across a range of tasks, finding that although both achieve low bias with large datasets, DSL often outperforms PPI on bias reduction and empirical efficiency, but its performance is less consistent across datasets. Our findings indicate that there is a bias-variance tradeoff at the level of debiasing methods, calling for more research on developing metrics for quantifying their efficiency in finite samples",
    "checked": true,
    "id": "c69080417c3950a881565a127b641102f5c943c5",
    "semantic_title": "benchmarking debiasing methods for llm-based parameter estimates",
    "citation_count": 3,
    "authors": [
      "Nicolas Audinet de Pieuchon",
      "Adel Daoud",
      "Connor Thomas Jerzak",
      "Moa Johansson",
      "Richard Johansson"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1001": {
    "title": "(Almost) Free Modality Stitching of Foundation Models",
    "volume": "main",
    "abstract": "Foundation multi-modal models are often designed by stitching of multiple existing pretrained uni-modal models: for example, an image classifier with a text model. This stitching process is performed by training a connector module that aims to align the representation spaces of these uni-modal models towards a multi-modal objective. However, given the complexity of training such connectors on large scale web-based datasets coupled with the ever-increasing number of available pretrained uni-modal models, the task of uni-modal models selection and subsequent connector module training becomes computationally demanding. To address this under-studied critical problem, we propose Hypernetwork Model Alignment (Hyma), a novel all-in-one solution for optimal uni-modal model selection and connector training by leveraging hypernetworks. Specifically, our framework utilizes the parameter prediction capability of a hypernetwork to obtain jointly trained connector modules for N × M combinations of uni-modal models. In our experiments, Hyma reduces the cost of searching for the best performing uni-modal model pair by 10×, while matching the ranking and trained connector performance obtained via grid search across a suite of diverse multi-modal benchmarks",
    "checked": true,
    "id": "99b759feb3d6d6db4829024614280c2a2a9c8006",
    "semantic_title": "(almost) free modality stitching of foundation models",
    "citation_count": 0,
    "authors": [
      "Jaisidh Singh",
      "Diganta Misra",
      "Boris Knyazev",
      "Antonio Orvieto"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1002": {
    "title": "VERITAS: Leveraging Vision Priors and Expert Fusion to Improve Multimodal Data",
    "volume": "main",
    "abstract": "The quality of supervised fine-tuning (SFT) data is crucial for the performance of large multimodal models (LMMs), yet current data enhancement methods often suffer from factual errors and hallucinations due to inadequate visual perception. To address this challenge, we propose VERITAS, a pipeline that systematically integrates vision priors and multiple state-of-the-art LMMs with statistical methods to enhance SFT data quality. VERITAS leverages visual recognition models (RAM++) and OCR systems (PP-OCRv4) to extract structured vision priors, which are combined with images, questions, and answers. Three LMMs (GPT-4o, Gemini-2.5-Pro, Doubao-1.5-pro) evaluate the original answers, providing critique rationales and scores that are statistically fused into a high-confidence consensus score serving as ground truth. Using this consensus, we train a lightweight critic model via Group Relative Policy Optimization (GRPO), enhancing reasoning capabilities efficiently. Each LMM then refines the original answers based on the critiques, generating new candidate answers; we select the highest-scoring one as the final refined answer. Experiments across six multimodal benchmarks demonstrate that models fine-tuned with data processed by VERITAS consistently outperform those using raw data, particularly in text-rich and fine-grained reasoning tasks. Our critic model exhibits enhanced capability comparable to state-of-the-art LMMs while being significantly more efficient. We release our pipeline, datasets, and model checkpoints to advance research in multimodal data optimization",
    "checked": true,
    "id": "52d81598bb6f6959b2101250327c74f98b09870e",
    "semantic_title": "veritas: leveraging vision priors and expert fusion to improve multimodal data",
    "citation_count": 0,
    "authors": [
      "Tingqiao Xu",
      "Ziru Zeng",
      "Jiayu Chen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1003": {
    "title": "Rescorla-Wagner Steering of LLMs for Undesired Behaviors over Disproportionate Inappropriate Context",
    "volume": "main",
    "abstract": "Incorporating external context can significantly enhance the response quality of Large Language Models (LLMs). However, real-world contexts often mix relevant information with disproportionate inappropriate content, posing reliability risks. How do LLMs process and prioritize mixed context? To study this, we introduce the Poisoned Context Testbed, pairing queries with real-world contexts containing relevant and inappropriate content. Inspired by associative learning in animals, we adapt the Rescorla-Wagner (RW) model from neuroscience to quantify how competing contextual signals influence LLM outputs. Our adapted model reveals a consistent behavioral pattern: LLMs exhibit a strong tendency to incorporate information that is less prevalent in the context. This susceptibility is harmful in real-world settings, where small amounts of inappropriate content can substantially degrade response quality. Empirical evaluations on our testbed further confirm this vulnerability. To tackle this, we introduce RW-Steering, a two-stage finetuning-based approach that enables the model to internally identify and ignore inappropriate signals. Unlike prior methods that rely on extensive supervision across diverse context mixtures, RW-Steering generalizes robustly across varying proportions of inappropriate content. Experiments show that our best fine-tuned model improves response quality by 39.8% and reverses the undesirable behavior curve, establishing RW-Steering as a robust, generalizable solution for improving LLM safety in real-world use",
    "checked": true,
    "id": "710e2e44c10fd8636f8350fd521101fd8de76804",
    "semantic_title": "rescorla-wagner steering of llms for undesired behaviors over disproportionate inappropriate context",
    "citation_count": 0,
    "authors": [
      "Rushi Wang",
      "Jiateng Liu",
      "Cheng Qian",
      "Yifan Shen",
      "Yanzhou Pan",
      "Zhaozhuo Xu",
      "Ahmed Abbasi",
      "Heng Ji",
      "Denghui Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1004": {
    "title": "Exploring Artificial Image Generation for Stance Detection",
    "volume": "main",
    "abstract": "Stance detection is a task aimed at identifying and analyzing the author's stance from text. Previous studies have primarily focused on the text, which may not fully capture the implicit stance conveyed by the author. To address this limitation, we propose a novel approach that transforms original texts into artificially generated images and uses the visual representation to enhance stance detection. Our approach first employs a text-to-image model to generate candidate images for each text. These images are carefully crafted to adhere to three specific criteria: textual relevance, target consistency, and stance consistency. Next, we introduce a comprehensive evaluation framework to select the optimal image for each text from its generated candidates. Subsequently, we introduce a multimodal stance detection model that leverages both the original textual content and the generated image to identify the author's stance. Experiments demonstrate the effectiveness of our approach and highlight the importance of artificially generated images for stance detection",
    "checked": true,
    "id": "f102ad0cece56a5629999a668a7c002c58ac4d7c",
    "semantic_title": "exploring artificial image generation for stance detection",
    "citation_count": 0,
    "authors": [
      "Zhengkang Zhang",
      "Zhongqing Wang",
      "Guodong Zhou"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1005": {
    "title": "Hope vs. Hate: Understanding User Interactions with LGBTQ+ News Content in Mainstream US News Media through the Lens of Hope Speech",
    "volume": "main",
    "abstract": "This paper makes three contributions. First, via a substantial corpus of 1,419,047 comments posted on 3,161 YouTube news videos of major US cable news outlets, we analyze how users engage with LGBTQ+ news content. Our analyses focus both on positive and negative content. In particular, we construct a hope speech classifier that detects positive (hope speech), negative, neutral, and irrelevant content. Second, in consultation with a public health expert specializing on LGBTQ+ health, we conduct an annotation study with a balanced and diverse political representation and release a dataset of 3,750 instances with crowd-sourced labels and detailed annotator demographic information. Finally, beyond providing a vital resource for the LGBTQ+ community, our annotation study and subsequent in-the-wild assessments reveal (1) strong association between rater political beliefs and how they rate content relevant to a marginalized community, (2) models trained on individual political beliefs exhibit considerable in-the-wild disagreement, and (3) zero-shot large language models (LLMs) align more with liberal raters",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathan Pofcher",
      "Christopher M Homan",
      "Randall Sell",
      "Ashiqur R. KhudaBukhsh"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1006": {
    "title": "Flaw or Artifact? Rethinking Prompt Sensitivity in Evaluating LLMs",
    "volume": "main",
    "abstract": "Prompt sensitivity, referring to the phenomenon where paraphrasing (that is, repeating something written or spoken using different words) leads to significant changes in large language model performance, has been widely accepted as a core limitation of large language models. In this work, we revisit this issue and ask: Is the widely reported high prompt sensitivity truly an inherent weakness of large language models, or is it largely an artifact of evaluation processes? To answer this question, we systematically evaluate seven large language models (for example, the GPT and Gemini families) across six benchmarks, including both multiple-choice and open-ended tasks on twelve diverse prompt templates. We find that much of the prompt sensitivity stems from heuristic evaluation methods, including log-likelihood scoring and rigid answer matching, which often overlook semantically correct responses expressed through alternative phrasings, such as synonyms or paraphrases. When we adopt large language model as a judge evaluations, we observe a substantial reduction in performance variance and a consistently higher correlation in model rankings across prompts. Our findings suggest that modern large language models are more robust to prompt templates than previously believed, and that prompt sensitivity may be more an artifact of evaluation than a flaw in the models",
    "checked": true,
    "id": "5595cf7f4f86e9c5aef2fe1facb80e484f31954a",
    "semantic_title": "flaw or artifact? rethinking prompt sensitivity in evaluating llms",
    "citation_count": 1,
    "authors": [
      "Andong Hua",
      "Kenan Tang",
      "Chenhe Gu",
      "Jindong Gu",
      "Eric Wong",
      "Yao Qin"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1007": {
    "title": "Topic Coverage-based Demonstration Retrieval for In-Context Learning",
    "volume": "main",
    "abstract": "The effectiveness of in-context learning relies heavily on selecting demonstrations that provide all the necessary information for a given test input.To achieve this, it is crucial to identify and cover fine-grained knowledge requirements. However, prior methods often retrieve demonstrations based solely on embedding similarity or generation probability, resulting in irrelevant or redundant examples.In this paper, we propose TopicK, a topic coverage-based retrieval framework that selects demonstrations to comprehensively cover topic-level knowledge relevant to both the test input and the model.Specifically, TopicK estimates the topics required by the input and assesses the model's knowledge on those topics.TopicK then iteratively selects demonstrations that introduce previously uncovered required topics, in which the model exhibits low topical knowledge.We validate the effectiveness of TopicK through extensive experiments across various datasets and both open- and closed-source LLMs.Our source code is available at https://github.com/WonbinKweon/TopicK_EMNLP2025",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wonbin Kweon",
      "SeongKu Kang",
      "Runchu Tian",
      "Pengcheng Jiang",
      "Jiawei Han",
      "Hwanjo Yu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1008": {
    "title": "On the Same Wavelength? Evaluating Pragmatic Reasoning in Language Models across Broad Concepts",
    "volume": "main",
    "abstract": "Language use is shaped by pragmatics—i.e., reasoning about communicative goals and norms in context. As language models (LMs) are increasingly used as conversational agents, it becomes ever more important to understand their pragmatic reasoning abilities. We propose an evaluation framework derived from *Wavelength*, a popular communication game where a speaker and a listener communicate about a broad range of concepts in a granular manner. We study a range of LMs on both language comprehension and language production using direct and Chain-of-Thought (CoT) prompting, and further explore a Rational Speech Act (RSA) approach to incorporating Bayesian pragmatic reasoning into LM inference. We find that state-of-the-art LMs, but not smaller ones, achieve strong performance on language comprehension, obtaining similar-to-human accuracy and exhibiting high correlations with human judgments even without CoT prompting or RSA. On language production, CoT can outperform direct prompting, and using RSA provides significant improvements over both approaches. Our study helps identify the strengths and limitations in LMs' pragmatic reasoning abilities and demonstrates the potential for improving them with RSA, opening up future avenues for understanding conceptual representation, language understanding, and social reasoning in LMs and humans",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linlu Qiu",
      "Cedegao E. Zhang",
      "Joshua B. Tenenbaum",
      "Yoon Kim",
      "Roger P. Levy"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1009": {
    "title": "MuseScorer: Idea Originality Scoring At Scale",
    "volume": "main",
    "abstract": "An objective, face-valid method for scoring idea originality is to measure each idea's statistical infrequency within a population—an approach long used in creativity research. Yet, computing these frequencies requires manually bucketing idea rephrasings, a process that is subjective, labor-intensive, error-prone, and brittle at scale. We introduce MuseScorer, a fully automated, psychometrically validated system for frequency-based originality scoring. MuseScorer integrates a Large Language Model (LLM) with externally orchestrated retrieval: given a new idea, it retrieves semantically similar prior idea-buckets and zero-shot prompts the LLM to judge whether the idea fits an existing bucket or forms a new one. These buckets enable frequency-based originality scoring without human annotation. Across five datasets (Nparticipants=1143, nideas=16,294), MuseScorer matches human annotators in idea clustering structure (AMI =0.59) and participant-level scoring (r = 0.89), while demonstrating strong convergent and external validity. The system enables scalable, intent-sensitive, and human-aligned originality assessment for creativity research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Sarosh Bangash",
      "Krish Veera",
      "Ishfat Abrar Islam",
      "Raiyan Abdul Baten"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1010": {
    "title": "SAFENUDGE: Safeguarding Large Language Models in Real-time with Tunable Safety-Performance Trade-offs",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have been shown to be susceptible to jailbreak attacks, or adversarial attacks used to illicit high risk behavior from a model, highlighting the critical need to safeguard widely-deployed models. Safeguarding approaches, which include fine-tuning models or having LLMs \"self-reflect,\" may lengthen the inference time of a model, incur a computational penalty, reduce the semantic fluency of an output, and restrict \"normal\" model behavior. Importantly, these Safety-Performance Trade-offs (SPTs) remain an understudied area. In this work, we make three contributions: (1) We introduce SAFENUDGE, a novel safeguard that combines Controlled Text Generation and \"nudging.\" SAFENUDGE triggers during text-generation while a jailbreak attack is being executed, and can reduce successful jailbreak attempts by between 28.1% and 37.3% by guiding the LLM towards a safe response. It adds minimal latency to inference and has a negligible impact on the semantic fluency of outputs. Second, it supports tunable SPTs, meaning practitioners can set their own tolerance for trade-offs balancing safety and restrictions to normal model behavior. Third, we release the source code for SAFENUDGE at https://github.com/joaopfonseca/SafeNudge. It is open source and compatible with the HuggingFace transformers library",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joao Fonseca",
      "Andrew Bell",
      "Julia Stoyanovich"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1011": {
    "title": "RaDeR: Reasoning-aware Dense Retrieval Models",
    "volume": "main",
    "abstract": "We propose RaDeR, a set of reasoning-based dense retrieval models trained with data derived from mathematical problem solving using large language models (LLMs). Our method leverages retrieval-augmented reasoning trajectories of an LLM and self-reflective relevance evaluation, enabling the creation of both diverse and hard-negative samples for reasoning-intensive relevance. RaDeR retrievers, trained for mathematical reasoning, effectively generalize to diverse reasoning tasks in the BRIGHT and RAR-b benchmarks, consistently outperforming strong baselines in overall performance. Notably, RaDeR achieves significantly higher performance than baselines on the Math and Coding splits. In addition, RaDeR presents the first dense retriever that outperforms BM25 when queries are Chain-of-Thought reasoning steps, underscoring the critical role of reasoning-based retrieval to augment reasoning language models. Furthermore RaDeR achieves comparable or superior performance while using only 2.5% of the training data used by the concurrent work ReasonIR, highlighting the quality of our synthesized training data. Our code, data, and retrieval models are publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Debrup Das",
      "Sam O’Nuallain",
      "Razieh Rahimi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1012": {
    "title": "A Culturally-diverse Multilingual Multimodal Video Benchmark & Model",
    "volume": "main",
    "abstract": "Large multimodal models (LMMs) have recently gained attention due to their effectiveness to understand and generate descriptions of visual content. Most existing LMMs are in English language. While few recent works explore multilingual image LMMs, to the best of our knowledge, moving beyond the English language for cultural and linguistic inclusivity is yet to be investigated in the context of video LMMs. In pursuit of more inclusive video LMMs, we introduce a multilingual Video LMM benchmark, named ViMUL-Bench, to evaluate Video LMMs across 14 languages, including both low- and high-resource languages: Arabic, Bengali, Chinese, English, French, German, Hindi, Japanese, Russian, Sinhala, Spanish, Swedish, Tamil, and Urdu. Our ViMUL-Bench is designed to rigorously test video LMMs across 15 categories including eight culturally diverse categories, ranging from lifestyles and festivals to foods and rituals and from local landmarks to prominent cultural personalities. ViMUL-Bench comprises both open-ended (short and long-form) and multiple-choice questions spanning various video durations (short, medium, and long) with 8k samples that are manually verified by native language speakers. In addition, we also introduce a machine translated multilingual video training set comprising 1.2 million samples and develop a simple multilingual video LMM, named ViMUL, that is shown to provide a better tradeoff between high-and low-resource languages for video understanding. We hope our ViMUL-Bench and multilingual video LMM along with a large-scale multilingual video training set will help ease future research in developing cultural and linguistic inclusive multilingual video LMMs. Our proposed benchmark, video LMM and training data will be publicly released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bhuiyan Sanjid Shafique",
      "Ashmal Vayani",
      "Muhammad Maaz",
      "Hanoona Abdul Rasheed",
      "Dinura Dissanayake",
      "Mohammed Irfan Kurpath",
      "Yahya Hmaiti",
      "Go Inoue",
      "Jean Lahoud",
      "Md. Safirur Rashid",
      "Shadid Intisar Quasem",
      "Maheen Fatima",
      "Franco Vidal",
      "Mykola Maslych",
      "Ketan Pravin More",
      "Sanoojan Baliah",
      "Hasindri Watawana",
      "Yuhao Li",
      "Fabian Farestam",
      "Leon Schaller",
      "Roman Tymtsiv",
      "Simon Weber",
      "Hisham Cholakkal",
      "Ivan Laptev",
      "Shin’ichi Satoh",
      "Michael Felsberg",
      "Mubarak Shah",
      "Salman Khan",
      "Fahad Shahbaz Khan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1013": {
    "title": "DRES: Fake news detection by dynamic representation and ensemble selection",
    "volume": "main",
    "abstract": "The rapid spread of information via social media has made text-based fake news detection critically important due to its societal impact. This paper presents a novel detection method called Dynamic Representation and Ensemble Selection (DRES) for identifying fake news based solely on text. DRES leverages instance hardness measures to estimate the classification difficulty for each news article across multiple textual feature representations. By dynamically selecting the textual representation and the most competent ensemble of classifiers for each instance, DRES significantly enhances prediction accuracy. Extensive experiments show that DRES achieves notable improvements over state-of-the-art methods, confirming the effectiveness of representation selection based on instance hardness and dynamic ensemble selection in boosting performance. Codes and data are available at: at:https://github.com/FFarhangian/FakeNewsDetection_DRES",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Faramarz Farhangian",
      "Leandro Augusto Ensina",
      "George D C Cavalcanti",
      "Rafael M. O. Cruz"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1014": {
    "title": "A Graph-Theoretical Framework for Analyzing the Behavior of Causal Language Models",
    "volume": "main",
    "abstract": "Recent progress in natural language processing has popularized causal language models, but their internal behavior remains poorly understood due to the high cost and reliance on large-scale benchmarks in existing analysis methods. To address these challenges, we introduce a graph-theoretical framework for analyzing causal language models. Our method constructs graphs from model outputs by linking high-probability token transitions and applies classical metrics to capture linguistic features of model behavior. Based on previous works, none have examined or applied graph analysis from this perspective. For the first time, a macroscopic view of the overall behavior of a language model is provided by analyzing the mathematical characteristics of small sample graphs derived from the generated outputs. We first discuss the metrics theoretically, then demonstrate how they work through experiments, followed by some applications of this graph-theoretical framework in natural language processing tasks. Through experiments across training steps and model sizes, we demonstrate that these metrics can reflect model evolution and predict performance with minimal data. We further validate our findings by comparing them with benchmark accuracy scores, highlighting the reliability of our metrics. In contrast to existing evaluation methods, our approach is lightweight, efficient, and especially well-suited for low-resource settings. Our implementation codes are available at this GitHub repository",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rashin Rahnamoun",
      "Mehrnoush Shamsfard"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1015": {
    "title": "Membership and Memorization in LLM Knowledge Distillation",
    "volume": "main",
    "abstract": "Recent advances in Knowledge Distillation (KD) aim to mitigate the high computational demands of Large Language Models (LLMs) by transferring knowledge from a large \"teacher\" to a smaller \"student\" model. However, students may inherit the teacher's privacy when the teacher is trained on private data. In this work, we systematically characterize and investigate membership privacy risks inherent in six LLM KD techniques.Using instruction-tuning settings that span seven NLP tasks, together with three teacher model families (GPT-2, LLAMA-2, and OPT), and various size student models, we demonstrate that all existing LLM KD approaches carry membership and memorization privacy risks from the teacher to its students. However, the extent of privacy risks varies across different KD techniques. We systematically analyse how key LLM KD components (KD objective functions, student training data and NLP tasks) impact such privacy risks. We also demonstrate a significant disagreement between memorization and membership privacy risks of LLM KD techniques. Finally, we characterize per-block privacy risk and demonstrate that the privacy risk varies across different blocks by a large margin",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziqi Zhang",
      "Ali Shahin Shamsabadi",
      "Hanxiao Lu",
      "Yifeng Cai",
      "Hamed Haddadi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1016": {
    "title": "Balanced Multi-Factor In-Context Learning for Multilingual Large Language Models",
    "volume": "main",
    "abstract": "Multilingual large language models (MLLMs) are able to leverage in-context learning (ICL) to achieve high performance by leveraging cross-lingual knowledge transfer without parameter updates. However, their effectiveness is highly sensitive to example selection, particularly in multilingual settings. Based on the findings of existing work, three key factors influence multilingual ICL: (1) semantic similarity, (2) linguistic alignment, and (3) language-specific performance. However, existing approaches address these factors independently, without explicitly disentangling their combined impact, leaving optimal example selection underexplored. To address this gap, we propose balanced multi-factor ICL (BMF-ICL), a method that quantifies and optimally balances these factors for improved example selection. Experiments on mCSQA and TYDI across four MLLMs demonstrate that BMF-ICL outperforms existing methods. Further analysis highlights the importance of incorporating all three factors and the importance of selecting examples from multiple languages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Masahiro Kaneko",
      "Alham Fikri Aji",
      "Timothy Baldwin"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1017": {
    "title": "Efficient Context Selection for Long-Context QA: No Tuning, No Iteration, Just Adaptive‐k",
    "volume": "main",
    "abstract": "Retrieval-augmented generation (RAG) and long-context language models (LCLMs) both address context limitations of LLMs in open-domain QA. However, optimal external context to retrieve remains an open problem: fixed retrieval budgets risk wasting tokens or omitting key evidence. Existing adaptive methods like Self-RAG and Self-Route rely on iterative LLM prompting and perform well on factoid QA, but struggle with aggregation QA where optimal context size is unknown and variable. We present Adaptive‐k retrieval, a simple and effective single-pass method that selects a query-specific number of passages by applying a threshold to the similarity scores between the query and candidate passages. It does not require model fine-tuning, extra LLM calls or changes to existing retriever–reader pipelines. On both factoid and aggregation QA benchmarks, Adaptive‐k matches or outperforms fixed‐k baselines while using up to 10x fewer tokens than full-context input, and still retrieves 70% of relevant passages. It improves accuracy across five LCLMs and two embedding models, highlighting that dynamically adjusting context size leads to more efficient and accurate QA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chihiro Taguchi",
      "Seiji Maekawa",
      "Nikita Bhutani"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1018": {
    "title": "Languages Still Left Behind: Toward a Better Multilingual Machine Translation Benchmark",
    "volume": "main",
    "abstract": "Multilingual machine translation (MT) benchmarks play a central role in evaluating the capabilities of modern MT systems. Among them, the FLORES+ benchmark is widely used, offering English-to-many translation data for over 200 languages, curated with strict quality control protocols. However, we study data in four languages (Asante Twi, Japanese, Jinghpaw, and South Azerbaijani) and uncover critical shortcomings in the benchmark's suitability for truly multilingual evaluation. Human assessments reveal that many translations fall below the claimed 90% quality standard, and the annotators report that source sentences are often too domain-specific and culturally biased toward the English-speaking world. We further demonstrate that simple heuristics, such as copying named entities, can yield non-trivial BLEU scores, suggesting vulnerabilities in the evaluation protocol. Notably, we show that MT models trained on naturalistic data perform poorly on FLORES+ while achieving significant gains on our domain-relevant evaluation set. Based on these findings, we advocate for multilingual MT benchmarks that use domain-general, named-entity-agnostic, and culturally neutral source texts to better reflect real-world translation challenges",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chihiro Taguchi",
      "Seng Mai",
      "Keita Kurabe",
      "Yusuke Sakai",
      "Georgina Agyei",
      "Soudabeh Eslami",
      "David Chiang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1019": {
    "title": "Think Globally, Group Locally: Evaluating LLMs Using Multi-Lingual Word Grouping Games",
    "volume": "main",
    "abstract": "Large language models (LLMs) can exhibit biases in reasoning capabilities due to linguistic modality, performing better on tasks in one language versus another, even with similar content. Most previous works evaluate this through reasoning tasks where reliance on strategies or knowledge can ensure success, such as in commonsense or math tasks. However, abstract reasoning is vital to reasoning for everyday life, where people apply \"out-of-the-box thinking\" to identify and use patterns for solutions, without a reliance on formulaic approaches. Comparatively, little work has evaluated linguistic biases in this task type. In this paper, we propose a task inspired by the New York Times Connections: GlobalGroup, that evaluates models in an abstract reasoning task across several languages. We constructed a game benchmark with five linguistic backgrounds – English, Spanish, Chinese, Hindi, and Arabic – in both the native language and an English translation for comparison. We also proposed game difficulty measurements to evaluate models on games with similar difficulty, enabling a more controlled comparison, which is particularly important in reasoning evaluations. Through experimentation, we find English modalities largely lead to better performance in this abstract reasoning task, and performance disparities between open- and closed-source models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "César Guerra-Solano",
      "Zhuochun Li",
      "Xiang Lorraine Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1020": {
    "title": "Pointing to a Llama and Call it a Camel: On the Sycophancy of Multimodal Large Language Models",
    "volume": "main",
    "abstract": "Multimodal large language models (MLLMs) have demonstrated extraordinary capabilities in conducting conversations based on image inputs. However, we observe that MLLMs exhibit a pronounced form of visual sycophantic behavior. While similar behavior has also been noted in text-based large language models (LLMs), it becomes significantly more prominent when MLLMs process image inputs. We refer to this phenomenon as the \"sycophantic modality gap.\" To better understand this issue, we further analyze the factors that contribute to the exacerbation of this gap. To mitigate the visual sycophantic behavior, we first experiment with naive supervised fine-tuning to help the MLLM resist misleading instructions from the user. However, we find that this approach also makes the MLLM overly resistant to corrective instructions (i.e., stubborn even if it is wrong). To alleviate this trade-off, we propose Sycophantic Reflective Tuning (SRT), which enables the MLLM to engage in reflective reasoning, allowing it to determine whether a user's instruction is misleading or corrective before drawing a conclusion. After applying SRT, we observe a significant reduction in sycophantic behavior toward misleading instructions, without resulting in excessive stubbornness when receiving corrective instructions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Renjie Pi",
      "Kehao Miao",
      "Li Peihang",
      "Runtao Liu",
      "Jiahui Gao",
      "Jipeng Zhang",
      "Xiaofang Zhou"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1021": {
    "title": "MR. Judge: Multimodal Reasoner as a Judge",
    "volume": "main",
    "abstract": "The paradigm of using Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) as evaluative judges has emerged as an effective approach in RLHF and inference-time scaling. In this work, we propose Multimodal Reasoner as a Judge (MR. Judge), a paradigm for empowering general-purpose MLLMs judges with strong reasoning capabilities. Instead of directly assigning scores for each response, we formulate the judgement process as a reasoning-inspired multiple-choice problem. Specifically, the judge model first conducts deliberate reasoning covering different aspects of the responses and eventually selects the best response from them. This reasoning process not only improves the interpretibility of the judgement, but also greatly enhances the performance of MLLM judges. To cope with the lack of questions with scored responses, we propose the following strategy to achieve automatic annotation: 1) Reverse Response Candidates Synthesis: starting from a supervised fine-tuning (SFT) dataset, we treat the original response as the best candidate and prompt the MLLM to generate plausible but flawed negative candidates. 2) Text-based reasoning distillation: we carefully design a data synthesis pipeline for distilling the reasoning capability from a text-based reasoning model, which is adopted to enable the MLLM judges to regain complex reasoning ability via warm up supervised fine-tuning. Experiments demonstrate that our MR. Judge is effective across a wide range of tasks. Specifically, our MR. Judge-7B surpasses GPT-4o by 9.9% on VL-RewardBench, and improves performance on MM-Vet during inference-time scaling by up to 7.7%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Renjie Pi",
      "Haoping Bai",
      "Qibin Chen",
      "Xiaoming Simon Wang",
      "Jiulong Shan",
      "Xiaojiang Liu",
      "Meng Cao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1022": {
    "title": "MobiZO: Enabling Efficient LLM Fine-Tuning at the Edge via Inference Engines",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) are currently pre-trained and fine-tuned on large cloud servers. The next frontier is LLM personalization, where a foundation model can be fine-tuned with user/task-specific data. Given the sensitive nature of such private data, it is desirable to fine-tune these models on edge devices to improve user trust. However, fine-tuning on resource-constrained edge devices presents significant challenges due to substantial memory and computational demands, as well as limited infrastructure support. We observe that inference engines (e.g., ExecuTorch) can be repurposed for fine-tuning by leveraging zeroth-order (ZO) optimization, which uses multiple forward passes to approximate gradients. While promising, direct application of ZO methods on edge devices is inefficient due to the high computational cost of multiple forward passes required for accurate gradient estimation, and their deployment has been largely unexplored in practice. We introduce MobiZO, a resource-efficient fine-tuning framework for LLMs specifically designed for edge devices. MobiZO combines three key innovations: (1) a parallelized randomized gradient estimator that employs both outer-loop and inner-loop parallelism to eliminate sequential forward passes, (2) a specialized Multi-Perturbed LoRA (MP-LoRA) module that enables efficient realization of both inner and outer loop parallelism, and (3) a seamless integration with ExecuTorch for on-device training, requiring no modifications to the runtime. Experiments demonstrate that MobiZO achieves substantial runtime speedups and memory savings while improving fine-tuning accuracy, paving the way for practical deployment of LLMs in real-time, on-device applications. Code available at: https://github.com/leigao97/MobiZO",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Gao",
      "Amir Ziashahabi",
      "Yue Niu",
      "Salman Avestimehr",
      "Murali Annavaram"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1023": {
    "title": "Fann or Flop: A Multigenre, Multiera Benchmark for Arabic Poetry Understanding in LLMs",
    "volume": "main",
    "abstract": "Arabic poetry stands as one of the most sophisticated and culturally embedded forms of expression in the Arabic language, known for its layered meanings, stylistic diversity, and deep historical continuity. Although large language models (LLMs) have demonstrated strong performance across languages and tasks, their ability to understand Arabic poetry remains largely unexplored. In this work, we introduce \"Fann or Flop\", the first benchmark designed to assess the comprehension of Arabic poetry by LLMs in twelve historical eras, covering 21 core poetic genres and a variety of metrical forms, from classical structures to contemporary free verse. The benchmark comprises a curated corpus of poems with explanations that assess semantic understanding, metaphor interpretation, prosodic awareness, and cultural context. We argue that poetic comprehension offers a strong indicator for testing how good the LLM is in understanding classical Arabic through the Arabic poetry. Unlike surface-level tasks, this domain demands deeper interpretive reasoning and cultural sensitivity. Our evaluation of state-of-the-art LLMs shows that most models struggle with poetic understanding despite strong results on standard Arabic benchmarks. We release \"Fann or Flop\" along with the evaluation suite as an open-source resource to enable rigorous evaluation and advancement for Arabic-capable language models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wafa Al Ghallabi",
      "Ritesh Thawkar",
      "Sara Ghaboura",
      "Ketan Pravin More",
      "Omkar Thawakar",
      "Hisham Cholakkal",
      "Salman Khan",
      "Rao Muhammad Anwer"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1024": {
    "title": "CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical Reasoning",
    "volume": "main",
    "abstract": "Mathematical reasoning remains a significant challenge for large language models (LLMs), despite progress in prompting techniques such as Chain-of-Thought (CoT). We present **Chain of Mathematically Annotated Thought (CoMAT)**, which enhances reasoning through two stages: *Symbolic Conversion* (converting natural language queries into symbolic form) and *Reasoning Execution* (deriving answers from symbolic representations). CoMAT operates entirely with a single LLM and without external solvers. Across four LLMs, CoMAT outperforms traditional CoT on six out of seven benchmarks, achieving gains of 4.48% on MMLU-Redux (MATH) and 4.58% on GaoKao MCQ. In addition to improved performance, CoMAT ensures faithfulness and verifiability, offering a transparent reasoning process for complex mathematical tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joshua Ong Jun Leang",
      "Aryo Pradipta Gema",
      "Shay B Cohen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1025": {
    "title": "s1: Simple test-time scaling",
    "volume": "main",
    "abstract": "Test-time scaling is a promising new approach to language modeling that uses extra test-time compute to improve performance. Recently, OpenAI's o1 model showed this capability but did not publicly share its methodology, leading to many replication efforts. We seek the simplest approach to achieve test-time scaling and strong reasoning performance. First, we curate a small dataset s1K of 1,000 questions paired with reasoning traces relying on three criteria we validate through ablations: difficulty, diversity, and quality. Second, we develop budget forcing to control test-time compute by forcefully terminating the model's thinking process or lengthening it by appending \"Wait\" multiple times to the model's generation when it tries to end. This can lead the model to double-check its answer, often fixing incorrect reasoning steps. After supervised finetuning the Qwen2.5-32B-Instruct language model on s1K and equipping it with budget forcing, our model s1 exceeds o1-preview on competition math questions by up to 27% (MATH and AIME24). Further, scaling s1 with budget forcing allows extrapolating beyond its performance without test-time intervention: from 50% to 57% on AIME24. Our model, data, and code are open-source at https://github.com/simplescaling/s1",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Niklas Muennighoff",
      "Zitong Yang",
      "Weijia Shi",
      "Xiang Lisa Li",
      "Li Fei-Fei",
      "Hannaneh Hajishirzi",
      "Luke Zettlemoyer",
      "Percy Liang",
      "Emmanuel Candes",
      "Tatsunori Hashimoto"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1026": {
    "title": "Learning Subjective Label Distributions via Sociocultural Descriptors",
    "volume": "main",
    "abstract": "Subjectivity in NLP tasks, _e.g._, toxicity classification, has emerged as a critical challenge precipitated by the increased deployment of NLP systems in content-sensitive domains. Conventional approaches aggregate annotator judgements (labels), ignoring minority perspectives, and overlooking the influence of the sociocultural context behind such annotations. We propose a framework where subjectivity in binary labels is modeled as an empirical distribution accounting for the variation in annotators through human values extracted from sociocultural descriptors using a language model. The framework also allows for downstream tasks such as population and sociocultural group-level majority label prediction. Experiments on three toxicity datasets covering human-chatbot conversations and social media posts annotated with diverse annotator pools demonstrate that our approach yields well-calibrated toxicity distribution predictions across binary toxicity labels, which are further used for majority label prediction across cultural subgroups, improving over existing methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammed Fayiz Parappan",
      "Ricardo Henao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1027": {
    "title": "COM-BOM: Bayesian Exemplar Search for Efficiently Exploring the Accuracy-Calibration Pareto Frontier",
    "volume": "main",
    "abstract": "Selecting an optimal set of exemplars is critical for good performance of in-context learning. However, prior exemplar search methods narrowly optimize for predictive accuracy, critically neglecting model calibration—a key determinant of trustworthiness and safe deployment. In this paper, we formulate exemplar selection as a multi-objective optimization problem, explicitly targeting both the maximization of predictive accuracy and the minimization of expected calibration error. We solve this problem with a sample-efficient Combinatorial Bayesian Optimization algorithm (COM-BOM) to find the Pareto-front that optimally trade-offs the two objectives of accuracy and calibration. We evaluate COM-BOM on multiple tasks from un-saturated MMLU-pro benchmark and find that COM-BOM beats or matches the baselines in jointly optimizing the two objectives, while requiring a minimal number of LLM API calls",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaoxiang Luo",
      "Aryan Deshwal"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1028": {
    "title": "ML-Promise: A Multilingual Dataset for Corporate Promise Verification",
    "volume": "main",
    "abstract": "Promises made by politicians, corporate leaders, and public figures have a significant impact on public perception, trust, and institutional reputation. However, the complexity and volume of such commitments, coupled with difficulties in verifying their fulfillment, necessitate innovative methods for assessing their credibility. This paper introduces the concept of Promise Verification, a systematic approach involving steps such as promise identification, evidence assessment, and the evaluation of timing for verification. We propose the first multilingual dataset, ML-Promise, which includes English, French, Chinese, Japanese, and Korean, aimed at facilitating in-depth verification of promises, particularly in the context of Environmental, Social, and Governance (ESG) reports. Given the growing emphasis on corporate environmental contributions, this dataset addresses the challenge of evaluating corporate promises, especially in light of practices like greenwashing. Our findings also explore textual and image-based baselines, with promising results from retrieval-augmented generation (RAG) approaches. This work aims to foster further discourse on the accountability of public commitments across multiple languages and domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yohei Seki",
      "Hakusen Shu",
      "Anaïs Lhuissier",
      "Hanwool Lee",
      "Juyeon Kang",
      "Min-Yuh Day",
      "Chung-Chi Chen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1029": {
    "title": "Reading Between the Prompts: How Stereotypes Shape LLM's Implicit Personalization",
    "volume": "main",
    "abstract": "Generative Large Language Models (LLMs) infer user's demographic information from subtle cues in the conversation — a phenomenon called implicit personalization. Prior work has shown that such inferences can lead to lower quality responses for users assumed to be from minority groups, even when no demographic information is explicitly provided. In this work, we systematically explore how LLMs respond to stereotypical cues using controlled synthetic conversations, by analyzing the models' latent user representations through both model internals and generated answers to targeted user questions. Our findings reveal that LLMs do infer demographic attributes based on these stereotypical signals, which for a number of groups even persists when the user explicitly identifies with a different demographic group. Finally, we show that this form of stereotype-driven implicit personalization can be effectively mitigated by intervening on the model's internal representations using a trained linear probe to steer them toward the explicitly stated identity. Our results highlight the need for greater transparency and control in how LLMs represent user identity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vera Neplenbroek",
      "Arianna Bisazza",
      "Raquel Fernández"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1030": {
    "title": "Paired by the Teacher: Turning Unpaired Data into High-Fidelity Pairs for Low-Resource Text Generation",
    "volume": "main",
    "abstract": "We present Paired by the Teacher (PbT), a two-stage teacher–student pipeline that synthesizes accurate input–output pairs without human labels or parallel data. In many low-resource natural language generation (NLG) scenarios, practitioners may have only raw outputs, like highlights, recaps, or questions, or only raw inputs, such as articles, dialogues, or paragraphs, but seldom both. This mismatch forces small models to learn from very few examples or rely on costly, broad-scope synthetic examples produced by large LLMs. PbT addresses this by asking a teacher LLM to compress each unpaired example into a concise intermediate representation (IR), and training a student to reconstruct inputs from IRs. This enables outputs to be paired with student-generated inputs, yielding high-quality synthetic data. We evaluate PbT on five benchmarks—document summarization (XSum, CNNDM), dialogue summarization (SAMSum, DialogSum), and question generation (SQuAD)—as well as an unpaired setting on SwitchBoard (paired with DialogSum summaries). An 8B student trained only on PbT data outperforms models trained on 70 B teacher-generated corpora and other unsupervised baselines, coming within 1.2 ROUGE-L of human-annotated pairs and closing 82% of the oracle gap at one-third the annotation cost of direct synthesis. Human evaluation on SwitchBoard further confirms that only PbT produces concise, faithful summaries aligned with the target style, highlighting its advantage of generating in-domain sources that avoid the mismatch, limiting direct synthesis",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yen-Ju Lu",
      "Thomas Thebaud",
      "Laureano Moro-Velazquez",
      "Najim Dehak",
      "Jesus Villalba"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1031": {
    "title": "Please Translate Again: Two Simple Experiments on Whether Human-Like Reasoning Helps Translation",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) demonstrate strong reasoning capabilities for many tasks, often by explicitly decomposing the task via Chain-of-Thought (CoT) reasoning. Recent work on LLM-based translation designs hand-crafted prompts to decompose translation, or trains models to incorporate intermediate steps. _Translating Step-by-step_ (Briakou et al., 2024), for instance, introduces a multi-step prompt with decomposition and refinement of translation with LLMs, which achieved state-of-the-art results on WMT24 test data. In this work, we scrutinise this strategy's effectiveness. Empirically, we find no clear evidence that performance gains stem from explicitly decomposing the translation process via CoT, at least for the models on test; and we show prompting LLMs to \"translate again\" and self-refine yields even better results than human-like step-by-step prompting. While the decomposition influences translation behaviour, faithfulness to the decomposition has both positive and negative effects on translation. Our analysis therefore suggests a divergence between the optimal translation strategies for humans and LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Di Wu",
      "Seth Aycock",
      "Christof Monz"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1032": {
    "title": "How Do Large Vision-Language Models See Text in Image? Unveiling the Distinctive Role of OCR Heads",
    "volume": "main",
    "abstract": "Despite significant advancements in Large Vision Language Models (LVLMs), a gap remains, particularly regarding their interpretability and how they locate and interpret textual information within images. In this paper, we explore various LVLMs to identify the specific heads responsible for recognizing text from images, which we term the Optical Character Recognition Head (OCR Head). Our findings regarding these heads are as follows: (1) Less Sparse: Unlike previous retrieval heads, a large number of heads are activated to extract textual information from images. (2) Qualitatively Distinct: OCR heads possess properties that differ significantly from general retrieval heads, exhibiting low similarity in their characteristics. (3) Statically Activated: The frequency of activation for these heads closely aligns with their OCR scores. We validate our findings in downstream tasks by applying Chain-of-Thought (CoT) to both OCR and conventional retrieval heads and by masking these heads. We also demonstrate that redistributing sink-token values within the OCR heads improves performance. These insights provide a deeper understanding of the internal mechanisms LVLMs employ in processing embedded textual information in images",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ingeol Baek",
      "Hwan Chang",
      "Sunghyun Ryu",
      "Hwanhee Lee"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1033": {
    "title": "Explainability and Interpretability of Multilingual Large Language Models: A Survey",
    "volume": "main",
    "abstract": "Multilingual large language models (MLLMs) demonstrate state-of-the-art capabilities across diverse cross-lingual and multilingual tasks. Their complex internal mechanisms, however, often lack transparency, posing significant challenges in elucidating their internal processing of multilingualism, cross-lingual transfer dynamics and handling of language-specific features. This paper addresses this critical gap by presenting a survey of current explainability and interpretability methods specifically for MLLMs. To our knowledge, it is the first comprehensive review of its kind. Existing literature is categorised according to the explainability techniques employed, the multilingual tasks addressed, the languages investigated and available resources. The survey further identifies key challenges, distils core findings and outlines promising avenues for future research within this rapidly evolving domain",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucas Resck",
      "Isabelle Augenstein",
      "Anna Korhonen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1034": {
    "title": "Decoding the Rule Book: Extracting Hidden Moderation Criteria from Reddit Communities",
    "volume": "main",
    "abstract": "Effective content moderation systems require explicit classification criteria, yet online communities like subreddits often operate with diverse, implicit standards. This work introduces a novel approach to identify and extract these implicit criteria from historical moderation data using an interpretable architecture. We represent moderation criteria as score tables of lexical expressions associated with content removal, enabling systematic comparison across different communities.Our experiments demonstrate that these extracted lexical patterns effectively replicate the performance of neural moderation models while providing transparent insights into decision-making processes. The resulting criteria matrix reveals significant variations in how seemingly shared norms are actually enforced, uncovering previously undocumented moderation patterns including community-specific tolerances for language, features for topical restrictions, and underlying subcategories of the toxic speech classification",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youngwoo Kim",
      "Himanshu Beniwal",
      "Steven L. Johnson",
      "Thomas Hartvigsen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1035": {
    "title": "AcT2I: Evaluating and Improving Action Depiction in Text-to-Image Models",
    "volume": "main",
    "abstract": "Text-to-Image (T2I) models have recently achieved remarkable success in generating images from textual descriptions. However, challenges still persist in accurately rendering complex scenes where actions and interactions form the primary semantic focus. Our key observation in this work is that T2I models frequently struggle to capture nuanced and often implicit attributes inherent in action depiction, leading to generating images that lack key contextual details. To enable systematic evaluation, we introduce AcT2I, a benchmark designed to evaluate the performance of T2I models in generating images from action-centric prompts. We experimentally validate that leading T2I models do not fare well on AcT2I. We further hypothesize that this shortcoming arises from the incomplete representation of the inherent attributes and contextual dependencies in the training corpora of existing T2I models. We build upon this by developing a training-free, knowledge distillation technique utilizing Large Language Models to address this limitation. Specifically, we enhance prompts by incorporating dense information across three dimensions, observing that injecting prompts with temporal details significantly improves image generation accuracy, with our best model achieving an increase of 72%. Our findings highlight the limitations of current T2I methods in generating images that require complex reasoning and demonstrate that integrating linguistic knowledge in a systematic way can notably advance the generation of nuanced and contextually accurate images. Project Page : https://vatsal-malaviya.github.io/AcT2I/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vatsal Malaviya",
      "Agneet Chatterjee",
      "Maitreya Patel",
      "Yezhou Yang",
      "Chitta Baral"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1036": {
    "title": "Assessing French Readability for Adults with Low Literacy: A Global and Local Perspective",
    "volume": "main",
    "abstract": "This study presents a novel approach to assessing French text readability for adults with low literacy skills, addressing both global (full-text) and local (segment-level) difficulty. We introduce a dataset of 461 texts annotated using a difficulty scale developed specifically for this population. Using this corpus, we conducted a systematic comparison of key readability modeling approaches, including machine learning techniques based on linguistic variables, fine-tuning of CamemBERT, a hybrid approach combining CamemBERT with linguistic variables, and the use of generative language models (LLMs) to carry out readability assessment at both global and local levels",
    "checked": true,
    "id": "cf9b9c95aa89b3d179f3c70c63f06e615fabf8c5",
    "semantic_title": "assessing french readability for adults with low literacy: a global and local perspective",
    "citation_count": 1,
    "authors": [
      "Wafa Aissa",
      "Thibault Bañeras-Roux",
      "Elodie Vanzeveren",
      "Lingyun Gao",
      "Rodrigo Wilkens",
      "Thomas François"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1037": {
    "title": "LILaC: Late Interacting in Layered Component Graph for Open-domain Multimodal Multihop Retrieval",
    "volume": "main",
    "abstract": "Multimodal document retrieval aims to retrieve query-relevant components from documents composed of textual, tabular, and visual elements. An effective multimodal retriever needs to handle two main challenges: (1) mitigate the effect of irrelevant contents caused by fixed, single-granular retrieval units, and (2) support multihop reasoning by effectively capturing semantic relationships among components within and across documents. To address these challenges, we propose LILaC, a multimodal retrieval framework featuring two core innovations. First, we introduce a layered component graph, explicitly representing multimodal information at two layers—each representing coarse and fine granularity—facilitating efficient yet precise reasoning. Second, we develop a late-interaction-based subgraph retrieval method, an edge-based approach that initially identifies coarse-grained nodes for efficient candidate generation, then performs fine-grained reasoning via late interaction. Extensive experiments demonstrate that LILaC achieves state-of-the-art retrieval performance on all five benchmarks, notably without additional fine-tuning. We make the artifacts publicly available at github.com/joohyung00/lilac",
    "checked": true,
    "id": "1873a09f7a5ddaf53f4e04884214effc1f13718a",
    "semantic_title": "lilac: late interacting in layered component graph for open-domain multimodal multihop retrieval",
    "citation_count": 0,
    "authors": [
      "Joohyung Yun",
      "Doyup Lee",
      "Wook-Shin Han"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1038": {
    "title": "DiCoRe: Enhancing Zero-shot Event Detection via Divergent-Convergent LLM Reasoning",
    "volume": "main",
    "abstract": "Zero-shot Event Detection (ED), the task of identifying event mentions in natural language text without any training data, is critical for document understanding in specialized domains. Understanding the complex event ontology, extracting domain-specific triggers from the passage, and structuring them appropriately overloads and limits the utility of Large Language Models (LLMs) for zero-shot ED. To this end, we propose DiCoRe, a divergent-convergent reasoning framework that decouples the task of ED using Dreamer and Grounder. Dreamer encourages divergent reasoning through open-ended event discovery, which helps to boost event coverage. Conversely, Grounder introduces convergent reasoning to align the free-form predictions with the task-specific instructions using finite-state machine guided constrained decoding. Additionally, an LLM-Judge verifies the final outputs to ensure high precision. Through extensive experiments on six datasets across five domains and nine LLMs, we demonstrate how DiCoRe consistently outperforms prior zero-shot, transfer-learning, and reasoning baselines, achieving 4–7% average F1 gains over the best baseline – establishing DiCoRe as a strong zero-shot ED framework",
    "checked": true,
    "id": "b1833423a88e5eec0de21e30c3a0847132bb6553",
    "semantic_title": "dicore: enhancing zero-shot event detection via divergent-convergent llm reasoning",
    "citation_count": 1,
    "authors": [
      "Tanmay Parekh",
      "Kartik Mehta",
      "Ninareh Mehrabi",
      "Kai-Wei Chang",
      "Nanyun Peng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1039": {
    "title": "SNaRe: Domain-aware Data Generation for Low-Resource Event Detection",
    "volume": "main",
    "abstract": "Event Detection (ED) – the task of identifying event mentions from natural language text – is critical for enabling reasoning in highly specialized domains such as biomedicine, law, and epidemiology. Data generation has proven to be effective in broadening its utility to wider applications without requiring expensive expert annotations. However, when existing generation approaches are applied to specialized domains, they struggle with label noise, where annotations are incorrect, and domain drift, characterized by a distributional mismatch between generated sentences and the target domain. To address these issues, we introduce SNaRe, a domain-aware synthetic data generation framework composed of three components: Scout, Narrator, and Refiner. Scout extracts triggers from unlabeled target domain data and curates a high-quality domain-specific trigger list using corpus-level statistics to mitigate domain drift. Narrator, conditioned on these triggers, generates high-quality domain-aligned sentences, and Refiner identifies additional event mentions, ensuring high annotation quality. Experimentation on three diverse domain ED datasets reveals how SNaRe outperforms the best baseline, achieving average F1 gains of 3-7% in the zero-shot/few-shot settings and 4-20% F1 improvement for multilingual generation. Analyzing the generated trigger hit rate and human evaluation substantiates SNaRe's stronger annotation quality and reduced domain drift",
    "checked": true,
    "id": "808d6866e1dab7a2b295bbbc31cd093b7d90b651",
    "semantic_title": "snare: domain-aware data generation for low-resource event detection",
    "citation_count": 0,
    "authors": [
      "Tanmay Parekh",
      "Yuxuan Dong",
      "Lucas Bandarkar",
      "Artin Kim",
      "I-Hung Hsu",
      "Kai-Wei Chang",
      "Nanyun Peng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1040": {
    "title": "Table-R1: Inference-Time Scaling for Table Reasoning Tasks",
    "volume": "main",
    "abstract": "In this work, we present the first study to explore inference-time scaling on table reasoning tasks. We develop and evaluate two post-training strategies to enable inference-time scaling: distillation from frontier model reasoning traces and reinforcement learning with verifiable rewards (RLVR). For distillation, we introduce a large-scale dataset of reasoning traces generated by DeepSeek-R1, which we use to fine-tune LLMs into the Table-R1-SFT model. For RLVR, we propose task-specific verifiable reward functions and apply the GRPO algorithm to obtain the Table-R1-Zero model. We evaluate our Table-R1-series models across diverse table reasoning tasks, including short-form QA, fact verification, and free-form QA. Notably, the Table-R1-Zero model matches or exceeds the performance of GPT-4.1 and DeepSeek-R1, while using only a 7B-parameter LLM. It also demonstrates strong generalization to out-of-domain datasets. Extensive ablation and qualitative analyses reveal the benefits of instruction tuning, model architecture choices, and cross-task generalization, as well as emergence of essential table reasoning skills during RL training",
    "checked": false,
    "id": "d9c149114c2d71fc6c00e9b3f7faae18d2ff1e08",
    "semantic_title": "table-r1: inference-time scaling for table reasoning",
    "citation_count": 9,
    "authors": [
      "Zheyuan Yang",
      "Lyuhao Chen",
      "Arman Cohan",
      "Yilun Zhao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1041": {
    "title": "LimRank: Less is More for Reasoning-Intensive Information Reranking",
    "volume": "main",
    "abstract": "Existing approaches typically rely on large-scale fine-tuning to adapt LLMs for information reranking tasks, which is computationally expensive. In this work, we demonstrate that modern LLMs can be effectively adapted using only minimal, high-quality supervision. To enable this, we design LIMRANK-SYNTHESIZER, a reusable and open-source pipeline for generating diverse, challenging, and realistic reranking examples. Using this synthetic data, we fine-tune our reranker model, LIMRANK. We evaluate LIMRANK on two challenging benchmarks, i.e., BRIGHT for reasoning-intensive retrieval and FollowIR for instruction-following retrieval. Our experiments demonstrate that LIMRANK achieves competitive performance, while being trained on less than 5% of the data typically used in prior work. Further ablation studies demonstrate the effectiveness of LIMRANK-SYNTHESIZER and the strong generalization capabilities of LIMRANK across downstream tasks, including scientific literature search and retrieval-augmented generation for knowledge-intensive problem solving",
    "checked": true,
    "id": "4d39dd15fca76a60b037e3e7c32e176172357889",
    "semantic_title": "limrank: less is more for reasoning-intensive information reranking",
    "citation_count": 0,
    "authors": [
      "Tingyu Song",
      "Yilun Zhao",
      "Siyue Zhang",
      "Chen Zhao",
      "Arman Cohan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1042": {
    "title": "PlanGEN: A Multi-Agent Framework for Generating Planning and Reasoning Trajectories for Complex Problem Solving",
    "volume": "main",
    "abstract": "Recent agent frameworks and inference-time algorithms often struggle with natural planning problems due to limitations in verifying generated plans or reasoning and varying complexity of instances within a single task. Many existing methods for these tasks either perform task-level verification without considering constraints or apply inference-time algorithms without adapting to instance-level complexity. To address these limitations, we propose PlanGEN, a model-agnostic and easily scalable agent framework with three key components: constraint, verification, and selection agents. Specifically, our approach proposes constraint-guided iterative verification to enhance performance of inference-time algorithms–Best of 𝒩, Tree-of-Thought, and REBASE. In PlanGEN framework, the selection agent optimizes algorithm choice based on instance complexity, ensuring better adaptability to complex planning problems. Experimental results demonstrate significant improvements over the strongest baseline across multiple benchmarks, achieving state-of-the-art results on NATURAL PLAN (~8%↑), OlympiadBench (~4%↑), DocFinQA (~7%↑), and GPQA (~1%↑). Our key finding highlights that constraint-guided iterative verification improves inference-time algorithms, and adaptive selection further boosts performance on complex planning and reasoning problems",
    "checked": true,
    "id": "ebc8b174a91acba20601d82353af8c87a4dc99e0",
    "semantic_title": "plangen: a multi-agent framework for generating planning and reasoning trajectories for complex problem solving",
    "citation_count": 9,
    "authors": [
      "Mihir Parmar",
      "Xin Liu",
      "Palash Goyal",
      "Yanfei Chen",
      "Long Le",
      "Swaroop Mishra",
      "Hossein Mobahi",
      "Jindong Gu",
      "Zifeng Wang",
      "Hootan Nakhost",
      "Chitta Baral",
      "Chen-Yu Lee",
      "Tomas Pfister",
      "Hamid Palangi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1043": {
    "title": "An Empirical Study on Strong-Weak Model Collaboration for Repo-level Code Generation",
    "volume": "main",
    "abstract": "We study cost-efficient collaboration between strong and weak language models for repository-level code generation, where the weak model handles simpler tasks at lower cost, and the most challenging tasks are delegated to the strong model. While many works propose architectures for this task, few analyze performance relative to cost. We evaluate a broad spectrum of collaboration strategies: context-based, pipeline-based, and dynamic, on GitHub issue resolution. Our most effective collaborative strategy achieves equivalent performance to the strong model while reducing the cost by 40%. Based on our findings, we offer actionable guidelines for choosing collaboration strategies under varying budget and performance constraints. Our results show that strong–weak collaboration substantially boosts the weak model's performance at a fraction of the cost, pipeline and context-based methods being most efficient",
    "checked": true,
    "id": "6713c249988aa6bcf3bd40eb8e1a64b5fb8d9eaa",
    "semantic_title": "an empirical study on strong-weak model collaboration for repo-level code generation",
    "citation_count": 0,
    "authors": [
      "Shubham Gandhi",
      "Atharva Naik",
      "Yiqing Xie",
      "Carolyn Rose"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1044": {
    "title": "What are Foundation Models Cooking in the Post-Soviet World?",
    "volume": "main",
    "abstract": "The culture of the Post-Soviet states is complex, shaped by a turbulent history that continues to influence current events. In this study, we investigate the Post-Soviet cultural food knowledge of foundation models by constructing BORSch, a multi-modal dataset encompassing 1147 and 823 dishes in the Russian and Ukrainian languages, centered around the Post-Soviet region. We demonstrate that leading models struggle to correctly identify the origins of dishes from Post-Soviet nations in both text-only and multi-modal Question Answering (QA), instead over-predicting countries linked to the language the question is asked in. Through analysis of pre-training data, we show that these results can be explained by misleading dish-origin co-occurrences, along with linguistic phenomena such as Russian-Ukrainian code mixing. Finally, to move beyond QA-based assessments, we test models' abilities to produce accurate visual descriptions of dishes. The weak correlation between this task and QA suggests that QA alone may be insufficient as an evaluation of cultural understanding",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anton Lavrouk",
      "Tarek Naous",
      "Alan Ritter",
      "Wei Xu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1045": {
    "title": "LogiDynamics: Unraveling the Dynamics of Inductive, Abductive and Deductive Logical Inferences in LLM Reasoning",
    "volume": "main",
    "abstract": "Modern large language models (LLMs) employ diverse logical inference mechanisms for reasoning, making the strategic optimization of these approaches critical for advancing their capabilities. This paper systematically investigate the **comparative dynamics** of inductive (System 1) versus abductive/deductive (System 2) inference in LLMs. We utilize a controlled analogical reasoning environment, varying modality (textual, visual, symbolic), difficulty, and task format (MCQ / free-text). Our analysis reveals System 2 pipelines generally excel, particularly in visual/symbolic modalities and harder tasks, while System 1 is competitive for textual and easier problems. Crucially, task format significantly influences their relative advantage, with System 1 sometimes outperforming System 2 in free-text rule-execution. These core findings generalize to broader in-context learning. Furthermore, we demonstrate that advanced System 2 strategies like hypothesis selection and iterative refinement can substantially scale LLM reasoning. This study offers foundational insights and actionable guidelines for strategically deploying logical inference to enhance LLM reasoning",
    "checked": true,
    "id": "68db9f11bead2940b023d99acf3e186c99ef1888",
    "semantic_title": "logidynamics: unraveling the dynamics of inductive, abductive and deductive logical inferences in llm reasoning",
    "citation_count": 0,
    "authors": [
      "Tianshi Zheng",
      "Cheng Jiayang",
      "Chunyang Li",
      "Haochen Shi",
      "Zihao Wang",
      "Jiaxin Bai",
      "Yangqiu Song",
      "Ginny Wong",
      "Simon See"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1046": {
    "title": "EcoLoRA: Communication-Efficient Federated Fine-Tuning of Large Language Models",
    "volume": "main",
    "abstract": "To address data locality and privacy restrictions, Federated Learning (FL) has recently been adopted to fine-tune large language models (LLMs), enabling improved performance on various downstream tasks without requiring aggregated data. However, the repeated exchange of model updates in FL can result in prohibitively high communication costs, hindering the distributed learning process. To address this challenge, we propose EcoLoRA, a novel communication-efficient federated fine-tuning framework for LLMs. Leveraging the modular structure, we propose a round-robin segment sharing scheme, where each client uploads only a complementary LoRA segment per round to reduce network bandwidth. It is further combined with adaptive sparsification methods tailored to LoRA's training dynamics and lossless encoding techniques. We conduct extensive evaluations on both question-answering and value-alignment tasks across multiple datasets and models. The results show that EcoLoRA significantly reduces communication overhead without compromising performance. For instance, it reduces communication time by up to 79% and total training time by up to 65%",
    "checked": true,
    "id": "a1c2cad865c078685574743768d7b3f76ebf756a",
    "semantic_title": "ecolora: communication-efficient federated fine-tuning of large language models",
    "citation_count": 0,
    "authors": [
      "Han Liu",
      "Ruoyao Wen",
      "Srijith Nair",
      "Jia Liu",
      "Wenjing Lou",
      "Chongjie Zhang",
      "William Yeoh",
      "Yevgeniy Vorobeychik",
      "Ning Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1047": {
    "title": "Memorization ≠ Understanding: Do Large Language Models Have the Ability of Scenario Cognition?",
    "volume": "main",
    "abstract": "Driven by vast and diverse textual data, large language models (LLMs) have demonstrated impressive performance across numerous natural language processing (NLP) tasks. Yet, a critical question persists: does their generalization arise from mere memorization of training data or from deep semantic understanding? To investigate this, we propose a bi-perspective evaluation framework to assess LLMs' scenario cognition—the ability to link semantic scenario elements with their arguments in context. Specifically, we introduce a novel scenario-based dataset comprising diverse textual descriptions of fictional facts, annotated with scenario elements. LLMs are evaluated through their capacity to answer scenario-related questions (model output perspective) and via probing their internal representations for encoded scenario elements-argument associations (internal representation perspective). Our experiments reveal that current LLMs predominantly rely on superficial memorization, failing to achieve robust semantic scenario cognition, even in simple cases. These findings expose critical limitations in LLMs' semantic understanding and offer cognitive insights for advancing their capabilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boxiang Ma",
      "Ru Li",
      "Wang Yuanlong",
      "Hongye Tan",
      "Xiaoli Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1048": {
    "title": "Priority on High-Quality: Selecting Instruction Data via Consistency Verification of Noise Injection",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have demonstrated a remarkable understanding of language nuances through instruction tuning, enabling them to effectively tackle various natural language processing tasks. Recent research has focused on the quality of instruction data rather than the quantity of instructions. However, existing high-quality instruction selection methods rely on external models or rules, overlooking the intrinsic association between pre-trained model and instruction data, making it difficult to select data that align with the preferences of pre-trained model. To address this challenge, we propose a strategy that utilizes noise injection to identify the quality of instruction data, without relying on external model. We also implement the strategy of combining inter-class diversity and intra-class diversity to improve model performance. The experimental results demonstrate that our method significantly outperforms the model trained on the entire dataset and established baselines. Our study provides a new perspective on noise injection in the field of instruction tuning, and also illustrates that the pre-trained model itself should be considered in defining high-quality. Additionally, we publish our selected high-quality instruction data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hong Zhang",
      "Feng Zhao",
      "Ruilin Zhao",
      "Cheng Yan",
      "Kangzheng Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1049": {
    "title": "Can Prompts Rewind Time for LLMs? Evaluating the Effectiveness of Prompted Knowledge Cutoffs",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) are widely used for temporal prediction, but their reliance on pretraining data raises contamination concerns, as accurate predictions on pre-cutoff test data may reflect memorization rather than reasoning, leading to an overestimation of their generalization capability. With the recent emergence of prompting-based unlearning techniques, a natural question arises: Can LLMs be prompted to simulate an earlier knowledge cutoff? In this work, we investigate the capability of prompting to simulate earlier knowledge cutoff in LLMs. We construct three evaluation datasets to assess the extent to which LLMs can forget (1) direct factual knowledge, (2) semantic shifts, and (3) causally related knowledge. Results demonstrate that while prompt-based simulated knowledge cutoffs show effectiveness when directly queried with the information after that date, they struggle to induce forgetting when the forgotten content is not directly asked but causally related to the query. These findings highlight the need for more rigorous evaluation settings when applying LLMs for temporal prediction tasks. The full dataset and evaluation code are available at https://github.com/gxx27/time_unlearn",
    "checked": true,
    "id": "552476e180432d489b6cf62d9f3feafa79a422cf",
    "semantic_title": "can prompts rewind time for llms? evaluating the effectiveness of prompted knowledge cutoffs",
    "citation_count": 0,
    "authors": [
      "Xin Gao",
      "Ruiyi Zhang",
      "Daniel Du",
      "Saurabh Mahindre",
      "Sai Ashish Somayajula",
      "Pengtao Xie"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1050": {
    "title": "DSVD: Dynamic Self-Verify Decoding for Faithful Generation in Large Language Models",
    "volume": "main",
    "abstract": "The reliability of large language models remains a critical challenge, particularly due to their susceptibility to hallucinations and factual inaccuracies during text generation. Existing solutions either underutilize models' self-correction with preemptive strategies or use costly post-hoc verification. To further explore the potential of real-time self-verification and correction, we present Dynamic Self-Verify Decoding (DSVD), a novel decoding framework that enhances generation reliability through real-time hallucination detection and efficient error correction. DSVD integrates two key components: (1) parallel self-verification architecture for continuous quality assessment, (2) dynamic rollback mechanism for targeted error recovery. Extensive experiments across five benchmarks demonstrate DSVD's effectiveness, achieving significant improvement in truthfulness (Quesetion-Answering) and factual accuracy (FActScore). Results show the DSVD can be further incorporated with existing faithful decoding methods to achieve stronger performance. Our work establishes that real-time self-verification during generation offers a viable path toward more trustworthy language models without sacrificing practical deployability",
    "checked": true,
    "id": "37dcc7d8e36e8d911c5672bf277db12fd2df30f7",
    "semantic_title": "dsvd: dynamic self-verify decoding for faithful generation in large language models",
    "citation_count": 1,
    "authors": [
      "YiQiu Guo",
      "Yuchen Yang",
      "Zhe Chen",
      "Pingjie Wang",
      "Yusheng Liao",
      "Ya Zhang",
      "Yanfeng Wang",
      "Yu Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1051": {
    "title": "Metric Calculating Benchmark: Code-Verifiable Complicate Instruction Following Benchmark for Large Language Models",
    "volume": "main",
    "abstract": "Recent frontier-level LLMs have saturated many previously difficult benchmarks, leaving little room for further differentiation. This progress highlights the need for challenging benchmarks that provide objective verification. In this paper, we introduce MCBench, a benchmark designed to evaluate whether LLMs can execute string-matching NLP metrics by strictly following step-by-step instructions. Unlike prior benchmarks that depend on subjective judgments or general reasoning, MCBench offers an objective, deterministic and code-verifiable evaluation. This setup allows us to systematically test whether LLMs can maintain accurate step-by-step execution, including instruction adherence, numerical computation, and long-range consistency in handling intermediate results. To ensure objective evaluation of these abilities, we provide a parallel reference code that can evaluate the accuracy of LLM output. We provide three evaluative metrics and three benchmark variants designed to measure the detailed instruction understanding capability of LLMs. Our analyses show that MCBench serves as an effective and objective tool for evaluating the capabilities of cutting-edge LLMs",
    "checked": true,
    "id": "b462838b7aaf940f79ea157f48f56924e80fd24d",
    "semantic_title": "metric calculating benchmark: code-verifiable complicate instruction following benchmark for large language models",
    "citation_count": 0,
    "authors": [
      "Hyeonseok Moon",
      "Seongtae Hong",
      "Jaehyung Seo",
      "Heuiseok Lim"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1052": {
    "title": "Generative Annotation for ASR Named Entity Correction",
    "volume": "main",
    "abstract": "End-to-end automatic speech recognition systems often fail to transcribe domain-speciffcnamed entities, causing catastrophic failuresin downstream tasks. Numerous fast and lightweight named entity correction (NEC) models have been proposed in recent years. These models, mainly leveraging phonetic-level edit distance algorithms, have shown impressive performances. However, when theforms of the wrongly-transcribed words(s) and the ground-truth entity are signiffcantly different, these methods often fail to locate the wrongly transcribed words in hypothesis, thus limiting their usage. We propose a novel NEC method that utilizes speech sound features to retrieve candidate entities. With speech sound features and candidate entities, we inovatively design a generative method to annotate entityerrors in ASR transcripts and replace the textwith correct entities. This method is effective inscenarios of word form difference. We test ourmethod using open-source and self-constructed test sets. The results demonstrate that our NEC method can bring signiffcant improvement to entity accuracy. We will open source our self constructed test set and training data",
    "checked": true,
    "id": "187c637a0834170dafd7205970543a7c6ca0f68e",
    "semantic_title": "generative annotation for asr named entity correction",
    "citation_count": 0,
    "authors": [
      "Yuanchang Luo",
      "Daimeng Wei",
      "Shaojun Li",
      "Hengchao Shang",
      "Jiaxin Guo",
      "Zongyao Li",
      "Zhanglin Wu",
      "Xiaoyu Chen",
      "Zhiqiang Rao",
      "Jinlong Yang",
      "Hao Yang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1053": {
    "title": "SOLAR: Towards Characterizing Subjectivity of Individuals through Modeling Value Conflicts and Trade-offs",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) not only have solved complex reasoning problems but also exhibit remarkable performance in tasks that require subjective decision-making. Existing studies suggest that LLM generations can convey subjectivity to some extent, yet exploring whether LLMs can account for individual-level subjectivity has not been sufficiently studied. In this paper, we characterize the subjectivity of individuals on social media and infer their moral judgments using LLMs. We propose a framework, SolAr (Subjective Ground with Value Abstraction), that observes value conflicts and trade-offs in the user-generated texts to better represent subjective ground of individuals. Empirical results demonstrate that our framework enhances overall inference performance, with notable improvements for users with limited data and in controversial situations. Additionally, we qualitatively show that SolAr provides explanations about individuals' value preferences, which can further account for their judgments",
    "checked": false,
    "id": "17a483eb8c56473829817afbebf6b96c087c97e8",
    "semantic_title": "towards characterizing subjectivity of individuals through modeling value conflicts and trade-offs",
    "citation_count": 0,
    "authors": [
      "Younghun Lee",
      "Dan Goldwasser"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1054": {
    "title": "LogicTree: Structured Proof Exploration for Coherent and Rigorous Logical Reasoning with Large Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) have achieved remarkable multi-step reasoning capabilities across various domains. However, LLMs still face distinct challenges in complex logical reasoning, as (1) proof-finding requires systematic exploration and the maintenance of logical coherence and (2) searching the right combination of premises at each reasoning step is inherently challenging in tasks with large premise space. To address this, we propose LogicTree, an inference-time modular framework employing algorithm-guided search to automate structured proof exploration and ensure logical coherence. Advancing beyond tree-of-thought (ToT), we incorporate caching mechanism into LogicTree to enable effective utilization of historical knowledge, preventing reasoning stagnation and minimizing redundancy. Furthermore, we address the combinatorial complexity of premise search by decomposing it into a linear process. The refined premise selection restricts subsequent inference to at most one derivation per step, enhancing reasoning granularity and enforcing strict step-by-step reasoning. Additionally, we introduce two LLM-free heuristics for premise prioritization, enabling strategic proof search. Experimental results on five datasets demonstrate that LogicTree optimally scales inference-time computation to achieve higher proof accuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6% and 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o outperforms o3-mini by 7.6% on average",
    "checked": true,
    "id": "37a1490bb31c626e3264a042989f186468020cf8",
    "semantic_title": "logictree: structured proof exploration for coherent and rigorous logical reasoning with large language models",
    "citation_count": 0,
    "authors": [
      "Kang He",
      "Kaushik Roy"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1055": {
    "title": "Unmasking Fake Careers: Detecting Machine-Generated Career Trajectories via Multi-layer Heterogeneous Graphs",
    "volume": "main",
    "abstract": "The rapid advancement of Large Language Models (LLMs) has enabled the generation of highly realistic synthetic data. We identify a new vulnerability, LLMs generating convincing career trajectories in fake resumes and explore effective detection methods. To address this challenge, we construct a dataset of machine-generated career trajectories using LLMs and various methods, and demonstrate that conventional text-based detectors perform poorly on structured career data. We propose CareerScape, a novel heterogeneous, hierarchical multi-layer graph framework that models career entities and their relations in a unified global graph built from genuine resumes. Unlike conventional classifiers that treat each instance independently, CareerScape employs a structure-aware framework that augments user-specific subgraphs with trusted neighborhood information from a global graph, enabling the model to capture both global structural patterns and local inconsistencies indicative of synthetic career paths. Experimental results show that CareerScape outperforms state-of-the-art baselines by 5.8-85.0% relatively, highlighting the importance of structure-aware detection for machine-generated content. Our codebase is available at https://github.com/mickeymst/careerscape",
    "checked": true,
    "id": "62bb2c5dad85426e1a05a20b1c0897f2676cda0f",
    "semantic_title": "unmasking fake careers: detecting machine-generated career trajectories via multi-layer heterogeneous graphs",
    "citation_count": 0,
    "authors": [
      "Michiharu Yamashita",
      "Thanh Tran",
      "Delvin Ce Zhang",
      "Dongwon Lee"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1056": {
    "title": "GAP: a Global Adaptive Pruning Method for Large Language Models",
    "volume": "main",
    "abstract": "The deployment of Large Language Models (LLMs) faces significant challenges due to high computational costs,driving the demand for effective pruning techniques. Existing structured pruning methods employ uniform compression rates across network layers, neglecting the varying importance of different network depths. To address this limitation, we propose a novel optimization framework that directly minimizes global capability loss through layer-adaptive pruning rates. The framework formulates the pruning task as a combinatorial optimization problem constrained by a total parameter budget, and an efficient dynamic programming solution is derived to determine optimal layer-wise compression rates.Experiments demonstrate that, when tuning is not included, our approach achieves comparable performance with state-of-the-art methods at high pruning rates (37-50% reduction), and shows significant advantages at low pruning rates (13-25% reduction). When tuning is included, our method achieves the best performance among the compared methods",
    "checked": true,
    "id": "065f6b8df9d634260a68a384c8976fc219aa3a98",
    "semantic_title": "gap: a global adaptive pruning method for large language models",
    "citation_count": 0,
    "authors": [
      "Zhihua Ban",
      "Haotian Ma",
      "Siheng Zhang",
      "Shengyu Liu",
      "Xichen Chen",
      "Ming Yang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1057": {
    "title": "Distribution Prompting: Understanding the Expressivity of Language Models Through the Next-Token Distributions They Can Produce",
    "volume": "main",
    "abstract": "Autoregressive neural language models (LMs) generate a probability distribution over tokens at each time step given a prompt. In this work, we attempt to systematically understand the probability distributions that LMs can produce, showing that some distributions are significantly harder to elicit than others. Specifically, for any target next-token distribution over the vocabulary, we attempt to find a prompt that induces the LM to output a distribution as close as possible to the target, using either soft or hard gradient-based prompt tuning. We find that (1) in general, distributions with very low or very high entropy are easier to approximate than those with moderate entropy; (2) among distributions with the same entropy, those containing \"outlier tokens\" are easier to approximate; (3) target distributions generated by LMs – even LMs with different tokenizers – are easier to approximate than randomly chosen targets. These results offer insights into the expressiveness of LMs and the challenges of using them as probability distribution proposers",
    "checked": true,
    "id": "1054fde6cb4f8b384d30692e2f2e3eb7c293218e",
    "semantic_title": "distribution prompting: understanding the expressivity of language models through the next-token distributions they can produce",
    "citation_count": 1,
    "authors": [
      "Haojin Wang",
      "Zining Zhu",
      "Freda Shi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1058": {
    "title": "LGA: LLM-GNN Aggregation for Temporal Evolution Attribute Graph Prediction",
    "volume": "main",
    "abstract": "Temporal evolution attribute graph prediction, a key task in graph machine learning, aims to forecast the dynamic evolution of node attributes over time. While recent advances in Large Language Models (LLMs) have enabled their use in enhancing node representations for integration with Graph Neural Networks (GNNs), their potential to directly perform GNN-like aggregation and interaction remains underexplored. Furthermore, traditional approaches to initializing attribute embeddings often disregard structural semantics, limiting the provision of rich prior knowledge to GNNs. Current methods also primarily focus on 1-hop neighborhood aggregation, lacking the capability to capture complex structural interactions. To address these limitations, we propose a novel prediction framework that integrates structural information into attribute embeddings through the introduction of an attribute embedding loss. We design specialized prompts to enable LLMs to perform GNN-like aggregation and incorporate a relation-aware Graph Convolutional Network to effectively capture long-range and complex structural dependencies. Extensive experiments on multiple real-world datasets validate the effectiveness of our approach, demonstrating significant improvements in predictive performance over existing methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feng Zhao",
      "Ruoyu Chai",
      "Kangzheng Liu",
      "Xianggan Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1059": {
    "title": "EIFBENCH: Extremely Complex Instruction Following Benchmark for Large Language Models",
    "volume": "main",
    "abstract": "With the development and widespread application of large language models (LLMs), the new paradigm of \"Model as Product\" is rapidly evolving, and demands higher capabilities to address complex user needs, often requiring precise workflow execution which involves the accurate understanding of multiple tasks. However, existing benchmarks focusing on single-task environments with limited constraints, lack the complexity required to fully reflect To bridge this gap, we present the Extremely Complex Instruction Following Benchmark (EIFBENCH), meticulously crafted to facilitate a more realistic and robust evaluation of LLMs. EIFBENCH not only includes multi-task scenarios that enable comprehensive assessment across diverse task types concurrently, but also integrates a variety of constraints, replicating complex operational environments. Furthermore, we propose the Segment Policy Optimization (SegPO) algorithm to enhance the LLM's ability to accurately fulfill multi-task workflow. Evaluations on EIFBENCH have unveiled considerable performance discrepancies in existing LLMs when challenged with these extremely complex instructions. This finding underscores the necessity for ongoing optimization to navigate the intricate challenges posed by real-world LLM applications",
    "checked": true,
    "id": "1d9c1d192e2b6d17fb19a85c1ee68c9138c95ac3",
    "semantic_title": "eifbench: extremely complex instruction following benchmark for large language models",
    "citation_count": 0,
    "authors": [
      "Tao Zou",
      "Xinghua Zhang",
      "Haiyang Yu",
      "Minzheng Wang",
      "Fei Huang",
      "Yongbin Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1060": {
    "title": "Tool Preferences in Agentic LLMs are Unreliable",
    "volume": "main",
    "abstract": "Large language models (LLMs) can now access a wide range of external tools, thanks to the Model Context Protocol (MCP). This greatly expands their abilities as various agents. However, LLMs rely entirely on the text descriptions of tools to decide which ones to use—a process that is surprisingly fragile. In this work, we expose a vulnerability in prevalent tool/function-calling protocols by investigating a series of edits to tool descriptions, some of which can drastically increase a tool's usage from LLMs when competing with alternatives. Through controlled experiments, we show that tools with properly edited descriptions receive **over 10 times more usage** from GPT-4.1 and Qwen2.5-7B than tools with original descriptions. We further evaluate how various edits to tool descriptions perform when competing directly with one another and how these trends generalize or differ across a broader set of 17 different models. These phenomena, while giving developers a powerful way to promote their tools, underscore the need for a more reliable foundation for agentic LLMs to select and utilize tools and resources. Our code is publicly available at [https://github.com/kazemf78/llm-unreliable-tool-preferences](https://github.com/kazemf78/llm-unreliable-tool-preferences)",
    "checked": true,
    "id": "c3c28427c1e3c2ac5d6513b7e39942b15adaab77",
    "semantic_title": "tool preferences in agentic llms are unreliable",
    "citation_count": 0,
    "authors": [
      "Kazem Faghih",
      "Wenxiao Wang",
      "Yize Cheng",
      "Siddhant Bharti",
      "Gaurang Sriramanan",
      "Sriram Balasubramanian",
      "Parsa Hosseini",
      "Soheil Feizi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1061": {
    "title": "Enhancing Large Language Model for Knowledge Graph Completion via Structure-Aware Alignment-Tuning",
    "volume": "main",
    "abstract": "Knowledge graph completion (KGC) aims to infer new knowledge and make predictions from knowledge graphs. Recently, large language models (LLMs) have exhibited remarkable reasoning capabilities. LLM-enhanced KGC methods primarily focus on designing task-specific instructions, achieving promising advancements. However, there are still two critical challenges. First, existing methods often ignore the inconsistent representation spaces between natural language and graph structures. Second, most approaches develop separate instructions for different KGC tasks, leading to duplicate works and time-consuming processes. To address these challenges, we propose SAT, a novel framework that enhances LLMs for KGC via structure-aware alignment-tuning. Specifically, we first introduce hierarchical knowledge alignment to align graph embeddings with the natural language space through multi-task contrastive learning. Then, we propose structural instruction tuning to guide LLMs in performing structure-aware reasoning over KGs, using a unified graph instruction combined with a lightweight knowledge adapter. Experimental results on two KGC tasks across four benchmark datasets demonstrate that SAT significantly outperforms state-of-the-art methods, especially in the link prediction task with improvements ranging from 8.7% to 29.8%",
    "checked": true,
    "id": "40235f9308abe8b36e42164125d061f82c2d2123",
    "semantic_title": "enhancing large language model for knowledge graph completion via structure-aware alignment-tuning",
    "citation_count": 0,
    "authors": [
      "Yu Liu",
      "Yanan Cao",
      "Xixun Lin",
      "Yanmin Shang",
      "Shi Wang",
      "Shirui Pan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1062": {
    "title": "MultiDocFusion : Hierarchical and Multimodal Chunking Pipeline for Enhanced RAG on Long Industrial Documents",
    "volume": "main",
    "abstract": "RAG-based QA has emerged as a powerful method for processing long industrial documents. However, conventional text chunking approaches often neglect complex and long industrial document structures, causing information loss and reduced answer quality. To address this, we introduce MultiDocFusion, a multimodal chunking pipeline that integrates: (i) detection of document regions using vision-based document parsing, (ii) text extraction from these regions via OCR, (iii) reconstruction of document structure into a hierarchical tree using large language model (LLM)-based document section hierarchical parsing (DSHP-LLM), and (iv) construction of hierarchical chunks through DFS-based grouping. Extensive experiments across industrial benchmarks demonstrate that MultiDocFusion improves retrieval precision by 8–15% and ANLS QA scores by 2–3% compared to baselines, emphasizing the critical role of explicitly leveraging document hierarchy for multimodal document-based QA. These significant performance gains underscore the necessity of structure-aware chunking in enhancing the fidelity of RAG-based QA systems",
    "checked": true,
    "id": "f4f3f34e3d1c4c86c6de6b99e3a5de041fc31fb0",
    "semantic_title": "multidocfusion : hierarchical and multimodal chunking pipeline for enhanced rag on long industrial documents",
    "citation_count": 0,
    "authors": [
      "Joongmin Shin",
      "Chanjun Park",
      "Jeongbae Park",
      "Jaehyung Seo",
      "Heuiseok Lim"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1063": {
    "title": "Attention-guided Self-reflection for Zero-shot Hallucination Detection in Large Language Models",
    "volume": "main",
    "abstract": "Hallucination has emerged as a significant barrier to the effective application of Large Language Models (LLMs). In this work, we introduce a novel Attention-Guided SElf-Reflection (AGSER) approach for zero-shot hallucination detection in LLMs. The AGSER method utilizes attention contributions to categorize the input query into attentive and non-attentive queries. Each query is then processed separately through the LLMs, allowing us to compute consistency scores between the generated responses and the original answer. The difference between the two consistency scores serves as a hallucination estimator. In addition to its efficacy in detecting hallucinations, AGSER notably reduces computational complexity, requiring only three passes through the LLM and utilizing two sets of tokens. We have conducted extensive experiments with four widely-used LLMs across three different hallucination benchmarks, demonstrating that our approach significantly outperforms existing methods in zero-shot hallucination detection",
    "checked": true,
    "id": "e33fceb7cfb825ae3c530de0bf093769169039fc",
    "semantic_title": "attention-guided self-reflection for zero-shot hallucination detection in large language models",
    "citation_count": 7,
    "authors": [
      "Qiang Liu",
      "Xinlong Chen",
      "Yue Ding",
      "Bowen Song",
      "Weiqiang Wang",
      "Shu Wu",
      "Liang Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1064": {
    "title": "‘Rich Dad, Poor Lad': How do Large Language Models Contextualize Socioeconomic Factors in College Admission ?",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) are increasingly involved in high-stakes domains, yet how they reason about socially-sensitive decisions still remain underexplored. We present a large-scale audit of LLMs' treatment of socioeconomic status (SES) in college admissions decisions using a novel dual-process framework inspired by cognitive science. Leveraging a synthetic dataset of 30,000 applicant profiles grounded in real-world correlations, we prompt 4 open-source LLMs (Qwen 2, Mistral v0.3, Gemma 2, Llama 3.1) under 2 modes: a fast, decision-only setup (System 1) and a slower, explanation-based setup (System 2). Results from 5 million prompts reveals that LLMs consistently favor low-SES applicants—even when controlling for academic performance—and that System 2 amplifies this tendency by explicitly invoking SES as compensatory justification, highlighting both their potential and volatility as decision-makers. We then propose DPAF, a dual-process audit framework to probe LLMs' reasoning behaviors in sensitive applications",
    "checked": false,
    "id": "bc706e1c98b650be1fe5d8d91004a71de62a1382",
    "semantic_title": "rich dad, poor lad': how do large language models contextualize socioeconomic factors in college admission ?",
    "citation_count": 1,
    "authors": [
      "Huy Nghiem",
      "Phuong-Anh Nguyen-Le",
      "John Prindle",
      "Rachel Rudinger",
      "Hal Daumé Iii"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1065": {
    "title": "Understanding and Mitigating Overrefusal in LLMs from an Unveiling Perspective of Safety Decision Boundary",
    "volume": "main",
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks, yet they often refuse to answer legitimate queries—a phenomenon known as overrefusal. Overrefusal typically stems from over-conservative safety alignment, causing models to treat many reasonable prompts as potentially risky. To systematically understand this issue, we probe and leverage the models' safety decision boundaries to analyze and mitigate overrefusal. Our findings reveal that overrefusal is closely tied to misalignment at these boundary regions, where models struggle to distinguish subtle differences between benign and harmful content. Building on these insights, we present **RASS**, an automated framework for prompt generation and selection that strategically targets overrefusal prompts near the safety boundary. By harnessing steering vectors in the representation space, **RASS** efficiently identifies and curates boundary-aligned prompts, enabling more effective and targeted mitigation of overrefusal. This approach not only provides a more precise and interpretable view of model safety decisions but also seamlessly extends to multilingual scenarios. We have explored the safety decision boundaries of various LLMs and construct the **MORBench** evaluation set to facilitate robust assessment of model safety and helpfulness across multiple languages. Code and datasets are available at https://github.com/Master-PLC/RASS",
    "checked": true,
    "id": "abf4aa19e956445b27e10db1025468cfc64dbe4b",
    "semantic_title": "understanding and mitigating overrefusal in llms from an unveiling perspective of safety decision boundary",
    "citation_count": 1,
    "authors": [
      "Licheng Pan",
      "Yongqi Tong",
      "Xin Zhang",
      "Xiaolu Zhang",
      "Jun Zhou",
      "Zhixuan Chu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1066": {
    "title": "MMAG: Multimodal Learning for Mucus Anomaly Grading in Nasal Endoscopy via Semantic Attribute Prompting",
    "volume": "main",
    "abstract": "Accurate grading of rhinitis severity in nasal endoscopy relies heavily on the characterization of key secretion types, notably clear nasal discharge (CND) and purulent nasal secretion (PUS). However, both exhibit ambiguous appearance and high structural variability, posing challenges to automated grading under weak supervision. To address this, we propose Multimodal Learning for Mucus Anomaly Grading (MMAG), which integrates structured prompts with rank-aware vision-language modeling for joint detection and grading. Attribute prompts are constructed from clinical descriptors (e.g., secretion type, severity, location) and aligned with multi-level visual features via a dual-branch encoder. During inference, the model localizes mucus anomalies and maps the input image to severity-specific prompts (e.g., \"moderate pus\"), projecting them into a rank-aware feature space for progressive similarity scoring.Extensive evaluations on CND and PUS datasets show that our method achieves consistent gains over Baseline, improving AUC by 6.31% and 4.79%, and F1 score by 12.85% and 6.03%, respectively.This framework enables interpretable, annotation-efficient, and semantically grounded assessment of rhinitis severity based on mucus anomalies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinpan Yuan",
      "Mingzhu Huang",
      "Liujie Hua",
      "Jianuo Ju",
      "Xu Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1067": {
    "title": "The Emperor's New Reasoning: Format Imitation Overshadows Genuine Mathematical Understanding in SFT",
    "volume": "main",
    "abstract": "Recent advances in large language models (LLMs) have yielded impressive gains on mathematical reasoning benchmarks via supervised fine-tuning (SFT). However, the brittleness of these models under input perturbations has cast doubt on whether such improvements reflect genuine reasoning abilities or merely superficial alignment with expected output formats. We investigate the mechanisms behind SFT improvements in small-scale LLMs, addressing four key questions: (1) Are performance gains primarily due to format alignment rather than reasoning? (2) Can high-quality supervision encourage genuine reasoning? (3) Does scaling data shift learning from format alignment to deeper reasoning? (4) Are format alignment gains consistent across model sizes and architectures? Through controlled experiments, we find that most performance improvements arise from format alignment rather than genuine reasoning enhancement. Moreover, SFT's effectiveness is strongly influenced by the alignment between the base model's inductive biases and the teacher model's output distribution, rather than the teacher's raw strength. Finally, scaling up training data offers diminishing returns and does not fundamentally alter the model's reasoning behavior. These findings suggest that current SFT practices may overestimate the reasoning abilities of LLMs and underscore the need for more rigorous evaluation methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linyao Yang",
      "Jian-Tao Huang",
      "Yafei Lu",
      "Zhenhui Jessie Li",
      "Guirong Xue"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1068": {
    "title": "Step Guided Reasoning: Improving Mathematical Reasoning using Guidance Generation and Step Reasoning",
    "volume": "main",
    "abstract": "Mathematical reasoning has been challenging for large language models (LLMs), and the introduction of step-by-step Chain-of-Thought (CoT) inference has significantly advanced the mathematical capabilities of LLMs. However, current approaches either necessitate extensive inference datasets for training or depend on few-shot methods that frequently compromise computational accuracy. To address these fundamental limitations, we propose Step Guided Reasoning, a novel training-free adaptation framework that efficiently equips general-purpose pre-trained language models with enhanced mathematical reasoning capabilities. In this approach, LLMs reflect on small reasoning steps, similar to how humans deliberate and focus attention on what to do next. By incorporating this reflective process into the inference stage, LLMs can effectively guide their reasoning from one step to the next. Through extensive experiments, we demonstrate the significant effect of Step Guided Reasoning in enhancing mathematical performance in state-of-the-art language models – Qwen2-72B-Instruct outperforms its math-specific counterpart, Qwen2.5-72B-Math-Instruct, on MMLU-STEM with a score of 90.9%, compared to 87.3%. The average scores of Qwen2-7B-Instruct and Qwen2-72B-Instruct increase from 27.1% to 36. 3% and from 36. 5% to 47.4% in the math domain, respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lang Cao",
      "Yingtian Zou",
      "Chao Peng",
      "Renhong Chen",
      "Wu Ning",
      "Yitong Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1069": {
    "title": "Flexibly Utilize Memory for Long-Term Conversation via a Fragment-then-Compose Framework",
    "volume": "main",
    "abstract": "Large language models (LLMs) have made significant breakthroughs in extracting useful information from conversation history to enhance the response in long-term conversations. Summarizing useful information from historical conversations has achieved remarkable performance, which, however, may introduce irrelevant or redundant information, making it difficult to flexibly choose and integrate key information from different sessions during memory retrieval. To address this issue, we propose a Fragment-then-Compose framework, a novel memory utilization approach for long-term open-domain conversation, called *FraCom*. To be specific, inspired by the concept of proposition representation from Cognitive Psychology, we first represent the conversation history as a series of predicates plus arguments for propositional representation to preserve key information useful for memory (\"**Fragment**\"). Then, we compose propositional graphs for the conversation history based on the connection between shared arguments (\"**Compose**\"). During retrieval, we retrieve relevant propositions from the graph based on arguments from the current query. This essentially allows for flexible and effective utilization of related information in long-term memory for better response generation towards a query. Experimental results on four long-term open-domain conversation datasets demonstrate the effectiveness of our *FraCom* in memory utilization and its ability to enhance response generation for LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cai Ke",
      "Yiming Du",
      "Bin Liang",
      "Yifan Xiang",
      "Lin Gui",
      "Zhongyang Li",
      "Baojun Wang",
      "Yue Yu",
      "Hui Wang",
      "Kam-Fai Wong",
      "Ruifeng Xu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1070": {
    "title": "STRICT: Stress-Test of Rendering Image Containing Text",
    "volume": "main",
    "abstract": "While diffusion models have revolutionized text-to-image generation with their ability to synthesize realistic and diverse scenes, they continue to struggle with generating consistent and legible text within images. This shortcoming is commonly attributed to the locality bias inherent in diffusion-based generation, which limits their capacity to model long-range spatial dependencies. In this paper, we introduce STRICT, a benchmark designed to systematically stress-test the ability of diffusion models to render coherent and instruction-aligned text in images. Our benchmark evaluates models across multiple dimensions: (1) the maximum length of readable text that can be generated and (2) the correctness and legibility of the generated text. We assess several state-of-the-art models, including proprietary and open-source variants, and reveal persistent limitations in long-range consistency and instruction-following capabilities. Our findings provide insights into architectural bottlenecks and motivate future research directions in multimodal generative modeling",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyu Zhang",
      "Xinyu Wang",
      "Lu Li",
      "Zhenghan Tai",
      "Jijun Chi",
      "Jingrui Tian",
      "Hailin He",
      "Suyuchen Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1071": {
    "title": "A Sequential Multi-Stage Approach for Code Vulnerability Detection via Confidence- and Collaboration-based Decision Making",
    "volume": "main",
    "abstract": "While large language models (LLMs) have shown strong capabilities across diverse domains, their application to code vulnerability detection holds great potential for identifying security flaws and improving software safety. In this paper, we propose a sequential multi-stage approach via confidence- and collaboration-based decision making (ConfColl). The system adopts a three-stage sequential classification framework, proceeding through a single agent, retrieval-augmented generation (RAG) with external examples, and multi-agent reasoning enhanced with RAG. The decision process selects among these strategies to balance performance and cost, with the process terminating at any stage where a high-certainty prediction is achieved. Experiments on a benchmark dataset and a low-resource language demonstrate the effectiveness of our framework in enhancing code vulnerability detection performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chung-Nan Tsai",
      "Xin Wang",
      "Cheng-Hsiung Lee",
      "Ching-Sheng Lin"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1072": {
    "title": "Leveraging Large Models to Evaluate Novel Content: A Case Study on Advertisement Creativity",
    "volume": "main",
    "abstract": "Evaluating creativity is challenging, even for humans, not only because of its subjectivity but also because it involves complex cognitive processes. Inspired by work in marketing, we attempt to break down visual advertisement creativity into atypicality and originality. With fine-grained human annotations on these dimensions, we propose a suite of tasks specifically for such a subjective problem. We also evaluate the alignment between state-of-the-art (SoTA) vision language models (VLMs) and humans on our proposed benchmark, demonstrating both the promises and challenges of using VLMs for automatic creativity assessment",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoyi Joey Hou",
      "Adriana Kovashka",
      "Xiang Lorraine Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1073": {
    "title": "BIRD: Bronze Inscription Restoration and Dating",
    "volume": "main",
    "abstract": "Bronze inscriptions from early China are fragmentary and difficult to date. We introduce BIRD (Bronze Inscription Restoration and Dating), a fully encoded dataset grounded in standard scholarly transcriptions and chronological labels. We further propose an allograph-aware masked language modeling framework that integrates domain- and task-adaptive pretraining with a Glyph Net (GN), which links graphemes and allographs. Experiments show that GN improves restoration, while glyph-biased sampling yields gains in dating",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjie Hua",
      "Hoang H Nguyen",
      "Gangyan Ge"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1074": {
    "title": "DCP: Dual-Cue Pruning for Efficient Large Vision-Language Models",
    "volume": "main",
    "abstract": "Large Vision-Language Models (LVLMs) achieve remarkable performance in multimodal tasks but suffer from high computational costs due to the large number of visual tokens. Existing pruning methods either apply after visual tokens enter the LLM or perform pre-pruning based solely on visual attention. Both fail to balance efficiency and semantic alignment, as post-pruning incurs redundant computation, while visual-only pre-pruning overlooks multimodal relevance.To address this limitation, we propose Dual-Cue Pruning (DCP), a novel cross-modal pruning framework that jointly considers textual semantics and visual self-attention. DCP consists of a text-aware computation module, which employs a gradient-weighted attention mechanism to enhance text-visual alignment, and an image-aware computation module, which utilizes deep-layer self-attention distributions to retain essential structural information. By integrating both cues, DCP adaptively selects the most informative visual tokens, achieving efficient inference acceleration while maintaining strong task performance. Experimental results show that DCP can retain only 25% of the visual tokens, with a minimal performance degradation of only 0.063% on LLaVA-1.5-13B, demonstrating its effectiveness in balancing efficiency and accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Jiang",
      "Zixun Zhang",
      "Yuting Zeng",
      "Chunzhao Xie",
      "Tongxuan Liu",
      "Zhen Li",
      "Lechao Cheng",
      "Xiaohua Xu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1075": {
    "title": "Improving Context Fidelity via Native Retrieval-Augmented Reasoning",
    "volume": "main",
    "abstract": "Large language models (LLMs) often struggle with context fidelity, producing inconsistent answers when responding to questions based on provided information. Existing approaches either rely on expensive supervised fine-tuning to generate evidence post-answer or train models to perform web searches without necessarily improving utilization of the given context. We propose CARE, a novel native retrieval-augmented reasoning framework that teaches LLMs to explicitly integrate in-context evidence within their reasoning process with the model's own retrieval capabilities. Our method requires limited labeled evidence data while significantly enhancing both retrieval accuracy and answer generation performance through strategically retrieved in-context tokens in the reasoning chain. Extensive experiments on multiple real-world and counterfactual QA benchmarks demonstrate that our approach substantially outperforms supervised fine-tuning, traditional retrieval-augmented generation methods, and external retrieval solutions. This work represents a fundamental advancement in making LLMs more accurate, reliable, and efficient for knowledge-intensive tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suyuchen Wang",
      "Jinlin Wang",
      "Xinyu Wang",
      "Shiqi Li",
      "Xiangru Tang",
      "Sirui Hong",
      "Xiao-Wen Chang",
      "Chenglin Wu",
      "Bang Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1076": {
    "title": "Koel-TTS: Enhancing LLM based Speech Generation with Preference Alignment and Classifier Free Guidance",
    "volume": "main",
    "abstract": "Autoregressive speech token generation models produce speech with remarkable variety and naturalness but often suffer from hallucinations and undesired vocalizations that do not conform to conditioning inputs. To address these challenges, we introduce Koel-TTS, an encoder-decoder transformer model for multilingual TTS that improves contextual adherence of speech generation LLMs through preference alignment and classifier-free guidance (CFG). For preference alignment, we design a reward system that ranks model outputs using automatic metrics derived from speech recognition and speaker verification models, encouraging generations that better match the input text and speaker identity. CFG further allows fine-grained control over the influence of conditioning inputs during inference by interpolating conditional and unconditional logits. Notably, applying CFG to a preference-aligned model yields additional gains in transcription accuracy and speaker similarity, demonstrating the complementary benefits of both techniques. Koel-TTS achieves state-of-the-art results in zero-shot TTS, outperforming prior LLM-based models on intelligibility, speaker similarity, and naturalness, despite being trained on significantly less data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shehzeen Samarah Hussain",
      "Paarth Neekhara",
      "Xuesong Yang",
      "Edresson Casanova",
      "Subhankar Ghosh",
      "Roy Fejgin",
      "Mikyas T. Desta",
      "Rafael Valle",
      "Jason Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1077": {
    "title": "Mixing Inference-time Experts for Enhancing LLM Reasoning",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have demonstrated impressive reasoning abilities, but their generated rationales often suffer from issues such as reasoning inconsistency and factual errors, undermining their reliability. Prior work has explored improving rationale quality via multi-reward fine-tuning or reinforcement learning (RL), where models are optimized for diverse objectives. While effective, these approaches train the model in a fixed manner and do not have any inference-time adaptability, nor can they generalize reasoning requirements for new test-time inputs. Another approach is to train specialized reasoning experts using reward signals and use them to improve generation at inference time. Existing methods in this paradigm are limited to using only a single expert and cannot improve upon multiple reasoning aspects. To address this, we propose MIXIE, a novel inference-time expert-mixing framework that dynamically determines mixing proportions for each expert, enabling contextualized and flexible fusion. We demonstrate the effectiveness of MIXIE on improving chain-of-thought reasoning in LLMs by merging commonsense and entailment reasoning experts finetuned on reward-filtered data. Our approach outperforms existing baselines on three question-answering datasets: StrategyQA, CommonsenseQA, and ARC, highlighting its potential to enhance LLM reasoning with efficient, adaptable expert integration",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soumya Sanyal",
      "Tianyi Xiao",
      "Xiang Ren"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1078": {
    "title": "Reinforced Query Reasoners for Reasoning-intensive Retrieval Tasks",
    "volume": "main",
    "abstract": "Traditional information retrieval (IR) methods excel at textual and semantic matching but struggle in reasoning-intensive retrieval tasks that require multi-hop inference or complex semantic understanding between queries and documents. One promising solution is to explicitly rewrite or augment queries using large language models (LLMs) to elicit reasoning-relevant content prior to retrieval. However, the widespread use of large-scale LLMs like GPT-4 or LLaMA3-70B remains impractical due to their high inference cost and limited deployability in real-world systems. In this work, we introduce Reinforced Query Reasoner (RQR), a family of small-scale language models for query reasoning and rewriting in reasoning-intensive retrieval. Our approach frames query reformulation as a reinforcement learning problem and employs a novel semi-rule-based reward function. This enables smaller language models, e.g., Qwen2.5-7B-Instruct and Qwen2.5-1.5B-Instruct, to achieve reasoning performance rivaling large-scale LLMs without their prohibitive inference costs. Experiment results on BRIGHT benchmark show that, with BM25 as retrievers, both RQR-7B and RQR-1.5B models significantly outperform existing baselines, including prompt-based query reasoners and some latest dense retrievers trained for reasoning-intensive retrieval tasks, offering superior adaptability for real-world deployment. All code and dataset will be publicly released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xubo Qin",
      "Jun Bai",
      "Jiaqi Li",
      "Zixia Jia",
      "Zilong Zheng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1079": {
    "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation for LLMs via Dynamic Token-Level KV Cache Selection",
    "volume": "main",
    "abstract": "Rapid advances in Large Language Models (LLMs) have spurred demand for processing extended context sequences in contemporary applications. However, this progress faces two challenges: performance degradation due to sequence lengths out-of-distribution, and excessively long inference times caused by the quadratic computational complexity of attention. These issues limit LLMs in long-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache Selection (*TokenSelect*), a training-free method for efficient and accurate long-context inference. *TokenSelect* builds upon the observation of non-contiguous attention sparsity, using QK dot products to measure per-head KV Cache criticality at token-level. By per-head soft voting mechanism, *TokenSelect* selectively involves a few critical KV cache tokens in attention calculation without sacrificing accuracy. To further accelerate *TokenSelect*, we design the Selection Cache based on observations of consecutive Query similarity and implemented the efficient Paged Dot Product Kernel, significantly reducing the selection overhead. A comprehensive evaluation of *TokenSelect* demonstrates up to 23.84× speedup in attention computation and up to 2.28× acceleration in end-to-end latency, while providing superior performance compared to state-of-the-art long-context inference methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Wu",
      "Zhuoshi Pan",
      "Kun Fu",
      "Chao Wang",
      "Liyi Chen",
      "Yunchu Bai",
      "Tianfu Wang",
      "Zheng Wang",
      "Hui Xiong"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1080": {
    "title": "MUSE: MCTS-Driven Red Teaming Framework for Enhanced Multi-Turn Dialogue Safety in Large Language Models",
    "volume": "main",
    "abstract": "As large language models (LLMs) become widely adopted, ensuring their alignment with human values is crucial to prevent jailbreaks where adversaries manipulate models to produce harmful content. While most defenses target single-turn attacks, real-world usage often involves multi-turn dialogues, exposing models to attacks that exploit conversational context to bypass safety measures. We introduce MUSE, a comprehensive framework tackling multi-turn jailbreaks from both attack and defense angles. For attacks, we propose MUSE-A, a method that uses frame semantics and heuristic tree search to explore diverse semantic trajectories. For defense, we present MUSE-D, a fine-grained safety alignment approach that intervenes early in dialogues to reduce vulnerabilities. Extensive experiments on various models show that MUSE effectively identifies and mitigates multi-turn vulnerabilities. Code is available at https://anonymous.4open.science/r/MUSE-75F7",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyu Yan",
      "Long Zeng",
      "Xuecheng Wu",
      "Chengcheng Han",
      "Kongcheng Zhang",
      "Chong Peng",
      "Xuezhi Cao",
      "Xunliang Cai",
      "Chenjuan Guo"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1081": {
    "title": "EnAnchored-X2X: English-Anchored Optimization for Many-to-Many Translation",
    "volume": "main",
    "abstract": "Large language models (LLMs) have demonstrated strong machine translation capabilities for English-centric language pairs but underperform in direct non-English (x2x) translation. This work addresses this limitation through a synthetic data generation framework that leverages models' established English-to-x (en2x) capabilities. By extending English parallel corpora into omnidirectional datasets and developing an English-referenced quality evaluation proxy, we enable effective collection of high-quality x2x training data. Combined with preference-based optimization, our method achieves significant improvement across 72 x2x directions for widely used LLMs, while generalizing to enhance en2x performance. The results demonstrate that strategic exploitation of English-centric strengths can bootstrap comprehensive multilingual translation capabilities in LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sen Yang",
      "Yu Bao",
      "Yu Lu",
      "Jiajun Chen",
      "Shujian Huang",
      "Shanbo Cheng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1082": {
    "title": "I've Decided to Leak\": Probing Internals Behind Prompt Leakage Intents",
    "volume": "main",
    "abstract": "Large language models (LLMs) exhibit prompt leakage vulnerabilities, where they may be coaxed into revealing system prompts embedded in LLM services, raising intellectual property and confidentiality concerns. An intriguing question arises: Do LLMs genuinely internalize prompt leakage intents in their hidden states before generating tokens? In this work, we use probing techniques to capture LLMs' intent-related internal representations and confirm that the answer is yes. We start by comprehensively inducing prompt leakage behaviors across diverse system prompts, attack queries, and decoding methods. We develop a hybrid labeling pipeline, enabling the identification of broader prompt leakage behaviors beyond mere verbatim leaks. Our results show that a simple linear probe can predict prompt leakage risks from pre-generation hidden states without generating any tokens. Across all tested models, linear probes consistently achieve 90%+ AUROC, even when applied to new system prompts and attacks. Understanding the model internals behind prompt leakage drives practical applications, including intention-based detection of prompt leakage risks. Code is available at: https://github.com/jianshuod/Probing-leak-intents",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianshuo Dong",
      "Yutong Zhang",
      "Liu Yan",
      "Zhenyu Zhong",
      "Tao Wei",
      "Ke Xu",
      "Minlie Huang",
      "Chao Zhang",
      "Han Qiu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1083": {
    "title": "Nullspace Disentanglement for Red Teaming Language Models",
    "volume": "main",
    "abstract": "With the widespread deployment of generative language models, concerns about safety issues have continuously grown. High-quality fine-tuning data generated from red teaming plays a crucial role in the model's safety. Recently, automated red teaming approaches have been proposed to create test cases. However, these approaches, which rely on open-ended generation, encounter issues related to inefficiency and low attack success rates. In this work, we introduce a black-box approach that ingeniously exploits the unique properties of the nullspace to disentangle and regulate the crucial success information within test cases. Our study provides a brand-new perspective for automated red team research. Experimental results demonstrate that our approach outperforms baseline methods regarding the attack success rate. The generated test cases also excel in aspects of diversity and fluency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Han",
      "Yuanxing Liu",
      "Weinan Zhang",
      "Ting Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1084": {
    "title": "Supervised Attention Mechanism for Low-quality Multimodal Data",
    "volume": "main",
    "abstract": "In practical applications, multimodal data are often of low quality, with noisy modalities and missing modalities being typical forms that severely hinder model performance, robustness, and applicability. However, current studies address these issues separately. To this end, we propose a framework for multimodal affective computing that jointly addresses missing and noisy modalities to enhance model robustness in low-quality data scenarios. Specifically, we view missing modality as a special case of noisy modality, and propose a supervised attention framework. In contrast to traditional attention mechanisms that rely on main task loss to update the parameters, we design supervisory signals for the learning of attention weights, ensuring that attention mechanisms can focus on discriminative information and suppress noisy information. We further propose a ranking-based optimization strategy to compare the relative importance of different interactions by adding a ranking constraint for attention weights, avoiding training noise caused by inaccurate absolute labels. The proposed model consistently outperforms state-of-the-art baselines on multiple datasets under the settings of complete modalities, missing modalities, and noisy modalities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sijie Mai",
      "Shiqin Han",
      "Haifeng Hu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1085": {
    "title": "Reinforcement Learning for Large Language Models via Group Preference Reward Shaping",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) require alignment via reinforcement learning (RL) to effectively perform task-specific objectives, such as human preference alignment and enhanced reasoning. While Proximal Policy Optimization (PPO) is widely adopted, its computational overhead, stemming from additional value model requirements, limits applicability. Existing alternatives, like Group Relative Policy Optimization (GRPO), mitigate computational costs but remain sensitive to reward model quality. To address this, we introduce Group Preference Reward Shaping (GPRS), a novel method that leverages preference-based comparisons rather than precise numerical rewards. GPRS requires no extra model components and remains robust across varying reward model sizes and qualities. Extensive experiments demonstrate that GPRS consistently outperforms existing critic-model-free RL algorithms in Reinforcement Learning from Human Feedback (RLHF) and reasoning tasks, providing stable and good alignment performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huaisheng Zhu",
      "Siyuan Xu",
      "Hangfan Zhang",
      "Teng Xiao",
      "Zhimeng Guo",
      "Shijie Zhou",
      "Shuyue Hu",
      "Vasant G. Honavar"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1086": {
    "title": "zFLoRA: Zero-Latency Fused Low-Rank Adapters",
    "volume": "main",
    "abstract": "Large language models (LLMs) are increasingly deployed with task-specific adapters catering to multiple downstream applications. In such a scenario, the additional compute associated with these apparently insignificant number of adapter parameters (typically less than 1% of the base model) turns out to be disproportionately significant during inference time (up to 2.5x times that of the base model). In this paper, we propose a new zero-latency fused low-rank adapter (zFLoRA) that introduces zero or negligible latency overhead on top of the base model. Experimental results on LLMs of size 1B, 3B and 7B show that zFLoRA compares favorably against the popular supervised fine-tuning benchmarks including low-rank adapters (LoRA) as well as full fine-tuning (FFT). Experiments are conducted on 18 different tasks across three different categories namely commonsense reasoning, math reasoning and summary-dialogue. Latency measurements made on NPU (Samsung Galaxy S25+) as well as GPU (NVIDIA H100) platforms show that the proposed zFLoRA adapters introduce zero to negligible latency overhead",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dhananjaya Gowda",
      "Seoha Song",
      "Harshith Goka",
      "Junhyun Lee"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1087": {
    "title": "PLAN-TUNING: Post-Training Language Models to Learn Step-by-Step Planning for Complex Problem Solving",
    "volume": "main",
    "abstract": "Recently, decomposing complex problems into simple subtasks–a crucial part of human-like natural planning–to solve the given problem has significantly boosted the performance of large language models (LLMs). However, leveraging such planning structures during post-training to boost the performance of smaller open-source LLMs remains underexplored. Motivated by this, we introduce PLAN-TUNING, a unified post-training framework that (i) distills synthetic task decompositions (termed \"planning trajectories\") from large-scale LLMs and (ii) fine-tunes smaller models via supervised and reinforcement-learning objectives designed to mimic these planning processes to improve complex reasoning. On GSM8k and the MATH benchmarks, plan-tuned models outperform strong baselines by an average ~7%. Furthermore, plan-tuned models show better generalization capabilities on out-of-domain datasets, with average ~10% and ~12% performance improvements on OlympiadBench and AIME 2024, respectively. Our detailed analysis demonstrates how planning trajectories improves complex reasoning capabilities, showing that PLAN-TUNING is an effective strategy for improving task-specific performance of smaller LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mihir Parmar",
      "Palash Goyal",
      "Xin Liu",
      "Yiwen Song",
      "Mingyang Ling",
      "Chitta Baral",
      "Hamid Palangi",
      "Tomas Pfister"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1088": {
    "title": "Semantic Inversion, Identical Replies: Revisiting Negation Blindness in Large Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) often fail to capture semantic changes in queries due to negation, and generate incorrect responses. Negation frequently exists in the real world and is useful for understanding the opposite or absence of a statement, so it is an essential element in logical reasoning. Previous studies have explored LLMs' ability to capture negations ‘separately' from their ability to properly ground knowledge for positive queries. However, this perspective is limited in that it cannot clearly distinguish whether the cause of incorrect responses is the logical incoherence caused by negations or the lack of grounding ability for the given context. To address this issue, we focus on the phenomenon of the model failing to capture semantic contradictions in negated queries despite its accurate understanding of knowledge about positive queries. We term this phenomenon negation blindness on the query. We propose a verification framework that includes task design and measurement methods to verify this issue. In detail, we establish two criteria for systematic task design–i) ‘complexity' and ii) ‘constrainedness'–and devise four verification tasks accordingly. Moreover, we analyze the results extensively and provide insights into problem alleviation feasibility through experiments on various approaches. Our code and resources can be found at https://www.github.com/jin62304/NegationBlindness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinsung Kim",
      "Seonmin Koo",
      "Heuiseok Lim"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1089": {
    "title": "AMACE: Automatic Multi-Agent Chart Evolution for Iteratively Tailored Chart Generation",
    "volume": "main",
    "abstract": "Many statistical facts are conveyed through charts. While various methods have emerged for chart understanding, chart generation typically requires users to manually input code, intent, and other parameters to obtain the desired format on chart generation tools. Recently, the advent of image-generating Large Language Models has facilitated chart generation; however, even this process often requires users to provide numerous constraints for accurate results. In this paper, we propose a loop-based framework for automatically evolving charts in a multi-agent environment. Within this framework, three distinct agents—Chart Code Generator, Chart Replier, and Chart Quality Evaluator—collaborate for iterative, user-tailored chart generation using large language models. Our approach demonstrates an improvement of up to 29.97% in performance compared to first generation, while also reducing generation time by up to 86.9% compared to manual prompt-based methods, showcasing the effectiveness of this multi-agent collaboration in enhancing the quality and efficiency of chart generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyuk Namgoong",
      "Jeesu Jung",
      "Hyeonseok Kang",
      "Yohan Lee",
      "Sangkeun Jung"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1090": {
    "title": "ActionStudio: A Lightweight Framework for Data and Training of Large Action Models",
    "volume": "main",
    "abstract": "Large Action models are essential for enabling autonomous agents to perform complex tasks. However, training such models remains challenging due to the diversity of agent environments and the complexity of noisy agentic data. Existing infrastructure offers limited support for scalable, agent-specific fine-tuning and standardized agent data processing. We introduce ActionStudio, a lightweight and extensible data and training framework designed for large action models. ActionStudio unifies diverse agent trajectories using our proposed Unified Format 2.0, supports a range of training workflows with optimized multi-node distributed setup, and integrates robust preprocessing and real-time verification tools. ActionStudio demonstrates up to 9× higher throughput compared to existing agentic training frameworks, and our trained models yield top performances across public and realistic agent benchmarks. To support the broader research community, we open-source the ActionStudio framework and release actionstudio-98k, a curated dataset of 98k high-quality trajectories",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianguo Zhang",
      "Thai Quoc Hoang",
      "Ming Zhu",
      "Zuxin Liu",
      "Shiyu Wang",
      "Tulika Manoj Awalgaonkar",
      "Akshara Prabhakar",
      "Haolin Chen",
      "Weiran Yao",
      "Zhiwei Liu",
      "Juntao Tan",
      "Juan Carlos Niebles",
      "Shelby Heinecke",
      "Huan Wang",
      "Silvio Savarese",
      "Caiming Xiong"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1091": {
    "title": "Interpretation Meets Safety: A Survey on Interpretation Methods and Tools for Improving LLM Safety",
    "volume": "main",
    "abstract": "As large language models (LLMs) see wider real-world use, understanding and mitigating their unsafe behaviors is critical. Interpretation techniques can reveal causes of unsafe outputs and guide safety, but such connections with safety are often overlooked in prior surveys. We present the first survey that bridges this gap, introducing a unified framework that connects safety-focused interpretation methods, the safety enhancements they inform, and the tools that operationalize them. Our novel taxonomy, organized by LLM workflow stages, summarizes nearly 70 works at their intersections. We conclude with open challenges and future directions. This timely survey helps researchers and practitioners navigate key advancements for safer, more interpretable LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seongmin Lee",
      "Aeree Cho",
      "Grace C. Kim",
      "ShengYun Peng",
      "Mansi Phute",
      "Duen Horng Chau"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1092": {
    "title": "Unveiling the Response of Large Vision-Language Models to Visually Absent Tokens",
    "volume": "main",
    "abstract": "Large Vision-Language Models (LVLMs) generate contextually relevant responses by jointly interpreting visual and textual inputs. However, our finding reveals they often mistakenly perceive text inputs lacking visual evidence as being part of the image, leading to erroneous responses. In light of this finding, we probe whether LVLMs possess an internal capability to determine if textual concepts are grounded in the image, and discover a specific subset of Feed-Forward Network (FFN) neurons, termed Visual Absence-aware (VA) neurons, that consistently signal the visual absence through a distinctive activation pattern. Leveraging these patterns, we develop a detection module that systematically classifies whether an input token is visually grounded. Guided by its prediction, we propose a method to refine the outputs by reinterpreting question prompts or replacing the detected absent tokens during generation. Extensive experiments show that our method effectively mitigates the models' tendency to falsely presume the visual presence of text input and its generality across various LVLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sohee Kim",
      "Soohyun Ryu",
      "Joonhyung Park",
      "Eunho Yang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1093": {
    "title": "Improving Task Diversity in Label Efficient Supervised Finetuning of LLMs",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but developing high-performing models for specialized applications often requires substantial human annotation — a process that is time-consuming, labor-intensive, and expensive. In this paper, we address the label-efficient learning problem for supervised finetuning (SFT) by leveraging task-diversity as a fundamental principle for effective data selection. This is markedly different from existing methods based on the prompt-diversity. Our approach is based on two key observations: 1) task labels for different prompts are often readily available; 2) pre-trained models have significantly varying levels of confidence across tasks. We combine these facts to devise a simple yet effective sampling strategy: we select examples across tasks using an inverse confidence weighting strategy. This produces models comparable to or better than those trained with more complex sampling procedures, while being significantly easier to implement and less computationally intensive. Notably, our experimental results demonstrate that this method can achieve better accuracy than training on the complete dataset (a 4% increase in MMLU score). Across various annotation budgets and two instruction finetuning datasets, our algorithm consistently performs at or above the level of the best existing methods, while reducing annotation costs by up to 80%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhinav Arabelly",
      "Jagrut Nemade",
      "Robert D Nowak",
      "Jifan Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1094": {
    "title": "Look Beyond Feeling: Unveiling Latent Needs from Implicit Expressions for Proactive Emotional Support",
    "volume": "main",
    "abstract": "In recent years, Large Language Models (LLMs) have made significant progress in emotional support dialogue. However, there are two major challenges for LLM-based support systems. First, users may be hesitant to fully disclose their emotions at the outset. Second, direct probing or excessive questioning can induce discomfort or even resistance. To bridge this gap, we propose COCOON, a proactive emotional support framework that leverages principles of active listening to uncover implicit user needs. We design a multi-stage data curation pipeline and an annotation mechanism for support strategies. Based on this framework, we build COCOON-Llama3, a fine-tuned large language model, and evaluate it using both standard metrics and psychological scales. Experimental results indicate that our model more effectively elicits implicit emotional needs and delivers empathetic support compared to existing baselines, suggesting its utility for building more inclusive emotional support dialogue systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xing Fu",
      "Haozhen Li",
      "Bichen Wang",
      "Hao Yang",
      "Yanyan Zhao",
      "Bing Qin"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1095": {
    "title": "s3: You Don't Need That Much Data to Train a Search Agent via RL",
    "volume": "main",
    "abstract": "Retrieval-augmented generation (RAG) systems empower large language models (LLMs) to access external knowledge during inference. Recent advances have enabled LLMs to act as search agents via reinforcement learning (RL), improving information acquisition through multi-turn interactions with retrieval engines. However, existing approaches either optimize retrieval using search-only metrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM to jointly reason and retrieve—entangling retrieval with generation and limiting the real search utility and compatibility with frozen or proprietary models. In this work, we propose **s3**, a lightweight, model-agnostic framework that decouples the searcher from the generator and trains the searcher using a Gain Beyond RAG reward: the improvement in generation accuracy over naïve RAG. **s3** requires only 2.4k training samples to outperform baselines trained on over 70 × more data, consistently delivering stronger downstream performance across six general QA and five medical QA benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengcheng Jiang",
      "Xueqiang Xu",
      "Jiacheng Lin",
      "Jinfeng Xiao",
      "Zifeng Wang",
      "Jimeng Sun",
      "Jiawei Han"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1096": {
    "title": "FuseChat: Knowledge Fusion of Chat Models",
    "volume": "main",
    "abstract": "While training large language models (LLMs) from scratch can indeed lead to models with distinct capabilities and strengths, it incurs substantial costs and may lead to redundancy in competencies. Knowledge fusion aims to integrate existing LLMs of diverse architectures and capabilities into a more potent LLM through lightweight continual training, thereby reducing the need for costly LLM development. In this work, we propose a new framework for the knowledge fusion of chat LLMs through two main stages, resulting in FuseChat. Firstly, we conduct pairwise knowledge fusion on source chat LLMs of varying structures and scales to create multiple target LLMs with identical structure and size via lightweight fine-tuning. During this process, a statistics-based token alignment approach is introduced as the cornerstone for fusing LLMs with different structures. Secondly, we merge these target LLMs within the parameter space, where we propose a novel method for determining the merging coefficients based on the magnitude of parameter updates before and after fine-tuning. We implement and validate FuseChat using six prominent chat LLMs with diverse architectures and scales. Experimental results on two instruction-following benchmarks, AlpacaEval 2.0 and MT-Bench, demonstrate the superiority of FuseChat-7B over baselines of various sizes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fanqi Wan",
      "Longguang Zhong",
      "Ziyi Yang",
      "Ruijun Chen",
      "Xiaojun Quan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1097": {
    "title": "Continuous-Time Attention: PDE-Guided Mechanisms for Long-Sequence Transformers",
    "volume": "main",
    "abstract": "We present Continuous-Time Attention, a novel framework that infuses partial differential equations (PDEs) into the Transformer's attention mechanism to better handle long sequences. Instead of relying on a static attention matrix, we allow attention weights to evolve along a pseudo-time dimension governed by diffusion, wave, or reaction-diffusion dynamics. This dynamic process systematically smooths local noise, strengthens long-range dependencies, and improves gradient stability during training.Our theoretical analysis shows that PDE-driven attention mitigates the exponential decay of distant interactions and improves the optimization landscape. Empirically, Continuous-Time Attention achieves consistent performance gains over both standard and long-sequence Transformer variants across a range of tasks. These results suggest that embedding continuous-time dynamics into attention mechanisms is a promising direction for enhancing global coherence and scalability in Transformer models. Code is publicly available at:https://github.com/XueqingZhou/Continuous-Time-Attention",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yukun Zhang",
      "Xueqing Zhou"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1098": {
    "title": "Forget What You Know about LLMs Evaluations - LLMs are Like a Chameleon",
    "volume": "main",
    "abstract": "Large language models (LLMs) often appear to excel on public benchmarks, but these high scores may mask an overreliance on dataset-specific surface cues rather than true language understanding. We introduce the **Chameleon Benchmark Overfit Detector (C-BOD)**, a meta-evaluation framework designed to reveal such overfitting. C-BOD systematically rephrases benchmark inputs via a parameterized transformation that preserves semantic content and labels, enabling the detection of performance degradation indicative of superficial pattern reliance.We conduct extensive experiments across two datasets, three rephrasing models, and multiple distortion levels, evaluating 32 state-of-the-art LLMs. On the MMLU benchmark, C-BOD reveals an average performance drop of 2.75% under modest rephrasings, with over 80% of models exhibiting statistically significant differences. Notably, higher-performing models and larger LLMs tend to show greater sensitivity, suggesting a deeper dependence on benchmark-specific phrasing.Due to its dataset and model-agnostic design, C-BOD can be easily integrated into evaluation pipelines and offers a promising foundation for overfitting mitigation strategies. Our findings challenge the community to look beyond leaderboard scores and prioritize resilience and generalization in LLM evaluation. Our code and benchmark datasets are availableat: https://github.com/nuritci/cbod",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nurit Cohen Inger",
      "Yehonatan Elisha",
      "Bracha Shapira",
      "Lior Rokach",
      "Seffi Cohen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1099": {
    "title": "Memorization or Reasoning? Exploring the Idiom Understanding of LLMs",
    "volume": "main",
    "abstract": "Idioms have long posed a challenge due to their unique linguistic properties, which set them apart from other common expressions. While recent studies have leveraged large language models (LLMs) to handle idioms across various tasks, e.g., idiom-containing sentence generation and idiomatic machine translation, little is known about the underlying mechanisms of idiom processing in LLMs, particularly in multilingual settings. To this end, we introduce MIDAS, a new large-scale dataset of idioms in six languages, each paired with its corresponding meaning. Leveraging this resource, we conduct a comprehensive evaluation of LLMs' idiom processing ability, identifying key factors that influence their performance. Our findings suggest that LLMs rely not only on memorization, but also adopt a hybrid approach that integrates contextual cues and reasoning, especially when processing compositional idioms. This implies that idiom understanding in LLMs emerges from an interplay between internal knowledge retrieval and reasoning-based inference",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jisu Kim",
      "Youngwoo Shin",
      "Uiji Hwang",
      "Jihun Choi",
      "Richeng Xuan",
      "Taeuk Kim"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1100": {
    "title": "RD-MCSA: A Multi-Class Sentiment Analysis Approach Integrating In-Context Classification Rationales and Demonstrations",
    "volume": "main",
    "abstract": "This paper addresses the important yet underexplored task of **multi-class sentiment analysis (MCSA)**, which remains challenging due to the subtle semantic differences between adjacent sentiment categories and the scarcity of high-quality annotated data. To tackle these challenges, we propose **RD-MCSA** (**R**ationales and **D**emonstrations-based **M**ulti-**C**lass **S**entiment **A**nalysis), an In-Context Learning (ICL) framework designed to enhance MCSA performance under limited supervision by integrating classification rationales with adaptively selected demonstrations. First, semantically grounded classification rationales are generated from a representative, class-balanced subset of annotated samples selected using a tailored balanced coreset algorithm. These rationales are then paired with demonstrations chosen through a similarity-based mechanism powered by a **multi-kernel Gaussian process (MK-GP)**, enabling large language models (LLMs) to more effectively capture fine-grained sentiment distinctions. Experiments on five benchmark datasets demonstrate that RD-MCSA consistently outperforms both supervised baselines and standard ICL methods across various evaluation metrics",
    "checked": true,
    "id": "81e5fe8f7bbfc407bfec9327f8e6eb982cf5cc75",
    "semantic_title": "rd-mcsa: a multi-class sentiment analysis approach integrating in-context classification rationales and demonstrations",
    "citation_count": 0,
    "authors": [
      "Haihua Xie",
      "Yinzhu Cheng",
      "Yaqing Wang",
      "Miao He",
      "Mingming Sun"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1101": {
    "title": "Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint",
    "volume": "main",
    "abstract": "Rebus puzzles, visual riddles that encode language through imagery, spatial arrangement, and symbolic substitution, pose a unique challenge to current vision-language models (VLMs). Unlike traditional image captioning or question answering tasks, rebus solving requires multimodal abstraction, symbolic reasoning, and a grasp of cultural, phonetic and linguistic puns. In this short paper, we investigate the capacity of contemporary VLMs to interpret and solve rebus puzzles by constructing a hand-generated and annotated benchmark of diverse english-language rebus puzzles, ranging from simple pictographic substitutions to spatially-dependent cues (\"head\" over \"heels\"). We analyze how different VLMs perform, and our findings reveal that while VLMs exhibit some surprising capabilities in decoding simple visual clues, they struggle significantly with tasks requiring abstract reasoning, lateral thinking, and understanding visual metaphors",
    "checked": true,
    "id": "dffa7d18141338a4863302ddcf44bf0199e36c13",
    "semantic_title": "puzzled by puzzles: when vision-language models can't take a hint",
    "citation_count": 1,
    "authors": [
      "Heekyung Lee",
      "Jiaxin Ge",
      "Tsung-Han Wu",
      "Minwoo Kang",
      "Trevor Darrell",
      "David M. Chan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1102": {
    "title": "CREPE: Rapid Chest X-ray Report Evaluation by Predicting Multi-category Error Counts",
    "volume": "main",
    "abstract": "We introduce CREPE (Rapid Chest X-ray Report Evaluation by Predicting Multi-category Error Counts), a rapid, interpretable, and clinically grounded metric for automated chest X-ray report generation. CREPE uses a domain-specific BERT model fine-tuned with a multi-head regression architecture to predict error counts across six clinically meaningful categories. Trained on a large-scale synthetic dataset of 32,000 annotated report pairs, CREPE demonstrates strong generalization and interpretability. On the expert-annotated ReXVal dataset, CREPE achieves a Kendall's tau correlation of 0.786 with radiologist error counts, outperforming traditional and recent metrics. CREPE achieves these results with an inference speed approximately 280 times faster than large language model (LLM)-based approaches, enabling rapid and fine-grained evaluation for scalable development of chest X-ray report generation models",
    "checked": true,
    "id": "ab77d90fb9a599e70683da7b819e973d39f7a9c5",
    "semantic_title": "crepe: rapid chest x-ray report evaluation by predicting multi-category error counts",
    "citation_count": 0,
    "authors": [
      "Gihun Cho",
      "Seunghyun Jang",
      "Hanbin Ko",
      "Inhyeok Baek",
      "Chang Min Park"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1103": {
    "title": "TIDES: Technical Information Discovery and Extraction System",
    "volume": "main",
    "abstract": "Addressing the challenges in QA for specific technical domains requires identifying relevant portions of extensive documents and generating answers based on this focused content. Traditional pre-trained LLMs often struggle with domain-specific terminology, while fine-tuned LLMs demand substantial computational resources. To overcome these limitations, we propose TIDES, Technical Information Distillation and Extraction System. TIDES is a training-free approach that combines traditional TF-IDF techniques with prompt-based LLMs in a hybrid process, effectively addressing complex technical questions. It uses TF-IDF to identify and prioritize domain-specific words that are rare in other documents and LLMs to refine the candidate pool by focusing on the most relevant segments in documents through multiple stages. Our approach improves the precision and efficiency of QA systems in technical contexts without LLM retraining",
    "checked": true,
    "id": "5f1dc23756c12587dc74c3c84b58369a2a7abc12",
    "semantic_title": "tides: technical information discovery and extraction system",
    "citation_count": 0,
    "authors": [
      "Jihee Kim",
      "Subeen Park",
      "Hakyung Lee",
      "YongTaek Lim",
      "Hyo-won Suh",
      "Kyungwoo Song"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1104": {
    "title": "Learning to Ask: When LLM Agents Meet Unclear Instruction",
    "volume": "main",
    "abstract": "Equipped with the capability to call functions, modern LLM agents can leverage external tools for addressing a range of tasks unattainable through language skills alone. However, the effective execution of these tools relies heavily not just on the advanced capabilities of LLM agents but also on precise user instructions, which often cannot be ensured in the real world. To evaluate the performance of LLM agents tool-use under imperfect instructions, we meticulously examine the real-world instructions queried from users, analyze the error patterns, and build a challenging tool-use benchmark called Noisy ToolBench. We find that due to the next-token prediction training objective, LLM agents tend to arbitrarily generate the missed argument, which may lead to hallucinations and risks. To address this issue, we propose a novel framework, Ask-when-Needed, which prompts LLM agents to ask questions to users whenever they encounter obstacles due to unclear instructions. Moreover, to reduce the manual labor involved in user-LLM interaction and assess LLM agents' performance in tool utilization from both accuracy and efficiency perspectives, we design an automated evaluation tool named ToolEvaluator. Our experiments demonstrate that the Ask-when-Needed significantly outperforms existing frameworks for tool learning in the Noisy ToolBench. We will release all related code and datasets to support future research",
    "checked": true,
    "id": "33148bf6cdae160973b735904473bf45555b05a1",
    "semantic_title": "learning to ask: when llm agents meet unclear instruction",
    "citation_count": 4,
    "authors": [
      "Wenxuan Wang",
      "Shi Juluan",
      "Zixuan Ling",
      "Yuk-Kit Chan",
      "Chaozheng Wang",
      "Cheryl Lee",
      "Youliang Yuan",
      "Jen-tse Huang",
      "Wenxiang Jiao",
      "Michael R. Lyu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1105": {
    "title": "RICO: Improving Accuracy and Completeness in Image Recaptioning via Visual Reconstruction",
    "volume": "main",
    "abstract": "Image recaptioning is widely used to generate training datasets with enhanced quality for various multimodal tasks. Existing recaptioning methods typically rely on powerful multimodal large language models (MLLMs) to enhance textual descriptions, but often suffer from inaccuracies due to hallucinations and incompleteness caused by missing fine-grained details. To address these limitations, we propose RICO, a novel framework that refines captions through visual reconstruction. Specifically, we leverage a text-to-image model to reconstruct a caption into a reference image, and prompt an MLLM to identify discrepancies between the original and reconstructed images to refine the caption. This process is performed iteratively, further progressively promoting the generation of more faithful and comprehensive descriptions. To mitigate the additional computational cost induced by the iterative process, we introduce RICO-Flash, which learns to generate captions like RICO using DPO. Extensive experiments demonstrate that our approach significantly improves caption accuracy and completeness, outperforms most baselines by approximately 10% on both CapsBench and CompreCap",
    "checked": true,
    "id": "7e10a5223adfd36121541ff64cb14addaef5475e",
    "semantic_title": "rico: improving accuracy and completeness in image recaptioning via visual reconstruction",
    "citation_count": 1,
    "authors": [
      "Yuchi Wang",
      "Yishuo Cai",
      "Shuhuai Ren",
      "Sihan Yang",
      "Linli Yao",
      "Yuanxin Liu",
      "Yuanxing Zhang",
      "Pengfei Wan",
      "Xu Sun"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1106": {
    "title": "StepSearch: Igniting LLMs Search Ability via Step-Wise Proximal Policy Optimization",
    "volume": "main",
    "abstract": "Efficient multi-hop reasoning requires Large Language Models (LLMs) based agents to acquire high-value external knowledge iteratively. Previous work has explored reinforcement learning (RL) to train LLMs to perform search-based document retrieval, achieving notable improvements in QA performance, but underperform on complex, multi-hop QA resulting from the sparse rewards from global signal only. To address this gap in existing research, we introduce StepSearch, a framework for search LLMs that trained with step-wise proximal policy optimization method. It consists of richer and more detailed intermediate search rewards and token-level process supervision based on information gain and redundancy penalties to better guide each search step. We constructed a fine-grained question-answering dataset containing sub-question-level search trajectories based on open source datasets through a set of data pipeline method. On standard multi-hop QA benchmarks, it significantly outperforms global-reward baselines, achieving 11.2% and 4.2% absolute improvements for 3B and 7B models over various search with RL baselines using only 19k training data, demonstrating the effectiveness of fine-grained, stepwise supervision in optimizing deep search LLMs. The project is open source at https://github.com/Zillwang/StepSearch",
    "checked": true,
    "id": "53662e87ba9fc22011472ab78f49792a9ebd53c7",
    "semantic_title": "stepsearch: igniting llms search ability via step-wise proximal policy optimization",
    "citation_count": 20,
    "authors": [
      "Xuhui Zheng",
      "Kang An",
      "Ziliang Wang",
      "Yuhang Wang",
      "Yichao Wu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1107": {
    "title": "Dynamic Model-Bank Test-Time Adaptation for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "End-to-end automatic speech recognition (ASR) based on deep learning has achieved impressive progress in recent years. However, the performance of ASR foundation model often degrades significantly on out-of-domain data due to real-world domain shifts. Test-Time Adaptation (TTA) methods aim to mitigate this issue by adapting models during inference without access to source data. Despite recent progress, existing ASR TTA methods often struggle with instability under continual and long-term distribution shifts. To alleviate the risk of performance collapse due to error accumulation, we propose Dynamic Model-bank Single-Utterance Test-time Adaptation (DMSUTA), a sustainable continual TTA framework based on adaptive ASR model ensembling. DMSUTA maintains a dynamic model bank, from which a subset of checkpoints is selected for each test sample based on confidence and uncertainty criteria. To preserve both model plasticity and long-term stability, DMSUTA actively manages the bank by filtering out potentially collapsed models. This design allows DMSUTA to continually adapt to evolving domain shifts in ASR test-time scenarios. Experiments on diverse, continuously shifting ASR TTA benchmarks show that DMSUTA consistently outperforms existing continual TTA baselines, demonstrating superior robustness to domain shifts in ASR",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanshuo Wang",
      "Yanghao Zhou",
      "Yukang Lin",
      "Haoxing Chen",
      "Jin Zhang",
      "Wentao Zhu",
      "Jie Hong",
      "Xuesong Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1108": {
    "title": "Mitigating Catastrophic Forgetting in Large Language Models with Forgetting-aware Pruning",
    "volume": "main",
    "abstract": "Recent advancements in large language models (LLMs) have shown impressive capabilities in various downstream tasks but typically face Catastrophic Forgetting (CF) during fine-tuning. In this paper, we propose the Forgetting-Aware Pruning Metric (FAPM), a novel pruning-based approach to balance CF and downstream task performance. Our investigation reveals that the degree to which task vectors (i.e., the subtraction of pre-trained weights from the weights fine-tuned on downstream tasks) overlap with pre-trained model parameters is a critical factor for CF. Based on this finding, FAPM employs the ratio of the task vector to pre-trained model parameters as a metric to quantify CF, integrating this measure into the pruning criteria. Importantly, FAPM does not necessitate modifications to the training process or model architecture, nor does it require any auxiliary data. We conducted extensive experiments across eight datasets, covering natural language inference, General Q&A, Medical Q&A, Math Q&A, reading comprehension, and cloze tests. The results demonstrate that FAPM limits CF to just 0.25% while maintaining 99.67% accuracy on downstream tasks. We provide the codes of FAPM at an anonymous repository(https://anonymous.4open.science/r/FAPM-65CF)",
    "checked": true,
    "id": "92c5d06e2390690826bdff9892b936a0fa0724d6",
    "semantic_title": "mitigating catastrophic forgetting in large language models with forgetting-aware pruning",
    "citation_count": 0,
    "authors": [
      "Wei Huang",
      "Anda Cheng",
      "Yinggui Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1109": {
    "title": "Does Localization Inform Unlearning? A Rigorous Examination of Local Parameter Attribution for Knowledge Unlearning in Language Models",
    "volume": "main",
    "abstract": "Large language models often retain unintended content, prompting growing interest in knowledge unlearning.Recent approaches emphasize localized unlearning, restricting parameter updates to specific regions in an effort to remove target knowledge while preserving unrelated general knowledge. However, their effectiveness remains uncertain due to the lack of robust and thorough evaluation of the trade-off between the competing goals of unlearning.In this paper, we begin by revisiting existing localized unlearning approaches. We then conduct controlled experiments to rigorously evaluate whether local parameter updates causally contribute to unlearning.Our findings reveal that the set of parameters that must be modified for effective unlearning is not strictly determined, challenging the core assumption of localized unlearning that parameter locality is inherently indicative of effective knowledge removal",
    "checked": true,
    "id": "23075dfc49f0dd086eb00acb1890f8bd417d611a",
    "semantic_title": "does localization inform unlearning? a rigorous examination of local parameter attribution for knowledge unlearning in language models",
    "citation_count": 1,
    "authors": [
      "Hwiyeong Lee",
      "Uiji Hwang",
      "Hyelim Lim",
      "Taeuk Kim"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1110": {
    "title": "ArgCMV: An Argument Summarization Benchmark for the LLM-era",
    "volume": "main",
    "abstract": "Key point extraction is an important task in argument summarization which involves extracting high-level short summaries from arguments. Existing approaches for KP extraction have been mostly evaluated on the popular ArgKP21 dataset. In this paper, we highlight some of the major limitations of the ArgKP21 dataset and demonstrate the need for new benchmarks that are more representative of actual human conversations. Using SoTA large language models (LLMs), we curate a new argument key point extraction dataset called ArgCMV comprising of ∼12K arguments from actual online human debates spread across ∼3K topics. Our dataset exhibits higher complexity such as longer, co-referencing arguments, higher presence of subjective discourse units, and a larger range of topics over ArgKP21. We show that existing methods do not adapt well to ArgCMV and provide extensive benchmark results by experimenting with existing baselines and latest open source models. This work introduces a novel KP extraction dataset for long-context online discussions, setting the stage for the next generation of LLM-driven summarization research",
    "checked": true,
    "id": "260061c8275638047b82426914e3dcd0cd9ec33d",
    "semantic_title": "argcmv: an argument summarization benchmark for the llm-era",
    "citation_count": 0,
    "authors": [
      "Omkar Gurjar",
      "Agam Goyal",
      "Eshwar Chandrasekharan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1111": {
    "title": "VistaWise: Building Cost-Effective Agent with Cross-Modal Knowledge Graph for Minecraft",
    "volume": "main",
    "abstract": "Large language models (LLMs) have shown significant promise in embodied decision-making tasks within virtual open-world environments. Nonetheless, their performance is hindered by the absence of domain-specific knowledge. Methods that finetune on large-scale domain-specific data entail prohibitive development costs. This paper introduces VistaWise, a cost-effective agent framework that integrates cross-modal domain knowledge and finetunes a dedicated object detection model for visual analysis. It reduces the requirement for domain-specific training data from millions of samples to a few hundred. VistaWise integrates visual information and textual dependencies into a cross-modal knowledge graph (KG), enabling a comprehensive and accurate understanding of multimodal environments. We also equip the agent with a retrieval-based pooling strategy to extract task-related information from the KG, and a desktop-level skill library to support direct operation of the Minecraft desktop client via mouse and keyboard inputs. Experimental results demonstrate that VistaWise achieves state-of-the-art performance across various open-world tasks, highlighting its effectiveness in reducing development costs while enhancing agent performance",
    "checked": true,
    "id": "ae430a480ac82254111939b780c639e17abbd63f",
    "semantic_title": "vistawise: building cost-effective agent with cross-modal knowledge graph for minecraft",
    "citation_count": 3,
    "authors": [
      "Honghao Fu",
      "Junlong Ren",
      "Qi Chai",
      "Deheng Ye",
      "Yujun Cai",
      "Hao Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1112": {
    "title": "GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV Cache Eviction",
    "volume": "main",
    "abstract": "Efficient Key-Value (KV) cache management is essential for processing long text sequences in large language models (LLMs), where memory constraints often limit performance. Conventional KV eviction strategies, such as top-k selection based on attention scores, depend on static heuristics that fail to capture the evolving implicit dependencies among tokens during inference. To overcome this, we propose GraphKV, a graph-based framework that redefines token selection for KV cache compression. In GraphKV, tokens are modeled as nodes with importance scores, and edges represent their similarity relationships. Through a decay-signal-propagation mechanism, token importance is dynamically updated by propagating information across the graph, enabling adaptive retention of the most contextually significant tokens. GraphKV can be seamlessly utilized in existing KV cache eviction methods such as SnapKV and PyramidKV in a plug-and-play manner. Codes are available in the supplementary materials and will be released on Github",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuelin Li",
      "Xiangqi Jin",
      "Linfeng Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1113": {
    "title": "Joint Modeling of Entities and Discourse Relations for Coherence Assessment",
    "volume": "main",
    "abstract": "In linguistics, coherence can be achieved by different means, such as by maintaining reference to the same set of entities across sentences and by establishing discourse relations between them. However, most existing work on coherence modeling focuses exclusively on either entity features or discourse relation features, with little attention given to combining the two. In this study, we explore two methods for jointly modeling entities and discourse relations for coherence assessment. Experiments on three benchmark datasets show that integrating both types of features significantly enhances the performance of coherence models, highlighting the benefits of modeling both simultaneously for coherence evaluation",
    "checked": true,
    "id": "efadeaa189f261c73b4faec8296a768c66839fd7",
    "semantic_title": "joint modeling of entities and discourse relations for coherence assessment",
    "citation_count": 1,
    "authors": [
      "Wei Liu",
      "Michael Strube"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1114": {
    "title": "Understanding and Leveraging the Expert Specialization of Context Faithfulness in Mixture-of-Experts LLMs",
    "volume": "main",
    "abstract": "Context faithfulness is essential for reliable reasoning in context-dependent scenarios. However, large language models often struggle to ground their outputs in the provided context, resulting in irrelevant responses.Inspired by the emergent expert specialization observed in mixture-of-experts architectures, this work investigates whether certain experts exhibit specialization in context utilization—offering a potential pathway toward targeted optimization for improved context faithfulness.To explore this, we propose Router Lens, a method that accurately identifies context-faithful experts. Our analysis reveals that these experts progressively amplify attention to relevant contextual information, thereby enhancing context grounding.Building on this insight, we introduce Context-faithful Expert Fine-Tuning (CEFT), a lightweight optimization approach that selectively fine-tunes context-faithful experts.Experiments across a wide range of benchmarks and models demonstrate that CEFT matches or surpasses the performance of full fine-tuning while being significantly more efficient",
    "checked": true,
    "id": "ea30a8e194c31e15977aac38e45c3b4091d27f55",
    "semantic_title": "understanding and leveraging the expert specialization of context faithfulness in mixture-of-experts llms",
    "citation_count": 1,
    "authors": [
      "Jun Bai",
      "Minghao Tong",
      "Yang Liu",
      "Zixia Jia",
      "Zilong Zheng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1115": {
    "title": "HMoE: Heterogeneous Mixture of Experts for Language Modeling",
    "volume": "main",
    "abstract": "Mixture of Experts (MoE) offers remarkable performance and computational efficiency by selectively activating subsets of model parameters. Traditionally, MoE models use homogeneous experts, each with identical capacity. However, varying complexity in input data necessitates experts with diverse capabilities, while homogeneous MoE hinders effective expert specialization and efficient parameter utilization. In this study, we propose a novel Heterogeneous Mixture of Experts (HMoE) framework, where experts differ in size and thus possess diverse capacities. This heterogeneity allows for more specialized experts to handle varying token complexities more effectively. To address the imbalance in expert activation, we propose a novel training objective that encourages the frequent activation of smaller experts, so as to improve computational efficiency and parameter utilization. Extensive experiments demonstrate that HMoE achieves a lower loss rate with fewer activated parameters and outperforms conventional homogeneous MoE models on various pre-training evaluation benchmarks. Codes will be released upon acceptance",
    "checked": true,
    "id": "77a06e41868fa7dffe87e6197dbb8da73381e12a",
    "semantic_title": "hmoe: heterogeneous mixture of experts for language modeling",
    "citation_count": 0,
    "authors": [
      "An Wang",
      "Xingwu Sun",
      "Ruobing Xie",
      "Shuaipeng Li",
      "Jiaqi Zhu",
      "Zhen Yang",
      "Pinxue Zhao",
      "Weidong Han",
      "Zhanhui Kang",
      "Di Wang",
      "Naoaki Okazaki",
      "Cheng-zhong Xu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1116": {
    "title": "The Ranking Blind Spot: Decision Hijacking in LLM-based Text Ranking",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have demonstrated strong performance in information retrieval tasks like passage ranking. Our research examines how instruction-following capabilities in LLMs interact with multi-document comparison tasks, identifying what we term the \"Ranking Blind Spot\"—a characteristic of LLM decision processes during comparative evaluation. We analyze how this ranking blind spot affects LLM evaluation systems through two approaches: **Decision Objective Hijacking**, which alters the evaluation goal in pairwise ranking systems, and **Decision Criteria Hijacking**, which modifies relevance standards across ranking schemes. These approaches demonstrate how content providers could potentially influence LLM-based ranking systems to affect document positioning. These attacks aim to force the LLM ranker to prefer a specific passage and rank it at the top. Malicious content providers can exploit this weakness, which helps them gain additional exposure by attacking the ranker. In our experiment, We empirically show that the proposed attacks are effective in various LLMs and can be generalized to multiple ranking schemes. We apply these attack to real-world examples to show their effectiveness. We also found stronger LLMs are more vulnerable to these attacks",
    "checked": true,
    "id": "9bba369ba8f6594a1792b649e86c57439261116e",
    "semantic_title": "the ranking blind spot: decision hijacking in llm-based text ranking",
    "citation_count": 0,
    "authors": [
      "Yaoyao Qian",
      "Yifan Zeng",
      "Yuchao Jiang",
      "Chelsi Jain",
      "Huazheng Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1117": {
    "title": "Uniform Information Density and Syntactic Reduction: Revisiting *that*-Mentioning in English Complement Clauses",
    "volume": "main",
    "abstract": "Speakers often have multiple ways to express the same meaning. The Uniform Information Density (UID) hypothesis suggests that speakers exploit this variability to maintain a consistent rate of information transmission during language production. Building on prior work linking UID to syntactic reduction, we revisit the finding that the optional complementizer *that* in English complement clauses is more likely to be omitted when the clause has low information density (i.e., more predictable). We advance this line of research by analyzing a large-scale, contemporary conversational corpus and using machine learning and neural language models to refine estimates of information density. Our results replicated the established relationship between information density and *that*-mentioning. However, we found that previous measures of information density based on matrix verbs' subcategorization probability capture substantial idiosyncratic lexical variation. By contrast, estimates derived from contextual word embeddings account for additional variance in patterns of complementizer usage",
    "checked": false,
    "id": "a68f1b21850acd155c4b73f997e8bcddd1c0ac7e",
    "semantic_title": "uniform information density and syntactic reduction: revisiting that-mentioning in english complement clauses",
    "citation_count": 0,
    "authors": [
      "Hailin Hao",
      "Elsi Kaiser"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1118": {
    "title": "GRIT: Guided Relational Integration for Efficient Multi-Table Understanding",
    "volume": "main",
    "abstract": "Recent advances in large language models (LLMs) have opened new possibilities for table-based tasks. However, most existing methods remain confined to single-table settings, limiting their applicability to real-world databases composed of multiple interrelated tables. In multi-table scenarios, LLMs face two key challenges: reasoning over relational structures beyond sequential text, and handling the input length limitations imposed by large-scale table concatenation. To address these issues, we propose Guided Relational Integration for multiple Tables (GRIT), a lightweight method that converts relational schemas into LLM-friendly textual representations. GRIT employs hashing-based techniques to efficiently infer primary–foreign key relationships and constructs prompts that explicitly encode relevant join paths and question-relevant columns. When applied to off-the-shelf LLMs, GRIT consistently improves table-column retrieval performance across diverse multi-table benchmarks while significantly reducing memory and computational overhead",
    "checked": true,
    "id": "c8d5df55086859e22b4448680f71699d3588b0c6",
    "semantic_title": "grit: guided relational integration for efficient multi-table understanding",
    "citation_count": 0,
    "authors": [
      "Yujin Kang",
      "Park Seong Woo",
      "Yoon-Sik Cho"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1119": {
    "title": "RPDR: A Round-trip Prediction-Based Data Augmentation Framework for Long-Tail Question Answering",
    "volume": "main",
    "abstract": "Long-tail question answering presents significant challenges for large language models (LLMs) due to their limited ability to acquire and accurately recall less common knowledge. Retrieval-augmented generation (RAG) systems have shown great promise in mitigating this limitation by integrating external retrieval mechanisms. However, dense retrieval models often face the same difficulties when generalizing to rare or niche knowledge. In this study, we introduce RPDR, a novel data augmentation framework that selects high-quality easy-to-learn training data, to enhance dense retrievers. Our approach is built around three core components: synthetic data generation, data selection with Round-Trip prediction to identify easy-to-learn instances, and retriever training with these instances. We evaluate RPDR on two long-tail retrieval benchmarks, PopQA and EntityQuestion, demonstrating substantial improvements over existing retrievers like BM25 and Contriver, especially on extremely long-tail categories. We identify the strengths and limitations of RPDR through detailed human analysis and propose a dynamic routing mechanism to dynamically route queries to specialized retrieval modules to further improve retrieval performance",
    "checked": true,
    "id": "960cc1ebccffecf0b8a8a1ae8d411653e5c5035b",
    "semantic_title": "rpdr: a round-trip prediction-based data augmentation framework for long-tail question answering",
    "citation_count": 0,
    "authors": [
      "Yiming Zhang",
      "Siyue Zhang",
      "Junbo Zhao",
      "Chen Zhao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1120": {
    "title": "Discrepancy Detection at the Data Level: Toward Consistent Multilingual Question Answering",
    "volume": "main",
    "abstract": "Multilingual question answering (QA) systems must ensure factual consistency across languages, especially for objective queries such as What is jaundice?, while also accounting for cultural variation in subjective responses. We propose MIND, a user-in-the-loop fact-checking pipeline to detect factual and cultural discrepancies in multilingual QA knowledge bases. MIND highlights divergent answers to culturally sensitive questions (e.g., Who assists in childbirth?) that vary by region and context. We evaluate MIND on a bilingual QA system in the maternal and infant health domain and release a dataset of bilingual questions annotated for factual and cultural inconsistencies. We further test MIND on datasets from other domains to assess generalization. In all cases, MIND reliably identifies inconsistencies, supporting the development of more culturally aware and factually consistent QA systems",
    "checked": true,
    "id": "0070170e3701354ad7e18c117281a0c263a43b48",
    "semantic_title": "discrepancy detection at the data level: toward consistent multilingual question answering",
    "citation_count": 0,
    "authors": [
      "Lorena Calvo-Bartolomé",
      "Valérie Aldana",
      "Karla Cantarero",
      "Alonso Madroñal de Mesa",
      "Jerónimo Arenas-García",
      "Jordan Lee Boyd-Graber"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1121": {
    "title": "Data-Efficient Selection via Grammatical Complexity in Continual Pre-training of Domain-Specific LLMs",
    "volume": "main",
    "abstract": "Data efficiency is crucial in domain-specific continual pre-training (CPT) of large language models (LLMs), especially under resource constraints. Aiming for \"small data, big impact,\" this work addresses the limitations of existing domain-specific data selection strategies, which often rely on scarce labeled data or computationally expensive LLMs. We introduce CDF Sampling with Grammatical Complexity (CDF-GC), an annotation-independent, efficient and interpretable data selection framework for CPT. Our approach comprehensively evaluates grammatical complexity using lexical diversity and syntactic complexity, and employs a cumulative distribution function (CDF)-based sampling strategy to balance complexity and diversity. To validate the effectiveness of CDF-GC, we conducted experiments on a financial dataset. The results demonstrate that CDF-GC significantly outperforms baselines, achieving 2.0% improvement in financial QA at the same selection ratio and even surpassing full-data training by 1.7% using only 20% of the data",
    "checked": true,
    "id": "95143c69811335ce48430707af0cd3d4953fe364",
    "semantic_title": "data-efficient selection via grammatical complexity in continual pre-training of domain-specific llms",
    "citation_count": 0,
    "authors": [
      "Yizhou Ying",
      "Geng Zhang",
      "Cui Danxin",
      "Chengyu Du",
      "Guanglei Yue",
      "Sihang Jiang",
      "Jiaqing Liang",
      "Yifei Fu",
      "Hailin Hu",
      "Yanghua Xiao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1122": {
    "title": "Comprehensive and Efficient Distillation for Lightweight Sentiment Analysis Models",
    "volume": "main",
    "abstract": "Recent efforts leverage knowledge distillation techniques to develop lightweight and practical sentiment analysis models. These methods are grounded in human-written instructions and large-scale user texts. Despite the promising results, two key challenges remain: (1) manually written instructions are limited in diversity and quantity, making them insufficient to ensure comprehensive coverage of distilled knowledge; (2) large-scale user texts incur high computational cost, hindering the practicality of these methods. To this end, we introduce CompEffDist, a comprehensive and efficient distillation framework for sentiment analysis. Our framework consists of two key modules: attribute-based automatic instruction construction and difficulty-based data filtering, which correspondingly tackle the aforementioned challenges. Applying our method across multiple model series (Llama-3, Qwen-3, and Gemma-3), we enable 3B student models to match the performance of 20x larger teacher models on most tasks. In addition, our approach greatly outperforms baseline methods in data efficiency, attaining the same performance level with only 10% of the data. All codes are available at https://github.com/HITSZ-HLT/COMPEFFDIST",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangyu Xie",
      "Yice Zhang",
      "Jianzhu Bao",
      "Qianlong Wang",
      "Yang Sun",
      "Bingbing Wang",
      "Ruifeng Xu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1123": {
    "title": "One Planner To Guide Them All ! Learning Adaptive Conversational Planners for Goal-oriented Dialogues",
    "volume": "main",
    "abstract": "Goal-oriented dialogues, such as recommendation and negotiation, often require balancing multiple, conflicting objectives. Existing methods typically involve training separate models for specific combinations of objectives, leading to computational and scalability issues. In this work, we aim to develop a new dialogue policy method that can adapt to varying objective preferences at inference time without retraining. This raises several challenges in terms of both (1) optimization strategy and (2) knowledge utilization. To address these, we propose a novel learning framework, Preference Adaptive Dialogue Policy Planner (PADPP), for multi-objective goal-oriented dialogues. Specifically, to tackle the former, we introduce a novel policy optimization scheme, which leverages information gained from training the model on previously updated objective weights, accelerating the learning capability on new weight settings. To address the latter, we utilize Generalized Policy Improvement (GPI) to ensure the effectiveness of leveraged knowledge. Experimental results demonstrate that PADPP achieves superior adaptability and performance compared to state-of-the-art approaches, offering a scalable and flexible solution for multi-objective, goal-oriented dialogues. Code and data are available at the anonymous link",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huy Quang Dao",
      "Lizi Liao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1124": {
    "title": "Unsupervised Hallucination Detection by Inspecting Reasoning Processes",
    "volume": "main",
    "abstract": "Unsupervised hallucination detection aims to identify hallucinated content generated by large language models (LLMs) without relying on labeled data. While unsupervised methods have gained popularity by eliminating labor-intensive human annotations, they frequently rely on proxy signals unrelated to factual correctness. This misalignment biases detection probes toward superficial or non-truth-related aspects, limiting generalizability across datasets and scenarios. To overcome these limitations, we propose IRIS, an unsupervised hallucination detection framework, leveraging internal representations intrinsic to factual correctness. IRIS prompts the LLM to carefully verify the truthfulness of a given statement, and obtain its contextualized embedding as informative features for training. Meanwhile, the uncertainty of each response is considered a soft pseudolabel for truthfulness. Experimental results demonstrate that IRIS consistently outperforms existing unsupervised methods. Our approach is fully unsupervised, computationally low cost, and works well even with few training data, making it suitable for real-time detection",
    "checked": true,
    "id": "720196a0eb44fa7a2f99272ab2ec69e9efcc5d93",
    "semantic_title": "unsupervised hallucination detection by inspecting reasoning processes",
    "citation_count": 0,
    "authors": [
      "Ponhvoan Srey",
      "Xiaobao Wu",
      "Anh Tuan Luu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1125": {
    "title": "Multimodal Neural Machine Translation: A Survey of the State of the Art",
    "volume": "main",
    "abstract": "Multimodal neural machine translation (MNMT) has received increasing attention due to its widespread applications in various fields such as cross-border e-commerce and cross-border social media platforms. The task aims to integrate other modalities, such as the visual modality, with textual data to enhance translation performance. We survey the major milestones in MNMT research, providing a comprehensive overview of relevant datasets and recent methodologies, and discussing key challenges and promising research directions",
    "checked": true,
    "id": "ad13d49bb944aedeaa8722be5e2b5abd715c7a38",
    "semantic_title": "multimodal neural machine translation: a survey of the state of the art",
    "citation_count": 0,
    "authors": [
      "Yi Feng",
      "Chuanyi Li",
      "Jiatong He",
      "Zhenyu Hou",
      "Vincent Ng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1126": {
    "title": "Lemmatization of Polish Multi-word Expressions",
    "volume": "main",
    "abstract": "This paper explores the lemmatization of multi-word expressions (MWEs) and proper names in Polish – tasks complicated by linguistic irregularities and historical factors. Instead of using rule-based methods, we apply a machine learning approach with fine-tuned plT5 and mT5 models. We trained and validated the models on enhanced gold-standard data from the 2019 PolEval task and evaluated the impact of additional fine-tuning on a silver-standard dataset derived from Wikipedia. Two setups were tested: one without context, and one using left-side context of the target MWE. Our best model achieved 86.23% AccCS (Accuracy Case-Sensitive), 89.43% AccCI (Accuracy Case-Insensitive), and a combined score of 88.79%, setting a new state-of-the-art for Polish MWE and named entity lemmatization, as confirmed by the PolEval maintainers. We also evaluated optimization and quantization techniques to reduce model size and inference time with minimal quality loss",
    "checked": true,
    "id": "3bbde425752963d488dd36d7b328eed177f2b5b6",
    "semantic_title": "lemmatization of polish multi-word expressions",
    "citation_count": 0,
    "authors": [
      "Magdalena Król",
      "Aleksander Smywiński-Pohl",
      "Zbigniew Kaleta",
      "Paweł Lewkowicz"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1127": {
    "title": "Targeted Distillation for Sentiment Analysis",
    "volume": "main",
    "abstract": "This paper explores targeted distillation methods for sentiment analysis, aiming to build compact and practical models that preserve strong and generalizable sentiment analysis capabilities. To this end, we conceptually decouple the distillation target into knowledge and alignment and accordingly propose a two-stage distillation framework. Moreover, we introduce SentiBench, a comprehensive and systematic sentiment analysis benchmark that covers a diverse set of tasks across 12 datasets. We evaluate a wide range of models on this benchmark. Experimental results show that our approach substantially enhances the performance of compact models across diverse sentiment analysis tasks, and the resulting models demonstrate strong generalization to unseen tasks, showcasing robust competitiveness against existing small-scale models",
    "checked": true,
    "id": "117632dbd72b296622adf16cea98116c64c17e4b",
    "semantic_title": "targeted distillation for sentiment analysis",
    "citation_count": 2,
    "authors": [
      "Yice Zhang",
      "Guangyu Xie",
      "Jingjie Lin",
      "Jianzhu Bao",
      "Qianlong Wang",
      "Xi Zeng",
      "Ruifeng Xu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1128": {
    "title": "DiffusionAttacker: Diffusion-Driven Prompt Manipulation for LLM Jailbreak",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) are susceptible to generating harmful content when prompted with carefully crafted inputs, a vulnerability known as LLM jailbreaking. As LLMs become more powerful, studying jailbreak methods is critical to enhancing security and aligning models with human values. Traditionally, jailbreak techniques have relied on suffix addition or prompt templates, but these methods suffer from limited attack diversity. This paper introduces DiffusionAttacker, an end-to-end generative approach for jailbreak rewriting inspired by diffusion models. Our method employs a sequence-to-sequence (seq2seq) text diffusion model as a generator, conditioning on the original prompt and guiding the denoising process with a novel attack loss. Unlike previous approaches that use autoregressive LLMs to generate jailbreak prompts, which limit the modification of already generated tokens and restrict the rewriting space, DiffusionAttacker utilizes a seq2seq diffusion model, allowing more flexible token modifications. This approach preserves the semantic content of the original prompt while producing harmful content. Additionally, we leverage the Gumbel-Softmax technique to make the sampling process from the diffusion model's output distribution differentiable, eliminating the need for iterative token search. Extensive experiments on Advbench and Harmbench demonstrate that DiffusionAttacker outperforms previous methods across various evaluation metrics, including attack success rate (ASR), fluency, and diversity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Wang",
      "Hao Li",
      "Junda Zhu",
      "Xinyuan Wang",
      "Chengwei Pan",
      "Minlie Huang",
      "Lei Sha"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1129": {
    "title": "Rank-Awareness and Angular Constraints: A New Perspective on Learning Sentence Embeddings from NLI Data",
    "volume": "main",
    "abstract": "Learning high-quality sentence embeddings from Natural Language Inference (NLI) data is often challenged by a critical signal conflict between discrete labels and the continuous spectrum of semantic similarity, as well as information loss from discarded neutral sentence pairs during training. To address this, we introduce Rank-Awareness and Angular Optimization Embeddings (RAOE), a framework that leverages the full NLI dataset (Entailment, Neutral, Contradiction) augmented with pre-computed continuous similarity scores (S). RAOE employs a novel composite objective which features: (1) a Rank Margin objective that enforces rank consistency against S using an explicit margin, and (2) a Gated Angular objective that conditionally refines embedding geometry based on NLI label (L) and S score agreement. Extensive evaluations on STS tasks and the MTEB benchmark demonstrate RAOE's effectiveness. Our general-purpose RAOE-S1 model (BERT-base) significantly outperforms strong baselines, achieving an average Spearman's correlation of 85.11 (vs. SimCSE's 81.57 and AnglE's 82.43), and shows consistent improvements on MTEB. Further STS-specialized fine-tuning (RAOE-S2) establishes new state-of-the-art performance on STS (88.17 with BERT-base). These results confirm RAOE's ability to efficiently learn robust and nuanced sentence representations through the synergy of rank-awareness and conditional angular constraints. Code is available at https://github.com/Shengjingwa/RAOE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zicheng Zhou",
      "Min Huang",
      "Qinghai Miao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1130": {
    "title": "LLM-Guided Semantic Relational Reasoning for Multimodal Intent Recognition",
    "volume": "main",
    "abstract": "Understanding human intents from multimodal signals is critical for analyzing human behaviors and enhancing human-machine interactions in real-world scenarios. However, existing methods exhibit limitations in their modality-level reliance, constraining relational reasoning over fine-grained semantics for complex intent understanding. This paper proposes a novel LLM-Guided Semantic Relational Reasoning (LGSRR) method, which harnesses the expansive knowledge of large language models (LLMs) to establish semantic foundations that boost smaller models' relational reasoning performance. Specifically, an LLM-based strategy is proposed to extract fine-grained semantics as guidance for subsequent reasoning, driven by a shallow-to-deep Chain-of-Thought (CoT) that autonomously uncovers, describes, and ranks semantic cues by their importance without relying on manually defined priors. Besides, we formally model three fundamental types of semantic relations grounded in logical principles and analyze their nuanced interplay to enable more effective relational reasoning. Extensive experiments on multimodal intent and dialogue act recognition tasks demonstrate LGSRR's superiority over state-of-the-art methods, with consistent performance gains across diverse semantic understanding scenarios. The complete data and code are available at https://github.com/thuiar/LGSRR",
    "checked": true,
    "id": "9f008f46f6af88b44fd6534f7038194005da7990",
    "semantic_title": "llm-guided semantic relational reasoning for multimodal intent recognition",
    "citation_count": 0,
    "authors": [
      "Qianrui Zhou",
      "Hua Xu",
      "Yifan Wang",
      "Xinzhi Dong",
      "Hanlei Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1131": {
    "title": "Seeing Culture: A Benchmark for Visual Reasoning and Grounding",
    "volume": "main",
    "abstract": "Multimodal vision-language models (VLMs) have made substantial progress in various tasks that require a combined understanding of visual and textual content, particularly in cultural understanding tasks, with the emergence of new cultural datasets. However, these datasets frequently fall short of providing cultural reasoning while underrepresenting many cultures.In this paper, we introduce the Seeing Culture Benchmark (SCB), focusing on cultural reasoning with a novel approach that requires VLMs to reason on culturally rich images in two stages: i) selecting the correct visual option with multiple-choice visual question answering (VQA), and ii) segmenting the relevant cultural artifact as evidence of reasoning. Visual options in the first stage are systematically organized into three types: those originating from the same country, those from different countries, or a mixed group. Notably, all options are derived from a singular category for each type. Progression to the second stage occurs only after a correct visual option is chosen. The SCB benchmark comprises 1,065 images that capture 138 cultural artifacts across five categories from seven Southeast Asia countries, whose diverse cultures are often overlooked, accompanied by 3,178 questions, of which 1,093 are unique and meticulously curated by human annotators. Our evaluation of various VLMs reveals the complexities involved in cross-modal cultural reasoning and highlights the disparity between visual reasoning and spatial grounding in culturally nuanced scenarios. The SCB serves as a crucial benchmark for identifying these shortcomings, thereby guiding future developments in the field of cultural reasoning. https://github.com/buraksatar/SeeingCulture",
    "checked": true,
    "id": "b0c446041b282bfefae50f73be4d0bb6f81bf164",
    "semantic_title": "seeing culture: a benchmark for visual reasoning and grounding",
    "citation_count": 1,
    "authors": [
      "Burak Satar",
      "Zhixin Ma",
      "Patrick Amadeus Irawan",
      "Wilfried Ariel Mulyawan",
      "Jing Jiang",
      "Ee-Peng Lim",
      "Chong-Wah Ngo"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1132": {
    "title": "GRADA: Graph-based Reranking against Adversarial Documents Attack",
    "volume": "main",
    "abstract": "Retrieval Augmented Generation (RAG) frameworks can improve the factual accuracy of large language models (LLMs) by integrating external knowledge from retrieved documents, thereby overcoming the limitations of models' static intrinsic knowledge. However, these systems are susceptible to adversarial attacks that manipulate the retrieval process by introducing documents that are adversarial yet semantically similar to the query. Notably, while these adversarial documents resemble the query, they exhibit weak similarity to benign documents in the retrieval set. Thus, we propose a simple yet effective **G**raph-based **R**eranking against **A**dversarial **D**ocument **A**ttacks (GRADA) framework aiming at preserving retrieval quality while significantly reducing the success of adversaries. Our study evaluates the effectiveness of our approach through experiments conducted on six LLMs: GPT-3.5-Turbo, GPT-4o, Llama3.1-8b-Instruct, Llama3.1-70b-Instruct, Qwen2.5-7b-Instruct and Qwen2.5-14b-Instruct. We use three datasets to assess performance, with results from the Natural Questions dataset demonstrating up to an 80% reduction in attack success rates while maintaining minimal loss in accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingjie Zheng",
      "Aryo Pradipta Gema",
      "Giwon Hong",
      "Xuanli He",
      "Pasquale Minervini",
      "Youcheng Sun",
      "Qiongkai Xu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1133": {
    "title": "Orchestrating Audio: Multi-Agent Framework for Long-Video Audio Synthesis",
    "volume": "main",
    "abstract": "Video-to-audio synthesis, which generates synchronized audio for visual content, critically enhances viewer immersion and narrative coherence in film and interactive media. However, video-to-audio dubbing for long-form content remains an unsolved challenge due to dynamic semantic shifts, audio diversity and the absence of dedicated datasets. While existing methods excel in short videos, they falter in long scenarios (e.g., movies) due to fragmented synthesis and inadequate cross-scene consistency. We propose LVAS-Agent, a multi-agent framework that offers a coordinated, multi-component approach to long-video audio generation. Our approach decomposes long-video synthesis into four steps including scene segmentation, script generation, audio design and audio synthesis. To enable systematic evaluation, we introduce LVAS-Bench, the first benchmark with 207 professionally curated long videos spanning diverse scenarios. Experiments show that our method outperforms state-of-the-art V2A models in overall audio synthesis quality",
    "checked": true,
    "id": "5d6721526a408524aa3b20f6e5dd31412f5ed204",
    "semantic_title": "orchestrating audio: multi-agent framework for long-video audio synthesis",
    "citation_count": 0,
    "authors": [
      "Yehang Zhang",
      "Xinli Xu",
      "Xiaojie Xu",
      "Doudou Zhang",
      "Li Liu",
      "Ying-Cong Chen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1134": {
    "title": "MADAWSD: Multi-Agent Debate Framework for Adversarial Word Sense Disambiguation",
    "volume": "main",
    "abstract": "Word sense disambiguation (WSD) is a fundamental yet challenging task in natural language processing. In recent years, the advent of large language models (LLMs) has led to significant advancements in regular WSD tasks. However, most existing LLMs face two major issues that hinder their performance in WSD. Firstly, these models are often prone to misclassifying the correct meaning of an ambiguous word when confronted with contexts containing adversarial information. Secondly, there is a lack of sufficient adversarial WSD datasets, which severely limits the development and evaluation of adversarial WSD systems. To address these gaps, we propose a novel Multi-Agent Debate framework for Adversarial Word Sense Disambiguation (MADAWSD). The MADAWSD framework simulates a real-world debate environment where multiple agent roles, namely, the Debater, Moderator, Consensus-seeker, and Judge, engage in discussions about ambiguous words in the context of adversarial information. Through a collaborative mechanism among these agents, it achieves accurate WSD. Additionally, a novel dataset for Chinese adversarial WSD has been constructed, focusing on improving and evaluating the performance of WSD models in the Chinese language. Extensive experiments on both English and Chinese adversarial WSD datasets demonstrate that MADAWSD can seamlessly integrate with existing LLMs and significantly enhance their performance, showcasing broad generality and outstanding effectiveness",
    "checked": true,
    "id": "ef0b693cf8133a0643c9046e70c7b5dad4266a6a",
    "semantic_title": "madawsd: multi-agent debate framework for adversarial word sense disambiguation",
    "citation_count": 0,
    "authors": [
      "Kaiyuan Zhang",
      "Qian Liu",
      "Luyang Zhang",
      "Chaoqun Zheng",
      "Shuaimin Li",
      "Bing Xu",
      "Muyun Yang",
      "Xinxiao Qiao",
      "Wenpeng Lu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1135": {
    "title": "Interpretable Text Embeddings and Text Similarity Explanation: A Survey",
    "volume": "main",
    "abstract": "Text embeddings are a fundamental component in many NLP tasks, including classification, regression, clustering, and semantic search. However, despite their ubiquitous application, challenges persist in interpreting embeddings and explaining similarities between them.In this work, we provide a structured overview of methods specializing in inherently interpretable text embeddings and text similarity explanation, an underexplored research area. We characterize the main ideas, approaches, and trade-offs. We compare means of evaluation, discuss overarching lessons learned and finally identify opportunities and open challenges for future research",
    "checked": true,
    "id": "e82d5b17d58e370864eb446f49aa918c6fbaafe7",
    "semantic_title": "interpretable text embeddings and text similarity explanation: a survey",
    "citation_count": 2,
    "authors": [
      "Juri Opitz",
      "Lucas Moeller",
      "Andrianos Michail",
      "Sebastian Padó",
      "Simon Clematide"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1136": {
    "title": "Dyve: Thinking Fast and Slow for Dynamic Process Verification",
    "volume": "main",
    "abstract": "Large Language Models have advanced significantly in complex reasoning, often leveraging external reward model to improve the reliability of their multi-step processes. However, existing process verification methods struggle with reliably assessing incomplete reasoning traces and are limited by the cost of high-quality human annotations or the inherent noise in automatically generated labels. Therefore, we present Dyve, a dynamic process verifier that enhances reasoning error detection in large language models by integrating fast and slow thinking, inspired by Kahneman's Systems Theory. Dyve adaptively applies immediate token-level confirmation (System 1) for straightforward steps and comprehensive analysis (System 2) for complex ones. Unlike traditional verifiers that only evaluate final outputs, Dyve employs a step-wise consensus-filtered supervision strategy, leveraging Monte Carlo estimation, LLM-as-a-Judge, and specialized reasoning models to extract high-quality training signals from noisy rollouts. Experimental results on ProcessBench and the MATH dataset confirm that Dyve significantly outperforms existing process-based verifiers and boosts performance in Best-of-N settings while maintaining computational efficiency by strategically allocating verification resources",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianyuan Zhong",
      "Zeju Li",
      "Zhijian Xu",
      "Xiangyu Wen",
      "Qiang Xu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1137": {
    "title": "PERSEVAL: A Framework for Perspectivist Classification Evaluation",
    "volume": "main",
    "abstract": "Data perspectivism goes beyond majority vote label aggregation by recognizing various perspectives as legitimate ground truths.However, current evaluation practices remain fragmented, making it difficult to compare perspectivist approaches and analyze their impact on different users and demographic subgroups. To address this gap, we introduce PersEval, the first unified framework for evaluating perspectivist models in NLP. A key innovation is its evaluation at the individual annotator level and its treatment of annotators and users as distinct entities, consistently with real-world scenarios. We demonstrate PersEval's capabilities through experiments with both Encoder-based and Decoder-based approaches, as well as an analysis of the effect of sociodemographic prompting. By considering global, text-, trait- and user-level evaluation metrics, we show that PersEval is a powerful tool for examining how models are influenced by user-specific information and identifying the biases this information may introduce",
    "checked": true,
    "id": "4a9a95dd11a07cba1ac43d83de4702156b4c967e",
    "semantic_title": "perseval: a framework for perspectivist classification evaluation",
    "citation_count": 1,
    "authors": [
      "Soda Marem Lo",
      "Silvia Casola",
      "Erhan Sezerer",
      "Valerio Basile",
      "Franco Sansonetti",
      "Antonio Uva",
      "Davide Bernardi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1138": {
    "title": "Massive Supervised Fine-tuning Experiments Reveal How Data, Layer, and Training Factors Shape LLM Alignment Quality",
    "volume": "main",
    "abstract": "Supervised fine-tuning (SFT) is a critical step in aligning large language models (LLMs) with human instructions and values, yet many aspects of SFT remain poorly understood. We trained a wide range of base models on a variety of datasets including code generation, mathematical reasoning, and general-domain tasks, resulting in 1,000+ SFT models under controlled conditions. We then identified the dataset properties that matter most and examined the layer-wise modifications introduced by SFT.Our findings reveal that some training–task synergies persist across all models while others vary substantially, emphasizing the importance of model-specific strategies. Moreover, we demonstrate that perplexity consistently predicts SFT effectiveness, often surpassing superficial similarity between the training data and the benchmark, and that mid-layer weight changes correlate most strongly with performance gains. We release these 1,000+ SFT models and benchmark results to accelerate further research. All resources are available at https://github.com/llm-jp/massive-sft",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuto Harada",
      "Yusuke Yamauchi",
      "Yusuke Oda",
      "Yohei Oseki",
      "Yusuke Miyao",
      "Yu Takagi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1139": {
    "title": "IndiGEC: Multilingual Grammar Error Correction for Low-Resource Indian Languages",
    "volume": "main",
    "abstract": "Grammatical Error Correction (GEC) for low-resource Indic languages faces significant challenges due to the scarcity of annotated data. In this work, we introduce the Mask-Translate&Fill (MTF) framework, a novel approach for generating high-quality synthetic data for GEC using only monolingual corpora. MTF leverages a machine translation system and a pretrained masked language model to introduce synthetic errors and tries to mimic errors made by second-language learners. Our experimental results on English, Hindi, Bengali, Marathi, and Tamil demonstrate that MTF consistently outperforms other monolingual synthetic data generation methods and achieves performance comparable to the Translation Language Modeling (TLM)-based approach, which uses a bilingual corpus, in both independent and multilingual settings. Under multilingual training, MTF yields significant improvements across Indic languages, with particularly notable gains in Bengali and Tamil, achieving +1.6 and +3.14 GLEU over the TLM-based method, respectively. To support further research, we also introduce the IndiGEC Corpus, a high-quality, human-written, manually validated GEC dataset for these four Indic languages, comprising over 8,000 sentence pairs with separate development and test splits",
    "checked": true,
    "id": "a8ab58b89a4221316098abe76a51c162f6c4cea9",
    "semantic_title": "indigec: multilingual grammar error correction for low-resource indian languages",
    "citation_count": 0,
    "authors": [
      "Ujjwal Sharma",
      "Pushpak Bhattacharyya"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1140": {
    "title": "Bias Beware: The Impact of Cognitive Biases on LLM-Driven Product Recommendations",
    "volume": "main",
    "abstract": "The advent of Large Language Models (LLMs) has revolutionized product recommenders, yet their susceptibility to adversarial manipulation poses critical challenges, particularly in real-world commercial applications. Our approach is the first one to tap into human psychological principles, seamlessly modifying product descriptions, making such manipulations hard to detect. In this work, we investigate cognitive biases as black-box adversarial strategies, drawing parallels between their effects on LLMs and human purchasing behavior. Through extensive evaluation across models of varying scale, we find that certain biases, such as social proof, consistently boost product recommendation rate and ranking, while others, like scarcity and exclusivity, surprisingly reduce visibility. Our results demonstrate that cognitive biases are deeply embedded in state-of-the-art LLMs, leading to highly unpredictable behavior in product recommendations and posing significant challenges for effective mitigation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giorgos Filandrianos",
      "Angeliki Dimitriou",
      "Maria Lymperaiou",
      "Konstantinos Thomas",
      "Giorgos Stamou"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1141": {
    "title": "T2R-BENCH: A Benchmark for Real World Table-to-Report Task",
    "volume": "main",
    "abstract": "Extensive research has been conducted to explore the capabilities of large language models (LLMs) in table reasoning. However, the essential task of transforming tables information into reports remains a significant challenge for industrial applications. This task is plagued by two critical issues: 1) the complexity and diversity of tables lead to suboptimal reasoning outcomes; and 2) existing table benchmarks lack the capacity to adequately assess the practical application of this task. To fill this gap, we propose the table-to-report task and construct a bilingual benchmark named T2R-bench, where the key information flow from the tables to the reports for this task. The benchmark comprises 457 industrial tables, all derived from real-world scenarios and encompassing 19 industry domains as well as four types of industrial tables. Furthermore, we propose a novel evaluation criteria to fairly measure the quality of report generation. Expeimental results show that Deepseek-R1 only achieves the best performance with 62.71% overall score, indicating that LLMs still have room for improvement on T2R-bench",
    "checked": true,
    "id": "1ce3c796c530926108f5fca9f5b3faefb8bfeadc",
    "semantic_title": "t2r-bench: a benchmark for real world table-to-report task",
    "citation_count": 0,
    "authors": [
      "Jie Zhang",
      "Changzai Pan",
      "Sishi Xiong",
      "Kaiwen Wei",
      "Yu Zhao",
      "Xiangyu Li",
      "Jiaxin Peng",
      "Xiaoyan Gu",
      "Jian Yang",
      "Wenhan Chang",
      "Zhenhe Wu",
      "Jiang Zhong",
      "Shuangyong Song",
      "Xuelong Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1142": {
    "title": "TCP: a Benchmark for Temporal Constraint-Based Planning",
    "volume": "main",
    "abstract": "Temporal reasoning and planning are essential capabilities for large language models (LLMs), yet most existing benchmarks evaluate them in isolation and under limited forms of complexity. To address this gap, we introduce the Temporal Constraint-based Planning (TCP) benchmark, that jointly assesses both capabilities. Each instance in TCP features a naturalistic dialogue around a collaborative project, where diverse and interdependent temporal constraints are explicitly or implicitly expressed, and models must infer an optimal schedule that satisfies all constraints. To construct TCP, we generate abstract problem prototypes that are then paired with realistic scenarios from various domains and enriched into dialogues using an LLM. A human quality check is performed on a sampled subset to confirm the reliability of our benchmark. We evaluate state-of-the-art LLMs and find that even the strongest models may struggle with TCP, highlighting its difficulty and revealing limitations in LLMs' temporal constraint-based planning abilities. We analyze underlying failure cases, open source our benchmark, and hope our findings can inspire future research",
    "checked": true,
    "id": "7b9904325c54357b5085c8277dc158abf05b19a9",
    "semantic_title": "tcp: a benchmark for temporal constraint-based planning",
    "citation_count": 3,
    "authors": [
      "Zifeng Ding",
      "Sikuan Yan",
      "Moy Yuan",
      "Xianglong Hu",
      "Fangru Lin",
      "Andreas Vlachos"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1143": {
    "title": "The Role of Outgoing Connection Heterogeneity in Feedforward Layers of Large Language Models",
    "volume": "main",
    "abstract": "We report on investigations into the characteristics of outgoing connections in feedforward layers of large language models. Our findings show that inner neurons with diverse outgoing connection strengths are more critical to model performance than those with uniform connections. We propose a new fine-tuning loss that takes advantage of this observation by decreasing the outgoing connection entropy in feedforward layers. Using this loss yields gains over standard fine-tuning across two different model families (PaLM-2 and Gemma-2) for downstream tasks in math, coding, and language understanding. To further elucidate the role of outgoing connection heterogeneity, we develop a data-free structured pruning method, which uses entropy to identify and remove neurons. This method is considerably more effective than removing neurons either randomly or based on their magnitude",
    "checked": true,
    "id": "f8301980dac61dd2ddc312ff883a4609570604b4",
    "semantic_title": "the role of outgoing connection heterogeneity in feedforward layers of large language models",
    "citation_count": 0,
    "authors": [
      "Felix Stahlberg",
      "Shankar Kumar"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1144": {
    "title": "Follow the Flow: Fine-grained Flowchart Attribution with Neurosymbolic Agents",
    "volume": "main",
    "abstract": "Flowcharts are a critical tool for visualizing decision-making processes. However, their non-linear structure and complex visual-textual relationships make it challenging to interpret them using LLMs, as vision-language models frequently hallucinate nonexistent connections and decision paths when analyzing these diagrams. This leads to compromised reliability for automated flowchart processing in critical domains such as logistics, health, and engineering. We introduce the task of Fine-grained Flowchart Attribution, which traces specific components grounding a flowchart referring LLM response. Flowchart Attribution ensures the verifiability of LLM predictions and improves explainability by linking generated responses to the flowchart's structure. We propose FlowPathAgent, a neurosymbolic agent that performs fine-grained post hoc attribution through graph-based reasoning. It first segments the flowchart, then converts it into a structured symbolic graph, and then employs an agentic approach to dynamically interact with the graph, to generate attribution paths. Additionally, we present FlowExplainBench, a novel benchmark for evaluating flowchart attributions across diverse styles, domains, and question types. Experimental results show that FlowPathAgent mitigates visual hallucinations in LLM answers over flowchart QA, outperforming strong baselines by 10–14% on our proposed FlowExplainBench dataset",
    "checked": true,
    "id": "3d088ed24f60df323f1ec6083ead19dc804b4bef",
    "semantic_title": "follow the flow: fine-grained flowchart attribution with neurosymbolic agents",
    "citation_count": 0,
    "authors": [
      "Manan Suri",
      "Puneet Mathur",
      "Nedim Lipka",
      "Franck Dernoncourt",
      "Ryan A. Rossi",
      "Vivek Gupta",
      "Dinesh Manocha"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1145": {
    "title": "Collaborative Rational Speech Act: Pragmatic Reasoning for Multi-Turn Dialog",
    "volume": "main",
    "abstract": "As AI systems take on collaborative roles, they must reason about shared goals and beliefs—not just generate fluent language. The Rational Speech Act (RSA) framework offers a principled approach to pragmatic reasoning, but existing extensions face challenges in scaling to multi-turn, collaborative scenarios. In this paper, we introduce Collaborative Rational Speech Act (CRSA), an information-theoretic (IT) extension of RSA that models multi-turn dialog by optimizing a gain function adapted from rate-distortion theory. This gain is an extension of the gain model that is maximized in the original RSA model but takes into account the scenario in which both agents in a conversation have private information and produce utterances conditioned on the dialog. We demonstrate the effectiveness of CRSA on referential games and template-based doctor–patient dialogs in the medical domain. Empirical results show that CRSA yields more consistent, interpretable, and collaborative behavior than existing baselines—paving the way for more pragmatic and socially aware language agents",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lautaro Estienne",
      "Gabriel Ben Zenou",
      "Nona Naderi",
      "Jackie CK Cheung",
      "Pablo Piantanida"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1146": {
    "title": "Understanding Subword Compositionality of Large Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) take sequences of subwords as input, requiring them to effective compose subword representations into meaningful word-level representations. In this paper, we present a comprehensive set of experiments to probe how LLMs compose subword information, focusing on three key aspects: structural similarity, semantic decomposability, and form retention. Our analysis of the experiments suggests that these five LLM families can be classified into three distinct groups, likely reflecting difference in their underlying composition strategies. Specifically, we observe (i) three distinct patterns in the evolution of structural similarity between subword compositions and whole-word representations across layers; (ii) great performance when probing layer by layer their sensitivity to semantic decompositionality; and (iii) three distinct patterns when probing sensitivity to formal features, e.g., character sequence length. These findings provide valuable insights into the compositional dynamics of LLMs and highlight different compositional pattens in how LLMs encode and integrate subword information",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiwei Peng",
      "Yekun Chai",
      "Anders Søgaard"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1147": {
    "title": "Internal Chain-of-Thought: Empirical Evidence for Layer‐wise Subtask Scheduling in LLMs",
    "volume": "main",
    "abstract": "We show that large language models (LLMs) exhibit an internal chain-of-thought: they sequentially decompose and execute composite tasks layer-by-layer. Two claims ground our study: (i) distinct subtasks are learned at different network depths, and (ii) these subtasks are executed sequentially across layers. On a benchmark of 15 two-step composite tasks, we employ layer-from context-masking and propose a novel cross-task patching method, confirming (i). To examine claim (ii), we apply LogitLens to decode hidden states, revealing a consistent layerwise execution pattern. We further replicate our analysis on the real-world TRACE benchmark, observing the same stepwise dynamics. Together, our results enhance LLMs transparency by showing their capacity to internally plan and execute subtasks (or instructions), opening avenues for fine-grained, instruction-level activation steering",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhipeng Yang",
      "Junzhuo Li",
      "Siyu Xia",
      "Xuming Hu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1148": {
    "title": "From Understanding to Generation: An Efficient Shortcut for Evaluating Language Models",
    "volume": "main",
    "abstract": "Iterative evaluation of LLMs during training is essential to ensure expected capability development, but can be time- and compute-intensive. While NLU tasks, where the model selects from fixed answer choices, are cheap to evaluate, essential capabilities like reasoning and code generation rely on the more time-consuming NLG (token-by-token generation) format. In this work, our aim is to decrease the computational burden of NLG benchmarks in order to enable monitoring crucial LLM capabilities during model training. We reformulate generative tasks into computationally cheaper NLU alternatives. We test the performance correlation between the original and reformulated tasks using 8 LMs of various sizes and 4 capabilities: mathematical reasoning, code generation, factual knowledge and reading comprehension. Our results show a strong correlation between task formats, supporting capability assessment via cheaper alternatives and achieving over 35x average reduction in evaluation time. Our project is available at: https://github.com/Fraunhofer-IIS/EvalShortcut",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Viktor Hangya",
      "Fabian Küch",
      "Darina Gold"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1149": {
    "title": "Debiasing Multilingual LLMs in Cross-lingual Latent Space",
    "volume": "main",
    "abstract": "Debiasing techniques such as SentDebias aim to reduce bias in large language models (LLMs). Previous studies have evaluated their cross-lingual transferability by directly applying these methods to LLM representations, revealing their limited effectiveness across languages. In this work, we therefore propose to perform debiasing in a joint latent space rather than directly on LLM representations. We construct a well-aligned cross-lingual latent space using an autoencoder trained on parallel TED talk scripts. Our experiments with Aya-expanse and two debiasing techniques across four languages (English, French, German, Dutch) demonstrate that a) autoencoders effectively construct a well-aligned cross-lingual latent space, and b) applying debiasing techniques in the learned cross-lingual latent space significantly improves both the overall debiasing performance and cross-lingual transferability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiwei Peng",
      "Guimin Hu",
      "Yekun Chai",
      "Anders Søgaard"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1150": {
    "title": "Context is Gold to find the Gold Passage: Evaluating and Training Contextual Document Embeddings",
    "volume": "main",
    "abstract": "A limitation of modern document retrieval embedding methods is that they typically encode passages (chunks) from the same documents independently, often overlooking crucial contextual information from the rest of the document that could greatly improve individual chunk representations.In this work, we introduce ConTEB (Context-aware Text Embedding Benchmark), a benchmark designed to evaluate retrieval models on their ability to leverage document-wide context. Our results show that state-of-the-art embedding models struggle in retrieval scenarios where context is required. To address this limitation, we propose InSeNT (In-sequence Negative Training), a novel contrastive post-training approach which combined with late chunking pooling enhances contextual representation learning while preserving computational efficiency. Our method significantly improves retrieval quality on ConTEB without sacrificing base model performance. We further find chunks embedded with our method are more robust to suboptimal chunking strategies and larger retrieval corpus sizes.We open-source all artifacts at https://github.com/illuin-tech/contextual-embeddings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Max Conti",
      "Manuel Faysse",
      "Gautier Viaud",
      "Antoine Bosselut",
      "Celine Hudelot",
      "Pierre Colombo"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1151": {
    "title": "MS-RAG: Simple and Effective Multi-Semantic Retrieval-Augmented Generation",
    "volume": "main",
    "abstract": "To alleviate the hallucination problem of large language model (LLM), retrieval-augmented generation (RAG) has been proposed and widely adopted. Due to the limitations in cross-chunk summarization task of naive RAG, graph-based RAG has emerged as a promising solution. However, a close study reveals several flaws in these works. First, most graph-based RAGs suffer from less efficient indexing process, which leads to information loss and expensive costs. Second, they heavily rely on LLM for retrieval thus inference slowly, which hinders their application in industry. To build a more efficient and effective RAG, we propose the multi-semantic RAG (MS-RAG). In this work, we combine knowledge graphs with dense vector to build a multi-semantic RAG. To be specific, (i) at indexing stage, we create multiple semantic-level indexes, including chunk-level, relation-level, and entity-level, to leverage the merits of dense vector and knowledge graph. (ii) at retrieval stage, unlike the previous LLM-empowered entity extraction, we propose a novel mix recall algorithm. Finally, we employ a multi-semantic rerank module to purify the results. Extensive experiments show that MS-RAG achieves superior performance. In terms of retrieval effect, MS-RAG achieves state-of-the-art performance, which is about 10%-30% improvement than the existing methods. In terms of question-answering effect, MS-RAG still achieves promising results with faster inference speed. More analysis and experiments are provided in Appendix",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaozhou You",
      "Yahui Luo",
      "Lihong Gu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1152": {
    "title": "Transitive self-consistency evaluation of NLI models without gold labels",
    "volume": "main",
    "abstract": "Natural Language Inference (NLI) is an important task in natural language processing. NLI models are aimed at automatically determining logical relationships between pairs of sentences. However, recent studies based on gold labels assigned to sentence pairs by human experts have provided some evidence that NLI models tend to make inconsistent model decisions during inference. Previous studies have used existing NLI datasets to test the transitive consistency of language models. However, they test only variations of two transitive consistency rules out of four. To further evaluate the transitive consistency of NLI models, we propose a novel evaluation approach that allows us to test all four rules automatically by generating adversarial examples via antonym replacements. Since we are testing self-consistency, human labeling of generated adversarial examples is unnecessary. Our experiments on several benchmark datasets indicate that the examples generated by the proposed antonym replacement methodology can reveal transitive inconsistencies in the state-of-the-art NLI models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Wu",
      "Mark Last"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1153": {
    "title": "MiLQ: Benchmarking IR Models for Bilingual Web Search with Mixed Language Queries",
    "volume": "main",
    "abstract": "Despite bilingual speakers frequently using mixed-language queries in web searches, Information Retrieval (IR) research on them remains scarce. To address this, we introduce ***MiLQ***, ***Mi***xed-***L***anguage ***Q***uery test set, the first public benchmark of mixed-language queries, qualified as realistic and relatively preferred. Experiments show that multilingual IR models perform moderately on MiLQ and inconsistently across native, English, and mixed-language queries, also suggesting code-switched training data's potential for robust IR models handling such queries. Meanwhile, intentional English mixing in queries proves an effective strategy for bilinguals searching English documents, which our analysis attributes to enhanced token matching compared to native queries",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonghwi Kim",
      "Deokhyung Kang",
      "Seonjeong Hwang",
      "Yunsu Kim",
      "Jungseul Ok",
      "Gary Lee"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1154": {
    "title": "Enhancing Chinese Offensive Language Detection with Homophonic Perturbation",
    "volume": "main",
    "abstract": "Detecting offensive language in Chinese is challenging due to homophonic substitutions used to evade detection. We propose a framework to improve large language models' robustness against such phonetic attacks. First, we construct HED-COLD, the first large-scale and systematic homophonic dataset for Chinese offensive language detection. Additionally, we design a homophone-aware pretraining strategy that learns the mappings among orthography, phonetics, and semantics between original and perturbed text. Experimental results show that our approach achieves state-of-the-art performance on both the COLD test set and the toxicity benchmark ToxiCloakCN. Notably, it achieves greater gains in domains susceptible to homophonic attacks, such as gender and regional content. These results demonstrate improved robustness and generalization against phonetic adversarial attacks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junqi Wu",
      "Shujie Ji",
      "Kang Zhong",
      "Huiling Peng",
      "Zhendongxiao",
      "Xiongding Liu",
      "Wu Wei"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1155": {
    "title": "Persona-Augmented Benchmarking: Evaluating LLMs Across Diverse Writing Styles",
    "volume": "main",
    "abstract": "Current benchmarks for evaluating Large Language Models (LLMs) often do not exhibit enough writing style diversity, with many adhering primarily to standardized conventions. Such benchmarks do not fully capture the rich variety of communication patterns exhibited by humans. Thus, it is possible that LLMs, which are optimized on these benchmarks, may demonstrate brittle performance when faced with \"non-standard\" input. In this work, we test this hypothesis by rewriting evaluation prompts using persona-based LLM prompting, a low-cost method to emulate diverse writing styles. Our results show that, even with identical semantic content, variations in writing style and prompt formatting significantly impact the estimated performance of the LLM under evaluation. Notably, we identify distinct writing styles that consistently trigger either low or high performance across a range of models and tasks, irrespective of model family, size, or recency. Our work offers a scalable approach to augment existing benchmarks, improving the external validity of the assessments they provide for LLM performance across linguistic variations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kimberly Truong",
      "Riccardo Fogliato",
      "Hoda Heidari",
      "Steven Wu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1156": {
    "title": "Computational Analysis of Character Development in Holocaust Testimonies",
    "volume": "main",
    "abstract": "This work presents a computational approach to analyze character development along the narrative timeline. The analysis characterizes changes in the protagonist's views and behavior and the interplay between them. We consider transcripts of Holocaust survivor testimonies as a test case, each telling the story of an individual in first-person terms. We focus on the survivor's religious trajectory, examining the evolution of their disposition toward religious belief and practice as it is reflected in the testimony. Clustering the resulting trajectories in the dataset, we identify common sequences in the data. Our findings highlight multiple common structures of religiosity across the narratives: in terms of belief, a constant disposition is common, while for practice, most present an oscillating structure, serving as valuable material for historical and sociological research. This work demonstrates the potential of natural language processing for analyzing character evolution through thematic trajectories in narratives",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Esther Shizgal",
      "Eitan Wagner",
      "Renana Keydar",
      "Omri Abend"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1157": {
    "title": "TASO: Task-Aligned Sparse Optimization for Parameter-Efficient Model Adaptation",
    "volume": "main",
    "abstract": "LoRA has become one of the most widely used parameter-efficient fine-tuning methods due to its simplicity and effectiveness. However, numerous studies have shown that LoRA often introduces substantial parameter redundancy, which not only increases the number of trainable parameters but also hinders the effectiveness of fine-tuning. Since identifying redundant parameters in LoRA is inherently difficult, how to eliminate them efficiently and accurately remains a challenging problem. In this paper, we propose TASO, a redundancy reduction method that leverages importance information from the pretrained model's weights to mitigate LoRA redundancy. Specifically, we estimate parameter importance on downstream tasks and identify task-specific core regions based on the distribution of importance scores. The location information of these core regions is then used to determine the sparse structure of LoRA modules, enabling redundancy removal before fine-tuning. Our approach significantly reduces the number of trainable parameters required for task adaptation, while providing a novel task-aligned perspective for LoRA redundancy reduction. Experimental results demonstrate that, with a parameter budget comparable to LoRA with rank r = 1, TASO consistently outperforms standard LoRA across multiple tasks, achieving strong fine-tuning performance while effectively eliminating redundant parameters",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daiye Miao",
      "Yufang Liu",
      "Jie Wang",
      "Changzhi Sun",
      "Yunke Zhang",
      "Demei Yan",
      "Shaokang Dong",
      "Qi Zhang",
      "Yuanbin Wu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1158": {
    "title": "Dual-Path Counterfactual Integration for Multimodal Aspect-Based Sentiment Classification",
    "volume": "main",
    "abstract": "Multimodal aspect-based sentiment classification (MABSC) requires fine-grained reasoning over both textual and visual content to infer sentiments toward specific aspects. However, existing methods often rely on superficial correlations—particularly between aspect terms and sentiment labels—leading to poor generalization and vulnerability to spurious cues. To address this limitation, we propose DPCI, a novel Dual-Path Counterfactual Integration framework that enhances model robustness by explicitly modeling counterfactual reasoning in multimodal contexts. Specifically, we design a dual counterfactual generation module that simulates two types of interventions: replacing aspect terms and rewriting descriptive content, thereby disentangling the spurious dependencies from causal sentiment cues. We further introduce a sample-aware counterfactual selection strategy to retain high-quality, diverse counterfactuals tailored to each generation path. Finally, a confidence-guided integration mechanism adaptively fuses counterfactual signals into the main prediction stream. Extensive experiments on standard MABSC benchmarks demonstrate that DPCI not only achieves state-of-the-art performance but also significantly improves model robustness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Liu",
      "Jiahao Cao",
      "Jiaqian Ren",
      "Xu Bai",
      "Yanan Cao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1159": {
    "title": "Job Unfair: An Investigation of Gender and Occupational Bias in Free-Form Text Completions by LLMs",
    "volume": "main",
    "abstract": "Disentangling how gender and occupations are encoded by LLMs is crucial to identify possible biases and prevent harms, especially given the widespread use of LLMs in sensitive domains such as human resources.In this work, we carry out an in-depth investigation of gender and occupational biases in English and Italian as expressed by 9 different LLMs (both base and instruction-tuned). Specifically, we focus on the analysis of sentence completions when LLMs are prompted with job-related sentences including different gender representations. We carry out a manual analysis of 4,500 generated texts over 4 dimensions that can reflect bias, we propose a novel embedding-based method to investigate biases in generated texts and, finally, we carry out a lexical analysis of the model completions. In our qualitative and quantitative evaluation we show that many facets of social bias remain unaccounted for even in aligned models, and LLMs in general still reflect existing gender biases in both languages. Finally, we find that models still struggle with gender-neutral expressions, especially beyond English",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Camilla Casula",
      "Sebastiano Vecellio Salto",
      "Elisa Leonardelli",
      "Sara Tonelli"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1160": {
    "title": "C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring Challenges in Complex Conversations",
    "volume": "main",
    "abstract": "Spoken Dialogue Models (SDMs) have recently attracted significant attention for their ability to generate voice responses directly to users' spoken queries. Despite their increasing popularity, there exists a gap in research focused on comprehensively understanding their practical effectiveness in comprehending and emulating human conversations. This is especially true compared to text-based Large Language Models (LLMs), which benefit from extensive benchmarking. Human voice interactions are inherently more complex than text due to characteristics unique to spoken dialogue. Ambiguity poses one challenge, stemming from semantic factors like polysemy, as well as phonological aspects such as heterograph, heteronyms, and stress patterns. Additionally, context-dependency, like omission, coreference, and multi-turn interaction, adds further complexity to human conversational dynamics. To illuminate the current state of SDM development and to address these challenges, we present a benchmark dataset in this paper, which comprises 1,079 instances in English and Chinese. Accompanied by an LLM-based evaluation method that closely aligns with human judgment, this dataset facilitates a comprehensive exploration of the performance of SDMs in tackling these practical challenges",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengqian Ma",
      "Wei Tao",
      "Steven Y. Guo"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1161": {
    "title": "Understanding LLMs' Cross-Lingual Context Retrieval: How Good It Is And Where It Comes From",
    "volume": "main",
    "abstract": "Cross-lingual context retrieval (extracting contextual information in one language based on requests in another) is a fundamental aspect of cross-lingual alignment, but the performance and mechanism of it for large language models (LLMs) remains unclear. In this paper, we evaluate the cross-lingual context retrieval of over 40 LLMs across 12 languages, using cross-lingual machine reading comprehension (xMRC) as a representative scenario. Our results show that post-trained open LLMs show strong cross-lingual context retrieval ability, comparable to closed-source LLMs such as GPT-4o, and their estimated oracle performances greatly improve after post-training. Our mechanism analysis shows that the cross-lingual context retrieval process can be divided into two main phases: question encoding and answer retrieval, which are formed in pre-training and post-training respectively. The phasing stability correlates with xMRC performance, and the xMRC bottleneck lies at the last model layers in the second phase, where the effect of post-training can be evidently observed. Our results also indicate that larger-scale pretraining cannot improve the xMRC performance. Instead, larger LLMs need further multilingual post-training to fully unlock their cross-lingual context retrieval potential",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changjiang Gao",
      "Hankun Lin",
      "Xin Huang",
      "Xue Han",
      "Junlan Feng",
      "Chao Deng",
      "Jiajun Chen",
      "Shujian Huang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1162": {
    "title": "Blind Men and the Elephant: Diverse Perspectives on Gender Stereotypes in Benchmark Datasets",
    "volume": "main",
    "abstract": "Accurately measuring gender stereotypical bias in language models is a complex task with many hidden aspects. Current benchmarks have underestimated this multifaceted challenge and failed to capture the full extent of the problem. This paper examines the inconsistencies between intrinsic stereotype benchmarks. We propose that currently available benchmarks each capture only partial facets of gender stereotypes, and when considered in isolation, they provide just a fragmented view of the broader landscape of bias in language models. Using StereoSet and CrowS-Pairs as case studies, we investigated how data distribution affects benchmark results. By applying a framework from social psychology to balance the data of these benchmarks across various components of gender stereotypes, we demonstrated that even simple balancing techniques can significantly improve the correlation between different measurement approaches. Our findings underscore the complexity of gender stereotyping in language models and point to new directions for developing more refined techniques to detect and reduce bias",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mahdi Zakizadeh",
      "Mohammad Taher Pilehvar"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1163": {
    "title": "Linguistic and Embedding-Based Profiling of Texts Generated by Humans and Large Language Models",
    "volume": "main",
    "abstract": "The rapid advancements in large language models (LLMs) have significantly improved their ability to generate natural language, making texts generated by LLMs increasingly indistinguishable from human-written texts. While recent research has primarily focused on using LLMs to classify text as either human-written or machine-generated texts, our study focuses on characterizing these texts using a set of linguistic features across different linguistic levels such as morphology, syntax, and semantics. We select a dataset of human-written and machine-generated texts spanning 8 domains and produced by 11 different LLMs. We calculate different linguistic features such as dependency length and emotionality, and we use them for characterizing human-written and machine-generated texts along with different sampling strategies, repetition controls, and model release dates. Our statistical analysis reveals that human-written texts tend to exhibit simpler syntactic structures and more diverse semantic content. Furthermore, we calculate the variability of our set of features across models and domains. Both human- and machine-generated texts show stylistic diversity across domains, with human-written texts displaying greater variation in our features. Finally, we apply style embeddings to further test variability among human-written and machine-generated texts. Notably, newer models output text that is similarly variable, pointing to a homogenization of machine-generated texts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sergio E. Zanotto",
      "Segun Aroyehun"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1164": {
    "title": "An Interdisciplinary Approach to Human-Centered Machine Translation",
    "volume": "main",
    "abstract": "Machine Translation (MT) tools are widely used today, often in contexts where professional translators are not present. Despite progress in MT technology, a gap persists between system development and real-world usage, particularly for non-expert users who may struggle to assess translation reliability.This paper advocates for a human-centered approach to MT, emphasizing the alignment of system design with diverse communicative goals and contexts of use. We survey the literature in Translation Studies and Human-Computer Interaction to recontextualize MT evaluation and design to address the diverse real-world scenarios in which MT is used today",
    "checked": true,
    "id": "2e7b07be22eb622325a9400a69d49900e1c6f14e",
    "semantic_title": "an interdisciplinary approach to human-centered machine translation",
    "citation_count": 1,
    "authors": [
      "Marine Carpuat",
      "Omri Asscher",
      "Kalika Bali",
      "Luisa Bentivogli",
      "Fred Blain",
      "Lynne Bowker",
      "Monojit Choudhury",
      "Hal Daumé Iii",
      "Kevin Duh",
      "Ge Gao",
      "Alvin C Grissom II",
      "Marzena Karpinska",
      "Elaine C Khoong",
      "William D. Lewis",
      "Andre Martins",
      "Mary Nurminen",
      "Douglas W. Oard",
      "Maja Popovic",
      "Michel Simard",
      "François Yvon"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1165": {
    "title": "Exploring the Hidden Capacity of LLMs for One-Step Text Generation",
    "volume": "main",
    "abstract": "A recent study showed that large language models (LLMs) can reconstruct surprisingly long texts — up to thousands of tokens — via autoregressive generation from just one trained input embedding. In this work, we explore whether autoregressive decoding is essential for such reconstruction. We show that frozen LLMs can generate hundreds of accurate tokens in just one token-parallel forward pass, when provided with only two learned embeddings. This reveals a surprising and underexplored multi-token generation capability of autoregressive LLMs. We examine these embeddings and characterize the information they encode. We also empirically show that, although these representations are not unique for a given text, they form connected and local regions in embedding space — suggesting the potential to train a practical encoder. The existence of such representations hints that multi-token generation may be natively accessible in off-the-shelf LLMs via a learned input encoder, eliminating heavy retraining and helping to overcome the fundamental bottleneck of autoregressive decoding while reusing already-trained models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gleb Mezentsev",
      "Ivan Oseledets"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1166": {
    "title": "Mixture of Weight-shared Heterogeneous Group Attention Experts for Dynamic Token-wise KV Optimization",
    "volume": "main",
    "abstract": "Transformer models face scalability challenges in causal language modeling (CLM) due to inefficient memory allocation for growing key-value (KV) caches, which strains compute and storage resources. Existing methods like Grouped Query Attention (GQA) and token-level KV optimization improve efficiency but rely on rigid resource allocation, often discarding \"low-priority\" tokens or statically grouping them, failing to address the dynamic spectrum of token importance. We propose mixSGA, a novel mixture-of-expert (MoE) approach that dynamically optimizes token-wise computation and memory allocation. Unlike prior approaches, mixSGA retains all tokens while adaptively routing them to specialized experts with varying KV group sizes, balancing granularity and efficiency. Our key novelties include: (1) a token-wise expert-choice routing mechanism guided by learned importance scores, enabling proportional resource allocation without token discard; (2) weight-sharing across grouped attention projections to minimize parameter overhead; and (3) an auxiliary loss to ensure one-hot routing decisions for training-inference consistency in CLMs. Extensive evaluations across Llama3, TinyLlama, OPT, and Gemma2 model families show mixSGA's superiority over static baselines. On instruction-following and continued pretraining tasks, mixSGA achieves higher ROUGE-L and lower perplexity under the same KV budgets",
    "checked": true,
    "id": "4f92b00ce472ec2595dd4ad76f951840467bfffb",
    "semantic_title": "mixture of weight-shared heterogeneous group attention experts for dynamic token-wise kv optimization",
    "citation_count": 0,
    "authors": [
      "Guanghui Song",
      "Dongping Liao",
      "Yiren Zhao",
      "Kejiang Ye",
      "Cheng-zhong Xu",
      "Xitong Gao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1167": {
    "title": "PathwiseRAG: Multi-Dimensional Exploration and Integration Framework",
    "volume": "main",
    "abstract": "Conventional retrieval-augmented generation(RAG) systems employ rigid retrieval strategies that create: (1) knowledge blind spots across domain boundaries, (2) reasoning fragmentation when processing interdependent concepts, and (3) contradictions from conflicting evidence sources. Motivated by these limitations, we introduce PathwiseRAG, which addresses these challenges through: intent-aware strategy selection to eliminate blind spots, dynamic reasoning networks that capture sub-problem interdependencies to overcome fragmentation, and parallel path exploration with adaptive refinement to resolve conflicts. The framework models query intent across semantic and reasoning dimensions, constructs a directed acyclic graph of interconnected sub-problems, and explores multiple reasoning trajectories while continuously adapting to emerging evidence. Evaluation across challenging benchmarks demonstrates significant improvements over state-of-the-art RAG systems, with average accuracy gains of 4.9% and up to 6.9% on complex queries, establishing a new paradigm for knowledge-intensive reasoning by transforming static retrieval into dynamic, multi-dimensional exploration",
    "checked": true,
    "id": "7910c5312828f9a3df4955e550366c12e1623eb7",
    "semantic_title": "pathwiserag: multi-dimensional exploration and integration framework",
    "citation_count": 0,
    "authors": [
      "Hengrui Zhang",
      "Pin-Siang Huang",
      "Zhen Zhang",
      "Peican Lin",
      "Yao-Ching Yu",
      "Bo Hu",
      "Yulu Du"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1168": {
    "title": "Mm, Wat?\" Detecting Other-initiated Repair Requests in Dialogue",
    "volume": "main",
    "abstract": "Maintaining mutual understanding is a key component in human-human conversation to avoid conversation breakdowns, in which repair, particularly Other-Initiated Repair (OIR, when one speaker signals trouble and prompts the other to resolve), plays a vital role. However, Conversational Agents (CAs) still fail to recognize user repair initiation, leading to breakdowns or disengagement. This work proposes a multimodal model to automatically detect repair initiation in Dutch dialogues by integrating linguistic and prosodic features grounded in Conversation Analysis. The results show that prosodic cues complement linguistic features and significantly improve the results of pretrained text and audio embeddings, offering insights into how different features interact. Future directions include incorporating visual cues, exploring multilingual and cross-context corpora to assess the robustness and generalizability",
    "checked": true,
    "id": "f5e8b9da6ee915965a2a7be9041e960b816c8c3c",
    "semantic_title": "mm, wat?\" detecting other-initiated repair requests in dialogue",
    "citation_count": 0,
    "authors": [
      "Anh Ha Ngo",
      "Nicolas Rollet",
      "Catherine Pelachaud",
      "Chloé Clavel"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1169": {
    "title": "R-BPE: Improving BPE-Tokenizers with Token Reuse",
    "volume": "main",
    "abstract": "This paper presents R-BPE, a lightweight framework for adapting existing Byte-Pair Encoding (BPE) tokenizers to better support a specified target language. It reuses tokens from user-excluded languages and creates ID-based maps to resolve the new tokens of the chosen language. We evaluate R-BPE on Arabic as a target language. R-BPE reduced subword fertility by an average of 24.4% across the LLaMA 3.1 8B, Command R 35B, and Qwen 3 8B models. Applied to LLaMA 3.1 8B in continued pretraining mode, R-BPE yields a 7.33% reduction in training time. On the ArabicMMLU benchmark, the resulting model improved by 5.09 points on five in-domain topics and matched the original model's overall performance. It also preserved performance on EnglishMMLU. R-BPE effectively leverages existing models' tokenizers, embedding layers, and performance to better support target languages without incurring model size changes. We release an R-BPE implementation that is compatible with HuggingFace interfaces and thereby readily applicable to a wide range of existing models at https://acr.ps/1L9GPmL",
    "checked": true,
    "id": "4c386c6bb12f4db81cd64633566cf0d2e28c63d3",
    "semantic_title": "r-bpe: improving bpe-tokenizers with token reuse",
    "citation_count": 0,
    "authors": [
      "Nancy Hamdan",
      "Osama Rakan Al Mraikhat",
      "Fadi A. Zaraket"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1170": {
    "title": "Language Models Can be Efficiently Steered via Minimal Embedding Layer Transformations",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) are increasingly costly to fine-tune due to their size, with embedding layers alone accounting for up to 20% of model parameters. While Parameter-Efficient Fine-Tuning (PEFT) methods exist, they largely overlook the embedding layer. In this paper, we introduce TinyTE, a novel PEFT approach that steers model behavior via minimal translational transformations in the embedding space. TinyTE modifies input embeddings without altering hidden layers, achieving competitive performance while requiring approximately 0.0001% of the parameters needed for full fine-tuning. Experiments across architectures provide a new lens for understanding the relationship between input representations and model behavior—revealing them to be more flexible at their foundation than previously thought",
    "checked": true,
    "id": "eb84af17fc881c689a44aac60fc68db2792affdb",
    "semantic_title": "language models can be efficiently steered via minimal embedding layer transformations",
    "citation_count": 0,
    "authors": [
      "Diogo Tavares",
      "David Semedo",
      "Alexander Rudnicky",
      "Joao Magalhaes"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1171": {
    "title": "Adversarial Attacks Against Automated Fact-Checking: A Survey",
    "volume": "main",
    "abstract": "In an era where misinformation spreads freely, fact-checking (FC) plays a crucial role in verifying claims and promoting reliable information. While automated fact-checking (AFC) has advanced significantly, existing systems remain vulnerable to adversarial attacks that manipulate or generate claims, evidence, or claim-evidence pairs. These attacks can distort the truth, mislead decision-makers, and ultimately undermine the reliability of FC models. Despite growing research interest in adversarial attacks against AFC systems, a comprehensive, holistic overview of key challenges remains lacking. These challenges include understanding attack strategies, assessing the resilience of current models, and identifying ways to enhance robustness. This survey provides the first in-depth review of adversarial attacks targeting FC, categorizing existing attack methodologies and evaluating their impact on AFC systems. Additionally, we examine recent advancements in adversary-aware defenses and highlight open research questions that require further exploration. Our findings underscore the urgent need for resilient FC frameworks capable of withstanding adversarial manipulations in pursuit of preserving high verification accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fanzhen Liu",
      "Sharif Abuadbba",
      "Kristen Moore",
      "Surya Nepal",
      "Cecile Paris",
      "Jia Wu",
      "Jian Yang",
      "Quan Z. Sheng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1172": {
    "title": "WildDoc: How Far Are We from Achieving Comprehensive and Robust Document Understanding in the Wild?",
    "volume": "main",
    "abstract": "The rapid advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced capabilities in Document Understanding. However, prevailing benchmarks like DocVQA and ChartQA predominantly comprise scanned or digital documents, inadequately reflecting the intricate challenges posed by diverse real-world scenarios such as variable illumination and physical distortions. This paper introduces WildDoc, the inaugural benchmark designed specifically for assessing document understanding in natural environments. WildDoc incorporates a diverse set of manually captured document images reflecting real-world conditions and leverages document sources from established benchmarks to facilitate comprehensive comparisons with digital or scanned documents. Further, to rigorously evaluate model robustness, each document is captured four times under different conditions. Evaluations of state-of-the-art MLLMs on WildDoc expose substantial performance declines and underscore the models' inadequate robustness compared to traditional benchmarks, highlighting the unique challenges posed by real-world document understanding",
    "checked": true,
    "id": "94de6c79df326f89172df675a1aad543c464cfe3",
    "semantic_title": "wilddoc: how far are we from achieving comprehensive and robust document understanding in the wild?",
    "citation_count": 2,
    "authors": [
      "An-Lan Wang",
      "Jingqun Tang",
      "Lei Liao",
      "Hao Feng",
      "Qi Liu",
      "Xiang Fei",
      "Jinghui Lu",
      "Han Wang",
      "Hao Liu",
      "Yuliang Liu",
      "Xiang Bai",
      "Can Huang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1173": {
    "title": "DCR: Quantifying Data Contamination in LLMs Evaluation",
    "volume": "main",
    "abstract": "The rapid advancement of large language models (LLMs) has heightened concerns about benchmark data contamination (BDC), where models inadvertently memorize evaluation data during the training process, inflating performance metrics, and undermining genuine generalization assessment. This paper introduces the Data Contamination Risk (DCR) framework, a lightweight, interpretable pipeline designed to detect and quantify BDC risk across four granular levels: semantic, informational, data, and label. By synthesizing contamination scores via a fuzzy inference system, DCR produces a unified DCR Factor that adjusts raw accuracy to reflect contamination-aware performance. Validated on 9 LLMs (0.5B-72B) across sentiment analysis, fake news detection, and arithmetic reasoning tasks, the DCR framework reliably diagnoses contamination severity and with accuracy adjusted using the DCR Factor to within 4% average error across the three benchmarks compared to the uncontaminated baseline. Emphasizing computational efficiency and transparency, DCR provides a practical tool for integrating contamination assessment into routine evaluations, fostering fairer comparisons and enhancing the credibility of LLM benchmarking practices",
    "checked": true,
    "id": "3acaa8592735cd7fed7e4d1d251bfa894141e81b",
    "semantic_title": "dcr: quantifying data contamination in llms evaluation",
    "citation_count": 0,
    "authors": [
      "Cheng Xu",
      "Nan Yan",
      "Shuhao Guan",
      "Changhong Jin",
      "Yuke Mei",
      "Yibing Guo",
      "Tahar Kechadi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1174": {
    "title": "Building Trust in Clinical LLMs: Bias Analysis and Dataset Transparency",
    "volume": "main",
    "abstract": "Large language models offer transformative potential for healthcare, yet their responsible and equitable development depends critically on a deeper understanding of how training data characteristics influence model behavior, including the potential for bias. Current practices in dataset curation and bias assessment often lack the necessary transparency, creating an urgent need for comprehensive evaluation frameworks to foster trust and guide improvements. In this study, we present an in-depth analysis of potential downstream biases in clinical language models, with a focus on differential opioid prescription tendencies across diverse demographic groups, such as ethnicity, gender, and age. As part of this investigation, we introduce HC4: Healthcare Comprehensive Commons Corpus, a novel and extensively curated pretraining dataset exceeding 89 billion tokens. Our evaluation leverages both established general benchmarks and a novel, healthcare-specific methodology, offering crucial insights to support fairness and safety in clinical AI applications",
    "checked": true,
    "id": "de30e406de571b22c43443f32deb1fb5d9082ca8",
    "semantic_title": "building trust in clinical llms: bias analysis and dataset transparency",
    "citation_count": 0,
    "authors": [
      "Svetlana Maslenkova",
      "Clement Christophe",
      "Marco AF Pimentel",
      "Tathagata Raha",
      "Muhammad Umar Salman",
      "Ahmed Al Mahrooqi",
      "Avani Gupta",
      "Shadab Khan",
      "Ronnie Rajan",
      "Praveenkumar Kanithi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1175": {
    "title": "Surprise Calibration for Better In-Context Learning",
    "volume": "main",
    "abstract": "In-context learning (ICL) has emerged as a powerful paradigm for task adaptation in large language models (LLMs), where models infer underlying task structures from a few demonstrations. However, ICL remains susceptible to biases that arise from prior knowledge and contextual demonstrations, which can degrade the performance of LLMs. Existing bias calibration methods typically apply fixed class priors across all inputs, limiting their efficacy in dynamic ICL settings where the context for each query differs. To address these limitations, we adopt implicit sequential Bayesian inference as a framework for interpreting ICL, identify \"surprise\" as an informative signal for class prior shift, and introduce a novel method—Surprise Calibration (SC). SC leverages the notion of surprise to capture the temporal dynamics of class priors, providing a more adaptive and computationally efficient solution for in-context learning. We empirically demonstrate the superiority of SC over existing bias calibration techniques across a range of benchmark natural language processing tasks",
    "checked": true,
    "id": "80f27fa4dbc76fc62c20cdc5693abde5d3dc42a0",
    "semantic_title": "surprise calibration for better in-context learning",
    "citation_count": 0,
    "authors": [
      "Zhihang Tan",
      "Jingrui Hou",
      "Ping Wang",
      "Qibiao Hu",
      "Peng Zhu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1176": {
    "title": "SPARK: Simulating the Co-evolution of Stance and Topic Dynamics in Online Discourse with LLM-based Agents",
    "volume": "main",
    "abstract": "Topic evolution and stance dynamics are deeply intertwined in online social media, shaping the fragmentation and polarization of public discourse. Yet existing dynamic topic models and stance analysis approaches usually consider these processes in isolation, relying on abstractions that lack interpretability and agent-level behavioral fidelity. We present stance and topic evolution reasoning framework (SPARK), the first LLM-based multi-agent simulation framework for jointly modeling the co-evolution of topics and stances through natural language interactions. In SPARK, each agent is instantiated as an LLM persona with unique demographic and psychological traits, equipped with memory and reflective reasoning. Agents engage in daily conversations, adapt their stances, and organically introduce emergent subtopics, enabling interpretable, fine-grained simulation of discourse dynamics at scale. Experiments across five real-world domains show that SPARK captures key empirical patterns—such as rapid topic innovation in technology, domain-specific stance polarization, and the influence of personality on stance shifts and topic emergence. Our framework quantitatively reveals the bidirectional mechanisms by which stance shifts and topic evolution reinforce each other, a phenomenon rarely addressed in prior work. SPARK provides actionable insights and a scalable tool for understanding and mitigating polarization in online discourse. Code and simulation resources will be released after acceptance",
    "checked": true,
    "id": "36525b344f9320a0aaafe3f34e2956a7cb0edc1a",
    "semantic_title": "spark: simulating the co-evolution of stance and topic dynamics in online discourse with llm-based agents",
    "citation_count": 0,
    "authors": [
      "Bowen Zhang",
      "Yi Yang",
      "Fuqiang Niu",
      "Xianghua Fu",
      "Genan Dai",
      "Hu Huang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1177": {
    "title": "Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth",
    "volume": "main",
    "abstract": "We introduce Drivelology, a unique linguistic phenomenon characterised as \"nonsense with depth\" - utterances that are syntactically coherent yet pragmatically paradoxical, emotionally loaded, or rhetorically subversive. While such expressions may resemble surface-level nonsense, they encode implicit meaning requiring contextual inference, moral reasoning, or emotional interpretation. We find that current large language models (LLMs), despite excelling at many natural language processing (NLP) tasks, consistently fail to grasp the layered semantics of Drivelological text. To investigate this, we construct a benchmark dataset of over 1,200+ meticulously curated and diverse examples across English, Mandarin, Spanish, French, Japanese, and Korean. Each example underwent careful expert review to verify its Drivelological characteristics, involving multiple rounds of discussion and adjudication to address disagreements. Using this dataset, we evaluate a range of LLMs on classification, generation, and reasoning tasks. Our results reveal clear limitations of LLMs: models often confuse Drivelology with shallow nonsense, produce incoherent justifications, or miss implied rhetorical functions altogether. These findings highlight a deep representational gap in LLMs' pragmatic understanding and challenge the assumption that statistical fluency implies cognitive comprehension. We release our dataset and code to facilitate further research in modelling linguistic depth beyond surface-level coherence",
    "checked": true,
    "id": "be01082d5d90361a1b2c4398e0899e758169a14d",
    "semantic_title": "drivel-ology: challenging llms with interpreting nonsense with depth",
    "citation_count": 1,
    "authors": [
      "Yang Wang",
      "Chenghao Xiao",
      "Chia-Yi Hsiao",
      "Zi Yan Chang",
      "Chi-Li Chen",
      "Tyler Loakman",
      "Chenghua Lin"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1178": {
    "title": "Can Large Language Models be Effective Online Opinion Miners?",
    "volume": "main",
    "abstract": "The surge of user-generated online content presents a wealth of insights into customer preferences and market trends.However, the highly diverse, complex, and context-rich nature of such content poses significant challenges to traditional opinion mining approaches.To address this, we introduce Online Opinion Mining Benchmark (OOMB), a novel dataset and evaluation protocol designed to assess the ability of large language models (LLMs) to mine opinions effectively from diverse and intricate online environments. OOMB provides, for each content instance, an extensive set of (entity, feature, opinion) tuples and a corresponding opinion-centric insight that highlights key opinion topics, thereby enabling the evaluation of both the extractive and abstractive capabilities of models.Through our proposed benchmark, we conduct a comprehensive analysis of which aspects remain challenging and where LLMs exhibit adaptability, to explore whether they can effectively serve as opinion miners in realistic online scenarios.This study lays the foundation for LLM-based opinion mining and discusses directions for future research in this field",
    "checked": true,
    "id": "24e8bff93ff6a3c7e5c47db01e92d8273b6aa35b",
    "semantic_title": "can large language models be effective online opinion miners?",
    "citation_count": 0,
    "authors": [
      "Ryang Heo",
      "Yongsik Seo",
      "Junseong Lee",
      "Dongha Lee"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1179": {
    "title": "Can Large Language Models Translate Unseen Languages in Underrepresented Scripts?",
    "volume": "main",
    "abstract": "Large language models (LLMs) have demonstrated impressive performance in machine translation, but still struggle with unseen low-resource languages, especially those written in underrepresented scripts. To investigate whether LLMs can translate such languages with the help of linguistic resources, we introduce Lotus, a benchmark designed to evaluate translation for Mongolian (in traditional script) and Yi. Our study shows that while linguistic resources can improve translation quality as measured by automatic metrics, LLMs remain limited in their ability to handle these languages effectively. We hope our work provides insights for the low-resource NLP community and fosters further progress in machine translation for underrepresented script low-resource languages. Our code and data are available",
    "checked": true,
    "id": "66a3083904441c9c4683cc1faf55a01801da072c",
    "semantic_title": "can large language models translate unseen languages in underrepresented scripts?",
    "citation_count": 0,
    "authors": [
      "Dianqing Lin",
      "Aruukhan",
      "Hongxu Hou",
      "Shuo Sun",
      "Wei Chen",
      "Yichen Yang",
      "Guo Dong Shi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1180": {
    "title": "InterIDEAS: Philosophical Intertextuality via LLMs",
    "volume": "main",
    "abstract": "The formation and circulation of ideas in philosophy have profound implications for understanding philosophical dynamism–enabling us to identify seminal texts, delineate intellectual traditions, and track changing conventions in the act of philosophizing. However, traditional analyses of these issues often depend on manual reading and subjective interpretation, constrained by human cognitive limits. We introduce InterIDEAS, a pioneering dataset designed to bridge philosophy, literary studies, and natural language processing (NLP). By merging theories of intertextuality from literary studies with bibliometric techniques and recent LLMs, InterIDEAS enables both quantitative and qualitative analysis of the intellectual, social, and historical relations embedded within authentic philosophical texts. This dataset not only assists the study of philosophy but also contributes to the development of language models by providing a training corpus that challenges and enhances their interpretative capacity",
    "checked": true,
    "id": "12821e6cb37b5e21acfe0aa5d53171e73ed32d24",
    "semantic_title": "interideas: philosophical intertextuality via llms",
    "citation_count": 0,
    "authors": [
      "Yue Yang",
      "Yinzhi Xu",
      "Chenghao Huang",
      "JohnMichael Jurgensen",
      "Han Hu",
      "Hao Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1181": {
    "title": "KCS: Diversify Multi-hop Question Generation with Knowledge Composition Sampling",
    "volume": "main",
    "abstract": "Multi-hop question answering faces substantial challenges due to data sparsity, which increases the likelihood of language models learning spurious patterns. To address this issue, prior research has focused on diversifying question generation through content planning and varied expression. However, these approaches often emphasize generating simple questions and neglect the integration of essential knowledge, such as relevant sentences within documents. This paper introduces the **Knowledge Composition Sampling (KCS)**, an innovative framework designed to expand the diversity of generated multi-hop questions by sampling varied knowledge compositions within a given context. KCS models the knowledge composition selection as a sentence-level conditional prediction task and utilizes a probabilistic contrastive loss to predict the next most relevant piece of knowledge. During inference, we employ a stochastic decoding strategy to effectively balance accuracy and diversity. Compared to competitive baselines, our KCS improves the overall accuracy of knowledge composition selection by 3.9%, and its application for data augmentation yields improvements on HotpotQA and 2WikiMultihopQA datasets. Our code is available at: https://github.com/yangfanww/kcs",
    "checked": true,
    "id": "35f8fdbfb6d4b810053a9dc00c56a433162c935e",
    "semantic_title": "kcs: diversify multi-hop question generation with knowledge composition sampling",
    "citation_count": 0,
    "authors": [
      "Yangfan Wang",
      "Jie Liu",
      "Chen Tang",
      "Lian Yan",
      "Jingchi Jiang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1182": {
    "title": "Fooling the LVLM Judges: Visual Biases in LVLM-Based Evaluation",
    "volume": "main",
    "abstract": "Recently, large vision–language models (LVLMs) have emerged as the preferred tools for judging text–image alignment, yet their robustness along the visual modality remains underexplored. This work is the first study to address a key research question: Can adversarial visual manipulations systematically fool LVLM judges into assigning unfairly inflated scores? We define potential image-induced biases within the context of T2I evaluation and examine how these biases affect the evaluations of LVLM judges. Moreover, we introduce a novel, fine-grained, multi-domain meta-evaluation benchmark named FRAME, which is deliberately constructed to exhibit diverse score distributions. By introducing the defined biases into the benchmark, we reveal that all tested LVLM judges exhibit vulnerability across all domains, consistently inflating scores for manipulated images. Further analysis reveals that combining multiple biases amplifies their effects, and pairwise evaluations are similarly susceptible. Moreover, we observe that visual biases persist despite prompt-based mitigation strategies, highlighting the vulnerability of current LVLM evaluation systems and underscoring the urgent need for more robust LVLM judges",
    "checked": true,
    "id": "b82480623b20b16db13da20e4ce6759502e35ef7",
    "semantic_title": "fooling the lvlm judges: visual biases in lvlm-based evaluation",
    "citation_count": 1,
    "authors": [
      "Yerin Hwang",
      "Dongryeol Lee",
      "Kyungmin Min",
      "Taegwan Kang",
      "Yongil Kim",
      "Kyomin Jung"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1183": {
    "title": "Disentangled Information Bottleneck for Adversarial Text Defense",
    "volume": "main",
    "abstract": "Adversarial text defense is a significant strategy to protect modern NLP models from being attacked. Typical text defense methods usually enhance the model's robustness by model retraining or equipping it with a data preprocessing step, aiming to eliminate the non-robust features and preserve the robust ones. Although some efforts have been made to recognize the robust features, e.g., by the information bottleneck (IB) technique, how to fully disentangle the robust and non-robust representation remains a big challenge. To alleviate this problem, we propose a novel text defense method, named Disentangled Information Bottleneck (DisIB), with two major merits. Firstly, we separate the robust features and non-robust features with a disentangled two-line framework rather than the one-line compression network in IB. This prevents the loss of robust features caused by information compression and produces complete robust features. Secondly, we design a discriminator network to approximate the minimum mutual information of the two lines, which sufficiently disentangles robust and non-robust features. To validate the effectiveness of our DisIB, we conduct a total of 96 defense experiments on four datasets by defending four popular attack methods. Experimental results elaborate that our method significantly outperforms six baselines, with accuracy improvements ranging from 3.8% to 20.7%",
    "checked": true,
    "id": "c74a7ed038ac3cc591f300c4119307299c4fa80f",
    "semantic_title": "disentangled information bottleneck for adversarial text defense",
    "citation_count": 0,
    "authors": [
      "Yidan Xu",
      "Xinghao Yang",
      "Wei Liu",
      "Bao-di Liu",
      "Weifeng Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1184": {
    "title": "How do Language Models Reshape Entity Alignment? A Survey of LM-Driven EA Methods: Advances, Benchmarks, and Future",
    "volume": "main",
    "abstract": "Entity alignment (EA), critical for knowledge graph (KG) integration, identifies equivalent entities across different KGs. Traditional methods often face challenges in semantic understanding and scalability. The rise of language models (LMs), particularly large language models (LLMs), has provided powerful new strategies. This paper systematically reviews LM-driven EA methods, proposing a novel taxonomy that categorizes methods in three key stages: data preparation, feature embedding, and alignment. We further summarize key benchmarks, evaluation metrics, and discuss future directions. This paper aims to provide researchers and practitioners with a clear and comprehensive understanding of how language models reshape the field of entity alignment",
    "checked": true,
    "id": "3d552287ec8e3b30cb1880c96b9efada088e09c5",
    "semantic_title": "how do language models reshape entity alignment? a survey of lm-driven ea methods: advances, benchmarks, and future",
    "citation_count": 0,
    "authors": [
      "Zerui Chen",
      "Huiming Fan",
      "Qianyu Wang",
      "Tao He",
      "Ming Liu",
      "Heng Chang",
      "Weijiang Yu",
      "Ze Li",
      "Bing Qin"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1185": {
    "title": "Enhancing LLM-Based Social Bot via an Adversarial Learning Framework",
    "volume": "main",
    "abstract": "Developing Large Language Model (LLM) agents that exhibit human-like behavior, encompassing not only individual heterogeneity rooted in unique user profiles but also adaptive response to socially connected neighbors, is a significant research challenge. Social media platforms, with their diverse user data and explicit social structures, provide an ideal testbed for such investigations. This paper introduces EvoBot, an Evolving LLM-based social Bot that significantly enhances human-like generative capabilities through a novel adversarial learning framework. EvoBot is initialized by Supervised Fine-Tuning (SFT) on representative data from social media and then iteratively refines its generation of sophisticated, human-like content via Direct Preference Optimization (DPO). This refinement is guided by feedback from a co-adapting Detector which concurrently improves its ability to distinguish EvoBot from humans, thereby creating an increasingly challenging learning environment for EvoBot. Experiments demonstrate that EvoBot generates content aligned with diverse user profiles, increasingly bypassing the co-adapting Detector through human-like expression. Moreover, it exhibits strong social responsiveness, more accurately modeling real-world opinion dynamics and information spread in multi-agent simulations. The framework also yields a more robust Detector, underscoring its broader utility for both advanced agent development and related detection tasks. The code is available at https://github.com/kfq20/EvoBot",
    "checked": true,
    "id": "051e7d6caaa4c4c05c57062e21e094a2fdff10cd",
    "semantic_title": "enhancing llm-based social bot via an adversarial learning framework",
    "citation_count": 4,
    "authors": [
      "Fanqi Kong",
      "Xiaoyuan Zhang",
      "Xinyu Chen",
      "Yaodong Yang",
      "Song-Chun Zhu",
      "Xue Feng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1186": {
    "title": "GER-LLM: Efficient and Effective Geospatial Entity Resolution with Large Language Model",
    "volume": "main",
    "abstract": "Geospatial Entity Resolution (GER) plays a central role in integrating spatial data from diverse sources. However, existing methods are limited by their reliance on large amounts of training data and their inability to incorporate commonsense knowledge. While recent advances in Large Language Models (LLMs) offer strong semantic reasoning and zero-shot capabilities, directly applying them to GER remains inadequate due to their limited spatial understanding and high inference cost. In this work, we present GER-LLM, a framework that integrates LLMs into the GER pipeline. To address the challenge of spatial understanding, we design a spatially informed blocking strategy based on adaptive quadtree partitioning and Area of Interest (AOI) detection, preserving both spatial proximity and functional relationships. To mitigate inference overhead, we introduce a group prompting mechanism with graph-based conflict resolution, enabling joint evaluation of diverse candidate pairs and enforcing global consistency across alignment decisions. Extensive experiments on real-world datasets demonstrate the effectiveness of our approach, yielding significant improvements over state-of-the-art methods",
    "checked": true,
    "id": "e4efeaeba8f11c3feeefe0f4561958ebba9683b1",
    "semantic_title": "ger-llm: efficient and effective geospatial entity resolution with large language model",
    "citation_count": 0,
    "authors": [
      "Haojia Zhu",
      "Zhicheng Li",
      "Jiahui Jin"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1187": {
    "title": "CodeRAG: Finding Relevant and Necessary Knowledge for Retrieval-Augmented Repository-Level Code Completion",
    "volume": "main",
    "abstract": "Repository-level code completion automatically predicts the unfinished code based on the broader information from the repository. Recent strides in Code Large Language Models (code LLMs) have spurred the development of repository-level code completion methods, yielding promising results. Nevertheless, they suffer from issues such as inappropriate query construction, single-path code retrieval, and misalignment between code retriever and code LLM. To address these problems, we introduce CodeRAG, a framework tailored to identify relevant and necessary knowledge for retrieval-augmented repository-level code completion. Its core components include log probability guided query construction, multi-path code retrieval, and preference-aligned BestFit reranking. Extensive experiments on benchmarks ReccEval and CCEval demonstrate that CodeRAG significantly and consistently outperforms state-of-the-art methods. The implementation of CodeRAG is available at https://github.com/KDEGroup/CodeRAG",
    "checked": true,
    "id": "ba9db9ccb8e3677830da0c55b4a0b6c97f6bbd29",
    "semantic_title": "coderag: finding relevant and necessary knowledge for retrieval-augmented repository-level code completion",
    "citation_count": 0,
    "authors": [
      "Sheng Zhang",
      "Yifan Ding",
      "Shuquan Lian",
      "Shun Song",
      "Hui Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1188": {
    "title": "Searching for the Most Human-like Emergent Language",
    "volume": "main",
    "abstract": "In this paper, we design a signalling game-based emergent communication environment to generate state-of-the-art emergent languages in terms of similarity to human language. This is done with hyperparameter optimization, using XferBench as the objective function. XferBench quantifies the statistical similarity of emergent language to human language by measuring its suitability for deep transfer learning to human language. Additionally, we demonstrate the predictive power of entropy on the transfer learning performance of emergent language as well as corroborate previous results on the entropy-minimization properties of emergent communication systems. Finally, we report generalizations regarding what hyperparameters produce more realistic emergent languages, that is, ones which transfer better to human language",
    "checked": true,
    "id": "9f08ba06e5e2f45e213c0c38d29a9262bc15c515",
    "semantic_title": "searching for the most human-like emergent language",
    "citation_count": 0,
    "authors": [
      "Brendon Boldt",
      "David R. Mortensen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1189": {
    "title": "Does Context Matter? A Prosodic Comparison of English and Spanish in Monolingual and Multilingual Discourse Settings",
    "volume": "main",
    "abstract": "Different languages are known to have typical and distinctive prosodic profiles. However, the majority of work on prosody across languages has been restricted to monolingual discourse contexts. We build on prior studies by asking: how does the nature of the discourse context influence variations in the prosody of monolingual speech? To answer this question, we compare the prosody of spontaneous, conversational monolingual English and Spanish both in monolingual and in multilingual speech settings. For both languages, we find that monolingual speech produced in a monolingual context is prosodically different from that produced in a multilingual context, with more marked differences having increased proximity to multilingual discourse. Our work is the first to incorporate multilingual discourse contexts into the study of native-level monolingual prosody, and has potential downstream applications for the recognition and synthesis of multilingual speech",
    "checked": true,
    "id": "fa7d54e847a8c3c2791515d9eddc541b126b5522",
    "semantic_title": "does context matter? a prosodic comparison of english and spanish in monolingual and multilingual discourse settings",
    "citation_count": 0,
    "authors": [
      "Debasmita Bhattacharya",
      "David Sasu",
      "Michela Marchini",
      "Natalie Schluter",
      "Julia Hirschberg"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1190": {
    "title": "ZERA: Zero-init Instruction Evolving Refinement Agent – From Zero Instructions to Structured Prompts via Principle-based Optimization",
    "volume": "main",
    "abstract": "Automatic Prompt Optimization (APO) improves large language model (LLM) performance by refining prompts for specific tasks. However, prior APO methods typically focus only on user prompts, rely on unstructured feedback, and require large sample sizes and long iteration cycles—making them costly and brittle. We propose ZERA (Zero-init Instruction Evolving Refinement Agent), a novel framework that jointly optimizes both system and user prompts through principled, low-overhead refinement. ZERA scores prompts using eight generalizable criteria with automatically inferred weights, and revises prompts based on these structured critiques. This enables fast convergence to high-quality prompts using minimal examples and short iteration cycles. We evaluate ZERA across five LLMs and nine diverse datasets spanning reasoning, summarization, and code generation tasks. Experimental results demonstrate consistent improvements over strong baselines. Further ablation studies highlight the contribution of each component to more effective prompt construction. Our implementation including all prompts is publicly available at https://github.com/younatics/zera-agent",
    "checked": false,
    "id": "76570565a38a3ad0b8c2551acbe12b70a2b9374a",
    "semantic_title": "zera: zero-init instruction evolving refinement agent - from zero instructions to structured prompts via principle-based optimization",
    "citation_count": 0,
    "authors": [
      "Seungyoun Yi",
      "Minsoo Khang",
      "Sungrae Park"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1191": {
    "title": "Toward Machine Interpreting: Lessons from Human Interpreting Studies",
    "volume": "main",
    "abstract": "Current speech translation systems, while having achieved impressive accuracies, are rather static in their behavior and do not adapt to real-world situations in ways human interpreters do. In order to improve their practical usefulness and enable interpreting-like experiences, a precise understanding of the nature of human interpreting is crucial. To this end, we discuss human interpreting literature from the perspective of the machine translation field, while considering both operational and qualitative aspects. We identify implications for the development of speech translation systems and argue that there is great potential to adopt many human interpreting principles using recent modeling techniques. We hope that our findings provide inspiration for closing the perceived usability gap, and can motivate progress toward true machine interpreting",
    "checked": true,
    "id": "cd50e8cd08db3d54a44435ceff965f0241c2b6f1",
    "semantic_title": "toward machine interpreting: lessons from human interpreting studies",
    "citation_count": 0,
    "authors": [
      "Matthias Sperber",
      "Maureen de Seyssel",
      "Jiajun Bao",
      "Matthias Paulik"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1192": {
    "title": "FlashAdventure: A Benchmark for GUI Agents Solving Full Story Arcs in Diverse Adventure Games",
    "volume": "main",
    "abstract": "GUI agents powered by LLMs show promise in interacting with diverse digital environments. Among these, video games offer a valuable testbed due to their varied interfaces, with adventure games posing additional challenges through complex, narrative-driven interactions. Existing game benchmarks, however, lack diversity and rarely evaluate agents on completing entire storylines. To address this, we introduce FlashAdventure, a benchmark of 34 Flash-based adventure games designed to test full story arc completion and tackle the observation-behavior gap—the challenge of remembering and acting on earlier gameplay information. We also propose CUA-as-a-judge, an automated gameplay evaluator, and COAST, an agentic framework leveraging long-term clue memory to better plan and solve sequential tasks. Experiments show current GUI agents struggle with full story arcs, while COAST improves milestone completion by bridging the observation-behavior gap. Nonetheless, a marked discrepancy between humans and best-performing agents warrants continued research efforts to narrow this divide",
    "checked": true,
    "id": "a641feba82a9f80664d485767e8a81e5457f9260",
    "semantic_title": "flashadventure: a benchmark for gui agents solving full story arcs in diverse adventure games",
    "citation_count": 0,
    "authors": [
      "Jaewoo Ahn",
      "Junseo Kim",
      "Heeseung Yun",
      "Jaehyeon Son",
      "Dongmin Park",
      "Jaewoong Cho",
      "Gunhee Kim"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1193": {
    "title": "FLARE: Faithful Logic-Aided Reasoning and Exploration",
    "volume": "main",
    "abstract": "Modern Question Answering (QA) and Reasoning approaches with Large Language Models (LLMs) commonly use Chain-of-Thought (CoT) prompting but struggle with generating outputs faithful to their intermediate reasoning chains. While neuro-symbolic methods like Faithful CoT (F-CoT) offer higher faithfulness through external solvers, they require code-specialized models and struggle with ambiguous tasks.We introduce Faithful Logic-Aided Reasoning and Exploration (FLARE), which uses LLMs to plan solutions, formalize queries into logic programs, and simulate code execution through multi-hop search without external solvers. Our method achieves SOTA results on 𝟕 out of 𝟗 diverse reasoning benchmarks and 3 out of 3 logic inference benchmarks while enabling measurement of reasoning faithfulness. We demonstrate that model faithfulness correlates with performance and that successful reasoning traces show an 18.1% increase in unique emergent facts, 8.6% higher overlap between code-defined and execution-trace relations, and 3.6% reduction in unused relations",
    "checked": true,
    "id": "03e0b834e4077047cc2b3426a2b52b7b368968fe",
    "semantic_title": "flare: faithful logic-aided reasoning and exploration",
    "citation_count": 3,
    "authors": [
      "Erik Arakelyan",
      "Pasquale Minervini",
      "Patrick Lewis",
      "Pat Verga",
      "Isabelle Augenstein"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1194": {
    "title": "Discourse-Driven Code-Switching: Analyzing the Role of Content and Communicative Function in Spanish-English Bilingual Speech",
    "volume": "main",
    "abstract": "Code-switching (CSW) is commonly observed among bilingual speakers, and is motivated by various paralinguistic, syntactic, and morphological aspects of conversation. We build on prior work by asking: how do discourse-level aspects of dialogue – i.e. the content and function of speech – influence patterns of CSW? To answer this, we analyze the named entities and dialogue acts present in a Spanish-English spontaneous speech corpus, and build a predictive model of CSW based on our statistical findings. We show that discourse content and function interact with patterns of CSW to varying degrees, with a stronger influence from function overall. Our work is the first to take a discourse-sensitive approach to understanding the pragmatic and referential cues of bilingual speech and has potential applications in improving the prediction, recognition, and synthesis of code-switched speech that is grounded in authentic aspects of multilingual discourse",
    "checked": true,
    "id": "5c0079701c709976bcda655a194414348fe094dc",
    "semantic_title": "discourse-driven code-switching: analyzing the role of content and communicative function in spanish-english bilingual speech",
    "citation_count": 0,
    "authors": [
      "Debasmita Bhattacharya",
      "Juan Junco",
      "Divya Tadimeti",
      "Julia Hirschberg"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1195": {
    "title": "Can Large Language Models Translate Spoken-Only Languages through International Phonetic Transcription?",
    "volume": "main",
    "abstract": "Spoken-only languages are languages without a writing system. They remain excluded from modern Natural Language Processing (NLP) advancements like Large Language Models (LLMs) due to their lack of textual data. Existing NLP research focuses primarily on high-resource or written low-resource languages, leaving spoken-only languages critically underexplored. As a popular NLP paradigm, LLMs have demonstrated strong few-shot and cross-lingual generalization abilities, making them a promising solution for understanding and translating spoken-only languages. In this paper, we investigate how LLMs can translate spoken-only languages into high-resource languages by leveraging international phonetic transcription as an intermediate representation. We propose UNILANG, a unified language understanding framework that learns to translate spoken-only languages via in-context learning. Through automatic dictionary construction and knowledge retrieval, UNILANG equips LLMs with more fine-grained knowledge for improving word-level semantic alignment. To support this study, we introduce the SOLAN dataset, which consists of Bai (a spoken-only language) and its corresponding translations in a high-resource language. A series of experiments demonstrates the effectiveness of UNILANG in translating spoken-only languages, potentially contributing to the preservation of linguistic and cultural diversity. Our dataset and code will be publicly released",
    "checked": true,
    "id": "5c4792e0f3d34b30e23fec240ce7ca0448c9487a",
    "semantic_title": "can large language models translate spoken-only languages through international phonetic transcription?",
    "citation_count": 0,
    "authors": [
      "Jiale Chen",
      "Xuelian Dong",
      "Qihao Yang",
      "Wenxiu Xie",
      "Tianyong Hao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1196": {
    "title": "ClimateViz: A Benchmark for Statistical Reasoning and Fact Verification on Scientific Charts",
    "volume": "main",
    "abstract": "Scientific fact-checking has largely focused on textual and tabular sources, neglecting scientific charts—a primary medium for conveying quantitative evidence and supporting statistical reasoning in research communication. We introduce ClimateViz, the first large-scale benchmark for scientific fact-checking grounded in real-world, expert-curated scientific charts. ClimateViz comprises 49,862 claims paired with 2,896 visualizations, each labeled as support, refute, or not enough information. To enable interpretable verification, each instance includes structured knowledge graph explanations that capture statistical patterns, temporal trends, spatial comparisons, and causal relations. We conduct a comprehensive evaluation of state-of-the-art multimodal large language models, including proprietary and open-source ones, under zero-shot and few-shot settings. Our results show that current models struggle to perform fact-checking when statistical reasoning over charts is required: even the best-performing systems, such as Gemini 2.5 and InternVL 2.5, achieve only 76.2–77.8% accuracy in label-only output settings, which is far below human performance (89.3% and 92.7%). While few-shot prompting yields limited improvements, explanation-augmented outputs significantly enhance performance in some closed-source models, notably o3 and Gemini 2.5",
    "checked": true,
    "id": "7e8cfd4aabb155a07156b7d5099ccc4f54bf610a",
    "semantic_title": "climateviz: a benchmark for statistical reasoning and fact verification on scientific charts",
    "citation_count": 0,
    "authors": [
      "Ruiran Su",
      "Jiasheng Si",
      "Zhijiang Guo",
      "Janet B. Pierrehumbert"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1197": {
    "title": "Bridging the Gap Between Molecule and Textual Descriptions via Substructure-aware Alignment",
    "volume": "main",
    "abstract": "Molecule and text representation learning has gained increasing interest due to its potential for enhancing the understanding of chemical information. However, existing models often struggle to capture subtle differences between molecules and their descriptions, as they lack the ability to learn fine-grained alignments between molecular substructures and chemical phrases. To address this limitation, we introduce MolBridge, a novel molecule–text learning framework based on substructure-aware alignments. Specifically, we augment the original molecule–description pairs with additional alignment signals derived from molecular substructures and chemical phrases. To effectively learn from these enriched alignments, MolBridge employs substructure-aware contrastive learning, coupled with a self-refinement mechanism that filters out noisy alignment signals. Experimental results show that MolBridge effectively captures fine-grained correspondences and outperforms state-of-the-art baselines on a wide range of molecular benchmarks, underscoring the importance of substructure-aware alignment in molecule-text learning",
    "checked": true,
    "id": "b347e94c05346a3a0dc3902b990d5c65680ce9d8",
    "semantic_title": "bridging the gap between molecule and textual descriptions via substructure-aware alignment",
    "citation_count": 0,
    "authors": [
      "Hyuntae Park",
      "Yeachan Kim",
      "SangKeun Lee"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1198": {
    "title": "SLlama: Parameter-Efficient Language Model Architecture for Enhanced Linguistic Competence Under Strict Data Constraints",
    "volume": "main",
    "abstract": "Scaling data and model size has driven recent advances in language modeling, but this strategy falters under scenarios with strict data constraints, as in the BabyLM Challenge. However, insights from Chinchilla highlights that smaller models trained on more data outperform larger counterparts trained inadequately, emphasizing the need for compact architectures. Furthermore, while embedding weight tying is a common parameter-saving technique, we find it significantly diminishes linguistic competence in compact models.In response, we explore alternative architectural strategies that preserve the parameter efficiency of tied models without sacrificing the representational benefits of untied embeddings. Consequently, we introduce SLlama a Llama3 architecture variant which incorporates targeted modifications—Repeated Reduced Hidden Size and Projection (RRHP), Permutated Weight Attention (PWA), Shared Projection Multi-Layer Perceptron (SPMLP), and Layer Weight Sharing—to compress Transformer components. Without relying on distillation, SLlama achieves a 31.72% improvement in linguistic knowledge acquisition over the BabyLlama baseline, with a comparable GLUE score and significantly lower parameter count. These results demonstrate that well-designed, compact models can rival larger ones under strict data constraints",
    "checked": true,
    "id": "b9769b22fb04588069af1b014c9794149748b024",
    "semantic_title": "sllama: parameter-efficient language model architecture for enhanced linguistic competence under strict data constraints",
    "citation_count": 0,
    "authors": [
      "Victor Adelakun Omolaoye",
      "Babajide Alamu Owoyele",
      "Gerard de Melo"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1199": {
    "title": "What You See is What You Ask: Evaluating Audio Descriptions",
    "volume": "main",
    "abstract": "Audio descriptions (ADs) narrate important visual details in movies, enabling Blind and Low Vision (BLV) users to understand narratives and appreciate visual details. Existing works in automatic AD generation mostly focus on few-second trimmed clips, and evaluate them by comparing against a single ground-truth reference AD. However, writing ADs is inherently subjective. Through alignment and analysis of two independent AD tracks for the same movies, we quantify the subjectivity in when and whether to describe, and what and how to highlight. Thus, we show that working with trimmed clips is inadequate. We propose ADQA, a QA benchmark that evaluates ADs at the level of few-minute long, coherent video segments, testing whether they would help BLV users understand the story and appreciate visual details. ADQA features visual appreciation (VA) questions about visual facts and narrative understanding (NU) questions based on the plot. Through ADQA, we show that current AD generation methods lag far behind human-authored ADs. We conclude with several recommendations for future work and introduce a public leaderboard for benchmarking",
    "checked": true,
    "id": "8e2129f8d2c9fea45a6a85331ffb79fd9c25c969",
    "semantic_title": "what you see is what you ask: evaluating audio descriptions",
    "citation_count": 1,
    "authors": [
      "Divy Kala",
      "Eshika Khandelwal",
      "Makarand Tapaswi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1200": {
    "title": "TAPS: Tool-Augmented Personalisation via Structured Tagging",
    "volume": "main",
    "abstract": "Recent advancements in tool-augmented large language models have enabled them to interact with external tools, enhancing their ability to perform complex user tasks. However, existing approaches overlook the role of personalisation in guiding tool use. This work investigates how user preferences can be effectively integrated into goal-oriented dialogue agents. Through extensive analysis, we identify key weaknesses in the ability of LLMs to personalise tool use. To this end, we introduce TAPS, a novel solution that enhances personalised tool use by leveraging a structured tagging tool and an uncertainty-based tool detector. TAPS significantly improves the ability of LLMs to incorporate user preferences, achieving the new state-of-the-art for open source models on the NLSI task",
    "checked": true,
    "id": "c92d1d8f0e9a4748f16635ac7774da10bc87ff38",
    "semantic_title": "taps: tool-augmented personalisation via structured tagging",
    "citation_count": 0,
    "authors": [
      "Ekaterina Taktasheva",
      "Jeff Dalton"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1201": {
    "title": "Investigating How Pre-training Data Leakage Affects Models' Reproduction and Detection Capabilities",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) are trained on massive web-crawled corpora, often containing personal information, copyrighted text, and benchmark datasets. This inadvertent inclusion in the training dataset, known as data leakage, poses significant risks and could compromise the safety of LLM outputs. Despite its criticality, existing studies do not examine how leaked instances in the pre-training data influence LLMs' output and detection capabilities. In this paper, we conduct an experimental survey to elucidate the relationship between data leakage in training datasets and its effects on the generation and detection by LLMs. Our experiments reveal that LLMs often generate outputs containing leaked information, even when there is little such data in the training dataset. Moreover, the fewer the leaked instances, the more difficult it becomes to detect such leakage. Finally, we demonstrate that enhancing leakage detection through few-shot learning can help mitigate the impact of the leakage rate in the training data on detection performance",
    "checked": true,
    "id": "a030ff5d47740980aab2828813eb8a8157ef6b73",
    "semantic_title": "investigating how pre-training data leakage affects models' reproduction and detection capabilities",
    "citation_count": 0,
    "authors": [
      "Masahiro Kaneko",
      "Timothy Baldwin"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1202": {
    "title": "Walk and Read Less: Improving the Efficiency of Vision-and-Language Navigation via Tuning-Free Multimodal Token Pruning",
    "volume": "main",
    "abstract": "Large models achieve strong performance on Vision-and-Language Navigation (VLN) tasks, but are costly to run in resource-limited environments. Token pruning offers appealing tradeoffs for efficiency with minimal performance loss by reducing model input size, but prior work overlooks VLN-specific challenges. For example, information loss from pruning can effectively increase computational cost due to longer walks. Thus, the inability to identify uninformative tokens undermines the supposed efficiency gains from pruning.To address this, we propose Navigation-Aware Pruning (NAP), which uses navigation-specific traits to simplify the pruning process by pre-filtering tokens into foreground and background. For example, image views are filtered based on whether the agent can navigate in that direction. We also extract navigation-relevant instructions using a Large Language Model. After filtering, we focus pruning on background tokens, minimizing information loss. To further help avoid increases in navigation length, we discourage backtracking by removing low-importance navigation nodes.Experiments on standard VLN benchmarks show NAP significantly outperforms prior work, preserving higher success rates while saving more than 50% FLOPS",
    "checked": true,
    "id": "d31fd7ee46d56e7b87f199bbacddd7241e895e41",
    "semantic_title": "walk and read less: improving the efficiency of vision-and-language navigation via tuning-free multimodal token pruning",
    "citation_count": 0,
    "authors": [
      "Wenda Qin",
      "Andrea Burns",
      "Bryan A. Plummer",
      "Margrit Betke"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1203": {
    "title": "Connecting the Knowledge Dots: Retrieval-augmented Knowledge Connection for Commonsense Reasoning",
    "volume": "main",
    "abstract": "While large language models (LLMs) have achieved remarkable performance across various natural language processing (NLP) tasks, LLMs exhibit a limited understanding of commonsense reasoning due to the necessity of implicit knowledge that is rarely expressed in text. Recently, retrieval-augmented language models (RALMs) have enhanced their commonsense reasoning ability by incorporating background knowledge from external corpora. However, previous RALMs overlook the implicit nature of commonsense knowledge, potentially resulting in the retrieved documents not directly containing information needed to answer questions. In this paper, we propose Retrieval-augmented knowledge Connection, ReConnect, which transforms indirectly relevant documents into a direct explanation to answer the given question. To this end, we extract relevant knowledge from various retrieved document subsets and aggregate them into a direct explanation. Experimental results show that ReConnect outperforms state-of-the-art (SOTA) baselines, achieving improvements of +2.0% and +4.6% average accuracy on in-domain (ID) and out-of-domain (OOD) benchmarks, respectively",
    "checked": true,
    "id": "6a07d80c636f5b3a6f4c56b9453613738e881acc",
    "semantic_title": "connecting the knowledge dots: retrieval-augmented knowledge connection for commonsense reasoning",
    "citation_count": 0,
    "authors": [
      "Junho Kim",
      "Soyeon Bak",
      "Mingyu Lee",
      "Minju Hong",
      "Songha Kim",
      "Tae-Eui Kam",
      "SangKeun Lee"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1204": {
    "title": "Agent-as-Judge for Factual Summarization of Long Narratives",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have demonstrated near-human performance in summarization tasks based on traditional metrics such as ROUGE and BERTScore. However, these metrics do not adequately capture critical aspects of summarization quality, such as factual accuracy, particularly for long narratives (>100K tokens). Recent advances, such as LLM-as-a-Judge, address the limitations of metrics based on lexical similarity but still exhibit factual inconsistencies, especially in understanding character relationships and states. In this work, we introduce NarrativeFactScore (NFS), the first \"Agent-as-a-Judge\" framework that evaluates and refines factuality in narrative summarization. By leveraging a Character Knowledge Graph (CKG) extracted from input narrative, NarrativeFactScore evaluates the factuality and provides actionable guidance for refinement, such as identifying missing or erroneous facts. Our experimental results demonstrate that constructing the CKG enables reasoning with 1/3 of the factuality computation used in the prior approach, and achieve three times higher correlation with human judgments. Furthermore, refinement with actionable guidance improves the quality of the summary",
    "checked": true,
    "id": "1f0a73396a54a4f0441960833f98a9e62c8daf2d",
    "semantic_title": "agent-as-judge for factual summarization of long narratives",
    "citation_count": 6,
    "authors": [
      "Yeonseok Jeong",
      "Minsoo Kim",
      "Seung-won Hwang",
      "Byung-Hak Kim"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1205": {
    "title": "DnDScore: Decontextualization and Decomposition for Factuality Verification in Long-Form Text Generation",
    "volume": "main",
    "abstract": "The decompose-then-verify strategy for verification of Large Language Model (LLM) generations decomposes claims that are then independently verified. Decontextualization augments text (claims) to ensure it can be verified outside of the original context, enabling reliable verification. While decomposition and decontextualization have been explored independently, their interactions in a complete system have not been investigated. Their conflicting purposes can create tensions: decomposition isolates atomic facts while decontextualization inserts relevant information. Furthermore, a decontextualized subclaim presents a challenge to the verification step: what part of the augmented text should be verified as it now contains multiple atomic facts? We conduct an evaluation of different decomposition, decontextualization, and verification strategies and find that the choice of strategy matters in the resulting factuality scores. Additionally, we introduce DnDScore, a decontextualization aware verification method that validates subclaims in the context of contextual information",
    "checked": true,
    "id": "fb92acd5032c2f597b91783cc6fcac2b8c9100dc",
    "semantic_title": "dndscore: decontextualization and decomposition for factuality verification in long-form text generation",
    "citation_count": 0,
    "authors": [
      "Miriam Wanner",
      "Benjamin Van Durme",
      "Mark Dredze"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1206": {
    "title": "RAcQUEt: Unveiling the Dangers of Overlooked Referential Ambiguity in Visual LLMs",
    "volume": "main",
    "abstract": "Ambiguity resolution is key to effective communication. While humans effortlessly address ambiguity through conversational grounding strategies, the extent to which current language models can emulate these strategies remains unclear. In this work, we examine referential ambiguity in image-based question answering by introducing RAcQUEt, a carefully curated dataset targeting distinct aspects of ambiguity. Through a series of evaluations, we reveal significant limitations and problems of overconfidence of state-of-the-art large multimodal language models in addressing ambiguity in their responses. The overconfidence issue becomes particularly relevant for RAcQUEt-BIAS, a subset designed to analyze a critical yet underexplored problem: failing to address ambiguity leads to stereotypical, socially biased responses. Our results underscore the urgency of equipping models with robust strategies to deal with uncertainty without resorting to undesirable stereotypes",
    "checked": true,
    "id": "2dc533f6ea4e72ca8b3ca64940dee2d83102a3ee",
    "semantic_title": "racquet: unveiling the dangers of overlooked referential ambiguity in visual llms",
    "citation_count": 3,
    "authors": [
      "Alberto Testoni",
      "Barbara Plank",
      "Raquel Fernández"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1207": {
    "title": "Resource-Rational Noisy-Channel Language Processing: Testing the Effect of Algorithmic Constraints on Inferences",
    "volume": "main",
    "abstract": "Human language use is robust to errors: comprehenders can and do mentally correct utterances that are implausible or anomalous. How are humans able to solve these problems in real time, picking out alternatives from an unbounded space of options using limited cognitive resources? And can language models trained on next-word prediction for typical language be augmented to handle language anomalies in a human-like way? Using a language model as a prior and an error model to encode likelihoods, we use Sequential Monte Carlo with optional rejuvenation to perform incremental and approximate probabilistic inference over intended sentences and production errors. We demonstrate that the model captures previously established patterns in human sentence processing, and that a trade-off between human-like noisy-channel inferences and computational resources falls out of this model. From a psycholinguistic perspective, our results offer a candidate algorithmic model of rational inference in language processing. From an NLP perspective, our results showcase how to elicit human-like noisy-channel inference behavior from a relatively small LLM while controlling the amount of computation available during inference. Our model is implemented in the Gen.jl probabilistic programming language, and our code is available at https://github.com/thomashikaru/noisy_channel_model",
    "checked": true,
    "id": "9267d4358194ce3d46e18bee590c55270fb32b21",
    "semantic_title": "resource-rational noisy-channel language processing: testing the effect of algorithmic constraints on inferences",
    "citation_count": 0,
    "authors": [
      "Thomas Hikaru Clark",
      "Jacob Hoover Vigly",
      "Edward Gibson",
      "Roger P. Levy"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1208": {
    "title": "In Benchmarks We Trust ... Or Not?",
    "volume": "main",
    "abstract": "Standardized benchmarks are central to evaluating and comparing model performance in Natural Language Processing (NLP). However, Large Language Models (LLMs) have exposed shortcomings in existing benchmarks, and so far there is no clear solution. In this paper, we survey a wide scope of benchmarking issues, and provide an overview of solutions as they are suggested in the literature. We observe that these solutions often tackle a limited number of issues, neglecting other facets. Therefore, we propose concrete checklists to cover all aspects of benchmarking issues, both for benchmark creation and usage. We illustrate the use of our checklists by applying them to three popular NLP benchmarks (i.e., SuperGLUE, WinoGrande, and ARC-AGI). Additionally, we discuss the potential advantages of adding minimal-sized test-suites to benchmarking, which would ensure downstream applicability on real-world use cases",
    "checked": true,
    "id": "cd6c49f723960e2685755b4c0863da35a8ede640",
    "semantic_title": "in benchmarks we trust ... or not?",
    "citation_count": 0,
    "authors": [
      "Ine Gevers",
      "Victor De Marez",
      "Jens Van Nooten",
      "Jens Lemmens",
      "Andriy Kosar",
      "Ehsan Lotfi",
      "Nikolay Banar",
      "Pieter Fivez",
      "Luna De Bruyne",
      "Walter Daelemans"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1209": {
    "title": "Video2Roleplay: A Multimodal Dataset and Framework for Video-Guided Role-playing Agents",
    "volume": "main",
    "abstract": "Role-playing agents (RPAs) have attracted growing interest for their ability to simulate immersive and interactive characters. However, existing approaches primarily focus on static role profiles, overlooking the dynamic perceptual abilities inherent to humans. To bridge this gap, we introduce the concept of dynamic role profiles by incorporating video modality into RPAs. To support this, we construct Role-playing-Video60k, a large-scale, high-quality dataset comprising 60k videos and 700k corresponding dialogues. Based on this dataset, we develop a comprehensive RPA framework that combines adaptive temporal sampling with both dynamic and static role profile representations. Specifically, the dynamic profile is created by adaptively sampling video frames and feeding them to the LLM in temporal order, while the static profile consists of (1) character dialogues from training videos during fine-tuning, and (2) a summary context from the input video during inference. This joint integration enables RPAs to generate greater responses. Furthermore, we propose a robust evaluation method covering eight metrics. Experimental results demonstrate the effectiveness of our framework, highlighting the importance of dynamic role profiles in developing RPAs",
    "checked": true,
    "id": "5eec6853074e24180b4f284bfe4acb251dc5b392",
    "semantic_title": "video2roleplay: a multimodal dataset and framework for video-guided role-playing agents",
    "citation_count": 0,
    "authors": [
      "Xueqiao Zhang",
      "Chao Zhang",
      "Jingtao Xu",
      "Yifan Zhu",
      "Xin Shi",
      "Yi Yang",
      "Yawei Luo"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1210": {
    "title": "Discriminating Form and Meaning in Multilingual Models with Minimal-Pair ABX Tasks",
    "volume": "main",
    "abstract": "We introduce a set of training-free ABX-style discrimination tasks to evaluate how multilingual language models represent language identity (form) and semantic content (meaning). Inspired from speech processing, these zero-shot tasks measure whether minimal differences in representation can be reliably detected. This offers a flexible and interpretable alternative to probing. Applied to XLM-R (Conneau et al, 2020) across pretraining checkpoints and layers, we find that language discrimination declines over training and becomes concentrated in lower layers, while meaning discrimination strengthens over time and stabilizes in deeper layers. We then explore probing tasks, showing some alignment between our metrics and linguistic learning performance.Our results position ABX tasks as a lightweight framework for analyzing the structure of multilingual representations",
    "checked": true,
    "id": "f065490f549b03b35ad500e56dbb4392d84b3ad7",
    "semantic_title": "discriminating form and meaning in multilingual models with minimal-pair abx tasks",
    "citation_count": 0,
    "authors": [
      "Maureen de Seyssel",
      "Jie Chi",
      "Skyler Seto",
      "Maartje Ter Hoeve",
      "Masha Fedzechkina",
      "Natalie Schluter"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1211": {
    "title": "Rethinking Text-based Protein Understanding: Retrieval or LLM?",
    "volume": "main",
    "abstract": "In recent years, protein-text models have gained significant attention for their potential in protein generation and understanding. Current approaches focus on integrating protein-related knowledge into large language models through continued pretraining and multi-modal alignment, enabling simultaneous comprehension of textual descriptions and protein sequences. Through a thorough analysis of existing model architectures and text-based protein understanding benchmarks, we identify significant data leakage issues present in current benchmarks. Moreover, conventional metrics derived from natural language processing fail to assess the model's performance in this domain accurately. To address these limitations, we reorganize existing datasets and introduce a novel evaluation framework based on biological entities. Motivated by our observation, we propose a retrieval-enhanced method, which significantly outperforms fine-tuned LLMs for protein-to-text generation and shows accuracy and efficiency in training-free scenarios. Our code and data will be available",
    "checked": true,
    "id": "447101bf2b9aa22037fa624dc561a7cd119ff8ed",
    "semantic_title": "rethinking text-based protein understanding: retrieval or llm?",
    "citation_count": 2,
    "authors": [
      "Juntong Wu",
      "Zijing Liu",
      "He Cao",
      "Li Hao",
      "Bin Feng",
      "Zishan Shu",
      "Ke Yu",
      "Li Yuan",
      "Yu Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1212": {
    "title": "Grounded Semantic Role Labelling from Synthetic Multimodal Data for Situated Robot Commands",
    "volume": "main",
    "abstract": "Understanding natural language commands in situated Human-Robot Interaction (HRI) requires linking linguistic input to perceptual context. Traditional symbolic parsers lack the flexibility to operate in complex, dynamic environments. We introduce a novel Multimodal Grounded Semantic Role Labelling (G-SRL) framework that combines frame semantics with perceptual grounding, enabling robots to interpret commands via multimodal logical forms. Our approach leverages modern Visual Language Models (VLLMs), which jointly process text and images, and is supported by an automated pipeline that generates high-quality training data. Structured command annotations are converted into photorealistic scenes via LLM-guided prompt engineering and diffusion models, then rigorously validated through object detection and visual question answering. The pipeline produces over 11,000 image-command pairs (3,500+ manually validated), while approaching the quality of manually curated datasets at significantly lower cost",
    "checked": true,
    "id": "9062b8669be81727ac4f895d3bd35d01189bb564",
    "semantic_title": "grounded semantic role labelling from synthetic multimodal data for situated robot commands",
    "citation_count": 0,
    "authors": [
      "Claudiu Daniel Hromei",
      "Antonio Scaiella",
      "Danilo Croce",
      "Roberto Basili"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1213": {
    "title": "Easy as PIE? Identifying Multi-Word Expressions with LLMs",
    "volume": "main",
    "abstract": "We investigate the identification of idiomatic expressions—a semantically non-compositional subclass of multiword expressions (MWEs)—in running text using large language models (LLMs) without any fine-tuning. Instead, we adopt a prompt-based approach and evaluate a range of prompting strategies, including zero-shot, few-shot, and chain-of-thought variants, across multiple languages, datasets, and model types. Our experiments show that, with well-crafted prompts, LLMs can perform competitively with supervised models trained on annotated data. These findings highlight the potential of prompt-based LLMs as a flexible and effective alternative for idiomatic expression identification",
    "checked": true,
    "id": "e6f3e27aa16d827f1fbe8c76b2095d1934f06d4a",
    "semantic_title": "easy as pie? identifying multi-word expressions with llms",
    "citation_count": 0,
    "authors": [
      "Kai Golan Hashiloni",
      "Ofri Hefetz",
      "Kfir Bar"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1214": {
    "title": "Query-Focused Retrieval Heads Improve Long-Context Reasoning and Re-ranking",
    "volume": "main",
    "abstract": "Recent work has identified retrieval heads (Wu et al., 2025), a subset of attention heads responsible for retrieving salient information in long-context language models (LMs), as measured by their copy-paste behavior in Needle-in-a-Haystack tasks. In this paper, we introduce QRHead (Query-Focused Retrieval Head), an improved set of attention heads that enhance retrieval from long context. We identify QRHead by aggregating attention scores with respect to the input query, using a handful of examples from real-world tasks (e.g., long-context QA). We further introduce QRRetriever, an efficient and effective retriever that uses the accumulated attention mass of QRHead as retrieval scores. We use QRRetriever for long-context reasoning by selecting the most relevant parts with the highest retrieval scores. On multi-hop reasoning tasks LongMemEval and CLIPPER, this yields over 10% performance gains over full context and outperforms strong dense retrievers. We also evaluate QRRetriever as a re-ranker on the BEIR benchmark and find that it achieves strong zero-shot performance, outperforming other LLM-based re-rankers such as RankGPT. Further analysis shows that both the query-context attention scoring and task selection are crucial for identifying QRHead with strong downstream utility. Overall, our work contributes a general-purpose retriever and offers interpretability insights into the long-context capabilities of LMs",
    "checked": true,
    "id": "1e94331d12051b41a8fc80948e8531acdb0613c8",
    "semantic_title": "query-focused retrieval heads improve long-context reasoning and re-ranking",
    "citation_count": 4,
    "authors": [
      "Wuwei Zhang",
      "Fangcong Yin",
      "Howard Yen",
      "Danqi Chen",
      "Xi Ye"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1215": {
    "title": "Robust Adaptation of Large Multimodal Models for Retrieval Augmented Hateful Meme Detection",
    "volume": "main",
    "abstract": "Hateful memes have become a significant concern on the Internet, necessitating robust automated detection systems. While Large Multimodal Models (LMMs) have shown promise in hateful meme detection, they face notable challenges like sub-optimal performance and limited out-of-domain generalization capabilities. Recent studies further reveal the limitations of both supervised fine-tuning (SFT) and in-context learning when applied to LMMs in this setting. To address these issues, we propose a robust adaptation framework for hateful meme detection that enhances in-domain accuracy and cross-domain generalization while preserving the general vision-language capabilities of LMMs. Analysis reveals that our approach achieves improved robustness under adversarial attacks compared to SFT models. Experiments on six meme classification datasets show that our approach achieves state-of-the-art performance, outperforming larger agentic systems.Moreover, our method generates higher-quality rationales for explaining hateful content compared to standard SFT, enhancing model interpretability. Code available at https://github.com/JingbiaoMei/RGCL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingbiao Mei",
      "Jinghong Chen",
      "Guangyu Yang",
      "Weizhe Lin",
      "Bill Byrne"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1216": {
    "title": "Audio-Reasoner: Improving Reasoning Capability in Large Audio Language Models",
    "volume": "main",
    "abstract": "Recent advancements in multimodal reasoning overlook the audio modality. We introduce Audio-Reasoner, a large-scale audio language model for deep reasoning. We meticulously curated a large-scale and diverse multi-task audio dataset with simple annotations. Then, we leverage closed-source models to conduct secondary labeling, QA generation, along with structured COT process. These datasets together form a high-quality reasoning dataset with 1.2 million reasoning-rich samples, which we name CoTA. Following inference scaling principles, we train Audio-Reasoner on CoTA, enabling it to achieve great logical capabilities in audio reasoning. Experiments show state-of-the-art performance across key benchmarks, including MMAU-mini (+25.42%), AIR-Bench chat/foundation (+14.57%/+10.13%), and MELD (+8.01%). Our findings stress the core of structured CoT training in advancing audio reasoning. The model, dataset, and code are open-sourced at [https://github.com/xzf-thu/Audio-Reasoner](https://github.com/xzf-thu/Audio-Reasoner) or [https://huggingface.co/datasets/zhifeixie/Audio-Reasoner-CoTA](https://huggingface.co/datasets/zhifeixie/Audio-Reasoner-CoTA)",
    "checked": true,
    "id": "e956692fb68766632c52cceb8469649bf023b7d3",
    "semantic_title": "audio-reasoner: improving reasoning capability in large audio language models",
    "citation_count": 57,
    "authors": [
      "Xie Zhifei",
      "Mingbao Lin",
      "Zihang Liu",
      "Pengcheng Wu",
      "Shuicheng Yan",
      "Chunyan Miao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1217": {
    "title": "From perception to production: how acoustic invariance facilitates articulatory learning in a self-supervised vocal imitation model",
    "volume": "main",
    "abstract": "Human infants face a formidable challenge in speech acquisition: mapping extremely variable acoustic inputs into appropriate articulatory movements without explicit instruction. We present a computational model that addresses the acoustic-to-articulatory mapping problem through self-supervised learning. Our model comprises a feature extractor that transforms speech into latent representations, an inverse model that maps these representations to articulatory parameters, and a synthesizer that generates speech outputs. Experiments conducted in both single- and multi-speaker settings reveal that intermediate layers of a pre-trained wav2vec 2.0 model provide optimal representations for articulatory learning, significantly outperforming MFCC features. These representations enable our model to learn articulatory trajectories that correlate with human patterns, discriminate between places of articulation, and produce intelligible speech. Critical to successful articulatory learning are representations that balance phonetic discriminability with speaker invariance – precisely the characteristics of self-supervised representation learning models. Our findings provide computational evidence consistent with developmental theories proposing that perceptual learning of phonetic categories guides articulatory development, offering insights into how infants might acquire speech production capabilities despite the complex mapping problem they face",
    "checked": true,
    "id": "d2d8f1ed743a8c266760cb26d685e6888c9693bc",
    "semantic_title": "from perception to production: how acoustic invariance facilitates articulatory learning in a self-supervised vocal imitation model",
    "citation_count": 0,
    "authors": [
      "Marvin Lavechin",
      "Thomas Hueber"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1218": {
    "title": "REALM: Recursive Relevance Modeling for LLM-based Document Re-Ranking",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have shown strong capabilities in document re-ranking, a key component in modern Information Retrieval (IR) systems. However, existing LLM-based approaches face notable limitations, including ranking uncertainty, unstable top-k recovery, and high token cost due to token-intensive prompting. To effectively address these limitations, we propose REALM, an uncertainty-aware re-ranking framework that models LLM-derived relevance as Gaussian distributions and refines them through recursive Bayesian updates. By explicitly capturing uncertainty and minimizing redundant queries, REALM achieves better rankings more efficiently. Experimental results demonstrate that our REALM surpasses state-of-the-art re-rankers while significantly reducing token usage and latency, improving NDCG@10 by 0.7-11.9 and simultaneously reducing the number of LLM inferences by 23.4-84.4%, promoting it as the next-generation re-ranker for modern IR systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pinhuan Wang",
      "Zhiqiu Xia",
      "Chunhua Liao",
      "Feiyi Wang",
      "Hang Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1219": {
    "title": "PLLuM-Align: Polish Preference Dataset for Large Language Model Alignment",
    "volume": "main",
    "abstract": "Alignment is the critical process of minimizing harmful outputs by teaching large language models (LLMs) to prefer safe, helpful and appropriate responses. While the majority of alignment research and datasets remain overwhelmingly English-centric, ensuring safety across diverse linguistic and cultural contexts requires localized resources. In this paper, we introduce the first Polish preference dataset PLLuM-Align, created entirely through human annotation to reflect Polish language and cultural nuances. The dataset includes response rating, ranking, and multi-turn dialog data. Designed to reflect the linguistic subtleties and cultural norms of Polish, this resource lays the groundwork for more aligned Polish LLMs and contributes to the broader goal of multilingual alignment in underrepresented languages",
    "checked": true,
    "id": "1e78d2ac02684e7fb6ad2932902789ce732a9cbf",
    "semantic_title": "pllum-align: polish preference dataset for large language model alignment",
    "citation_count": 0,
    "authors": [
      "Karolina Seweryn",
      "Anna Kołos",
      "Agnieszka Karlińska",
      "Katarzyna Lorenc",
      "Katarzyna Dziewulska",
      "Maciej Chrabaszcz",
      "Aleksandra Krasnodebska",
      "Paula Betscher",
      "Zofia Cieślińska",
      "Katarzyna Kowol",
      "Julia Moska",
      "Dawid Motyka",
      "Paweł Walkowiak",
      "Bartosz Żuk",
      "Arkadiusz Janz"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1220": {
    "title": "Graph-R1: Incentivizing the Zero-Shot Graph Learning Capability in LLMs via Explicit Reasoning",
    "volume": "main",
    "abstract": "Generalizing to unseen graph tasks without task-specific supervision remains challenging. Graph Neural Networks (GNNs) are limited by fixed label spaces, while Large Language Models (LLMs) lack structural inductive biases. Recent advances in Large Reasoning Models (LRMs) provide a zero-shot alternative via explicit, long chain-of-thought reasoning. Inspired by this, we propose a GNN-free approach that reformulates graph tasks—node classification, link prediction, and graph classification—as textual reasoning problems solved by LRMs. We introduce the first datasets with detailed reasoning traces for these tasks and develop Graph-R1, a reinforcement learning framework that leverages task-specific rethink templates to guide reasoning over linearized graphs. Experiments demonstrate that Graph-R1 outperforms state-of-the-art baselines in zero-shot settings, producing interpretable and effective predictions. Our work highlights the promise of explicit reasoning for graph learning and provides new resources for future research. Codes are available at https://github.com/lgybuaa/Graph-R1",
    "checked": true,
    "id": "11fe96140d30557ab1908bb1ebeecfd78021891a",
    "semantic_title": "graph-r1: incentivizing the zero-shot graph learning capability in llms via explicit reasoning",
    "citation_count": 1,
    "authors": [
      "Yicong Wu",
      "Guangyue Lu",
      "Yuan Zuo",
      "Huarong Zhang",
      "Junjie Wu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1221": {
    "title": "Scalable and Culturally Specific Stereotype Dataset Construction via Human-LLM Collaboration",
    "volume": "main",
    "abstract": "Research on stereotypes in large language models (LLMs) has largely focused on English-speaking contexts, due to the lack of datasets in other languages and the high cost of manual annotation in underrepresented cultures. To address this gap, we introduce a cost-efficient human-LLM collaborative annotation framework and apply it to construct EspanStereo, a Spanish-language stereotype dataset spanning multiple Spanish-speaking countries across Europe and Latin America. EspanStereo captures both well-documented stereotypes from prior literature and culturally specific biases absent from English-centric resources. Using LLMs to generate candidate stereotypes and in-culture annotators to validate them, we demonstrate the framework's effectiveness in identifying nuanced, region-specific biases. Our evaluation of Spanish-supporting LLMs using EspanStereo reveals significant variation in stereotypical behavior across countries, highlighting the need for more culturally grounded assessments. Beyond Spanish, our framework is adaptable to other languages and regions, offering a scalable path toward multilingual stereotype benchmarks. This work broadens the scope of stereotype analysis in LLMs and lays the groundwork for comprehensive cross-cultural bias evaluation",
    "checked": true,
    "id": "be3f6181d9cddf67af2b666435ef10e53043776d",
    "semantic_title": "scalable and culturally specific stereotype dataset construction via human-llm collaboration",
    "citation_count": 0,
    "authors": [
      "Weicheng Ma",
      "John J. Guerrerio",
      "Soroush Vosoughi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1222": {
    "title": "Can Large Language Models Be Good Language Teachers?",
    "volume": "main",
    "abstract": "Large language models (LLMs) have achieved remarkable success across diverse domains. However, their potential as effective language teachers—particularly in complex pedagogical scenarios like teaching Chinese as a second language—remains inadequately assessed. To address this gap, we propose the first pedagogical competence benchmark for LLMs, rigorously evaluating their performance against international standards for Chinese language teachers. Our framework spans three core dimensions: (1) basic knowledge evaluation, covering 32 subtopics across five major categories; (2) international teacher examination, based on data collected from international Chinese teacher certification exams; and (3) teaching practice evaluation, where target LLMs summarize knowledge points and design instructional content for student models, followed by testing the student models to assess the LLM's ability to distill and teach key concepts.We conduct a comprehensive evaluation of 13 latest multilingual and Chinese LLMs. While most models demonstrate promising pedagogical potential, there remains substantial room for improvement in their teaching capabilities. This study contributes to the development of AI-assisted language education tools capable of rivaling human teaching excellence. The benchmark dataset and evaluation scripts used in this study are publicly available at https://github.com/Line-Kite/CLTE",
    "checked": true,
    "id": "517697e0935168486448a8673067b3d59cff1222",
    "semantic_title": "can large language models be good language teachers?",
    "citation_count": 0,
    "authors": [
      "LiQing Xu",
      "Qiwei Li",
      "Tianshuo Peng",
      "Zuchao Li",
      "Hai Zhao",
      "Ping Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1223": {
    "title": "Empowering Math Problem Generation and Reasoning for Large Language Model via Synthetic Data based Continual Learning Framework",
    "volume": "main",
    "abstract": "The large language models (LLMs) learning framework for math problem generation (MPG) mostly performs homogeneous training in different epochs on small-scale manually annotated data. This pattern struggles to provide large-scale new quality data to support continual improvement, and fails to stimulate the mutual promotion reaction between generation and reasoning ability of math problem, resulting in the lack of reliable solving process. This paper proposes a synthetic data based continual learning framework to improve LLMs ability for MPG and math reasoning. The framework cycles through three stages, \"supervised fine-tuning, data synthesis, direct preference optimization\", continuously and steadily improve performance. We propose a synthetic data method with dual mechanism of model self-play and multi-agent cooperation is proposed, which ensures the consistency and validity of synthetic data through sample filtering and rewriting strategies, and overcomes the dependence of continual learning on manually annotated data. A data replay strategy that assesses sample importance via loss differentials is designed to mitigate catastrophic forgetting. Experimental analysis on abundant authoritative math datasets demonstrates the superiority and effectiveness of our framework",
    "checked": true,
    "id": "6a3a8b19c1fd7957dea275ee8bb9a216739dc2c1",
    "semantic_title": "empowering math problem generation and reasoning for large language model via synthetic data based continual learning framework",
    "citation_count": 0,
    "authors": [
      "Qian Wan",
      "Wangzi Shi",
      "Jintian Feng",
      "Shengyingjie Liu",
      "Luona Wei",
      "Zhicheng Dai",
      "Jianwen Sun"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1224": {
    "title": "Tokenization and Representation Biases in Multilingual Models on Dialectal NLP Tasks",
    "volume": "main",
    "abstract": "Dialectal data are characterized by linguistic variation that appears small to humans but has a significant impact on the performance of models. This dialect gap has been related to various factors (e.g., data size, economic and social factors) whose impact, however, turns out to be inconsistent. In this work, we investigate factors impacting the model performance more directly: we correlate Tokenization Parity (TP) and Information Parity (IP), as measures of representational biases in pre-trained multilingual models, with the downstream performance. We compare state-of-the-art decoder-only LLMs with encoder-based models across three tasks: dialect classification, topic classification, and extractive question answering, controlling for varying scripts (Latin vs. non-Latin) and resource availability (high vs. low). Our analysis reveals that TP is a better predictor of the performance on tasks reliant on syntactic and morphological cues (e.g., extractive QA), while IP better predicts performance in semantic tasks (e.g., topic classification). Complementary analyses, including tokenizer behavior, vocabulary coverage, and qualitative insights, reveal that the language support claims of LLMs often might mask deeper mismatches at the script or token level",
    "checked": true,
    "id": "c4483403a6b56dc477576fb53e00193bc324db39",
    "semantic_title": "tokenization and representation biases in multilingual models on dialectal nlp tasks",
    "citation_count": 0,
    "authors": [
      "Vani Kanjirangat",
      "Tanja Samardzic",
      "Ljiljana Dolamic",
      "Fabio Rinaldi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1225": {
    "title": "Evaluating the Evaluators: Are readability metrics good measures of readability?",
    "volume": "main",
    "abstract": "Plain Language Summarization (PLS) aims to distill complex documents into accessible summaries for non-expert audiences. In this paper, we conduct a thorough survey of PLS literature, and identify that the current standard practice for readability evaluation is to use traditional readability metrics, such as Flesch-Kincaid Grade Level (FKGL). However, despite proven utility in other fields, these metrics have not been compared to human readability judgments in PLS. We evaluate 8 readability metrics and show that most correlate poorly with human judgments, including the most popular metric, FKGL. We then show that Language Models (LMs) are better judges of readability, with the best-performing model achieving a Pearson correlation of 0.56 with human judgments. Extending our analysis to PLS datasets, which contain summaries aimed at non-expert audiences, we find that LMs better capture deeper measures of readability, such as required background knowledge, and lead to different conclusions than the traditional metrics. Based on these findings, we offer recommendations for best practices in the evaluation of plain language summaries",
    "checked": true,
    "id": "cde790b32ef5948e6c5fa077d12a2cf9d600e843",
    "semantic_title": "evaluating the evaluators: are readability metrics good measures of readability?",
    "citation_count": 0,
    "authors": [
      "Isabel Cachola",
      "Daniel Khashabi",
      "Mark Dredze"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1226": {
    "title": "Text Takes Over: A Study of Modality Bias in Multimodal Intent Detection",
    "volume": "main",
    "abstract": "The rise of multimodal data, integrating text, audio, and visuals, has created new opportunities for studying multimodal tasks such as intent detection. This work investigates the effectiveness of Large Language Models (LLMs) and non-LLMs, including text-only and multimodal models, in the multimodal intent detection task. Our study reveals that Mistral-7B, a text-only LLM, outperforms most competitive multimodal models by approximately 9% on MIntRec-1 and 4% on MIntRec2.0 dataset. This performance advantage comes from a strong textual bias in these datasets, where over 90% of the samples require textual input, either alone or in combination with other modalities, for correct classification. We confirm the modality bias of these datasets via human evaluation, too. Next, we propose a framework to debias the datasets, and upon debiasing, more than 70% of the samples in MIntRec-1 and more than 50% in MIntRec2.0 get removed, resulting in significant performance degradation across all models, with smaller multimodal fusion models being the most affected with an accuracy drop of over 50 - 60%. Further, we analyze the context-specific relevance of different modalities through empirical analysis. Our findings highlight the challenges posed by modality bias in multimodal intent datasets and emphasize the need for unbiased datasets to evaluate multimodal models effectively. We release both the code and the dataset used for this work at https://github.com/Text-Takes-Over-EMNLP-2025/MultiModal-Intent-EMNLP-2025",
    "checked": true,
    "id": "2a1a2472d52e03dd0e079da01f723bfdb395b4e1",
    "semantic_title": "text takes over: a study of modality bias in multimodal intent detection",
    "citation_count": 1,
    "authors": [
      "Ankan Mullick",
      "Saransh Sharma",
      "Abhik Jana",
      "Pawan Goyal"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1227": {
    "title": "What's in a prompt? Language models encode literary style in prompt embeddings",
    "volume": "main",
    "abstract": "Large language models use high-dimensional latent spaces to encode and process textual information. Much work has investigated how the conceptual content of words translates into geometrical relationships between their vector representations. Fewer studies analyze how the cumulative information of an entire prompt becomes condensed into individual embeddings under the action of transformer layers. We use literary pieces to show that information about intangible, rather than factual, aspects of the prompt are contained in deep representations. We observe that short excerpts (10 - 100 tokens) from different novels separate in the latent space independently from what next-token prediction they converge towards. Ensembles from books from the same authors are much more entangled than across authors, suggesting that embeddings encode stylistic features. This geometry of style may have applications for authorship attribution and literary analysis, but most importantly reveals the sophistication of information processing and compression accomplished by language models",
    "checked": true,
    "id": "002ade2c5cbf2a002f562f00a6f81439a82511dd",
    "semantic_title": "what's in a prompt? language models encode literary style in prompt embeddings",
    "citation_count": 0,
    "authors": [
      "Raphaël Sarfati",
      "Haley Moller",
      "Toni J.b. Liu",
      "Nicolas Boulle",
      "Christopher Earls"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1228": {
    "title": "Identifying and Answering Questions with False Assumptions: An Interpretable Approach",
    "volume": "main",
    "abstract": "People often ask questions with false assumptions, a type of question that does not have regular answers. Answering such questions requires first identifying the false assumptions. Large Language Models (LLMs) often generate misleading answers to these questions because of hallucinations. In this paper, we focus on identifying and answering questions with false assumptions in several domains. We first investigate whether the problem reduces to fact verification. Then, we present an approach leveraging external evidence to mitigate hallucinations. Experiments with five LLMs demonstrate that (1) incorporating retrieved evidence is beneficial and (2) generating and validating atomic assumptions yields more improvements and provides an interpretable answer by pinpointing the false assumptions",
    "checked": true,
    "id": "552cac47cdedb0503fa2cd50ff2056eb4cedfc1a",
    "semantic_title": "identifying and answering questions with false assumptions: an interpretable approach",
    "citation_count": 0,
    "authors": [
      "Zijie Wang",
      "Eduardo Blanco"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1229": {
    "title": "VisFinEval: A Scenario-Driven Chinese Multimodal Benchmark for Holistic Financial Understanding",
    "volume": "main",
    "abstract": "Multimodal large language models (MLLMs) hold great promise for automating complex financial analysis. To comprehensively evaluate their capabilities, we introduce VisFinEval, the first large-scale Chinese benchmark that spans the full front-middle-back office lifecycle of financial tasks. VisFinEval comprises 15,848 annotated question–answer pairs drawn from eight common financial image modalities (e.g., K-line charts, financial statements, official seals), organized into three hierarchical scenario depths: Financial Knowledge & Data Analysis, Financial Analysis & Decision Support, and Financial Risk Control & Asset Optimization. We evaluate 21 state-of-the-art MLLMs in a zero-shot setting. The top model, Qwen-VL-max, achieves an overall accuracy of 76.3%, outperforming non-expert humans but trailing financial experts by over 14 percentage points. Our error analysis uncovers six recurring failure modes—including cross-modal misalignment, hallucinations, and lapses in business-process reasoning—that highlight critical avenues for future research. VisFinEval aims to accelerate the development of robust, domain-tailored MLLMs capable of seamlessly integrating textual and visual financial information. The data and the code are available at https://github.com/SUFE-AIFLM-Lab/VisFinEval",
    "checked": true,
    "id": "c6ff2f8672c5bb68a7a0732e707ff294d029d275",
    "semantic_title": "visfineval: a scenario-driven chinese multimodal benchmark for holistic financial understanding",
    "citation_count": 1,
    "authors": [
      "Zhaowei Liu",
      "Xin Guo",
      "Haotian Xia",
      "Lingfeng Zeng",
      "Fangqi Lou",
      "Jinyi Niu",
      "Mengping Li",
      "Qi Qi",
      "Jiahuan Li",
      "Wei Zhang",
      "Yinglong Wang",
      "Weige Cai",
      "Weining Shen",
      "Liwen Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1230": {
    "title": "Socratic-MCTS: Test-Time Visual Reasoning by Asking the Right Questions",
    "volume": "main",
    "abstract": "Recent research in vision-language models (VLMs) has centered around the possibility of equipping them with implicit long-form chain-of-thought reasoning—akin to the success observed in language models—via distillation and reinforcement learning. But what about the non-reasoning models already trained and deployed across the internet? Should we simply abandon them, or is there hope for a search mechanism that can elicit hidden knowledge and induce long reasoning traces— without any additional training or supervision? In this paper, we explore this possibility using a Monte Carlo Tree Search (MCTS)-inspired algorithm, which injects subquestion–subanswer pairs into the model's output stream. We show that framing reasoning as a search process—where subquestions act as latent decisions within a broader inference trajectory—helps the model \"connect the dots\" between fragmented knowledge and produce extended reasoning traces in non-reasoning models. We evaluate our method across three benchmarks and observe consistent improvements. Notably, our approach yields a 2% overall improvement on MMMU-PRO, including a significant 9% gain in Liberal Arts",
    "checked": true,
    "id": "eb92eaf6cfa98816c59db365c7a3e042afc71807",
    "semantic_title": "socratic-mcts: test-time visual reasoning by asking the right questions",
    "citation_count": 2,
    "authors": [
      "David Acuna",
      "Ximing Lu",
      "Jaehun Jung",
      "Hyunwoo Kim",
      "Amlan Kar",
      "Sanja Fidler",
      "Yejin Choi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1231": {
    "title": "LLMs Don't Know Their Own Decision Boundaries: The Unreliability of Self-Generated Counterfactual Explanations",
    "volume": "main",
    "abstract": "To collaborate effectively with humans, language models must be able to explain their decisions in natural language. We study a specific type of self-explanation: self-generated counterfactual explanations (SCEs), where a model explains its prediction by modifying the input such that it would have predicted a different outcome. We evaluate whether LLMs can produce SCEs that are valid, achieving the intended outcome, and minimal, modifying the input no more than necessary. When asked to generate counterfactuals, we find that LLMs typically produce SCEs that are valid, but far from minimal, offering little insight into their decision-making behaviour. Worryingly, when asked to generate minimal counterfactuals, LLMs typically make excessively small edits that fail to change predictions. The observed validity-minimality trade-off is consistent across several LLMs, datasets, and evaluation settings. Our findings suggest that SCEs are, at best, an ineffective explainability tool and, at worst, can provide misleading insights into model behaviour. Proposals to deploy LLMs in high-stakes settings must consider the impact of unreliable self-explanations on downstream decision-making. Our code is available at https://github.com/HarryMayne/SCEs",
    "checked": true,
    "id": "156d890219f3ed2e393516ee56db5ed95fd8b115",
    "semantic_title": "llms don't know their own decision boundaries: the unreliability of self-generated counterfactual explanations",
    "citation_count": 1,
    "authors": [
      "Harry Mayne",
      "Ryan Othniel Kearns",
      "Yushi Yang",
      "Andrew M. Bean",
      "Eoin D. Delaney",
      "Chris Russell",
      "Adam Mahdi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1232": {
    "title": "Grounding Multilingual Multimodal LLMs With Cultural Knowledge",
    "volume": "main",
    "abstract": "Multimodal Large Language Models excel in high-resource settings, but often misinterpret long-tail cultural entities and underperform in low-resource languages. To address this gap, we propose a data-centric approach that directly grounds MLLMs in cultural knowledge. Leveraging a large scale knowledge graph from Wikidata, we collect images that represent culturally significant entities, and generate synthetic multilingual visual question answering data. The resulting dataset, CulturalGround, comprises 22 million high-quality, culturally-rich VQA pairs spanning 42 countries and 39 languages. We train an open-source MLLM CulturalPangea on CulturalGround, interleaving standard multilingual instruction-tuning data to preserve general abilities. Cultural-Pangea achieves state-of-the-art performance among open models on various culture-focused multilingual multimodal benchmarks, outperforming prior models by an average of +5.0%without degrading results on mainstream vision–language tasks. Our findings show that our targeted, culturally grounded approach could substantially narrow the cultural gap in MLLMs and offer a practical path towards globally inclusive multimodal systems",
    "checked": true,
    "id": "b4b72ad20477d644e95729c5767d5ce5ef428fa3",
    "semantic_title": "grounding multilingual multimodal llms with cultural knowledge",
    "citation_count": 3,
    "authors": [
      "Jean De Dieu Nyandwi",
      "Yueqi Song",
      "Simran Khanuja",
      "Graham Neubig"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1233": {
    "title": "Following Length Constraints in Instructions",
    "volume": "main",
    "abstract": "Aligned instruction following models can better fulfill user requests than their unaligned counterparts. However, it has been shown that there is a length bias in evaluation of such models, and that training algorithms tend to exploit this bias by learning longer responses. In this work we show how to train models that can be controlled at inference time with instructions containing desired length constraints. Such models are superior in length instructed evaluations, outperforming standard instruction following models such as GPT4, Llama 3 and Mixtral",
    "checked": true,
    "id": "03808773f965ef5fcba04ccd6a537ccebcd52980",
    "semantic_title": "following length constraints in instructions",
    "citation_count": 0,
    "authors": [
      "Weizhe Yuan",
      "Ilia Kulikov",
      "Ping Yu",
      "Kyunghyun Cho",
      "Sainbayar Sukhbaatar",
      "Jason E Weston",
      "Jing Xu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1234": {
    "title": "Memory-QA: Answering Recall Questions Based on Multimodal Memories",
    "volume": "main",
    "abstract": "We introduce Memory-QA, a novel real-world task that involves answering recall questions about visual content from previously stored multimodal memories. This task poses unique challenges, including the creation of task-oriented memories, the effective utilization of temporal and location information within memories, and the ability to draw upon multiple memories to answer a recall question. To address these challenges, we propose a comprehensive pipeline, Pensieve, integrating memory-specific augmentation, time- and location-aware multi-signal retrieval, and multi-memory QA fine-tuning. We created a multimodal benchmark to illustrate various real challenges in this task, and show the superior performance of Pensieve over state-of-the-art solutions (up to +14% on QA accuracy)",
    "checked": true,
    "id": "d04d8121d0f0edb0bbf8667deb3e5aa4a93d9121",
    "semantic_title": "memory-qa: answering recall questions based on multimodal memories",
    "citation_count": 0,
    "authors": [
      "Hongda Jiang",
      "Xinyuan Zhang",
      "Siddhant Garg",
      "Rishab Arora",
      "Shiun-Zu Kuo",
      "Jiayang Xu",
      "Aaron Colak",
      "Xin Luna Dong"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1235": {
    "title": "NEXUS: Network Exploration for eXploiting Unsafe Sequences in Multi-Turn LLM Jailbreaks",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have revolutionized natural language processing, yet remain vulnerable to jailbreak attacks—particularly multi-turn jailbreaks that distribute malicious intent across benign exchanges, thereby bypassing alignment mechanisms. Existing approaches often suffer from limited exploration of the adversarial space, rely on hand-crafted heuristics, or lack systematic query refinement. We propose NEXUS (Network Exploration for eXploiting Unsafe Sequences), a modular framework for constructing, refining, and executing optimized multi-turn attacks. NEXUS comprises: (1) ThoughtNet, which hierarchically expands a harmful intent into a structured semantic network of topics, entities, and query chains; (2) a feedback-driven Simulator that iteratively refines and prunes these chains through attacker–victim–judge LLM collaboration using harmfulness and semantic-similarity benchmarks; and (3) a Network Traverser that adaptively navigates the refined query space for real-time attacks. This pipeline systematically uncovers stealthy, high-success adversarial paths across LLMs. Our experimental results on several closed-source and open-source LLMs show that NEXUS can achieve a higher attack success rate, between 2.1% and 19.4%, compared to state-of-the-art approaches. Our source code is available at https://github.com/inspire-lab/NEXUS",
    "checked": true,
    "id": "2935d6fc607cbd4419ec7d32b145005311da53b3",
    "semantic_title": "nexus: network exploration for exploiting unsafe sequences in multi-turn llm jailbreaks",
    "citation_count": 0,
    "authors": [
      "Javad Rafiei Asl",
      "Sidhant Narula",
      "Mohammad Ghasemigol",
      "Eduardo Blanco",
      "Daniel Takabi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1236": {
    "title": "Sketch-of-Thought: Efficient LLM Reasoning with Adaptive Cognitive-Inspired Sketching",
    "volume": "main",
    "abstract": "Recent advances in large language models (LLMs) have enabled strong reasoning capabilities through Chain-of-Thought (CoT) prompting, which elicits step-by-step problem solving, but often at the cost of excessive verbosity in intermediate outputs, leading to increased computational overhead. We propose Sketch-of-Thought (SoT), a prompting framework that integrates cognitively inspired reasoning paradigms with linguistic constraints to reduce token usage while preserving reasoning accuracy. SoT is designed as a flexible, modular approach and is instantiated with three paradigms—Conceptual Chaining, Chunked Symbolism, and Expert Lexicons—each tailored to distinct reasoning tasks and selected dynamically at test-time by a lightweight routing model. Across 18 reasoning datasets spanning multiple domains, languages, and modalities, SoT achieves token reductions of up to 84% with minimal accuracy loss. In tasks such as mathematical and multi-hop reasoning, it even improves accuracy while shortening outputs",
    "checked": true,
    "id": "90f2a86c37db66fcfbd2d2ec8df2ec136caf7df6",
    "semantic_title": "sketch-of-thought: efficient llm reasoning with adaptive cognitive-inspired sketching",
    "citation_count": 78,
    "authors": [
      "Simon A. Aytes",
      "Jinheon Baek",
      "Sung Ju Hwang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1237": {
    "title": "From Language to Cognition: How LLMs Outgrow the Human Language Network",
    "volume": "main",
    "abstract": "Large language models (LLMs) exhibit remarkable similarity to neural activity in the human language network. However, the key properties of language underlying this alignment—and how brain-like representations emerge and change across training—remain unclear. We here benchmark 34 training checkpoints spanning 300B tokens across 8 different model sizes to analyze how brain alignment relates to linguistic competence. Specifically, we find that brain alignment tracks the development of formal linguistic competence—i.e., knowledge of linguistic rules—more closely than functional linguistic competence. While functional competence, which involves world knowledge and reasoning, continues to develop throughout training, its relationship with brain alignment is weaker, suggesting that the human language network primarily encodes formal linguistic structure rather than broader cognitive functions. Notably, we find that the correlation between next-word prediction, behavioral alignment, and brain alignment fades once models surpass human language proficiency. We further show that model size is not a reliable predictor of brain alignment when controlling for the number of features. Finally, using the largest set of rigorous neural language benchmarks to date, we show that language brain alignment benchmarks remain unsaturated, highlighting opportunities for improving future models. Taken together, our findings suggest that the human language network is best modeled by formal, rather than functional, aspects of language",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Badr AlKhamissi",
      "Greta Tuckute",
      "Yingtian Tang",
      "Taha Osama A Binhuraib",
      "Antoine Bosselut",
      "Martin Schrimpf"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1238": {
    "title": "Logos as a Well-Tempered Pre-train for Sign Language Recognition",
    "volume": "main",
    "abstract": "This paper examines two aspects of the isolated sign language recognition (ISLR) task. First, although a certain number of datasets is available, the data for individual sign languages is limited. It poses the challenge of cross-language ISLR model training, including transfer learning. Second, similar signs can have different semantic meanings. It leads to ambiguity in dataset labeling and raises the question of the best policy for annotating such signs. To address these issues, this study presents Logos, a novel Russian Sign Language (RSL) dataset, the most extensive available ISLR dataset by the number of signers, one of the most extensive datasets in size and vocabulary, and the largest RSL dataset. It is shown that a model, pre-trained on the Logos dataset can be used as a universal encoder for other language SLR tasks, including few-shot learning. We explore cross-language transfer learning approaches and find that joint training using multiple classification heads benefits accuracy for the target low-resource datasets the most. The key feature of the Logos dataset is explicitly annotated visually similar sign groups. We show that explicitly labeling visually similar signs improves trained model quality as a visual encoder for downstream tasks. Based on the proposed contributions, we outperform current state-of-the-art results for the WLASL dataset and get competitive results for the AUTSL dataset, with a single stream model processing solely RGB video. The source code, dataset, and pre-trained models are publicly available",
    "checked": true,
    "id": "63eea5048d8a6f1f2163f07e5bd8f3e950a0a08f",
    "semantic_title": "logos as a well-tempered pre-train for sign language recognition",
    "citation_count": 0,
    "authors": [
      "Ilya Ovodov",
      "Petr Surovtsev",
      "Karina Kvanchiani",
      "Alexander Kapitanov",
      "Alexander Nagaev"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1239": {
    "title": "Hallucination Detection in LLMs Using Spectral Features of Attention Maps",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across various tasks but remain prone to hallucinations. Detecting hallucinations is essential for safety-critical applications, and recent methods leverage attention map properties to this end, though their effectiveness remains limited. In this work, we investigate the spectral features of attention maps by interpreting them as adjacency matrices of graph structures. We propose the LapEigvals method, which utilises the top-k eigenvalues of the Laplacian matrix derived from the attention maps as an input to hallucination detection probes. Empirical evaluations demonstrate that our approach achieves state-of-the-art hallucination detection performance among attention-based methods. Extensive ablation studies further highlight the robustness and generalisation of LapEigvals, paving the way for future advancements in the hallucination detection domain",
    "checked": true,
    "id": "550f47dbfa0f12c35ba4331008503d80ab8f41c5",
    "semantic_title": "hallucination detection in llms using spectral features of attention maps",
    "citation_count": 7,
    "authors": [
      "Jakub Binkowski",
      "Denis Janiak",
      "Albert Sawczyn",
      "Bogdan Gabrys",
      "Tomasz Jan Kajdanowicz"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1240": {
    "title": "Composable Cross-prompt Essay Scoring by Merging Models",
    "volume": "main",
    "abstract": "Recent advances in cross-prompt automated essay scoring typically train models jointly on all available source domains, often requiring simultaneous access to unlabeled target domain samples. However, using all sources can lead to suboptimal transfer and high computational cost. Moreover, repeatedly accessing the source essays for continual adaptation raises privacy concerns. We propose a source-free adaptation approach that selectively merges the parameters of individually trained source models without further access to the source datasets. In particular, we mix the task vectors—the parameter updates from fine-tuning—via a weighted sum to efficiently simulate selective joint-training. We use Bayesian optimization to determine the mixing weights using our proposed Prior-encoded Information Maximization (PIM), an unsupervised objective which promotes score discriminability by leveraging useful priors pre-computed from the sources. Experimental results with LLMs on in-dataset and cross-dataset adaptation show that our method (1) consistently outperforms joint-training on all sources, (2) maintains superior robustness compared to other merging methods, (3) excels under severe distribution shifts where recent leading cross-prompt methods struggle, all while retaining computational efficiency",
    "checked": true,
    "id": "10fa0add68401cfcd754b94efff8f3adf48d7ed9",
    "semantic_title": "composable cross-prompt essay scoring by merging models",
    "citation_count": 1,
    "authors": [
      "Sanwoo Lee",
      "Kun Liang",
      "Yunfang Wu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1241": {
    "title": "Towards a Holistic and Automated Evaluation Framework for Multi-Level Comprehension of LLMs in Book-Length Contexts",
    "volume": "main",
    "abstract": "We introduce HAMLET, a holistic and automated framework for evaluating the long-context comprehension of large language models (LLMs). HAMLET structures key information of source texts into a three-level hierarchy at root-, branch-, and leaf-levels, and employs query-focused summarization to evaluate how well models faithfully recall the key information at each level. To validate the reliability of our fully automated pipeline, we conduct a systematic human study, demonstrating that our automatic evaluation achieves over 90% agreement with expert human judgments, while reducing the evaluation cost by up to 25×. HAMLET reveals that LLMs struggle with fine-grained comprehension, especially at the leaf level, and are sensitive to positional effects like the lost-in-the-middle. Analytical queries pose greater challenges than narrative ones, and consistent performance gaps emerge between open-source and proprietary models, as well as across model scales. Our code and dataset are publicly available at https://github.com/DISL-Lab/HAMLET",
    "checked": true,
    "id": "987bf58a3688385cb608a2fe99ba69dd091e7174",
    "semantic_title": "towards a holistic and automated evaluation framework for multi-level comprehension of llms in book-length contexts",
    "citation_count": 0,
    "authors": [
      "Yuho Lee",
      "Jiaqi Deng",
      "Nicole Hee-Yeon Kim",
      "Hyangsuk Min",
      "Taewon Yun",
      "Minjeong Ban",
      "Kim Yul",
      "Hwanjun Song"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1242": {
    "title": "Improving Large Language Models Function Calling and Interpretability via Guided-Structured Templates",
    "volume": "main",
    "abstract": "Large language models (LLMs) have demonstrated strong reasoning and tool-use capabilities, yet they often fail in real-world tool-interactions due to incorrect parameterization, poor tool selection, or misinterpretation of user intent. These issues often stem from an incomplete understanding of user goals and inadequate comprehension of tool documentation. While Chain-of-Thought (CoT) prompting has proven effective for enhancing reasoning in general contexts, our analysis reveals that free-form CoT is insufficient and sometimes counterproductive for structured function-calling tasks. To address this, we introduce a curriculum-inspired framework that leverages structured reasoning templates to guide LLMs through more deliberate step-by-step instructions for generating function callings. Experimental results show that our method reduces tool-use errors, achieving 3–12% relative improvements over strong baselines across diverse model series and approaches. Moreover, our framework enhances the robustness, interpretability, and transparency of tool-using agents, advancing the development of more reliable AI assistants for real-world applications",
    "checked": true,
    "id": "95e4926084941d82156fdbfd69a1170a6ea8a19a",
    "semantic_title": "improving large language models function calling and interpretability via guided-structured templates",
    "citation_count": 1,
    "authors": [
      "Hy Dang",
      "Tianyi Liu",
      "Zhuofeng Wu",
      "Jingfeng Yang",
      "Haoming Jiang",
      "Tao Yang",
      "Pei Chen",
      "Zhengyang Wang",
      "Helen Wang",
      "Huasheng Li",
      "Bing Yin",
      "Meng Jiang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1243": {
    "title": "Evaluation and Facilitation of Online Discussions in the LLM Era: A Survey",
    "volume": "main",
    "abstract": "We present a survey of methods for assessing and enhancing the quality of online discussions, focusing on the potential of Large Language Models (LLMs). While online discourses aim, at least in theory, to foster mutual understanding, they often devolve into harmful exchanges, such as hate speech, threatening social cohesion and democratic values. Recent advancements in LLMs enable artificial facilitation agents to not only moderate content, but also actively improve the quality of interactions. Our survey synthesizes ideas from Natural Language Processing (NLP) and Social Sciences to provide (a) a new taxonomy on discussion quality evaluation, (b) an overview of intervention and facilitation strategies, (c) along with a new taxonomy of conversation facilitation datasets, (d) an LLM-oriented roadmap of good practices and future research directions, from technological and societal perspectives",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Katerina Korre",
      "Dimitris Tsirmpas",
      "Nikos Gkoumas",
      "Emma Cabalé",
      "Danai Myrtzani",
      "Theodoros Evgeniou",
      "Ion Androutsopoulos",
      "John Pavlopoulos"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1244": {
    "title": "Temporal Scaling Law for Large Language Models",
    "volume": "main",
    "abstract": "Recently, Large Language Models (LLMs) have been widely adopted in a wide range of tasks, leading to increasing attention towards the research on how scaling LLMs affects their performance. Existing works, termed Scaling Laws, have discovered that the final test loss of LLMs scales as power-laws with model size, computational budget, and dataset size. However, the temporal change of the test loss of an LLM throughout its pretraining process remains unexplored, though it is valuable in many aspects, such as selecting better hyperparameters *directly* on the target LLM. In this paper, we propose the novel concept of Temporal Scaling Law, studying how the test loss of an LLM evolves as the training steps scale up. In contrast to modeling the test loss as a whole in a coarse-grained manner, we break it down and dive into the fine-grained test loss of each token position, and further develop a dynamic hyperbolic-law. Afterwards, we derive the much more precise temporal scaling law by studying the temporal patterns of the parameters in the dynamic hyperbolic-law. Results on both in-distribution (ID) and out-of-distribution (OOD) validation datasets demonstrate that our temporal scaling law accurately predicts the test loss of LLMs across training steps. Our temporal scaling law has broad practical applications. First, it enables direct and efficient hyperparameter selection on the target LLM, such as data mixture proportions. Secondly, viewing the LLM pretraining dynamics from the token position granularity provides some insights to enhance the understanding of LLM pretraining",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yizhe Xiong",
      "Xiansheng Chen",
      "Xin Ye",
      "Hui Chen",
      "Zijia Lin",
      "Haoran Lian",
      "Zhenpeng Su",
      "Wei Huang",
      "Jianwei Niu",
      "Jungong Han",
      "Guiguang Ding"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1245": {
    "title": "Reframe Your Life Story: Interactive Narrative Therapist and Innovative Moment Assessment with Large Language Models",
    "volume": "main",
    "abstract": "Recent progress in large language models (LLMs) has opened new possibilities for mental health support, yet current approaches lack realism in simulating specialized psychotherapy and fail to capture therapeutic progression over time. Narrative therapy, which helps individuals transform problematic life stories into empowering alternatives, remains underutilized due to limited access and social stigma. We address these limitations through a comprehensive framework with two core components. First, **INT** (Interactive Narrative Therapist) simulates expert narrative therapists by planning therapeutic stages, guiding reflection levels, and generating contextually appropriate responses through retrieval-augmentation. Second, **IMA** (Innovative Moment Assessment) provides a therapy-centric evaluation method that quantifies effectiveness by tracking \"Innovative Moments\" (IMs), critical narrative shifts in client speech signaling therapy progress. Experimental results on 260 simulated clients and 230 human participants reveal that **INT** consistently outperforms standard methods in therapeutic quality and depth. We further demonstrate the effectiveness of **INT** in synthesizing high-quality support conversations to facilitate social applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Feng",
      "Jiaqi Wang",
      "Wenxuan Zhang",
      "Zhuang Chen",
      "Shen Yutong",
      "Xiyao Xiao",
      "Minlie Huang",
      "Liping Jing",
      "Jian Yu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1246": {
    "title": "From Word to World: Evaluate and Mitigate Culture Bias in LLMs via Word Association Test",
    "volume": "main",
    "abstract": "The human-centered word association test (WAT) serves as a cognitive proxy, revealing sociocultural variations through culturally shared semantic expectations and implicit linguistic patterns shaped by lived experiences. We extend this test into an LLM-adaptive, free-relation task to assess the alignment of large language models (LLMs) with cross-cultural cognition. To address culture preference, we propose CultureSteer, an innovative approach that moves beyond superficial cultural prompting by embedding cultural-specific semantic associations directly within the model's internal representation space. Experiments show that current LLMs exhibit significant bias toward Western (notably American) schemas at the word association level. In contrast, our model substantially improves cross-cultural alignment, capturing diverse semantic associations. Further validation on culture-sensitive downstream tasks confirms its efficacy in fostering cognitive alignment across cultures. This work contributes a novel methodological paradigm for enhancing cultural awareness in LLMs, advancing the development of more inclusive language technologies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xunlian Dai",
      "Li Zhou",
      "Benyou Wang",
      "Haizhou Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1247": {
    "title": "Mitigating the Privacy Issues in Retrieval-Augmented Generation (RAG) via Pure Synthetic Data",
    "volume": "main",
    "abstract": "Retrieval-augmented generation (RAG) enhances the outputs of language models by integrating relevant information retrieved from external knowledge sources. However, when the retrieval process involves private data, RAG systems may face severe privacy risks, potentially leading to the leakage of sensitive information. To address this issue, we propose using synthetic data as a privacy-preserving alternative for the retrieval data. We propose SAGE, a novel two-stage synthetic data generation paradigm. In the stage-1, we employ an attribute-based extraction and generation approach to preserve key contextual information from the original data. In the stage-2, we further enhance the privacy properties of the synthetic data through an agent-based iterative refinement process. Extensive experiments demonstrate that using our synthetic data as the retrieval context achieves comparable performance to using the original data while substantially reducing privacy risks. Our work takes the first step towards investigating the possibility of generating high-utility and privacy-preserving synthetic data for RAG, opening up new opportunities for the safe application of RAG systems in various domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shenglai Zeng",
      "Jiankun Zhang",
      "Pengfei He",
      "Jie Ren",
      "Tianqi Zheng",
      "Hanqing Lu",
      "Han Xu",
      "Hui Liu",
      "Yue Xing",
      "Jiliang Tang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1248": {
    "title": "AdaSteer: Your Aligned LLM is Inherently an Adaptive Jailbreak Defender",
    "volume": "main",
    "abstract": "Despite extensive efforts in safety alignment, large language models (LLMs) remain vulnerable to jailbreak attacks. Activation steering offers a training-free defense method but relies on fixed steering coefficients, resulting in suboptimal protection and increased false rejections of benign inputs. To address this, we propose AdaSteer, an adaptive activation steering method that dynamically adjusts model behavior based on input characteristics. We identify two key properties: Rejection Law (R-Law), which shows that stronger steering is needed for jailbreak inputs opposing the rejection direction, and Harmfulness Law (H-Law), which differentiates adversarial and benign inputs. AdaSteer steers input representations along both the Rejection Direction (RD) and Harmfulness Direction (HD), with adaptive coefficients learned via logistic regression, ensuring robust jailbreak defense while preserving benign input handling. Experiments on LLaMA-3.1, Gemma-2, and Qwen2.5 show that AdaSteer outperforms baseline methods across multiple jailbreak attacks with minimal impact on utility. Our results highlight the potential of interpretable model internals for real-time, flexible safety enforcement in LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weixiang Zhao",
      "Jiahe Guo",
      "Yulin Hu",
      "Yang Deng",
      "An Zhang",
      "Xingyu Sui",
      "Xinyang Han",
      "Yanyan Zhao",
      "Bing Qin",
      "Tat-Seng Chua",
      "Ting Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1249": {
    "title": "Large Language Models Meet Knowledge Graphs for Question Answering: Synthesis and Opportunities",
    "volume": "main",
    "abstract": "Large language models (LLMs) have demonstrated remarkable performance on question-answering (QA) tasks because of their superior capabilities in natural language understanding and generation. However, LLM-based QA struggles with complex QA tasks due to poor reasoning capacity, outdated knowledge, and hallucinations. Several recent works synthesize LLMs and knowledge graphs (KGs) for QA to address the above challenges. In this survey, we propose a new structured taxonomy that categorizes the methodology of synthesizing LLMs and KGs for QA according to the categories of QA and the KG's role when integrating with LLMs. We systematically survey state-of-the-art methods in synthesizing LLMs and KGs for QA and compare and analyze these approaches in terms of strength, limitations, and KG requirements. We then align the approaches with QA and discuss how these approaches address the main challenges of different complex QA. Finally, we summarize the advancements, evaluation metrics, and benchmark datasets and highlight open challenges and opportunities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuangtao Ma",
      "Yongrui Chen",
      "Tianxing Wu",
      "Arijit Khan",
      "Haofen Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1250": {
    "title": "TFDP: Token-Efficient Disparity Audits for Autoregressive LLMs via Single-Token Masked Evaluation",
    "volume": "main",
    "abstract": "Auditing autoregressive Large Language Models (LLMs) for disparities is often impeded by high token costs and limited precision. We introduce Token-Focused Disparity Probing (TFDP), a novel methodology overcoming these challenges by adapting single-token masked prediction to autoregressive architectures via targeted token querying. Disparities between minimally contrastive sentence pairs are quantified through a multi-scale semantic alignment score that integrates sentence, local-context, and token embeddings with adaptive weighting. We propose three disparity metrics: Preference Score (\\mathcal{PS}), Prediction Set Divergence (\\mathcal{PSD}), and Weighted Final Score (\\mathcal{WFS}), for comprehensive assessment. Evaluated on our customized Proverbs Disparity Dataset (PDD) with controlled attribute toggles (e.g., gender bias, misinformation susceptibility), TFDP precisely detects disparities while achieving up to 42 times fewer output tokens than minimal n-token continuations, offering a scalable tool for responsible LLM evaluation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Inderjeet Singh",
      "Ramya Srinivasan",
      "Roman Vainshtein",
      "Hisashi Kojima"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1251": {
    "title": "Hanfu-Bench: A Multimodal Benchmark on Cross-Temporal Cultural Understanding and Transcreation",
    "volume": "main",
    "abstract": "Culture is a rich and dynamic domain that evolves across both geography and time. However, existing studies on cultural understanding with vision-language models (VLMs) primarily emphasize geographic diversity, often overlooking the critical temporal dimensions. To bridge this gap, we introduce Hanfu-Bench, a novel, expert-curated multimodal dataset. Hanfu, a traditional garment spanning ancient Chinese dynasties, serves as a representative cultural heritage that reflects the profound temporal aspects of Chinese culture while remaining highly popular in Chinese contemporary society. Hanfu-Bench comprises two core tasks: cultural visual understanding and cultural image transcreation. The former task examines temporal-cultural feature recognition based on single- or multi-image inputs through multiple-choice visual question answering, while the latter focuses on transforming traditional attire into modern designs through cultural element inheritance and modern context adaptation. Our evaluation shows that closed VLMs perform comparably to non-experts on visual cutural understanding but fall short by 10% to human experts, while open VLMs lags further behind non-experts. For the transcreation task, multi-faceted human evaluation indicates that the best-performing model achieves a success rate of only 42%. Our benchmark provides an essential testbed, revealing significant challenges in this new direction of temporal cultural understanding and creative adaptation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li Zhou",
      "Lutong Yu",
      "Dongchu Xie",
      "Shaohuan Cheng",
      "Wenyan Li",
      "Haizhou Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1252": {
    "title": "MERMAID: Multi-perspective Self-reflective Agents with Generative Augmentation for Emotion Recognition",
    "volume": "main",
    "abstract": "Multimodal large language models (MLLMs) have demonstrated strong performance across diverse multimodal tasks, achieving promising outcomes. However, their application to emotion recognition in natural images remains underexplored. MLLMs struggle to handle ambiguous emotional expressions and implicit affective cues, whose capability is crucial for affective understanding but largely overlooked. To address these challenges, we propose MERMAID, a novel multi-agent framework that integrates a multi-perspective self-reflection module, an emotion-guided visual augmentation module, and a cross-modal verification module. These components enable agents to interact across modalities and reinforce subtle emotional semantics, thereby enhancing emotion recognition and supporting autonomous performance. Extensive experiments show that MERMAID outperforms existing methods, achieving absolute accuracy gains of 8.70%–27.90% across diverse benchmarks and exhibiting greater robustness in emotionally diverse scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongyu Yang",
      "Junhao Song",
      "Siyang Song",
      "Wei Pang",
      "Yingfang Yuan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1253": {
    "title": "Personality Vector: Modulating Personality of Large Language Models by Model Merging",
    "volume": "main",
    "abstract": "Driven by the demand for personalized AI systems, there is growing interest in aligning the behavior of large language models (LLMs) with human traits such as personality. Previous attempts to induce personality in LLMs have shown promising results, but they struggle to capture the continuous and multidimensional nature of human traits. In this work, we propose a novel method for personality modulation in LLMs via model merging. Specifically, we construct personality vectors by subtracting the weights of a pre-trained model from those of the fine-tuned model on a given personality trait. By merging personality vectors, we enable LLMs to exhibit desired personality traits without additional training. Extensive experiments show that personality vectors enable continuous control over trait intensity and support the composition of multiple traits. Furthermore, personality vectors transfer across diverse downstream models, suggesting that they encode generalizable representations of personality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seungjong Sun",
      "Seo Yeon Baek",
      "Jang Hyun Kim"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1254": {
    "title": "Beyond Outlining: Heterogeneous Recursive Planning for Adaptive Long-form Writing with Language Models",
    "volume": "main",
    "abstract": "Long-form writing agents require flexible integration and interaction across information retrieval, reasoning, and composition. Current approaches rely on predefined workflows and rigid thinking patterns to generate outlines before writing, resulting in constrained adaptability during writing. In this paper we propose WriteHERE, a general agent framework that achieves human-like adaptive writing through recursive task decomposition and dynamic integration of three fundamental task types: retrieval, reasoning, and composition. Our methodology features: 1) a planning mechanism that interleaves recursive task decomposition and execution, eliminating artificial restrictions on writing workflow; and 2) integration of task types that facilitates heterogeneous task decomposition. Evaluations on both fiction writing and technical report generation show that our method consistently outperforms state-of-the-art approaches across all automatic evaluation metrics, demonstrating the effectiveness and broad applicability of our proposed framework. We have publicly released our code and prompts to facilitate further research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruibin Xiong",
      "Yimeng Chen",
      "Dmitrii Khizbullin",
      "Mingchen Zhuge",
      "Jürgen Schmidhuber"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1255": {
    "title": "Hidden in Plain Sight: Reasoning in Underspecified and Misspecified Scenarios for Multimodal LLMs",
    "volume": "main",
    "abstract": "Multimodal large language models (MLLMs) are increasingly deployed in open-ended, real-world environments where inputs are messy, underspecified, and not always trustworthy. Unlike curated benchmarks, these settings frequently involve instructions that reference missing objects or contradictory facts, rely on ambiguous cues, or request infeasible actions. In such cases, success hinges not merely on task execution, but on the model's ability to detect when something is silently wrong. This paper presents a systematic analysis of how current MLLMs handle such underspecified and misspecified scenarios: cases where flaws must be inferred from context rather than explicitly stated. Using a curated diagnostic suite spanning four categories of real-world failure modes, we evaluate nine MLLMs, including o3 and GPT-4o, and find that models often fail to surface hidden issues, even when they possess the necessary perceptual and reasoning skills. Explicit prompting reveals that the underlying capabilities exist but are frequently suppressed in favor of user compliance.We further show that simple inference-time interventions, such as cautious persona prompting and, in particular, requiring a clarifying question, can substantially recover performance. Our findings highlight a persistent gap between reasoning competence and behavioral compliance in current MLLMs, and suggest practical strategies for making these systems more trustworthy in underconstrained environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qianqi Yan",
      "Hongquan Li",
      "Shan Jiang",
      "Yang Zhao",
      "Xinze Guan",
      "Ching-Chen Kuo",
      "Xin Eric Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1256": {
    "title": "PrimeX: A Dataset of Worldview, Opinion, and Explanation",
    "volume": "main",
    "abstract": "As the adoption of language models advances, so does the need to better represent individual users to the model. Are there aspects of an individual's belief system that a language model can utilize for improved alignment? Following prior research, we investigate this question in the domain of opinion prediction by developing PrimeX, a dataset of public opinion survey data from 858 US residents with two additional sources of belief information: written explanations from the respondents for why they hold specific opinions, and the Primal World Belief survey for assessing respondent worldview. We provide an extensive initial analysis of our data and show the value of belief explanations and worldview for personalizing language models. Our results demonstrate how the additional belief information in PrimeX can benefit both the NLP and psychological research communities, opening up avenues for further study",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rik Koncel-Kedziorski",
      "Brihi Joshi",
      "Tim Paek"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1257": {
    "title": "LASER: An LLM-based ASR Scoring and Evaluation Rubric",
    "volume": "main",
    "abstract": "Standard ASR evaluation metrics like Word Error Rate (WER) tend to unfairly penalize morphological and syntactic nuances that do not significantly alter sentence semantics. We introduce an LLM-based scoring rubric LASER that leverages state-of-the-art LLMs' in-context learning abilities to learn from prompts with detailed examples. Hindi LASER scores using Gemini 2.5 Pro achieved a very high correlation score of 94% with human annotations. Hindi examples in the prompt were also effective in analyzing errors in other Indian languages such as Marathi, Kannada and Malayalam. We also demonstrate how a smaller LLM like Llama 3 can be finetuned on word-pair examples derived from reference and ASR predictions to predict what kind of penalty should be applied with close to 89% accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amruta Parulekar",
      "Preethi Jyothi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1258": {
    "title": "Improving Zero-shot Sentence Decontextualisation with Content Selection and Planning",
    "volume": "main",
    "abstract": "Extracting individual sentences from a document as evidence or reasoning steps is commonly done in many NLP tasks. However, extracted sentences often lack context necessary to make them understood, e.g., coreference and background information. To this end, we propose a content selection and planning framework for zero-shot decontextualisation, which determines what content should be mentioned and in what order for a sentence to be understood out of context. Specifically, given a potentially ambiguous sentence and its context, we first segment it into basic semantically-independent units. We then identify potentially ambiguous units from the given sentence, and extract relevant units from the context based on their discourse relations. Finally, we generate a content plan to rewrite the sentence by enriching each ambiguous unit with its relevant units. Experimental results demonstrate that our approach is competitive for sentence decontextualisation, producing sentences that exhibit better semantic integrity and discourse coherence, outperforming existing methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenyun Deng",
      "Yulong Chen",
      "Andreas Vlachos"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1259": {
    "title": "Beyond Text: Unveiling Privacy Vulnerabilities in Multi-modal Retrieval-Augmented Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiankun Zhang",
      "Shenglai Zeng",
      "Jie Ren",
      "Tianqi Zheng",
      "Hui Liu",
      "Xianfeng Tang",
      "Hui Liu",
      "Yi Chang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1260": {
    "title": "Code Execution as Grounded Supervision for LLM Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongwon Jung",
      "Wenxuan Zhou",
      "Muhao Chen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1261": {
    "title": "Subjective Behaviors and Preferences in LLM: Language of Browsing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sai Sundaresan",
      "Harshita Chopra",
      "Atanu R. Sinha",
      "Koustava Goswami",
      "Nagasai Saketh Naidu",
      "Raghav Karan",
      "N Anushka"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1262": {
    "title": "Pixels Versus Priors: Controlling Knowledge Priors in Vision-Language Models through Visual Counterfacts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michal Golovanevsky",
      "William Rudman",
      "Michael A. Lepori",
      "Amir Bar",
      "Ritambhara Singh",
      "Carsten Eickhoff"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1263": {
    "title": "Balcony: A Lightweight Approach to Dynamic Inference of Generative Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benyamin Jamialahmadi",
      "Parsa Kavehzadeh",
      "Mehdi Rezagholizadeh",
      "Parsa Farinneya",
      "Hossein Rajabzadeh",
      "Aref Jafari",
      "Boxing Chen",
      "Marzieh S. Tahaei"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1264": {
    "title": "Social Genome: Grounded Social Reasoning Abilities of Multimodal Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leena Mathur",
      "Marian Qian",
      "Paul Pu Liang",
      "Louis-Philippe Morency"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1265": {
    "title": "Profiler: Black-box AI-generated Text Origin Detection via Context-aware Inference Pattern Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanxi Guo",
      "Siyuan Cheng",
      "Xiaolong Jin",
      "Zhuo Zhang",
      "Guangyu Shen",
      "Kaiyuan Zhang",
      "Shengwei An",
      "Guanhong Tao",
      "Xiangyu Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1266": {
    "title": "Speech Discrete Tokens or Continuous Features? A Comparative Analysis for Spoken Language Understanding in SpeechLLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dingdong Wang",
      "Junan Li",
      "Mingyu Cui",
      "Dongchao Yang",
      "Xueyuan Chen",
      "Helen M. Meng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1267": {
    "title": "RAG-Zeval: Enhancing RAG Responses Evaluator through End-to-End Reasoning and Ranking-Based Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kun Li",
      "Yunxiang Li",
      "Tianhua Zhang",
      "Hongyin Luo",
      "Xixin Wu",
      "James R. Glass",
      "Helen M. Meng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1268": {
    "title": "Infini-gram mini: Exact n-gram Search at the Internet Scale with FM-Index",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Xu",
      "Jiacheng Liu",
      "Yejin Choi",
      "Noah A. Smith",
      "Hannaneh Hajishirzi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1269": {
    "title": "Mahānāma: A Unique Testbed for Literary Entity Discovery and Linking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sujoy Sarkar",
      "Gourav Sarkar",
      "Manoj Balaji Jagadeeshan",
      "Jivnesh Sandhan",
      "Amrith Krishna",
      "Pawan Goyal"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1270": {
    "title": "Adaptively profiling models with task elicitation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Davis Brown",
      "Prithvi Balehannina",
      "Helen Jin",
      "Shreya Havaldar",
      "Hamed Hassani",
      "Eric Wong"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1271": {
    "title": "Causal Interventions Reveal Shared Structure Across English Filler–Gap Constructions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sasha Boguraev",
      "Christopher Potts",
      "Kyle Mahowald"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1272": {
    "title": "TactfulToM: Do LLMs have the Theory of Mind ability to understand White Lies?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiwei Liu",
      "Emma Jane Pretty",
      "Jiahao Huang",
      "Saku Sugawara"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1273": {
    "title": "Don't Sweat the Small Stuff: Segment-Level Meta-Evaluation Based on Pairwise Difference Correlation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Colten DiIanni",
      "Daniel Deutsch"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1274": {
    "title": "SMART: Simulated Students Aligned with Item Response Theory for Question Difficulty Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Scarlatos",
      "Nigel Fernandez",
      "Christopher Ormerod",
      "Susan Lottridge",
      "Andrew Lan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1275": {
    "title": "HESEIA: A community-based dataset for evaluating social biases in large language models, co-designed in real school settings in Latin America",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guido Ivetta",
      "Marcos J Gomez",
      "Sofía Martinelli",
      "Pietro Palombini",
      "M Emilia Echeveste",
      "Nair Carolina Mazzeo",
      "Beatriz Busaniche",
      "Luciana Benotti"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1276": {
    "title": "WebMMU: A Benchmark for Multimodal Multilingual Website Understanding and Code Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rabiul Awal",
      "Mahsa Massoud",
      "Aarash Feizi",
      "Zichao Li",
      "Suyuchen Wang",
      "Christopher Pal",
      "Aishwarya Agrawal",
      "David Vazquez",
      "Siva Reddy",
      "Juan A. Rodriguez",
      "Perouz Taslakian",
      "Spandana Gella",
      "Sai Rajeswar"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1277": {
    "title": "Analyzing values about gendered language reform in LLMs' revisions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jules Watson",
      "Xi Wang",
      "Raymond Liu",
      "Suzanne Stevenson",
      "Barend Beekhuizen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1278": {
    "title": "ALLabel: Three-stage Active Learning for LLM-based Entity Recognition using Demonstration Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihan Chen",
      "Lei Shi",
      "Weize Wu",
      "Qiji Zhou",
      "Yue Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1279": {
    "title": "HyperKGR: Knowledge Graph Reasoning in Hyperbolic Space with Graph Neural Network Encoding Symbolic Path",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lihui Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1280": {
    "title": "LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Chiang",
      "Elvis Hsieh",
      "Chia-Hong Chou",
      "Janosh Riebesell"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1281": {
    "title": "ReSeeding Latent States for Sequential Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stéphane Aroca-Ouellette",
      "Katharina von der Wense",
      "Alessandro Roncone"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1282": {
    "title": "DPED: Multi-Layer Noise Distillation for Privacy-Preserving Text Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuya Feng",
      "Yuan Hong"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1283": {
    "title": "Identifying & Interactively Refining Ambiguous User Goals for Data Visualization Code Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mert Inan",
      "Anthony Sicilia",
      "Alex Xie",
      "Saujas Vaduguru",
      "Daniel Fried",
      "Malihe Alikhani"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1284": {
    "title": "Morpheme Induction for Emergent Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brendon Boldt",
      "David R. Mortensen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1285": {
    "title": "Stepwise Informativeness Search for Improving LLM Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyuan Wang",
      "Enda Zhao",
      "Xiang Ren"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1286": {
    "title": "Social Good or Scientific Curiosity? Uncovering the Research Framing Behind NLP Artefacts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eric Chamoun",
      "Nedjma Ousidhoum",
      "Michael Sejr Schlichtkrull",
      "Andreas Vlachos"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1287": {
    "title": "FairGen: Controlling Sensitive Attributes for Fair Generations in Diffusion Models via Adaptive Latent Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mintong Kang",
      "Vinayshekhar Bannihatti Kumar",
      "Shamik Roy",
      "Abhishek Kumar",
      "Sopan Khosla",
      "Balakrishnan Murali Narayanaswamy",
      "Rashmi Gangadharaiah"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1288": {
    "title": "Contra4: Evaluating Contrastive Cross-Modal Reasoning in Audio, Video, Image, and 3D",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Artemis Panagopoulou",
      "Le Xue",
      "Honglu Zhou",
      "Silvio Savarese",
      "Ran Xu",
      "Caiming Xiong",
      "Chris Callison-Burch",
      "Mark Yatskar",
      "Juan Carlos Niebles"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1289": {
    "title": "Proactive Hearing Assistants that Isolate Egocentric Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guilin Hu",
      "Malek Itani",
      "Tuochao Chen",
      "Shyamnath Gollakota"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1290": {
    "title": "fLSA: Learning Semantic Structures in Document Collections Using Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weijia Xu",
      "Nebojsa Jojic",
      "Nicolas Le Roux"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1291": {
    "title": "SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiwen Zhou",
      "Xuandong Zhao",
      "Jayanth Srinivasa",
      "Gaowen Liu",
      "Aosong Feng",
      "Dawn Song",
      "Xin Eric Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1292": {
    "title": "HypER: Literature-grounded Hypothesis Generation and Distillation with Provenance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rosni Vasu",
      "Chandrayee Basu",
      "Bhavana Dalvi Mishra",
      "Cristina Sarasua",
      "Peter Clark",
      "Abraham Bernstein"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1293": {
    "title": "Empowering GraphRAG with Knowledge Filtering and Integration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Guo",
      "Harry Shomer",
      "Shenglai Zeng",
      "Haoyu Han",
      "Yu Wang",
      "Jiliang Tang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1294": {
    "title": "Interpretable Mnemonic Generation for Kanji Learning via Expectation-Maximization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaewook Lee",
      "Alexander Scarlatos",
      "Andrew Lan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1295": {
    "title": "Refining Attention for Explainable and Noise-Robust Fact-Checking with Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jean-Flavien Bussotti",
      "Paolo Papotti"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1296": {
    "title": "Harmful Prompt Laundering: Jailbreaking LLMs with Abductive Styles and Symbolic Encoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seongho Joo",
      "Hyukhun Koh",
      "Kyomin Jung"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1297": {
    "title": "Pathway to Relevance: How Cross-Encoders Implement a Semantic Variant of BM25",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meng Lu",
      "Catherine Chen",
      "Carsten Eickhoff"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1298": {
    "title": "Rewarding the Unlikely: Lifting GRPO Beyond Distribution Sharpening",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andre Wang He",
      "Daniel Fried",
      "Sean Welleck"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1299": {
    "title": "PhoniTale: Phonologically Grounded Mnemonic Generation for Typologically Distant Language Pairs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sana Kang",
      "Myeongseok Gwon",
      "Su Young Kwon",
      "Jaewook Lee",
      "Andrew Lan",
      "Bhiksha Raj",
      "Rita Singh"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1300": {
    "title": "Amulet: Putting Complex Multi-Turn Conversations on the Stand with LLM Juries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sahana Ramnath",
      "Anurag Mudgil",
      "Brihi Joshi",
      "Skyler Hallinan",
      "Xiang Ren"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1301": {
    "title": "Exploring Chain-of-Thought Reasoning for Steerable Pluralistic Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunfan Zhang",
      "Kathleen McKeown",
      "Smaranda Muresan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1302": {
    "title": "CMedCalc-Bench: A Fine-Grained Benchmark for Chinese Medical Calculations in LLM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunyan Zhang",
      "Zhihong Zhu",
      "Xian Wu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1303": {
    "title": "Evaluating Robustness of Large Audio Language Models to Audio Injection: An Empirical Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanyu Hou",
      "Jiaming He",
      "Yinhang Zhou",
      "Ji Guo",
      "Yitong Qiao",
      "Rui Zhang",
      "Wenbo Jiang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1304": {
    "title": "How Far Can LLMs Improve from Experience? Measuring Test-Time Learning Ability in LLMs with Human Comparison",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayin Wang",
      "Zhiqiang Guo",
      "Weizhi Ma",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1305": {
    "title": "Subtle Risks, Critical Failures: A Framework for Diagnosing Physical Safety of LLMs for Embodied Decision Making",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yejin Son",
      "Minseo Kim",
      "Sungwoong Kim",
      "Seungju Han",
      "Jian Kim",
      "Dongju Jang",
      "Youngjae Yu",
      "Chan Young Park"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1306": {
    "title": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving Model Transformation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aurick Qiao",
      "Zhewei Yao",
      "Samyam Rajbhandari",
      "Yuxiong He"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1307": {
    "title": "Co-Eval: Augmenting LLM-based Evaluation with Machine Metrics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ling-I Wu",
      "Weijie Wu",
      "Minyu Chen",
      "Jianxin Xue",
      "Guoqiang Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1308": {
    "title": "Sali4Vid: Saliency-Aware Video Reweighting and Adaptive Caption Retrieval for Dense Video Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "MinJu Jeon",
      "Si-Woo Kim",
      "Ye-Chan Kim",
      "HyunGee Kim",
      "Dong-Jin Kim"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1309": {
    "title": "Semantic Networks Extracted from Students' Think-Aloud Data are Correlated with Students' Learning Performance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pingjing Yang",
      "Sullam Jeoung",
      "Jennifer Cromley",
      "Jana Diesner"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1310": {
    "title": "Less is More: The Effectiveness of Compact Typological Language Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "York Hay Ng",
      "Phuong Hanh Hoang",
      "En-Shiun Annie Lee"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1311": {
    "title": "Sparse Activation Editing for Reliable Instruction Following in Narratives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runcong Zhao",
      "Chengyu Cao",
      "Qinglin Zhu",
      "Xiucheng Ly",
      "Shun Shao",
      "Lin Gui",
      "Ruifeng Xu",
      "Yulan He"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1312": {
    "title": "Inceptive Transformers: Enhancing Contextual Representations through Multi-Scale Feature Learning Across Domains and Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Asif Shahriar",
      "Rifat Shahriyar",
      "M Saifur Rahman"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1313": {
    "title": "Causal Tree Extraction from Medical Case Reports: A Novel Task for Experts-like Text Comprehension",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sakiko Yahata",
      "Zhen Wan",
      "Fei Cheng",
      "Sadao Kurohashi",
      "Hisahiko Sato",
      "Ryozo Nagai"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1314": {
    "title": "OWL: Probing Cross-Lingual Recall of Memorized Texts via World Literature",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alisha Srivastava",
      "Emir Kaan Korukluoglu",
      "Minh Nhat Le",
      "Duyen Tran",
      "Chau Minh Pham",
      "Marzena Karpinska",
      "Mohit Iyyer"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1315": {
    "title": "Enhanced Noun-Noun Compound Interpretation through Textual Enrichment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bingyang Ye",
      "Jingxuan Tu",
      "James Pustejovsky"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1316": {
    "title": "ICL CIPHERS: Quantifying \"Learning\" in In-Context Learning via Substitution Ciphers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhouxiang Fang",
      "Aayush Mishra",
      "Muhan Gao",
      "Anqi Liu",
      "Daniel Khashabi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1317": {
    "title": "Corrupted but Not Broken: Understanding and Mitigating the Negative Impacts of Corrupted Data in Visual Instruction Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunhao Gou",
      "Hansi Yang",
      "Zhili Liu",
      "Kai Chen",
      "Yihan Zeng",
      "Lanqing Hong",
      "Zhenguo Li",
      "Qun Liu",
      "Bo Han",
      "James Kwok",
      "Yu Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1318": {
    "title": "Memory OS of AI Agent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiazheng Kang",
      "Mingming Ji",
      "Zhe Zhao",
      "Ting Bai"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1319": {
    "title": "Rule Discovery for Natural Language Inference Data Generation Using Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juyoung Han",
      "Hyunsun Hwang",
      "Changki Lee"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1320": {
    "title": "Jigsaw-Puzzles: From Seeing to Understanding to Reasoning in Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zesen Lyu",
      "Dandan Zhang",
      "Wei Ye",
      "Fangdi Li",
      "Zhihang Jiang",
      "Yao Yang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1321": {
    "title": "Definition Generation for Word Meaning Modeling: Monolingual, Multilingual, and Cross-Lingual Perspectives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesco Periti",
      "Roksana Goworek",
      "Haim Dubossarsky",
      "Nina Tahmasebi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1322": {
    "title": "Language Model Based Text-to-Audio Generation: Anti-Causally Aligned Collaborative Residual Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juncheng Wang",
      "Chao Xu",
      "Cheng Yu",
      "Zhe Hu",
      "Haoyu Xie",
      "Guoqi Yu",
      "Lei Shang",
      "Shujun Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1323": {
    "title": "HELENE: Hessian Layer-wise Clipping and Gradient Annealing for Accelerating Fine-tuning LLM with Zeroth-order Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huaqin Zhao",
      "Jiaxi Li",
      "Yi Pan",
      "Shizhe Liang",
      "Xiaofeng Yang",
      "Fei Dou",
      "Tianming Liu",
      "Jin Lu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1324": {
    "title": "Zero-shot Multimodal Document Retrieval via Cross-modal Question Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yejin Choi",
      "Jaewoo Park",
      "Janghan Yoon",
      "Saejin Kim",
      "Jaehyun Jeon",
      "Youngjae Yu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1325": {
    "title": "From Parameters to Performance: A Data-Driven Study on LLM Structure and Development",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suqing Wang",
      "Zuchao Li",
      "Shi Luohe",
      "Bo Du",
      "Hai Zhao",
      "Yun Li",
      "Qianren Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1326": {
    "title": "Logical Reasoning with Outcome Reward Models for Test-Time Scaling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ramya Keerthy Thatikonda",
      "Wray Buntine",
      "Ehsan Shareghi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1327": {
    "title": "Speculating LLMs' Chinese Training Data Pollution from Their Tokens",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingjie Zhang",
      "Di Wang",
      "Haoting Qian",
      "Liu Yan",
      "Tianwei Zhang",
      "Ke Xu",
      "Qi Li",
      "Minlie Huang",
      "Hewu Li",
      "Han Qiu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1328": {
    "title": "NovelHopQA: Diagnosing Multi-Hop Reasoning Failures in Long Narrative Contexts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhay Gupta",
      "Kevin Zhu",
      "Vasu Sharma",
      "Sean O’Brien",
      "Michael Lu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1329": {
    "title": "Weights-Rotated Preference Optimization for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenxu Yang",
      "Ruipeng Jia",
      "Mingyu Zheng",
      "Naibin Gu",
      "Zheng Lin",
      "Siyuan Chen",
      "Weichong Yin",
      "Hua Wu",
      "Weiping Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1330": {
    "title": "The Stepwise Deception: Simulating the Evolution from True News to Fake News with LLM Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhan Liu",
      "Zirui Song",
      "Juntian Zhang",
      "Xiaoqing Zhang",
      "Xiuying Chen",
      "Rui Yan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1331": {
    "title": "How to inject knowledge efficiently? Knowledge Infusion Scaling Law for Pre-training Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kangtao Lv",
      "Haibin Chen",
      "Yujin Yuan",
      "Langming Liu",
      "Shilei Liu",
      "Yongwei Wang",
      "Wenbo Su",
      "Bo Zheng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1332": {
    "title": "SMEC:Rethinking Matryoshka Representation Learning for Retrieval Embedding Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Biao Zhang",
      "Lixin Chen",
      "Tong Liu",
      "Bo Zheng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1333": {
    "title": "Reverse Prompt Engineering: A Zero-Shot, Genetic Algorithm Approach to Language Model Inversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanqing Li",
      "Diego Klabjan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1334": {
    "title": "DiMo-GUI: Advancing Test-time Scaling in GUI Grounding via Modality-Aware Visual Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hang Wu",
      "Hongkai Chen",
      "Yujun Cai",
      "Chang Liu",
      "Qingwen Ye",
      "Ming-Hsuan Yang",
      "Yiwei Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1335": {
    "title": "SocioBench: Modeling Human Behavior in Sociological Surveys with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jia Wang",
      "Ziyu Zhao",
      "Tingjuntao Ni",
      "Zhongyu Wei"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1336": {
    "title": "Financial Risk Relation Identification through Dual-view Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei-Ning Chiu",
      "Yu-Hsiang Wang",
      "Andy Hsiao",
      "Yu-Shiang Huang",
      "Chuan-Ju Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1337": {
    "title": "CopySpec: Accelerating LLMs with Speculative Copy-and-Paste",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Razvan-Gabriel Dumitru",
      "Minglai Yang",
      "Vikas Yadav",
      "Mihai Surdeanu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1338": {
    "title": "GRASP: Replace Redundant Layers with Adaptive Singular Parameters for Efficient Model Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kainan Liu",
      "Yong Zhang",
      "Ning Cheng",
      "Zhitao Li",
      "Shaojun Wang",
      "Jing Xiao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1339": {
    "title": "GraphAgent: Agentic Graph Language Assistant",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhao Yang",
      "Jiabin Tang",
      "Lianghao Xia",
      "Xingchen Zou",
      "Yuxuan Liang",
      "Chao Huang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1340": {
    "title": "DDO: Dual-Decision Optimization for LLM-Based Medical Consultation via Multi-Agent Collaboration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihao Jia",
      "Mingyi Jia",
      "Junwen Duan",
      "Jianxin Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1341": {
    "title": "FedMABench: Benchmarking Mobile GUI Agents on Decentralized Heterogeneous User Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "WenHao Wang",
      "Zijie Yu",
      "Rui Ye",
      "Jianqing Zhang",
      "Guangyi Liu",
      "Liang Liu",
      "Siheng Chen",
      "Yanfeng Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1342": {
    "title": "VLA-Mark: A cross modal watermark for large vision-language alignment models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuliang Liu",
      "Zheng Qi",
      "Jesse Jiaxi Xu",
      "Yibo Yan",
      "Junyan Zhang",
      "He Geng",
      "Aiwei Liu",
      "Peijie Jiang",
      "Jia Liu",
      "Yik-Cheung Tam",
      "Xuming Hu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1343": {
    "title": "Sentence Smith: Controllable Edits for Evaluating Text Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongji Li",
      "Andrianos Michail",
      "Reto Gubelmann",
      "Simon Clematide",
      "Juri Opitz"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1344": {
    "title": "ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Sun",
      "Xingyu Qian",
      "Weiwen Xu",
      "Hao Zhang",
      "Chenghao Xiao",
      "Long Li",
      "Deli Zhao",
      "Wenbing Huang",
      "Tingyang Xu",
      "Qifeng Bai",
      "Yu Rong"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1345": {
    "title": "Decoding Dense Embeddings: Sparse Autoencoders for Interpreting and Discretizing Dense Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seongwan Park",
      "Taeklim Kim",
      "Youngjoong Ko"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1346": {
    "title": "UICOMPASS: UI Map Guided Mobile Task Automation via Adaptive Action Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanzhang Lin",
      "Zhe Zhang",
      "He Rui",
      "Qingao Dong",
      "Mingyi Zhou",
      "Jing Zhang",
      "Xiang Gao",
      "Hailong Sun"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1347": {
    "title": "Leaky Thoughts: Large Reasoning Models Are Not Private Thinkers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tommaso Green",
      "Martin Gubri",
      "Haritz Puerto",
      "Sangdoo Yun",
      "Seong Joon Oh"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1348": {
    "title": "Model Unlearning via Sparse Autoencoder Subspace Guided Projections",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xu Wang",
      "Zihao Li",
      "Benyou Wang",
      "Yan Hu",
      "Difan Zou"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1349": {
    "title": "ConvSearch-R1: Enhancing Query Reformulation for Conversational Search with Reasoning via Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changtai Zhu",
      "Siyin Wang",
      "Ruijun Feng",
      "Kai Song",
      "Xipeng Qiu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1350": {
    "title": "How to Make Large Language Models Generate 100% Valid Molecules?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wen Tao",
      "Jing Tang",
      "Alvin Chan",
      "Bryan Hooi",
      "Baolong Bi",
      "Nanyun Peng",
      "Yuansheng Liu",
      "Yiwei Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1351": {
    "title": "Exploring Quality and Diversity in Synthetic Data Generation for Argument Mining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianzhu Bao",
      "Yuqi Huang",
      "Yang Sun",
      "Wenya Wang",
      "Yice Zhang",
      "Bojun Jin",
      "Ruifeng Xu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1352": {
    "title": "Dynamic Jointly Batch Selection for Data Efficient Machine Translation Fine-Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Amin Ghanizadeh",
      "Mohammad Javad Dousti"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1353": {
    "title": "3MDBench: Medical Multimodal Multi-agent Dialogue Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ivan Sviridov",
      "Amina Miftakhova",
      "Artemiy Tereshchenko",
      "Galina Zubkova",
      "Pavel Blinov",
      "Andrey Savchenko"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1354": {
    "title": "OpenTuringBench: An Open-Model-based Benchmark and Framework for Machine-Generated Text Detection and Attribution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucio La Cava",
      "Andrea Tagarelli"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1355": {
    "title": "CRITICTOOL: Evaluating Self-Critique Capabilities of Large Language Models in Tool-Calling Error Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiting Huang",
      "Zhen Fang",
      "Zehui Chen",
      "Siyu Yuan",
      "Junjie Ye",
      "Yu Zeng",
      "Lin Chen",
      "Qi Mao",
      "Feng Zhao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1356": {
    "title": "Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marek Kadlčík",
      "Michal Štefánik",
      "Timothee Mickus",
      "Josef Kuchař",
      "Michal Spiegel"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1357": {
    "title": "Enhancing Large Vision-Language Models with Ultra-Detailed Image Caption Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Zeng",
      "Yukun Qi",
      "Yiming Zhao",
      "Xikun Bao",
      "Lin Chen",
      "Zehui Chen",
      "Shiting Huang",
      "Jie Zhao",
      "Feng Zhao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1358": {
    "title": "Translate Smart, not Hard: Cascaded Translation Systems with Quality-Aware Deferral",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "António Farinhas",
      "Nuno M Guerreiro",
      "Sweta Agrawal",
      "Ricardo Rei",
      "Andre Martins"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1359": {
    "title": "iVISPAR — An Interactive Visual-Spatial Reasoning Benchmark for VLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julius Mayer",
      "Mohamad Ballout",
      "Serwan Jassim",
      "Farbod Nosrat Nezami",
      "Elia Bruni"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1360": {
    "title": "Are LLMs Better than Reported? Detecting Label Errors and Mitigating Their Effect on Model Performance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Omer Nahum",
      "Nitay Calderon",
      "Orgad Keller",
      "Idan Szpektor",
      "Roi Reichart"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1361": {
    "title": "Detecting Legal Citations in United Kingdom Court Judgments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Holli Sargeant",
      "Andreas Östling",
      "Måns Magnusson"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1362": {
    "title": "Large Language Models Badly Generalize across Option Length, Problem Types, and Irrelevant Noun Replacements",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangxiang Zhao",
      "Saier Hu",
      "Xiaoqi Jian",
      "Wu Jinzhu",
      "Yuhan Wu",
      "Lin Sun",
      "Xiangzheng Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1363": {
    "title": "Studying the Role of Input-Neighbor Overlap in Retrieval-Augmented Language Models Training Efficiency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ehsan Doostmohammadi",
      "Marco Kuhlmann"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1364": {
    "title": "Principled Personas: Defining and Measuring the Intended Effects of Persona Prompting on Task Performance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pedro Henrique Luz de Araujo",
      "Paul Röttger",
      "Dirk Hovy",
      "Benjamin Roth"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1365": {
    "title": "HydraOpt: Navigating the Efficiency-Performance Trade-off of Adapter Merging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taha Ceritli",
      "Ondrej Bohdal",
      "Mete Ozay",
      "Jijoong Moon",
      "Kyenghun Lee",
      "Hyeonmok Ko",
      "Umberto Michieli"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1366": {
    "title": "Parrot: A Training Pipeline Enhances Both Program CoT and Natural Language CoT for Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Senjie Jin",
      "Lu Chen",
      "Zhiheng Xi",
      "Yuhui Wang",
      "Sirui Song",
      "Yuhao Zhou",
      "Xinbo Zhang",
      "Peng Sun",
      "Hong Lu",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1367": {
    "title": "Spec-VLA: Speculative Decoding for Vision-Language-Action Models with Relaxed Acceptance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Songsheng Wang",
      "Rucheng Yu",
      "Zhihang Yuan",
      "Chao Yu",
      "Feng Gao",
      "Yu Wang",
      "Derek F. Wong"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1368": {
    "title": "Leveraging Text-to-Text Transformers as Classifier Chain for Few-Shot Multi-Label Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quang Anh Nguyen",
      "Nadi Tomeh",
      "Mustapha Lebbah",
      "Thierry Charnois",
      "Hanane Azzag"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1369": {
    "title": "M-Wanda: Improving One-Shot Pruning for Multilingual LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rochelle Choenni",
      "Ivan Titov"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1370": {
    "title": "Beyond Hate Speech: NLP's Challenges and Opportunities in Uncovering Dehumanizing Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hamidreza Saffari",
      "Mohammadamin Shafiei",
      "Hezhao Zhang",
      "Lasana T. Harris",
      "Nafise Sadat Moosavi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1371": {
    "title": "Conflict-Aware Soft Prompting for Retrieval-Augmented Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eunseong Choi",
      "June Park",
      "Hyeri Lee",
      "Jongwuk Lee"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1372": {
    "title": "R-CHAR: A Metacognition-Driven Framework for Role-Playing in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiming Qin",
      "Jiwei Zhang",
      "Wei Zhang",
      "KeZhong Lu",
      "Mingyang Zhou",
      "Hao Liao",
      "Rui Mao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1373": {
    "title": "Annotating Training Data for Conditional Semantic Textual Similarity Measurement using Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaifan Zhang",
      "Yi Zhou",
      "Danushka Bollegala"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1374": {
    "title": "When Words Smile: Generating Diverse Emotional Facial Expressions from Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haidong Xu",
      "Meishan Zhang",
      "Hao Ju",
      "Zhedong Zheng",
      "Erik Cambria",
      "Min Zhang",
      "Hao Fei"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1375": {
    "title": "Improving Online Job Advertisement Analysis via Compositional Entity Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Krüger",
      "Johanna Binnewitt",
      "Kathrin Ehmann",
      "Stefan Winnige",
      "Alan Akbik"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1376": {
    "title": "Correlation-Aware Example Selection for In-Context Learning with Nonsymmetric Determinantal Point Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiunan Du",
      "Zhiliang Tian",
      "Zhen Huang",
      "Kailun Bian",
      "Tianlun Liu",
      "Zhaoning Zhang",
      "Xinwang Liu",
      "Feng Liu",
      "Dongsheng Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1377": {
    "title": "Leveraging Cognitive Complexity of Texts for Contextualization in Dense Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Effrosyni Sokli",
      "Georgios Peikos",
      "Pranav Kasela",
      "Gabriella Pasi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1378": {
    "title": "Beyond Online Sampling: Bridging Offline-to-Online Alignment via Dynamic Data Transformation for LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhang Zhang",
      "Guhao Feng",
      "Jian Guan",
      "Di He",
      "Wei Wu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1379": {
    "title": "CAVE : Detecting and Explaining Commonsense Anomalies in Visual Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rishika Bhagwatkar",
      "Syrielle Montariol",
      "Angelika Romanou",
      "Beatriz Borges",
      "Irina Rish",
      "Antoine Bosselut"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1380": {
    "title": "Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linjuan Wu",
      "Hao-Ran Wei",
      "Huan Lin",
      "Tianhao Li",
      "Baosong Yang",
      "Fei Huang",
      "Weiming Lu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1381": {
    "title": "SemVink: Advancing VLMs' Semantic Understanding of Optical Illusions via Visual Global Thinking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sifan Li",
      "Yujun Cai",
      "Yiwei Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1382": {
    "title": "Order Doesn't Matter, But Reasoning Does: Training LLMs with Order-Centric Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qianxi He",
      "Qianyu He",
      "Jiaqing Liang",
      "Weikang Zhou",
      "Zeye Sun",
      "Fei Yu",
      "Yanghua Xiao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1383": {
    "title": "Type-Less yet Type-Aware Inductive Link Prediction with Pretrained Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alessandro De Bellis",
      "Salvatore Bufi",
      "Giovanni Servedio",
      "Vito Walter Anelli",
      "Tommaso Di Noia",
      "Eugenio Di Sciascio"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1384": {
    "title": "Extracting Linguistic Information from Large Language Models: Syntactic Relations and Derivational Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tsedeniya Kinfe Temesgen",
      "Marion Di Marco",
      "Alexander Fraser"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1385": {
    "title": "Beyond Correctness: Confidence-Aware Reward Modeling for Enhancing Large Language Model Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qianxi He",
      "Qingyu Ren",
      "Shanzhe Lei",
      "Xuhong Wang",
      "Yingchun Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1386": {
    "title": "TrojanStego: Your Language Model Can Secretly Be A Steganographic Privacy Leaking Agent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dominik Meier",
      "Jan Philip Wahle",
      "Paul Röttger",
      "Terry Ruas",
      "Bela Gipp"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1387": {
    "title": "Frequency & Compositionality in Emergent Communication",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jean-Baptiste Sevestre",
      "Emmanuel Dupoux"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1388": {
    "title": "Summarizing Speech: A Comprehensive Survey",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fabian Retkowski",
      "Maike Züfle",
      "Andreas Sudmann",
      "Dinah Pfau",
      "Shinji Watanabe",
      "Jan Niehues",
      "Alexander Waibel"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1389": {
    "title": "CogDual: Enhancing Dual Cognition of LLMs via Reinforcement Learning with Implicit Rule-Based Rewards",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Liu",
      "Yifei Lu",
      "Fanghua Ye",
      "Jian Li",
      "Xingyu Chen",
      "Feiliang Ren",
      "Zhaopeng Tu",
      "Xiaolong Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1390": {
    "title": "Assay2Mol: Large Language Model-based Drug Design Using BioAssay Context",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Deng",
      "Spencer S Ericksen",
      "Anthony Gitter"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1391": {
    "title": "Frame First, Then Extract: A Frame-Semantic Reasoning Pipeline for Zero-Shot Relation Triplet Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zehan Li",
      "Fu Zhang",
      "Wenqing Zhang",
      "Jiawei Li",
      "Zhou Li",
      "Jingwei Cheng",
      "Tianyue Peng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1392": {
    "title": "MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yahan Yang",
      "Soham Dan",
      "Shuo Li",
      "Dan Roth",
      "Insup Lee"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1393": {
    "title": "TALON: A Multi-Agent Framework for Long-Table Exploration and Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruochun Jin",
      "Xiyue Wang",
      "Dong Wang",
      "Haoqi Zheng",
      "Yunpeng Qi",
      "Silin Yang",
      "Meng Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1394": {
    "title": "You Are What You Train: Effects of Data Composition on Training Context-aware Machine Translation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pawel Maka",
      "Yusuf Can Semerci",
      "Jan Scholtes",
      "Gerasimos Spanakis"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1395": {
    "title": "Improving Neutral Point-of-View Generation with Data- and Parameter-Efficient RL",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jessica Hoffmann",
      "Christiane Ahlheim",
      "Zac Yu",
      "Aria Walfrand",
      "Jarvis Jin",
      "Marie Tano",
      "Ahmad Beirami",
      "Erin MacMurray van Liemt",
      "Nithum Thain",
      "Hakim Sidahmed",
      "Lucas Dixon"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1396": {
    "title": "Randomized Smoothing Meets Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emmanouil Seferis",
      "Changshun Wu",
      "Stefanos Kollias",
      "Saddek Bensalem",
      "Chih-Hong Cheng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1397": {
    "title": "PIIvot: A Lightweight NLP Anonymization Framework for Question-Anchored Tutoring Dialogues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew Zent",
      "Digory Smith",
      "Simon Woodhead"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1398": {
    "title": "Trustworthy Medical Question Answering: An Evaluation-Centric Survey",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinuo Wang",
      "Baiyang Wang",
      "Robert Mercer",
      "Frank Rudzicz",
      "Sudipta Singha Roy",
      "Pengjie Ren",
      "Zhumin Chen",
      "Xindi Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1399": {
    "title": "Unpacking Let Alone: Human-Scale Models Generalize to a Rare Construction in Form but not Meaning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wesley Scivetti",
      "Tatsuya Aoyama",
      "Ethan Wilcox",
      "Nathan Schneider"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1400": {
    "title": "BOUQuET : dataset, Benchmark and Open initiative for Universal Quality Evaluation in Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pierre Andrews",
      "Mikel Artetxe",
      "Mariano Coria Meglioli",
      "Marta R. Costa-jussà",
      "Joe Chuang",
      "David Dale",
      "Mark Duppenthaler",
      "Nathanial Paul Ekberg",
      "Cynthia Gao",
      "Daniel Edward Licht",
      "Jean Maillard",
      "Alexandre Mourachko",
      "Christophe Ropers",
      "Safiyyah Saleem",
      "Eduardo Sánchez",
      "Ioannis Tsiamas",
      "Arina Turkatenko",
      "Albert Ventayol-Boada",
      "Shireen Yates"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1401": {
    "title": "HealthCards: Exploring Text-to-Image Generation as Visual Aids for Healthcare Knowledge Democratizing and Education",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qian Wu",
      "Zheyao Gao",
      "Longfei Gou",
      "Yifan Hou",
      "Ann Sin Nga Lau",
      "Qi Dou"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1402": {
    "title": "When Life Gives You Samples: The Benefits of Scaling up Inference Compute for Multilingual LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ammar Khairi",
      "Daniel D’souza",
      "Ye Shen",
      "Julia Kreutzer",
      "Sara Hooker"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1403": {
    "title": "Creativity in LLM-based Multi-Agent Systems: A Survey",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi-Cheng Lin",
      "Kang-Chieh Chen",
      "Zhe-Yan Li",
      "Tzu-Heng Wu",
      "Tzu-Hsuan Wu",
      "Kuan-Yu Chen",
      "Hung-yi Lee",
      "Yun-Nung Chen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1404": {
    "title": "Context and POS in Action: A Comparative Study of Chinese Homonym Disambiguation in Human and Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xie Chenwei",
      "Matthew King-Hang Ma",
      "Wenbo Wang",
      "William Shiyuan Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1405": {
    "title": "Attacking Misinformation Detection Using Adversarial Examples Generated by Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Piotr Przybyła",
      "Euan McGill",
      "Horacio Saggion"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1406": {
    "title": "Leveraging Loanword Constraints for Improving Machine Translation in a Low-Resource Multilingual Context",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Felermino D. M. A. Ali",
      "Henrique Lopes Cardoso",
      "Rui Sousa-Silva"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1407": {
    "title": "Linguistic Neuron Overlap Patterns to Facilitate Cross-lingual Transfer on Low-resource Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuemei Xu",
      "Kexin Xu",
      "Jian Zhou",
      "Ling Hu",
      "Lin Gui"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1408": {
    "title": "Scaling Low-Resource MT via Synthetic Data Generation with LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ona de Gibert",
      "Joseph Attieh",
      "Teemu Vahtola",
      "Mikko Aulamo",
      "Zihao Li",
      "Raúl Vázquez",
      "Tiancheng Hu",
      "Jörg Tiedemann"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1409": {
    "title": "Tailoring Table Retrieval from a Field-aware Hybrid Matching Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Da Li",
      "Keping Bi",
      "Jiafeng Guo",
      "Xueqi Cheng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1410": {
    "title": "Randomly Removing 50% of Dimensions in Text Embeddings has Minimal Impact on Retrieval and Classification Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sotaro Takeshita",
      "Yurina Takeshita",
      "Daniel Ruffinelli",
      "Simone Paolo Ponzetto"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1411": {
    "title": "Morables: A Benchmark for Assessing Abstract Moral Reasoning in LLMs with Fables",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matteo Marcuzzo",
      "Alessandro Zangari",
      "Andrea Albarelli",
      "Jose Camacho-Collados",
      "Mohammad Taher Pilehvar"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1412": {
    "title": "MessIRve: A Large-Scale Spanish Information Retrieval Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francisco Valentini",
      "Viviana Cotik",
      "Damián Furman",
      "Ivan Bercovich",
      "Edgar Altszyler",
      "Juan Manuel Pérez"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1413": {
    "title": "AFRIDOC-MT: Document-level MT Corpus for African Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jesujoba Oluwadara Alabi",
      "Israel Abebe Azime",
      "Miaoran Zhang",
      "Cristina España-Bonet",
      "Rachel Bawden",
      "Dawei Zhu",
      "David Ifeoluwa Adelani",
      "Clement Oyeleke Odoje",
      "Idris Akinade",
      "Iffat Maab",
      "Davis David",
      "Shamsuddeen Hassan Muhammad",
      "Neo Putini",
      "David O. Ademuyiwa",
      "Andrew Caines",
      "Dietrich Klakow"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1414": {
    "title": "Charting the Landscape of African NLP: Mapping Progress and Shaping the Road Ahead",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jesujoba Oluwadara Alabi",
      "Michael A. Hedderich",
      "David Ifeoluwa Adelani",
      "Dietrich Klakow"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1415": {
    "title": "GLIMPSE: Do Large Vision-Language Models Truly Think With Videos or Just Glimpse at Them?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiyang Zhou",
      "Linjie Li",
      "Shi Qiu",
      "Zhengyuan Yang",
      "Yuyang Zhao",
      "Siwei Han",
      "Yangfan He",
      "Kangqi Li",
      "Haonian Ji",
      "Zihao Zhao",
      "Haibo Tong",
      "Lijuan Wang",
      "Huaxiu Yao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1416": {
    "title": "Social Bias in Multilingual Language Models: A Survey",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lance Calvin Lim Gamboa",
      "Yue Feng",
      "Mark G. Lee"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1417": {
    "title": "BYOKG-RAG: Multi-Strategy Graph Retrieval for Knowledge Graph Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Costas Mavromatis",
      "Soji Adeshina",
      "Vassilis N. Ioannidis",
      "Zhen Han",
      "Qi Zhu",
      "Ian Robinson",
      "Bryan Thompson",
      "Huzefa Rangwala",
      "George Karypis"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1418": {
    "title": "Synth-SBDH: A Synthetic Dataset of Social and Behavioral Determinants of Health for Clinical Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Avijit Mitra",
      "Zhichao Yang",
      "Emily Druhl",
      "Raelene Goodwin",
      "Hong Yu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1419": {
    "title": "Pun Unintended: LLMs and the Illusion of Humor Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alessandro Zangari",
      "Matteo Marcuzzo",
      "Andrea Albarelli",
      "Mohammad Taher Pilehvar",
      "Jose Camacho-Collados"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1420": {
    "title": "RACCooN: Versatile Instructional Video Editing with Auto-Generated Narratives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaehong Yoon",
      "Shoubin Yu",
      "Mohit Bansal"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1421": {
    "title": "Pre-trained Models Perform the Best When Token Distributions Follow Zipf's Law",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanjin He",
      "Qingkai Zeng",
      "Meng Jiang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1422": {
    "title": "Do RAG Systems Really Suffer From Positional Bias?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Florin Cuconasu",
      "Simone Filice",
      "Guy Horowitz",
      "Yoelle Maarek",
      "Fabrizio Silvestri"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1423": {
    "title": "Aspect-Oriented Summarization for Psychiatric Short-Term Readmission Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "WonJin Yoon",
      "Boyu Ren",
      "Spencer Thomas",
      "Chanhwi Kim",
      "Guergana K Savova",
      "Mei-Hua Hall",
      "Timothy A. Miller"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1424": {
    "title": "Adapting Bias Evaluation to Domain Contexts using Generative Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tamara Quiroga",
      "Felipe Bravo-Marquez",
      "Valentin Barriere"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1425": {
    "title": "Emergent morpho-phonological representations in self-supervised speech models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jon Gauthier",
      "Canaan Breiss",
      "Matthew K Leonard",
      "Edward F. Chang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1426": {
    "title": "Multilingual Language Model Pretraining using Machine-translated Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayi Wang",
      "Yao Lu",
      "Maurice Weber",
      "Max Ryabinin",
      "David Ifeoluwa Adelani",
      "Yihong Chen",
      "Raphael Tang",
      "Pontus Stenetorp"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1427": {
    "title": "IntentionFrame: A Semi-Structured, Multi-Aspect Framework for Fine-Grained Conversational Intention Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinggui Liang",
      "Dung Vo",
      "Lizi Liao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1428": {
    "title": "Video-RTS: Rethinking Reinforcement Learning and Test-Time Scaling for Efficient and Enhanced Video Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyang Wang",
      "Jaehong Yoon",
      "Shoubin Yu",
      "Md Mohaiminul Islam",
      "Gedas Bertasius",
      "Mohit Bansal"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1429": {
    "title": "Efficient Compositional Multi-tasking for On-device Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ondrej Bohdal",
      "Mete Ozay",
      "Jijoong Moon",
      "Kyenghun Lee",
      "Hyeonmok Ko",
      "Umberto Michieli"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1430": {
    "title": "Improving Large Language Model Safety with Contrastive Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Simko",
      "Mrinmaya Sachan",
      "Bernhard Schölkopf",
      "Zhijing Jin"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1431": {
    "title": "Leveraging What's Overfixed: Post-Correction via LLM Grammatical Error Overcorrection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taehee Park",
      "Heejin Do",
      "Gary Lee"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1432": {
    "title": "Scaling Up Temporal Domain Generalization via Temporal Experts Averaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aoming Liu",
      "Kevin Miller",
      "Venkatesh Saligrama",
      "Kate Saenko",
      "Boqing Gong",
      "Ser-Nam Lim",
      "Bryan A. Plummer"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1433": {
    "title": "LinguaLens: Towards Interpreting Linguistic Mechanisms of Large Language Models via Sparse Auto-Encoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Jing",
      "Zijun Yao",
      "Hongzhu Guo",
      "Lingxu Ran",
      "Xiaozhi Wang",
      "Lei Hou",
      "Juanzi Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1434": {
    "title": "The Strawberry Problem: Emergence of Character-level Understanding in Tokenized Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adrian Cosma",
      "Stefan Ruseti",
      "Emilian Radoi",
      "Mihai Dascalu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1435": {
    "title": "Improving the Quality of Web-mined Parallel Corpora of Low-Resource Languages using Debiasing Heuristics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Surangika Ranathunga",
      "Aloka Fernando",
      "Menan Velayuthan",
      "Charitha Rathnayaka",
      "Nisansa de Silva"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1436": {
    "title": "Weaver: Interweaving SQL and LLM for Table Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rohit Khoja",
      "Devanshu Gupta",
      "Yanjie Fu",
      "Dan Roth",
      "Vivek Gupta"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1437": {
    "title": "ECO Decoding: Entropy-Based Control for Controllability and Fluency in Controllable Dialogue Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seungmin Shin",
      "Dooyoung Kim",
      "Youngjoong Ko"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1438": {
    "title": "Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antara Raaghavi Bhattacharya",
      "Isabel Papadimitriou",
      "Kathryn Davidson",
      "David Alvarez-Melis"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1439": {
    "title": "Unsupervised Concept Vector Extraction for Bias Control in LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hannah Cyberey",
      "Yangfeng Ji",
      "David Evans"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1440": {
    "title": "Seeing the Same Story Differently: Framing‐Divergent Event Coreference for Computational Framing Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jin Zhao",
      "Xinrui Hu",
      "Nianwen Xue"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1441": {
    "title": "LLMs are Better Than You Think: Label-Guided In-Context Learning for Named Entity Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Bai",
      "Hamid Hassanzadeh",
      "Ardavan Saeedi",
      "Mark Dredze"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1442": {
    "title": "COUNTDOWN: Contextually Sparse Activation Filtering Out Unnecessary Weights in Down Projection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaewon Cheon",
      "Pilsung Kang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1443": {
    "title": "SimpleDoc: Multi‐Modal Document Understanding with Dual‐Cue Page Retrieval and Iterative Refinement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chelsi Jain",
      "Yiran Wu",
      "Yifan Zeng",
      "Jiale Liu",
      "Shengyu Dai",
      "Zhenwen Shao",
      "Qingyun Wu",
      "Huazheng Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1444": {
    "title": "VLP: Vision-Language Preference Learning for Embodied Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runze Liu",
      "Chenjia Bai",
      "Jiafei Lyu",
      "Shengjie Sun",
      "Yali Du",
      "Xiu Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1445": {
    "title": "QG-CoC: Question-Guided Chain-of-Captions for Large Multimodal Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kuei-Chun Kao",
      "Hsu Tzu-Yin",
      "Yunqi Hong",
      "Ruochen Wang",
      "Cho-Jui Hsieh"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1446": {
    "title": "EGOILLUSION: Benchmarking Hallucinations in Egocentric Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashish Seth",
      "Utkarsh Tyagi",
      "Ramaneswaran Selvakumar",
      "Nishit Anand",
      "Sonal Kumar",
      "Sreyan Ghosh",
      "Ramani Duraiswami",
      "Chirag Agarwal",
      "Dinesh Manocha"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1447": {
    "title": "MULTIVOX: A Benchmark for Evaluating Voice Assistants for Multimodal Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ramaneswaran Selvakumar",
      "Ashish Seth",
      "Nishit Anand",
      "Utkarsh Tyagi",
      "Sonal Kumar",
      "Sreyan Ghosh",
      "Dinesh Manocha"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1448": {
    "title": "Do All Autoregressive Transformers Remember Facts the Same Way? A Cross-Architecture Analysis of Recall Mechanisms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minyeong Choe",
      "Haehyun Cho",
      "Changho Seo",
      "Hyunil Kim"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1449": {
    "title": "Probing Narrative Morals: A New Character-Focused MFT Framework for Use with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luca Mitran",
      "Sophie Wu",
      "Andrew Piper"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1450": {
    "title": "Probing and Boosting Large Language Models Capabilities via Attention Heads",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dezhi Zhao",
      "Xin Liu",
      "Xiaocheng Feng",
      "Hui Wang",
      "Bing Qin"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1451": {
    "title": "A Survey of Link Prediction in N-ary Knowledge Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiyao Wei",
      "Saiping Guan",
      "Da Li",
      "Zhongni Hou",
      "Miao Su",
      "Yucan Guo",
      "Xiaolong Jin",
      "Jiafeng Guo",
      "Xueqi Cheng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1452": {
    "title": "Multi-Frequency Contrastive Decoding: Alleviating Hallucinations for Large Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bingqian Liu",
      "Fu Zhang",
      "Guoqing Chen",
      "Jingwei Cheng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1453": {
    "title": "ORPP: Self-Optimizing Role-playing Prompts to Enhance Language Model Capabilities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Duan",
      "Yihong Tang",
      "Kehai Chen",
      "Liqiang Nie",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1454": {
    "title": "BrailleLLM: Braille Instruction Tuning with Large Language Models for Braille Domain Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyuan Huang",
      "Zepeng Zhu",
      "Hangdi Xing",
      "Zirui Shao",
      "Zhi Yu",
      "Chaoxiong Yang",
      "Jiaxian He",
      "Xiaozhong Liu",
      "Jiajun Bu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1455": {
    "title": "MAviS: A Multimodal Conversational Assistant For Avian Species",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yevheniia Kryklyvets",
      "Mohammed Irfan Kurpath",
      "Sahal Shaji Mullappilly",
      "Jinxing Zhou",
      "Fahad Shahbaz Khan",
      "Rao Muhammad Anwer",
      "Salman Khan",
      "Hisham Cholakkal"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1456": {
    "title": "Refining Text Generation for Realistic Conversational Recommendation via Direct Preference Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manato Tajiri",
      "Michimasa Inaba"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1457": {
    "title": "Large Language Models Threaten Language's Epistemic and Communicative Foundations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shashank Srivastava"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1458": {
    "title": "Detecting Knowledge Boundary of Vision Large Language Models by Sampling-Based Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuo Chen",
      "Xinyu Wang",
      "Yong Jiang",
      "Zhen Zhang",
      "Xinyu Geng",
      "Pengjun Xie",
      "Fei Huang",
      "Kewei Tu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1459": {
    "title": "Multi-view-guided Passage Reranking with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeongwoo Na",
      "Jun Kwon",
      "Eunseong Choi",
      "Jongwuk Lee"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1460": {
    "title": "Disentangling Subjectivity and Uncertainty for Hate Speech Annotation and Modeling using Gaze",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Özge Alacam",
      "Sanne Hoeken",
      "Andreas Säuberli",
      "Hannes Gröner",
      "Diego Frassinelli",
      "Sina Zarrieß",
      "Barbara Plank"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1461": {
    "title": "VoiceBBQ: Investigating Effect of Content and Acoustics in Social Bias of Spoken Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junhyuk Choi",
      "Ro-hoon Oh",
      "Jihwan Seol",
      "Bugeun Kim"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1462": {
    "title": "Explaining Differences Between Model Pairs in Natural Language through Sample Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Advaith Malladi",
      "Rakesh R Menon",
      "Yuvraj Jain",
      "Shashank Srivastava"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1463": {
    "title": "Compound AI Systems Optimization: A Survey of Methods, Challenges, and Future Directions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu-Ang Lee",
      "Guan-Ting Yi",
      "Mei-Yi Liu",
      "Jui-Chao Lu",
      "Guan-Bo Yang",
      "Yun-Nung Chen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1464": {
    "title": "A Multi-Level Benchmark for Causal Language Understanding in Social Media Discourse",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaohan Ding",
      "Kaike Ping",
      "Buse Çarık",
      "Eugenia Rho"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1465": {
    "title": "Causal Representation Learning from Multimodal Clinical Records under Non-Random Modality Missingness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihan Liang",
      "Ziwen Pan",
      "Ruoxuan Xiong"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1466": {
    "title": "XLQA: A Benchmark for Locale-Aware Multilingual Open-Domain Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keonwoo Roh",
      "Yeong-Joon Ju",
      "Seong-Whan Lee"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1467": {
    "title": "Transformer-Based Temporal Information Extraction and Application: A Review",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Su",
      "Phillip Howard",
      "Steven Bethard"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1468": {
    "title": "How to Protect Yourself from 5G Radiation? Investigating LLM Responses to Implicit Misinformation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruohao Guo",
      "Wei Xu",
      "Alan Ritter"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1469": {
    "title": "AmpleHate: Amplifying the Attention for Versatile Implicit Hate Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yejin Lee",
      "Joonghyuk Hahn",
      "Hyeseon Ahn",
      "Yo-Sub Han"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1470": {
    "title": "Can Large Language Models Act as Ensembler for Multi-GNNs?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanqi Duan",
      "Yao Cheng",
      "Jianxiang Yu",
      "Yao Liu",
      "Xiang Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1471": {
    "title": "Agent-to-Agent Theory of Mind: Testing Interlocutor Awareness among Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Younwoo Choi",
      "Changling Li",
      "Yongjin Yang",
      "Zhijing Jin"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1472": {
    "title": "From Charts to Fair Narratives: Uncovering and Mitigating Geo-Economic Biases in Chart-to-Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ridwan Mahbub",
      "Mohammed Saidul Islam",
      "Mir Tafseer Nayeem",
      "Md Tahmid Rahman Laskar",
      "Mizanur Rahman",
      "Shafiq Joty",
      "Enamul Hoque"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1473": {
    "title": "Real-time Ad Retrieval via LLM-generative Commercial Intention for Sponsored Search Advertising",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tongtong Liu",
      "Zhaohui Wang",
      "Meiyue Qin",
      "Zenghui Lu",
      "Xudong Chen",
      "Yuekui Yang",
      "Peng Shu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1474": {
    "title": "Toward Efficient Sparse Autoencoder-Guided Steering for Improved In-Context Learning in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ikhyun Cho",
      "Julia Hockenmaier"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1475": {
    "title": "CLMTracing: Black-box User-level Watermarking for Code Language Model Tracing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boyu Zhang",
      "Ping He",
      "Tianyu Du",
      "Xuhong Zhang",
      "Lei Yun",
      "Kingsum Chow",
      "Jianwei Yin"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1476": {
    "title": "The Good, the Bad and the Constructive: Automatically Measuring Peer Review's Utility for Authors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abdelrahman Sadallah",
      "Tim Baumgärtner",
      "Iryna Gurevych",
      "Ted Briscoe"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1477": {
    "title": "Evolving Chinese Spelling Correction with Corrector-Verifier Collaboration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linfeng Liu",
      "Hongqiu Wu",
      "Hai Zhao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1478": {
    "title": "M2Edit: Locate and Edit Multi-Granularity Knowledge in Multimodal Large Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Zhou",
      "Pengfei Cao",
      "Yubo Chen",
      "Qingbin Liu",
      "Dianbo Sui",
      "Xi Chen",
      "Kang Liu",
      "Jun Zhao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1479": {
    "title": "Do LLMs Behave as Claimed? Investigating How LLMs Follow Their Own Claims using Counterfactual Questions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haochen Shi",
      "Shaobo Li",
      "Guoqing Chao",
      "Xiaoliang Shi",
      "Wentao Chen",
      "Zhenzhou Ji"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1480": {
    "title": "Multilingual vs Crosslingual Retrieval of Fact-Checked Claims: A Tale of Two Approaches",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alan Ramponi",
      "Marco Rovera",
      "Robert Moro",
      "Sara Tonelli"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1481": {
    "title": "How Much Do LLMs Hallucinate across Languages? On Realistic Multilingual Estimation of LLM Hallucination",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saad Obaid Ul Islam",
      "Anne Lauscher",
      "Goran Glavaš"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1482": {
    "title": "LiTransProQA: An LLM-based Literary Translation Evaluation Metric with Professional Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ran Zhang",
      "Wei Zhao",
      "Lieve Macken",
      "Steffen Eger"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1483": {
    "title": "Improving Handshape Representations for Sign Language Processing: A Graph Neural Network Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alessa Carbo",
      "Eric Nalisnick"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1484": {
    "title": "Instructing Large Language Models for Low-Resource Languages: A Systematic Study for Basque",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oscar Sainz",
      "Naiara Perez",
      "Julen Etxaniz",
      "Joseba Fernandez de Landa",
      "Itziar Aldabe",
      "Iker García-Ferrero",
      "Aimar Zabala",
      "Ekhi Azurmendi",
      "German Rigau",
      "Eneko Agirre",
      "Mikel Artetxe",
      "Aitor Soroa"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1485": {
    "title": "SOCIAL SCAFFOLDS: A Generalization Framework for Social Understanding Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ritam Dutt",
      "Carolyn Rose",
      "Maarten Sap"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1486": {
    "title": "Beyond A Single AI Cluster: A Survey of Decentralized LLM Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haotian Dong",
      "Jingyan Jiang",
      "Rongwei Lu",
      "Jiajun Luo",
      "Jiajun Song",
      "Bowen Li",
      "Ying Shen",
      "Zhi Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1487": {
    "title": "Can LLM Agents Maintain a Persona in Discourse?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pranav Bhandari",
      "Nicolas Fay",
      "Michael J Wise",
      "Amitava Datta",
      "Stephanie Meek",
      "Usman Naseem",
      "Mehwish Nasim"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1488": {
    "title": "Iterative Multilingual Spectral Attribute Erasure",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shun Shao",
      "Yftah Ziser",
      "Zheng Zhao",
      "Yifu Qiu",
      "Shay B Cohen",
      "Anna Korhonen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1489": {
    "title": "TinySQL: A Progressive Text-to-SQL Dataset for Mechanistic Interpretability Research",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abir Harrasse",
      "Philip Quirke",
      "Clement Neo",
      "Dhruv Nathawani",
      "Luke Marks",
      "Amir Abdullah"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1490": {
    "title": "SCRIBE: Structured Chain Reasoning for Interactive Behaviour Explanations using Tool Calling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fares Fawzi",
      "Vinitra Swamy",
      "Dominik Glandorf",
      "Tanya Nazaretsky",
      "Tanja Käser"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1491": {
    "title": "Logit Space Constrained Fine-Tuning for Mitigating Hallucinations in LLM-Based Recommender Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianfeng Deng",
      "Qingfeng Chen",
      "Debo Cheng",
      "Jiuyong Li",
      "Lin Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1492": {
    "title": "PACHAT: Persona-Aware Speech Assistant for Multi-party Dialogue",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongjie Fu",
      "Xize Cheng",
      "Linjun Li",
      "Xiaoda Yang",
      "Lujia Yang",
      "Tao Jin"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1493": {
    "title": "Reasoning-to-Defend: Safety-Aware Reasoning Can Defend Large Language Models from Jailbreaking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junda Zhu",
      "Lingyong Yan",
      "Shuaiqiang Wang",
      "Dawei Yin",
      "Lei Sha"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1494": {
    "title": "Graph-Guided Textual Explanation Generation Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuzhou Yuan",
      "Jingyi Sun",
      "Ran Zhang",
      "Michael Färber",
      "Steffen Eger",
      "Pepa Atanasova",
      "Isabelle Augenstein"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1495": {
    "title": "The Validation Gap: A Mechanistic Analysis of How Language Models Compute Arithmetic but Fail to Validate It",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leonardo Bertolazzi",
      "Philipp Mondorf",
      "Barbara Plank",
      "Raffaella Bernardi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1496": {
    "title": "A Causal Lens for Evaluating Faithfulness Metrics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kerem Zaman",
      "Shashank Srivastava"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1497": {
    "title": "Sequential-NIAH: A Needle-In-A-Haystack Benchmark for Extracting Sequential Needles from Long Contexts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Yu",
      "Qian-Wen Zhang",
      "Lingfeng Qiao",
      "Di Yin",
      "Fang Li",
      "Jie Wang",
      "Chen Zeng Xi",
      "Suncong Zheng",
      "Xiaolong Liang",
      "Xing Sun"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1498": {
    "title": "FISTAPruner: Layer-wise Post-training Pruning for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengxiang Zhao",
      "Hanyu Hu",
      "Ping Li",
      "Yi Zheng",
      "Zhefeng Wang",
      "Xiaoming Yuan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1499": {
    "title": "Do LLMs Encode Frame Semantics? Evidence from Frame Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jayanth Krishna Chundru",
      "Rudrashis Poddar",
      "Jie Cao",
      "Tianyu Jiang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1500": {
    "title": "StepER: Step-wise Knowledge Distillation for Enhancing Reasoning Ability in Multi-Step Retrieval-Augmented Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyumin Lee",
      "Minjin Jeon",
      "Sanghwan Jang",
      "Hwanjo Yu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1501": {
    "title": "How Does DPO Reduce Toxicity? A Mechanistic Neuron-Level Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yushi Yang",
      "Filip Sondej",
      "Harry Mayne",
      "Andrew Lee",
      "Adam Mahdi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1502": {
    "title": "It's All About In-Context Learning! Teaching Extremely Low-Resource Languages to LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Li",
      "Zhixue Zhao",
      "Carolina Scarton"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1503": {
    "title": "Where to show Demos in Your Prompt: A Positional Bias of In-Context Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kwesi Adu Cobbina",
      "Tianyi Zhou"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1504": {
    "title": "Multilingual Pretraining for Pixel Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ilker Kesen",
      "Jonas F. Lotz",
      "Ingo Ziegler",
      "Phillip Rust",
      "Desmond Elliott"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1505": {
    "title": "MetaFaith: Faithful Natural Language Uncertainty Expression in LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gabrielle Kaili-May Liu",
      "Gal Yona",
      "Avi Caciularu",
      "Idan Szpektor",
      "Tim G. J. Rudner",
      "Arman Cohan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1506": {
    "title": "Machine-generated text detection prevents language model collapse",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "George Drayson",
      "Emine Yilmaz",
      "Vasileios Lampos"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1507": {
    "title": "Data-Efficient Hate Speech Detection via Cross-Lingual Nearest Neighbor Retrieval with Limited Labeled Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Faeze Ghorbanpour",
      "Daryna Dementieva",
      "Alexander Fraser"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1508": {
    "title": "V-VAE: A Variational Auto Encoding Framework Towards Fine-Grained Control over Human-Like Chat",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Lin",
      "Weikai Xu",
      "Lisi Chen",
      "Bin Dai"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1509": {
    "title": "Mixture of Languages: Improved Multilingual Encoders Through Language Grouping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "João Maria Janeiro",
      "Belen Alastruey",
      "Francisco Massa",
      "Maha Elbayad",
      "Benjamin Piwowarski",
      "Patrick Gallinari",
      "Loic Barrault"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1510": {
    "title": "Too Helpful, Too Harmless, Too Honest or Just Right?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gautam Siddharth Kashyap",
      "Mark Dras",
      "Usman Naseem"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1511": {
    "title": "Cardiverse: Harnessing LLMs for Novel Card Game Prototyping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Danrui Li",
      "Sen Zhang",
      "Samuel S. Sohn",
      "Kaidong Hu",
      "Muhammad Usman",
      "Mubbasir Kapadia"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1512": {
    "title": "Assessing effective de-escalation of crisis conversations using transformer-based models and trend statistics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ignacio J. Tripodi",
      "Greg Buda",
      "Margaret Meagher",
      "Elizabeth A. Olson"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1513": {
    "title": "Measuring and Mitigating Media Outlet Name Bias in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seong-Jin Park",
      "Kang-Min Kim"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1514": {
    "title": "The Good, the Bad, and the Debatable: A Survey on the Impacts of Data for In-Context Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stephanie Schoch",
      "Yangfeng Ji"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1515": {
    "title": "Where Confabulation Lives: Latent Feature Discovery in LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thibaud Ardoin",
      "Yi Cai",
      "Gerhard Wunder"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1516": {
    "title": "Analysing Chain of Thought Dynamics: Active Guidance or Unfaithful Post-hoc Rationalisation?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Lewis-Lim",
      "Xingwei Tan",
      "Zhixue Zhao",
      "Nikolaos Aletras"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1517": {
    "title": "Playpen: An Environment for Exploring Learning From Dialogue Game Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicola Horst",
      "Davide Mazzaccara",
      "Antonia Schmidt",
      "Michael Sullivan",
      "Filippo Momentè",
      "Luca Franceschetti",
      "Philipp Sadler",
      "Sherzod Hakimov",
      "Alberto Testoni",
      "Raffaella Bernardi",
      "Raquel Fernández",
      "Alexander Koller",
      "Oliver Lemon",
      "David Schlangen",
      "Mario Giulianelli",
      "Alessandro Suglia"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1518": {
    "title": "GenLink: Generation-Driven Schema-Linking via Multi-Model Learning for Text-to-SQL",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhifeng Hao",
      "Junqi Huang",
      "Shaobin Shi",
      "Ruichu Cai",
      "Boyan Xu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1519": {
    "title": "TSVer: A Benchmark for Fact Verification Against Time-Series Evidence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marek Strong",
      "Andreas Vlachos"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1520": {
    "title": "Cross-MoE: An Efficient Temporal Prediction Framework Integrating Textual Modality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruizheng Huang",
      "Zhicheng Zhang",
      "Yong Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1521": {
    "title": "Sparse Autoencoder Features for Classifications and Transferability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jack Gallifant",
      "Shan Chen",
      "Kuleen Sasse",
      "Hugo Aerts",
      "Thomas Hartvigsen",
      "Danielle Bitterman"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1522": {
    "title": "KGE Calibrator: An Efficient Probability Calibration Method of Knowledge Graph Embedding Models for Trustworthy Link Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Yang",
      "Mohan Timilsina",
      "Edward Curry"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1523": {
    "title": "LCES: Zero-shot Automated Essay Scoring via Pairwise Comparisons Using Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Takumi Shibata",
      "Yuichi Miyamura"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1524": {
    "title": "The Arabic Generality Score: Another Dimension of Modeling Arabic Dialectness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanad Sha’ban",
      "Nizar Habash"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1525": {
    "title": "Lemmatization as a Classification Task: Results from Arabic across Multiple Genres",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mostafa Saeed",
      "Nizar Habash"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1526": {
    "title": "A Comprehensive Framework to Operationalize Social Stereotypes for Responsible AI Evaluations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aida Mostafazadeh Davani",
      "Sunipa Dev",
      "Héctor Pérez-Urbina",
      "Vinodkumar Prabhakaran"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1527": {
    "title": "Correct-Detect: Balancing Performance and Ambiguity Through the Lens of Coreference Resolution in LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amber Shore",
      "Russell Scheinberg",
      "Ameeta Agrawal",
      "So Young Lee"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1528": {
    "title": "GRAID: Synthetic Data Generation with Geometric Constraints and Multi-Agentic Reflection for Harmful Content Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Melissa Kazemi Rad",
      "Alberto Purpura",
      "Himanshu Kumar",
      "Emily Chen",
      "Mohammad Shahed Sorower"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1529": {
    "title": "LaMDAgent: An Autonomous Framework for Post-Training Pipeline Optimization via LLM Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taro Yano",
      "Yoichi Ishibashi",
      "Masafumi Oyamada"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1530": {
    "title": "Finetuning LLMs for Human Behavior Prediction in Social Science Experiments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akaash Kolluri",
      "Shengguang Wu",
      "Joon Sung Park",
      "Michael S. Bernstein"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1531": {
    "title": "How Private are Language Models in Abstractive Summarization?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anthony Hughes",
      "Nikolaos Aletras",
      "Ning Ma"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1532": {
    "title": "Expectation Preference Optimization: Reliable Preference Estimation for Improving the Reasoning Capability of Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zelin Li",
      "Dawei Song"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1533": {
    "title": "Split-Merge: Scalable and Memory-Efficient Merging of Expert LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sruthi Gorantla",
      "Aditya Rawal",
      "Devamanyu Hazarika",
      "Kaixiang Lin",
      "Mingyi Hong",
      "Mahdi Namazifar"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1534": {
    "title": "Model Consistency as a Cheap yet Predictive Proxy for LLM Elo Scores",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashwin Ramaswamy",
      "Nestor Demeure",
      "Ermal Rrapaj"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1535": {
    "title": "Plutus: Benchmarking Large Language Models in Low-Resource Greek Finance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xueqing Peng",
      "Triantafillos Papadopoulos",
      "Efstathia Soufleri",
      "Polydoros Giannouris",
      "Ruoyu Xiang",
      "Yan Wang",
      "Lingfei Qian",
      "Jimin Huang",
      "Qianqian Xie",
      "Sophia Ananiadou"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1536": {
    "title": "TaxoAlign: Scholarly Taxonomy Generation Using Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Avishek Lahiri",
      "Yufang Hou",
      "Debarshi Kumar Sanyal"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1537": {
    "title": "DiNaM: Disinformation Narrative Mining with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Witold Sosnowski",
      "Arkadiusz Modzelewski",
      "Kinga Skorupska",
      "Adam Wierzbicki"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1538": {
    "title": "VeriLocc: End-to-End Cross-Architecture Register Allocation via LLM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lesheng Jin",
      "Zhenyuan Ruan",
      "Haohui Mai",
      "Jingbo Shang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1539": {
    "title": "MemeIntel: Explainable Detection of Propagandistic and Hateful Memes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohamed Bayan Kmainasi",
      "Abul Hasnat",
      "Md Arid Hasan",
      "Ali Ezzat Shahroor",
      "Firoj Alam"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1540": {
    "title": "FLUID QA: A Multilingual Benchmark for Figurative Language Usage in Dialogue across English, Chinese, and Korean",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seoyoon Park",
      "Hyeji Choi",
      "Minseon Kim",
      "Subin An",
      "Xiaonan Wang",
      "Gyuri Choi",
      "Hansaem Kim"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1541": {
    "title": "Structured Moral Reasoning in Language Models: A Value-Grounded Evaluation Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohna Chakraborty",
      "Lu Wang",
      "David Jurgens"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1542": {
    "title": "VerIF: Verification Engineering for Reinforcement Learning in Instruction Following",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Peng",
      "Yunjia Qi",
      "Xiaozhi Wang",
      "Bin Xu",
      "Lei Hou",
      "Juanzi Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1543": {
    "title": "UNCLE: Benchmarking Uncertainty Expressions in Long-Form Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruihan Yang",
      "Caiqi Zhang",
      "Zhisong Zhang",
      "Xinting Huang",
      "Dong Yu",
      "Nigel Collier",
      "Deqing Yang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1544": {
    "title": "Enhancing Study-Level Inference from Clinical Trial Papers via Reinforcement Learning-Based Numeric Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Massimiliano Pronesti",
      "Michela Lorandi",
      "Paul Flanagan",
      "Oisín Redmond",
      "Anya Belz",
      "Yufang Hou"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1545": {
    "title": "Context-aware Biases for Length Extrapolation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Veisi",
      "Hamidreza Amirzadeh",
      "Amir M. Mansourian"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1546": {
    "title": "AutoSDT: Scaling Data-Driven Discovery Tasks Toward Open Co-Scientists",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Li",
      "Hanane Nour Moussa",
      "Ziru Chen",
      "Shijie Chen",
      "Botao Yu",
      "Mingyi Xue",
      "Benjamin Burns",
      "Tzu-Yao Chiu",
      "Vishal Dey",
      "Zitong Lu",
      "Chen Wei",
      "Qianheng Zhang",
      "Tianyu Zhang",
      "Song Gao",
      "Xuhui Huang",
      "Xia Ning",
      "Nesreen K. Ahmed",
      "Ali Payani",
      "Huan Sun"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1547": {
    "title": "Finding your MUSE: Mining Unexpected Solutions Engine",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nir Sweed",
      "Hanit Hakim",
      "Ben Wolfson",
      "Hila Lifshitz",
      "Dafna Shahaf"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1548": {
    "title": "Quantized but Deceptive? A Multi-Dimensional Truthfulness Evaluation of Quantized LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yao Fu",
      "Xianxuan Long",
      "Runchao Li",
      "Haotian Yu",
      "Mu Sheng",
      "Xiaotian Han",
      "Yu Yin",
      "Pan Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1549": {
    "title": "Leveraging Knowledge Graph-Enhanced LLMs for Context-Aware Medical Consultation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Su-Hyeong Park",
      "Ho-Beom Kim",
      "Seong-Jin Park",
      "Dinara Aliyeva",
      "Kang-Min Kim"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1550": {
    "title": "Reflective Agreement: Combining Self-Mixture of Agents with a Sequence Tagger for Robust Event Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fatemeh Haji",
      "Mazal Bethany",
      "Cho-Yu Jason Chiang",
      "Anthony Rios",
      "Peyman Najafirad"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1551": {
    "title": "Simple Yet Effective: An Information-Theoretic Approach to Multi-LLM Uncertainty Quantification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maya Kruse",
      "Majid Afshar",
      "Saksham Khatwani",
      "Anoop Mayampurath",
      "Guanhua Chen",
      "Yanjun Gao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1552": {
    "title": "Exploring morphology-aware tokenization: A case study on Spanish language modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alba Táboas García",
      "Piotr Przybyła",
      "Leo Wanner"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1553": {
    "title": "Studying Rhetorically Ambiguous Questions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oghenevovwe Ikumariegbe",
      "Eduardo Blanco",
      "Ellen Riloff"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1554": {
    "title": "Estimating LLM Consistency: A User Baseline vs Surrogate Metrics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyuan Wu",
      "Weiran Lin",
      "Omer Akgul",
      "Lujo Bauer"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1555": {
    "title": "Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "DongGeon Lee",
      "Joonwon Jang",
      "Jihae Jeong",
      "Hwanjo Yu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1556": {
    "title": "Improving Rule-based Reasoning in LLMs using Neurosymbolic Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Varun Dhanraj",
      "Chris Eliasmith"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1557": {
    "title": "Can LLMs Extract Frame-Semantic Arguments?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jacob Devasier",
      "Rishabh Mediratta",
      "Chengkai Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1558": {
    "title": "Accelerated Test-Time Scaling with Model-Free Speculative Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Woomin Song",
      "Saket Dingliwal",
      "Sai Muralidhar Jayanthi",
      "Bhavana Ganesh",
      "Jinwoo Shin",
      "Aram Galstyan",
      "Sravan Babu Bodapati"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1559": {
    "title": "Enhancing RLHF with Human Gaze Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karim Galliamov",
      "Ivan Titov",
      "Ilya Pershin"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1560": {
    "title": "Mapping semantic networks to Dutch word embeddings as a diagnostic tool for cognitive decline",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maithe van Noort",
      "Michal Korenar",
      "Jelke Bloem"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1561": {
    "title": "CausalVLBench: Benchmarking Visual Causal Reasoning in Large Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aneesh Komanduri",
      "Karuna Bhaila",
      "Xintao Wu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1562": {
    "title": "Implicit Behavioral Alignment of Language Agents in High-Stakes Crowd Simulations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunzhe Wang",
      "Gale Lucas",
      "Burcin Becerik-Gerber",
      "Volkan Ustun"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1563": {
    "title": "Are Language Models Consequentialist or Deontological Moral Reasoners?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keenan Samway",
      "Max Kleiman-Weiner",
      "David Guzman Piedrahita",
      "Rada Mihalcea",
      "Bernhard Schölkopf",
      "Zhijing Jin"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1564": {
    "title": "PatentScore: Multi-dimensional Evaluation of LLM-Generated Patent Claims",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongmin Yoo",
      "Qiongkai Xu",
      "Longbing Cao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1565": {
    "title": "All for One: LLMs Solve Mental Math at the Last Token With Information Transferred From Other Tokens",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siddarth Mamidanna",
      "Daking Rai",
      "Ziyu Yao",
      "Yilun Zhou"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1566": {
    "title": "A Position Paper on the Automatic Generation of Machine Learning Leaderboards",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roelien C. Timmer",
      "Yufang Hou",
      "Stephen Wan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1567": {
    "title": "SimMark: A Robust Sentence-Level Similarity-Based Watermarking Algorithm for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amirhossein Dabiriaghdam",
      "Lele Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1568": {
    "title": "SERVAL: Surprisingly Effective Zero-Shot Visual Document Retrieval Powered by Large Vision and Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thong Nguyen",
      "Yibin Lei",
      "Jia-Huei Ju",
      "Andrew Yates"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1569": {
    "title": "Meta-Semantics Augmented Few-Shot Relational Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Wu",
      "Jie Yin"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1570": {
    "title": "ProLongVid: A Simple but Strong Baseline for Long-context Video Instruction Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Wang",
      "Bohao Li",
      "Xiyang Dai",
      "Jianwei Yang",
      "Yi-Ling Chen",
      "Zhen Xing",
      "Yifan Yang",
      "Dongdong Chen",
      "Xipeng Qiu",
      "Zuxuan Wu",
      "Yu-Gang Jiang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1571": {
    "title": "ModelCitizens: Representing Community Voices in Online Safety",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashima Suvarna",
      "Christina A Chance",
      "Karolina Naranjo",
      "Hamid Palangi",
      "Sophie Hao",
      "Thomas Hartvigsen",
      "Saadia Gabriel"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1572": {
    "title": "UnifiedVisual: A Framework for Constructing Unified Vision-Language Datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengyu Wang",
      "Shaojun Zhou",
      "Chenkun Tan",
      "Xinghao Wang",
      "Wei Huang",
      "Zhen Ye",
      "Zhaowei Li",
      "Botian Jiang",
      "Dong Zhang",
      "Xipeng Qiu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1573": {
    "title": "The Pursuit of Empathy: Evaluating Small Language Models for PTSD Dialogue Support",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suhas Bn",
      "Yash Mahajan",
      "Dominik O. Mattioli",
      "Andrew M. Sherrill",
      "Rosa I. Arriaga",
      "Christopher Wiese",
      "Saeed Abdullah"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1574": {
    "title": "Is Cognition Consistent with Perception? Assessing and Mitigating Multimodal Knowledge Conflicts in Document Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zirui Shao",
      "Feiyu Gao",
      "Zhaoqing Zhu",
      "Chuwei Luo",
      "Hangdi Xing",
      "Zhi Yu",
      "Qi Zheng",
      "Ming Yan",
      "Jiajun Bu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1575": {
    "title": "AutoCT: Automating Interpretable Clinical Trial Prediction with LLM Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fengze Liu",
      "Haoyu Wang",
      "Joonhyuk Cho",
      "Dan Roth",
      "Andrew Lo"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1576": {
    "title": "MMDocIR: Benchmarking Multimodal Retrieval for Long Documents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kuicai Dong",
      "Yujing Chang",
      "Derrick Goh Xin Deik",
      "Dexun Li",
      "Ruiming Tang",
      "Yong Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1577": {
    "title": "Program of Thoughts for Financial Reasoning: Leveraging Dynamic In-Context Examples and Generative Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Subhendu Khatuya",
      "Shashwat Naidu",
      "Pawan Goyal",
      "Niloy Ganguly"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1578": {
    "title": "Waste-Bench: A Comprehensive Benchmark for Evaluating VLLMs in Cluttered Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Ali",
      "Salman Khan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1579": {
    "title": "Demystifying Domain-adaptive Post-training for Financial LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixuan Ke",
      "Yifei Ming",
      "Xuan-Phi Nguyen",
      "Caiming Xiong",
      "Shafiq Joty"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1580": {
    "title": "HICode: Hierarchical Inductive Coding with LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mian Zhong",
      "Pristina Wang",
      "Anjalie Field"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1581": {
    "title": "Cacheback: Speculative Decoding With Nothing But Cache",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyao Ma",
      "In Gim",
      "Lin Zhong"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1582": {
    "title": "MA-DPR: Manifold-aware Distance Metrics for Dense Passage Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Liu",
      "Qianfeng Wen",
      "Mark Zhao",
      "Jiazhou Liang",
      "Scott Sanner"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1583": {
    "title": "LLM-Guided Co-Training for Text Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Mezbaur Rahman",
      "Cornelia Caragea"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1584": {
    "title": "LeanK: Learnable K Cache Channel Pruning for Efficient Decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yike Zhang",
      "Zhiyuan He",
      "Huiqiang Jiang",
      "Chengruidong Zhang",
      "Yuqing Yang",
      "Jianyong Wang",
      "Lili Qiu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1585": {
    "title": "DELOC: Document Element Localizer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hammad Ayyubi",
      "Puneet Mathur",
      "Mehrab Tanjim",
      "Vlad I Morariu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1586": {
    "title": "NL2Lean: Translating Natural Language into Lean 4 through Multi-Aspect Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Fang",
      "Shaohan Huang",
      "Xin Yu",
      "Haizhen Huang",
      "Zihan Zhang",
      "Weiwei Deng",
      "Furu Wei",
      "Feng Sun",
      "Qi Zhang",
      "Zhi Jin"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1587": {
    "title": "A Multilingual, Culture-First Approach to Addressing Misgendering in LLM Applications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sunayana Sitaram",
      "Adrian de Wynter",
      "Isobel McCrum",
      "Qilong Gu",
      "Si-Qing Chen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1588": {
    "title": "X-CoT: Explainable Text-to-Video Retrieval via LLM-based Chain-of-Thought Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prasanna Reddy Pulakurthi",
      "Jiamian Wang",
      "Majid Rabbani",
      "Sohail Dianat",
      "Raghuveer Rao",
      "Zhiqiang Tao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1589": {
    "title": "Token-level Proximal Policy Optimization for Query Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yichen Ouyang",
      "Lu Wang",
      "Fangkai Yang",
      "Pu Zhao",
      "Chenghua Huang",
      "Jianfeng Liu",
      "Bochen Pang",
      "Yaming Yang",
      "Yuefeng Zhan",
      "Hao Sun",
      "Qingwei Lin",
      "Saravan Rajmohan",
      "Weiwei Deng",
      "Dongmei Zhang",
      "Feng Sun"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1590": {
    "title": "Prior Prompt Engineering for Reinforcement Fine-Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pittawat Taveekitworachai",
      "Potsawee Manakul",
      "Sarana Nutanong",
      "Kunat Pipatanakul"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1591": {
    "title": "Beyond WER: Probing Whisper's Sub‐token Decoder Across Diverse Language Resource Levels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyu Liang",
      "Nicolas Ballier",
      "Gina-Anne Levow",
      "Richard Wright"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1592": {
    "title": "ThinkTuning: Instilling Cognitive Reflections without Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aswin Rrv",
      "Jacob Dineen",
      "Divij Handa",
      "Md Nayem Uddin",
      "Mihir Parmar",
      "Chitta Baral",
      "Ben Zhou"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1593": {
    "title": "Droid: A Resource Suite for AI-Generated Code Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniil Orel",
      "Indraneil Paul",
      "Iryna Gurevych",
      "Preslav Nakov"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1594": {
    "title": "LoRACoE: Improving Large Language Model via Composition-based LoRA Expert",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanyu Li",
      "Zhiheng Xi",
      "Zhihao Zhang",
      "Boyang Hong",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1595": {
    "title": "Same Question, Different Words: A Latent Adversarial Framework for Prompt Robustness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tingchen Fu",
      "Fazl Barez"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1596": {
    "title": "Pluralistic Alignment for Healthcare: A Role-Driven Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayou Zhong",
      "Anudeex Shetty",
      "Chao Jia",
      "Xuanrui Lin",
      "Usman Naseem"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1597": {
    "title": "Flexible-length Text Infilling for Discrete Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrew Zhang",
      "Anushka Sivakumar",
      "Chia-Wei Tang",
      "Chris Thomas"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1598": {
    "title": "Beyond the Leaderboard: Understanding Performance Disparities in Large Language Models via Model Diffing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sabri Boughorbel",
      "Fahim Dalvi",
      "Nadir Durrani",
      "Majd Hawasly"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1599": {
    "title": "Explicit Learning and the LLM in Machine Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Malik Marmonier",
      "Rachel Bawden",
      "Benoît Sagot"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1600": {
    "title": "Towards Language-Agnostic STIPA: Universal Phonetic Transcription to Support Language Documentation at Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jacob Lee Suchardt",
      "Hana El-Shazli",
      "Pierluigi Cassotti"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1601": {
    "title": "Beyond Pairwise: Global Zero-shot Temporal Graph Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alon Eirew",
      "Kfir Bar",
      "Ido Dagan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1602": {
    "title": "Feels Feminine to Me\": Understanding Perceived Gendered Style through Human Annotations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyu Chen",
      "Neele Falk",
      "Michael Roth",
      "Agnieszka Falenska"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1603": {
    "title": "RALS: Resources and Baselines for Romanian Automatic Lexical Simplification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fabian Anghel",
      "Cristea Petru-Theodor",
      "Claudiu Creanga",
      "Sergiu Nisioi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1604": {
    "title": "How Do Social Bots Participate in Misinformation Spread? A Comprehensive Dataset and Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Herun Wan",
      "Minnan Luo",
      "Zihan Ma",
      "Guang Dai",
      "Xiang Zhao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1605": {
    "title": "Are Stereotypes Leading LLMs' Zero-Shot Stance Detection ?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anthony Dubreuil",
      "Antoine Gourru",
      "Christine Largeron",
      "Amine Trabelsi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1606": {
    "title": "Multi-Modal Framing Analysis of News",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arnav Arora",
      "Srishti Yadav",
      "Maria Antoniak",
      "Serge Belongie",
      "Isabelle Augenstein"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1607": {
    "title": "TempParaphraser: \"Heating Up\" Text to Evade AI-Text Detection through Paraphrasing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junjie Huang",
      "Ruiquan Zhang",
      "Jinsong Su",
      "Yidong Chen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1608": {
    "title": "ComicScene154: A Scene Dataset for Comic Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sandro Paval",
      "Pascal Meißner",
      "Ivan P. Yamshchikov"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1609": {
    "title": "MedLinkDE – MedDRA Entity Linking for German with Guided Chain of Thought Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roman Christof",
      "Farnaz Zeidi",
      "Manuela Messelhäußer",
      "Dirk Mentzer",
      "Renate Koenig",
      "Liam Childs",
      "Alexander Mehler"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1610": {
    "title": "HookMoE: A learnable performance compensation strategy of Mixture-of-Experts for LLM inference acceleration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Longkai",
      "Along He",
      "Mulin Li",
      "Xie Xueshuo",
      "Tao Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1611": {
    "title": "Cross-Document Cross-Lingual NLI via RST-Enhanced Graph Fusion and Interpretability Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengying Yuan",
      "WenHao Wang",
      "Zixuan Wang",
      "Yujie Huang",
      "Kangli Wei",
      "Fei Li",
      "Chong Teng",
      "Donghong Ji"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1612": {
    "title": "3R: Enhancing Sentence Representation Learning via Redundant Representation Reduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Longxuan Ma",
      "Xiao Wu",
      "Yuxin Huang",
      "Shengxiang Gao",
      "Zhengtao Yu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1613": {
    "title": "When Big Models Train Small Ones: Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhirama Subramanyam Penamakuri",
      "Navlika Singh",
      "Piyush Arora",
      "Anand Mishra"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1614": {
    "title": "ProReason: Multi-Modal Proactive Reasoning with Decoupled Eyesight and Wisdom",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingqi Zhou",
      "Sheng Wang",
      "Jingwei Dong",
      "Kai Liu",
      "Lei Li",
      "Jiahui Gao",
      "Jiyue Jiang",
      "Lingpeng Kong",
      "Chuan Wu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1615": {
    "title": "Extractive Fact Decomposition for Interpretable Natural Language Inference in one Forward Pass",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicholas Popovič",
      "Michael Färber"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1616": {
    "title": "Structure-Conditional Minimum Bayes Risk Decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bryan Eikema",
      "Anna Rutkiewicz",
      "Mario Giulianelli"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1617": {
    "title": "Label Set Optimization via Activation Distribution Kurtosis for Zero-Shot Classification with Generative Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Li",
      "Zhixue Zhao",
      "Carolina Scarton"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1618": {
    "title": "The Transfer Neurons Hypothesis: An Underlying Mechanism for Language Latent Space Transitions in Multilingual LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hinata Tezuka",
      "Naoya Inoue"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1619": {
    "title": "VEHME: A Vision-Language Model For Evaluating Handwritten Mathematics Expressions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thu Phuong Nguyen",
      "Duc M. Nguyen",
      "Hyotaek Jeon",
      "Hyunwook Lee",
      "Hyunmin Song",
      "Sungahn Ko",
      "Taehwan Kim"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1620": {
    "title": "All Roads Lead to Rome: Graph-Based Confidence Estimation for Large Language Model Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Caiqi Zhang",
      "Chang Shu",
      "Ehsan Shareghi",
      "Nigel Collier"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1621": {
    "title": "SEMMA: A Semantic Aware Knowledge Graph Foundation Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arvindh Arun",
      "Sumit Kumar",
      "Mojtaba Nayyeri",
      "Bo Xiong",
      "Ponnurangam Kumaraguru",
      "Antonio Vergari",
      "Steffen Staab"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1622": {
    "title": "Text2Vis: A Challenging and Diverse Benchmark for Generating Multimodal Visualizations from Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mizanur Rahman",
      "Md Tahmid Rahman Laskar",
      "Shafiq Joty",
      "Enamul Hoque"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1623": {
    "title": "Predicting Prosodic Boundaries for Children's Texts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mansi Dhamne",
      "Sneha Raman",
      "Preeti Rao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1624": {
    "title": "Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingwei Tan",
      "Marco Valentino",
      "Mahmud Elahi Akhter",
      "Maria Liakata",
      "Nikolaos Aletras"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1625": {
    "title": "Can Large Language Models Outperform Non-Experts in Poetry Evaluation? A Comparative Study Using the Consensual Assessment Technique",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Piotr Sawicki",
      "Marek Grzes",
      "Dan Brown",
      "Fabricio Goes"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1626": {
    "title": "Beyond Human Labels: A Multi-Linguistic Auto-Generated Benchmark for Evaluating Large Language Models on Resume Parsing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijian Ling",
      "Han Zhang",
      "Jiahao Cui",
      "Zhequn Wu",
      "Xu Sun",
      "Guohao Li",
      "Xiangjian He"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1627": {
    "title": "Orthogonal Finetuning Made Scalable",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeju Qiu",
      "Weiyang Liu",
      "Adrian Weller",
      "Bernhard Schölkopf"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1628": {
    "title": "AIR: Complex Instruction Generation via Automatic Iterative Refinement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Liu",
      "Yancheng He",
      "Yu Li",
      "Hui Huang",
      "Chengwei Hu",
      "Jiaheng Liu",
      "Shilong Li",
      "Wenbo Su",
      "Bo Zheng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1629": {
    "title": "SQUiD: Synthesizing Relational Databases from Unstructured Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mushtari Sadia",
      "Zhenning Yang",
      "Yunming Xiao",
      "Ang Chen",
      "Amrita Roy Chowdhury"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1630": {
    "title": "RAG+: Enhancing Retrieval-Augmented Generation with Application-Aware Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Wang",
      "Shiwan Zhao",
      "Zhihu Wang",
      "Ming Fan",
      "Xicheng Zhang",
      "Yubo Zhang",
      "Zhengfan Wang",
      "Heyuan Huang",
      "Ting Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1631": {
    "title": "Rapid Word Learning Through Meta In-Context Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wentao Wang",
      "Guangyuan Jiang",
      "Tal Linzen",
      "Brenden Lake"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1632": {
    "title": "EuroGEST: Investigating gender stereotypes in multilingual language models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jacqueline Rowe",
      "Mateusz Klimaszewski",
      "Liane Guillou",
      "Shannon Vallor",
      "Alexandra Birch"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1633": {
    "title": "How Persuasive Is Your Context?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tu Nguyen",
      "Kevin Du",
      "Alexander Miserlis Hoyle",
      "Ryan Cotterell"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1634": {
    "title": "The Medium Is Not the Message: Deconfounding Document Embeddings via Linear Concept Erasure",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Fan",
      "Yang Tian",
      "Shauli Ravfogel",
      "Mrinmaya Sachan",
      "Elliott Ash",
      "Alexander Miserlis Hoyle"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1635": {
    "title": "Measuring scalar constructs in social science with LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hauke Licht",
      "Rupak Sarkar",
      "Patrick Y. Wu",
      "Pranav Goel",
      "Niklas Stoehr",
      "Elliott Ash",
      "Alexander Miserlis Hoyle"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1636": {
    "title": "Text Detoxification: Data Efficiency, Semantic Preservation and Model Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Yu",
      "Yibo Zhao",
      "Jiapeng Zhu",
      "Wenming Shao",
      "Bo Pang",
      "Zhao Zhang",
      "Xiang Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1637": {
    "title": "Not What the Doctor Ordered: Surveying LLM-based De-identification and Quantifying Clinical Information Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kiana Aghakasiri",
      "Noopur Zambare",
      "JoAnn Thai",
      "Carrie Ye",
      "Mayur Mehta",
      "J Ross Mitchell",
      "Mohamed Abdalla"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1638": {
    "title": "Reasoning under Uncertainty: Efficient LLM Inference via Unsupervised Confidence Dilution and Convergent Adaptive Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenning Shi",
      "Yijia Zhu",
      "Yi Xie",
      "Junhan Shi",
      "Guorui Xie",
      "Haotian Zhang",
      "Yong Jiang",
      "Congcong Miao",
      "Qing Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1639": {
    "title": "Africa Health Check: Probing Cultural Bias in Medical LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Charles Nimo",
      "Shuheng Liu",
      "Irfan Essa",
      "Michael L. Best"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1640": {
    "title": "Assumed Identities: Quantifying Gender Bias in Machine Translation of Gender-Ambiguous Occupational Terms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Orfeas Menis Mastromichalakis",
      "Giorgos Filandrianos",
      "Maria Symeonaki",
      "Giorgos Stamou"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1641": {
    "title": "REVIVING YOUR MNEME: Predicting The Side Effects of LLM Unlearning and Fine-Tuning via Sparse Model Diffing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aly M. Kassem",
      "Zhuan Shi",
      "Negar Rostamzadeh",
      "Golnoosh Farnadi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1642": {
    "title": "ToM-SSI: Evaluating Theory of Mind in Situated Social Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matteo Bortoletto",
      "Constantin Ruhdorfer",
      "Andreas Bulling"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1643": {
    "title": "Recursive Training Loops in LLMs: How training data properties modulate distribution shift in generated data?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Grgur Kovač",
      "Jérémy Perez",
      "Rémy Portelas",
      "Peter Ford Dominey",
      "Pierre-Yves Oudeyer"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1644": {
    "title": "Detecting LLM Hallucination Through Layer-wise Information Deficiency: Analysis of Ambiguous Prompts and Unanswerable Questions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hazel Kim",
      "Tom A. Lamb",
      "Adel Bibi",
      "Philip Torr",
      "Yarin Gal"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1645": {
    "title": "Extending Automatic Machine Translation Evaluation to Book-Length Documents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kuang-Da Wang",
      "Shuoyang Ding",
      "Chao-Han Huck Yang",
      "Ping-Chun Hsieh",
      "Wen-Chih Peng",
      "Vitaly Lavrukhin",
      "Boris Ginsburg"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1646": {
    "title": "MedFact: A Large-scale Chinese Dataset for Evidence-based Medical Fact-checking of LLM Responses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tong Chen",
      "Zimu Wang",
      "Yiyi Miao",
      "Haoran Luo",
      "Sun Yuanfei",
      "Wei Wang",
      "Zhengyong Jiang",
      "Procheta Sen",
      "Jionglong Su"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1647": {
    "title": "VideoPASTA: 7K Preference Pairs That Matter for Video-LLM Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yogesh Kulkarni",
      "Pooyan Fazli"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1648": {
    "title": "Do LLMs Adhere to Label Definitions? Examining Their Receptivity to External Label Definitions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seyedali Mohammadi",
      "Bhaskara Hanuma Vedula",
      "Hemank Lamba",
      "Edward Raff",
      "Ponnurangam Kumaraguru",
      "Francis Ferraro",
      "Manas Gaur"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1649": {
    "title": "Group-Aware Reinforcement Learning for Output Diversity in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oron Anschel",
      "Alon Shoshan",
      "Adam Botach",
      "Shunit Haviv Hakimi",
      "Asaf Gendler",
      "Emanuel Ben Baruch",
      "Nadav Bhonker",
      "Igor Kviatkovsky",
      "Manoj Aggarwal",
      "Gerard Medioni"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1650": {
    "title": "Model-Based Ranking of Source Languages for Zero-Shot Cross-Lingual Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abteen Ebrahimi",
      "Adam Wiemerslage",
      "Katharina von der Wense"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1651": {
    "title": "PruneCD: Contrasting Pruned Self Model to Improve Decoding Factuality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Byeongho Yu",
      "Changhun Lee",
      "Jun-gyu Jin",
      "Eunhyeok Park"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1652": {
    "title": "Crisp: Cognitive Restructuring of Negative Thoughts through Multi-turn Supportive Dialogues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinfeng Zhou",
      "Yuxuan Chen",
      "Jianing Yin",
      "Yongkang Huang",
      "Yihan Shi",
      "Xikun Zhang",
      "Libiao Peng",
      "Rongsheng Zhang",
      "Tangjie Lv",
      "Zhipeng Hu",
      "Hongning Wang",
      "Minlie Huang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1653": {
    "title": "AccessEval: Benchmarking Disability Bias in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Srikant Panda",
      "Amit Agarwal",
      "Hitesh Laxmichand Patel"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1654": {
    "title": "The Impact of Language Mixing on Bilingual LLM Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihao Li",
      "Jiayi Xin",
      "Miranda Muqing Miao",
      "Qi Long",
      "Lyle Ungar"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1655": {
    "title": "VISaGE: Understanding Visual Generics and Exceptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stella Frank",
      "Emily Allaway"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1656": {
    "title": "Stronger Baselines for Retrieval-Augmented Generation with Long-Context Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alex Laitenberger",
      "Christopher D Manning",
      "Nelson F. Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1657": {
    "title": "Discursive Circuits: How Do Language Models Understand Discourse Relations?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yisong Miao",
      "Min-Yen Kan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1658": {
    "title": "Making VLMs More Robot-Friendly: Self-Critical Distillation of Low-Level Procedural Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chan Young Park",
      "Jillian Fisher",
      "Marius Memmel",
      "Dipika Khullar",
      "Seoho Yun",
      "Abhishek Gupta",
      "Yejin Choi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1659": {
    "title": "ThinkSLM: Towards Reasoning in Small Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaurav Srivastava",
      "Shuxiang Cao",
      "Xuan Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1660": {
    "title": "MAgICoRe: Multi-Agent, Iterative, Coarse-to-Fine Refinement for Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Justin Chen",
      "Archiki Prasad",
      "Swarnadeep Saha",
      "Elias Stengel-Eskin",
      "Mohit Bansal"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1661": {
    "title": "Batched Self-Consistency Improves LLM Relevance Assessment and Ranking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anton Korikov",
      "Pan Du",
      "Scott Sanner",
      "Navid Rekabsaz"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1662": {
    "title": "SemCSE: Semantic Contrastive Sentence Embeddings Using LLM-Generated Summaries For Scientific Abstracts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marc Felix Brinner",
      "Sina Zarrieß"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1663": {
    "title": "Controlled Generation for Private Synthetic Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihao Zhao",
      "Anjalie Field"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1664": {
    "title": "Towards AI-Assisted Psychotherapy: Emotion-Guided Generative Interventions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kilichbek Haydarov",
      "Youssef Mohamed",
      "Emilio Goldenhersch",
      "Paul OCallaghan",
      "Li-jia Li",
      "Mohamed Elhoseiny"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1665": {
    "title": "From Shortcuts to Balance: Attribution Analysis of Speech-Text Feature Utilization in Distinguishing Original from Machine-Translated Texts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongjian Chen",
      "Antonio Toral"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1666": {
    "title": "DEBATE, TRAIN, EVOLVE: Self‐Evolution of Language Model Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaurav Srivastava",
      "Zhenyu Bi",
      "Meng Lu",
      "Xuan Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1667": {
    "title": "From Chat Logs to Collective Insights: Aggregative Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wentao Zhang",
      "Woojeong Kim",
      "Yuntian Deng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1668": {
    "title": "A Text-Based Recommender System that Leverages Explicit Affective State Preferences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tonmoy Hasan",
      "Razvan Bunescu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1669": {
    "title": "CARE: Multilingual Human Preference Learning for Cultural Awareness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Geyang Guo",
      "Tarek Naous",
      "Hiromi Wakaki",
      "Yukiko Nishimura",
      "Yuki Mitsufuji",
      "Alan Ritter",
      "Wei Xu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1670": {
    "title": "Multilingual Dialogue Generation and Localization with Dialogue Act Scripting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Justin Vasselli",
      "Eunike Andriani Kardinata",
      "Yusuke Sakai",
      "Taro Watanabe"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1671": {
    "title": "SUE: Sparsity-based Uncertainty Estimation via Sparse Dictionary Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tamás Ficsor",
      "Gábor Berend"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1672": {
    "title": "Planning-Aware Code Infilling via Horizon-Length Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifeng Ding",
      "Hantian Ding",
      "Shiqi Wang",
      "Qing Sun",
      "Varun Kumar",
      "Zijian Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1673": {
    "title": "SinhalaMMLU: A Comprehensive Benchmark for Evaluating Multitask Language Understanding in Sinhala",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashmari Pramodya",
      "Nirasha Nelki",
      "Heshan Shalinda",
      "Chamila Liyanage",
      "Yusuke Sakai",
      "Randil Pushpananda",
      "Ruvan Weerasinghe",
      "Hidetaka Kamigaito",
      "Taro Watanabe"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1674": {
    "title": "OG-RAG: Ontology-grounded retrieval-augmented generation for large language models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kartik Sharma",
      "Peeyush Kumar",
      "Yunqing Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1675": {
    "title": "Convergence and Divergence of Language Models under Different Random Seeds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Finlay Fehlauer",
      "Kyle Mahowald",
      "Tiago Pimentel"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1676": {
    "title": "Analyzing and Modeling LLM Response Lengths with Extreme Value Theory: Anchoring Effects and Hybrid Distributions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liuxuan Jiao",
      "Chen Gao",
      "Yiqian Yang",
      "Chenliang Zhou",
      "YiXian Huang",
      "Xinlei Chen",
      "Yong Li"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1677": {
    "title": "Language Models Identify Ambiguities and Exploit Loopholes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jio Choi",
      "Mohit Bansal",
      "Elias Stengel-Eskin"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1678": {
    "title": "Benchmarking LLMs for Translating Classical Chinese Poetry: Evaluating Adequacy, Fluency, and Elegance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andong Chen",
      "Lianzhang Lou",
      "Kehai Chen",
      "Xuefeng Bai",
      "Yang Xiang",
      "Muyun Yang",
      "Tiejun Zhao",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1679": {
    "title": "AraEval: An Arabic Multi-Task Evaluation Suite for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alhanoof Althnian",
      "Norah A. Alzahrani",
      "Shaykhah Z. Alsubaie",
      "Eman Albilali",
      "Ahmed Abdelali",
      "Nouf M. Alotaibi",
      "M Saiful Bari",
      "Yazeed Alnumay",
      "Abdulhamed Alothaimen",
      "Maryam Saif",
      "Shahad D. Alzaidi",
      "Faisal Abdulrahman Mirza",
      "Yousef Almushayqih",
      "Mohammed Al Saleem",
      "Ghadah Alabduljabbar",
      "Abdulmohsen Al-Thubaity",
      "Areeb Alowisheq",
      "Nora Al-Twairesh"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1680": {
    "title": "QUIDS: Query Intent Description for Exploratory Search via Dual Space Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yumeng Wang",
      "Xiuying Chen",
      "Suzan Verberne"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1681": {
    "title": "A Systematic Survey of Automatic Prompt Optimization Techniques",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kiran Ramnath",
      "Kang Zhou",
      "Sheng Guan",
      "Soumya Smruti Mishra",
      "Xuan Qi",
      "Zhengyuan Shen",
      "Shuai Wang",
      "Sangmin Woo",
      "Sullam Jeoung",
      "Yawei Wang",
      "Haozhu Wang",
      "Han Ding",
      "Yuzhe Lu",
      "Zhichao Xu",
      "Yun Zhou",
      "Balasubramaniam Srinivasan",
      "Qiaojing Yan",
      "Yueyan Chen",
      "Haibo Ding",
      "Panpan Xu",
      "Lin Lee Cheong"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1682": {
    "title": "Threading the Needle: Reweaving Chain-of-Thought Reasoning to Explain Human Label Variation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Beiduo Chen",
      "Yang Janet Liu",
      "Anna Korhonen",
      "Barbara Plank"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1683": {
    "title": "MemInsight: Autonomous Memory Augmentation for LLM Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rana Salama",
      "Jason Cai",
      "Michelle Yuan",
      "Anna Currey",
      "Monica Sunkara",
      "Yi Zhang",
      "Yassine Benajiba"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1684": {
    "title": "Breaking the Noise Barrier: LLM-Guided Semantic Filtering and Enhancement for Multi-Modal Entity Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenglong Lu",
      "Chenxiao Li",
      "Jingwei Cheng",
      "Yongquan Ji",
      "Guoqing Chen",
      "Fu Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1685": {
    "title": "ImpliRet: Benchmarking the Implicit Fact Retrieval Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeinab Sadat Taghavi",
      "Ali Modarressi",
      "Yunpu Ma",
      "Hinrich Schuetze"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1686": {
    "title": "No Need for Explanations: LLMs can implicitly learn from mistakes in-context",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lisa Alazraki",
      "Maximilian Mozes",
      "Jon Ander Campos",
      "Tan Yi-Chern",
      "Marek Rei",
      "Max Bartolo"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1687": {
    "title": "MoVa: Towards Generalizable Classification of Human Morals and Values",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyu Chen",
      "Junfei Sun",
      "Chenxi Li",
      "Tuan Dung Nguyen",
      "Jing Yao",
      "Xiaoyuan Yi",
      "Xing Xie",
      "Chenhao Tan",
      "Lexing Xie"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1688": {
    "title": "GUI-Bee: Align GUI Action Grounding to Novel Environments via Autonomous Exploration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Fan",
      "Handong Zhao",
      "Ruiyi Zhang",
      "Yu Shen",
      "Xin Eric Wang",
      "Gang Wu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1689": {
    "title": "Revealing and Mitigating the Challenge of Detecting Character Knowledge Errors in LLM Role-Playing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenyuan Zhang",
      "Shuaiyi Nie",
      "Jiawei Sheng",
      "Zefeng Zhang",
      "Xinghua Zhang",
      "Yongquan He",
      "Tingwen Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1690": {
    "title": "Taking Notes Brings Focus? Towards Multi-Turn Multimodal Dialogue Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiazheng Liu",
      "Sipeng Zheng",
      "Börje F. Karlsson",
      "Zongqing Lu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1691": {
    "title": "Graph-Based Multi-Trait Essay Scoring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengjie Li",
      "Vincent Ng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1692": {
    "title": "Benchmarking LLMs on Semantic Overlap Summarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "John Salvador",
      "Naman Bansal",
      "Mousumi Akter",
      "Souvika Sarkar",
      "Anupam Das",
      "Santu Karmaker"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1693": {
    "title": "N-CORE: N-View Consistency Regularization for Disentangled Representation Learning in Nonverbal Vocalizations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siddhant Bikram Shah",
      "Kristina T. Johnson"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1694": {
    "title": "Probability Distribution Collapse: A Critical Bottleneck to Compact Unsupervised Neural Grammar Induction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinwook Park",
      "Kangil Kim"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1695": {
    "title": "Spatial Layouts in News Homepages Capture Human Preferences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Spangher",
      "Michael Vu",
      "Arda Kaz",
      "Naitian Zhou",
      "Ben Welsh"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1696": {
    "title": "KRETA: A Benchmark for Korean Reading and Reasoning in Text-Rich VQA Attuned to Diverse Visual Contexts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taebaek Hwang",
      "Minseo Kim",
      "Gisang Lee",
      "Seonuk Kim",
      "Hyunjun Eun"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1697": {
    "title": "ReflAct: World-Grounded Decision Making in LLM Agents via Goal-State Reflection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeonghye Kim",
      "Sojeong Rhee",
      "Minbeom Kim",
      "Dohyung Kim",
      "Sangmook Lee",
      "Youngchul Sung",
      "Kyomin Jung"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1698": {
    "title": "CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and Outcome Reward",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shudong Liu",
      "Hongwei Liu",
      "Junnan Liu",
      "Linchen Xiao",
      "Songyang Gao",
      "Chengqi Lyu",
      "Yuzhe Gu",
      "Wenwei Zhang",
      "Derek F. Wong",
      "Songyang Zhang",
      "Kai Chen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1699": {
    "title": "A Knowledge-driven Adaptive Collaboration of LLMs for Enhancing Medical Decision-making",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Wu",
      "Ting-Zhu Huang",
      "Liang-Jian Deng",
      "Yanyuan Qiao",
      "Imran Razzak",
      "Yutong Xie"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1700": {
    "title": "Castle: Causal Cascade Updates in Relational Databases with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongye Su",
      "Yucheng Zhang",
      "Zeru Shi",
      "Bruno Ribeiro",
      "Elisa Bertino"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1701": {
    "title": "Idiosyncratic Versus Normative Modeling of Atypical Speech Recognition: Dysarthric Case Studies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vishnu Raja",
      "Adithya V Ganesan",
      "Anand Syamkumar",
      "Ritwik Banerjee",
      "H. Schwartz"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1702": {
    "title": "NESTFUL: A Benchmark for Evaluating LLMs on Nested Sequences of API Calls",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kinjal Basu",
      "Ibrahim Abdelaziz",
      "Kiran Kate",
      "Mayank Agarwal",
      "Maxwell Crouse",
      "Yara Rizk",
      "Kelsey Bradford",
      "Asim Munawar",
      "Sadhana Kumaravel",
      "Saurabh Goyal",
      "Xin Wang",
      "Luis A. Lastras",
      "Pavan Kapanipathi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1703": {
    "title": "Benchmarking and Mitigating MCQA Selection Bias of Large Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md. Atabuzzaman",
      "Ali Asgarov",
      "Chris Thomas"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1704": {
    "title": "Can Large Language Models Unlock Novel Scientific Research Ideas?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sandeep Kumar",
      "Tirthankar Ghosal",
      "Vinayak Goyal",
      "Asif Ekbal"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1705": {
    "title": "Word Salad Chopper: Reasoning Models Waste A Ton Of Decoding Budget On Useless Repetitions, Self-Knowingly",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenya Xie",
      "Shaochen Zhong",
      "Hoang Anh Duy Le",
      "Zhaozhuo Xu",
      "Jianwen Xie",
      "Zirui Liu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1706": {
    "title": "DIWALI - Diversity and Inclusivity aWare cuLture specific Items for India: Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian Context",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pramit Sahoo",
      "Maharaj Brahma",
      "Maunendra Sankar Desarkar"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1707": {
    "title": "SYNC: A Synthetic Long-Context Understanding Benchmark for Controlled Comparisons of Model Capabilities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuyang Cao",
      "Kaijian Zou",
      "Lu Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1708": {
    "title": "OpenNER 1.0: Standardized Open-Access Named Entity Recognition Datasets in 50+ Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chester Palen-Michel",
      "Maxwell Pickering",
      "Maya Kruse",
      "Jonne Sälevä",
      "Constantine Lignos"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1709": {
    "title": "Mondrian: A Framework for Logical Abstract (Re)Structuring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elizabeth Grace Orwig",
      "Shinwoo Park",
      "Hyundong Jin",
      "Yo-Sub Han"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1710": {
    "title": "Case-Based Decision-Theoretic Decoding with Quality Memories",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hiroyuki Deguchi",
      "Masaaki Nagata"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1711": {
    "title": "PRIME: Large Language Model Personalization with Cognitive Dual-Memory and Personalized Thought Process",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinliang Frederick Zhang",
      "Nicholas Beauchamp",
      "Lu Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1712": {
    "title": "Mechanisms vs. Outcomes: Probing for Syntax Fails to Explain Performance on Targeted Syntactic Evaluations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ananth Agarwal",
      "Jasper Jian",
      "Christopher D Manning",
      "Shikhar Murty"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1713": {
    "title": "Image Difference Captioning via Adversarial Preference Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihan Huang",
      "Junda Wu",
      "Rohan Surana",
      "Tong Yu",
      "David Arbour",
      "Ritwik Sinha",
      "Julian McAuley"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1714": {
    "title": "seqBench: A Tunable Benchmark to Quantify Sequential Reasoning Limits of LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Ramezanali",
      "Mo Vazifeh",
      "Paolo Santi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1715": {
    "title": "NormGenesis: Multicultural Dialogue Generation via Exemplar-Guided Social Norm Modeling and Violation Recovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minki Hong",
      "Jangho Choi",
      "Jihie Kim"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1716": {
    "title": "SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle Generation from SAT Formulas",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anjiang Wei",
      "Yuheng Wu",
      "Yingjia Wan",
      "Tarun Suresh",
      "Huanmi Tan",
      "Zhanke Zhou",
      "Sanmi Koyejo",
      "Ke Wang",
      "Alex Aiken"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1717": {
    "title": "Data Descriptions from Large Language Models with Influence Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaeri Kim",
      "Jaeyeon Bae",
      "Taehwan Kim"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1718": {
    "title": "EquiBench: Benchmarking Large Language Models' Reasoning about Program Semantics via Equivalence Checking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anjiang Wei",
      "Jiannan Cao",
      "Ran Li",
      "Hongyu Chen",
      "Yuhui Zhang",
      "Ziheng Wang",
      "Yuan Liu",
      "Thiago S. F. X. Teixeira",
      "Diyi Yang",
      "Ke Wang",
      "Alex Aiken"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1719": {
    "title": "MicroEdit: Neuron-level Knowledge Disentanglement and Localization in Lifelong Model Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiqi Wang",
      "Qi Wang",
      "Runliang Niu",
      "He Kong",
      "Yi Chang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1720": {
    "title": "Do Large Language Models Understand Word Senses?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Domenico Meconi",
      "Simone Stirpe",
      "Federico Martelli",
      "Leonardo Lavalle",
      "Roberto Navigli"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1721": {
    "title": "Diverse, not Short: A Length-Controlled Data Selection Strategy for Improving Response Diversity of Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vijeta Deshpande",
      "Debasmita Ghose",
      "John D Patterson",
      "Roger E. Beaty",
      "Anna Rumshisky"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1722": {
    "title": "Uncovering the Bigger Picture: Comprehensive Event Understanding Via Diverse News Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixuan Tang",
      "Yuanyuan Shi",
      "Yiqun Sun",
      "Anthony Kum Hoe Tung"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1723": {
    "title": "Personalized LLM Decoding via Contrasting Personal Preference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyungjune Bu",
      "ChanJoo Jung",
      "Minjae Kang",
      "Jaehyung Kim"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1724": {
    "title": "The Missing Parts: Augmenting Fact Verification with Half Truth Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixuan Tang",
      "Jincheng Wang",
      "Anthony Kum Hoe Tung"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1725": {
    "title": "Toward Machine Translation Literacy: How Lay Users Perceive and Rely on Imperfect Translations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yimin Xiao",
      "Yongle Zhang",
      "Dayeon Ki",
      "Calvin Bao",
      "Marianna J. Martindale",
      "Charlotte Vaughn",
      "Ge Gao",
      "Marine Carpuat"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1726": {
    "title": "Personalization up to a Point: Why Personalized Content Moderation Needs Boundaries, and How We Can Enforce Them",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emanuele Moscato",
      "Tiancheng Hu",
      "Matthias Orlikowski",
      "Paul Röttger",
      "Debora Nozza"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1727": {
    "title": "MPCG: Multi-Round Persona-Conditioned Generation for Modeling the Evolution of Misinformation with LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chong Jun Rong Brian",
      "Yixuan Tang",
      "Anthony Kum Hoe Tung"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1728": {
    "title": "LiTEx: A Linguistic Taxonomy of Explanations for Understanding Within-Label Variation in Natural Language Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pingjun Hong",
      "Beiduo Chen",
      "Siyao Peng",
      "Marie-Catherine de Marneffe",
      "Barbara Plank"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1729": {
    "title": "LiteraryQA: Towards Effective Evaluation of Long-document Narrative QA",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tommaso Bonomo",
      "Luca Gioffré",
      "Roberto Navigli"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1730": {
    "title": "FillerSpeech: Towards Human-Like Text-to-Speech Synthesis with Filler Insertion and Filler Style Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seung-Bin Kim",
      "Jun-Hyeok Cha",
      "Hyung-Seok Oh",
      "Heejin Choi",
      "Seong-Whan Lee"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1731": {
    "title": "Multi-LMentry: Can Multilingual LLMs Solve Elementary Tasks Across Languages?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luca Moroni",
      "Javier Aula-Blasco",
      "Simone Conia",
      "Irene Baucells",
      "Naiara Perez",
      "Silvia Paniagua Suárez",
      "Anna Sallés",
      "Malte Ostendorff",
      "Júlia Falcão",
      "Guijin Son",
      "Aitor Gonzalez-Agirre",
      "Roberto Navigli",
      "Marta Villegas"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1732": {
    "title": "Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via Pseudo Query",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixuan Wang",
      "Shiyu Ji",
      "Yijun Liu",
      "Yuzhuang Xu",
      "Yang Xu",
      "Qingfu Zhu",
      "Wanxiang Che"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1733": {
    "title": "PerspectiveMod: A Perspectivist Resource for Deliberative Moderation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eva Maria Vecchi",
      "Neele Falk",
      "Carlotta Quensel",
      "Iman Jundi",
      "Gabriella Lapesa"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1734": {
    "title": "LoCt-Instruct: An Automatic Pipeline for Constructing Datasets of Logical Continuous Instructions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyu Sun",
      "Yusuke Sakai",
      "Haruki Sakajo",
      "Shintaro Ozaki",
      "Kazuki Hayashi",
      "Hidetaka Kamigaito",
      "Taro Watanabe"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1735": {
    "title": "CodeSSM: Towards State Space Models for Code Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shweta Verma",
      "Abhinav Anand",
      "Mira Mezini"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1736": {
    "title": "EduAdapt: A Question Answer Benchmark Dataset for Evaluating Grade-Level Adaptability in LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Numaan Naeem",
      "Abdellah El Mekki",
      "Muhammad Abdul-Mageed"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1737": {
    "title": "xCoRe: Cross-context Coreference Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giuliano Martinelli",
      "Bruno Gatti",
      "Roberto Navigli"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1738": {
    "title": "Retrieval-Augmented Generation with Estimation of Source Reliability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeongyeon Hwang",
      "Junyoung Park",
      "Hyejin Park",
      "Dongwoo Kim",
      "Sangdon Park",
      "Jungseul Ok"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1739": {
    "title": "NitiBench: Benchmarking LLM Frameworks on Thai Legal Question Answering Capabilities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pawitsapak Akarajaradwong",
      "Pirat Pothavorn",
      "Chompakorn Chaksangchaichot",
      "Panuthep Tasawong",
      "Thitiwat Nopparatbundit",
      "Keerakiat Pratai",
      "Sarana Nutanong"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1740": {
    "title": "From Input Perception to Predictive Insight: Modeling Model Blind Spots Before They Become Errors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maggie Mi",
      "Aline Villavicencio",
      "Nafise Sadat Moosavi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1741": {
    "title": "WojoodRelations: Arabic Relation Extraction Corpus and Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alaa Aljabari",
      "Mohammed Khalilia",
      "Mustafa Jarrar"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1742": {
    "title": "Conflicting Needles in a Haystack: How LLMs behave when faced with contradictory information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Murathan Kurfali",
      "Robert Östling"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1743": {
    "title": "Towards Event Extraction with Massive Types: LLM-based Collaborative Annotation and Partitioning Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenxuan Liu",
      "Zixuan Li",
      "Long Bai",
      "Yuxin Zuo",
      "Daozhu Xu",
      "Xiaolong Jin",
      "Jiafeng Guo",
      "Xueqi Cheng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1744": {
    "title": "Liaozhai through the Looking-Glass: On Paratextual Explicitation of Culture-Bound Terms in Machine Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sherrie Shen",
      "Weixuan Wang",
      "Alexandra Birch"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1745": {
    "title": "Concept-pedia: a Wide-coverage Semantically-annotated Multimodal Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karim Ghonim",
      "Andrei Stefan Bejgu",
      "Alberte Fernández-Castro",
      "Roberto Navigli"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1746": {
    "title": "RAED: Retrieval-Augmented Entity Description Generation for Emerging Entity Linking and Disambiguation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karim Ghonim",
      "Pere-Lluís Huguet Cabot",
      "Riccardo Orlando",
      "Roberto Navigli"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1747": {
    "title": "Personalized Language Models via Privacy-Preserving Evolutionary Model Merging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyuyoung Kim",
      "Jinwoo Shin",
      "Jaehyung Kim"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1748": {
    "title": "Aligning Text/Speech Representations from Multimodal Models with MEG Brain Activity During Listening",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Padakanti Srijith",
      "Khushbu Pahwa",
      "Radhika Mamidi",
      "Bapi Raju Surampudi",
      "Manish Gupta",
      "Subba Reddy Oota"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1749": {
    "title": "STARQA: A Question Answering Dataset for Complex Analytical Reasoning over Structured Databases",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mounica Maddela",
      "Lingjue Xie",
      "Daniel Preotiuc-Pietro",
      "Mausam"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1750": {
    "title": "Slim-SC: Thought Pruning for Efficient Scaling with Self-Consistency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Colin Hong",
      "Xu Guo",
      "Anand Chaanan Singh",
      "Esha Choukse",
      "Dmitrii Ustiugov"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1751": {
    "title": "Long Chain-of-Thought Fine-tuning via Understanding-to-Reasoning Transition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenxin An",
      "Zhihui Xie",
      "Xiaonan Li",
      "Ming Zhong",
      "Shansan Gong",
      "Lei Li",
      "Jun Zhang",
      "Jingjing Xu",
      "Lingpeng Kong"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1752": {
    "title": "Exploring Large Language Models for Detecting Mental Disorders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gleb Kuzmin",
      "Petr Strepetov",
      "Maksim Stankevich",
      "Natalia Chudova",
      "Artem Shelmanov",
      "Ivan Smirnov"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1753": {
    "title": "Efficient Real-time Refinement of Language Model Text Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joonho Ko",
      "Jinheon Baek",
      "Sung Ju Hwang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1754": {
    "title": "Reward-Weighted Sampling: Enhancing Non-Autoregressive Characteristics in Masked Diffusion LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daehoon Gwak",
      "Minseo Jung",
      "Junwoo Park",
      "Minho Park",
      "ChaeHun Park",
      "Junha Hyung",
      "Jaegul Choo"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1755": {
    "title": "AI Argues Differently: Distinct Argumentative and Linguistic Patterns of LLMs in Persuasive Contexts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Esra Dönmez",
      "Maximilian Maurer",
      "Gabriella Lapesa",
      "Agnieszka Falenska"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1756": {
    "title": "TounsiBench: Benchmarking Large Language Models for Tunisian Arabic",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Souha Ben Hassine",
      "Asma Arrak",
      "Marouene Addhoum",
      "Steven R Wilson"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1757": {
    "title": "Moral Framing in Politics (MFiP): A new resource and models for moral framing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ines Rehbein",
      "Ines Reinig",
      "Simone Paolo Ponzetto"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1758": {
    "title": "ReDepress: A Cognitive Framework for Detecting Depression Relapse from Social Media",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aakash Kumar Agarwal",
      "Saprativa Bhattacharjee",
      "Mauli Rastogi",
      "Jemima S. Jacob",
      "Biplab Banerjee",
      "Rashmi Gupta",
      "Pushpak Bhattacharyya"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1759": {
    "title": "iKnow-audio: Integrating Knowledge Graphs with Audio-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michel Olvera",
      "Changhong Wang",
      "Paraskevas Stamatiadis",
      "Gaël Richard",
      "Slim Essid"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1760": {
    "title": "EduVidQA: Generating and Evaluating Long-form Answers to Student Questions based on Lecture Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sourjyadip Ray",
      "Shubham Sharma",
      "Somak Aditya",
      "Pawan Goyal"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1761": {
    "title": "The Illusion of Progress: Re-evaluating Hallucination Detection in LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Denis Janiak",
      "Jakub Binkowski",
      "Albert Sawczyn",
      "Bogdan Gabrys",
      "Ravid Shwartz-Ziv",
      "Tomasz Jan Kajdanowicz"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1762": {
    "title": "Turning Logic Against Itself: Probing Model Defenses Through Contrastive Questions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rachneet Singh Sachdeva",
      "Rima Hazra",
      "Iryna Gurevych"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1763": {
    "title": "CHURRO: Making History Readable with an Open-Weight Large Vision-Language Model for High-Accuracy, Low-Cost Historical Text Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sina Semnani",
      "Han Zhang",
      "Xinyan He",
      "Merve Tekgurler",
      "Monica Lam"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1764": {
    "title": "Towards Author-informed NLP: Mind the Social Bias",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Inbar Pendzel",
      "Einat Minkov"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1765": {
    "title": "Detecting Corpus-Level Knowledge Inconsistencies in Wikipedia with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sina Semnani",
      "Jirayu Burapacheep",
      "Arpandeep Khatua",
      "Thanawan Atchariyachanvanit",
      "Zheng Wang",
      "Monica Lam"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1766": {
    "title": "Leveraging Multilingual Training for Authorship Representation: Enhancing Generalization across Languages and Domains",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junghwan Kim",
      "Haotian Zhang",
      "David Jurgens"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1767": {
    "title": "DrFrattn: Directly Learn Adaptive Policy from Attention for Simultaneous Machine Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Libo Zhao",
      "Jing Li",
      "Ziqian Zeng"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1768": {
    "title": "The Sound of Syntax: Finetuning and Comprehensive Evaluation of Language Models for Speech Pathology",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fagun Patel",
      "Duc Quang Nguyen",
      "Sang T. Truong",
      "Jody Vaynshtok",
      "Sanmi Koyejo",
      "Nick Haber"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1769": {
    "title": "NormXLogit: The Head-on-Top Never Lies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sina Abbasi",
      "Mohammad Reza Modarres",
      "Mohammad Taher Pilehvar"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1770": {
    "title": "Doc2Chart: Intent-Driven Zero-Shot Chart Generation from Documents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akriti Jain",
      "Pritika Ramu",
      "Aparna Garimella",
      "Apoorv Saxena"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1771": {
    "title": "Breaking Agents: Compromising Autonomous LLM Agents Through Malfunction Amplification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boyang Zhang",
      "Yicong Tan",
      "Yun Shen",
      "Ahmed Salem",
      "Michael Backes",
      "Savvas Zannettou",
      "Yang Zhang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1772": {
    "title": "FoREST: Frame of Reference Evaluation in Spatial Reasoning Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tanawan Premsri",
      "Parisa Kordjamshidi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1773": {
    "title": "Multilinguality Does not Make Sense: Investigating Factors Behind Zero-Shot Cross-Lingual Transfer in Sense-Aware Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roksana Goworek",
      "Haim Dubossarsky"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1774": {
    "title": "Translating Domain-Specific Terminology in Typologically-Diverse Languages: A Study in Tax and Financial Education",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arturo Oncevay",
      "Elena Kochkina",
      "Keshav Ramani",
      "Toyin Aguda",
      "Simerjot Kaur",
      "Charese Smiley"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1775": {
    "title": "Train It and Forget It: Merge Lists are Unnecessary for BPE Inference in Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tomohiro Sawada",
      "Kartik Goyal"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1776": {
    "title": "Spectral Scaling Laws in Language Models: emphHow Effectively Do Feed-Forward Networks Use Their Latent Space?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nandan Kumar Jha",
      "Brandon Reagen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1777": {
    "title": "TLUE: A Tibetan Language Understanding Evaluation Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Gao",
      "Cheng Huang",
      "Yutong Liu",
      "Nyima Tashi",
      "Xiangxiang Wang",
      "Thupten Tsering",
      "Ban Ma-bao",
      "Renzeng Duojie",
      "Gadeng Luosang",
      "Rinchen Dongrub",
      "Dorje Tashi",
      "Xiao Feng Cd",
      "Yongbin Yu",
      "Hao Wang"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1778": {
    "title": "Retrieving Support to Rank Answers in Open-Domain Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyu Zhang",
      "Alessandro Moschitti",
      "Thuy Vu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1779": {
    "title": "Trojsten Benchmark: Evaluating LLM Problem-Solving in Slovak STEM Competition Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adam Zahradník",
      "Marek Suppa"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1780": {
    "title": "BRSpeech-DF: A Deep Fake Synthetic Speech Dataset for Portuguese Zero-Shot TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexandre Costa Ferro Filho",
      "Rafaello Virgilli",
      "Lucas Alcantara Souza",
      "F S de Oliveira",
      "Marcelo Henrique Lopes Ferreira",
      "Daniel Tunnermann",
      "Gustavo Dos Reis Oliveira",
      "Anderson Da Silva Soares",
      "Arlindo Rodrigues Galvão Filho"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1781": {
    "title": "A Simple Yet Effective Method for Non-Refusing Context Relevant Fine-grained Safety Steering in LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaona Ghosh",
      "Amrita Bhattacharjee",
      "Yftah Ziser",
      "Christopher Parisien"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1782": {
    "title": "Statistical and Neural Methods for Hawaiian Orthography Modernization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaden Kapali",
      "Keaton Williamson",
      "Winston Wu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1783": {
    "title": "so much depends / upon / a whitespace: Why Whitespace Matters for Poets and LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sriharsh Bhyravajjula",
      "Melanie Walsh",
      "Anna Preus",
      "Maria Antoniak"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1784": {
    "title": "Certified Mitigation of Worst-Case LLM Copyright Infringement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyu Zhang",
      "Jiacan Yu",
      "Marc Marone",
      "Benjamin Van Durme",
      "Daniel Khashabi"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1785": {
    "title": "Quantifying Logical Consistency in Transformers via Query-Key Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eduard Tulchinskii",
      "Laida Kushnareva",
      "Anastasia Voznyuk",
      "Andrei Andriiainen",
      "Irina Piontkovskaya",
      "Evgeny Burnaev",
      "Serguei Barannikov"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1786": {
    "title": "SimulatorArena: Are User Simulators Reliable Proxies for Multi-Turn Evaluation of AI Assistants?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yao Dou",
      "Michel Galley",
      "Baolin Peng",
      "Chris Kedzie",
      "Weixin Cai",
      "Alan Ritter",
      "Chris Quirk",
      "Wei Xu",
      "Jianfeng Gao"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1787": {
    "title": "CourtReasoner: Can LLM Agents Reason Like Judges?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sophia Simeng Han",
      "Yoshiki Takashima",
      "Shannon Zejiang Shen",
      "Chen Liu",
      "Yixin Liu",
      "Roque K. Thuo",
      "Sonia Knowlton",
      "Ruzica Piskac",
      "Scott J Shapiro",
      "Arman Cohan"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1788": {
    "title": "Not Your Typical Government Tipline: LLM-Assisted Routing of Environmental Protection Agency Citizen Tips",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sharanya Majumder",
      "Zehua Li",
      "Derek Ouyang",
      "Kit T Rodolfa",
      "Elena Eneva",
      "Julian Nyarko",
      "Daniel E. Ho"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1789": {
    "title": "Retracing the Past: LLMs Emit Training Data When They Get Lost",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Myeongseob Ko",
      "Nikhil Reddy Billa",
      "Adam Nguyen",
      "Charles Fleming",
      "Ming Jin",
      "Ruoxi Jia"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1790": {
    "title": "Layer-wise Minimal Pair Probing Reveals Contextual Grammatical-Conceptual Hierarchy in Speech Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linyang He",
      "Qiaolin Wang",
      "Xilin Jiang",
      "Nima Mesgarani"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1791": {
    "title": "Current Semantic-change Quantification Methods Struggle with Semantic Change Discovery in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Khonzoda Umarova",
      "Lillian Lee",
      "Laerdon Kim"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1792": {
    "title": "Evaluating Large Language Models for Detecting Antisemitism",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jay Patel",
      "Hrudayangam Mehta",
      "Jeremy Blackburn"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1793": {
    "title": "D-RAG: Differentiable Retrieval-Augmented Generation for Knowledge Graph Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangze Gao",
      "Zixuan Li",
      "Chunfeng Yuan",
      "Jiawei Li",
      "Wu Jianzhuo",
      "Yuehao Zhang",
      "Xiaolong Jin",
      "Bing Li",
      "Weiming Hu"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1794": {
    "title": "Towards Robust Mathematical Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thang Luong",
      "Dawsen Hwang",
      "Hoang H Nguyen",
      "Golnaz Ghiasi",
      "Yuri Chervonyi",
      "Insuk Seo",
      "Junsu Kim",
      "Garrett Bingham",
      "Jonathan Lee",
      "Swaroop Mishra",
      "Alex Zhai",
      "Huiyi Hu",
      "Henryk Michalewski",
      "Jimin Kim",
      "Jeonghyun Ahn",
      "Junhwi Bae",
      "Xingyou Song",
      "Trieu Hoang Trinh",
      "Quoc V Le",
      "Junehyuk Jung"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1795": {
    "title": "Table-LLM-Specialist: Language Model Specialists for Tables using Iterative Fine-tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junjie Xing",
      "Yeye He",
      "Mengyu Zhou",
      "Haoyu Dong",
      "Shi Han",
      "Dongmei Zhang",
      "Surajit Chaudhuri"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1796": {
    "title": "Introducing Spotlight: A Novel Approach for Generating Captivating Key Information from Documents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ankan Mullick",
      "Sombit Bose",
      "Rounak Saha",
      "Ayan Kumar Bhowmick",
      "Aditya Vempaty",
      "Prasenjit Dey",
      "Ravi Kokku",
      "Pawan Goyal",
      "Niloy Ganguly"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1797": {
    "title": "Argument Summarization and its Evaluation in the Era of Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Moritz Altemeyer",
      "Steffen Eger",
      "Johannes Daxenberger",
      "Yanran Chen",
      "Tim Altendorf",
      "Philipp Cimiano",
      "Benjamin Schiller"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1798": {
    "title": "Computational Analysis of Conversation Dynamics through Participant Responsivity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Margaret Hughes",
      "Brandon Roy",
      "Elinor Poole-Dayan",
      "Deb Roy",
      "Jad Kabbara"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1799": {
    "title": "AMQ: Enabling AutoML for Mixed-precision Weight-Only Quantization of Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sangjun Lee",
      "Seung-taek Woo",
      "Jun-gyu Jin",
      "Changhun Lee",
      "Eunhyeok Park"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1800": {
    "title": "Beyond Averages: Learning with Annotator Disagreement in STS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alejandro Benito-Santos",
      "Adrian Ghajari"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1801": {
    "title": "Dipper: Diversity in Prompts for Producing Large Language Model Ensembles in Reasoning Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenyang Hu",
      "Gregory Kang Ruey Lau",
      "Liu Diwen",
      "Chen Jizhuo",
      "See-Kiong Ng",
      "Bryan Kian Hsiang Low"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1802": {
    "title": "Constrained Non-negative Matrix Factorization for Guided Topic Modeling of Minority Topics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seyedeh Fatemeh Ebrahimi",
      "Jaakko Peltonen"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1803": {
    "title": "Which Word Orders Facilitate Length Generalization in LMs? An Investigation with GCG-Based Artificial Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nadine El-Naggar",
      "Tatsuki Kuribayashi",
      "Ted Briscoe"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1804": {
    "title": "Training compute-optimal transformer encoder models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Megi Dervishi",
      "Alexandre Allauzen",
      "Gabriel Synnaeve",
      "Yann LeCun"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1805": {
    "title": "Mind the Blind Spots: A Focus-Level Evaluation Framework for LLM Reviews",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyungyu Shin",
      "Jingyu Tang",
      "Yoonjoo Lee",
      "Nayoung Kim",
      "Hyunseung Lim",
      "Ji Yong Cho",
      "Hwajung Hong",
      "Moontae Lee",
      "Juho Kim"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1806": {
    "title": "Seeing Through Words, Speaking Through Pixels: Deep Representational Alignment Between Vision and Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zoe Wanying He",
      "Sean Trott",
      "Meenakshi Khosla"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1807": {
    "title": "Unconditional Truthfulness: Learning Unconditional Uncertainty of Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Artem Vazhentsev",
      "Ekaterina Fadeeva",
      "Rui Xing",
      "Gleb Kuzmin",
      "Ivan Lazichny",
      "Alexander Panchenko",
      "Preslav Nakov",
      "Timothy Baldwin",
      "Maxim Panov",
      "Artem Shelmanov"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1808": {
    "title": "Chinese Toxic Language Mitigation via Sentiment Polarity Consistent Rewrites",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xintong Wang",
      "Yixiao Liu",
      "Jingheng Pan",
      "Liang Ding",
      "Longyue Wang",
      "Chris Biemann"
    ]
  },
  "https://aclanthology.org/2025.emnlp-main.1809": {
    "title": "A Head to Predict and a Head to Question: Pre-trained Uncertainty Quantification Heads for Hallucination Detection in LLM Outputs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Artem Shelmanov",
      "Ekaterina Fadeeva",
      "Akim Tsvigun",
      "Ivan Tsvigun",
      "Zhuohan Xie",
      "Igor Kiselev",
      "Nico Daheim",
      "Caiqi Zhang",
      "Artem Vazhentsev",
      "Mrinmaya Sachan",
      "Preslav Nakov",
      "Timothy Baldwin"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1": {
    "title": "Automating Alternative Generation in Decision-Making",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yevhen Kostiuk",
      "Clara Seyfried",
      "Chris Reed"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.2": {
    "title": "Bias Analysis and Mitigation through Protected Attribute Detection and Regard Classification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Takuma Udagawa",
      "Yang Zhao",
      "Hiroshi Kanayama",
      "Bishwaranjan Bhattacharjee"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.3": {
    "title": "Large Language Models Might Not Care What You Are Saying: Prompt Format Beats Descriptions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenming Tang",
      "Zhixiang Wang",
      "Hao Sun",
      "Yunfang Wu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.4": {
    "title": "Boundary Matters: Leveraging Structured Text Plots for Long Text Outline Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanchi Ma",
      "Jiamou Liu",
      "Hui He",
      "Libo Zhang",
      "Haoyuan Li",
      "Zhendong Niu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.5": {
    "title": "Can Large Language Models Personalize Dialogues to Generational Styles?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pier Felice Balestrucci",
      "Ondrej Dusek",
      "Luca Anselma",
      "Alessandro Mazzei"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.6": {
    "title": "Toward Optimal LLM Alignments Using Two-Player Games",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Zheng",
      "Hongyi Guo",
      "Zhihan Liu",
      "Xiaoying Zhang",
      "Yuanshun Yao",
      "Xiaojun Xu",
      "Zhaoran Wang",
      "Zhiheng Xi",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang",
      "Yang Liu",
      "Hang Li"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.7": {
    "title": "Structural Patent Classification Using Label Hierarchy Optimization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengting Gui",
      "Shufeng Hao",
      "Chongyang Shi",
      "Qi Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.8": {
    "title": "Exploring Hyperbolic Hierarchical Structure for Multimodal Rumor Detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Mahbubur Rahman",
      "Shufeng Hao",
      "Chongyang Shi",
      "An Lao",
      "Jinyan Liu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.9": {
    "title": "Multi-Surrogate-Objective Optimization for Neural Topic Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tue Le",
      "Hoang Tran Vuong",
      "Tung Nguyen",
      "Linh Ngo Van",
      "Dinh Viet Sang",
      "Trung Le",
      "Thien Huu Nguyen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.10": {
    "title": "How Diversely Can Language Models Solve Problems? Exploring the Algorithmic Diversity of Model-Generated Code",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seonghyeon Lee",
      "HeeJae Chon",
      "Joonwon Jang",
      "Dongha Lee",
      "Hwanjo Yu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.11": {
    "title": "ReAL: How Can LLMs Simulate the Real Teacher? Retrieval-enhanced Agent for Adaptive Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Lv",
      "Qi Liu",
      "Weibo Gao",
      "Jiatong Li",
      "Kai Zhang",
      "Shiwei Tong"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.12": {
    "title": "LLMsPark: A Benchmark for Evaluating Large Language Models in Strategic Gaming Contexts",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junhao Chen",
      "Jingbo Sun",
      "Xiang Li",
      "Haidong Xin",
      "Yuhao Xue",
      "Yibin Xu",
      "Hao Zhao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.13": {
    "title": "Versatile Framework for Song Generation with Prompt-based Control",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Zhang",
      "Wenxiang Guo",
      "Changhao Pan",
      "Zhiyuan Zhu",
      "Ruiqi Li",
      "Jingyu Lu",
      "Rongjie Huang",
      "Ruiyuan Zhang",
      "Zhiqing Hong",
      "Ziyue Jiang",
      "Zhou Zhao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.14": {
    "title": "InsBank: Evolving Instruction Subset for Ongoing Alignment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayi Shi",
      "Yiwei Li",
      "Shaoxiong Feng",
      "Peiwen Yuan",
      "Xinglin Wang",
      "Yueqi Zhang",
      "Chuyi Tan",
      "Boyuan Pan",
      "Huan Ren",
      "Yao Hu",
      "Kan Li"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.15": {
    "title": "TL-Training: A Task-Feature-Based Framework for Training Large Language Models in Tool Use",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junjie Ye",
      "Yilong Wu",
      "Sixian Li",
      "Yuming Yang",
      "Zhiheng Xi",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang",
      "Peng Wang",
      "Zhongchao Shi",
      "Jianping Fan",
      "Zhengyin Du"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.16": {
    "title": "DCMKC: A Dual Consistency Matching Approach for Multi-hop Question Answering in LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyi Wang",
      "Yiping Song",
      "Chang Liu",
      "Tingjin Luo",
      "Bo Liu",
      "Zheng Xie",
      "Minlie Huang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.17": {
    "title": "On Domain-Adaptive Post-Training for Multimodal Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daixuan Cheng",
      "Shaohan Huang",
      "Ziyu Zhu",
      "Xintong Zhang",
      "Xin Zhao",
      "Zhongzhi Luan",
      "Bo Dai",
      "Zhenliang Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.18": {
    "title": "CPO: Addressing Reward Ambiguity in Role-playing Dialogue via Comparative Policy Optimization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Ye",
      "Rui Wang",
      "Yuchuan Wu",
      "Victor Ma",
      "Feiteng Fang",
      "Fei Huang",
      "Yongbin Li"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.19": {
    "title": "SPPD: Self-training with Process Preference Learning Using Dynamic Value Margin",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Yi",
      "Qingyang Li",
      "Yulan Hu",
      "Fuzheng Zhang",
      "Di Zhang",
      "Yong Liu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.20": {
    "title": "Error Classification of Large Language Models on Math Word Problems: A Dynamically Adaptive Framework",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhangyue Yin",
      "YuHong Sun",
      "Xuanjing Huang",
      "Xipeng Qiu",
      "Hui Zhao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.21": {
    "title": "sudoLLM: On Multi-role Alignment of Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soumadeep Saha",
      "Akshay Chaturvedi",
      "Joy Mahapatra",
      "Utpal Garain"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.22": {
    "title": "DAC: Decomposed Automation Correction for Text-to-SQL",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dingzirui Wang",
      "Longxu Dou",
      "Xuanliang Zhang",
      "Qingfu Zhu",
      "Wanxiang Che"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.23": {
    "title": "VehicleWorld: A Highly Integrated Multi-Device Environment for Intelligent Vehicle Interaction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Yang",
      "Jiajun Chen",
      "Zhangyue Yin",
      "Shuo Chen",
      "Yuxin Wang",
      "Yiran Guo",
      "Yuan Li",
      "Yining Zheng",
      "Xuanjing Huang",
      "Xipeng Qiu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.24": {
    "title": "End-to-End Optimization for Multimodal Retrieval-Augmented Generation via Reward Backpropagation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan Fan",
      "Longfei Yun",
      "Ming Yan",
      "Yumeng Wang",
      "Dadi Guo",
      "Brian Mak",
      "James Kwok",
      "Yi R. Fung"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.25": {
    "title": "Audio-Aware Large Language Models as Judges for Speaking Styles",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng-Han Chiang",
      "Xiaofei Wang",
      "Chung-Ching Lin",
      "Kevin Lin",
      "Linjie Li",
      "Radu Kopetz",
      "Yao Qian",
      "Zhendong Wang",
      "Zhengyuan Yang",
      "Hung-yi Lee",
      "Lijuan Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.26": {
    "title": "Evaluation of Text-to-Image Generation from a Creativity Perspective",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinhao Wang",
      "Xinyu Ma",
      "ShengYong Ding",
      "Derek F. Wong"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.27": {
    "title": "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Liu",
      "Penglei Sun",
      "Shuyan Chen",
      "Longhan Zhang",
      "Peijie Dong",
      "Huajie You",
      "Yongqi Zhang",
      "Chang Yan",
      "Xiaowen Chu",
      "Tong-yi Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.28": {
    "title": "ProPy: Building Interactive Prompt Pyramids upon CLIP for Partially Relevant Video Retrieval",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Pan",
      "Yujia Zhang",
      "Michael Kampffmeyer",
      "Xiaoguang Zhao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.29": {
    "title": "Multilingual Datasets for Custom Input Extraction and Explanation Requests Parsing in Conversational XAI Systems",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qianli Wang",
      "Tatiana Anikina",
      "Nils Feldhus",
      "Simon Ostermann",
      "Fedor Splitt",
      "Jiaao Li",
      "Yoana Tsoneva",
      "Sebastian Möller",
      "Vera Schmitt"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.30": {
    "title": "Toolscaler: Scalable Generative Tool Calling via Structure-Aware Semantic Tokenization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunyue Su",
      "Zhang Jinshuai",
      "Bowen Fang",
      "Wen Ye",
      "Jinghao Zhang",
      "Bowen Song",
      "Weiqiang Wang",
      "Qiang Liu",
      "Liang Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.31": {
    "title": "LaMP-Val: Large Language Models Empower Personalized Valuation in Auction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Sun",
      "Tianyu Zhang",
      "Houcheng Jiang",
      "Kexin Huang",
      "Xiang Shu",
      "Zhibo Zhu",
      "Lintao Ma",
      "Xingyu Lu",
      "Jun Zhou",
      "Junkang Wu",
      "Chi Luo",
      "An Zhang",
      "Jiancan Wu",
      "Xiang Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.32": {
    "title": "Exploring Model Kinship for Merging Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yedi Hu",
      "Yunzhi Yao",
      "Ningyu Zhang",
      "Huajun Chen",
      "Shumin Deng"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.33": {
    "title": "MULTITAT: Benchmarking Multilingual Table-and-Text Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuanliang Zhang",
      "Dingzirui Wang",
      "Keyan Xu",
      "Qingfu Zhu",
      "Wanxiang Che"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.34": {
    "title": "LoRA-MGPO: Mitigating Double Descent in Low-Rank Adaptation via Momentum-Guided Perturbation Optimization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yupeng Chang",
      "Chenlu Guo",
      "Yi Chang",
      "Yuan Wu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.35": {
    "title": "R-LoRA: Randomized Multi-Head LoRA for Efficient Multi-task Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinda Liu",
      "Yi Chang",
      "Yuan Wu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.36": {
    "title": "RACQC: Advanced Retrieval-Augmented Generation for Chinese Query Correction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinbo Su",
      "Lingzhe Gao",
      "Wei Li",
      "Shihao Liu",
      "Haojie Lei",
      "Xinyi Wang",
      "Yuanzhao Guo",
      "Ke Wang",
      "Daiting Shi",
      "Dawei Yin"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.37": {
    "title": "Mechanistic Understanding and Mitigation of Language Confusion in English-Centric Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ercong Nie",
      "Helmut Schmid",
      "Hinrich Schuetze"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.38": {
    "title": "Assessing and Mitigating Medical Knowledge Drift and Conflicts in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiyi Wu",
      "Xinwen Xu",
      "Chongyang Gao",
      "Xingjian Diao",
      "Siting Li",
      "Lucas A. Salas",
      "Jiang Gui"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.39": {
    "title": "Improving LLM Reasoning through Interpretable Role-Playing Steering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anyi Wang",
      "Dong Shu",
      "Yifan Wang",
      "Yunpu Ma",
      "Mengnan Du"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.40": {
    "title": "R2A-TLS: Reflective Retrieval-Augmented Timeline Summarization with Causal-Semantic Integration",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenlong Bao",
      "Shijie Li",
      "Minghao Hu",
      "Ming Qiao",
      "Bin Zhang",
      "Jin-Tao Tang",
      "Shasha Li",
      "Ting Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.41": {
    "title": "MedEBench: Diagnosing Reliability in Text-Guided Medical Image Editing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minghao Liu",
      "Zhitao He",
      "Zhiyuan Fan",
      "Qingyun Wang",
      "Yi R. Fung"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.42": {
    "title": "FairCoT: Enhancing Fairness in Text-to-Image Generation via Chain of Thought Reasoning with Multimodal Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zahraa Al Sahili",
      "Ioannis Patras",
      "Matthew Purver"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.43": {
    "title": "Bag of Tricks for Sparse Mixture-of-Experts: A Benchmark Across Reasoning, Efficiency, and Safety",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mufan Qiu",
      "Zheyu Shen",
      "Pingzhi Li",
      "Ang Li",
      "Tianlong Chen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.44": {
    "title": "Don't Take the Premise for Granted: Evaluating the Premise Critique Ability of Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinzhe Li",
      "Gengxu Li",
      "Yi Chang",
      "Yuan Wu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.45": {
    "title": "Mitigating Geospatial Knowledge Hallucination in Large Language Models: Benchmarking and Dynamic Factuality Aligning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengyuan Wang",
      "Jie Feng",
      "Tianhui Liu",
      "Dan Pei",
      "Yong Li"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.46": {
    "title": "The Power of Framing: How News Headlines Guide Search Behavior",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amrit Poudel",
      "Maria Milkowski",
      "Tim Weninger"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.47": {
    "title": "DivLogicEval: A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tsz Ting Chung",
      "Lemao Liu",
      "Mo Yu",
      "Dit-Yan Yeung"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.48": {
    "title": "THCM-CAL: Temporal-Hierarchical Causal Modelling with Conformal Calibration for Clinical Risk Prediction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Zhang",
      "Qiyu Wei",
      "Yingjie Zhu",
      "Fanyi Wu",
      "Sophia Ananiadou"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.49": {
    "title": "GenPilot: A Multi-Agent System for Test-Time Prompt Optimization in Image Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wen Ye",
      "Zhaocheng Liu",
      "Gui Yuwei",
      "Tingyu Yuan",
      "Yunyue Su",
      "Bowen Fang",
      "Chaoyang Zhao",
      "Qiang Liu",
      "Liang Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.50": {
    "title": "Grounded-VideoLLM: Sharpening Fine-grained Temporal Grounding in Video Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haibo Wang",
      "Zhiyang Xu",
      "Yu Cheng",
      "Shizhe Diao",
      "Yufan Zhou",
      "Yixin Cao",
      "Qifan Wang",
      "Weifeng Ge",
      "Lifu Huang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.51": {
    "title": "DongbaMIE: A Multimodal Information Extraction Dataset for Evaluating Semantic Understanding of Dongba Pictograms",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaojun Bi",
      "Shuo Li",
      "Junyao Xing",
      "Ziyue Wang",
      "Fuwen Luo",
      "Weizheng Qiao",
      "Lu Han",
      "Ziwei Sun",
      "Peng Li",
      "Yang Liu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.52": {
    "title": "Optimizing Cross-Client Domain Coverage for Federated Instruction Tuning of Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zezhou Wang",
      "Yaxin Du",
      "Xingjun Ma",
      "Yu-Gang Jiang",
      "Zhuzhong Qian",
      "Siheng Chen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.53": {
    "title": "Aligning Black-Box LLMs for Aspect Sentiment Quad Prediction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shichen Li",
      "Jiawei Zhang",
      "Zhongqing Wang",
      "Peifeng Li"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.54": {
    "title": "Multifaceted Evaluation of Audio-Visual Capability for MLLMs: Effectiveness, Efficiency, Generalizability and Robustness",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yusheng Zhao",
      "Xiao Luo",
      "Junyu Luo",
      "Weizhi Zhang",
      "Zhiping Xiao",
      "Wei Ju",
      "Philip S. Yu",
      "Ming Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.55": {
    "title": "Two Steps from Hell: Compositionality on Chemical LMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Veronika Ganeeva",
      "Kuzma Khrabrov",
      "Artur Kadurin",
      "Elena Tutubalina"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.56": {
    "title": "GTA: Supervised-Guided Reinforcement Learning for Text Classification with Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Min Zeng",
      "Jingfei Sun",
      "Xueyou Luo",
      "Shiqi Zhang",
      "Li Xie",
      "Caiquan Liu",
      "Xiaoxin Chen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.57": {
    "title": "Unearthing Gems from Stones: Policy Optimization with Negative Sample Augmentation for LLM Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaohui Yang",
      "Yuxiao Ye",
      "Shilei Jiang",
      "Shihong Deng",
      "Chen Hu",
      "Linjing Li",
      "Daxin Jiang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.58": {
    "title": "LEAF: Large Language Diffusion Model for Time Series Forecasting",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhang Pei",
      "Tao Ren",
      "Yifan Wang",
      "Zhipeng Sun",
      "Wei Ju",
      "Chong Chen",
      "Xian-Sheng Hua",
      "Xiao Luo"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.59": {
    "title": "SPFT-SQL: Enhancing Large Language Model for Text-to-SQL Parsing by Self-Play Fine-Tuning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhao Zhang",
      "Shaoming Duan",
      "Jinhang Su",
      "Chuanyi Liu",
      "Peiyi Han"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.60": {
    "title": "Multilingual Verbalisation of Knowledge Graphs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Song",
      "William Soto Martinez",
      "Anna Nikiforovskaya",
      "Evan Parker Kelly Chapple",
      "Claire Gardent"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.61": {
    "title": "LAGCL4Rec: When LLMs Activate Interactions Potential in Graph Contrastive Learning for Recommendation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leqi Zheng",
      "Chaokun Wang",
      "Canzhi Chen",
      "Jiajun Zhang",
      "Cheng Wu",
      "Zixin Song",
      "Shannan Yan",
      "Ziyang Liu",
      "Hongwei Li"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.62": {
    "title": "English as Defense Proxy: Mitigating Multilingual Jailbreak via Eliciting English Safety Knowledge",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zekai Zhang",
      "Yiduo Guo",
      "Jiuheng Lin",
      "Shanghaoran Quan",
      "Huishuai Zhang",
      "Dongyan Zhao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.63": {
    "title": "Dagger Behind Smile: Fool LLMs with a Happy Ending Story",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xurui Song",
      "Zhixin Xie",
      "Shuo Huai",
      "Jiayi Kong",
      "Jun Luo"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.64": {
    "title": "Mitigating Object Hallucinations in MLLMs via Multi-Frequency Perturbations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuo Li",
      "Jiajun Sun",
      "Guodong Zheng",
      "Xiaoran Fan",
      "Yujiong Shen",
      "Yi Lu",
      "Zhiheng Xi",
      "Yuming Yang",
      "Wenming Tan",
      "Tao Ji",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.65": {
    "title": "Natural Context Drift Undermines the Natural Language Understanding of Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yulong Wu",
      "Viktor Schlegel",
      "Riza Batista-Navarro"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.66": {
    "title": "Minimal Ranks, Maximum Confidence: Parameter-efficient Uncertainty Quantification for LoRA",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Patryk Marszałek",
      "Klaudia Bałazy",
      "Jacek Tabor",
      "Tomasz Kuśmierczyk"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.67": {
    "title": "Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language Models: An Empirical Evaluation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahao Cheng",
      "Tiancheng Su",
      "Jia Yuan",
      "Guoxiu He",
      "Jiawei Liu",
      "Xinqi Tao",
      "Jingwen Xie",
      "Huaxia Li"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.68": {
    "title": "Large Language Model Evaluation via Matrix Nuclear-Norm",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yahan Li",
      "Tingyu Xia",
      "Yuan Wu",
      "Yi Chang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.69": {
    "title": "From Grounding to Manipulation: Case Studies of Foundation Model Integration in Embodied Robotic Systems",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiuchao Sui",
      "Daiying Tian",
      "Qi Sun",
      "Ruirui Chen",
      "Dongkyu Choi",
      "Kenneth Kwok",
      "Soujanya Poria"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.70": {
    "title": "Flexible Thinking for Multimodal Emotional Support Conversation via Reinforcement Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fanfan Wang",
      "Xiangqing Shen",
      "Jianfei Yu",
      "Rui Xia"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.71": {
    "title": "ORAL: Prompting Your Large-Scale LoRAs via Conditional Recurrent Diffusion",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rana Shahroz",
      "Dongwen Tang",
      "Pingzhi Li",
      "Kai Wang",
      "Tianlong Chen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.72": {
    "title": "NLoRA: Nyström-Initiated Low-Rank Adaptation for Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenlu Guo",
      "Yi Chang",
      "Yuan Wu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.73": {
    "title": "Bhaasha, Bhāṣā, Zaban: A Survey for Low-Resourced Languages in South Asia – Current Stage and Challenges",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sampoorna Poria",
      "Xiaolei Huang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.74": {
    "title": "DISCO Balances the Scales: Adaptive Domain- and Difficulty-Aware Reinforcement Learning on Imbalanced Data",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhang Zhou",
      "Jing Zhu",
      "Shengyi Qian",
      "Zhuokai Zhao",
      "Xiyao Wang",
      "Xiaoyu Liu",
      "Ming Li",
      "Paiheng Xu",
      "Wei Ai",
      "Furong Huang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.75": {
    "title": "What Makes for Good Image Captions?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Delong Chen",
      "Samuel Cahyawijaya",
      "Etsuko Ishii",
      "Ho Shu Chan",
      "Yejin Bang",
      "Pascale Fung"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.76": {
    "title": "What's Not Said Still Hurts: A Description-Based Evaluation Framework for Measuring Social Bias in LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinhao Pan",
      "Chahat Raj",
      "Ziyu Yao",
      "Ziwei Zhu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.77": {
    "title": "Identifying Rare Languages in Common Crawl Data is a Needles-in-a-Haystack Problem",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rasul Dent",
      "Pedro Ortiz Suarez",
      "Thibault Clérice",
      "Benoît Sagot"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.78": {
    "title": "Training Language Models to Critique With Multi-agent Feedback",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tian Lan",
      "Wenwei Zhang",
      "Chengqi Lyu",
      "Shuaibin Li",
      "Chen Xu",
      "Heyan Huang",
      "Dahua Lin",
      "Xian-Ling Mao",
      "Kai Chen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.79": {
    "title": "RELIC: Enhancing Reward Model Generalization for Low-Resource Indic Languages with Few-Shot Examples",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soumya Suvra Ghosal",
      "Vaibhav Singh",
      "Akash Ghosh",
      "Soumyabrata Pal",
      "Subhadip Baidya",
      "Sriparna Saha",
      "Dinesh Manocha"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.80": {
    "title": "Invoke Interfaces Only When Needed: Adaptive Invocation for Large Language Models in Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jihao Zhao",
      "Chunlai Zhou",
      "Daixuan Li",
      "Shuaishuai Zu",
      "Biao Qin"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.81": {
    "title": "SQLSpace: A Representation Space for Text-to-SQL to Discover and Mitigate Robustness Gaps",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Neha Srikanth",
      "Victor Bursztyn",
      "Puneet Mathur",
      "Ani Nenkova"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.82": {
    "title": "One More Modality: Does Abstract Meaning Representation Benefit Visual Question Answering?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhidip Bhattacharyya",
      "Emma Markle",
      "Shira Wein"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.83": {
    "title": "DP-GTR: Differentially Private Prompt Protection via Group Text Rewriting",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingchen Li",
      "Heng Fan",
      "Song Fu",
      "Junhua Ding",
      "Yunhe Feng"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.84": {
    "title": "Legal Mathematical Reasoning with LLMs: Procedural Alignment through Two-Stage Reinforcement Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kepu Zhang",
      "Guofu Xie",
      "Weijie Yu",
      "Mingyue Xu",
      "Xu Tang",
      "Yaxin Li",
      "Jun Xu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.85": {
    "title": "ModelingAgent: Bridging LLMs and Mathematical Modeling for Real-World Challenges",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Qian",
      "Hongyi Du",
      "Hongru Wang",
      "Xiusi Chen",
      "Yuji Zhang",
      "Avirup Sil",
      "ChengXiang Zhai",
      "Kathleen McKeown",
      "Heng Ji"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.86": {
    "title": "Beyond Coarse Labels: Fine-Grained Problem Augmentation and Multi-Dimensional Feedback for Emotional Support Conversation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanchen Shi",
      "Jiawang Hao",
      "Fang Kong"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.87": {
    "title": "FinHEAR: Human Expertise and Adaptive Risk-Aware Temporal Reasoning for Financial Decision-Making",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxiang Chen",
      "Mingxi Zou",
      "Zhuo Wang",
      "Qifan Wang",
      "Danny Dongning Sun",
      "Zhang Chi",
      "Zenglin Xu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.88": {
    "title": "EvolKV: Evolutionary KV Cache Compression for LLM Inference",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bohan Yu",
      "Yekun Chai"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.89": {
    "title": "A Survey on Sparse Autoencoders: Interpreting the Internal Mechanisms of Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong Shu",
      "Xuansheng Wu",
      "Haiyan Zhao",
      "Daking Rai",
      "Ziyu Yao",
      "Ninghao Liu",
      "Mengnan Du"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.90": {
    "title": "Large Vision-Language Model Alignment and Misalignment: A Survey Through the Lens of Explainability",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong Shu",
      "Haiyan Zhao",
      "Jingyu Hu",
      "Weiru Liu",
      "Ali Payani",
      "Lu Cheng",
      "Mengnan Du"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.91": {
    "title": "Attention Consistency for LLMs Explanation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tian Lan",
      "Jinyuan Xu",
      "Xue He",
      "Jenq-Neng Hwang",
      "Lei Li"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.92": {
    "title": "Confusion is the Final Barrier: Rethinking Jailbreak Evaluation and Investigating the Real Misuse Threat of LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Yan",
      "Sheng Sun",
      "Zhe Wang",
      "Yijun Lin",
      "Zenghao Duan",
      "Zhifei Zheng",
      "Min Liu",
      "Zhiyi Yin",
      "Jianping Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.93": {
    "title": "CCL-XCoT: An Efficient Cross-Lingual Knowledge Transfer Method for Mitigating Hallucination Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheng Weihua",
      "Roy Ka-Wei Lee",
      "Zhengyuan Liu",
      "Wu Kui",
      "AiTi Aw",
      "Bowei Zou"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.94": {
    "title": "Evaluating Step-by-step Reasoning Traces: A Survey",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinu Lee",
      "Julia Hockenmaier"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.95": {
    "title": "Beyond Guilt: Legal Judgment Prediction with Trichotomous Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kepu Zhang",
      "Haoyue Yang",
      "Xu Tang",
      "Weijie Yu",
      "Jun Xu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.96": {
    "title": "Not Every Token Needs Forgetting: Selective Unlearning Balancing Forgetting and Utility in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixin Wan",
      "Anil Ramakrishna",
      "Kai-Wei Chang",
      "Volkan Cevher",
      "Rahul Gupta"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.97": {
    "title": "DisastIR: A Comprehensive Information Retrieval Benchmark for Disaster Management",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Yin",
      "Xiangjue Dong",
      "Chengkai Liu",
      "Lipai Huang",
      "Yiming Xiao",
      "Zhewei Liu",
      "Ali Mostafavi",
      "James Caverlee"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.98": {
    "title": "Data or Language Supervision: What Makes CLIP Better than DINO?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Liu",
      "Yuhui Zhang",
      "Dhruba Ghosh",
      "Ludwig Schmidt",
      "Serena Yeung-Levy"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.99": {
    "title": "Do LLMs Understand Wine Descriptors Across Cultures? A Benchmark for Cultural Adaptations of Wine Reviews",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenye Zou",
      "Xingyue Wen",
      "Tianyi Hu",
      "Qian Janice Wang",
      "Daniel Hershcovich"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.100": {
    "title": "DeFT-X: Denoised Sparse Fine-Tuning for Zero-Shot Cross-Lingual Transfer",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sona Elza Simon",
      "Preethi Jyothi"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.101": {
    "title": "Memory-enhanced Large Language Model for Cross-lingual Dependency Parsing via Deep Hierarchical Syntax Understanding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianjian Liu",
      "Ying Li",
      "Zhengtao Yu",
      "Shun Su",
      "Shengxiang Gao",
      "Yuxin Huang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.102": {
    "title": "Developing and Utilizing a Large-Scale Cantonese Dataset for Multi-Tasking in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiyue Jiang",
      "Alfred Kar Yin Truong",
      "Yanyu Chen",
      "Qinghang Bao",
      "Sheng Wang",
      "Pengan Chen",
      "Jiuming Wang",
      "Lingpeng Kong",
      "Yu Li",
      "Chuan Wu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.103": {
    "title": "A Structured Framework for Evaluating and Enhancing Interpretive Capabilities of Multimodal LLMs in Culturally Situated Tasks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haorui Yu",
      "Ramon Ruiz-Dolz",
      "Qiufeng Yi"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.104": {
    "title": "Train a Unified Multimodal Data Quality Classifier with Synthetic Data",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weizhi Wang",
      "Rongmei Lin",
      "Shiyang Li",
      "Colin Lockard",
      "Ritesh Sarkhel",
      "Sanket Lokegaonkar",
      "Jingbo Shang",
      "Xifeng Yan",
      "Nasser Zalmout",
      "Xian Li"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.105": {
    "title": "Self-Improvement in Multimodal Large Language Models: A Survey",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shijian Deng",
      "Kai Wang",
      "Tianyu Yang",
      "Harsh Singh",
      "Yapeng Tian"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.106": {
    "title": "Towards Achieving Concept Completeness for Textual Concept Bottleneck Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Milan Bhan",
      "Yann Choho",
      "Jean-Noël Vittaut",
      "Nicolas Chesneau",
      "Pierre Moreau",
      "Marie-Jeanne Lesot"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.107": {
    "title": "EmoBench-UA: A Benchmark Dataset for Emotion Detection in Ukrainian",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daryna Dementieva",
      "Nikolay Babakov",
      "Alexander Fraser"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.108": {
    "title": "Scientific Paper Retrieval with LLM-Guided Semantic-Based Ranking",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunyi Zhang",
      "Ruozhen Yang",
      "Siqi Jiao",
      "SeongKu Kang",
      "Jiawei Han"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.109": {
    "title": "DLIR: Spherical Adaptation for Cross-Lingual Knowledge Transfer of Sociological Concepts Alignment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeqiang Wang",
      "Jon Johnson",
      "Suparna De"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.110": {
    "title": "Test-Time Steering for Lossless Text Compression via Weighted Product of Experts",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qihang Zhang",
      "Muchen Li",
      "Ziao Wang",
      "Renjie Liao",
      "Lele Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.111": {
    "title": "Zero-Shot Contextual Embeddings via Offline Synthetic Corpus Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Philip Lippmann",
      "Jie Yang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.112": {
    "title": "The Hallucination Tax of Reinforcement Finetuning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linxin Song",
      "Taiwei Shi",
      "Jieyu Zhao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.113": {
    "title": "Tracing Multilingual Factual Knowledge Acquisition in Pretraining",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihong Liu",
      "Mingyang Wang",
      "Amir Hossein Kargaran",
      "Felicia Körner",
      "Ercong Nie",
      "Barbara Plank",
      "François Yvon",
      "Hinrich Schuetze"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.114": {
    "title": "Exploring the Vulnerability of the Content Moderation Guardrail in Large Language Models via Intent Manipulation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Zhuang",
      "Haibo Jin",
      "Ye Zhang",
      "Zhengjian Kang",
      "Wenbin Zhang",
      "Gaby G. Dagher",
      "Haohan Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.115": {
    "title": "Examining Multilingual Embedding Models Cross-Lingually Through LLM-Generated Adversarial Examples",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrianos Michail",
      "Simon Clematide",
      "Rico Sennrich"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.116": {
    "title": "EmoGist: Efficient In-Context Learning for Visual Emotion Understanding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ronald Seoh",
      "Dan Goldwasser"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.117": {
    "title": "Soft Token Attacks Cannot Reliably Audit Unlearning in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haokun Chen",
      "Sebastian Szyller",
      "Weilin Xu",
      "Nageen Himayat"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.118": {
    "title": "Bridging the Editing Gap in LLMs: FineEdit for Precise and Targeted Text Modifications",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Zeng",
      "Wanhao Yu",
      "Zexin Li",
      "Tao Ren",
      "Yu Ma",
      "Jinghan Cao",
      "Xiyan Chen",
      "Tingting Yu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.119": {
    "title": "LLM-based Conversational Recommendation Agents with Collaborative Verbalized Experience",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaochen Zhu",
      "Harald Steck",
      "Dawen Liang",
      "Yinhan He",
      "Nathan Kallus",
      "Jundong Li"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.120": {
    "title": "Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Mark Chen",
      "Wayne Luk",
      "Yiu Ka Fai Cedric",
      "Rui Li",
      "Konstantin Mishchenko",
      "Stylianos Venieris",
      "Hongxiang Fan"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.121": {
    "title": "Measuring Sycophancy of Language Models in Multi-turn Dialogues",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiseung Hong",
      "Grace Byun",
      "Seungone Kim",
      "Kai Shu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.122": {
    "title": "On the Role of Entity and Event Level Conceptualization in Generalizable Reasoning: A Survey of Tasks, Methods, Applications, and Future Directions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiqi Wang",
      "Tianqing Fang",
      "Haochen Shi",
      "Baixuan Xu",
      "Wenxuan Ding",
      "Liyu Zhang",
      "Wei Fan",
      "Jiaxin Bai",
      "Haoran Li",
      "Xin Liu",
      "Yangqiu Song"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.123": {
    "title": "Mitigating Visual Knowledge Forgetting in MLLM Instruction-tuning via Modality-decoupled Gradient Descent",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junda Wu",
      "Yuxin Xiong",
      "Xintong Li",
      "Yu Xia",
      "Ruoyu Wang",
      "Yu Wang",
      "Tong Yu",
      "Sungchul Kim",
      "Ryan A. Rossi",
      "Lina Yao",
      "Jingbo Shang",
      "Julian McAuley"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.124": {
    "title": "PathoHR: Hierarchical Reasoning for Vision-Language Models in Pathology",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yating Huang",
      "Ziyan Huang",
      "Lintao Xiang",
      "Qijun Yang",
      "Hujun Yin"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.125": {
    "title": "What's Up, Doc?\": Analyzing How Users Seek Health Information in Large-Scale Conversational AI Datasets",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akshay Paruchuri",
      "Maryam Aziz",
      "Rohit Vartak",
      "Ayman Ali",
      "Best Uchehara",
      "Xin Liu",
      "Ishan Chatterjee",
      "Monica Agrawal"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.126": {
    "title": "Dynamic Evaluation for Oversensitivity in LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sophia Xiao Pu",
      "Sitao Cheng",
      "Xin Eric Wang",
      "William Yang Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.127": {
    "title": "Self-Correcting Code Generation Using Small Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeonghun Cho",
      "Deokhyung Kang",
      "Hyounghun Kim",
      "Gary Lee"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.128": {
    "title": "A Unified Framework for N-ary Property Information Extraction in Materials Science",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Van-Thuy Phi",
      "Yuji Matsumoto"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.129": {
    "title": "A Benchmark for Translations Across Styles and Language Variants",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Tan",
      "Bowei Zou",
      "AiTi Aw"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.130": {
    "title": "ManuSearch: Democratizing Deep Search in Large Language Models with a Transparent and Open Multi-Agent Framework",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lisheng Huang",
      "Yichen Liu",
      "Jinhao Jiang",
      "Rongxiang Zhang",
      "Jiahao Yan",
      "Junyi Li",
      "Xin Zhao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.131": {
    "title": "Proactive User Information Acquisition via Chats on User-Favored Topics",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiki Sato",
      "Jun Baba",
      "Asahi Hentona",
      "Shinji Iwata",
      "Akifumi Yoshimoto",
      "Koichiro Yoshino"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.132": {
    "title": "Evaluating Text Generation Quality Using Spectral Distances of Surprisal",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhichen Liu",
      "Yongyuan Li",
      "Yang Xu",
      "Yu Wang",
      "Yingfang Yuan",
      "Zuhao Yang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.133": {
    "title": "NLP-ADBench: NLP Anomaly Detection Benchmark",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuangang Li",
      "Jiaqi Li",
      "Zhuo Xiao",
      "Tiankai Yang",
      "Yi Nian",
      "Xiyang Hu",
      "Yue Zhao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.134": {
    "title": "Toward Inclusive Language Models: Sparsity-Driven Calibration for Systematic and Interpretable Mitigation of Social Biases in LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prommy Sultana Hossain",
      "Chahat Raj",
      "Ziwei Zhu",
      "Jessica Lin",
      "Emanuela Marasco"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.135": {
    "title": "Table-Text Alignment: Explaining Claim Verification Against Tables in Scientific Papers",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xanh Ho",
      "Sunisth Kumar",
      "Yun-Ang Wu",
      "Florian Boudin",
      "Atsuhiro Takasu",
      "Akiko Aizawa"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.136": {
    "title": "DCRM: A Heuristic to Measure Response Pair Quality in Preference Optimization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengyu Huang",
      "Tanya Goyal"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.137": {
    "title": "Advancing Reasoning with Off-the-Shelf LLMs: A Semantic Structure Perspective",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengfei He",
      "Zitao Li",
      "Yue Xing",
      "Yaliang Li",
      "Jiliang Tang",
      "Bolin Ding"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.138": {
    "title": "LLM-based Open Domain Planning by Leveraging Entity-Attribute-Level Domain Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongning Rao",
      "Songlin He",
      "Zhihua Jiang",
      "Ruishi Liang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.139": {
    "title": "DICP: Deep In-Context Prompt for Event Causality Identification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lin Mu",
      "Jun Shen",
      "Li Ni",
      "Lei Sang",
      "Zhize Wu",
      "Peiquan Jin",
      "Yiwen Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.140": {
    "title": "Seeing is Believing: Emotion-Aware Audio-Visual Language Modeling for Expressive Speech Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiting Tan",
      "Jiachen Lian",
      "Hirofumi Inaguma",
      "Paden Tomasello",
      "Philipp Koehn",
      "Xutai Ma"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.141": {
    "title": "GRV-KBQA: A Three-Stage Framework for Knowledge Base Question Answering with Decoupled Logical Structure, Semantic Grounding and Structure-Aware Validation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhang Tian",
      "Pan Yang",
      "Dandan Song",
      "Zhijing Wu",
      "Hao Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.142": {
    "title": "Improving Prompt Generalization for Cross-prompt Essay Trait Scoring from the Scoring-invariance Perspective",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiong Wang",
      "Shengquan Yu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.143": {
    "title": "When Format Changes Meaning: Investigating Semantic Inconsistency of Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheongwoong Kang",
      "Jongeun Baek",
      "Yeonjea Kim",
      "Jaesik Choi"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.144": {
    "title": "ASTPrompter: Preference-Aligned Automated Language Model Red-Teaming to Generate Low-Perplexity Unsafe Prompts",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amelia Hardy",
      "Houjun Liu",
      "Allie Griffith",
      "Bernard Lange",
      "Duncan Eddy",
      "Mykel Kochenderfer"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.145": {
    "title": "How Do Large Language Models Perform on PDE Discovery: A Coarse-to-fine Perspective",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Luo",
      "Changhu Wang",
      "Yizhou Sun",
      "Wei Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.146": {
    "title": "Rethinking Data Selection at Scale: Random Selection is Almost All You Need",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tingyu Xia",
      "Bowen Yu",
      "Kai Dang",
      "An Yang",
      "Yuan Wu",
      "Yuan Tian",
      "Yi Chang",
      "Junyang Lin"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.147": {
    "title": "PromptKeeper: Safeguarding System Prompts for LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhifeng Jiang",
      "Zhihua Jin",
      "Guoliang He"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.148": {
    "title": "Automating eHMI Action Design with LLMs for Automated Vehicle Communication",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ding Xia",
      "Xinyue Gui",
      "Fan Gao",
      "Dongyuan Li",
      "Mark Colley",
      "Takeo Igarashi"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.149": {
    "title": "A Dynamic Fusion Model for Consistent Crisis Response",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoying Song",
      "Anirban Saha Anik",
      "Eduardo Blanco",
      "Vanessa Frias-Martinez",
      "Lingzi Hong"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.150": {
    "title": "UIOrchestra: Generating High-Fidelity Code from UI Designs with a Multi-agent System",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuhuai Yue",
      "Jiajun Chai",
      "Yufei Zhang",
      "Zixiang Ding",
      "Xihao Liang",
      "Peixin Wang",
      "Shihai Chen",
      "Wang Yixuan",
      "Wangyanping",
      "Guojun Yin",
      "Wei Lin"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.151": {
    "title": "CrossQG: Improving Difficulty-Controllable Question Generation through Consistency Enhancement",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kunze Li",
      "Yu Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.152": {
    "title": "Progressive Facial Granularity Aggregation with Bilateral Attribute-based Enhancement for Face-to-Speech Synthesis",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yejin Jeon",
      "Youngjae Kim",
      "Jihyun Lee",
      "Hyounghun Kim",
      "Gary Lee"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.153": {
    "title": "Speaking at the Right Level: Literacy-Controlled Counterspeech Generation with RAG-RL",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoying Song",
      "Anirban Saha Anik",
      "Dibakar Barua",
      "Pengcheng Luo",
      "Junhua Ding",
      "Lingzi Hong"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.154": {
    "title": "FNSCC: Fuzzy Neighborhood-Aware Self-Supervised Contrastive Clustering for Short Text",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijian Zheng",
      "Yonghe Lu",
      "Jian Yin"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.155": {
    "title": "AuraDial: A Large-Scale Human-Centric Dialogue Dataset for Chinese AI Psychological Counseling",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiantao Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.156": {
    "title": "TS-SQL: Test-driven Self-refinement for Text-to-SQL",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenbo Xu",
      "Haifeng Zhu",
      "Liang Yan",
      "Chuanyi Liu",
      "Peiyi Han",
      "Shaoming Duan",
      "Jeff Z. Pan"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.157": {
    "title": "DemonAgent: Dynamically Encrypted Multi-Backdoor Implantation Attack on LLM-based Agent",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengyu Zhu",
      "Zhenhong Zhou",
      "Yuanhe Zhang",
      "Shilinlu Yan",
      "Kun Wang",
      "Sen Su"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.158": {
    "title": "MotivGraph-SoIQ: Integrating Motivational Knowledge Graphs and Socratic Dialogue for Enhanced LLM Ideation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinping Lei",
      "Tong Zhou",
      "Yubo Chen",
      "Kang Liu",
      "Jun Zhao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.159": {
    "title": "ExpertGenQA: Open-ended QA generation in Specialized Domains",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haz Sameen Shahgir",
      "Chansong Lim",
      "Jia Chen",
      "Evangelos E. Papalexakis",
      "Yue Dong"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.160": {
    "title": "VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuansheng Ni",
      "Ping Nie",
      "Kai Zou",
      "Xiang Yue",
      "Wenhu Chen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.161": {
    "title": "Conversational Education at Scale: A Multi-LLM Agent Workflow for Procedural Learning and Pedagogic Quality Assessment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahuan Pei",
      "Fanghua Ye",
      "Xin Sun",
      "Wentao Deng",
      "Koen Hindriks",
      "Junxiao Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.162": {
    "title": "Visual Program Distillation with Template-Based Augmentation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michal Shlapentokh-Rothman",
      "Yu-Xiong Wang",
      "Derek Hoiem"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.163": {
    "title": "NeighXLM: Enhancing Cross-Lingual Transfer in Low-Resource Languages via Neighbor-Augmented Contrastive Pretraining",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sicheng Wang",
      "Wenyi Wu",
      "Zibo Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.164": {
    "title": "ICLER: Intent CLassification with Enhanced Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dezheng Gao",
      "Dong Xiaozheng",
      "SHuangtao Yang",
      "Bo Fu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.165": {
    "title": "PreGenie: An Agentic Framework for High-quality Visual Presentation Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaojie Xu",
      "Xinli Xu",
      "Sirui Chen",
      "Haoyu Chen",
      "Fan Zhang",
      "Ying-Cong Chen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.166": {
    "title": "RIVAL: Reinforcement Learning with Iterative and Adversarial Optimization for Machine Translation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianjiao Li",
      "Mengran Yu",
      "Chenyu Shi",
      "Yanjun Zhao",
      "Xiaojing Liu",
      "Qi Zhang",
      "Xuanjing Huang",
      "Qiang Zhang",
      "Jiayin Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.167": {
    "title": "MRAG: A Modular Retrieval Framework for Time-Sensitive Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyue Zhang",
      "Yuxiang Xue",
      "Yiming Zhang",
      "Xiaobao Wu",
      "Anh Tuan Luu",
      "Chen Zhao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.168": {
    "title": "CoT-RAG: Integrating Chain of Thought and Retrieval-Augmented Generation to Enhance Reasoning in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feiyang Li",
      "Peng Fang",
      "Zhan Shi",
      "Arijit Khan",
      "Fang Wang",
      "Weihao Wang",
      "Zhangxin-hw",
      "Cui Yongjian"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.169": {
    "title": "TabDSR: Decompose, Sanitize, and Reason for Complex Numerical Reasoning in Tabular Data",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changjiang Jiang",
      "Fengchang Yu",
      "Haihua Chen",
      "Wei Lu",
      "Jin Zeng"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.170": {
    "title": "Chain-of-Thought Matters: Improving Long-Context Language Models with Reasoning Path Supervision",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dawei Zhu",
      "Xiyu Wei",
      "Guangxiang Zhao",
      "Wenhao Wu",
      "Haosheng Zou",
      "Junfeng Ran",
      "XWang",
      "Lin Sun",
      "Xiangzheng Zhang",
      "Sujian Li"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.171": {
    "title": "Multimodal Document-level Triple Extraction via Dynamic Graph Enhancement and Relation-Aware Reflection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Li",
      "Runhai Jiao",
      "Zhou Changyu",
      "Shoupeng Qiao",
      "Ruojiao Qiao",
      "Ruifan Li"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.172": {
    "title": "Distill Visual Chart Reasoning Ability from LLMs to MLLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei He",
      "Zhiheng Xi",
      "Wanxu Zhao",
      "Xiaoran Fan",
      "Yiwen Ding",
      "Zifei Shan",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.173": {
    "title": "FlowMalTrans: Unsupervised Binary Code Translation for Malware Detection Using Flow-Adapter Architecture",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minghao Hu",
      "Junzhe Wang",
      "Weisen Zhao",
      "Qiang Zeng",
      "Lannan Luo"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.174": {
    "title": "AdaTP: Attention-Debiased Token Pruning for Video Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fengyuan Sun",
      "Leqi Shen",
      "Hui Chen",
      "Sicheng Zhao",
      "Jungong Han",
      "Guiguang Ding"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.175": {
    "title": "AdaptFlow: Adaptive Workflow Optimization via Meta-Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runchuan Zhu",
      "Bowen Jiang",
      "Lingrui Mei",
      "Fangkai Yang",
      "Lu Wang",
      "Haoxiang Gao",
      "Fengshuo Bai",
      "Pu Zhao",
      "Qingwei Lin",
      "Saravan Rajmohan",
      "Dongmei Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.176": {
    "title": "LMUNIT: Fine-grained Evaluation with Natural Language Unit Tests",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jon Saad-Falcon",
      "Rajan Pathe Vivek",
      "William Berrios",
      "Nandita Shankar Naik",
      "Matija Franklin",
      "Bertie Vidgen",
      "Amanpreet Singh",
      "Douwe Kiela",
      "Shikib Mehri"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.177": {
    "title": "ThinkAnswer Loss: Balancing Semantic Similarity and Exact Matching for LLM Reasoning Enhancement",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shan Yang",
      "Kun Wu",
      "Zeju Li",
      "Linlin Zhang",
      "Xiangyu Pei",
      "Leike An",
      "Yu Liu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.178": {
    "title": "Detecting Stealthy Backdoor Samples based on Intra-class Distance for Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinwen Chen",
      "Hainan Zhang",
      "Fei Sun",
      "Qinnan Zhang",
      "Sijia Wen",
      "Ziwei Wang",
      "Zhiming Zheng"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.179": {
    "title": "Rust-doctor: Enhanced Feature for Rust Ownership and Lifetime Repair with Balanced Training Data Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenzhang Yang",
      "Xiaoning Ren",
      "Cuifeng Gao",
      "Yinxing Xue"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.180": {
    "title": "SLIM: Subtrajectory-Level Elimination for More Effective Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xifeng Yao",
      "Chengyuan Ma",
      "Dongyu Lang",
      "Yinhao Ni",
      "Zhiwei Xu",
      "Huarui Xie",
      "Zihao Chen",
      "Guang Shen",
      "Dandan Tu",
      "Yi Bai",
      "Changzheng Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.181": {
    "title": "From Cross-Task Examples to In-Task Prompts: A Graph-Based Pseudo-Labeling Framework for In-context Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihan Chen",
      "Song Wang",
      "Xingbo Fu",
      "Chengshuai Shi",
      "Zhenyu Lei",
      "Cong Shen",
      "Jundong Li"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.182": {
    "title": "Instance-level Randomization: Toward More Stable LLM Evaluations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiyang Li",
      "Yonghuang Wu",
      "Ying Luo",
      "Liangtai Sun",
      "Zishu Qin",
      "Lin Qiu",
      "Xuezhi Cao",
      "Xunliang Cai"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.183": {
    "title": "Not All Voices Are Rewarded Equally: Probing and Repairing Reward Models across Human Diversity",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihao Li",
      "Feihao Fang",
      "Xitong Zhang",
      "Jiaru Zou",
      "Zhining Liu",
      "Wei Xiong",
      "Ziwei Wu",
      "Baoyu Jing",
      "Jingrui He"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.184": {
    "title": "PAMN: Multi-phase Correlation Modeling for Contrast-Enhanced 3D Medical Image Retrieval",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haonan Tong",
      "Ke Liu",
      "Chuang Zhang",
      "Xinglin Zhang",
      "Tao Chen",
      "Jenq-Neng Hwang",
      "Lei Li"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.185": {
    "title": "Safety in Large Reasoning Models: A Survey",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Wang",
      "Yue Liu",
      "Baolong Bi",
      "Duzhen Zhang",
      "Zhong-Zhi Li",
      "Yingwei Ma",
      "Yufei He",
      "Shengju Yu",
      "Xinfeng Li",
      "Junfeng Fang",
      "Jiaheng Zhang",
      "Bryan Hooi"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.186": {
    "title": "SafeConf: A Confidence-Calibrated Safety Self-Evaluation Method for Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Zhang",
      "Cong Gao",
      "Linkang Yang",
      "Bingxu Han",
      "Minghao Hu",
      "Zhunchen Luo",
      "Guotong Geng",
      "Xiaoying Bai",
      "Jun Zhang",
      "Wen Yao",
      "Zhong Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.187": {
    "title": "DocAssistant: Integrating Key-region Reading and Step-wise Reasoning for Robust Document Visual Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinxu Zhang",
      "Qiyuan Fan",
      "Yu Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.188": {
    "title": "LNE-Blocking: An Efficient Framework for Contamination Mitigation Evaluation on Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruijie Hou",
      "Yueyang Jiao",
      "Hanxu Hu",
      "Yingming Li",
      "Wai Lam",
      "Huajian Zhang",
      "Hongyuan Lu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.189": {
    "title": "Enhancing Hate Speech Classifiers through a Gradient-assisted Counterfactual Text Generation Strategy",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Van Supranes",
      "Shaowen Peng",
      "Shoko Wakamiya",
      "Eiji Aramaki"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.190": {
    "title": "Learning SQL Like a Human: Structure-Aware Curriculum Learning for Text-to-SQL Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaohu Zhu",
      "Qian Li",
      "Lizhen Cui",
      "Yuntao Du"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.191": {
    "title": "Chain-of-Interactions: Multi-step Iterative ICL Framework for Abstractive Task-Oriented Dialogue Summarization of Conversational AI Interactions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jason S Lucas",
      "Ali Al Lawati",
      "Mahjabin Nahar",
      "John Chen",
      "Mahnoosh Mehrabani"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.192": {
    "title": "Your Semantic-Independent Watermark is Fragile: A Semantic Perturbation Attack against EaaS Watermark",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zekun Fei",
      "Biao Yi",
      "Jianing Geng",
      "He Ruiqi",
      "Lihai Nie",
      "Zheli Liu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.193": {
    "title": "Query Optimization for Parametric Knowledge Refinement in Retrieval-Augmented Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youan Cong",
      "Pritom Saha Akash",
      "Cheng Wang",
      "Kevin Chen-Chuan Chang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.194": {
    "title": "SKA-Bench: A Fine-Grained Benchmark for Evaluating Structured Knowledge Understanding of LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiqiang Liu",
      "Enpei Niu",
      "Yin Hua",
      "Mengshu Sun",
      "Lei Liang",
      "Huajun Chen",
      "Wen Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.195": {
    "title": "PD3F: A Pluggable and Dynamic DoS-Defense Framework against resource consumption attacks targeting Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanhe Zhang",
      "Xinyue Wang",
      "Haoran Gao",
      "Zhenhong Zhou",
      "Fanyu Meng",
      "Yuyao Zhang",
      "Sen Su"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.196": {
    "title": "From Implicit Exploration to Structured Reasoning: Guideline and Refinement for LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxiang Chen",
      "Zhuo Wang",
      "Mingxi Zou",
      "Zhucong Li",
      "Zhijian Zhou",
      "Song Wang",
      "Zenglin Xu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.197": {
    "title": "PIP: Perturbation-based Iterative Pruning for Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Cao",
      "Wei-Jie Xu",
      "Yucheng Shen",
      "Weijie Shi",
      "Chi-Min Chan",
      "Jianfeng Qu",
      "Jiajie Xu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.198": {
    "title": "Convolutional LoRA Aggregation for Unseen Tasks Adaptation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinhao Wu",
      "Jialin Liu",
      "Yutai Duan",
      "Jie Liu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.199": {
    "title": "CDT: A Comprehensive Capability Framework for Large Language Models Across Cognition, Domain, and Task",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haosi Mo",
      "Xinyu Ma",
      "Xuebo Liu",
      "Derek F. Wong",
      "Yu Li",
      "Jie Liu",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.200": {
    "title": "Multilingual Collaborative Defense for Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongliang Li",
      "Jinan Xu",
      "Gengping Cui",
      "Changhao Guan",
      "Fengran Mo",
      "Kaiyu Huang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.201": {
    "title": "Role-Guided Annotation and Prototype-Aligned Representation Learning for Historical Literature Sentiment Classification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongfei Du",
      "Jiacheng Shi",
      "Jacobo Myerston",
      "Sidi Lu",
      "Gang Zhou",
      "Ashley Gao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.202": {
    "title": "MetaMixSpeech: Meta Task Augmentation for Low-Resource Speech Recognition",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaqi Chen",
      "Hao Zhang",
      "Wenlin Zhang",
      "XuKui Yang",
      "Dan Qu",
      "Yunpeng Liu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.203": {
    "title": "RECAST: Retrieval-Augmented Contextual ASR via Decoder-State Keyword Spotting",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashish Mittal",
      "Sunita Sarawagi",
      "Preethi Jyothi"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.204": {
    "title": "PREE: Towards Harmless and Adaptive Fingerprint Editing in Large Language Models via Knowledge Prefix Enhancement",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xubin Yue",
      "Zhenhua Xu",
      "Wenpeng Xing",
      "Jiahui Yu",
      "Mohan Li",
      "Meng Han"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.205": {
    "title": "Beyond Spurious Signals: Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zichen Wu",
      "Hsiu-Yuan Huang",
      "Yunfang Wu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.206": {
    "title": "Text-centric Alignment for Bridging Test-time Unseen Modality",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yun-Da Tsai",
      "Ting-Yu Yen",
      "Pei-Fu Guo",
      "Zhe-Yan Li",
      "Shou-De Lin"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.207": {
    "title": "HierPrompt: Zero-Shot Hierarchical Text Classification with LLM-Enhanced Prototypes",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qian Zhang",
      "Qinliang Su",
      "Wei Zhu",
      "Pang Yachun"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.208": {
    "title": "RouterEval: A Comprehensive Benchmark for Routing LLMs to Explore Model-level Scaling Up in LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongzhan Huang",
      "Guoming Ling",
      "Yupei Lin",
      "Yandong Chen",
      "Shanshan Zhong",
      "Hefeng Wu",
      "Liang Lin"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.209": {
    "title": "Can We Steer Reasoning Direction by Thinking Intervention?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingsheng Zhang",
      "Luxi Xing",
      "Chen Zhang",
      "Yanbing Liu",
      "Yifan Deng",
      "Yunpeng Li",
      "Yue Hu",
      "Chenxu Niu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.210": {
    "title": "MPO: Boosting LLM Agents with Meta Plan Optimization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weimin Xiong",
      "Yifan Song",
      "Qingxiu Dong",
      "Bingchan Zhao",
      "Feifan Song",
      "XWang",
      "Sujian Li"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.211": {
    "title": "Exploring the Generalizability of Factual Hallucination Mitigation via Enhancing Precise Knowledge Utilization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyuan Zhang",
      "Yichi Zhang",
      "Yinpeng Dong",
      "Hang Su"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.212": {
    "title": "Learning What to Remember: Adaptive Probabilistic Memory Retention for Memory-Efficient Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "S M Rafiuddin",
      "Muntaha Nujat Khan"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.213": {
    "title": "Unlocking Smarter Device Control: Foresighted Planning with a World Model-Driven Code Execution Approach",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoran Yin",
      "Xu Luo",
      "Hao Wu",
      "Lianli Gao",
      "Jingkuan Song"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.214": {
    "title": "RGAR: Recurrence Generation-augmented Retrieval for Factual-aware Medical Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sichu Liang",
      "Linhai Zhang",
      "Hongyu Zhu",
      "Wenwen Wang",
      "Yulan He",
      "Deyu Zhou"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.215": {
    "title": "EcoSafeRAG: Efficient Security through Context Analysis in Retrieval-Augmented Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruobing Yao",
      "Yifei Zhang",
      "Shuang Song",
      "Neng Gao",
      "Chenyang Tu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.216": {
    "title": "StereoDetect: Detecting Stereotypes and Anti-stereotypes the Correct Way Using Social Psychological Underpinnings",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaustubh Shivshankar Shejole",
      "Pushpak Bhattacharyya"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.217": {
    "title": "Sparkle: Mastering Basic Spatial Capabilities in Vision Language Models Elicits Generalization to Spatial Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihong Tang",
      "Ao Qu",
      "Zhaokai Wang",
      "Dingyi Zhuang",
      "Zhaofeng Wu",
      "Wei Ma",
      "Shenhao Wang",
      "Yunhan Zheng",
      "Zhan Zhao",
      "Jinhua Zhao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.218": {
    "title": "How Does Knowledge Selection Help Retrieval Augmented Generation?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangci Li",
      "Jessica Ouyang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.219": {
    "title": "UPLex: Fine-Grained Personality Control in Large Language Models via Unsupervised Lexical Modulation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianlong Li",
      "Wenhao Liu",
      "Muling Wu",
      "Shihan Dou",
      "Zhenghua Wang",
      "Changze Lv",
      "Xiaohua Wang",
      "Xiaoqing Zheng",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.220": {
    "title": "ParetoRAG: Leveraging Sentence-Context Attention for Robust and Efficient Retrieval-Augmented Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruobing Yao",
      "Yifei Zhang",
      "Shuang Song",
      "Yuhan Liu",
      "Neng Gao",
      "Chenyang Tu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.221": {
    "title": "FlexQuant: A Flexible and Efficient Dynamic Precision Switching Framework for LLM Quantization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangxin Liu",
      "Zongwu Wang",
      "Jinhong Xia",
      "Junping Zhao",
      "Shouren Zhao",
      "Jinjin Li",
      "Jian Liu",
      "Li Jiang",
      "Haibing Guan"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.222": {
    "title": "ReLoop: \"Seeing Twice and Thinking Backwards\" via Closed-loop Training to Mitigate Hallucinations in Multimodal understanding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianjiang Yang",
      "Yanshu Li",
      "Ziyan Huang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.223": {
    "title": "Sequence Structure Aware Retriever for Procedural Document Retrieval: A New Dataset and Baseline",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenqi Ye",
      "HaoPeng Ren",
      "Yi Cai",
      "Qingbao Huang",
      "Jing Qin",
      "Pinli Zhu",
      "Songwen Gong"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.224": {
    "title": "The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Stap",
      "Christof Monz"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.225": {
    "title": "David vs. Goliath: Cost-Efficient Financial QA via Cascaded Multi-Agent Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenghao Liu",
      "Qian Liu",
      "Ziqin Zhu",
      "Hao Fei",
      "Aniket Mahanti"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.226": {
    "title": "Benchmarking Uncertainty Metrics for LLM Target-Aware Search",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pei-Fu Guo",
      "Yun-Da Tsai",
      "Shou-De Lin"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.227": {
    "title": "ZOGRASCOPE: A New Benchmark for Semantic Parsing over Property Graphs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesco Cazzaro",
      "Justin Kleindienst",
      "Sofia Márquez Gomez",
      "Ariadna Quattoni"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.228": {
    "title": "FG-PRM: Fine-grained Hallucination Detection and Mitigation in Language Model Mathematical Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruosen Li",
      "Ziming Luo",
      "Xinya Du"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.229": {
    "title": "Recipe2Plan: Evaluating Planning Abilities of LLMs for Efficient and Feasible Multitasking with Time Constraints Between Actions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zirui Wu",
      "Xiao Liu",
      "Jiayi Li",
      "Lingpeng Kong",
      "Yansong Feng"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.230": {
    "title": "Unlocking the Effectiveness of LoRA-FP for Seamless Transfer Implantation of Fingerprints in Downstream Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenhua Xu",
      "Zhaokun Yan",
      "Binhan Xu",
      "Xin Tong",
      "Haitao Xu",
      "Yourong Chen",
      "Meng Han"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.231": {
    "title": "AELC: Adaptive Entity Linking with LLM-Driven Contextualization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fang Wang",
      "Zhengwei Tao",
      "Ming Wang",
      "Minghao Hu",
      "Xiaoying Bai"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.232": {
    "title": "MetaLadder: Ascending Mathematical Solution Quality via Analogical-Problem Reasoning Transfer",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Honglin Lin",
      "Zhuoshi Pan",
      "Qizhi Pei",
      "Xin Gao",
      "Yu Li",
      "Mengzhang Cai",
      "Conghui He",
      "Lijun Wu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.233": {
    "title": "GLProtein: Global-and-Local Structure Aware Protein Representation Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunqing Liu",
      "Wenqi Fan",
      "Xiaoyong Wei",
      "Li Qing"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.234": {
    "title": "Reward Mixology: Crafting Hybrid Signals for Reinforcement Learning Driven In-Context Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changshuo Zhang",
      "Ang Gao",
      "Xiao Zhang",
      "Yong Liu",
      "Deyang Li",
      "Fangchao Liu",
      "Xinyu Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.235": {
    "title": "Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on Materials Characterization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengzhao Lai",
      "Youbin Zheng",
      "Zhenyang Cai",
      "Haonan Lyu",
      "Jingpu Yang",
      "Hong-Qing Liang",
      "Yan Hu",
      "Benyou Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.236": {
    "title": "GRADE: Generating multi-hop QA and fine-gRAined Difficulty matrix for RAG Evaluation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeongsoo Lee",
      "Daeyong Kwon",
      "Kyohoon Jin"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.237": {
    "title": "FusionDTI: Fine-grained Binding Discovery with Token-level Fusion for Drug-Target Interaction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaohan Meng",
      "Zaiqiao Meng",
      "Ke Yuan",
      "Iadh Ounis"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.238": {
    "title": "A Survey on Training-free Alignment of Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Birong Pan",
      "Yongqi Li",
      "Weiyu Zhang",
      "Wenpeng Lu",
      "Mayi Xu",
      "Shen Zhou",
      "Yuanyuan Zhu",
      "Ming Zhong",
      "Tieyun Qian"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.239": {
    "title": "CIVET: Systematic Evaluation of Understanding in VLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Massimo Rizzoli",
      "Simone Alghisi",
      "Olha Khomyn",
      "Gabriel Roccabruna",
      "Seyed Mahed Mousavi",
      "Giuseppe Riccardi"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.240": {
    "title": "How Does Cognitive Bias Affect Large Language Models? A Case Study on the Anchoring Effect in Price Negotiation Simulations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yoshiki Takenami",
      "Yin Jou Huang",
      "Yugo Murawaki",
      "Chenhui Chu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.241": {
    "title": "Enhancing Speech-to-Speech Dialogue Modeling with End-to-End Retrieval-Augmented Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengchao Feng",
      "Ziyang Ma",
      "Wenxi Chen",
      "Yao Li",
      "Sheng Wang",
      "Kai Yu",
      "Xie Chen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.242": {
    "title": "Backdoor-Powered Prompt Injection Attacks Nullify Defense Methods",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yulin Chen",
      "Haoran Li",
      "Yuan Sui",
      "Yangqiu Song",
      "Bryan Hooi"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.243": {
    "title": "Path-enhanced Pre-trained Language Model for Knowledge Graph Completion",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Wang",
      "Dandan Song",
      "Zhijing Wu",
      "Yuhang Tian",
      "Pan Yang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.244": {
    "title": "Zero-shot Cross-lingual NER via Mitigating Language Difference: An Entity-aligned Translation Perspective",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihao Zhang",
      "Sophia Yat Mei Lee",
      "Dong Zhang",
      "Shoushan Li",
      "Guodong Zhou"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.245": {
    "title": "Zero-Shot Cross-Domain Aspect-Based Sentiment Analysis via Domain-Contextualized Chain-of-Thought Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuming Shen",
      "Wei Wei",
      "Dong Wang",
      "Zhong-Hao Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.246": {
    "title": "Tree of Agents: Improving Long-Context Capabilities of Large Language Models through Multi-Perspective Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Song Yu",
      "Xiaofei Xu",
      "Ke Deng",
      "Li Li",
      "Lin Tian"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.247": {
    "title": "Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from the Arab World",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saeed Almheiri",
      "Rania Elbadry",
      "Mena Attia",
      "Chenxi Wang",
      "Preslav Nakov",
      "Timothy Baldwin",
      "Fajri Koto"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.248": {
    "title": "Enhancing Partially Relevant Video Retrieval with Robust Alignment Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Long Zhang",
      "Peipei Song",
      "Jianfeng Dong",
      "Kun Li",
      "Xun Yang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.249": {
    "title": "Multi-level Diagnosis and Evaluation for Robust Tabular Feature Engineering with Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yebin Lim",
      "Susik Yoon"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.250": {
    "title": "Prejudge-Before-Think: Enhancing Large Language Models at Test-Time by Process Prejudge Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianing Wang",
      "Jin Jiang",
      "Yang Liu",
      "Mengdi Zhang",
      "Xunliang Cai"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.251": {
    "title": "FroM: Frobenius Norm-Based Data-Free Adaptive Model Merging",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijian Li",
      "Xiaocheng Feng",
      "Huixin Liu",
      "Yichong Huang",
      "Ting Liu",
      "Bing Qin"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.252": {
    "title": "Dynamic Simulation Framework for Disinformation Dissemination and Correction With Social Bots",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boyu Qiao",
      "Kun Li",
      "Wei Zhou",
      "Songlin Hu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.253": {
    "title": "Beyond the First Error: Process Reward Models for Reflective Mathematical Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaohui Yang",
      "Chenghua He",
      "Xiaowen Shi",
      "Shihong Deng",
      "Linjing Li",
      "Qiyue Yin",
      "Daxin Jiang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.254": {
    "title": "PrAd: Prompt Adaptive Tuning for Decoder-only Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youneng Ma",
      "Junyi He",
      "Haojun Fei"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.255": {
    "title": "Personalized Question Answering with User Profile Generation and Compression",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hang Su",
      "Yun Yang",
      "Tianyang Liu",
      "Xin Liu",
      "Peng Pu",
      "Xuesong Lu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.256": {
    "title": "Dream to Chat: Model-based Reinforcement Learning on Dialogues with User Belief Modeling",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Zhao",
      "Xiaoyu Wang",
      "Dan Wang",
      "Zhonglin Jiang",
      "Qingqing Gu",
      "Teng Chen",
      "Ningyuan Xi",
      "Jinxian Qu",
      "Yong Chen",
      "Luo Ji"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.257": {
    "title": "FakeSV-VLM: Taming VLM for Detecting Fake Short-Video News via Progressive Mixture-Of-Experts Adapter",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "JunXi Wang",
      "Yaxiong Wang",
      "Lechao Cheng",
      "Zhun Zhong"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.258": {
    "title": "Beyond Inherent Cognition Biases in LLM-Based Event Forecasting: A Multi-Cognition Agentic Framework",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Wang",
      "Xi Zhou",
      "Yating Yang",
      "Bo Ma",
      "Lei Wang",
      "Rui Dong",
      "Azmat Anwar"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.259": {
    "title": "Breaking the Reviewer: Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tzu-Ling Lin",
      "Wei-Chih Chen",
      "Teng-Fang Hsiao",
      "Hou-I Liu",
      "Ya-Hsin Yeh",
      "Yu-Kai Chan",
      "Wen-Sheng Lien",
      "Po-Yen Kuo",
      "Philip S. Yu",
      "Hong-Han Shuai"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.260": {
    "title": "Watermarking with Low-Entropy POS-Guided Token Partitioning and Z-Score-Driven Dynamic Bias for Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "He Li",
      "Xiaojun Chen",
      "Zhendong Zhao",
      "Yunfei Yang",
      "Xin Zhao",
      "Jingcheng He"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.261": {
    "title": "Knowledge Graph-Driven Memory Editing with Directional Interventions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinhu Fu",
      "Kun Wang",
      "Chongye Guo",
      "Junfeng Fang",
      "Wentao Zhang",
      "Sen Su"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.262": {
    "title": "DTDES-KGE: Dual-Teacher Knowledge Distillation with Distinct Embedding Spaces for Knowledge Graph Embeddings",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bofan Wei",
      "Hongyuan Xu",
      "Yuhang Niu",
      "Jiarui Ren",
      "Yanlong Wen",
      "Xiaojie Yuan"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.263": {
    "title": "LLMEval-Med: A Real-world Clinical Benchmark for Medical LLMs with Physician Validation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming Zhang",
      "Yujiong Shen",
      "Zelin Li",
      "Huayu Sha",
      "Binze Hu",
      "Yuhui Wang",
      "Chenhao Huang",
      "Shichun Liu",
      "Jingqi Tong",
      "Changhao Jiang",
      "Mingxu Chai",
      "Zhiheng Xi",
      "Shihan Dou",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.264": {
    "title": "Watermark Smoothing Attacks against Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyan Chang",
      "Hamed Hassani",
      "Reza Shokri"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.265": {
    "title": "PICD-Instruct: A Generative Instruction Learning Framework for Few-Shot Multi-Intent Spoken Language Understanding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenbin Hua",
      "Rui Fan",
      "Tingting He",
      "Ming Dong"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.266": {
    "title": "Forewarned is Forearmed: Pre-Synthesizing Jailbreak-like Instructions to Enhance LLM Safety Guardrail to Potential Attacks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng Liu",
      "Qiang Sheng",
      "Danding Wang",
      "Yang Li",
      "Guang Yang",
      "Juan Cao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.267": {
    "title": "Are Knowledge and Reference in Multilingual Language Models Cross-Lingually Consistent?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xi Ai",
      "Mahardika Krisna Ihsani",
      "Min-Yen Kan"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.268": {
    "title": "Krikri: Advancing Open Large Language Models for Greek",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dimitris Roussis",
      "Leon Voukoutis",
      "Georgios Paraskevopoulos",
      "Sokratis Sofianopoulos",
      "Prokopis Prokopidis",
      "Vassilis Papavassileiou",
      "Athanasios Katsamanis",
      "Stelios Piperidis",
      "Vassilis Katsouros"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.269": {
    "title": "Beyond the Scientific Document: A Citation-Aware Multi-Granular Summarization Approach with Heterogeneous Graphs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quoc-An Nguyen",
      "Xuan-Hung Le",
      "Thi-Minh-Thu Vu",
      "Hoang-Quynh Le"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.270": {
    "title": "Detecting Continuously Evolving Scam Calls under Limited Annotation: A LLM-Augmented Expert Rule Framework",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Ma",
      "Qinliang Su",
      "Minhua Huang",
      "Wu Kai"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.271": {
    "title": "An Empirical Study of Position Bias in Modern Information Retrieval",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyang Zeng",
      "Dun Zhang",
      "Jiacheng Li",
      "Zoupanxiang",
      "Yudong Zhou",
      "Yuqing Yang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.272": {
    "title": "GenPoE: Generative Passage-level Mixture of Experts for Knowledge Enhancement of LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuebing Liu",
      "Shanbao Qiao",
      "Seung-Hoon Na"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.273": {
    "title": "CoRanking: Collaborative Ranking with Small and Large Ranking Agents",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhan Liu",
      "Xinyu Ma",
      "Yutao Zhu",
      "Lixin Su",
      "Shuaiqiang Wang",
      "Dawei Yin",
      "Zhicheng Dou"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.274": {
    "title": "HIRAG: Hierarchical-Thought Instruction-Tuning Retrieval-Augmented Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihan Jiao",
      "Zhehao Tan",
      "Dan Yang",
      "Duolin Sun",
      "Jie Feng",
      "Yue Shen",
      "Jian Wang",
      "Peng Wei"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.275": {
    "title": "Towards Personalized Conversational Sales Agents: Contextual User Profiling for Strategic Action",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tongyoung Kim",
      "Jeongeun Lee",
      "SooJin Yoon",
      "SungHwan Kim",
      "Dongha Lee"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.276": {
    "title": "WebCoT: Enhancing Web Agent Reasoning by Reconstructing Chain-of-Thought in Reflection, Branching, and Rollback",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minda Hu",
      "Tianqing Fang",
      "Jianshu Zhang",
      "Jun-Yu Ma",
      "Zhisong Zhang",
      "Jingyan Zhou",
      "Hongming Zhang",
      "Haitao Mi",
      "Dong Yu",
      "Irwin King"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.277": {
    "title": "Interesting Culture: Social Relation Recognition from Videos via Culture De-confounding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Zhang",
      "Yangfu Zhu",
      "Haorui Wang",
      "Bin Wu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.278": {
    "title": "ThinkSwitcher: When to Think Hard, When to Think Fast",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guosheng Liang",
      "Longguang Zhong",
      "Ziyi Yang",
      "Xiaojun Quan"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.279": {
    "title": "MaGiX: A Multi-Granular Adaptive Graph Intelligence Framework for Enhancing Cross-Lingual RAG",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nguyen Manh Hieu",
      "Vu Lam Anh",
      "Hung Pham Van",
      "Nam Le Hai",
      "Linh Ngo Van",
      "Nguyen Thi Ngoc Diep",
      "Thien Huu Nguyen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.280": {
    "title": "LexTime: A Benchmark for Temporal Ordering of Legal Events",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Claire Barale",
      "Leslie Barrett",
      "Vikram Sunil Bajaj",
      "Michael Rovatsos"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.281": {
    "title": "Beyond the Surface: A Solution-Aware Retrieval Model for Competition-level Code Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiwen Zhang",
      "Lingxiang Wang",
      "Hainan Zhang",
      "Ziwei Wang",
      "Sijia Wen",
      "Zhiming Zheng"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.282": {
    "title": "X-Boundary: Establishing Exact Safety Boundary to Shield LLMs from Jailbreak Attacks without Compromising Usability",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoya Lu",
      "Dongrui Liu",
      "Yi Yu",
      "Luxin Xu",
      "Jing Shao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.283": {
    "title": "Tag&Tab: Pretraining Data Detection in Large Language Models Using Keyword-Based Membership Inference Attack",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sagiv Antebi",
      "Edan Habler",
      "Asaf Shabtai",
      "Yuval Elovici"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.284": {
    "title": "EcoLANG: Efficient and Effective Agent Communication Language Induction for Social Simulation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyi Mou",
      "Chen Qian",
      "Wei Liu",
      "Ling Yan",
      "Yao Hu",
      "Xuanjing Huang",
      "Zhongyu Wei"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.285": {
    "title": "Revealing the Inherent Instructability of Pre-Trained Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seokhyun An",
      "Minji Kim",
      "Hyounghun Kim"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.286": {
    "title": "What Media Frames Reveal About Stance: A Dataset and Study about Memes in Climate Change Discourse",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shijia Zhou",
      "Siyao Peng",
      "Simon M. Luebke",
      "Jörg Haßler",
      "Mario Haim",
      "Saif M. Mohammad",
      "Barbara Plank"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.287": {
    "title": "Rethinking Personality Assessment from Human-Agent Dialogues: Fewer Rounds May Be Better Than More",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baiqiao Zhang",
      "Zhifeng Liao",
      "Xiangxian Li",
      "Chao Zhou",
      "Juan Liu",
      "Xiaojuan Ma",
      "Yulong Bian"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.288": {
    "title": "TailorRPA: A Retrieval-Based Framework for Eliciting Personalized and Coherent Role-Playing Agents in General Domain",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenpeng Gao",
      "Xiaofen Xing",
      "Xiangmin Xu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.289": {
    "title": "SCE: Semantic Consistency Enhanced Reinforcement Learning for Multi-Hop Knowledge Graph Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanwen Huang",
      "Yao Liu",
      "Qiao Liu",
      "Rui Hou",
      "Tingting Dai"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.290": {
    "title": "ReGraphRAG: Reorganizing Fragmented Knowledge Graphs for Multi-Perspective Retrieval-Augmented Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soohyeong Kim",
      "Seok Jun Hwang",
      "JungHyoun Kim",
      "Jeonghyeon Park",
      "Yong Suk Choi"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.291": {
    "title": "GASE: Generatively Augmented Sentence Encoding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manuel Frank",
      "Haithem Afli"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.292": {
    "title": "The \"r\" in \"woman\" stands for rights. Auditing LLMs in Uncovering Social Dynamics in Implicit Misogyny",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arianna Muti",
      "Chris Emmery",
      "Debora Nozza",
      "Alberto Barrón-Cedeño",
      "Tommaso Caselli"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.293": {
    "title": "Fact Verification on Knowledge Graph via Programmatic Graph Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanzhen Hao",
      "Desheng Wu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.294": {
    "title": "Agent Trading Arena: A Study on Numerical Understanding in LLM-Based Agents",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianmi Ma",
      "Jiawei Du",
      "Wenxin Huang",
      "Wenjie Wang",
      "Liang Xie",
      "Xian Zhong",
      "Joey Tianyi Zhou"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.295": {
    "title": "Why We Feel What We Feel: Joint Detection of Emotions and Their Opinion Triggers in E-commerce",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arnav Attri",
      "Anuj Attri",
      "Suman Banerjee",
      "Amey Patil",
      "Muthusamy Chelliah",
      "Nikesh Garera",
      "Pushpak Bhattacharyya"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.296": {
    "title": "Use Random Selection for Now: Investigation of Few-Shot Selection Strategies in LLM-based Text Augmentation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan Cegin",
      "Branislav Pecher",
      "Jakub Simko",
      "Ivan Srba",
      "Maria Bielikova",
      "Peter Brusilovsky"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.297": {
    "title": "BanglaByT5: Byte-Level Modelling for Bangla",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pramit Bhattacharyya",
      "Arnab Bhattacharya"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.298": {
    "title": "XTRA: Cross-Lingual Topic Modeling with Topic and Representation Alignments",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tien Phat Nguyen",
      "Ngo Vu Minh",
      "Tung Nguyen",
      "Linh Ngo Van",
      "Duc Anh Nguyen",
      "Dinh Viet Sang",
      "Trung Le"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.299": {
    "title": "CodeContests+: High-Quality Test Case Generation for Competitive Programming",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihan Wang",
      "Siyao Liu",
      "Yang Sun",
      "Ming Ding",
      "Hongyan Li"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.300": {
    "title": "SPO: Self Preference Optimization with Self Regularization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhao Sun",
      "Yifan Zhang",
      "Quandong Wang",
      "Qinzhuo Wu",
      "Wei Liu",
      "Jian Luan"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.301": {
    "title": "Long-context Language Models Fail in Basic Retrieval Tasks Without Sufficient Reasoning Steps",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yijiong Yu",
      "Zhixiao Qi",
      "Yongfeng Huang",
      "Wei Wang",
      "Weifeng.liu",
      "Ran Chen",
      "Ji Pei"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.302": {
    "title": "Benchmarking Critical Questions Generation: A Challenging Reasoning Task for Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Blanca Calvo Figueras",
      "Rodrigo Agerri"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.303": {
    "title": "ResearchArena: Benchmarking Large Language Models' Ability to Collect and Organize Information as Research Agents",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Kang",
      "Chenyan Xiong"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.304": {
    "title": "LLMs are Privacy Erasable",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zipeng Ye",
      "Wenjian Luo"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.305": {
    "title": "How Good are LLM-based Rerankers? An Empirical Analysis of State-of-the-Art Reranking Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abdelrahman Abdallah",
      "Bhawna Piryani",
      "Jamshid Mozafari",
      "Mohammed Ali",
      "Adam Jatowt"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.306": {
    "title": "DeAR: Dual-Stage Document Reranking with Reasoning Agents via LLM Distillation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abdelrahman Abdallah",
      "Jamshid Mozafari",
      "Bhawna Piryani",
      "Adam Jatowt"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.307": {
    "title": "CANDY: Benchmarking LLMs' Limitations and Assistive Potential in Chinese Misinformation Fact-Checking",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiling Guo",
      "Xinwei Yang",
      "Chen Huang",
      "Tong Zhang",
      "Yong Hu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.308": {
    "title": "E-Verify: A Paradigm Shift to Scalable Embedding-based Factuality Verification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyang Liu",
      "Jingfeng Xue",
      "Xiuqi Yang",
      "Wenbiao Du",
      "Jiarun Fu",
      "Junbao Chen",
      "Wenjie Guo",
      "Yong Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.309": {
    "title": "LLM Jailbreak Detection for (Almost) Free!",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guorui Chen",
      "Yifan Xia",
      "Xiaojun Jia",
      "Zhijiang Li",
      "Philip Torr",
      "Jindong Gu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.310": {
    "title": "When to Continue Thinking: Adaptive Thinking Mode Switching for Efficient Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyun Zhang",
      "Jingqing Ruan",
      "Xing Ma",
      "Yawen Zhu",
      "Haodong Zhao",
      "Hao Li",
      "Jiansong Chen",
      "Ke Zeng",
      "Xunliang Cai"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.311": {
    "title": "Plugging Schema Graph into Multi-Table QA: A Human-Guided Framework for Reducing LLM Reliance",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xixi Wang",
      "Miguel Costa",
      "Jordanka Kovaceva",
      "Shuai Wang",
      "Francisco C. Pereira"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.312": {
    "title": "Evolution in Simulation: AI-Agent School with Dual Memory for High-Fidelity Educational Dynamics",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng Jin",
      "Haoming Wang",
      "Zhiqi Gao",
      "Yongbo Yang",
      "Bao Chunjia",
      "Chengliang Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.313": {
    "title": "Retrieval-Augmented Machine Translation with Unstructured Knowledge",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaan Wang",
      "Fandong Meng",
      "Yingxue Zhang",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.314": {
    "title": "MARS-Bench: A Multi-turn Athletic Real-world Scenario Benchmark for Dialogue Evaluation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenghao Yang",
      "Yinbo Luo",
      "Zhoufutu Wen",
      "Qi Chu",
      "Tao Gong",
      "Longxiang Liu",
      "Kaiyuan Zhang",
      "Jianpeng Jiao",
      "Ge Zhang",
      "Wenhao Huang",
      "Nenghai Yu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.315": {
    "title": "UTMath: A Benchmark for Math Evaluation with Unit Test",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Yang",
      "Qingping Yang",
      "Yingwei Ma",
      "Runtao Liu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.316": {
    "title": "The Green KNIGHT: Green Machine Translation with Knowledge-Distilled, Narrow, Inexpensive, Greedy, Hybrid Transformers",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andreas Guta",
      "Frithjof Petrick",
      "Peter Polák"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.317": {
    "title": "Constructing Your Model's Value Distinction: Towards LLM Alignment with Anchor Words Tuning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Yang",
      "Ping Jian",
      "Chengzhi Li",
      "Chenxu Wang",
      "Xinyue Zhang",
      "Wenpeng Lu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.318": {
    "title": "MCiteBench: A Multimodal Benchmark for Generating Text with Citations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Caiyu Hu",
      "Yikai Zhang",
      "Tinghui Zhu",
      "Yiwei Ye",
      "Yanghua Xiao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.319": {
    "title": "Do LLMs Know and Understand Domain Conceptual Knowledge?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sijia Shen",
      "Feiyan Jiang",
      "Peiyan Wang",
      "Yubo Feng",
      "Yuchen Jiang",
      "Chang Liu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.320": {
    "title": "Agent Laboratory: Using LLM Agents as Research Assistants",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Schmidgall",
      "Yusheng Su",
      "Ze Wang",
      "Ximeng Sun",
      "Jialian Wu",
      "Xiaodong Yu",
      "Jiang Liu",
      "Michael Moor",
      "Zicheng Liu",
      "Emad Barsoum"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.321": {
    "title": "Retrieval-Augmented Generation with Hierarchical Knowledge",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Huang",
      "Yongfeng Huang",
      "Yang Junjie",
      "Zhenyu Pan",
      "Yongqiang Chen",
      "Kaili Ma",
      "Hongzhi Chen",
      "James Cheng"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.322": {
    "title": "Regularized Contrastive Decoding with Hard Negative Samples for LLM Hallucination Mitigation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haonan Sheng",
      "Dou Hu",
      "Lingwei Wei",
      "Wei Zhou",
      "Songlin Hu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.323": {
    "title": "CharacterCraft: Bridging the Literature-Reality Dialogue Gap for Practical Role-Playing Agents",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuyan Yin",
      "Xinran Yang",
      "Zihao Li",
      "Lixin Zou",
      "Chenliang Li"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.324": {
    "title": "Drift: Decoding-time Personalized Alignments with Implicit User Preferences",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minbeom Kim",
      "Kang-il Lee",
      "Seongho Joo",
      "Hwaran Lee",
      "Thibaut Thonet",
      "Kyomin Jung"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.325": {
    "title": "Discovering Semantic Subdimensions through Disentangled Conceptual Representations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunhao Zhang",
      "Shaonan Wang",
      "Nan Lin",
      "Xinyi Dong",
      "Chong Li",
      "Chengqing Zong"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.326": {
    "title": "Identifying Aspects in Peer Reviews",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng Lu",
      "Ilia Kuznetsov",
      "Iryna Gurevych"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.327": {
    "title": "Tree-Structured Non-Autoregressive Decoding for Sequence-to-Sequence Text Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengyu Ji",
      "Yufei Liu",
      "Xiang Hu",
      "Kewei Tu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.328": {
    "title": "Towards More Efficient Post-training via Fourier Domain Adapter Framework",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yijia Fan",
      "Jusheng Zhang",
      "Keze Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.329": {
    "title": "KERAG: Knowledge-Enhanced Retrieval-Augmented Generation for Advanced Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yushi Sun",
      "Kai Sun",
      "Yifan Ethan Xu",
      "Xiao Yang",
      "Xin Luna Dong",
      "Nan Tang",
      "Lei Chen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.330": {
    "title": "Not All Features Deserve Attention: Graph-Guided Dependency Learning for Tabular Data Generation with Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheyu Zhang",
      "Shuo Yang",
      "Bardh Prenkaj",
      "Gjergji Kasneci"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.331": {
    "title": "CCG: Rare-Label Prediction via Neural SEM–Driven Causal Game",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yijia Fan",
      "Jusheng Zhang",
      "Kaitong Cai",
      "Jing Yang",
      "Keze Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.332": {
    "title": "Multimodal Emotion Recognition in Conversations: A Survey of Methods, Trends, Challenges and Prospects",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "ChengYan Wu",
      "Yiqiang Cai",
      "Yang Liu",
      "Pengxu Zhu",
      "Yun Xue",
      "Ziwei Gong",
      "Julia Hirschberg",
      "Bolei Ma"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.333": {
    "title": "When Allies Turn Foes: Exploring Group Characteristics of LLM-Based Multi-Agent Collaborative Systems Under Adversarial Attacks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahao Zhang",
      "Baoshuo Kan",
      "Tao Gong",
      "Fu Lee Wang",
      "Tianyong Hao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.334": {
    "title": "EditID: Training-Free Editable ID Customization for Text-to-Image Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guandong Li",
      "Zhaobin Chu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.335": {
    "title": "OSC: Cognitive Orchestration through Dynamic Knowledge Alignment in Multi-Agent LLM Collaboration",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jusheng Zhang",
      "Yijia Fan",
      "Kaitong Cai",
      "Xiaofei Sun",
      "Keze Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.336": {
    "title": "VideoLLM Knows When to Speak: Enhancing Time-Sensitive Video Comprehension with Video-Text Duet Interaction Format",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yueqian Wang",
      "Xiaojun Meng",
      "Yuxuan Wang",
      "Jianxin Liang",
      "Jiansheng Wei",
      "Huishuai Zhang",
      "Dongyan Zhao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.337": {
    "title": "To Answer or Not to Answer (TAONA): A Robust Textual Graph Understanding and Question Answering Approach",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchen Yan",
      "Aakash Kolekar",
      "Sahika Genc",
      "Wenju Xu",
      "Edward W Huang",
      "Anirudh Srinivasan",
      "Mukesh Jain",
      "Qi He",
      "Hanghang Tong"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.338": {
    "title": "Understanding Refusal in Language Models with Sparse Autoencoders",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Jie Yeo",
      "Nirmalendu Prakash",
      "Clement Neo",
      "Ranjan Satapathy",
      "Roy Ka-Wei Lee",
      "Erik Cambria"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.339": {
    "title": "Where Did That Come From? Sentence-Level Error-Tolerant Attribution",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ori Ernst",
      "Aviv Slobodkin",
      "Meng Cao",
      "Sihui Wei",
      "Jackie CK Cheung"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.340": {
    "title": "Alleviating Performance Degradation Caused by Out-of-Distribution Issues in Embedding-Based Retrieval",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haotong Bao",
      "Jianjin Zhang",
      "Qi Chen",
      "Weihao Han",
      "Zhengxin Zeng",
      "Ruiheng Chang",
      "Mingzheng Li",
      "Hao Sun",
      "Weiwei Deng",
      "Feng Sun",
      "Qi Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.341": {
    "title": "Can LLMs Find a Needle in a Haystack? A Look at Anomaly Detection Language Modeling",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leslie Barrett",
      "Vikram Sunil Bajaj",
      "Robert John Kingan"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.342": {
    "title": "Beyond Single Frames: Can LMMs Comprehend Implicit Narratives in Comic Strip?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaochen Wang",
      "Heming Xia",
      "Jialin Song",
      "Longyu Guan",
      "Qingxiu Dong",
      "Rui Li",
      "Yixin Yang",
      "Yifan Pu",
      "Weiyao Luo",
      "Yiru Wang",
      "Xiangdi Meng",
      "Wenjie Li",
      "Zhifang Sui"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.343": {
    "title": "Enhancing Multi-Agent Debate System Performance via Confidence Expression",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijie Lin",
      "Bryan Hooi"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.344": {
    "title": "The Face of Persuasion: Analyzing Bias and Generating Culture-Aware Ads",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aysan Aghazadeh",
      "Adriana Kovashka"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.345": {
    "title": "SIFT: Grounding LLM Reasoning in Contexts via Stickers",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihao Zeng",
      "Xuyao Huang",
      "Boxiu Li",
      "Zhijie Deng"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.346": {
    "title": "When Inverse Data Outperforms: Exploring the Pitfalls of Mixed Data in Multi-Stage Fine-Tuning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengyi Deng",
      "Xin Li",
      "Tingyu Zhu",
      "Zhicheng Yang",
      "Zhijiang Guo",
      "Wei Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.347": {
    "title": "LUME: LLM Unlearning with Multitask Evaluations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anil Ramakrishna",
      "Yixin Wan",
      "Xiaomeng Jin",
      "Kai-Wei Chang",
      "Zhiqi Bu",
      "Bhanukiran Vinzamuri",
      "Volkan Cevher",
      "Mingyi Hong",
      "Rahul Gupta"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.348": {
    "title": "How do Language Models Generate Slang: A Systematic Comparison between Human and Machine-Generated Slang Usages",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyang Wu",
      "Zhewei Sun"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.349": {
    "title": "Bridging the Dynamic Perception Gap: Training-Free Draft Chain-of-Thought for Dynamic Multimodal Spatial Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siqu Ou",
      "Hongcheng Liu",
      "Pingjie Wang",
      "Yusheng Liao",
      "Chuan Xuan",
      "Yanfeng Wang",
      "Yu Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.350": {
    "title": "MedCOD: Enhancing English-to-Spanish Medical Translation of Large Language Models Using Enriched Chain-of-Dictionary Framework",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Shahidul Salim",
      "Lian Fu",
      "Arav Adikesh Ramakrishnan",
      "Zonghai Yao",
      "Hong Yu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.351": {
    "title": "Chatbot To Help Patients Understand Their Health",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Won Seok Jang",
      "Hieu Tran",
      "Manav Shaileshkumar Mistry",
      "Sai Kiran Gandluri",
      "Yifan Zhang",
      "Sharmin Sultana",
      "Sunjae Kwon",
      "Yuan Zhang",
      "Zonghai Yao",
      "Hong Yu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.352": {
    "title": "A Knapsack by Any Other Name: Presentation impacts LLM performance on NP-hard problems",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alex Duchnowski",
      "Ellie Pavlick",
      "Alexander Koller"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.353": {
    "title": "Is Safety Standard Same for Everyone? User-Specific Safety Evaluation of Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yeonjun In",
      "Wonjoong Kim",
      "Kanghoon Yoon",
      "Sungchul Kim",
      "Mehrab Tanjim",
      "Sangwu Park",
      "Kibum Kim",
      "Chanyoung Park"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.354": {
    "title": "Jailbreak Attack Initializations as Extractors of Compliance Directions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amit LeVi",
      "Rom Himelstein",
      "Yaniv Nemcovsky",
      "Avi Mendelson",
      "Chaim Baskin"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.355": {
    "title": "Train Once for All: A Transitional Approach for Efficient Aspect Sentiment Triplet Extraction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinmeng Hou",
      "Lingyue Fu",
      "Chenhao Meng",
      "Kounianhua Du",
      "Hai Hu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.356": {
    "title": "A Comprehensive Survey on the Trustworthiness of Large Language Models in Healthcare",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manar Aljohani",
      "Jun Hou",
      "Sindhura Kommu",
      "Xuan Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.357": {
    "title": "Self-Correction Makes LLMs Better Parsers",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyan Zhang",
      "Yang Hou",
      "Chen Gong",
      "Zhenghua Li"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.358": {
    "title": "Explaining Length Bias in LLM-Based Preference Evaluations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengyu Hu",
      "Linxin Song",
      "Jieyu Zhang",
      "Zheyuan Xiao",
      "Tianfu Wang",
      "Zhengyu Chen",
      "Nicholas Jing Yuan",
      "Jianxun Lian",
      "Kaize Ding",
      "Hui Xiong"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.359": {
    "title": "Investigating Controversy Framing across Topics on Social Media",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maxwell Weinzierl",
      "Sanda M. Harabagiu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.360": {
    "title": "HEAL: Hybrid Enhancement with LLM-based Agents for Text-attributed Hypergraph Self-supervised Representation Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruochang Li",
      "Xiao Luo",
      "Zhiping Xiao",
      "Wei Ju",
      "Ming Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.361": {
    "title": "ReMamba: Equip Mamba with Effective Long-Sequence Modeling",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Danlong Yuan",
      "Jiahao Liu",
      "Bei Li",
      "Huishuai Zhang",
      "Jingang Wang",
      "Xunliang Cai",
      "Dongyan Zhao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.362": {
    "title": "QUITO-X: A New Perspective on Context Compression from the Information Bottleneck Theory",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihang Wang",
      "Xu Huang",
      "Bowen Tian",
      "Yueyang Su",
      "Lei Yu",
      "Huaming Liao",
      "Yixing Fan",
      "Jiafeng Guo",
      "Xueqi Cheng"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.363": {
    "title": "Conv-Basis: A New Paradigm for Efficient Attention Inference and Gradient Computation in Transformers",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingyu Liang",
      "Heshan Liu",
      "Zhenmei Shi",
      "Zhao Song",
      "Zhuoyan Xu",
      "Jiale Zhao",
      "Zhen Zhuang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.364": {
    "title": "Mitigating Gender Bias via Fostering Exploratory Thinking in LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kangda Wei",
      "Hasnat Md Abdullah",
      "Ruihong Huang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.365": {
    "title": "Beyond the Textual: Generating Coherent Visual Options for MCQs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wanqiang Wang",
      "Longzhu He",
      "Wei Zheng"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.366": {
    "title": "SafeSwitch: Steering Unsafe LLM Behavior via Internal Activation Signals",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peixuan Han",
      "Cheng Qian",
      "Xiusi Chen",
      "Yuji Zhang",
      "Heng Ji",
      "Denghui Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.367": {
    "title": "MADD: Multi-Agent Drug Discovery Orchestra",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gleb Vitalevich Solovev",
      "Alina Borisovna Zhidkovskaya",
      "Anastasia Orlova",
      "Nina Gubina",
      "Anastasia Vepreva",
      "Rodion Golovinskii",
      "Ilya Tonkii",
      "Ivan Dubrovsky",
      "Ivan Gurev",
      "Dmitry Gilemkhanov",
      "Denis Chistiakov",
      "Timur A. Aliev",
      "Ivan Poddiakov",
      "Galina Zubkova",
      "Ekaterina V. Skorb",
      "Vladimir Vinogradov",
      "Alexander Boukhanovsky",
      "Nikolay Nikitin",
      "Andrei Dmitrenko",
      "Anna Kalyuzhnaya",
      "Andrey Savchenko"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.368": {
    "title": "PersonaGym: Evaluating Persona Agents and LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vinay Samuel",
      "Henry Peng Zou",
      "Yue Zhou",
      "Shreyas Chaudhari",
      "Ashwin Kalyan",
      "Tanmay Rajpurohit",
      "Ameet Deshpande",
      "Karthik R Narasimhan",
      "Vishvak Murahari"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.369": {
    "title": "LM2Protein: A Structure-to-Token Protein Large Language Model",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chang Zhou",
      "Yuheng Shan",
      "Pengan Chen",
      "Xiangyu Shi",
      "Zikang Wang",
      "Yanting Li",
      "Jiyue Jiang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.370": {
    "title": "How Well Can Reasoning Models Identify and Recover from Unhelpful Thoughts?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sohee Yang",
      "Sang-Woo Lee",
      "Nora Kassner",
      "Daniela Gottesman",
      "Sebastian Riedel",
      "Mor Geva"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.371": {
    "title": "From Token to Action: State Machine Reasoning to Mitigate Overthinking in Information Retrieval",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dohyeon Lee",
      "Yeonseok Jeong",
      "Seung-won Hwang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.372": {
    "title": "Locate-then-Merge: Neuron-Level Parameter Fusion for Mitigating Catastrophic Forgetting in Multimodal LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeping Yu",
      "Sophia Ananiadou"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.373": {
    "title": "Improving Influence-based Instruction Tuning Data Selection for Balanced Learning of Diverse Capabilities",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qirun Dai",
      "Dylan Zhang",
      "Jiaqi W. Ma",
      "Hao Peng"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.374": {
    "title": "Diagnosing Moral Reasoning Acquisition in Language Models: Pragmatics and Generalization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangliang Liu",
      "Zimo Qi",
      "Xitong Zhang",
      "Lei Jiang",
      "Kristen Johnson"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.375": {
    "title": "Discourse Heuristics For Paradoxically Moral Self-Correction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangliang Liu",
      "Zimo Qi",
      "Xitong Zhang",
      "Kristen Johnson"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.376": {
    "title": "Invisible Prompts, Visible Threats: Malicious Font Injection in External Resources for Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junjie Xiong",
      "Changjia Zhu",
      "Shuhang Lin",
      "Chong Zhang",
      "Yongfeng Zhang",
      "Yao Liu",
      "Lingyao Li"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.377": {
    "title": "Turning the Tide: Repository-based Code Reflection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Zhang",
      "Jian Yang",
      "Jiaxi Yang",
      "Ya Wang",
      "Zhoujun Li",
      "Zeyu Cui",
      "Binyuan Hui",
      "Junyang Lin"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.378": {
    "title": "Reinforcement Learning with Supervised Alignment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "João Luís Lins",
      "Jia Xu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.379": {
    "title": "EmByte: Decomposition and Compression Learning for Small yet Private NLP",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shenglan Li",
      "Jia Xu",
      "Mengjiao Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.380": {
    "title": "GUARD: Glocal Uncertainty-Aware Robust Decoding for Effective and Efficient Open-Ended Text Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanhao Ding",
      "Esteban Garces Arias",
      "Meimingwei Li",
      "Julian Rodemann",
      "Matthias Aßenmacher",
      "Danlu Chen",
      "Gaojuan Fan",
      "Christian Heumann",
      "Chongsheng Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.381": {
    "title": "Efficiently Editing Mixture-of-Experts Models with Compressed Experts",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei He",
      "Yang Liu",
      "Chen Liang",
      "Hany Hassan Awadalla"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.382": {
    "title": "FinGEAR: Financial Mapping-Guided Enhanced Answer Retrieval",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ying Li",
      "Mengyu Wang",
      "Miguel de Carvalho",
      "Sotirios Sabanis",
      "Tiejun Ma"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.383": {
    "title": "FM2DS: Few-Shot Multimodal Multihop Data Synthesis with Knowledge Distillation for Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amirhossein Abaskohi",
      "Spandana Gella",
      "Giuseppe Carenini",
      "Issam H. Laradji"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.384": {
    "title": "SQUARE: Unsupervised Retrieval Adaptation via Synthetic Data",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinsung Yoon",
      "Junhao Zeng",
      "Sercan O Arik"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.385": {
    "title": "Knowledge-enhanced Multimodal ECG Representation Learning with Arbitrary-Lead Inputs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Che Liu",
      "Cheng Ouyang",
      "Zhongwei Wan",
      "Haozhe Wang",
      "Wenjia Bai",
      "Rossella Arcucci"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.386": {
    "title": "Seeing Race, Feeling Bias: Emotion Stereotyping in Multimodal Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mahammed Kamruzzaman",
      "Amanda Cercas Curry",
      "Alba Cercas Curry",
      "Flor Miriam Plaza-del-Arco"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.387": {
    "title": "AdaptMerge: Inference Time Adaptive Visual and Language-Guided Token Merging for Efficient Large Multimodal Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zahidul Islam",
      "Mrigank Rochan"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.388": {
    "title": "Federated Retrieval-Augmented Generation: A Systematic Mapping Study",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhijit Chakraborty",
      "Chahana Dahal",
      "Vivek Gupta"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.389": {
    "title": "A Survey of Pun Generation: Datasets, Evaluations and Methodologies",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchen Su",
      "Yonghua Zhu",
      "Ruofan Wang",
      "Zijian Huang",
      "Diana Benavides-Prado",
      "Michael J. Witbrock"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.390": {
    "title": "Evaluating the Robustness and Accuracy of Text Watermarking Under Real-World Cross-Lingual Manipulations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mansour Al Ghanim",
      "Jiaqi Xue",
      "Rochana Prih Hastuti",
      "Mengxin Zheng",
      "Yan Solihin",
      "Qian Lou"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.391": {
    "title": "HDiff: Confidence-Guided Denoising Diffusion for Robust Hyper-relational Link Prediction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangfeng Luo",
      "Ruoxin Zheng",
      "Jianqiang Huang",
      "Hang Yu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.392": {
    "title": "Spotlighter: Revisiting Prompt Tuning from a Representative Mining View",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yutong Gao",
      "Maoyuan Shao",
      "Xinyang Huang",
      "Chuang Zhu",
      "Yu Weng",
      "Xuan Liu",
      "Lijuan Sun",
      "Guoshun Nan"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.393": {
    "title": "Offloaded Reasoning: Efficient Inference for Large Language Models via Modular Reasoning and Refinement",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ishan Jindal",
      "Jayant Taneja",
      "Badrinath Chandana",
      "Vikas Kapur",
      "Sachin Dev Sharma"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.394": {
    "title": "Wait, We Don't Need to \"Wait\"! Removing Thinking Tokens Improves Reasoning Efficiency",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenlong Wang",
      "Yuanning Feng",
      "Dongping Chen",
      "Zhaoyang Chu",
      "Ranjay Krishna",
      "Tianyi Zhou"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.395": {
    "title": "Towards Reverse Engineering of Language Models: A Survey",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinpeng Ti",
      "Wentao Ye",
      "Zhifang Zhang",
      "Junbo Zhao",
      "Chang Yao",
      "Lei Feng",
      "Haobo Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.396": {
    "title": "LIFTED: Multimodal Clinical Trial Outcome Prediction via Large Language Models and Mixture-of-Experts",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhao Zheng",
      "Liaoyaqi Wang",
      "Dongshen Peng",
      "Hongxia Xu",
      "Yun Li",
      "Hongtu Zhu",
      "Tianfan Fu",
      "Huaxiu Yao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.397": {
    "title": "Addition in Four Movements: Mapping Layer-wise Information Trajectories in LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yao Yan"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.398": {
    "title": "CoMoE: Contrastive Representation for Mixture-of-Experts in Parameter-Efficient Fine-tuning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinyuan Feng",
      "ChaoPeng Wei",
      "Tenghai Qiu",
      "Tianyi Hu",
      "Zhiqiang Pu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.399": {
    "title": "GuiLoMo: Allocating Experts and Ranks for LoRA-MoE via Bilevel Optimization with GuidedSelection Vectors",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinrong Chen",
      "Hengyuan Zhang",
      "Yingmin Qiu",
      "Xiao Liang",
      "Ziyue Li",
      "Guanyu Wang",
      "Weiping Li",
      "Tong Mo",
      "Hayden Kwok-Hay So",
      "Ngai Wong"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.400": {
    "title": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating Rotation and Learnable Non-uniform Quantizer",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Euntae Choi",
      "Sumin Song",
      "Woosang Lim",
      "Sungjoo Yoo"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.401": {
    "title": "Decoding in Latent Spaces for Efficient Inference in LLM-based Recommendation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengbing Wang",
      "Yang Zhang",
      "Zhicheng Wang",
      "Tianhao Shi",
      "Keqin Bao",
      "Fuli Feng",
      "Tat-Seng Chua"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.402": {
    "title": "Forget for Get: A Lightweight Two-phase Gradient Method for Knowledge Editing in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanhong Li",
      "Min Yang",
      "Xiping Hu",
      "Chengming Li"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.403": {
    "title": "AutoEvolve: Automatically Evolving Queries for Applicable and Scalable Retrieval-Augmented Generation Benchmarking",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ding-Chu Zhang",
      "Xiaowen Zhang",
      "Yue Fei",
      "Renjun Hu",
      "Xiao-Wen Yang",
      "Zhi Zhou",
      "Baixuan Li",
      "Yu-Feng Li",
      "Xing Shi",
      "Wei Lin"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.404": {
    "title": "Temporal Alignment of Time Sensitive Facts with Activation Engineering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanjay Govindan",
      "Maurice Pagnucco",
      "Yang Song"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.405": {
    "title": "ChronoBias: A Benchmark for Evaluating Temporal Group Bias in the Time-sensitive Knowledge of Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyungmin Kim",
      "Youngbin Choi",
      "Hyounghun Kim",
      "Dongwoo Kim",
      "Sangdon Park"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.406": {
    "title": "MC2: A Minimum-Coverage and Dataset-Agnostic Framework for Compositional Generalization of LLMs on Semantic Parsing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyao Xu",
      "Zhe Yang",
      "Houfeng Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.407": {
    "title": "Learning to Instruct: Fine-Tuning a Task-Aware Instruction Optimizer for Black-Box LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunzhe Qi",
      "Jinjin Tian",
      "Tianci Liu",
      "Ruirui Li",
      "Tianxin Wei",
      "Hui Liu",
      "Xianfeng Tang",
      "Monica Xiao Cheng",
      "Jingrui He"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.408": {
    "title": "Enriching Patent Claim Generation with European Patent Dataset",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lekang Jiang",
      "Chengzu Li",
      "Stefan Goetz"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.409": {
    "title": "StepKE: Stepwise Knowledge Editing for Multi-Hop Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaewook Lee",
      "Dahyun Jung",
      "Heuiseok Lim"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.410": {
    "title": "AutoDCWorkflow: LLM-based Data Cleaning Workflow Auto-Generation and Benchmark",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lan Li",
      "Liri Fang",
      "Bertram Ludäscher",
      "Vetle I Torvik"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.411": {
    "title": "Hidden Ghost Hand: Unveiling Backdoor Vulnerabilities in MLLM-Powered Mobile GUI Agents",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengzhou Cheng",
      "Haowen Hu",
      "Zheng Wu",
      "Zongru Wu",
      "Tianjie Ju",
      "Daizong Ding",
      "Zhuosheng Zhang",
      "Gongshen Liu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.412": {
    "title": "Scale Down to Speed Up: Dynamic Data Selection for Reinforcement Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoyue Chen",
      "Jihai Zhang",
      "Ben Liu",
      "Fangquan Lin",
      "Wotao Yin"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.413": {
    "title": "Towards Efficient CoT Distillation: Self-Guided Rationale Selector for Better Performance with Fewer Rationales",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "JianZhi Yan",
      "Le Liu",
      "Youcheng Pan",
      "Shiwei Chen",
      "Yang Xiang",
      "Buzhou Tang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.414": {
    "title": "GeoDANO: Geometric VLM with Domain Agnostic Vision Encoder",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seunghyuk Cho",
      "Zhenyue Qin",
      "Yang Liu",
      "Youngbin Choi",
      "Seungbeom Lee",
      "Dongwoo Kim"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.415": {
    "title": "Leveraging 3D Gaussian for Temporal Knowledge Graph Embedding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiang Li",
      "Xiangdong Su",
      "Guanglai Gao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.416": {
    "title": "LLMAP: LLM-Assisted Multi-Objective Route Planning with User Preferences",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liangqi Yuan",
      "Dong-Jun Han",
      "Christopher Brinton",
      "Sabine Brunswicker"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.417": {
    "title": "ZEBRA: Leveraging Model-Behavioral Knowledge for Zero-Annotation Preference Dataset Construction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeesu Jung",
      "Chanjun Park",
      "Sangkeun Jung"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.418": {
    "title": "Token Knowledge: A New Perspective For Knowledge in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jieyong Wang",
      "Chunyao Song",
      "Tingjian Ge"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.419": {
    "title": "Adaptive Schema-aware Event Extraction with Retrieval-Augmented Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng Liang",
      "Hang Lv",
      "Zhihao Wen",
      "Yaxiong Wu",
      "Yongyue Zhang",
      "Hao Wang",
      "Yong Liu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.420": {
    "title": "Enhancing Attributed Question Answering using Tailored Progressive Curriculum Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhan Chen",
      "Bowei Zou",
      "Yifan Fan",
      "Yuchong Chen",
      "Shujun Cao",
      "Yu Hong"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.421": {
    "title": "REAR: Reinforced Reasoning Optimization for Event Argument Extraction with Relation-Aware Support",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianwen Luo",
      "Yu Hong",
      "Shuai Yang",
      "Jianmin Yao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.422": {
    "title": "COMI-LINGUA: Expert Annotated Large-Scale Dataset for Multitask NLP in Hindi-English Code-Mixing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rajvee Sheth",
      "Himanshu Beniwal",
      "Mayank Singh"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.423": {
    "title": "Nine Ways to Break Copyright Law and Why Our LLM Won't: A Fair Use Aligned Generation Framework",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aakash Sen Sharma",
      "Debdeep Sanyal",
      "Priyansh Srivastava",
      "Sundar Athreya H",
      "Shirish Karande",
      "Mohan Kankanhalli",
      "Murari Mandal"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.424": {
    "title": "InteractSpeech: A Speech Dialogue Interaction Corpus for Spoken Dialogue Model",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifu Chen",
      "Shengpeng Ji",
      "Ziqing Wang",
      "Hanting Wang",
      "Zhou Zhao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.425": {
    "title": "Enhancing SQL Table Acquisition with Reverse Engineering for Text-to-SQL",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shixin Liu",
      "Haoyu Xu",
      "Yu Hong"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.426": {
    "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiabin Zhou",
      "Wenbin Wang",
      "Minyan Zeng",
      "Jiaxian Guo",
      "Xuebo Liu",
      "Li Shen",
      "Min Zhang",
      "Liang Ding"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.427": {
    "title": "ASD-iLLM:An Intervention Large Language Model for Autistic Children based on Real Clinical Dialogue Intervention Dataset",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuzhong Lai",
      "Chenxi Li",
      "Junhong Lai",
      "Yucun Zhong",
      "Chenyu Yan",
      "Xiang Li",
      "Haifeng Li",
      "Gang Pan",
      "Lin Yao",
      "Yueming Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.428": {
    "title": "GDLLM: A Global Distance-aware Modeling Approach Based on Large Language Models for Event Temporal Relation Extraction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Zhao",
      "Wanting Ning",
      "Yuxiao Fei",
      "Yubo Feng",
      "Lishuang Li"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.429": {
    "title": "More Tokens, Lower Precision: Towards the Optimal Token-Precision Trade-off in KV Cache Compression",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiebin Zhang",
      "Dawei Zhu",
      "Yifan Song",
      "Wenhao Wu",
      "Chuqiao Kuang",
      "Xiaoguang Li",
      "Lifeng Shang",
      "Qun Liu",
      "Sujian Li"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.430": {
    "title": "cAST: Enhancing Code Retrieval-Augmented Generation with Structural Chunking via Abstract Syntax Tree",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilin Zhang",
      "Xinran Zhao",
      "Zora Zhiruo Wang",
      "Chenyang Yang",
      "Jiayi Wei",
      "Tongshuang Wu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.431": {
    "title": "A Group Fairness Lens for Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanqun Bi",
      "Yuqiang Xie",
      "Lei Shen",
      "Yanan Cao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.432": {
    "title": "VLM Is a Strong Reranker: Advancing Multimodal Retrieval-augmented Generation via Knowledge-enhanced Reranking and Noise-injected Training",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhanpeng Chen",
      "Chengjin Xu",
      "Yiyan Qi",
      "Xuhui Jiang",
      "Jian Guo"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.433": {
    "title": "Rethinking DPO: The Role of Rejected Responses in Preference Misalignment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jae Hyeon Cho",
      "JunHyeok Oh",
      "Myunsoo Kim",
      "Byung-Jun Lee"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.434": {
    "title": "Enhancing Recommendation Explanations through User-Centric Refinement",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingsen Zhang",
      "Zihang Tian",
      "Xueyang Feng",
      "Xu Chen",
      "Chong Chen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.435": {
    "title": "Distributional Surgery for Language Model Activations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bao Nguyen",
      "Binh Nguyen",
      "Duy Nguyen",
      "Viet Anh Nguyen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.436": {
    "title": "Improving Alignment in LVLMs with Debiased Self-Judgment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sihan Yang",
      "Chenhang Cui",
      "Zihao Zhao",
      "Yiyang Zhou",
      "Weilong Yan",
      "Ying Wei",
      "Huaxiu Yao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.437": {
    "title": "Low-Confidence Gold: Refining Low-Confidence Samples for Efficient Instruction Tuning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyi Cai",
      "Jie Li",
      "Mohammad Mahdinur Rahman",
      "Wenzhen Dong"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.438": {
    "title": "Safeguarding Privacy of Retrieval Data against Membership Inference Attacks: Is This Query Too Close to Home?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujin Choi",
      "Youngjoo Park",
      "Junyoung Byun",
      "Jaewook Lee",
      "Jinseong Park"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.439": {
    "title": "Causal-LLM: A Unified One-Shot Framework for Prompt- and Data-Driven Causal Graph Discovery",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amartya Roy",
      "N Devharish",
      "Shreya Ganguly",
      "Kripabandhu Ghosh"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.440": {
    "title": "LRPLAN: A Multi-Agent Collaboration of Large Language and Reasoning Models for Planning with Implicit & Explicit Constraints",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "T Karthikeyan",
      "Om Dehlan",
      "Mausam",
      "Manish Gupta"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.441": {
    "title": "DLPO: Towards a Robust, Efficient, and Generalizable Prompt Optimization Framework from a Deep-Learning Perspective",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dengyun Peng",
      "Yuhang Zhou",
      "Qiguang Chen",
      "JinHao Liu",
      "Jingjing Chen",
      "Libo Qin",
      "Wanxiang Che"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.442": {
    "title": "Towards Robust Few-Shot Relation Classification: Incorporating Relation Description with Agreement",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengting Hu",
      "Jianfeng Wu",
      "Ming Jiang",
      "Yalan Xie",
      "Zhunheng Wang",
      "Rui Ying",
      "Xiaoyi Liu",
      "Ruixuan Xu",
      "Hang Gao",
      "Renhong Cheng"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.443": {
    "title": "For a Fistful of Puns: Evaluating a Puns in Multiword Expressions Identification Algorithm Without Dedicated Dataset",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julien Bezançon",
      "Gaël Lejeune"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.444": {
    "title": "Watermarking for Factuality: Guiding Vision-Language Models Toward Truth via Tri-layer Contrastive Decoding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyungryul Back",
      "Seongbeom Park",
      "Milim Kim",
      "Mincheol Kwon",
      "SangHyeok Lee",
      "Hyunyoung Lee",
      "Junhee Cho",
      "Seunghyun Park",
      "Jinkyu Kim"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.445": {
    "title": "Are the Reasoning Models Good at Automated Essay Scoring?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lui Yoshida"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.446": {
    "title": "Rethinking LLM-Based Recommendations: A Personalized Query-Driven Parallel Integration",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Donghee Han",
      "Hwanjun Song",
      "Mun Yong Yi"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.447": {
    "title": "RefVNLI: Towards Scalable Evaluation of Subject-driven Text-to-image Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aviv Slobodkin",
      "Hagai Taitelbaum",
      "Yonatan Bitton",
      "Brian Gordon",
      "Michal Sokolik",
      "Nitzan Bitton Guetta",
      "Almog Gueta",
      "Royi Rassin",
      "Dani Lischinski",
      "Idan Szpektor"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.448": {
    "title": "What data should I include in my POS tagging training set?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zoey Liu",
      "Masoud Jasbi",
      "Christan Grant",
      "Kenji Sagae",
      "Emily Prud’hommeaux"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.449": {
    "title": "AttnComp: Attention-Guided Adaptive Context Compression for Retrieval-Augmented Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lvzhou Luo",
      "Yixuan Cao",
      "Ping Luo"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.450": {
    "title": "SafeInt: Shielding Large Language Models from Jailbreak Attacks via Safety-Aware Representation Intervention",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi Wu",
      "Chen Chen",
      "Chunyan Hou",
      "Xiaojie Yuan"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.451": {
    "title": "Staged Knowledge Distillation Through Least-to-Most Prompting: Optimizing Teacher Guidance via Difficulty-Aware Training",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengxiang Zhang",
      "Lingyuan Liu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.452": {
    "title": "LLM Distillation for Efficient Few-Shot Multiple Choice Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Patrick Sutanto",
      "Joan Santoso",
      "Esther Irawati Setiawan",
      "Aji Prasetya Wibawa"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.453": {
    "title": "Teaching LLMs to Plan, Not Just Solve: Plan Learning Boosts LLMs Generalization in Reasoning Tasks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianlong Wang",
      "Junzhe Chen",
      "Weibin Liao",
      "Xueting Han",
      "Jing Bai"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.454": {
    "title": "FedCoT: Federated Chain-of-Thought Distillation for Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Fan",
      "Weijing Chen",
      "Yan Kang",
      "Guoqiang Ma",
      "Hanlin Gu",
      "Yuanfeng Song",
      "Lixin Fan",
      "Qiang Yang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.455": {
    "title": "SalaMAnder: Shapley-based Mathematical Expression Attribution and Metric for Chain-of-Thought Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Xin",
      "Chen Shen",
      "Shaotian Yan",
      "Xiaosong Yuan",
      "Yaoming Wang",
      "Xiaofeng Zhang",
      "Chenxi Huang",
      "Jieping Ye"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.456": {
    "title": "Representing LLMs in Prompt Semantic Task Space",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Idan Kashani",
      "Avi Mendelson",
      "Yaniv Nemcovsky"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.457": {
    "title": "PersLLM: A Personified Training Approach for Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheni Zeng",
      "Jiayi Chen",
      "Huimin Chen",
      "Yukun Yan",
      "Yuxuan Chen",
      "Zhenghao Liu",
      "Zhiyuan Liu",
      "Maosong Sun"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.458": {
    "title": "The Illusion of Randomness: How LLMs Fail to Emulate Stochastic Decision-Making in Rock-Paper-Scissors Games?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihao Guo",
      "Hongtao Lv",
      "Chaoli Zhang",
      "Yibowen Zhao",
      "Yixin Zhang",
      "Lizhen Cui"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.459": {
    "title": "DAPE-BR: Distance-Aware Positional Encoding for Mitigating Object Hallucination in LVLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingrui Xie",
      "Tianxiang Xu",
      "Qianhai Tang",
      "Shanming Yao",
      "Xiaofeng Zhang",
      "Junliang Du"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.460": {
    "title": "From Confidence to Collapse in LLM Factual Robustness",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alina Fastowski",
      "Bardh Prenkaj",
      "Gjergji Kasneci"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.461": {
    "title": "CtrlNews: LLM-based Multi-Agent Controllable News Writing via Knowledge Gravitational Field",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Xu",
      "Yingjie Zong",
      "Wang Zhonghua",
      "Sirui Wu",
      "Yuan Rao",
      "Dan Zhang",
      "Shuiguang Deng"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.462": {
    "title": "Joint Enhancement of Relational Reasoning for Long-Context LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhirui Chen",
      "Wei Shen",
      "Jiashui Huang",
      "Ling Shao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.463": {
    "title": "Training Medical QA Models Based on Mixed Rewards from Multiple-Choice and Open-Ended Questions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Qiu",
      "Yujan Ting",
      "Pei Dong",
      "Terrence Chen",
      "Weijing Huang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.464": {
    "title": "Rethink Rumor Detection in the Era of LLMs: A Review",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chang Yang",
      "Peng Zhang",
      "Jing Zhang",
      "Hui Gao",
      "Changhao Song"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.465": {
    "title": "ScholarBench: A Bilingual Benchmark for Abstraction, Comprehension, and Reasoning Evaluation in Academic Contexts",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongwon Noh",
      "Donghyeok Koh",
      "Junghun Yuk",
      "Gyuwan Kim",
      "Jae Yong Lee",
      "KyungTae Lim",
      "Cheoneum Park"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.466": {
    "title": "MAGIC: A Multi-Hop and Graph-Based Benchmark for Inter-Context Conflicts in Retrieval-Augmented Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jungyeon Lee",
      "Lee Kangmin",
      "Taeuk Kim"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.467": {
    "title": "Align Attention Heads Before Merging Them: An Effective Way for Converting MHA to GQA",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingyun Jin",
      "Xiaohui Song",
      "Feng Zhou",
      "Zengchang Qin"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.468": {
    "title": "DRBO: Mitigating Short Board Effect via Dynamic Reward Balancing in Multi-reward LLM Optimization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nuo Chen",
      "Yufei Gao",
      "Yongnan Jin",
      "Yan Hu",
      "Anningzhe Gao",
      "Lingyong Yan",
      "Benyou Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.469": {
    "title": "Enhancing LLM Knowledge Learning through Generalization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingkang Zhu",
      "Xi Chen",
      "Zhongdao Wang",
      "Bei Yu",
      "Hengshuang Zhao",
      "Jiaya Jia"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.470": {
    "title": "FastCuRL: Curriculum Reinforcement Learning with Stage-wise Context Scaling for Efficient Training R1-like Reasoning Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingyang Song",
      "Mao Zheng",
      "Zheng Li",
      "Wenjie Yang",
      "Xuan Luo"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.471": {
    "title": "TR-MTEB: A Comprehensive Benchmark and Embedding Model Suite for Turkish Sentence Representations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mehmet Selman Baysan",
      "Tunga Gungor"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.472": {
    "title": "ImpRAG: Retrieval-Augmented Generation with Implicit Queries",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenzheng Zhang",
      "Xi Victoria Lin",
      "Karl Stratos",
      "Wen-tau Yih",
      "Mingda Chen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.473": {
    "title": "HEAL: A Hypothesis-Based Preference-Aware Analysis Framework",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifu Huo",
      "Chenglong Wang",
      "Qiren Zhu",
      "Shunjie Xing",
      "Tong Xiao",
      "Chunliang Zhang",
      "Tongran Liu",
      "JingBo Zhu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.474": {
    "title": "A Survey of Multilingual Reasoning in Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akash Ghosh",
      "Debayan Datta",
      "Sriparna Saha",
      "Chirag Agarwal"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.475": {
    "title": "CLEAR: A Framework Enabling Large Language Models to Discern Confusing Legal Paragraphs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Xu",
      "Qian Liu",
      "Hao Fei",
      "Hang Yu",
      "Shuhao Guan",
      "Xiao Wei"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.476": {
    "title": "NAP2: A Benchmark for Naturalness and Privacy-Preserving Text Rewriting by Learning from Human",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuo Huang",
      "William Maclean",
      "Xiaoxi Kang",
      "Qiongkai Xu",
      "Zhuang Li",
      "Xingliang Yuan",
      "Gholamreza Haffari",
      "Lizhen Qu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.477": {
    "title": "Chain of Ideas: Revolutionizing Research Via Novel Idea Development with LLM Agents",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Long Li",
      "Weiwen Xu",
      "Jiayan Guo",
      "Ruochen Zhao",
      "Xingxuan Li",
      "Yuqian Yuan",
      "Boqiang Zhang",
      "Yuming Jiang",
      "Yifei Xin",
      "Ronghao Dang",
      "Yu Rong",
      "Deli Zhao",
      "Tian Feng",
      "Lidong Bing"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.478": {
    "title": "Unveiling Multimodal Processing: Exploring Activation Patterns in Multimodal LLMs for Interpretability and Efficiency",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuan Wu",
      "Meng Su",
      "Youxuan Fang",
      "Shaolin Zhu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.479": {
    "title": "Self-Supervised Prompt Optimization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinyu Xiang",
      "Jiayi Zhang",
      "Zhaoyang Yu",
      "Xinbing Liang",
      "Fengwei Teng",
      "Jinhao Tu",
      "Fashen Ren",
      "Xiangru Tang",
      "Sirui Hong",
      "Chenglin Wu",
      "Yuyu Luo"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.480": {
    "title": "Polish-English medical knowledge transfer: A new benchmark and results",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Łukasz Grzybowski",
      "Jakub Pokrywka",
      "Michał Ciesiółka",
      "Jeremi Ignacy Kaczmarek",
      "Marek Kubis"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.481": {
    "title": "Hard Negatives, Hard Lessons: Revisiting Training Data Quality for Robust Information Retrieval with LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nandan Thakur",
      "Crystina Zhang",
      "Xueguang Ma",
      "Jimmy Lin"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.482": {
    "title": "EventRelBench: A Comprehensive Benchmark for Evaluating Event Relation Understanding in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Gong",
      "Biaoshuai Zheng",
      "Qiwang Hu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.483": {
    "title": "S2LPP: Small-to-Large Prompt Prediction across LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liang Cheng",
      "Tianyi Li",
      "Zhaowei Wang",
      "Mark Steedman"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.484": {
    "title": "DroidCall: A Dataset for LLM-powered Android Intent Invocation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weikai Xie",
      "Li Zhang",
      "Shihe Wang",
      "Rongjie Yi",
      "Mengwei Xu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.485": {
    "title": "Tool Zero: Training Tool-Augmented LLMs via Pure RL from Scratch",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yirong Zeng",
      "Xiao Ding",
      "Yutai Hou",
      "Yuxian Wang",
      "Li Du",
      "Juyi Dai",
      "Qiuyang Ding",
      "Duyu Tang",
      "Dandan Tu",
      "Weiwen Liu",
      "Bing Qin",
      "Ting Liu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.486": {
    "title": "INREACT: An Inspire-Then-Reinforce Training Framework For Multimodal GUI Agent",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanlei Wang",
      "Liuzhou Zhang",
      "Haohao Luo",
      "Ying Shen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.487": {
    "title": "Facts Fade Fast: Evaluating Memorization of Outdated Medical Knowledge in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juraj Vladika",
      "Mahdi Dhaini",
      "Florian Matthes"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.488": {
    "title": "Zero-Shot Privacy-Aware Text Rewriting via Iterative Tree Search",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuo Huang",
      "Xingliang Yuan",
      "Gholamreza Haffari",
      "Lizhen Qu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.489": {
    "title": "KoLEG: On-the-Fly Korean Legal Knowledge Editing with Continuous Retrieval",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaehyung Seo",
      "Dahyun Jung",
      "Jaewook Lee",
      "Yongchan Chun",
      "Dongjun Kim",
      "Hwijung Ryu",
      "Donghoon Shin",
      "Heuiseok Lim"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.490": {
    "title": "HARE: an entity and relation centric evaluation framework for histopathology reports",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunsoo Kim",
      "Michal Wen Sheue Ong",
      "Alex Shavick",
      "Honghan Wu",
      "Adam P. Levine"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.491": {
    "title": "VeriFastScore: Speeding up long-form factuality evaluation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rishanth Rajendhran",
      "Amir Zadeh",
      "Matthew Sarte",
      "Chuan Li",
      "Mohit Iyyer"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.492": {
    "title": "B-REASO: A Multi-Level Multi-Faceted Bengali Evaluation Suite for Foundation Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Tanzib Hosain",
      "Md Kishor Morol"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.493": {
    "title": "Extracting Conceptual Spaces from LLMs Using Prototype Embeddings",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nitesh Kumar",
      "Usashi Chatterjee",
      "Steven Schockaert"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.494": {
    "title": "FC-Attack: Jailbreaking Multimodal Large Language Models via Auto-Generated Flowcharts",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyi Zhang",
      "Zhen Sun",
      "Zongmin Zhang",
      "Jihui Guo",
      "Xinlei He"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.495": {
    "title": "Multilingual Data Filtering using Synthetic Data from Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonas Waldendorf",
      "Barry Haddow",
      "Alexandra Birch",
      "Mateusz Klimaszewski"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.496": {
    "title": "SAFE: A Sparse Autoencoder-Based Framework for Robust Query Enrichment and Hallucination Mitigation in LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samir Abdaljalil",
      "Filippo Pallucchini",
      "Andrea Seveso",
      "Hasan Kurban",
      "Fabio Mercorio",
      "Erchin Serpedin"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.497": {
    "title": "Soteria: Language-Specific Functional Parameter Steering for Multilingual Safety Alignment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Somnath Banerjee",
      "Sayan Layek",
      "Pratyush Chatterjee",
      "Animesh Mukherjee",
      "Rima Hazra"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.498": {
    "title": "LLMs as a synthesis between symbolic and distributed approaches to language",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gemma Boleda"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.499": {
    "title": "MIND: Towards Immersive Psychological Healing with Multi-Agent Inner Dialogue",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujia Chen",
      "Changsong Li",
      "Yiming Wang",
      "Tianjie Ju",
      "Qingqing Xiao",
      "Nan Zhang",
      "Zifan Kong",
      "Peng Wang",
      "Binyu Yan"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.500": {
    "title": "A Monte-Carlo Sampling Framework For Reliable Evaluation of Large Language Models Using Behavioral Analysis",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Davood Wadi",
      "Marc Fredette"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.501": {
    "title": "Understanding How Value Neurons Shape the Generation of Specified Values in LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Su",
      "Jiayi Zhang",
      "Shu Yang",
      "Xinhai Wang",
      "Lijie Hu",
      "Di Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.502": {
    "title": "Likelihood Variance as Text Importance for Resampling Texts to Map Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Momose Oyama",
      "Ryo Kishino",
      "Hiroaki Yamagiwa",
      "Hidetoshi Shimodaira"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.503": {
    "title": "Think Twice, Generate Once: Safeguarding by Progressive Self-Reflection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hoang Phan",
      "Victor Li",
      "Qi Lei"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.504": {
    "title": "Efficient Integration of External Knowledge to LLM-based World Models via Retrieval-Augmented Generation and Reinforcement Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chang Yang",
      "Xinrun Wang",
      "Qinggang Zhang",
      "Qi Jiang",
      "Xiao Huang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.505": {
    "title": "Comparing Apples to Oranges: A Dataset & Analysis of LLM Humour Understanding from Traditional Puns to Topical Jokes",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tyler Loakman",
      "William Thorne",
      "Chenghua Lin"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.506": {
    "title": "Modeling, Evaluating, and Embodying Personality in LLMs: A Survey",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Iago Alves Brito",
      "Julia Soares Dollis",
      "Fernanda Bufon Färber",
      "Pedro Schindler Freire Brasil Ribeiro",
      "Rafael Teixeira Sousa",
      "Arlindo Rodrigues Galvão Filho"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.507": {
    "title": "Benchmarking the Detection of LLMs-Generated Modern Chinese Poetry",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shanshan Wang",
      "Junchao Wu",
      "Fengying Ye",
      "Derek F. Wong",
      "Jingming Yao",
      "Lidia S. Chao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.508": {
    "title": "Leveraging the Cross-Domain & Cross-Linguistic Corpus for Low Resource NMT: A Case Study On Bhili-Hindi-English Parallel Corpus",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pooja Singh",
      "Shashwat Bhardwaj",
      "Vaibhav Sharma",
      "Sandeep Kumar"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.509": {
    "title": "Creative Preference Optimization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mete Ismayilzada",
      "Antonio Laverghetta Jr.",
      "Simone A. Luchini",
      "Reet Patel",
      "Antoine Bosselut",
      "Lonneke Van Der Plas",
      "Roger E. Beaty"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.510": {
    "title": "Assistant-Guided Mitigation of Teacher Preference Bias in LLM-as-a-Judge",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuo Liu",
      "Moxin Li",
      "Xun Deng",
      "Qifan Wang",
      "Fuli Feng"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.511": {
    "title": "Uplift-RAG: Uplift-Driven Knowledge Preference Alignment for Retrieval-Augmented Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changle Qu",
      "Sunhao Dai",
      "Hengyi Cai",
      "Yiyang Cheng",
      "Jun Xu",
      "Shuaiqiang Wang",
      "Dawei Yin"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.512": {
    "title": "Sugar-Coated Poison: Benign Generation Unlocks Jailbreaking",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhang Wu",
      "Yu-Jie Xiong",
      "Hao Zhang",
      "Jia-Chen Zhang",
      "Zheng Zhou"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.513": {
    "title": "DivScene: Towards Open-Vocabulary Object Navigation with Large Vision Language Models in Diverse Scenes",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaowei Wang",
      "Hongming Zhang",
      "Tianqing Fang",
      "Ye Tian",
      "Yue Yang",
      "Kaixin Ma",
      "Xiaoman Pan",
      "Yangqiu Song",
      "Dong Yu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.514": {
    "title": "Data-scarce Behavior Editing of Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joykirat Singh",
      "Subhabrata Dutta",
      "Tanmoy Chakraborty"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.515": {
    "title": "FIER: Fine-Grained and Efficient KV Cache Retrieval for Long-context LLM Inference",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongwei Wang",
      "Zijie Liu",
      "Song Wang",
      "Yuxin Ren",
      "Jianing Deng",
      "Jingtong Hu",
      "Tianlong Chen",
      "Huanrui Yang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.516": {
    "title": "SVeritas: Benchmark for Robust Speaker Verification under Diverse Conditions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Massa Baali",
      "Sarthak Bisht",
      "Francisco Teixeira",
      "Kateryna Shapovalenko",
      "Rita Singh",
      "Bhiksha Raj"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.517": {
    "title": "CAARMA: Class Augmentation with Adversarial Mixup Regularization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Massa Baali",
      "Xiang Li",
      "Hao Chen",
      "Syed Abdul Hannan",
      "Rita Singh",
      "Bhiksha Raj"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.518": {
    "title": "Bringing Pedagogy into Focus: Evaluating Virtual Teaching Assistants' Question-Answering in Asynchronous Learning Environments",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li Siyan",
      "Zhen Xu",
      "Vethavikashini Chithrra Raghuram",
      "Xuanming Zhang",
      "Renzhe Yu",
      "Zhou Yu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.519": {
    "title": "Demystifying Multilingual Reasoning in Process Reward Modeling",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weixuan Wang",
      "Minghao Wu",
      "Barry Haddow",
      "Alexandra Birch"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.520": {
    "title": "BehaviorSFT: Behavioral Token Conditioning for Health Agents Across the Proactivity Spectrum",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yubin Kim",
      "Zhiyuan Hu",
      "Hyewon Jeong",
      "Eugene W Park",
      "Shuyue Stella Li",
      "Chanwoo Park",
      "Shiyun Xiong",
      "MingYu Lu",
      "Hyeonhoon Lee",
      "Xin Liu",
      "Daniel McDuff",
      "Cynthia Breazeal",
      "Samir Tulebaev",
      "Hae Won Park"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.521": {
    "title": "LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure Profiles",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ho Yin Sam Ng",
      "Edward Hsu",
      "Aashish Anantha Ramakrishnan",
      "Branislav Kveton",
      "Nedim Lipka",
      "Franck Dernoncourt",
      "Dongwon Lee",
      "Tong Yu",
      "Sungchul Kim",
      "Ryan A. Rossi",
      "Ting-Hao Kenneth Huang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.522": {
    "title": "Efficient Dynamic Clustering-Based Document Compression for Retrieval-Augmented-Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weitao Li",
      "Xiangyu Zhang",
      "Kaiming Liu",
      "Xuanyu Lei",
      "Weizhi Ma",
      "Yang Liu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.523": {
    "title": "HebID: Detecting Social Identities in Hebrew-language Political Text",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guy Mor-Lan",
      "Naama Rivlin-Angert",
      "Yael R. Kaplan",
      "Tamir Sheafer",
      "Shaul R. Shenhav"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.524": {
    "title": "Dub-S2ST: Textless Speech-to-Speech Translation for Seamless Dubbing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeongsoo Choi",
      "Jaehun Kim",
      "Joon Son Chung"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.525": {
    "title": "FinGrAct: A Framework for FINe-GRrained Evaluation of ACTionability in Explainable Automatic Fact-Checking",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Islam Eldifrawi",
      "Shengrui Wang",
      "Amine Trabelsi"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.526": {
    "title": "What Has Been Lost with Synthetic Evaluation?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Gill",
      "Abhilasha Ravichander",
      "Ana Marasovic"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.527": {
    "title": "Bold Claims or Self-Doubt? Factuality Hallucination Type Detection via Belief State",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongyu Zhang",
      "Qingqing Hong",
      "Bingxuan Hou",
      "Jiayi Lin",
      "Chenyang Zhang",
      "Jialin Li",
      "Junli Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.528": {
    "title": "Proxy Barrier: A Hidden Repeater Layer Defense Against System Prompt Leakage and Jailbreaking",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pedro Schindler Freire Brasil Ribeiro",
      "Iago Alves Brito",
      "Rafael Teixeira Sousa",
      "Fernanda Bufon Färber",
      "Julia Soares Dollis",
      "Arlindo Rodrigues Galvão Filho"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.529": {
    "title": "AraSafe: Benchmarking Safety in Arabic LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hamdy Mubarak",
      "Abubakr Mohamed",
      "Majd Hawasly"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.530": {
    "title": "Nested Named Entity Recognition as Single-Pass Sequence Labeling",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alberto Muñoz-Ortiz",
      "David Vilares",
      "Caio Corro",
      "Carlos Gómez-Rodríguez"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.531": {
    "title": "DeCoRe: Decoding by Contrasting Retrieval Heads to Mitigate Hallucinations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aryo Pradipta Gema",
      "Chen Jin",
      "Ahmed Abdulaal",
      "Tom Diethe",
      "Philip Alexander Teare",
      "Beatrice Alex",
      "Pasquale Minervini",
      "Amrutha Saseendran"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.532": {
    "title": "Catch Me If You Can? Not Yet: LLMs Still Struggle to Imitate the Implicit Writing Styles of Everyday Authors",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengxiang Wang",
      "Nafis Irtiza Tripto",
      "Solha Park",
      "Zhenzhen Li",
      "Jiawei Zhou"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.533": {
    "title": "Fine-Tuning Encoder-Decoder Models with Contrastive Learning for In-Context Distractor Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elaf Alhazmi",
      "Quan Z. Sheng",
      "Wei Emma Zhang",
      "Mohammed I. Thanoon",
      "Haojie Zhuang",
      "Behnaz Soltani",
      "Munazza Zaib"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.534": {
    "title": "Conflicts in Texts: Data, Implications and Challenges",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyi Liu",
      "Dan Roth"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.535": {
    "title": "Recognizing Limits: Investigating Infeasibility in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenbo Zhang",
      "Zihang Xu",
      "Hengrui Cai"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.536": {
    "title": "VQA-Augmented Machine Translation with Cross-Modal Contrastive Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihui Zhang",
      "Shiliang Sun",
      "Jing Zhao",
      "Tengfei Song",
      "Hao Yang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.537": {
    "title": "Learning to Describe Implicit Changes: Noise-robust Pre-training for Image Difference Captioning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixin Guo",
      "Jiayang Sun",
      "Tzu-Jui Julius Wang",
      "Abduljalil Radman",
      "Selen Pehlivan",
      "Min Cao",
      "Jorma Laaksonen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.538": {
    "title": "SOLAR: Serendipity Optimized Language Model Aligned for Recommendation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zichen Yuan",
      "Lifan Sun",
      "Yucen Zhuang",
      "Yue Wang",
      "Xinyuan Song",
      "Tianqi Xu",
      "Siyuan Li",
      "Junchen Fu",
      "Youhua Li",
      "Sirui Hong",
      "Jiaqi Chen",
      "Joemon M. Jose",
      "Yongxin Ni"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.539": {
    "title": "AIRepr: An Analyst-Inspector Framework for Evaluating Reproducibility of LLMs in Data Science",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiuhai Zeng",
      "Claire Jin",
      "Xinyue Wang",
      "Yuhan Zheng",
      "Qunhua Li"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.540": {
    "title": "MisinfoBench: A Multi-Dimensional Benchmark for Evaluating LLMs' Resilience to Misinformation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ye Yang",
      "Donghe Li",
      "Zuchen Li",
      "Fengyuan Li",
      "Jingyi Liu",
      "Li Sun",
      "Qingyu Yang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.541": {
    "title": "Fuzzy Reasoning Chain (FRC): An Innovative Reasoning Framework from Fuzziness to Clarity",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ping Chen",
      "Xiang Liu",
      "Zhaoxiang Liu",
      "Zezhou Chen",
      "Xingpeng Zhang",
      "Huan Hu",
      "Zipeng Wang",
      "Kai Wang",
      "Shuming Shi",
      "Shiguo Lian"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.542": {
    "title": "HighMATH: Evaluating Math Reasoning of Large Language Models in Breadth and Depth",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Liu",
      "Minghui Zhang",
      "Bojian Xiong",
      "Yifan Xiao",
      "Yinong Sun",
      "Yating Mei",
      "Longyu Zeng",
      "Jingchao Yang",
      "Yang Wang",
      "Deyi Xiong"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.543": {
    "title": "CATCH: A Novel Data Synthesis Framework for High Therapy Fidelity and Memory-Driven Planning Chain of Thought in AI Counseling",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingyu Chen",
      "Jingkai Lin",
      "Zhaojie Chu",
      "Xiaofen Xing",
      "Yirong Chen",
      "Xiangmin Xu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.544": {
    "title": "MediVLM: A Vision Language Model for Radiology Report Generation from Medical Images",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Debanjan Goswami",
      "Ronast Subedi",
      "Shayok Chakraborty"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.545": {
    "title": "AdDriftBench: A Benchmark for Detecting Data Drift and Label Drift in Short Video Advertising",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinghao Song",
      "Xiangji Zeng",
      "Shuai Cui",
      "Lu Sun",
      "Zhaowei Liu",
      "Yuan Yuan",
      "Yulu Wang",
      "Hai Zhou",
      "Zhaohan Gong"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.546": {
    "title": "NIM: Neuro-symbolic Ideographic Metalanguage for Inclusive Communication",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prawaal Sharma",
      "Poonam Goyal",
      "Navneet Goyal",
      "Vidisha Sharma"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.547": {
    "title": "ViFT: Towards Visual Instruction-Free Fine-tuning for Large Vision-Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zikang Liu",
      "Kun Zhou",
      "Xin Zhao",
      "Dawei Gao",
      "Yaliang Li",
      "Ji-Rong Wen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.548": {
    "title": "Do Code Semantics Help? A Comprehensive Study on Execution Trace-Based Information for Code Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Jornbowrl Wang",
      "Xiaofei Xie",
      "Qiang Hu",
      "Shangqing Liu",
      "Yi Li"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.549": {
    "title": "LongWeave: A Long-Form Generation Benchmark Bridging Real-World Relevance and Verifiability",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zikai Xiao",
      "Fei Huang",
      "Jianhong Tu",
      "Jianhui Wei",
      "Wen Ma",
      "Yuxuan Zhou",
      "Jian Wu",
      "Bowen Yu",
      "Zuozhu Liu",
      "Junyang Lin"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.550": {
    "title": "XL-Suite: Cross-Lingual Synthetic Training and Evaluation Data for Open-Ended Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vivek Iyer",
      "Pinzhen Chen",
      "Ricardo Rei",
      "Alexandra Birch"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.551": {
    "title": "Accelerating LLM Reasoning via Early Rejection with Partial Reward Modeling",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seyyed Saeid Cheshmi",
      "Azal Ahmad Khan",
      "Xinran Wang",
      "Zirui Liu",
      "Ali Anwar"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.552": {
    "title": "CultureSynth: A Hierarchical Taxonomy-Guided and Retrieval-Augmented Framework for Cultural Question-Answer Synthesis",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Zhang",
      "Pei Zhang",
      "Shuang Luo",
      "Jialong Tang",
      "Yu Wan",
      "Baosong Yang",
      "Fei Huang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.553": {
    "title": "DesignCLIP: Multimodal Learning with CLIP for Design Patent Understanding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhu Wang",
      "Homaira Huda Shomee",
      "Sathya N. Ravi",
      "Sourav Medya"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.554": {
    "title": "R3-RAG: Learning Step-by-Step Reasoning and Retrieval for LLMs via Reinforcement Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Li",
      "Qi Luo",
      "Xiaonan Li",
      "Bufan Li",
      "Qinyuan Cheng",
      "Bo Wang",
      "Yining Zheng",
      "Yuxin Wang",
      "Zhangyue Yin",
      "Xipeng Qiu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.555": {
    "title": "‘Hello, World!': Making GNNs Talk with LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sunwoo Kim",
      "Soo Yong Lee",
      "Jaemin Yoo",
      "Kijung Shin"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.556": {
    "title": "Both Text and Images Leaked! A Systematic Analysis of Data Contamination in Multimodal LLM",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dingjie Song",
      "Sicheng Lai",
      "Mingxuan Wang",
      "Shunian Chen",
      "Lichao Sun",
      "Benyou Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.557": {
    "title": "NLKI: A Lightweight Natural Language Knowledge Integration Framework for Improving Small VLMs in Commonsense VQA Tasks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aritra Dutta",
      "Swapnanil Mukherjee",
      "Deepanway Ghosal",
      "Somak Aditya"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.558": {
    "title": "Text or Pixels? Evaluating Efficiency and Understanding of LLMs with Visual Text Inputs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanhong Li",
      "Zixuan Lan",
      "Jiawei Zhou"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.559": {
    "title": "Assessing Socio-Cultural Alignment and Technical Safety of Sovereign LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyubyung Chae",
      "Gihoon Kim",
      "Gyuseong Lee",
      "Taesup Kim",
      "Jaejin Lee",
      "Heejin Kim"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.560": {
    "title": "Sample Efficient Alignment Learning With Episodic Control",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Van Dai Do",
      "Quan Hung Tran",
      "Ahmed Kirmani",
      "Lu Zhang",
      "Hung Le"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.561": {
    "title": "Evaluating Automatic Speech Recognition Systems for Korean Meteorological Experts",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "ChaeHun Park",
      "Hojun Cho",
      "Jaegul Choo"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.562": {
    "title": "3D-Aware Vision-Language Models Fine-Tuning with Geometric Distillation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seonho Lee",
      "Jiho Choi",
      "Inha Kang",
      "Jiwook Kim",
      "Junsung Park",
      "Hyunjung Shim"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.563": {
    "title": "CAPE: Context-Aware Personality Evaluation Framework for Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jivnesh Sandhan",
      "Fei Cheng",
      "Tushar Sandhan",
      "Yugo Murawaki"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.564": {
    "title": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kangan Qian",
      "Sicong Jiang",
      "Yang Zhong",
      "Ziang Luo",
      "Zilin Huang",
      "Tianze Zhu",
      "Kun Jiang",
      "Mengmeng Yang",
      "Zheng Fu",
      "Jinyu Miao",
      "Yining Shi",
      "He Zhe Lim",
      "Li Liu",
      "Tianbao Zhou",
      "Hongyi Wang",
      "Huang Yu",
      "Yifei Hu",
      "Guang Li",
      "Guang Chen",
      "Hao Ye",
      "Lijun Sun",
      "Diange Yang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.565": {
    "title": "Select to Know: An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bolei He",
      "Xinran He",
      "Run Shao",
      "Shanfu Shu",
      "Xianwei Xue",
      "MingQuan Cheng",
      "Haifeng Li",
      "Zhen-Hua Ling"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.566": {
    "title": "GenPTQ: Green Post-Training Quantization for Large-Scale ASR Models with Mixed-Precision Bit Allocation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Beom Jin Kang",
      "Hyun Kim"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.567": {
    "title": "Where Does This Strange Smell Come from?\": Enabling Conversational Interfaces for Artificial Olfaction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xueyi Zhou",
      "Qi Lu",
      "Dong-Kyu Chae"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.568": {
    "title": "LightRAG: Simple and Fast Retrieval-Augmented Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zirui Guo",
      "Lianghao Xia",
      "Yanhua Yu",
      "Tu Ao",
      "Chao Huang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.569": {
    "title": "Beyond Distribution: Investigating Language Models' Understanding of Sino-Korean Morphemes",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taehee Jeon"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.570": {
    "title": "Sarcasm-R1: Enhancing Sarcasm Detection through Focused Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Yang",
      "Jingjie Zeng",
      "Liang Yang",
      "Kai Ma",
      "Hongfei Lin"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.571": {
    "title": "ISACL: Internal State Analyzer for Copyrighted Training Data Leakage",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangwei Zhang",
      "Qisheng Su",
      "Jiateng Liu",
      "Cheng Qian",
      "Yanzhou Pan",
      "Yanjie Fu",
      "Denghui Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.572": {
    "title": "Steering LVLMs via Sparse Autoencoder for Hallucination Mitigation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenglin Hua",
      "Jinghan He",
      "Zijun Yao",
      "Tianxu Han",
      "Haiyun Guo",
      "Yuheng Jia",
      "Junfeng Fang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.573": {
    "title": "On the Perception Bottleneck of VLMs for Chart Understanding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junteng Liu",
      "Weihao Zeng",
      "Xiwen Zhang",
      "Yijun Wang",
      "Zifei Shan",
      "Junxian He"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.574": {
    "title": "Self-Guided Function Calling in Large Language Models via Stepwise Experience Recall",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sijia Cui",
      "Aiyao He",
      "Shuai Xu",
      "Hongming Zhang",
      "Yanna Wang",
      "Qingyang Zhang",
      "Yajing Wang",
      "Bo Xu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.575": {
    "title": "Multilingual Generative Retrieval via Cross-lingual Semantic Compression",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxin Huang",
      "Simeng Wu",
      "Ran Song",
      "Yan Xiang",
      "Yantuan Xian",
      "Shengxiang Gao",
      "Zhengtao Yu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.576": {
    "title": "Towards Multi-Document Question Answering in Scientific Literature: Pipeline, Dataset, and Evaluation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hui Huang",
      "Julien Velcin",
      "Yacine Kessaci"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.577": {
    "title": "Multilingual Knowledge Graph Completion via Efficient Multilingual Knowledge Sharing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cunli Mao",
      "Xiaofei Gao",
      "Ran Song",
      "Shizhu He",
      "Shengxiang Gao",
      "Kang Liu",
      "Zhengtao Yu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.578": {
    "title": "Mitigating Attention Localization in Small Scale: Self-Attention Refinement via One-step Belief Propagation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nakyung Lee",
      "Yeongoon Kim",
      "Minhae Oh",
      "Suhwan Kim",
      "Jin Woo Koo",
      "Hyewon Jo",
      "Jungwoo Lee"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.579": {
    "title": "Imagination and Contemplation: A Balanced Framework for Semantic-Augmented Multimodal Machine Translation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuang Yu",
      "Shiliang Sun",
      "Jing Zhao",
      "Tengfei Song",
      "Hao Yang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.580": {
    "title": "NeLLCom-Lex: A Neural-agent Framework to Study the Interplay between Lexical Systems and Language Use",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuqing Zhang",
      "Ecesu Ürker",
      "Tessa Verhoef",
      "Gemma Boleda",
      "Arianna Bisazza"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.581": {
    "title": "RLMEval: Evaluating Research-Level Neural Theorem Proving",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Auguste Poiroux",
      "Antoine Bosselut",
      "Viktor Kunčak"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.582": {
    "title": "KaeDe: Progressive Generation of Logical Forms via Knowledge-Aware Question Decomposition for Improved KBQA",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ranran Bu",
      "Jian Cao",
      "Jianqi Gao",
      "Shiyou Qian",
      "Hongming Cai"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.583": {
    "title": "Where Fact Ends and Fairness Begins: Redefining AI Bias Evaluation through Cognitive Biases",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jen-tse Huang",
      "Yuhang Yan",
      "Linqi Liu",
      "Yixin Wan",
      "Wenxuan Wang",
      "Kai-Wei Chang",
      "Michael R. Lyu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.584": {
    "title": "Equal Truth: Rumor Detection with Invariant Group Fairness",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyi Chen",
      "Mengjia Wu",
      "Qian Liu",
      "Jing Sun",
      "Ying Ding",
      "Yi Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.585": {
    "title": "STEAM: A Semantic-Level Knowledge Editing Framework for Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Geunyeong Jeong",
      "Juoh Sun",
      "Seonghee Lee",
      "Harksoo Kim"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.586": {
    "title": "SoT: Structured-of-Thought Prompting Guides Multilingual Reasoning in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Qi",
      "Zhibo Man",
      "Yufeng Chen",
      "Fengran Mo",
      "Jinan Xu",
      "Kaiyu Huang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.587": {
    "title": "How Reliable is Multilingual LLM-as-a-Judge?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiyan Fu",
      "Wei Liu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.588": {
    "title": "Cognitive-Level Adaptive Generation via Capability-Aware Retrieval and Style Adaptation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingsong Wang",
      "Tao Wu",
      "Wang Lin",
      "Yueying Feng",
      "Gongsheng Yuan",
      "Chang Yao",
      "Jingyuan Chen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.589": {
    "title": "Data Doping or True Intelligence? Evaluating the Transferability of Injected Knowledge in LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Essa Jan",
      "Moiz Ali",
      "Muhammad Saram Hassan",
      "Muhammad Fareed Zaffar",
      "Yasir Zaki"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.590": {
    "title": "INDOORWORLD : Integrating Physical Task Solving and Social Simulation in A Heterogeneous Multi-Agent Environment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dekun Wu",
      "Frederik Brudy",
      "Bang Liu",
      "Yi Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.591": {
    "title": "ARXSA: A General Negative Feedback Control Theory in Vision-Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyu Zhang",
      "Tianqi Chen",
      "Yuki Todo"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.592": {
    "title": "Breaking the Attention Trap in Code LLMs: A Rejection Sampling Approach to Enhance Code Execution Prediction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingcheng Ruan",
      "Haoxiang Geng",
      "Yunhui Xia",
      "Bingran Zhao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.593": {
    "title": "HiMATE: A Hierarchical Multi-Agent Framework for Machine Translation Evaluation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shijie Zhang",
      "Renhao Li",
      "Songsheng Wang",
      "Philipp Koehn",
      "Min Yang",
      "Derek F. Wong"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.594": {
    "title": "ReliableEval: A Recipe for Stochastic LLM Evaluation via Method of Moments",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gili Lior",
      "Eliya Habba",
      "Shahar Levy",
      "Avi Caciularu",
      "Gabriel Stanovsky"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.595": {
    "title": "From Characters to Tokens: Dynamic Grouping with Hierarchical BPE",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rares Dolga",
      "Lucas Maystre",
      "Tudor Berariu",
      "David Barber"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.596": {
    "title": "Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in Smart Personal Assistant",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Shen",
      "Xiaoyu Shen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.597": {
    "title": "NER Retriever: Zero-Shot Named Entity Retrieval with Type-Aware Embeddings",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Or Shachar",
      "Uri Katz",
      "Yoav Goldberg",
      "Oren Glickman"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.598": {
    "title": "MMATH: A Multilingual Benchmark for Mathematical Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenyang Luo",
      "Xin Zhao",
      "Jing Sha",
      "Shijin Wang",
      "Ji-Rong Wen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.599": {
    "title": "MultiClaimNet: A Massively Multilingual Dataset of Fact-Checked Claim Clusters",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rrubaa Panchendrarajan",
      "Rubén Míguez Pérez",
      "Arkaitz Zubiaga"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.600": {
    "title": "DS-MHP: Improving Chain-of-Thought through Dynamic Subgraph-Guided Multi-Hop Path",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongqiang Liu",
      "Qiyao Peng",
      "Binrong Liu",
      "Hongtao Liu",
      "XueWei Li",
      "Wenjun Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.601": {
    "title": "LongTail-Swap: benchmarking language models' abilities on rare words",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Robin Algayres",
      "Charles-Éric Saint-James",
      "Mahi Luthra",
      "Jiayi Shen",
      "Youssef Benchekroun",
      "Dongyan Lin",
      "Rashel Moritz",
      "Juan Pino",
      "Emmanuel Dupoux"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.602": {
    "title": "TF-Mamba: Text-enhanced Fusion Mamba with Missing Modalities for Robust Multimodal Sentiment Analysis",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Li",
      "Xianfu Cheng",
      "Dezhuang Miao",
      "Xiaoming Zhang",
      "Zhoujun Li"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.603": {
    "title": "Are Economists Always More Introverted? Analyzing Consistency in Persona-Assigned LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manon Reusens",
      "Bart Baesens",
      "David Jurgens"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.604": {
    "title": "Can you SPLICE it together? A Human Curated Benchmark for Probing Visual Reasoning in VLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohamad Ballout",
      "Okajevo Wilfred",
      "Seyedalireza Yaghoubi",
      "Nohayr Muhammad Abdelmoneim",
      "Julius Mayer",
      "Elia Bruni"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.605": {
    "title": "On the Effectiveness of Prompt-Moderated LLMs for Math Tutoring at the Tertiary Level",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Steindl",
      "Fabian Brunner",
      "Nada Sissouno",
      "Dominik Schwagerl",
      "Florian Schöler-Niewiera",
      "Ulrich Schäfer"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.606": {
    "title": "SkewRoute: Training-Free LLM Routing for Knowledge Graph Retrieval-Augmented Generation via Score Skewness of Retrieved Context",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hairu Wang",
      "Yuan Feng",
      "Yukun Cao",
      "Xike Xie",
      "S Kevin Zhou"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.607": {
    "title": "Acquiescence Bias in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Braun"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.608": {
    "title": "Time to Talk: LLM Agents for Asynchronous Group Communication in Mafia Games",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Niv Eckhaus",
      "Uri Berger",
      "Gabriel Stanovsky"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.609": {
    "title": "How Sampling Affects the Detectability of Machine-written texts: A Comprehensive Study",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthieu Dubois",
      "François Yvon",
      "Pablo Piantanida"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.610": {
    "title": "An Improved, Strong Baseline for Pre-Trained Large Language Models as Task-Oriented Dialogue Systems",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Steindl",
      "André Kestler",
      "Ulrich Schäfer",
      "Bernd Ludwig"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.611": {
    "title": "MATCH: Task-Driven Code Evaluation through Contrastive Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marah Ghoummaid",
      "Vladimir Tchuiev",
      "Ofek Glick",
      "Michal Moshkovitz",
      "Dotan Di Castro"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.612": {
    "title": "Evaluating Large Language Models for Cross-Lingual Retrieval",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Longfei Zuo",
      "Pingjun Hong",
      "Oliver Kraus",
      "Barbara Plank",
      "Robert Litschko"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.613": {
    "title": "SGCD: Subtask-Guided Causal-Debiasing Framework for Robust Cross-Utterance Sentiment Quadruple Extraction in Dialogues",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Li",
      "Keyu Yao",
      "Gang Shen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.614": {
    "title": "FaMTEB: Massive Text Embedding Benchmark in Persian Language",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Erfan Zinvandi",
      "Morteza Alikhani",
      "Mehran Sarmadi",
      "Zahra Pourbahman",
      "Sepehr Arvin",
      "Reza Kazemi",
      "Arash Amini"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.615": {
    "title": "Leveraging High-Resource English Corpora for Cross-lingual Domain Adaptation in Low-Resource Japanese Medicine via Continued Pre-training",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kazuma Kobayashi",
      "Zhen Wan",
      "Fei Cheng",
      "Yuma Tsuta",
      "Xin Zhao",
      "Junfeng Jiang",
      "Jiahao Huang",
      "Zhiyi Huang",
      "Yusuke Oda",
      "Rio Yokota",
      "Yuki Arase",
      "Daisuke Kawahara",
      "Akiko Aizawa",
      "Sadao Kurohashi"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.616": {
    "title": "Structure Trumps Size: Rethinking Data Quality for LLM Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hu Xu",
      "Zeyan Li",
      "Rui Wang",
      "Jianfeng Xu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.617": {
    "title": "A Zero-Shot Neuro-Symbolic Approach for Complex Knowledge Graph Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prerna Agarwal",
      "Srikanta Bedathur"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.618": {
    "title": "Making Every Step Effective: Jailbreaking Large Vision-Language Models Through Hierarchical KV Equalization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuyang Hao",
      "Yiwei Wang",
      "Bryan Hooi",
      "Jun Liu",
      "Muhao Chen",
      "Zi Huang",
      "Yujun Cai"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.619": {
    "title": "MT-Mol: Multi Agent System with Tool-based Reasoning for Molecular Optimization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyomin Kim",
      "Yunhui Jang",
      "Sungsoo Ahn"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.620": {
    "title": "A Survey on LLM-powered Agents for Recommender Systems",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiyao Peng",
      "Hongtao Liu",
      "Hua Huang",
      "Jian Yang",
      "Qing Yang",
      "Minglai Shao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.621": {
    "title": "Efficiently Selecting Response Generation Strategies for Synthetic Data Construction by Self-Aligned Perplexity",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Ren",
      "Qi Chen",
      "Lingqiao Liu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.622": {
    "title": "Benchmarking for Domain-Specific LLMs: A Case Study on Academia and Beyond",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rubing Chen",
      "Jiaxin Wu",
      "Jian Wang",
      "Xulu Zhang",
      "Wenqi Fan",
      "Chenghua Lin",
      "Xiaoyong Wei",
      "Li Qing"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.623": {
    "title": "FrameEOL: Semantic Frame Induction using Causal Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chihiro Yano",
      "Kosuke Yamada",
      "Hayato Tsukagoshi",
      "Ryohei Sasano",
      "Koichi Takeda"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.624": {
    "title": "CaTER: A Framework for Context-aware Topology Entity Retrieval Contrastive Learning in End-to-End Task-Oriented Dialogue Systems",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Di Wu Hebeu",
      "Zhizhi Yu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.625": {
    "title": "Attribution and Application of Multiple Neurons in Multimodal Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feiyu Wang",
      "Ziran Zhao",
      "Dong Yu",
      "Pengyuan Liu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.626": {
    "title": "When Models Lie, We Learn: Multilingual Span-Level Hallucination Detection with PsiloQA",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elisei Rykov",
      "Kseniia Petrushina",
      "Maksim Savkin",
      "Valerii Olisov",
      "Artem Vazhentsev",
      "Kseniia Titova",
      "Alexander Panchenko",
      "Vasily Konovalov",
      "Julia Belikova"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.627": {
    "title": "Unraveling Misinformation Propagation in LLM Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiyang Feng",
      "Yichen Wang",
      "Shaobo Cui",
      "Boi Faltings",
      "Mina Lee",
      "Jiawei Zhou"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.628": {
    "title": "RAISE: Reinforced Adaptive Instruction Selection For Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingsong Lv",
      "Yangning Li",
      "Zihua Lan",
      "Zishan Xu",
      "Jiwei Tang",
      "Tingwei Lu",
      "Yinghui Li",
      "Wenhao Jiang",
      "Hong-Gee Kim",
      "Hai-Tao Zheng",
      "Philip S. Yu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.629": {
    "title": "Teaching According to Talents! Instruction Tuning LLMs with Competence-Aware Curriculum Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangning Li",
      "Tingwei Lu",
      "Yinghui Li",
      "Yankai Chen",
      "Wei-Chieh Huang",
      "Wenhao Jiang",
      "Hui Wang",
      "Hai-Tao Zheng",
      "Philip S. Yu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.630": {
    "title": "Let Them Down Easy! Contextual Effects of LLM Guardrails on User Perceptions and Preferences",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingqian Zheng",
      "Wenjia Hu",
      "Patrick Zhao",
      "Motahhare Eslami",
      "Jena D. Hwang",
      "Faeze Brahman",
      "Carolyn Rose",
      "Maarten Sap"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.631": {
    "title": "From Hypothesis to Publication: A Comprehensive Survey of AI-Driven Research Support Systems",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zekun Zhou",
      "Xiaocheng Feng",
      "Lei Huang",
      "Xiachong Feng",
      "Ziyun Song",
      "Ruihan Chen",
      "Liang Zhao",
      "Weitao Ma",
      "Yuxuan Gu",
      "Baoxin Wang",
      "Dayong Wu",
      "Guoping Hu",
      "Ting Liu",
      "Bing Qin"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.632": {
    "title": "Enhancing Model Privacy in Federated Learning with Random Masking and Quantization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhibo Xu",
      "Zhu JianHao",
      "Jingwen Xu",
      "Changze Lv",
      "Zhenghua Wang",
      "Zisu Huang",
      "Xiaohua Wang",
      "Muling Wu",
      "Qi Qian",
      "Xiaoqing Zheng",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.633": {
    "title": "SuPreME: A Supervised Pre-training Framework for Multimodal ECG Representation Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingsheng Cai",
      "Jiuming Jiang",
      "Wenhao Huang",
      "Che Liu",
      "Rossella Arcucci"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.634": {
    "title": "Ferret: Faster and Effective Automated Red Teaming with Reward-Based Scoring Technique",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tej Deep Pala",
      "Vernon Toh",
      "Rishabh Bhardwaj",
      "Soujanya Poria"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.635": {
    "title": "Do What? Teaching Vision-Language-Action Models to Reject the Impossible",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wen-Han Hsieh",
      "Elvis Hsieh",
      "Dantong Niu",
      "Trevor Darrell",
      "Roei Herzig",
      "David M. Chan"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.636": {
    "title": "AgentInit: Initializing LLM-based Multi-Agent Systems via Diversity and Expertise Orchestration for Effective and Efficient Collaboration",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chunhao Tian",
      "Yutong Wang",
      "Xuebo Liu",
      "Zhexuan Wang",
      "Liang Ding",
      "Miao Zhang",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.637": {
    "title": "Time to Revisit Exact Match",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Auss Abbood",
      "Zaiqiao Meng",
      "Nigel Collier"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.638": {
    "title": "LongTableBench: Benchmarking Long-Context Table Reasoning across Real-World Formats and Domains",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liyao Li",
      "Jiaming Tian",
      "Hao Chen",
      "Wentao Ye",
      "Chao Ye",
      "Haobo Wang",
      "Ningtao Wang",
      "Xing Fu",
      "Gang Chen",
      "Junbo Zhao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.639": {
    "title": "Exploring and Evaluating Multimodal Knowledge Reasoning Consistency of Multimodal Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boyu Jia",
      "Junzhe Zhang",
      "Huixuan Zhang",
      "Xiaojun Wan"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.640": {
    "title": "MPTA: MultiTask Personalization Assessment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthieu Tehenan",
      "Eric Chamoun",
      "Andreas Vlachos"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.641": {
    "title": "Semantic Geometry of Sentence Embeddings",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthieu Tehenan"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.642": {
    "title": "ReAlign: Structured Revision for Small Language Model Alignment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruijun Chen",
      "Jiajian Guo",
      "Hongzhan Chen",
      "Fanqi Wan",
      "Qifan Wang",
      "Xiaojun Quan"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.643": {
    "title": "Curr-ReFT: Overcoming Training Bottlenecks in Small-scale Vision-Language Models via Curriculum Reinforcement Finetuning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huilin Deng",
      "Ding Zou",
      "Xinghao Zhao",
      "Rui Ma",
      "Yanming Guo",
      "Yang Cao",
      "Yu Kang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.644": {
    "title": "Layer-Aware Task Arithmetic: Disentangling Task-Specific and Instruction-Following Knowledge",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan-Lun Chen",
      "Yi-Ru Wei",
      "Chia-Yi Hsu",
      "Chia-Mu Yu",
      "Chun-Ying Huang",
      "Ying-Dar Lin",
      "Yu-Sung Wu",
      "Wei-Bin Lee"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.645": {
    "title": "Revisiting Pruning vs Quantization for Small Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihan Zhou",
      "Simon Kurz",
      "Zhixue Zhao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.646": {
    "title": "CLaw: Benchmarking Chinese Legal Knowledge in Large Language Models - A Fine-grained Corpus and Reasoning Analysis",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinzhe Xu",
      "Liang Zhao",
      "Hongshen Xu",
      "Chenchenc"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.647": {
    "title": "polyBART: A Chemical Linguist for Polymer Property Prediction and Generative Design",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anagha Savit",
      "Harikrishna Sahu",
      "Shivank S. Shukla",
      "Wei Xiong",
      "Rampi Ramprasad"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.648": {
    "title": "A Survey of RAG-Reasoning Systems in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangning Li",
      "Weizhi Zhang",
      "Yuyao Yang",
      "Wei-Chieh Huang",
      "Yaozu Wu",
      "Junyu Luo",
      "Yuanchen Bei",
      "Henry Peng Zou",
      "Xiao Luo",
      "Yusheng Zhao",
      "Chunkit Chan",
      "Yankai Chen",
      "Zhongfen Deng",
      "Yinghui Li",
      "Hai-Tao Zheng",
      "Dongyuan Li",
      "Renhe Jiang",
      "Ming Zhang",
      "Yangqiu Song",
      "Philip S. Yu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.649": {
    "title": "REGen: A Reliable Evaluation Framework for Generative Event Argument Extraction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Omar Sharif",
      "Joseph Gatto",
      "Madhusudan Basak",
      "Sarah Masud Preum"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.650": {
    "title": "Mitigating Interviewer Bias in Multimodal Depression Detection: An Approach with Adversarial Learning and Contextual Positional Encoding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Enshi Zhang",
      "Christian Poellabauer"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.651": {
    "title": "AMIA: Automatic Masking and Joint Intention Analysis Makes LVLMs Robust Jailbreak Defenders",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuqi Zhang",
      "Yuchun Miao",
      "Zuchao Li",
      "Liang Ding"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.652": {
    "title": "Disentangling Language Understanding and Reasoning Structures in Cross-lingual Chain-of-Thought Prompting",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Khanh-Tung Tran",
      "Nguyet-Hang Vu",
      "Barry O’Sullivan",
      "Hoang D. Nguyen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.653": {
    "title": "MoRoVoc: A Large Dataset for Geographical Variation Identification of the Spoken Romanian Language",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrei-Marius Avram",
      "Bănescu Ema-Ioana",
      "Anda-Teodora Robea",
      "Dumitru-Clementin Cercel",
      "Mihaela-Claudia Cercel"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.654": {
    "title": "Language-Informed Synthesis of Rational Agent Models for Grounded Theory-of-Mind Reasoning On-the-fly",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lance Ying",
      "Ryan Truong",
      "Katherine M. Collins",
      "Cedegao E. Zhang",
      "Megan Wei",
      "Tyler BrookeWilson",
      "Tan Zhi-Xuan",
      "Lionel Wong",
      "Joshua B. Tenenbaum"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.655": {
    "title": "MOLE: Metadata Extraction and Validation in Scientific Papers Using LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zaid Alyafeai",
      "Maged S. Al-shaibani",
      "Bernard Ghanem"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.656": {
    "title": "MASSV: Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of Vision-Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mugilan Ganesan",
      "Shane Segal",
      "Ankur Aggarwal",
      "Nish Sinnadurai",
      "Sean Lie",
      "Vithursan Thangarasa"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.657": {
    "title": "FESTA: Functionally Equivalent Sampling for Trust Assessment of Multimodal LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Debarpan Bhattacharya",
      "Apoorva Kulkarni",
      "Sriram Ganapathy"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.658": {
    "title": "ClaimGen-CN: A Large-scale Chinese Dataset for Legal Claim Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siying Zhou",
      "Yiquan Wu",
      "Hui Chen",
      "Xueyu Hu",
      "Kun Kuang",
      "Adam Jatowt",
      "Chunyan Zheng",
      "Fei Wu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.659": {
    "title": "Summarize-Exemplify-Reflect: Data-driven Insight Distillation Empowers LLMs for Few-shot Tabular Classification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Yuan",
      "Jiatong Li",
      "Weijia Zhang",
      "Mohammad Aliannejadi",
      "Evangelos Kanoulas",
      "Renjun Hu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.660": {
    "title": "Rethinking LLM Uncertainty: A Multi-Agent Approach to Estimating Black-Box Model Uncertainty",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Feng",
      "Phu Mon Htut",
      "Zheng Qi",
      "Wei Xiao",
      "Manuel Mager",
      "Nikolaos Pappas",
      "Kishaloy Halder",
      "Yang Li",
      "Yassine Benajiba",
      "Dan Roth"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.661": {
    "title": "Stress-Testing the Reasoning Competence of Language Models With Formal Proofs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Konstantine Arkoudas",
      "Serafim Batzoglou"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.662": {
    "title": "Topic-Guided Reinforcement Learning with LLMs for Enhancing Multi-Document Summarization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuyuan Li",
      "Austin Xu",
      "Shafiq Joty",
      "Giuseppe Carenini"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.663": {
    "title": "FACTCHECKMATE: Preemptively Detecting and Mitigating Hallucinations in LMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Deema Alnuhait",
      "Neeraja Kirtane",
      "Muhammad Khalifa",
      "Hao Peng"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.664": {
    "title": "Dialectal Toxicity Detection: Evaluating LLM-as-a-Judge Consistency Across Language Varieties",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fahim Faisal",
      "Md Mushfiqur Rahman",
      "Antonios Anastasopoulos"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.665": {
    "title": "Mitigate One, Skew Another? Tackling Intersectional Biases in Text-to-Image Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pushkar Shukla",
      "Aditya Chinchure",
      "Emily Diana",
      "Alexander Tolbert",
      "Kartik Hosanagar",
      "Vineeth N. Balasubramanian",
      "Leonid Sigal",
      "Matthew A. Turk"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.666": {
    "title": "Language-Specific Layer Matters: Efficient Multilingual Enhancement for Large Vision-Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchun Fan",
      "Yilin Wang",
      "Yongyu Mu",
      "Lei Huang",
      "Bei Li",
      "Xiaocheng Feng",
      "Tong Xiao",
      "JingBo Zhu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.667": {
    "title": "InfAL: Inference Time Adversarial Learning for Improving Research Ideation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sikun Guo",
      "Amir Hassan Shariatmadari",
      "Peng Wang",
      "Albert Huang",
      "Aidong Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.668": {
    "title": "Speculative Decoding for Multi-Sample Inference",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiwei Li",
      "Jiayi Shi",
      "Shaoxiong Feng",
      "Peiwen Yuan",
      "Xinglin Wang",
      "Yueqi Zhang",
      "Ji Zhang",
      "Chuyi Tan",
      "Boyuan Pan",
      "Yao Hu",
      "Kan Li"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.669": {
    "title": "LSRL: Process-Supervised GRPO on Latent Recurrent States Improves Mathematical Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hangliang Ren"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.670": {
    "title": "Multi-token Mask-filling and Implicit Discourse Relations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meinan Liu",
      "Yunfang Dong",
      "Xixian Liao",
      "Bonnie Webber"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.671": {
    "title": "Schema Generation for Large Knowledge Graphs Using Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bohui Zhang",
      "Yuan He",
      "Lydia Pintscher",
      "Albert Meroño-Peñuela",
      "Elena Simperl"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.672": {
    "title": "MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree Search",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunhai Hu",
      "Yilun Zhao",
      "Chen Zhao",
      "Arman Cohan"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.673": {
    "title": "What if Othello-Playing Language Models Could See?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyi Chen",
      "Yifei Yuan",
      "Jiaang Li",
      "Serge Belongie",
      "Maarten de Rijke",
      "Anders Søgaard"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.674": {
    "title": "LLM-Based Web Data Collection for Research Dataset Creation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Berkane",
      "Marie-Laure Charpignon",
      "Maimuna S. Majumder"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.675": {
    "title": "PsyScam: A Benchmark for Psychological Techniques in Real-World Scams",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shang Ma",
      "Tianyi Ma",
      "Jiahao Liu",
      "Wei Song",
      "Zhenkai Liang",
      "Xusheng Xiao",
      "Yanfang Ye"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.676": {
    "title": "LoRaDA: Low-Rank Direct Attention Adaptation for Efficient LLM Fine-tuning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhangming Li",
      "Qinghao Hu",
      "Yiqun Chen",
      "Peisong Wang",
      "Yifan Zhang",
      "Jian Cheng"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.677": {
    "title": "Inductive Reasoning on Few-Shot Knowledge Graphs with Task-Aware Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Yan",
      "Feng Zhao",
      "Ruilin Zhao",
      "Hong Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.678": {
    "title": "ForestCast: Open-Ended Event Forecasting with Semantic News Forest",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zi Yu",
      "Shaoxiang Wang",
      "Guozheng Li",
      "Yu Zhang",
      "Chi Harold Liu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.679": {
    "title": "Agentic Medical Knowledge Graphs Enhance Medical Question Answering: Bridging the Gap Between LLMs and Evolving Medical Knowledge",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Reza Rezaei",
      "Reza Saadati Fard",
      "Jayson Lee Parker",
      "Rahul G Krishnan",
      "Milad Lankarany"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.680": {
    "title": "Text Anomaly Detection with Simplified Isolation Kernel",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Cao",
      "Sikun Yang",
      "Yujiu Yang",
      "Lianyong Qi",
      "Ming Liu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.681": {
    "title": "Idola Tribus of AI: Large Language Models tend to perceive order where none exists",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shin-nosuke Ishikawa",
      "Masato Todo",
      "Taiki Ogihara",
      "Hirotsugu Ohba"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.682": {
    "title": "Thunder-DeID: Accurate and Efficient De-identification Framework for Korean Court Judgments",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sungeun Hahm",
      "Heejin Kim",
      "Gyuseong Lee",
      "Hyunji M. Park",
      "Jaejin Lee"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.683": {
    "title": "Multi-Agent Autonomous Driving Systems with Large Language Models: A Survey of Recent Advances, Resources, and Future Directions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaozu Wu",
      "Dongyuan Li",
      "Yankai Chen",
      "Renhe Jiang",
      "Henry Peng Zou",
      "Wei-Chieh Huang",
      "Yangning Li",
      "Liancheng Fang",
      "Zhen Wang",
      "Philip S. Yu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.684": {
    "title": "Comprehensive Evaluation on Lexical Normalization: Boundary-Aware Approaches for Unsegmented Languages",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shohei Higashiyama",
      "Masao Utiyama"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.685": {
    "title": "Explainable Text Classification with LLMs: Enhancing Performance through Dialectical Prompting and Explanation-Guided Training",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huaming Du",
      "Lei Yuan",
      "Cancan Feng",
      "Guisong Liu",
      "Gang Kou",
      "Carl Yang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.686": {
    "title": "MultiPL-MoE: Multi-Programming-Lingual Extension of Large Language Models through Hybrid Mixture-of-Experts",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qing Wang",
      "Xue Han",
      "Jiahui Wang",
      "Lehao Xing",
      "Qian Hu",
      "Lianlian Zhang",
      "Chao Deng",
      "Junlan Feng"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.687": {
    "title": "AutoSpec: An Agentic Framework for Automatically Drafting Patent Specification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryan Shea",
      "Zhou Yu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.688": {
    "title": "LimaCost: Data Valuation for Instruction Tuning of Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyeonseok Moon",
      "Jaehyung Seo",
      "Seonmin Koo",
      "Jinsung Kim",
      "Young-kyoung Ham",
      "Jiwon Moon",
      "Heuiseok Lim"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.689": {
    "title": "Two Challenges, One Solution: Robust Multimodal Learning through Dynamic Modality Recognition and Enhancement",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lanxin Bi",
      "Yunqi Zhang",
      "Luyi Wang",
      "Yake Niu",
      "Hui Zhao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.690": {
    "title": "SwiftPrune: Hessian-Free Weight Pruning for Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhan Kang",
      "Yang Shi",
      "Mei Wen",
      "Jun He",
      "Jianchao Yang",
      "Zeyu Xue",
      "Jing Feng",
      "Xinwang Liu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.691": {
    "title": "Training LLMs for Optimization Modeling via Iterative Data Synthesis and Structured Validation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Wu",
      "Yifan Zhang",
      "Yurong Wu",
      "Yuran Wang",
      "Junkai Zhang",
      "Jian Cheng"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.692": {
    "title": "Exploiting Prompt-induced Confidence for Black-Box Attacks on LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meina Chen",
      "Yihong Tang",
      "Kehai Chen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.693": {
    "title": "DPF-CM: A Data Processing Framework with Privacy-Preserving Vector Databases for Chinese Medical LLMs Training and Deployment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Huang",
      "Anda Cheng",
      "Zhao Zhang",
      "Yinggui Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.694": {
    "title": "Graph-Reward-SQL: Execution-Free Reinforcement Learning for Text-to-SQL via Graph Matching and Stepwise Reward",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Weng",
      "Puzhen Wu",
      "Cui Longjie",
      "Yi Zhan",
      "Boyi Liu",
      "Yuanfeng Song",
      "Dun Zeng",
      "Yingxiang Yang",
      "Qianru Zhang",
      "Dong Huang",
      "Xiaoming Yin",
      "Yang Sun",
      "Xing Chen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.695": {
    "title": "StatsChartMWP: A Dataset for Evaluating Multimodal Mathematical Reasoning Abilities on Math Word Problems with Statistical Charts",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dan Zhu",
      "Tianqiao Liu",
      "Zitao Liu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.696": {
    "title": "Logic-Thinker: Teaching Large Language Models to Think more Logically",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengyao Wen",
      "Qiang Cheng",
      "Shaofei Wang",
      "Zhizhen Liu",
      "Deng Zhao",
      "Lei Liang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.697": {
    "title": "ACEBench: A Comprehensive Evaluation of LLM Tool Usage",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Chen",
      "Xinlong Hao",
      "Weiwen Liu",
      "Xu Huang",
      "Xingshan Zeng",
      "Shuai Yu",
      "Dexun Li",
      "Yuefeng Huang",
      "Xiangcheng Liu",
      "Wang Xinzhi",
      "Wu Liu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.698": {
    "title": "RevPRAG: Revealing Poisoning Attacks in Retrieval-Augmented Generation through LLM Activation Analysis",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xue Tan",
      "Hao Luan",
      "Mingyu Luo",
      "Xiaoyan Sun",
      "Ping Chen",
      "Jun Dai"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.699": {
    "title": "DaMoC: Efficiently Selecting the Optimal Large Language Model for Fine-tuning Domain Tasks Based on Data and Model Compression",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Huang",
      "Huang Wei",
      "Yinggui Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.700": {
    "title": "CoAT: Chain-of-Associated-Thoughts Framework for Enhancing Large Language Models Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianfeng Pan",
      "Senyou Deng",
      "Shaomang Huang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.701": {
    "title": "ChartM3: A Multi-Stage Code-Driven Pipeline for Constructing Multi-Dimensional and Multi-Step Visual Reasoning Data in Chart Comprehension",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Duo Xu",
      "Hao Cheng",
      "Xin Lin",
      "Zhen Xie",
      "Hao Henry Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.702": {
    "title": "Can LLMs Truly Plan? A Comprehensive Evaluation of Planning Capabilities",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gayeon Jung",
      "HyeonSeok Lim",
      "Minjun Kim",
      "Joon-ho Lim",
      "KyungTae Lim",
      "Hansaem Kim"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.703": {
    "title": "MARIO-0.5B: A Multi-Agent Lightweight Model for Real-Time Open Information Extraction in Low-Resource Settings",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Donghai Zhang",
      "SHuangtao Yang",
      "Dong Xiaozheng",
      "Wei Song",
      "Bo Fu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.704": {
    "title": "BiMax: Bidirectional MaxSim Score for Document-Level Alignment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaotian Wang",
      "Takehito Utsuro",
      "Masaaki Nagata"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.705": {
    "title": "DocMMIR: A Framework for Document Multi-modal Information Retrieval",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zirui Li",
      "Siwei Wu",
      "Yizhi Li",
      "Xingyu Wang",
      "Yi Zhou",
      "Chenghua Lin"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.706": {
    "title": "MoVoC: Morphology-Aware Subword Construction for Ge'ez Script Languages",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hailay Kidu Teklehaymanot",
      "Dren Fazlija",
      "Wolfgang Nejdl"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.707": {
    "title": "MMA: Cross-Domain Knowledge Integration via Mixture of Multi-Domain Agents",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kehang Jia",
      "Juntao Li",
      "Xiaobo Liang",
      "Yisheng Xiao",
      "Yixuan Yang",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.708": {
    "title": "HAWK: Highlighting Entity-aware Knowledge for Alleviating Information Sparsity in Long Contexts",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seonmin Koo",
      "Jinsung Kim",
      "Chanjun Park",
      "Heuiseok Lim"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.709": {
    "title": "Sensitivity-LoRA : Low-Load Sensitivity-Based Fine-Tuning for Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Zhang",
      "Bo Huang",
      "Zhenjia Li",
      "Xi Xiao",
      "Hui Yi Leong",
      "Zumeng Zhang",
      "Xinwei Long",
      "Tianyang Wang",
      "Hao Xu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.710": {
    "title": "ROSE: A Reward-Oriented Data Selection Framework for LLM Task-Specific Instruction Tuning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Wu",
      "Huayi Zhang",
      "Yizheng Jiao",
      "Lin Ma",
      "Xiaozhong Liu",
      "Jinhong Yu",
      "Dongyu Zhang",
      "Dezhi Yu",
      "Wei Xu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.711": {
    "title": "SimBA: Simplifying Benchmark Analysis Using Performance Matrices Alone",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nishant Subramani",
      "Alfredo Gomez",
      "Mona T. Diab"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.712": {
    "title": "MarathiEmoExplain: A Dataset for Sentiment, Emotion, and Explanation in Low-Resource Marathi",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anuj Kumar",
      "Mohammed Faisal Sayed",
      "Satyadev Ahlawat",
      "Yamuna Prasad"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.713": {
    "title": "Active Domain Knowledge Acquisition with 100-Dollar Budget: Enhancing LLMs via Cost-Efficient, Expert-Involved Interaction in Sensitive Domains",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Wu",
      "Raha Moraffah",
      "Rujing Yao",
      "Jinhong Yu",
      "Zhimin Tao",
      "Xiaozhong Liu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.714": {
    "title": "Structure-aware Propagation Generation with Large Language Models for Fake News Detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengyang Chen",
      "Lingwei Wei",
      "Wei Zhou",
      "Songlin Hu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.715": {
    "title": "UniCoM: A Universal Code-Switching Speech Generator",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sangmin Lee",
      "Woojin Chung",
      "Seyun Um",
      "Hong-Goo Kang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.716": {
    "title": "Mitigating Sequential Dependencies: A Survey of Algorithms and Systems for Generation-Refinement Frameworks in Autoregressive Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunhai Hu",
      "Zining Liu",
      "Zhenyuan Dong",
      "Tianfan Peng",
      "Bradley McDanel",
      "Sai Qian Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.717": {
    "title": "Do We Really Need All Those Dimensions? An Intrinsic Evaluation Framework for Compressed Embeddings",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nathan Inkiriwang",
      "Necva Bölücü",
      "Garth Tarr",
      "Maciej Rybinski"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.718": {
    "title": "Mixture of LoRA Experts for Continual Information Extraction with LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zitao Wang",
      "Xinyi Wang",
      "Wei Hu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.719": {
    "title": "Spelling-out is not Straightforward: LLMs' Capability of Tokenization from Token to Characters",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tatsuya Hiraoka",
      "Kentaro Inui"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.720": {
    "title": "OAgents: An Empirical Study of Building Effective Agents",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "He Zhu",
      "Tianrui Qin",
      "King Zhu",
      "Heyuan Huang",
      "Yeyi Guan",
      "Jinxiang Xia",
      "Hanhao Li",
      "Yi Yao",
      "Ningning Wang",
      "Pai Liu",
      "Tianhao Peng",
      "Xin Gui",
      "Li Xiaowan",
      "Yuhui Liu",
      "Xiangru Tang",
      "Jian Yang",
      "Ge Zhang",
      "Xitong Gao",
      "Yuchen Eleanor Jiang",
      "Changwang Zhang",
      "Jun Wang",
      "Jiaheng Liu",
      "Wangchunshu Zhou"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.721": {
    "title": "2Columns1Row: A Russian Benchmark for Textual and Multimodal Table Understanding and Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vildan Saburov",
      "Daniil Vodolazsky",
      "Danil Sazanakov",
      "Alena Fenogenova"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.722": {
    "title": "Permitted Knowledge Boundary: Evaluating the Knowledge-Constrained Responsiveness of Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenrui Bao",
      "Kai Wang",
      "Siqiang Luo",
      "Xiang Li"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.723": {
    "title": "A Closer Look at Bias and Chain-of-Thought Faithfulness of Large (Vision) Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sriram Balasubramanian",
      "Samyadeep Basu",
      "Soheil Feizi"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.724": {
    "title": "From Remembering to Metacognition: Do Existing Benchmarks Accurately Evaluate LLMs?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Geng Zhang",
      "Yizhou Ying",
      "Sihang Jiang",
      "Jiaqing Liang",
      "Guanglei Yue",
      "Yifei Fu",
      "Hailin Hu",
      "Yanghua Xiao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.725": {
    "title": "How a Bilingual LM Becomes Bilingual: Tracing Internal Representations with Sparse Autoencoders",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tatsuro Inaba",
      "Go Kamoda",
      "Kentaro Inui",
      "Masaru Isonuma",
      "Yusuke Miyao",
      "Yohei Oseki",
      "Yu Takagi",
      "Benjamin Heinzerling"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.726": {
    "title": "MultiConIR: Towards Multi-Condition Information Retrieval",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Lu",
      "Sifan Liu",
      "Bochao Yin",
      "Yongqi Li",
      "Xinghao Chen",
      "Hui Su",
      "Yaohui Jin",
      "Wenjun Zeng",
      "Xiaoyu Shen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.727": {
    "title": "HMCL: Task-Optimal Text Representation Adaptation through Hierarchical Contrastive Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenyi Wang",
      "Yapeng Jia",
      "Haiyan Ning",
      "Peng Wang",
      "Dan Wang",
      "Yitao Cao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.728": {
    "title": "KBAlign: Efficient Self Adaptation on Specific Textual Knowledge Bases",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheni Zeng",
      "Yuxuan Chen",
      "Shi Yu",
      "Ruobing Wang",
      "Yukun Yan",
      "Zhenghao Liu",
      "Shuo Wang",
      "Xu Han",
      "Zhiyuan Liu",
      "Maosong Sun"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.729": {
    "title": "Revisiting Chain-of-Thought Prompting: Zero-shot Can Be Stronger than Few-shot",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Cheng",
      "Chengyan Pan",
      "Minjun Zhao",
      "Deyang Li",
      "Fangchao Liu",
      "Xinyu Zhang",
      "Xiao Zhang",
      "Yong Liu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.730": {
    "title": "RMTBench: Benchmarking LLMs Through Multi-Turn User-Centric Role-Playing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Xiang",
      "Tianyi Tang",
      "Yang Su",
      "Bowen Yu",
      "An Yang",
      "Fei Huang",
      "Yichang Zhang",
      "Yaojie Lu",
      "Hongyu Lin",
      "Xianpei Han",
      "Jingren Zhou",
      "Junyang Lin",
      "Le Sun"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.731": {
    "title": "Smart-Searcher: Incentivizing the Dynamic Knowledge Acquisition of LLMs via Reinforcement Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huatong Song",
      "Jinhao Jiang",
      "Wenqing Tian",
      "Zhipeng Chen",
      "Yuhuan Wu",
      "Jiahao Zhao",
      "Yingqian Min",
      "Xin Zhao",
      "Lei Fang",
      "Ji-Rong Wen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.732": {
    "title": "InteGround: On the Evaluation of Verification and Retrieval Planning in Integrative Grounding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Jiayang",
      "Qianqian Zhuang",
      "Haoran Li",
      "Chunkit Chan",
      "Xin Liu",
      "Lin Qiu",
      "Yangqiu Song"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.733": {
    "title": "MM-CRITIC: A Holistic Evaluation of Large Multimodal Models as Multimodal Critique",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gailun Zeng",
      "Ziyang Luo",
      "Hongzhan Lin",
      "Yuchen Tian",
      "Kaixin Li",
      "Ziyang Gong",
      "Jianxiong Guo",
      "Jing Ma"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.734": {
    "title": "On the Correspondence between the Squared Norm and Information Content in Text Embeddings",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Enrique Amigo",
      "Adrian Ghajari",
      "Alejandro Benito-Santos",
      "Diego De La Fuente Rodríguez"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.735": {
    "title": "Adversary-Aware DPO: Enhancing Safety Alignment in Vision Language Models via Adversarial Training",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fenghua Weng",
      "Jian Lou",
      "Jun Feng",
      "Minlie Huang",
      "Wenjie Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.736": {
    "title": "SLiNT: Structure-aware Language Model with Injection and Contrastive Training for Knowledge Graph Completion",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengxue Yang",
      "Chun Yang",
      "Jiaqi Zhu",
      "Jiafan Li",
      "Jingqi Zhang",
      "Yuyang Li",
      "Ying Li"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.737": {
    "title": "LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiqun Shen",
      "Song Yuan",
      "Zhengze Zhang",
      "Xiaoliang Wang",
      "Daxin Jiang",
      "Nguyen Cam-Tu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.738": {
    "title": "LoRA-PAR: A Flexible Dual-System LoRA Partitioning Approach to Efficient LLM Fine-Tuning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yining Huang",
      "Bin Li",
      "Keke Tang",
      "Meilian Chen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.739": {
    "title": "SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning Trajectory Synthesis",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuang Sun",
      "Huatong Song",
      "Yuhao Wang",
      "Ruiyang Ren",
      "Jinhao Jiang",
      "Junjie Zhang",
      "Fei Bai",
      "Jia Deng",
      "Xin Zhao",
      "Zheng Liu",
      "Lei Fang",
      "Zhongyuan Wang",
      "Ji-Rong Wen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.740": {
    "title": "LLaVE: Large Language and Vision Embedding Models with Hardness-Weighted Contrastive Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhibin Lan",
      "Liqiang Niu",
      "Fandong Meng",
      "Jie Zhou",
      "Jinsong Su"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.741": {
    "title": "SampleMix: A Sample-wise Pre-training Data Mixing Strategy by Coordinating Data Quality and Diversity",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangyu Xi",
      "Deyang Kong",
      "Jian Yang",
      "Jiawei Yang",
      "Zhengyu Chen",
      "Wei Wang",
      "Jingang Wang",
      "Xunliang Cai",
      "Shikun Zhang",
      "Wei Ye"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.742": {
    "title": "Evaluating Test-Time Scaling LLMs for Legal Reasoning: OpenAI o1, DeepSeek-R1, and Beyond",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinghao Hu",
      "Yaoyao Yu",
      "Leilei Gan",
      "Bin Wei",
      "Kun Kuang",
      "Fei Wu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.743": {
    "title": "LLM Agents for Education: Advances and Applications",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhendong Chu",
      "Shen Wang",
      "Jian Xie",
      "Tinghui Zhu",
      "Yibo Yan",
      "Jingheng Ye",
      "Aoxiao Zhong",
      "Xuming Hu",
      "Jing Liang",
      "Philip S. Yu",
      "Qingsong Wen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.744": {
    "title": "Modeling Subjectivity in Cognitive Appraisal with Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxiang Zhou",
      "Hainiu Xu",
      "Desmond Ong",
      "Maria Liakata",
      "Petr Slovak",
      "Yulan He"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.745": {
    "title": "Dementia Through Different Eyes: Explainable Modeling of Human and LLM Perceptions for Early Awareness",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lotem Peled-Cohen",
      "Maya Zadok",
      "Nitay Calderon",
      "Hila Gonen",
      "Roi Reichart"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.746": {
    "title": "Mitigating Hallucinations in Large Vision-Language Models by Self-Injecting Hallucinations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Lu",
      "Ziqi Zhang",
      "Chunfeng Yuan",
      "Jun Gao",
      "Congxuan Zhang",
      "Xiaojuan Qi",
      "Bing Li",
      "Weiming Hu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.747": {
    "title": "How Much Do Large Language Models Know about Human Motion? A Case Study in 3D Avatar Control",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kunhang Li",
      "Jason Naradowsky",
      "Yansong Feng",
      "Yusuke Miyao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.748": {
    "title": "The Search for Conflicts of Interest: Open Information Extraction in Scientific Publications",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Garima Gaur",
      "Oana Balalau",
      "Ioana Manolescu",
      "Prajna Upadhyay"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.749": {
    "title": "On Collaborating Small and Large Models For Few-shot Intent Detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peng Chen",
      "Bang Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.750": {
    "title": "A Survey on LLMs for Story Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maria Teleki",
      "Vedangi Bengali",
      "Xiangjue Dong",
      "Sai Tejas Janjur",
      "Haoran Liu",
      "Tian Liu",
      "Cong Wang",
      "Ting Liu",
      "Yin Zhang",
      "Frank Shipman",
      "James Caverlee"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.751": {
    "title": "From Knowledge to Treatment: Large Language Model Assisted Biomedical Concept Representation for Drug Repurposing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengrui Xiang",
      "Tengfei Ma",
      "Xiangzheng Fu",
      "Yiping Liu",
      "Bosheng Song",
      "Xiangxiang Zeng"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.752": {
    "title": "SKRAG: A Retrieval-Augmented Generation Framework Guided by Reasoning Skeletons over Knowledge Graphs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaotong Xu",
      "Yizhao Wang",
      "Yunfei Liu",
      "Shengyang Li"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.753": {
    "title": "A Generative Framework for Personalized Sticker Retrieval",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changjiang Zhou",
      "Ruqing Zhang",
      "Jiafeng Guo",
      "Yu-An Liu",
      "Fan Zhang",
      "Ganyuan Luo",
      "Xueqi Cheng"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.754": {
    "title": "Bridging Semantic and Modality Gaps in Zero-Shot Captioning via Retrieval from Synthetic Data",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyue Liu",
      "Wenkai Zhou"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.755": {
    "title": "Humor in Pixels: Benchmarking Large Multimodal Models Understanding of Online Comics",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuriel Ryan",
      "Rui Yang Tan",
      "Kenny Tsu Wei Choo",
      "Roy Ka-Wei Lee"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.756": {
    "title": "BiMediX2 : Bio-Medical EXpert LMM for Diverse Medical Modalities",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sahal Shaji Mullappilly",
      "Mohammed Irfan Kurpath",
      "Sara Pieri",
      "Saeed Yahya Alseiari",
      "Shanavas Cholakkal",
      "Khaled M Aldahmani",
      "Fahad Shahbaz Khan",
      "Rao Muhammad Anwer",
      "Salman Khan",
      "Timothy Baldwin",
      "Hisham Cholakkal"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.757": {
    "title": "DeMAC: Enhancing Multi-Agent Coordination with Dynamic DAG and Manager-Player Feedback",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhan Liu",
      "Cong Xu",
      "Lu Liu",
      "Yihua Wang",
      "Feiyu Chen",
      "Qi Jia",
      "Yaqian Zhao",
      "Zhichun Wang",
      "Xiang Li"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.758": {
    "title": "Coherence of Argumentative Dialogue Snippets: A New Method for Large Scale Evaluation with an Application to Inference Anchoring Theory",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paul Piwek",
      "Jacopo Amidei",
      "Svetlana Stoyanchev"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.759": {
    "title": "Angular Dispersion Accelerates k-Nearest Neighbors Machine Translation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Evgeniia Tokarchuk",
      "Sergey Troshin",
      "Vlad Niculae"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.760": {
    "title": "Benchmarking Contextual and Paralinguistic Reasoning in Speech-LLMs: A Case Study with In-the-Wild Data",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiongqiong Wang",
      "Hardik Bhupendra Sailor",
      "Tianchi Liu",
      "Wenyu Zhang",
      "Muhammad Huzaifah",
      "Nattadaporn Lertcheva",
      "Shuo Sun",
      "Nancy F. Chen",
      "Jinyang Wu",
      "AiTi Aw"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.761": {
    "title": "This is not a Disimprovement: Improving Negation Reasoning in Large Language Models via Prompt Engineering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joshua Jose Dias Barreto",
      "Abhik Jana"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.762": {
    "title": "Make Every Letter Count: Building Dialect Variation Dictionaries from Monolingual Corpora",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Robert Litschko",
      "Verena Blaschke",
      "Diana Burkhardt",
      "Barbara Plank",
      "Diego Frassinelli"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.763": {
    "title": "SelfAug: Mitigating Catastrophic Forgetting in Retrieval-Augmented Generation via Distribution Self-Alignment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuqing Huang",
      "Rongyang Zhang",
      "Qimeng Wang",
      "Chengqiang Lu",
      "Yan Gao",
      "Yiwu",
      "Yao Hu",
      "Xuyang Zhi",
      "Guiquan Liu",
      "Xin Li",
      "Hao Wang",
      "Enhong Chen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.764": {
    "title": "SEKE: Specialised Experts for Keyword Extraction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matej Martinc",
      "Thi Hong Hanh Tran",
      "Senja Pollak",
      "Boshko Koloski"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.765": {
    "title": "1+1>2: A Synergistic Sparse and Low-Rank Compression Method for Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeliang Zong",
      "Kai Zhang",
      "Zheyang Li",
      "Wenming Tan",
      "Ye Ren",
      "Yiyan Zhai",
      "Jilin Hu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.766": {
    "title": "InfiMM-WebMath-40B: Advancing Multimodal Pre-Training for Enhanced Mathematical Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaotian Han",
      "Yiren Jian",
      "Xuefeng Hu",
      "Haogeng Liu",
      "Yiqi Wang",
      "Qihang Fan",
      "Yuang Ai",
      "Huaibo Huang",
      "Ran He",
      "Zhenheng Yang",
      "Quanzeng You"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.767": {
    "title": "Zero-Shot Defense Against Toxic Images via Inherent Multimodal Alignment in LVLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Zhao",
      "Zhe Li",
      "Yige Li",
      "Jun Sun"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.768": {
    "title": "Retrieval Augmented Generation based context discovery for ASR",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siskos Dimitrios",
      "Stavros Papadopoulos",
      "Pablo Peso Parada",
      "Jisi Zhang",
      "Karthikeyan Saravanan",
      "Anastasios Drosou"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.769": {
    "title": "pFedRAG: A Personalized Federated Retrieval-Augmented Generation System with Depth-Adaptive Tiered Embedding Tuning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hangyu He",
      "Xin Yuan",
      "Kai Wu",
      "Ren Ping Liu",
      "Wei Ni"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.770": {
    "title": "ReCUT: Balancing Reasoning Length and Accuracy in LLMs via Stepwise Trails and Preference Optimization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhensheng Jin",
      "Xinze Li",
      "Yifan Ji",
      "Chunyi Peng",
      "Zhenghao Liu",
      "Qi Shi",
      "Yukun Yan",
      "Shuo Wang",
      "Furong Peng",
      "Ge Yu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.771": {
    "title": "CURE: Controlled Unlearning for Robust Embeddings — Mitigating Conceptual Shortcuts in Pre-Trained Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aysenur Kocak",
      "Shuo Yang",
      "Bardh Prenkaj",
      "Gjergji Kasneci"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.772": {
    "title": "MLAlgo-Bench: Can Machines Implement Machine Learning Algorithms?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunfei Wang",
      "Yeqin Zhang",
      "Yuyang Wu",
      "Liang Lu",
      "Phi Le Nguyen",
      "Xiaoliang Wang",
      "Nguyen Cam-Tu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.773": {
    "title": "Fair Text-Attributed Graph Representation Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruilin Luo",
      "Tianle Gu",
      "Lin Wang",
      "Yunfeng Zhou",
      "Songtao Jiang",
      "Lei Wang",
      "Yujiu Yang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.774": {
    "title": "Human-Inspired Obfuscation for Model Unlearning: Local and Global Strategies with Hyperbolic Representations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zekun Wang",
      "Jingjie Zeng",
      "Yingxu Li",
      "Liang Yang",
      "Hongfei Lin"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.775": {
    "title": "Do Influence Functions Work on Large Language Models?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhe Li",
      "Wei Zhao",
      "Yige Li",
      "Jun Sun"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.776": {
    "title": "TRUEBench: Can LLM Response Meet Real-world Constraints as Productivity Assistant?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiho Park",
      "Jongyoon Song",
      "Minjin Choi",
      "Kyuho Heo",
      "Taehun Huh",
      "Ji Won Kim"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.777": {
    "title": "CausalMACE: Causality Empowered Multi-Agents in Minecraft Cooperative Tasks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Chai",
      "Zhang Zheng",
      "Junlong Ren",
      "Deheng Ye",
      "Zichuan Lin",
      "Hao Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.778": {
    "title": "Harry Potter is Still Here! Probing Knowledge Leakage in Targeted Unlearned Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bang Trinh Tran To",
      "Thai Le"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.779": {
    "title": "Learning Trajectories of Figurative Language for Pre-Trained Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicola Arici",
      "Luca Putelli",
      "Ejdis Gjinika",
      "Ivan Serina",
      "Alfonso Gerevini"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.780": {
    "title": "BcQLM: Efficient Vision-Language Understanding with Distilled Q-Gated Cross-Modal Fusion",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sike Xiang",
      "Shuang Chen",
      "Amir Atapour-Abarghouei"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.781": {
    "title": "HapticCap: A Multimodal Dataset and Task for Understanding User Experience of Vibration Haptic Signals",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guimin Hu",
      "Daniel Hershcovich",
      "Hasti Seifi"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.782": {
    "title": "SubDocTrans: Enhancing Document-level Machine Translation with Plug-and-play Multi-granularity Knowledge Augmentation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanghai Hong",
      "Yibo Xie",
      "Jiawei Zheng",
      "Xiaoli Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.783": {
    "title": "Social Bias Evaluation for Large Language Models Requires Prompt Variations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rem Hida",
      "Masahiro Kaneko",
      "Naoaki Okazaki"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.784": {
    "title": "Training with Fewer Bits: Unlocking Edge LLMs Training with Stochastic Rounding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taowen Liu",
      "Marta Andronic",
      "Deniz Gunduz",
      "George Anthony Constantinides"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.785": {
    "title": "FactReasoner: A Probabilistic Approach to Long-Form Factuality Assessment for Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Radu Marinescu",
      "Debarun Bhattacharjya",
      "Junkyu Lee",
      "Tigran T. Tchrakian",
      "Javier Carnerero-Cano",
      "Yufang Hou",
      "Elizabeth M. Daly",
      "Alessandra Pascale"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.786": {
    "title": "Robust Knowledge Editing via Explicit Reasoning Chains for Distractor-Resilient Multi-Hop QA",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchen Wu",
      "Liang Ding",
      "Li Shen",
      "Dacheng Tao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.787": {
    "title": "RadialRouter: Structured Representation for Efficient and Robust Large Language Models Routing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruihan Jin",
      "Pengpeng Shao",
      "Zhengqi Wen",
      "Jinyang Wu",
      "Mingkuan Feng",
      "Shuai Zhang",
      "Jianhua Tao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.788": {
    "title": "Decoding Uncertainty: The Impact of Decoding Strategies for Uncertainty Estimation in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wataru Hashimoto",
      "Hidetaka Kamigaito",
      "Taro Watanabe"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.789": {
    "title": "Elucidating Mechanisms of Demographic Bias in LLMs for Healthcare",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hiba Ahsan",
      "Arnab Sen Sharma",
      "Silvio Amir",
      "David Bau",
      "Byron C Wallace"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.790": {
    "title": "Can You Trick the Grader? Adversarial Persuasion of LLM Judges",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yerin Hwang",
      "Dongryeol Lee",
      "Taegwan Kang",
      "Yongil Kim",
      "Kyomin Jung"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.791": {
    "title": "Navigating the Unknown: Intent Classification and Out-of-Distribution Detection Using Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yusuf Sali",
      "Sıtkı Can Toraman"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.792": {
    "title": "Trust Me, I'm Wrong: LLMs Hallucinate with Certainty Despite Knowing the Answer",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adi Simhi",
      "Itay Itzhak",
      "Fazl Barez",
      "Gabriel Stanovsky",
      "Yonatan Belinkov"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.793": {
    "title": "QUARTZ: QA-based Unsupervised Abstractive Refinement for Task-oriented Dialogue Summarization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohamed Imed Eddine Ghebriout",
      "Gaël Guibon",
      "Ivan Lerner",
      "Emmanuel Vincent"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.794": {
    "title": "MDSEval: A Meta-Evaluation Benchmark for Multimodal Dialogue Summarization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinhong Liu",
      "Jianfeng He",
      "Hang Su",
      "Ruixue Lian",
      "Yi Nian",
      "Jake W. Vincent",
      "Srikanth Vishnubhotla",
      "Robinson Piramuthu",
      "Saab Mansour"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.795": {
    "title": "PMPO: Probabilistic Metric Prompt Optimization for Small and Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "ChenZhuo Zhao",
      "Ziqian Liu",
      "Xinda Wang",
      "Junting Lu",
      "Chaoyi Ruan"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.796": {
    "title": "Evaluating the Creativity of LLMs in Persian Literary Text Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Armin Tourajmehr",
      "Mohammad Reza Modarres",
      "Yadollah Yaghoobzadeh"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.797": {
    "title": "SCDTour: Embedding Axis Ordering and Merging for Interpretable Semantic Change Detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taichi Aida",
      "Danushka Bollegala"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.798": {
    "title": "Resolving UnderEdit & OverEdit with Iterative & Neighbor-Assisted Model Editing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bhiman Kumar Baghel",
      "Emma Jordan",
      "Zheyuan Ryan Shi",
      "Xiang Lorraine Li"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.799": {
    "title": "LLM-empowered Dynamic Prompt Routing for Vision-Language Models Tuning under Long-Tailed Distributions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongju Jia",
      "Jiarui Ma",
      "Xiangxian Li",
      "Baiqiao Zhang",
      "Xianhui Cao",
      "Juan Liu",
      "Yulong Bian"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.800": {
    "title": "HGAdapter: Hypergraph-based Adapters in Language Models for Code Summarization and Clone Detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guang Yang",
      "Yujie Zhu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.801": {
    "title": "Evaluating distillation methods for data-efficient syntax learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Takateru Yamakoshi",
      "Thomas L. Griffiths",
      "R. Thomas McCoy",
      "Robert D. Hawkins"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.802": {
    "title": "Going to a trap house\" conveys more fear than \"Going to a mall\": Benchmarking Emotion Context Sensitivity for LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eojin Jeon",
      "Mingyu Lee",
      "Sangyun Kim",
      "Junho Kim",
      "Wanzee Cho",
      "Tae-Eui Kam",
      "SangKeun Lee"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.803": {
    "title": "[MASK]ED - Language Modeling for Explainable Classification and Disentangling of Socially Unacceptable Discourse",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dimitra Niaouri",
      "Mohamed Rayane Ghilene",
      "Michele Linardi",
      "Julien Longhi"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.804": {
    "title": "A Survey of Cognitive Distortion Detection and Classification in NLP",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Archie Sage",
      "Jeroen Keppens",
      "Helen Yannakoudakis"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.805": {
    "title": "Curse of Knowledge: Your Guidance and Provided Knowledge are biasing LLM Judges in Complex Evaluation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiyuan Li",
      "Xintao Wang",
      "Siyu Yuan",
      "Rui Xu",
      "Jiangjie Chen",
      "Qingqing Dong",
      "Yanghua Xiao",
      "Deqing Yang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.806": {
    "title": "Self-Training Large Language Models with Confident Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyosoon Jang",
      "Yunhui Jang",
      "Sungjae Lee",
      "Jungseul Ok",
      "Sungsoo Ahn"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.807": {
    "title": "Error Typing for Smarter Rewards: Improving Process Reward Models with Error-Aware Hierarchical Supervision",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tej Deep Pala",
      "Panshul Sharma",
      "Amir Zadeh",
      "Chuan Li",
      "Soujanya Poria"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.808": {
    "title": "Enhancing LLM-Based Persuasion Simulations with Cultural and Speaker-Specific Information",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weicheng Ma",
      "Hefan Zhang",
      "Shiyu Ji",
      "Farnoosh Hashemi",
      "Qichao Wang",
      "Ivory Yang",
      "Joice Chen",
      "Juanwen Pan",
      "Michael Macy",
      "Saeed Hassanpour",
      "Soroush Vosoughi"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.809": {
    "title": "An LLM-based Temporal-spatial Data Generation and Fusion Approach for Early Detection of Late Onset Alzheimer's Disease (LOAD) Stagings Especially in Chinese and English-speaking Populations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Han",
      "Jacqueline C.k. Lam",
      "Victor O.k. Li",
      "Lawrence Y. L. Cheung"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.810": {
    "title": "Side Effects of Erasing Concepts from Diffusion Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaswati Saha",
      "Sourajit Saha",
      "Manas Gaur",
      "Tejas Gokhale"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.811": {
    "title": "SaCa: A Highly Compatible Reinforcing Framework for Knowledge Graph Embedding via Structural Pattern Contrast",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiashi Lin",
      "Changhong Jiang",
      "Yixiao Wang",
      "Xinyi Zhu",
      "Zhongtian Hu",
      "Wei Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.812": {
    "title": "Real, Fake, or Manipulated? Detecting Machine-Influenced Text",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yitong Wang",
      "Zhongping Zhang",
      "Margherita Piana",
      "Zheng Zhou",
      "Peter Gerstoft",
      "Bryan A. Plummer"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.813": {
    "title": "Character is Destiny: Can Persona-assigned Language Models Make Personal Choices?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Xu",
      "Xintao Wang",
      "Jiangjie Chen",
      "Siyu Yuan",
      "Xinfeng Yuan",
      "Jiaqing Liang",
      "Zulong Chen",
      "Xiaoqingdong",
      "Yanghua Xiao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.814": {
    "title": "Neutral Is Not Unbiased: Evaluating Implicit and Intersectional Identity Bias in LLMs Through Structured Narrative Scenarios",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saba Ghanbari Haez",
      "Mauro Dragoni"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.815": {
    "title": "BTW: A Non-Parametric Variance Stabilization Framework for Multimodal Model Integration",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Hou",
      "Le Wang",
      "Xuan Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.816": {
    "title": "Can LLMs Be Efficient Predictors of Conversational Derailment?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaustubh Olpadkar",
      "Vikram Sunil Bajaj",
      "Leslie Barrett"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.817": {
    "title": "Q-PRM: Adaptive Query Rewriting for Retrieval-Augmented Generation via Step-level Process Supervision",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaopeng Ye",
      "Chen Xu",
      "Chaoliang Zhang",
      "Zhaocheng Du",
      "Jun Xu",
      "Gang Wang",
      "Zhenhua Dong"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.818": {
    "title": "Factuality Beyond Coherence: Evaluating LLM Watermarking Methods for Medical Texts",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rochana Prih Hastuti",
      "Rian Adam Rajagede",
      "Mansour Al Ghanim",
      "Mengxin Zheng",
      "Qian Lou"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.819": {
    "title": "Guess What I am Thinking: A Benchmark for Inner Thought Reasoning of Role-Playing Language Agents",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Xu",
      "Mingyu Wang",
      "Xintao Wang",
      "Dakuan Lu",
      "Xiaoyu Tan",
      "Wei Chu",
      "Xu Yinghui"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.820": {
    "title": "Dropping Experts, Recombining Neurons: Retraining-Free Pruning for Sparse Mixture-of-Experts LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixiao Zhou",
      "Ziyu Zhao",
      "Dongzhou Cheng",
      "Zhiliang Wu",
      "Jie Gui",
      "Yi Yang",
      "Fei Wu",
      "Yu Cheng",
      "Hehe Fan"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.821": {
    "title": "BiasFilter: An Inference-Time Debiasing Framework for Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoqing Cheng",
      "Ruizhe Chen",
      "Hongying Zan",
      "Yuxiang Jia",
      "Min Peng"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.822": {
    "title": "X-LeBench: A Benchmark for Extremely Long Egocentric Video Understanding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenqi Zhou",
      "Kai Cao",
      "Hao Zheng",
      "Yunze Liu",
      "Xinyi Zheng",
      "Miao Liu",
      "Per Ola Kristensson",
      "Walterio W. Mayol-Cuevas",
      "Fan Zhang",
      "Weizhe Lin",
      "Junxiao Shen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.823": {
    "title": "A Survey on Multi-modal Intent Recognition: Recent Advances and New Frontiers",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihong Zhu",
      "Fan Zhang",
      "Yunyan Zhang",
      "Jinghan Sun",
      "Zhiqi Huang",
      "Qingqing Long",
      "Bowen Xing",
      "Xian Wu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.824": {
    "title": "Will Annotators Disagree? Identifying Subjectivity in Value-Laden Arguments",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amir Homayounirad",
      "Enrico Liscio",
      "Tong Wang",
      "Catholijn M Jonker",
      "Luciano Cavalcante Siebert"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.825": {
    "title": "LLMs Can Compensate for Deficiencies in Visual Representations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sho Takishita",
      "Jay Gala",
      "Abdelrahman Mohamed",
      "Kentaro Inui",
      "Yova Kementchedjhieva"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.826": {
    "title": "Adapting Large Language Models for Character-based Augmentative and Alternative Communication",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dylan Gaines",
      "Keith Vertanen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.827": {
    "title": "Token-Level Metrics for Detecting Incorrect Gold Annotations in Named Entity Recognition",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elena Merdjanovska",
      "Alan Akbik"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.828": {
    "title": "Exploring Paraphrasing Strategies for CEFR A1-Level Constraints in LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eugenio Marzona",
      "Maria Goikhman",
      "Alessio Palmero Aprosio",
      "Massimo Zancanaro"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.829": {
    "title": "Efficient Layer-wise LLM Fine-tuning for Revision Intention Prediction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhexiong Liu",
      "Diane Litman"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.830": {
    "title": "ConText-LE: Cross-Distribution Generalization for Longitudinal Experiential Data via Narrative-Based LLM Representations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahatsham Hayat",
      "Bilal Khan",
      "Mohammad Rashedul Hasan"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.831": {
    "title": "Chain of Strategy Optimization Makes Large Language Models Better Emotional Supporter",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weixiang Zhao",
      "Xingyu Sui",
      "Xinyang Han",
      "Yang Deng",
      "Yulin Hu",
      "Jiahe Guo",
      "Libo Qin",
      "Qianyun Du",
      "Shijin Wang",
      "Yanyan Zhao",
      "Bing Qin",
      "Ting Liu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.832": {
    "title": "Unlocking Legal Knowledge: A Multilingual Dataset for Judicial Summarization in Switzerland",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luca Rolshoven",
      "Vishvaksenan Rasiah",
      "Srinanda Brügger Bose",
      "Sarah Hostettler",
      "Lara Burkhalter",
      "Matthias Stürmer",
      "Joel Niklaus"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.833": {
    "title": "Context Minimization for Resource-Constrained Text Classification: Optimizing Performance-Efficiency Trade-offs through Linguistic Features",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nahid Hossain",
      "Md Faisal Kabir"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.834": {
    "title": "FLAIRR-TS - Forecasting LLM-Agents with Iterative Refinement and Retrieval for Time Series",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gunjan Jalori",
      "Preetika Verma",
      "Sercan O Arik"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.835": {
    "title": "ULTRABENCH: Benchmarking LLMs under Extreme Fine-grained Text Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Longfei Yun",
      "Letian Peng",
      "Jingbo Shang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.836": {
    "title": "The Price of Format: Diversity Collapse in LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Longfei Yun",
      "Chenyang An",
      "Zilong Wang",
      "Letian Peng",
      "Jingbo Shang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.837": {
    "title": "Zipf's and Heaps' Laws for Tokens and LLM-generated Texts",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikolay Mikhaylovskiy"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.838": {
    "title": "LLMs for Bayesian Optimization in Scientific Domains: Are We There Yet?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rushil Gupta",
      "Jason Hartford",
      "Bang Liu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.839": {
    "title": "A Comprehensive Taxonomy of Negation for NLP and Neural Retrievers",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roxana Petcu",
      "Samarth Bhargav",
      "Maarten de Rijke",
      "Evangelos Kanoulas"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.840": {
    "title": "Identifying Noise in Human-Created Datasets using Training Dynamics from Generative Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maeda Hanafi",
      "Ishan Jindal",
      "Yannis Katsis",
      "Lucian Popa",
      "Huaiyu Zhu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.841": {
    "title": "Can Multiple Responses from an LLM Reveal the Sources of Its Uncertainty?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Nan",
      "Pengfei He",
      "Ravi Tandon",
      "Han Xu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.842": {
    "title": "AfroXLMR-Social: Adapting Pre-trained Language Models for African Languages Social Media Text",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tadesse Destaw Belay",
      "Israel Abebe Azime",
      "Ibrahim Said Ahmad",
      "David Ifeoluwa Adelani",
      "Idris Abdulmumin",
      "Abinew Ali Ayele",
      "Shamsuddeen Hassan Muhammad",
      "Seid Muhie Yimam"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.843": {
    "title": "Teaching Language Models To Gather Information Proactively",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tenghao Huang",
      "Sihao Chen",
      "Muhao Chen",
      "Jonathan May",
      "Longqi Yang",
      "Mengting Wan",
      "Pei Zhou"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.844": {
    "title": "Linguistic Alignment Predicts Learning in Small Group Tutoring Sessions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dorothea French",
      "Robert Moulder",
      "Kelechi Ezema",
      "Katharina von der Wense",
      "Sidney K. DMello"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.845": {
    "title": "EfficientXLang: Towards Improving Token Efficiency Through Cross-Lingual Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanchit Ahuja",
      "Praneetha Vaddamanu",
      "Barun Patra"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.846": {
    "title": "Not Lost After All: How Cross-Encoder Attribution Challenges Position Bias Assumptions in LLM Summarization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elahe Rahimi",
      "Hassan Sajjad",
      "Domenic Rosati",
      "Abeer Badawi",
      "Elham Dolatabadi",
      "Frank Rudzicz"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.847": {
    "title": "FuzzAug: Data Augmentation by Coverage-guided Fuzzing for Neural Test Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifeng He",
      "Jicheng Wang",
      "Yuyang Rong",
      "Hao Chen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.848": {
    "title": "DrAgent: Empowering Large Language Models as Medical Agents for Multi-hop Medical Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fenglin Liu",
      "Zheng Li",
      "Hongjian Zhou",
      "Qingyu Yin",
      "Jingfeng Yang",
      "Xin Liu",
      "Zhengyang Wang",
      "Xianfeng Tang",
      "Shiyang Li",
      "Xiang He",
      "Ruijie Wang",
      "Bing Yin",
      "Xiao Gu",
      "Lei Clifton",
      "David A. Clifton"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.849": {
    "title": "XRAG: Cross-lingual Retrieval-Augmented Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Liu",
      "Sony Trenous",
      "Leonardo F. R. Ribeiro",
      "Bill Byrne",
      "Felix Hieber"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.850": {
    "title": "Can VLMs Recall Factual Associations From Visual References?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dhananjay Ashok",
      "Ashutosh Chaubey",
      "Hirona Jacqueline Arai",
      "Jonathan May",
      "Jesse Thomason"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.851": {
    "title": "MFTCXplain: A Multilingual Benchmark Dataset for Evaluating the Moral Reasoning of LLMs through Multi-hop Hate Speech Explanation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jackson Trager",
      "Francielle Vargas",
      "Diego Alves",
      "Matteo Guida",
      "Mikel K. Ngueajio",
      "Ameeta Agrawal",
      "Yalda Daryani",
      "Farzan Karimi Malekabadi",
      "Flor Miriam Plaza-del-Arco"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.852": {
    "title": "Large Language Models for Multilingual Previously Fact-Checked Claim Detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ivan Vykopal",
      "Matúš Pikuliak",
      "Simon Ostermann",
      "Tatiana Anikina",
      "Michal Gregor",
      "Marian Simko"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.853": {
    "title": "Debating for Better Reasoning in Vision-Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashutosh Adhikari",
      "Mirella Lapata"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.854": {
    "title": "Fine-tuning LLMs with Cross-Attention-based Weight Decay for Bias Mitigation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Farsheed Haque",
      "Zhe Fu",
      "Depeng Xu",
      "Shuhan Yuan",
      "Xi Niu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.855": {
    "title": "Profiling LLM's Copyright Infringement Risks under Adversarial Persuasive Prompting",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jikai Long",
      "Ming Liu",
      "Xiusi Chen",
      "Jialiang Xu",
      "Shenglan Li",
      "Zhaozhuo Xu",
      "Denghui Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.856": {
    "title": "Residualized Similarity for Faithfully Explainable Authorship Verification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peter Zeng",
      "Pegah Alipoormolabashi",
      "Jihu Mun",
      "Gourab Dey",
      "Nikita Soni",
      "Niranjan Balasubramanian",
      "Owen Rambow",
      "H. Schwartz"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.857": {
    "title": "Post-hoc Study of Climate Microtargeting on Social Media Ads with LLMs: Thematic Insights and Fairness Evaluation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tunazzina Islam",
      "Dan Goldwasser"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.858": {
    "title": "MRFD: Multi-Region Fusion Decoding with Self-Consistency for Mitigating Hallucinations in LVLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haonan Ge",
      "Yiwei Wang",
      "Ming-Hsuan Yang",
      "Yujun Cai"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.859": {
    "title": "SIMBA UQ: Similarity-Based Aggregation for Uncertainty Quantification in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Debarun Bhattacharjya",
      "Balaji Ganesan",
      "Junkyu Lee",
      "Radu Marinescu",
      "Katya Mirylenka",
      "Michael Glass",
      "Xiao Shou"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.860": {
    "title": "Mind the Dialect: NLP Advancements Uncover Fairness Disparities for Arabic Users in Recommendation Systems",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abdulla Alshabanah",
      "Murali Annavaram"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.861": {
    "title": "Hopscotch: Discovering and Skipping Redundancies in Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mustafa Eyceoz",
      "Nikhil Shivakumar Nayak",
      "Hao Wang",
      "Ligong Han",
      "Akash Srivastava"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.862": {
    "title": "CLEAR: A Clinically Grounded Tabular Framework for Radiology Report Evaluation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuyang Jiang",
      "Chacha Chen",
      "Shengyuan Wang",
      "Feng Li",
      "Zecong Tang",
      "Benjamin M. Mervak",
      "Lydia Chelala",
      "Christopher M Straus",
      "Reve Chahine",
      "Samuel G. Armato Iii",
      "Chenhao Tan"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.863": {
    "title": "Parsing the Switch: LLM-Based UD Annotation for Complex Code-Switched and Low-Resource Languages",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Olga Kellert",
      "Nemika Tyagi",
      "Muhammad Imran",
      "Nelvin Licona-Guevara",
      "Carlos Gómez-Rodríguez"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.864": {
    "title": "HetGCoT: Heterogeneous Graph-Enhanced Chain-of-Thought LLM Reasoning for Academic Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runsong Jia",
      "Mengjia Wu",
      "Ying Ding",
      "Jie Lu",
      "Yi Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.865": {
    "title": "S*: Test Time Scaling for Code Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dacheng Li",
      "Shiyi Cao",
      "Chengkun Cao",
      "Xiuyu Li",
      "Shangyin Tan",
      "Kurt Keutzer",
      "Jiarong Xing",
      "Joseph E. Gonzalez",
      "Ion Stoica"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.866": {
    "title": "Language Models Can Easily Learn to Reason from Demonstrations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dacheng Li",
      "Shiyi Cao",
      "Tyler Griggs",
      "Shu Liu",
      "Xiangxi Mo",
      "Eric Tang",
      "Sumanth Hegde",
      "Kourosh Hakhamaneshi",
      "Shishir G Patil",
      "Matei Zaharia",
      "Joseph E. Gonzalez",
      "Ion Stoica"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.867": {
    "title": "FSTs vs ICL: Generalisation in LLMs for an under-resourced language",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ximena Gutierrez",
      "Mikel Segura Elizalde",
      "Victor Mijangos"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.868": {
    "title": "SRM-LLM: Semantic Relationship Mining with LLMs for Temporal Knowledge Graph Extrapolation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fu Zhang",
      "Panfeng Zhang",
      "Jingwei Cheng"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.869": {
    "title": "Captioning for Text-Video Retrieval via Dual-Group Direct Preference Optimization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ji Soo Lee",
      "Byungoh Ko",
      "Jaewon Cho",
      "Howoong Lee",
      "Jaewoon Byun",
      "Hyunwoo J. Kim"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.870": {
    "title": "Benchmarking and Improving LLM Robustness for Personalized Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chimaobi Okite",
      "Naihao Deng",
      "Kiran Bodipati",
      "Huaidian Hou",
      "Joyce Chai",
      "Rada Mihalcea"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.871": {
    "title": "MemeInterpret: Towards an All-in-One Dataset for Meme Understanding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeongsik Park",
      "Khoi P. N. Nguyen",
      "Jihyung Park",
      "Minseok Kim",
      "Jaeheon Lee",
      "Jae Won Choi",
      "Kalyani Ganta",
      "Phalgun Ashrit Kasu",
      "Rohan Sarakinti",
      "Sanjana Vipperla",
      "Sai Sathanapalli",
      "Nishan Vaghani",
      "Vincent Ng"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.872": {
    "title": "CoRAG: Enhancing Hybrid Retrieval-Augmented Generation through a Cooperative Retriever Architecture",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zaiyi Zheng",
      "Song Wang",
      "Zihan Chen",
      "Yaochen Zhu",
      "Yinhan He",
      "Liangjie Hong",
      "Qi Guo",
      "Jundong Li"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.873": {
    "title": "Hallucination Detection in Structured Query Generation via LLM Self-Debating",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miaoran Li",
      "Jiangning Chen",
      "Minghua Xu",
      "Xiaolong Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.874": {
    "title": "Not All Options Are Created Equal: Textual Option Weighting for Token-Efficient LLM-Based Knowledge Tracing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jongwoo Kim",
      "SeongYeub Chu",
      "Bryan Wong",
      "Mun Yong Yi"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.875": {
    "title": "Public Data Assisted Differentially Private In-Context Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seongho Joo",
      "Hyukhun Koh",
      "Kyomin Jung"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.876": {
    "title": "Inducing Argument Facets for Faithful Opinion Summarization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Wang",
      "Yanjie Liang",
      "Yuqing Sun",
      "Bin Gong"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.877": {
    "title": "Scaling Laws Are Unreliable for Downstream Tasks: A Reality Check",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicholas Lourie",
      "Michael Y. Hu",
      "Kyunghyun Cho"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.878": {
    "title": "Familiarity-Aware Evidence Compression for Retrieval-Augmented Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongwon Jung",
      "Qin Liu",
      "Tenghao Huang",
      "Ben Zhou",
      "Muhao Chen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.879": {
    "title": "O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice Conversion",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huu Tuong Tu",
      "Huan Vu",
      "Cuong Tien Nguyen",
      "Dien Hy Ngo",
      "Nguyen Thi Thu Trang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.880": {
    "title": "Simple Factuality Probes Detect Hallucinations in Long-Form Natural Language Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiatong Han",
      "Neil Band",
      "Muhammed Razzak",
      "Jannik Kossen",
      "Tim G. J. Rudner",
      "Yarin Gal"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.881": {
    "title": "CESRec: Constructing Pseudo Interactions for Sequential Recommendation via Conversational Feedback",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Wang",
      "Shen Gao",
      "Jiabao Fang",
      "Rui Yan",
      "Billy Chiu",
      "Shuo Shang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.882": {
    "title": "TTPA: Token-level Tool-use Preference Alignment Training Framework with Fine-grained Evaluation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengrui Huang",
      "Shen Gao",
      "Zhengliang Shi",
      "Dongsheng Wang",
      "Shuo Shang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.883": {
    "title": "Avoiding Knowledge Edit Skipping in Multi-hop Question Answering with Guided Decomposition",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Liu",
      "Xiangrong Zhu",
      "Xiangyu Liu",
      "Wei Wei",
      "Wei Hu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.884": {
    "title": "Bridging the Creativity Understanding Gap: Small-Scale Human Alignment Enables Expert-Level Humor Ranking in LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kuan Lok Zhou",
      "Jiayi Chen",
      "Siddharth Suresh",
      "Reuben Narad",
      "Timothy T. Rogers",
      "Lalit K Jain",
      "Robert D Nowak",
      "Bob Mankoff",
      "Jifan Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.885": {
    "title": "SMARTMiner: Extracting and Evaluating SMART Goals from Low-Resource Health Coaching Notes",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Iva Bojic",
      "Qi Chwen Ong",
      "Stephanie Hilary Xinyi Ma",
      "Lin Ai",
      "Zheng Liu",
      "Ziwei Gong",
      "Julia Hirschberg",
      "Andy Hau Yan Ho",
      "Andy W. H. Khong"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.886": {
    "title": "GRIL: Knowledge Graph Retrieval-Integrated Learning with Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jialin Chen",
      "Houyu Zhang",
      "Seongjun Yun",
      "Alejandro Mottini",
      "Rex Ying",
      "Xiang Song",
      "Vassilis N. Ioannidis",
      "Zheng Li",
      "Qingjun Cui"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.887": {
    "title": "Exploring Deductive and Inductive Reasoning Capabilities of Large Language Models in Procedural Planning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiabao Kang",
      "Xinye Li",
      "Liyan Xu",
      "Qingbin Liu",
      "Xi Chen",
      "Zhiying Tu",
      "Dianhui Chu",
      "Dianbo Sui"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.888": {
    "title": "KELE: A Multi-Agent Framework for Structured Socratic Teaching with Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xian Peng",
      "Pan Yuan",
      "Dong Li",
      "Junlong Cheng",
      "Qin Fang",
      "Zhi Liu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.889": {
    "title": "VisualEDU: A Benchmark for Assessing Coding and Visual Comprehension through Educational Problem-Solving Video Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Chen",
      "Tianyu Shi",
      "Pengran Huang",
      "Zeyuan Li",
      "Jiahui Pan",
      "Qianglong Chen",
      "Lewei He"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.890": {
    "title": "OkraLong: A Flexible Retrieval-Augmented Framework for Long-Text Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yulong Hui",
      "Yihao Liu",
      "Yao Lu",
      "Huanchen Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.891": {
    "title": "VerifiAgent: a Unified Verification Agent in Language Model Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiuzhou Han",
      "Wray Buntine",
      "Ehsan Shareghi"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.892": {
    "title": "DrKGC: Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph Completion across General and Biomedical Domains",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongkang Xiao",
      "Sinian Zhang",
      "Yi Dai",
      "Huixue Zhou",
      "Jue Hou",
      "Jie Ding",
      "Rui Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.893": {
    "title": "Understanding the Language Model to Solve the Symbolic Multi-Step Reasoning Problem from the Perspective of Buffer Mechanism",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiwei Wang",
      "Yunji Wang",
      "Zhongwang Zhang",
      "Zhangchen Zhou",
      "Hui Jin",
      "Tianyang Hu",
      "Jiacheng Sun",
      "Zhenguo Li",
      "Yaoyu Zhang",
      "Zhi-Qin John Xu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.894": {
    "title": "TwT: Thinking without Tokens by Habitual Reasoning Distillation with Multi-Teachers' Guidance",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingxian Xu",
      "Mengyu Zhou",
      "Weichang Liu",
      "Hanbing Liu",
      "Shi Han",
      "Dongmei Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.895": {
    "title": "DAVIS: Planning Agent with Knowledge Graph-Powered Inner Monologue",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minh Pham Dinh",
      "Michael G Yankoski",
      "Munira Syed",
      "Trenton W. Ford"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.896": {
    "title": "When Instructions Multiply: Measuring and Estimating LLM Capabilities of Multiple Instructions Following",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keno Harada",
      "Yudai Yamazaki",
      "Masachika Taniguchi",
      "Edison Marrese-Taylor",
      "Takeshi Kojima",
      "Yusuke Iwasawa",
      "Yutaka Matsuo"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.897": {
    "title": "FormosanBench: Benchmarking Low-Resource Austronesian Languages in the Era of Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiying Kevin Lin",
      "Hsi-Yu Chen",
      "Haopeng Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.898": {
    "title": "SeaPO: Strategic Error Amplification for Robust Preference Optimization of Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Rao",
      "Yunjie Liao",
      "Xuebo Liu",
      "Zepeng Lin",
      "Lian Lian",
      "Dong Jin",
      "Shengjun Cheng",
      "Jun Yu",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.899": {
    "title": "FigEx: Aligned Extraction of Scientific Figures and Captions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jifeng Song",
      "Arun Das",
      "Ge Cui",
      "Yufei Huang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.900": {
    "title": "PATIMT-Bench: A Multi-Scenario Benchmark for Position-Aware Text Image Machine Translation in Large Vision-Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wanru Zhuang",
      "Wenbo Li",
      "Zhibin Lan",
      "Xu Han",
      "Peng Li",
      "Jinsong Su"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.901": {
    "title": "Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hua Farn",
      "Hsuan Su",
      "Shachi H. Kumar",
      "Saurav Sahay",
      "Shang-Tse Chen",
      "Hung-yi Lee"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.902": {
    "title": "Self-Ensemble: Mitigating Confidence Distortion for Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zicheng Xu",
      "Guanchu Wang",
      "Guangyao Zheng",
      "Yu-Neng Chuang",
      "Alex Szalay",
      "Xia Hu",
      "Vladimir Braverman"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.903": {
    "title": "Annotation-Efficient Language Model Alignment via Diverse and Representative Response Texts",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuu Jinnai",
      "Ukyo Honda"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.904": {
    "title": "Explainable Chain-of-Thought Reasoning: An Empirical Analysis on State-Aware Reasoning Dynamics",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheldon Yu",
      "Yuxin Xiong",
      "Junda Wu",
      "Xintong Li",
      "Tong Yu",
      "Xiang Chen",
      "Ritwik Sinha",
      "Jingbo Shang",
      "Julian McAuley"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.905": {
    "title": "DecisionFlow: Advancing Large Language Model as Principled Decision Maker",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiusi Chen",
      "Shanyong Wang",
      "Cheng Qian",
      "Hongru Wang",
      "Peixuan Han",
      "Heng Ji"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.906": {
    "title": "M-Ped: Multi-Prompt Ensemble Decoding for Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxin Guo",
      "Daimeng Wei",
      "Yuanchang Luo",
      "Hengchao Shang",
      "Zongyao Li",
      "Jinlong Yang",
      "Zhanglin Wu",
      "Zhiqiang Rao",
      "Shimin Tao",
      "Hao Yang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.907": {
    "title": "Butterfly Effects in Toolchains: A Comprehensive Analysis of Failed Parameter Filling in LLM Tool-Agent Systems",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qian Xiong",
      "Yuekai Huang",
      "Ziyou Jiang",
      "Zhiyuan Chang",
      "Yujia Zheng",
      "Tianhao Li",
      "Mingyang Li"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.908": {
    "title": "FinLFQA: Evaluating Attributed Text Generation of LLMs in Financial Long-Form Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yitao Long",
      "Tiansheng Hu",
      "Yilun Zhao",
      "Arman Cohan",
      "Chen Zhao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.909": {
    "title": "BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xu Huang",
      "Wenhao Zhu",
      "Hanxu Hu",
      "Conghui He",
      "Lei Li",
      "Shujian Huang",
      "Fei Yuan"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.910": {
    "title": "Assessing the Sensitivity and Alignment of FOL Closeness Metrics",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ramya Keerthy Thatikonda",
      "Wray Buntine",
      "Ehsan Shareghi"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.911": {
    "title": "FoodSafeSum: Enabling Natural Language Processing Applications for Food Safety Document Summarization and Analysis",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juli Bakagianni",
      "Korbinian Randl",
      "Guido Rocchietti",
      "Cosimo Rulli",
      "Franco Maria Nardini",
      "Salvatore Trani",
      "Aron Henriksson",
      "Anna Romanova",
      "John Pavlopoulos"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.912": {
    "title": "Self-adaptive Dataset Construction for Real-World Multimodal Safety Scenarios",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingen Qu",
      "Lijun Li",
      "Bo Zhang",
      "Yichen Yan",
      "Jing Shao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.913": {
    "title": "EnDive: A Cross-Dialect Benchmark for Fairness and Performance in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhay Gupta",
      "Jacob Cheung",
      "Philip Meng",
      "Shayan Sayyed",
      "Kevin Zhu",
      "Austen Liao",
      "Sean O’Brien"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.914": {
    "title": "FAEDKV: Infinite-Window Fourier Transform for Unbiased KV Cache Compression",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runchao Li",
      "Yao Fu",
      "Mu Sheng",
      "Xianxuan Long",
      "Haotian Yu",
      "Pan Li"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.915": {
    "title": "Dynamic Injection of Entity Knowledge into Dense Retrievers",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ikuya Yamada",
      "Ryokan Ri",
      "Takeshi Kojima",
      "Yusuke Iwasawa",
      "Yutaka Matsuo"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.916": {
    "title": "When Personalization Meets Reality: A Multi-Faceted Analysis of Personalized Preference Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yijiang River Dong",
      "Tiancheng Hu",
      "Yinhong Liu",
      "Ahmet Üstün",
      "Nigel Collier"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.917": {
    "title": "MASTER: Multi-Agent Security Through Exploration of Roles and Topological Structures - A Comprehensive Framework",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Zhu",
      "Chao Zhang",
      "Xin Shi",
      "Xueqiao Zhang",
      "Yi Yang",
      "Yawei Luo"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.918": {
    "title": "MONAQ: Multi-Objective Neural Architecture Querying for Time-Series Analysis on Resource-Constrained Devices",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Patara Trirat",
      "Jae-Gil Lee"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.919": {
    "title": "StandUp4AI: A New Multilingual Dataset for Humor Detection in Stand-up Comedy Videos",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Valentin Barriere",
      "Nahuel Gomez",
      "Léo Hemamou",
      "Sofia Callejas",
      "Brian Ravenet"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.920": {
    "title": "Does Visual Grounding Enhance the Understanding of Embodied Knowledge in Large Language Models?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihui Yang",
      "Yupei Wang",
      "Kaijie Mo",
      "Zhe Zhao",
      "Renfen Hu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.921": {
    "title": "Semantic Contribution-Aware Adaptive Retrieval for Black-Box Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qinhong Lin",
      "Zhongliang Yang",
      "Yuang Cai",
      "Dingfu Yu",
      "Xuan Xu",
      "Yu Li",
      "Linna Zhou"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.922": {
    "title": "On Guardrail Models' Robustness to Mutations and Adversarial Attacks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elias Bassani",
      "Ignacio Sanchez"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.923": {
    "title": "IP-Dialog: Evaluating Implicit Personalization in Dialogue Systems with Synthetic Data",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Peng",
      "Zhiheng Wang",
      "Heyang Gong",
      "Chaochao Lu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.924": {
    "title": "Zero-shot Graph Reasoning via Retrieval Augmented Framework with LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanqing Li",
      "Sharika Mahadevan",
      "Kiran Jyothi Sheena",
      "Henry Liang",
      "Diego Klabjan"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.925": {
    "title": "Privacy in Action: Towards Realistic Privacy Mitigation and Evaluation for LLM-Powered Agents",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shouju Wang",
      "Fenglin Yu",
      "Xirui Liu",
      "Xiaoting Qin",
      "Jue Zhang",
      "Qingwei Lin",
      "Dongmei Zhang",
      "Saravan Rajmohan"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.926": {
    "title": "Dissecting Logical Reasoning in LLMs: A Fine-Grained Evaluation and Supervision Study",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujun Zhou",
      "Jiayi Ye",
      "Zipeng Ling",
      "Yufei Han",
      "Yue Huang",
      "Haomin Zhuang",
      "Zhenwen Liang",
      "Kehan Guo",
      "Taicheng Guo",
      "Xiangqi Wang",
      "Xiangliang Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.927": {
    "title": "ConciseRL: Conciseness-Guided Reinforcement Learning for Efficient Reasoning Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Razvan-Gabriel Dumitru",
      "Darius Peteleaza",
      "Vikas Yadav",
      "Liangming Pan"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.928": {
    "title": "Faster and Better LLMs via Latency-Aware Test-Time Scaling",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zili Wang",
      "Tianyu Zhang",
      "Haoli Bai",
      "Lu Hou",
      "Xianzhi Yu",
      "Wulong Liu",
      "Shiming Xiang",
      "Lei Zhu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.929": {
    "title": "Reasoning-Augmented Conversation for Multi-Turn Jailbreak Attacks on Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zonghao Ying",
      "Deyue Zhang",
      "Zonglei Jing",
      "Yisong Xiao",
      "Quanchen Zou",
      "Aishan Liu",
      "Siyuan Liang",
      "Xiangzheng Zhang",
      "Xianglong Liu",
      "Dacheng Tao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.930": {
    "title": "Distilling Many-Shot In-Context Learning into a Cheat Sheet",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ukyo Honda",
      "Soichiro Murakami",
      "Peinan Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.931": {
    "title": "Tracing Training Footprints: A Calibration Approach for Membership Inference Attacks Against Multimodal Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaofan Zheng",
      "Huixuan Zhang",
      "Xiaojun Wan"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.932": {
    "title": "PolBiX: Detecting LLMs' Political Bias in Fact-Checking through X-phemisms",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Charlott Jakob",
      "David Harbecke",
      "Patrick Parschan",
      "Pia Wenzel Neves",
      "Vera Schmitt"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.933": {
    "title": "URO-Bench: Towards Comprehensive Evaluation for End-to-End Spoken Dialogue Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiqi Yan",
      "Xiquan Li",
      "Wenxi Chen",
      "Zhikang Niu",
      "Chen Yang",
      "Ziyang Ma",
      "Kai Yu",
      "Xie Chen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.934": {
    "title": "Low-Hallucination and Efficient Coreference Resolution with LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujian Gan",
      "Yuan Liang",
      "Jinxia Xie",
      "Yanni Lin",
      "Juntao Yu",
      "Massimo Poesio"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.935": {
    "title": "Your Mileage May Vary: How Empathy and Demographics Shape Human Preferences in LLM Responses",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yishan Wang",
      "Amanda Cercas Curry",
      "Flor Miriam Plaza-del-Arco"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.936": {
    "title": "Diving into Mitigating Hallucinations from a Vision Perspective for Large Vision-Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weihang Wang",
      "Xinhao Li",
      "Ziyue Wang",
      "Yan Pang",
      "Jielei Zhang",
      "Peiyi Li",
      "Qiang Zhang",
      "Longwen Gao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.937": {
    "title": "PhysicsArena: The First Multimodal Physics Reasoning Benchmark Exploring Variable, Process, and Solution Dimensions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Song Dai",
      "Yibo Yan",
      "Jiamin Su",
      "Zihao Dongfang",
      "Yubo Gao",
      "Yonghua Hei",
      "Jungang Li",
      "Junyan Zhang",
      "Sicheng Tao",
      "Zhuoran Gao",
      "Xuming Hu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.938": {
    "title": "Ko-LongRAG: A Korean Long-Context RAG Benchmark Built with a Retrieval-Free Approach",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongil Kim",
      "Heuiyeen Yeen",
      "Hyeongu Yun",
      "Jinsik Lee"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.939": {
    "title": "Choosing a Model, Shaping a Future: Comparing LLM Perspectives on Sustainability and its Relationship with AI",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Annika Bush",
      "Meltem Aksoy",
      "Markus Pauly",
      "Greta Ontrup"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.940": {
    "title": "Optimising Factual Consistency in Summarisation via Preference Learning from Multiple Imperfect Metrics",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Ye",
      "Raul Santos-Rodriguez",
      "Edwin Simpson"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.941": {
    "title": "Judging with Many Minds: Do More Perspectives Mean Less Prejudice? On Bias Amplification and Resistance in Multi-Agent Based LLM-as-Judge",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chiyu Ma",
      "Enpei Zhang",
      "Yilun Zhao",
      "Wenjun Liu",
      "Yaning Jia",
      "Peijun Qing",
      "Lin Shi",
      "Arman Cohan",
      "Yujun Yan",
      "Soroush Vosoughi"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.942": {
    "title": "Investigating the Impact of Conceptual Metaphors on LLM-based NLI through Shapley Interactions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meghdut Sengupta",
      "Maximilian Muschalik",
      "Fabian Fumagalli",
      "Barbara Hammer",
      "Eyke Hüllermeier",
      "Debanjan Ghosh",
      "Henning Wachsmuth"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.943": {
    "title": "KurTail : Kurtosis-based LLM Quantization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Sadegh Akhondzadeh",
      "Aleksandar Bojchevski",
      "Evangelos Eleftheriou",
      "Martino Dazzi"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.944": {
    "title": "VIVA+: Human-Centered Situational Decision-Making",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhe Hu",
      "Yixiao Ren",
      "Guanzhong Liu",
      "Jing Li",
      "Yu Yin"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.945": {
    "title": "QuantAgents: Towards Multi-agent Financial System via Simulated Trading",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangyu Li",
      "Yawen Zeng",
      "Xiaofen Xing",
      "Jin Xu",
      "Xiangmin Xu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.946": {
    "title": "LLMs Reproduce Stereotypes of Sexual and Gender Minorities",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruby Ostrow",
      "Adam Lopez"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.947": {
    "title": "Accept or Deny? Evaluating LLM Fairness and Performance in Loan Approval across Table-to-Text Serialization Approaches",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Israel Abebe Azime",
      "Deborah D. Kanubala",
      "Tejumade Afonja",
      "Mario Fritz",
      "Isabel Valera",
      "Dietrich Klakow",
      "Philipp Slusallek"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.948": {
    "title": "Transfer-Aware Data Selection for Domain Adaptation in Text Retrieval",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linzhu Yu",
      "Huan Li",
      "Ke Chen",
      "Lidan Shou"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.949": {
    "title": "Understanding and Improving Information Preservation in Prompt Compression for LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weronika Łajewska",
      "Momchil Hardalov",
      "Laura Aina",
      "Neha Anna John",
      "Hang Su",
      "Lluis Marquez"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.950": {
    "title": "A Benchmark for Hindi Verb-Argument Structure Alternations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kanishka Jain",
      "Ashwini Vaidya"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.951": {
    "title": "Beyond Binary Preferences: Semi-Online Label-Free GRACE-KTO with Group-Wise Adaptive Calibration for High-Quality Long-Text Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyang Deng",
      "Ran Chen",
      "Jo-Ku Cheng",
      "Jinwen Ma"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.952": {
    "title": "Representation-based Broad Hallucination Detectors Fail to Generalize Out of Distribution",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zuzanna Dubanowska",
      "Maciej Żelaszczyk",
      "Michał Brzozowski",
      "Paolo Mandica",
      "Michal P. Karpowicz"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.953": {
    "title": "MAFMO: Multi-modal Adaptive Fusion with Meta-template Optimization for Vision-Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingrui Xie",
      "Lulu Xu",
      "Junliang Du"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.954": {
    "title": "Multimodal UNcommonsense: From Odd to Ordinary and Ordinary to Odd",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yejin Son",
      "Saejin Kim",
      "Dongjun Min",
      "Youngjae Yu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.955": {
    "title": "Analyzing Gambling Addictions: A Spanish Corpus for Understanding Pathological Behavior",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manuel Couto",
      "Marcos Fernández-Pichel",
      "Mario Ezra Aragon",
      "David E. Losada"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.956": {
    "title": "Beyond Surface Alignment: Rebuilding LLMs Safety Mechanism via Probabilistically Ablating Refusal Direction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanbo Xie",
      "Yingjie Zhang",
      "Tianyun Liu",
      "Duohe Ma",
      "Tingwen Liu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.957": {
    "title": "Distributed LLM Serving on Consumer-Grade GPUs by Reconciling Computation and Communication",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lewei Jin",
      "Kui Zhang",
      "Yongqi Chen",
      "Zhuoyifan",
      "Renjie Li",
      "Yi Gao",
      "Bowei Yang",
      "Zhengong Cai",
      "Wei Dong"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.958": {
    "title": "SafeToolBench: Pioneering a Prospective Benchmark to Evaluating Tool Utilization Safety in LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongfei Xia",
      "Hongru Wang",
      "Zeming Liu",
      "Qian Yu",
      "Yuhang Guo",
      "Haifeng Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.959": {
    "title": "Sparsifying Mamba",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "An Wang",
      "Ruobing Xie",
      "Shuaipeng Li",
      "Xingwu Sun",
      "Zhanhui Kang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.960": {
    "title": "Beneath the Facade: Probing Safety Vulnerabilities in LLMs via Auto-Generated Jailbreak Prompts",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heehyeon Kim",
      "Kyeongryul Lee",
      "Joyce Jiyoung Whang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.961": {
    "title": "ET-MIER: Entity Type-guided Key Mention Identification and Evidence Retrieval for Document-level Relation Extraction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Li",
      "Huangming Xu",
      "Fu Zhang",
      "Jingwei Cheng"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.962": {
    "title": "Position IDs Matter: An Enhanced Position Layout for Efficient Context Compression in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runsong Zhao",
      "Xin Liu",
      "Xinyu Liu",
      "Pengcheng Huang",
      "Chunyang Xiao",
      "Tong Xiao",
      "JingBo Zhu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.963": {
    "title": "Can Role Vectors Affect LLM Behaviour?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniele Potertì",
      "Andrea Seveso",
      "Fabio Mercorio"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.964": {
    "title": "Semantic Component Analysis: Introducing Multi-Topic Distributions to Clustering-Based Topic Modeling",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Florian Eichin",
      "Carolin M. Schuster",
      "Georg Groh",
      "Michael A. Hedderich"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.965": {
    "title": "ThinkQE: Query Expansion via an Evolving Thinking Process",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yibin Lei",
      "Tao Shen",
      "Andrew Yates"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.966": {
    "title": "Hierarchical Reward Modeling for Fault Localization in Large Code Repositories",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiwei Zhang",
      "Jianxun Lian",
      "Haiming Qin",
      "Mingyang Zhou",
      "KeZhong Lu",
      "Rui Mao",
      "Hao Liao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.967": {
    "title": "Layer Duplication in LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Neo Eyal",
      "Nachum Dershowitz",
      "Kfir Bar"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.968": {
    "title": "Semantic-Aware Action Space Compression via LLM-DRL Synergy for Efficient Task-oriented Dialogue Policy Exploration",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangyang Zhao",
      "Ben Niu",
      "Yuxuan Tan",
      "Shihan Wang",
      "Libo Qin"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.969": {
    "title": "Linear Steerability in Language Models: When It Emerges and How It Evolves",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianshu She",
      "Xinyue Li",
      "Eric P. Xing",
      "Zhengzhong Liu",
      "Qirong Ho"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.970": {
    "title": "A Comprehensive Survey on Learning from Rewards for Large Language Models: Reward Models and Learning Strategies",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaobao Wu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.971": {
    "title": "InFact: Informativeness Alignment for Improved LLM Factuality",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roi Cohen",
      "Russa Biswas",
      "Gerard de Melo"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.972": {
    "title": "Large Language Model Agents in Finance: A Survey Bridging Research, Practice, and Real-World Deployment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Dong",
      "Fengyi Wu",
      "Kunlin Zhang",
      "Yilong Dai",
      "Sanjian Zhang",
      "Wanghao Ye",
      "Sihan Chen",
      "Zhi-Qi Cheng"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.973": {
    "title": "Problem Solved? Information Extraction Design Space for Layout-Rich Documents using LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaye Colakoglu",
      "Gürkan Solmaz",
      "Jonathan Fürst"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.974": {
    "title": "Generation-Augmented Retrieval: Rethinking the Role of Large Language Models in Zero-Shot Relation Extraction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zehan Li",
      "Fu Zhang",
      "Tianyue Peng",
      "He Liu",
      "Jingwei Cheng"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.975": {
    "title": "Following Occam's Razor: Dynamic Combination of Structured Knowledge for Multi-Hop Question Answering using LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Chen",
      "Zhi Zheng",
      "Lili Zhao",
      "Huijun Hou",
      "Tong Xu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.976": {
    "title": "Large Language Models as Reader for Bias Detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Luo",
      "Jing Li",
      "Zhong Wenzhong",
      "Geng Tu",
      "Ruifeng Xu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.977": {
    "title": "LOHRec: Leveraging Order and Hierarchy in Generative Sequential Recommendation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawen Xie",
      "Haiyang Wu",
      "Deyi Ji",
      "Yuekui Yang",
      "Shaoping Ma"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.978": {
    "title": "Biology-Instructions: A Dataset and Benchmark for Multi-Omics Sequence Understanding Capability of Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haonan He",
      "Yuchen Ren",
      "Yining Tang",
      "Ziyang Xu",
      "Junxian Li",
      "Minghao Yang",
      "Di Zhang",
      "Yuan Dong",
      "Tao Chen",
      "Shufei Zhang",
      "Yuqiang Li",
      "Nanqing Dong",
      "Wanli Ouyang",
      "Dongzhan Zhou",
      "Peng Ye"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.979": {
    "title": "AssistedDS: Benchmarking How External Domain Knowledge Assists LLMs in Automated Data Science",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "An Luo",
      "Xun Xian",
      "Jin Du",
      "Fangqiao Tian",
      "Ganghua Wang",
      "Ming Zhong",
      "Shengchun Zhao",
      "Xuan Bi",
      "Zirui Liu",
      "Jiawei Zhou",
      "Jayanth Srinivasa",
      "Ashish Kundu",
      "Charles Fleming",
      "Mingyi Hong",
      "Jie Ding"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.980": {
    "title": "Are you sure? Measuring models bias in content moderation through uncertainty",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alessandra Urbinati",
      "Mirko Lai",
      "Simona Frenda",
      "Marco Stranisci"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.981": {
    "title": "FOSSIL: Harnessing Feedback on Suboptimal Samples for Data-Efficient Generalisation with Imitation Learning for Embodied Vision-and-Language Tasks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sabrina McCallum",
      "Amit Parekh",
      "Alessandro Suglia"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.982": {
    "title": "Assess and Prompt: A Generative RL Framework for Improving Engagement in Online Mental Health Communities",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bhagesh Gaur",
      "Karan Gupta",
      "Aseem Srivastava",
      "Manish Gupta",
      "Md Shad Akhtar"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.983": {
    "title": "Logic: Long-form Outline Generation via Imitative and Critical Self-refinement",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hengwei Liu",
      "Yongliang Shen",
      "Zhe Zheng",
      "Haoyuan Ma",
      "Xingyu Wu",
      "Yin Zhang",
      "Weiming Lu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.984": {
    "title": "No Free Lunch: Retrieval-Augmented Generation Undermines Fairness in LLMs, Even for Vigilant Users",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengxuan Hu",
      "Hongyi Wu",
      "Ronghang Zhu",
      "Zihan Guan",
      "Dongliang Guo",
      "Daiqing Qi",
      "Sheng Li"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.985": {
    "title": "LegoSLM: Connecting LLM with Speech Encoder using CTC Posteriors",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rao Ma",
      "Tongzhou Chen",
      "Kartik Audhkhasi",
      "Bhuvana Ramabhadran"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.986": {
    "title": "Skeleton-Guided-Translation: A Benchmarking Framework for Code Repository Translation with Fine-Grained Quality Evaluation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xing Zhang",
      "Jiaheng Wen",
      "Fangkai Yang",
      "Yu Kang",
      "Pu Zhao",
      "Junhao Wang",
      "Maoquan Wang",
      "Yufan Huang",
      "Shengyu Fu",
      "Elsie Nallipogu",
      "Qingwei Lin",
      "Yingnong Dang",
      "Saravan Rajmohan",
      "Dongmei Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.987": {
    "title": "Parallel Communities Across the Surface Web and the Dark Web",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenchao Dong",
      "Megha Sundriyal",
      "Seongchan Park",
      "Jaehong Kim",
      "Meeyoung Cha",
      "Tanmoy Chakraborty",
      "Wonjae Lee"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.988": {
    "title": "Lemma Dilemma: On Lemma Generation Without Domain- or Language-Specific Training Data",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Olia Toporkov",
      "Alan Akbik",
      "Rodrigo Agerri"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.989": {
    "title": "LlmFixer: Fix the Helpfulness of Defensive Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zelong Yu",
      "Xiaoming Zhang",
      "Litian Zhang",
      "Yu Yuan",
      "Chaozhuo Li"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.990": {
    "title": "Universal Acoustic Adversarial Attacks for Flexible Control of Speech-LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rao Ma",
      "Mengjie Qian",
      "Vyas Raina",
      "Mark Gales",
      "Kate Knill"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.991": {
    "title": "Probing Semantic Routing in Large Mixture-of-Expert Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew Lyle Olson",
      "Neale Ratzlaff",
      "Musashi Hinck",
      "Man Luo",
      "Sungduk Yu",
      "Chendi Xue",
      "Vasudev Lal"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.992": {
    "title": "CMT-Eval: A Novel Chinese Multi-turn Dialogue Evaluation Dataset Addressing Real-world Conversational Challenges",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyu Tian",
      "Kaijie Mo",
      "Yupei Wang",
      "Renfen Hu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.993": {
    "title": "LastingBench: Defend Benchmarks Against Knowledge Leakage",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixiong Fang",
      "Tianran Sun",
      "Yuling Shi",
      "Min Wang",
      "Xiaodong Gu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.994": {
    "title": "Learning API Functionality from In-Context Demonstrations for Tool-based Agents",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bhrij Patel",
      "Ashish Jagmohan",
      "Aditya Vempaty"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.995": {
    "title": "Predicting Language Models' Success at Zero-Shot Probabilistic Prediction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kevin Ren",
      "Santiago Cortes-Gomez",
      "Carlos Miguel Patiño",
      "Ananya Joshi",
      "Ruiqi Lyu",
      "Jingjing Tang",
      "Alistair Turcan",
      "Khurram Yamin",
      "Steven Wu",
      "Bryan Wilder"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.996": {
    "title": "GAMIC: Graph-Aligned Molecular In-context Learning for Molecule Analysis via LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Al Lawati",
      "Jason S Lucas",
      "Zhiwei Zhang",
      "Prasenjit Mitra",
      "Suhang Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.997": {
    "title": "Rethinking Sign Language Translation: The Impact of Signer Dependence on Model Evaluation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keren Artiaga",
      "Sabyasachi Kamila",
      "Haithem Afli",
      "Conor Lynch",
      "Mohammed Hasanuzzaman"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.998": {
    "title": "Can Large Language Models Identify Implicit Suicidal Ideation? An Empirical Evaluation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tong Li",
      "Shu Yang",
      "Junchao Wu",
      "Jiyao Wei",
      "Lijie Hu",
      "Mengdi Li",
      "Derek F. Wong",
      "Joshua R. Oltmanns",
      "Di Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.999": {
    "title": "Adaptive Platt Scaling with Causal Interpretations for Self-Reflective Language Model Uncertainty Estimates",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anthony Sicilia",
      "Malihe Alikhani"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1000": {
    "title": "Treble Counterfactual VLMs: A Causal Approach to Hallucination",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li Li",
      "Jiashu Qu",
      "Linxin Song",
      "Yuxiao Zhou",
      "Yuehan Qin",
      "Tiankai Yang",
      "Yue Zhao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1001": {
    "title": "Video-Skill-CoT: Skill-based Chain-of-Thoughts for Domain-Adaptive Video Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daeun Lee",
      "Jaehong Yoon",
      "Jaemin Cho",
      "Mohit Bansal"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1002": {
    "title": "Glitter: A Multi-Sentence, Multi-Reference Benchmark for Gender-Fair German Machine Translation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "A Pranav",
      "Janiça Hackenbuchner",
      "Giuseppe Attanasio",
      "Manuel Lardelli",
      "Anne Lauscher"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1003": {
    "title": "From n-gram to Attention: How Model Architectures Learn and Propagate Bias in Language Modeling",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohsinul Kabir",
      "Tasfia Tahsin",
      "Sophia Ananiadou"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1004": {
    "title": "SENTRA: Selected-Next-Token Transformer for LLM Text Detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mitchell Plyler",
      "Yilun Zhang",
      "Alexander Tuzhilin",
      "Saoud Khalifah",
      "Sen Tian"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1005": {
    "title": "Automate Strategy Finding with LLM in Quant Investment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhizhuo Kou",
      "Holam Yu",
      "Junyu Luo",
      "Jingshu Peng",
      "Xujia Li",
      "Chengzhong Liu",
      "Juntao Dai",
      "Lei Chen",
      "Sirui Han",
      "Yike Guo"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1006": {
    "title": "Does Reasoning Introduce Bias? A Study of Social Bias Evaluation and Mitigation in LLM Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuyang Wu",
      "Jinming Nian",
      "Ting-Ruen Wei",
      "Zhiqiang Tao",
      "Hsin-Tai Wu",
      "Yi Fang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1007": {
    "title": "MT-RewardTree: A Comprehensive Framework for Advancing LLM-Based Machine Translation via Reward Modeling",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaopeng Feng",
      "Jiahan Ren",
      "Jiayuan Su",
      "Jiamei Zheng",
      "Hongwei Wang",
      "Zuozhu Liu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1008": {
    "title": "Bias after Prompting: Persistent Discrimination in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nivedha Sivakumar",
      "Natalie Mackraz",
      "Samira Khorshidi",
      "Krishna Patel",
      "Barry-John Theobald",
      "Luca Zappella",
      "Nicholas Apostoloff"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1009": {
    "title": "CARVQ: Corrective Adaptor with Group Residual Vector Quantization for LLM Embedding Compression",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dayin Gou",
      "Sanghyun Byun",
      "Nilesh Malpeddi",
      "Gabrielle De Micheli",
      "Prathamesh Vaste",
      "Jacob Song",
      "Woo Seong Chung"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1010": {
    "title": "Consistent Discourse-level Temporal Relation Extraction Using Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Fan",
      "Michael Strube"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1011": {
    "title": "MMPlanner: Zero-Shot Multimodal Procedural Planning with Chain-of-Thought Object State Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Afrina Tabassum",
      "Bin Guo",
      "Xiyao Ma",
      "Hoda Eldardiry",
      "Ismini Lourentzou"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1012": {
    "title": "Internal states before wait modulate reasoning patterns",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dmitrii Troitskii",
      "Koyena Pal",
      "Chris Wendler",
      "Callum Stuart McDougall"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1013": {
    "title": "Sparsity May Be All You Need: Sparse Random Parameter Adaptation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jesus Rios",
      "Pierre Dognin",
      "Ronny Luss",
      "Karthikeyan Natesan Ramamurthy"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1014": {
    "title": "Learning to Align: Addressing Character Frequency Distribution Shifts in Handwritten Text Recognition",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Panagiotis Kaliosis",
      "John Pavlopoulos"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1015": {
    "title": "MT-R1-Zero: Advancing LLM-based Machine Translation via R1-Zero-like Reinforcement Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaopeng Feng",
      "Shaosheng Cao",
      "Jiahan Ren",
      "Jiayuan Su",
      "Ruizhe Chen",
      "Yan Zhang",
      "Jian Wu",
      "Zuozhu Liu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1016": {
    "title": "Discrete Minds in a Continuous World: Do Language Models Know Time Passes?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minghan Wang",
      "Ye Bai",
      "Thuy-Trang Vu",
      "Ehsan Shareghi",
      "Gholamreza Haffari"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1017": {
    "title": "DLTKG: Denoising Logic-based Temporal Knowledge Graph Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoke Wang",
      "Fu Zhang",
      "Jingwei Cheng",
      "Yiwen Chi",
      "Jiashun Peng",
      "Yingsong Ning"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1018": {
    "title": "EMO-RL: Emotion-Rule-Based Reinforcement Learning Enhanced Audio-Language Model for Generalized Speech Emotion Recognition",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengcheng Li",
      "Botao Zhao",
      "Zuheng Kang",
      "Junqing Peng",
      "Xiaoyang Qu",
      "Yayun He",
      "Jianzong Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1019": {
    "title": "MANTA: A Scalable Pipeline for Transmuting Massive Web Corpora into Instruction Datasets",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heuiyeen Yeen",
      "Seokhee Hong",
      "Hyeongu Yun",
      "Jinsik Lee"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1020": {
    "title": "Fast Quiet-STaR: Thinking Without Thought Tokens",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Huang",
      "Yizhe Xiong",
      "Xin Ye",
      "Zhijie Deng",
      "Hui Chen",
      "Zijia Lin",
      "Guiguang Ding"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1021": {
    "title": "Lock on Target! Precision Unlearning via Directional Control",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuntao Wen",
      "Ruixiang Feng",
      "Feng Guo",
      "Yifan Wang",
      "Ran Le",
      "Yang Song",
      "Shen Gao",
      "Shuo Shang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1022": {
    "title": "UniRAG: A Unified RAG Framework for Knowledge-Intensive Queries with Decomposition, Break-Down Reasoning, and Iterative Rewriting",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gun Il Kim",
      "Jong Wook Kim",
      "Beakcheol Jang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1023": {
    "title": "One Shot Dominance: Knowledge Poisoning Attack on Retrieval-Augmented Generation Systems",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan Chang",
      "Mingyang Li",
      "Xiaojun Jia",
      "Junjie Wang",
      "Yuekai Huang",
      "Ziyou Jiang",
      "Yang Liu",
      "Qing Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1024": {
    "title": "From Generic Empathy to Personalized Emotional Support: A Self-Evolution Framework for User Preference Alignment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Ye",
      "Lu Xiang",
      "Yaping Zhang",
      "Chengqing Zong"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1025": {
    "title": "MaskCD: Mitigating LVLM Hallucinations by Image Head Masked Contrastive Decoding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyuan Deng",
      "Yujiu Yang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1026": {
    "title": "ClusterUCB: Efficient Gradient-Based Data Selection for Targeted Fine-Tuning of LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zige Wang",
      "Qi Zhu",
      "Fei Mi",
      "Minghui Xu",
      "Ruochun Jin",
      "Wenjing Yang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1027": {
    "title": "TrapDoc: Deceiving LLM Users by Injecting Imperceptible Phantom Tokens into Documents",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyundong Jin",
      "Sicheol Sung",
      "Shinwoo Park",
      "SeungYeop Baik",
      "Yo-Sub Han"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1028": {
    "title": "AraReasoner: Evaluating Reasoning-Based LLMs for Arabic NLP",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahmed Abul Hasanaath",
      "Aisha Alansari",
      "Ahmed Ashraf",
      "Salmane Chafik",
      "Hamzah Luqman",
      "Saad Ezzini"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1029": {
    "title": "Tales of Morality: Comparing Human- and LLM-Generated Moral Stories from Visual Cues",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rezvaneh Rezapour",
      "Sullam Jeoung",
      "Zhiwen You",
      "Jana Diesner"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1030": {
    "title": "AirRAG: Autonomous Strategic Planning and Reasoning Steer Retrieval Augmented Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenfeng Feng",
      "Chuzhan Hao",
      "Yuewei Zhang",
      "Guochao Jiang",
      "Jingyi Song"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1031": {
    "title": "Evaluating NL2SQL via SQL2NL",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammadtaher Safarzadeh",
      "Afshin Oroojlooy",
      "Dan Roth"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1032": {
    "title": "DB-Explore: Automated Database Exploration and Instruction Synthesis for Text-to-SQL",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyuan Ma",
      "Yongliang Shen",
      "Hengwei Liu",
      "Wenqi Zhang",
      "Haolei Xu",
      "Qiuying Peng",
      "Jun Wang",
      "Weiming Lu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1033": {
    "title": "Do BERT-Like Bidirectional Models Still Perform Better on Text Classification in the Era of LLMs?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyan Zhang",
      "Yiming Huang",
      "Shuliang Liu",
      "Yubo Gao",
      "Xuming Hu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1034": {
    "title": "Divide, Optimize, Merge: Scalable Fine-Grained Generative Optimization for LLM Agents",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiale Liu",
      "Yifan Zeng",
      "Shaokun Zhang",
      "Chi Zhang",
      "Malte Højmark-Bertelsen",
      "Marie Normann Gadeberg",
      "Huazheng Wang",
      "Qingyun Wu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1035": {
    "title": "Evaluating Evaluation Metrics – The Mirage of Hallucination Detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Atharva Kulkarni",
      "Yuan Zhang",
      "Joel Ruben Antony Moniz",
      "Xiou Ge",
      "Bo-Hsiang Tseng",
      "Dhivya Piraviperumal",
      "Swabha Swayamdipta",
      "Hong Yu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1036": {
    "title": "The Progress Illusion: Revisiting meta-evaluation standards of LLM evaluators",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianruo Rose Xu",
      "Vedant Gaur",
      "Liu Leqi",
      "Tanya Goyal"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1037": {
    "title": "MidPO: Dual Preference Optimization for Safety and Helpfulness in Large Language Models via a Mixture of Experts Framework",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yupeng Qi",
      "Ziyu Lyu",
      "Min Yang",
      "Yanlin Wang",
      "Lu Bai",
      "Lixin Cui"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1038": {
    "title": "From KMMLU-Redux to Pro: A Professional Korean Benchmark Suite for LLM Evaluation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seokhee Hong",
      "Sunkyoung Kim",
      "Guijin Son",
      "Soyeon Kim",
      "Yeonjung Hong",
      "Jinsik Lee"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1039": {
    "title": "RealBench: A Chinese Multi-image Understanding Benchmark Close to Real-world Scenarios",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fei Zhao",
      "Chengqiang Lu",
      "Yufan Shen",
      "Qimeng Wang",
      "Yicheng Qian",
      "Haoxin Zhang",
      "Yan Gao",
      "Yiwu",
      "Yao Hu",
      "Zhen Wu",
      "Shangyu Xing",
      "Xinyu Dai"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1040": {
    "title": "The More, The Better? A Critical Study of Multimodal Context in Radiology Report Summarization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mong Yuan Sim",
      "Wei Emma Zhang",
      "Xiang Dai",
      "Biaoyan Fang",
      "Sarbin Ranjitkar",
      "Arjun Burlakoti",
      "Jamie Taylor",
      "Haojie Zhuang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1041": {
    "title": "Localizing Malicious Outputs from CodeLLM",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mayukh Borana",
      "Junyi Liang",
      "Sai Sathiesh Rajan",
      "Sudipta Chattopadhyay"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1042": {
    "title": "Knowing More, Acting Better: Hierarchical Representation for Embodied Decision-Making",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chunhui Zhang",
      "Zhongyu Ouyang",
      "Xingjian Diao",
      "Zheyuan Liu",
      "Soroush Vosoughi"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1043": {
    "title": "Culture is Everywhere: A Call for Intentionally Cultural Evaluation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juhyun Oh",
      "Inha Cha",
      "Michael Saxon",
      "Hyunseung Lim",
      "Shaily Bhatt",
      "Alice Oh"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1044": {
    "title": "Fairness in Automatic Speech Recognition Isn't a One-Size-Fits-All",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hend ElGhazaly",
      "Bahman Mirheidari",
      "Heidi Christensen",
      "Nafise Sadat Moosavi"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1045": {
    "title": "Uncovering Factor-Level Preference to Improve Human-Model Alignment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juhyun Oh",
      "Eunsu Kim",
      "Jiseon Kim",
      "Wenda Xu",
      "Inha Cha",
      "William Yang Wang",
      "Alice Oh"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1046": {
    "title": "Adaptive Preference Optimization with Uncertainty-aware Utility Anchor",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaobo Wang",
      "Zixia Jia",
      "Jiaqi Li",
      "Qi Liu",
      "Zilong Zheng"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1047": {
    "title": "GRAD: Generative Retrieval-Aligned Demonstration Sampler for Efficient Few-Shot Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oussama Gabouj",
      "Kamel Charaf",
      "Ivan Zakazov",
      "Nicolas Baldwin",
      "Robert West"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1048": {
    "title": "IoTMigrator: LLM-driven Embedded IoT Code Migration across Different OSes for Cloud-device Integration",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yq",
      "Kaijie Gong",
      "Yi Gao",
      "Hao Wang",
      "Wei Dong"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1049": {
    "title": "ClueAnchor: Clue-Anchored Knowledge Reasoning Exploration and Optimization for Retrieval-Augmented Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Chen",
      "Yukun Yan",
      "Sen Mei",
      "Wanxiang Che",
      "Zhenghao Liu",
      "Qi Shi",
      "Xinze Li",
      "Yuchun Fan",
      "Pengcheng Huang",
      "Qiushi Xiong",
      "Zhiyuan Liu",
      "Maosong Sun"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1050": {
    "title": "BAGELS: Benchmarking the Automated Generation and Extraction of Limitations from Scholarly Text",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ibrahim Al Azher",
      "Miftahul Jannat Mokarrama",
      "Zhishuai Guo",
      "Sagnik Ray Choudhury",
      "Hamed Alhoori"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1051": {
    "title": "Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity Dilemma of Embeddings",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liyan Xu",
      "Zhenlin Su",
      "Mo Yu",
      "Jiangnan Li",
      "Fandong Meng",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1052": {
    "title": "Over-Generation and Compaction: A Prompting Strategy for Procedural Text Adaptation with Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyeongsik Kim",
      "Yanheng Xu",
      "Chaoqun Dong",
      "Fei Du"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1053": {
    "title": "TransBERT: A Framework for Synthetic Translation in Domain-Specific Language Modeling",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julien Knafou",
      "Luc Mottin",
      "Anaïs Mottaz",
      "Alexandre Flament",
      "Patrick Ruch"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1054": {
    "title": "Beyond Fixed-Length Calibration for Post-Training Compression of LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaehoon Oh",
      "Dokwan Oh"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1055": {
    "title": "Attributes as Textual Genes: Leveraging LLMs as Genetic Algorithm Simulators for Conditional Synthetic Data Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangzeng Han",
      "Weisi Liu",
      "Xiaolei Huang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1056": {
    "title": "ReCoVeR the Target Language: Language Steering without Sacrificing Task Performance",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hannah Sterz",
      "Fabian David Schmidt",
      "Goran Glavaš",
      "Ivan Vulić"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1057": {
    "title": "LC-Eval: A Bilingual Multi-Task Evaluation Benchmark for Long-Context Understanding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheikh Jubair",
      "Arwa Omayrah",
      "Amal Alshammari",
      "Alhanoof Althnian",
      "Abdulhamed Alothaimen",
      "Norah A. Alzahrani",
      "Shahad D. Alzaidi",
      "Nora Al-Twairesh",
      "Abdulmohsen Al-Thubaity"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1058": {
    "title": "OVFact: Measuring and Improving Open-Vocabulary Factuality for Long Caption Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Monika Wysoczańska",
      "Shyamal Buch",
      "Anurag Arnab",
      "Cordelia Schmid"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1059": {
    "title": "GRPO-Guided Modality Selection Enhanced LoRA-Tuned LLMs for Multimodal Emotion Recognition",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Chen",
      "Shuwan Yang",
      "Yan Xiang",
      "Ran Song",
      "Yuxin Huang",
      "Zhengtao Yu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1060": {
    "title": "Defending against Indirect Prompt Injection by Instruction Detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tongyu Wen",
      "Chenglong Wang",
      "Xiyuan Yang",
      "Haoyu Tang",
      "Yueqi Xie",
      "Lingjuan Lyu",
      "Zhicheng Dou",
      "Fangzhao Wu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1061": {
    "title": "MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation Capabilities in Any Language",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seyoung Song",
      "Seogyeong Jeong",
      "Eunsu Kim",
      "Jiho Jin",
      "Dongkwan Kim",
      "Jay Shin",
      "Alice Oh"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1062": {
    "title": "CAC-CoT: Connector-Aware Compact Chain-of-Thought for Efficient Reasoning Data Synthesis Across Dual-System Cognitive Tasks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sunguk Choi",
      "Yonghoon Kwon",
      "Heondeuk Lee"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1063": {
    "title": "On the Versatility of Sparse Autoencoders for In-Context Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ikhyun Cho",
      "Gaeul Kwon",
      "Julia Hockenmaier"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1064": {
    "title": "More Documents, Same Length: Isolating the Challenge of Multiple Documents in RAG",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shahar Levy",
      "Nir Mazor",
      "Lihi Shalmon",
      "Michael Hassid",
      "Gabriel Stanovsky"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1065": {
    "title": "CLEAR: A Comprehensive Linguistic Evaluation of Argument Rewriting by Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Huber",
      "Christina Niklaus"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1066": {
    "title": "ALRPHFS: Adversarially Learned Risk Patterns with Hierarchical Fast & Slow Reasoning for Robust Agent Defense",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiyu Xiang",
      "Tong Zhang",
      "Ronghao Chen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1067": {
    "title": "Stop Playing the Guessing Game! Evaluating Conversational Recommender Systems via Target-free User Simulation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "SungHwan Kim",
      "Kwangwook Seo",
      "Tongyoung Kim",
      "Jinyoung Yeo",
      "Dongha Lee"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1068": {
    "title": "Out-of-Context Reasoning in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathan Shaki",
      "Emanuele La Malfa",
      "Michael J. Wooldridge",
      "Sarit Kraus"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1069": {
    "title": "CodeComplex: Dataset for Worst-Case Time Complexity Prediction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "SeungYeop Baik",
      "Joonghyuk Hahn",
      "Jungin Kim",
      "Aditi",
      "Mingi Jeon",
      "Yo-Sub Han",
      "Sang-Ki Ko"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1070": {
    "title": "Weak2Wise: An Automated, Lightweight Framework for Weak-LLM-Friendly Reasoning Synthesis",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianing Lin",
      "Yuanfang Guo",
      "Shunning Liu",
      "Zeming Liu",
      "Yunhong Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1071": {
    "title": "From Tower to Spire: Adding the Speech Modality to a Translation-Specialist LLM",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kshitij Ambilduke",
      "Ben Peters",
      "Sonal Sannigrahi",
      "Anil Keshwani",
      "Tsz Kin Lam",
      "Bruno Martins",
      "Andre Martins",
      "Marcely Zanon Boito"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1072": {
    "title": "LLM Agents at the Roundtable: A Multi-Perspective and Dialectical Reasoning Framework for Essay Scoring",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinhee Jang",
      "Ayoung Moon",
      "Minkyoung Jung",
      "YoungBin Kim",
      "Seung Jin Lee"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1073": {
    "title": "DeepNote: Note-Centric Deep Retrieval-Augmented Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruobing Wang",
      "Qingfei Zhao",
      "Yukun Yan",
      "Daren Zha",
      "Yuxuan Chen",
      "Shi Yu",
      "Zhenghao Liu",
      "Yixuan Wang",
      "Shuo Wang",
      "Xu Han",
      "Zhiyuan Liu",
      "Maosong Sun"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1074": {
    "title": "NormAL LoRA: What is the perfect size?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aastik",
      "Topu Sai Meghana",
      "Chinmay Prakash Kulkarni",
      "Pragya Paramita Sahu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1075": {
    "title": "Inclusive Leadership in the Age of AI: A Dataset and Comparative Study of LLMs vs. Real-Life Leaders in Workplace Action Planning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vindhya Singh",
      "Sabine Schulte im Walde",
      "Ksenia Keplinger"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1076": {
    "title": "Token Preference Optimization with Self-Calibrated Visual-Anchored Rewards for Hallucination Mitigation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jihao Gu",
      "Yingyao Wang",
      "Meng Cao",
      "Pi Bu",
      "Jun Song",
      "Bo Zheng",
      "Yancheng He",
      "Shilong Li"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1077": {
    "title": "EZ-VC: Easy Zero-shot Any-to-Any Voice Conversion",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Advait Joglekar",
      "Divyanshu Singh",
      "Rooshil Rohit Bhatia",
      "Srinivasan Umesh"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1078": {
    "title": "Length Representations in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sangjun Moon",
      "Dasom Choi",
      "Jingun Kwon",
      "Hidetaka Kamigaito",
      "Manabu Okumura"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1079": {
    "title": "MultiLingPoT: Boosting Mathematical Reasoning in LLMs through Multilingual Program Integration",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nianqi Li",
      "Zujie Liang",
      "Siyu Yuan",
      "Jiaqing Liang",
      "Feng Wei",
      "Yanghua Xiao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1080": {
    "title": "Simulating Identity, Propagating Bias: Abstraction and Stereotypes in LLM-Generated Text",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pia Sommerauer",
      "Giulia Rambelli",
      "Tommaso Caselli"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1081": {
    "title": "Do LVLMs Know What They Know? A Systematic Study of Knowledge Boundary Perception in LVLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhikai Ding",
      "Shiyu Ni",
      "Keping Bi"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1082": {
    "title": "Benchmarking Large Language Models for Cryptanalysis and Side-Channel Vulnerabilities",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Utsav Maskey",
      "Chencheng Zhu",
      "Usman Naseem"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1083": {
    "title": "MTabVQA: Evaluating Multi-Tabular Reasoning of Language Models in Visual Space",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anshul Singh",
      "Chris Biemann",
      "Jan Strich"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1084": {
    "title": "TurnBench-MS: A Benchmark for Evaluating Multi-Turn, Multi-Step Reasoning in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiran Zhang",
      "Mo Wang",
      "Xiaoyang Li",
      "Kaixuan Ren",
      "Chencheng Zhu",
      "Usman Naseem"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1085": {
    "title": "Assessing LLM Reasoning Steps via Principal Knowledge Grounding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyeon Hwang",
      "Yewon Cho",
      "Chanwoong Yoon",
      "Yein Park",
      "Minju Song",
      "Kyungjae Lee",
      "Gangwoo Kim",
      "Jaewoo Kang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1086": {
    "title": "Stratified Selective Sampling for Instruction Tuning with Dedicated Scoring Strategy",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paramita Mirza",
      "Lucas Weber",
      "Fabian Küch"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1087": {
    "title": "CoTD-PO: Chain-of-Thought Distillation with Preference Optimization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lujie Niu",
      "Haochen Sun",
      "Fangkun Zhao",
      "Sheng Chen",
      "Zimeng Bai",
      "Jiawei Zhang",
      "Caixia Yuan",
      "Xiaojie Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1088": {
    "title": "Intelligent Document Parsing: Towards End-to-end Document Parsing via Decoupled Content Parsing and Layout Grounding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hangdi Xing",
      "Feiyu Gao",
      "Qi Zheng",
      "Zhaoqing Zhu",
      "Zirui Shao",
      "Ming Yan"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1089": {
    "title": "Feel the Difference? A Comparative Analysis of Emotional Arcs in Real and LLM-Generated CBT Sessions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyi Wang",
      "Jiwei Zhang",
      "Guangtao Zhang",
      "Honglei Guo"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1090": {
    "title": "Beyond Single-User Dialogue: Assessing Multi-User Dialogue State Tracking Capabilities of Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sangmin Song",
      "Juhwan Choi",
      "JungMin Yun",
      "YoungBin Kim"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1091": {
    "title": "All-in-one: Understanding and Generation in Multimodal Reasoning with the MAIA Benchmark",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Davide Testa",
      "Giovanni Bonetta",
      "Raffaella Bernardi",
      "Alessandro Bondielli",
      "Alessandro Lenci",
      "Alessio Miaschi",
      "Lucia Passaro",
      "Bernardo Magnini"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1092": {
    "title": "Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Filippo Momentè",
      "Alessandro Suglia",
      "Mario Giulianelli",
      "Ambra Ferrari",
      "Alexander Koller",
      "Oliver Lemon",
      "David Schlangen",
      "Raquel Fernández",
      "Raffaella Bernardi"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1093": {
    "title": "Entity Profile Generation and Reasoning with LLMs for Entity Alignment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rumana Ferdous Munne",
      "Md Mostafizur Rahman",
      "Yuji Matsumoto"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1094": {
    "title": "Re-FRAME the Meeting Summarization SCOPE: Fact-Based Summarization and Personalization via Questions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Frederic Kirstein",
      "Sonu Kumar",
      "Terry Ruas",
      "Bela Gipp"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1095": {
    "title": "Attack as Defense: Safeguarding Large Vision-Language Models from Jailbreaking by Adversarial Attacks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chongxin Li",
      "Hanzhang Wang",
      "Yuchun Fang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1096": {
    "title": "Emphasising Structured Information: Integrating Abstract Meaning Representation into LLMs for Enhanced Open-Domain Dialogue Evaluation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bohao Yang",
      "Kun Zhao",
      "Dong Liu",
      "Chen Tang",
      "Liang Zhan",
      "Chenghua Lin"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1097": {
    "title": "Differentiated Vision: Unveiling Entity-Specific Visual Modality Requirements for Multimodal Knowledge Graph",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minghang Liu",
      "Yinghan Shen",
      "Zihe Huang",
      "Yuanzhuo Wang",
      "Xuhui Jiang",
      "Huawei Shen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1098": {
    "title": "Post Persona Alignment for Multi-Session Dialogue Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi-Pei Chen",
      "Noriki Nishida",
      "Hideki Nakayama",
      "Yuji Matsumoto"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1099": {
    "title": "MASSIVE-Agents: A Benchmark for Multilingual Function-Calling in 52 Languages",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mayank Kulkarni",
      "Vittorio Mazzia",
      "Judith Gaspers",
      "Chris Hench",
      "Jack FitzGerald"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1100": {
    "title": "Crafting Customisable Characters with LLMs: A Persona-Driven Role-Playing Agent Framework",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bohao Yang",
      "Dong Liu",
      "Chenghao Xiao",
      "Kun Zhao",
      "Chen Tang",
      "Chao Li",
      "Lin Yuan",
      "Yang Guang",
      "Chenghua Lin"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1101": {
    "title": "Can LLMs Express Personality Across Cultures? Introducing CulturalPersonas for Evaluating Trait Alignment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Priyanka Dey",
      "Aayush Bothra",
      "Yugal Khanter",
      "Jieyu Zhao",
      "Emilio Ferrara"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1102": {
    "title": "Exploring the Hidden Reasoning Process of Large Language Models by Misleading Them",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanyu Chen",
      "Peiyang Wang",
      "Yizhou Jiang",
      "Yuqian Liu",
      "Chujie Zhao",
      "Ying Fang",
      "Tianren Zhang",
      "Feng Chen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1103": {
    "title": "When Models Reason in Your Language: Controlling Thinking Language Comes at the Cost of Accuracy",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jirui Qi",
      "Shan Chen",
      "Zidi Xiong",
      "Raquel Fernández",
      "Danielle Bitterman",
      "Arianna Bisazza"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1104": {
    "title": "The Role of Model Confidence on Bias Effects in Measured Uncertainties for Vision-Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyi Liu",
      "Weiguang Wang",
      "Hangfeng He"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1105": {
    "title": "GAttention: Gated Attention for the Detection of Abusive Language",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Horacio Jarquín Vásquez",
      "Hugo Jair Escalante",
      "Manuel Montes",
      "Mario Ezra Aragon"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1106": {
    "title": "Towards Low-Resource Alignment to Diverse Perspectives with Sparse Feedback",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chu Fei Luo",
      "Samuel Dahan",
      "Xiaodan Zhu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1107": {
    "title": "ProtoXTM: Cross-Lingual Topic Modeling with Document-Level Prototype-based Contrastive Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seung-Won Seo",
      "Soon-Sun Kwon"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1108": {
    "title": "One More Question is Enough, Expert Question Decomposition (EQD) Model for Domain Quantitative Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengyu Wang",
      "Sotirios Sabanis",
      "Miguel de Carvalho",
      "Shay B Cohen",
      "Tiejun Ma"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1109": {
    "title": "When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mikhail Seleznyov",
      "Mikhail Chaichuk",
      "Gleb Ershov",
      "Alexander Panchenko",
      "Elena Tutubalina",
      "Oleg Somov"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1110": {
    "title": "RAR2: Retrieval-Augmented Medical Reasoning via Thought-Driven Retrieval",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaishuai Xu",
      "Wenjun Hou",
      "Yi Cheng",
      "Wenjie Li"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1111": {
    "title": "The Security Threat of Compressed Projectors in Large Vision-Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yudong Zhang",
      "Ruobing Xie",
      "Xingwu Sun",
      "Jiansheng Chen",
      "Zhanhui Kang",
      "Di Wang",
      "Yu Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1112": {
    "title": "NarratEX Dataset: Explaining the Dominant Narratives in News Texts",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nuno Guimarães",
      "Purificação Silvano",
      "Ricardo Campos",
      "Alipio Jorge",
      "Ana Filipa Pacheco",
      "Dimitar Iliyanov Dimitrov",
      "Nikolaos Nikolaidis",
      "Roman Yangarber",
      "Elisa Sartori",
      "Nicolas Stefanovitch",
      "Preslav Nakov",
      "Jakub Piskorski",
      "Giovanni Da San Martino"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1113": {
    "title": "Radical Allomorphy: Phonological Surface Forms without Phonology",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Salam Khalifa",
      "Nizar Habash",
      "Owen Rambow"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1114": {
    "title": "Model Calibration for Emotion Detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mihaela Petre-Vlad",
      "Cornelia Caragea",
      "Florentina Hristea"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1115": {
    "title": "From Benchmark to Better Embeddings: Leveraging Synonym Substitution to Enhance Multimodal Models in Ukrainian",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Volodymyr Mudryi",
      "Yurii Laba"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1116": {
    "title": "Context Copying Modulation: The Role of Entropy Neurons in Managing Parametric and Contextual Knowledge Conflicts",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zineddine Tighidet",
      "Andrea Mogini",
      "Hedi Ben younes",
      "Jiali Mei",
      "Patrick Gallinari",
      "Benjamin Piwowarski"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1117": {
    "title": "A Generalizable Rhetorical Strategy Annotation Model Using LLM-based Debate Simulation and Labelling",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiyu Ji",
      "Farnoosh Hashemi",
      "Joice Chen",
      "Juanwen Pan",
      "Weicheng Ma",
      "Hefan Zhang",
      "Sophia Pan",
      "Ming Cheng",
      "Shubham Mohole",
      "Saeed Hassanpour",
      "Soroush Vosoughi",
      "Michael Macy"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1118": {
    "title": "SecDecoding: Steerable Decoding for Safer LLM Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayou Wang",
      "Rundong Liu",
      "Yue Hu",
      "Huijia Wu",
      "Zhaofeng He"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1119": {
    "title": "GENUINE: Graph Enhanced Multi-level Uncertainty Estimation for Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tuo Wang",
      "Adithya Kulkarni",
      "Tyler Cody",
      "Peter A. Beling",
      "Yujun Yan",
      "Dawei Zhou"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1120": {
    "title": "ReviewEval: An Evaluation Framework for AI-Generated Reviews",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Madhav Krishan Garg",
      "Tejash Prasad",
      "Tanmay Singhal",
      "Chhavi Kirtani",
      "Murari Mandal",
      "Dhruv Kumar"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1121": {
    "title": "Overcoming Black-box Attack Inefficiency with Hybrid and Dynamic Select Algorithms",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhinay Shankar Belde",
      "Rohit Ramkumar",
      "Jonathan Rusert"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1122": {
    "title": "GmSLM : Generative Marmoset Spoken Language Modeling",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Talia Sternberg",
      "Michael London",
      "David Omer",
      "Yossi Adi"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1123": {
    "title": "QA‐LIGN: Aligning LLMs through Constitutionally Decomposed QA",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jacob Dineen",
      "Aswin Rrv",
      "Qin Liu",
      "Zhikun Xu",
      "Xiao Ye",
      "Ming Shen",
      "Zhaonan Li",
      "Shijie Lu",
      "Chitta Baral",
      "Muhao Chen",
      "Ben Zhou"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1124": {
    "title": "Characterizing Positional Bias in Large Language Models: A Multi-Model Evaluation of Prompt Order Effects",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Patrick Schilcher",
      "Dominik Karasin",
      "Michael Schöpf",
      "Haisam Saleh",
      "Antonela Tommasel",
      "Markus Schedl"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1125": {
    "title": "You Only Use Reactive Attention Slice When Retrieving From Long Context",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yun Joon Soh",
      "Hanxian Huang",
      "Yuandong Tian",
      "Jishen Zhao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1126": {
    "title": "Fine-Tuned Thoughts: Leveraging Chain-of-Thought Reasoning for Industrial Asset Health Monitoring",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuxin Lin",
      "Dhaval C Patel",
      "Christodoulos Constantinides"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1127": {
    "title": "CoViPAL: Layer-wise Contextualized Visual Token Pruning for Large Vision-Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zicong Tang",
      "Ziyang Ma",
      "Suqing Wang",
      "Zuchao Li",
      "Lefei Zhang",
      "Hai Zhao",
      "Yun Li",
      "Qianren Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1128": {
    "title": "Large Language Models with Temporal Reasoning for Longitudinal Clinical Summarization and Prediction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maya Kruse",
      "Shiyue Hu",
      "Nicholas Derby",
      "Yifu Wu",
      "Samantha Stonbraker",
      "Bingsheng Yao",
      "Dakuo Wang",
      "Elizabeth M. Goldberg",
      "Yanjun Gao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1129": {
    "title": "TransAlign: Machine Translation Encoders are Strong Word Aligners, Too",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benedikt Ebing",
      "Christian Goldschmied",
      "Goran Glavaš"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1130": {
    "title": "Pruning Weights but Not Truth: Safeguarding Truthfulness While Pruning LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yao Fu",
      "Runchao Li",
      "Xianxuan Long",
      "Haotian Yu",
      "Xiaotian Han",
      "Yu Yin",
      "Pan Li"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1131": {
    "title": "Augment before You Try: Knowledge-Enhanced Table Question Answering via Table Expansion",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujian Liu",
      "Jiabao Ji",
      "Tong Yu",
      "Ryan A. Rossi",
      "Sungchul Kim",
      "Handong Zhao",
      "Ritwik Sinha",
      "Yang Zhang",
      "Shiyu Chang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1132": {
    "title": "Evaluating Large Language Models for Belief Inference: Mapping Belief Networks at Scale",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Trisevgeni Papakonstantinou",
      "Antonina Zhiteneva",
      "Ana Yutong Ma",
      "Derek Powell",
      "Zachary Horne"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1133": {
    "title": "Distinguishing fair from unfair compositional generalization tasks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahmad Jabbar",
      "Cleo Condoravdi",
      "Christopher Potts"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1134": {
    "title": "SA-CLIP: Language Guided Image Spatial and Action Feature Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanlin Li",
      "Wenhao Shao",
      "Praboda Rajapaksha",
      "Noel Crespi"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1135": {
    "title": "Inefficiencies of Meta Agents for Agent Design",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Batu El",
      "Mert Yuksekgonul",
      "James Zou"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1136": {
    "title": "SCoder: Progressive Self-Distillation for Bootstrapping Small-Scale Data Synthesizers to Empower Code LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Zhang",
      "Changzhi Zhou",
      "Linmei Hu",
      "Luhao Zhang",
      "Xiancai Chen",
      "Haomin Fu",
      "Yang Yang",
      "Mengdi Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1137": {
    "title": "Linguistically-Controlled Paraphrase Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohamed Elgaar",
      "Hadi Amiri"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1138": {
    "title": "LAWCAT: Efficient Distillation from Quadratic to Linear Attention with Convolution across Tokens for Long Context Modeling",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyu Liu",
      "Souvik Kundu",
      "Lianghao Jiang",
      "Anni Li",
      "Srikanth Ronanki",
      "Sravan Babu Bodapati",
      "Gourav Datta",
      "Peter Anthony Beerel"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1139": {
    "title": "Analyzing Dialectical Biases in LLMs for Knowledge and Reasoning Benchmarks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eileen Pan",
      "Anna Seo Gyeong Choi",
      "Maartje Ter Hoeve",
      "Skyler Seto",
      "Allison Koenecke"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1140": {
    "title": "TreeBoN: Enhancing Inference-Time Alignment with Speculative Tree-Search and Best-of-N Sampling",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahao Qiu",
      "Yifu Lu",
      "Yifan Zeng",
      "Jiacheng Guo",
      "Jiayi Geng",
      "Chenhao Zhu",
      "Xinzhe Juan",
      "Ling Yang",
      "Huazheng Wang",
      "Kaixuan Huang",
      "Yue Wu",
      "Mengdi Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1141": {
    "title": "CulturalFrames: Assessing Cultural Expectation Alignment in Text-to-Image Models and Evaluation Metrics",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shravan Nayak",
      "Mehar Bhatia",
      "Xiaofeng Zhang",
      "Verena Rieser",
      "Lisa Anne Hendricks",
      "Sjoerd Van Steenkiste",
      "Yash Goyal",
      "Karolina Stanczak",
      "Aishwarya Agrawal"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1142": {
    "title": "Decoupled Proxy Alignment: Mitigating Language Prior Conflict for Multimodal Alignment in MLLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenkun Tan",
      "Pengyu Wang",
      "Shaojun Zhou",
      "Botian Jiang",
      "Zhaowei Li",
      "Dong Zhang",
      "Xinghao Wang",
      "Yaqian Zhou",
      "Xipeng Qiu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1143": {
    "title": "Riemannian Optimization for LoRA on the Stiefel Manifold",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "JuneYoung Park",
      "Minjae Kang",
      "Seongbae Lee",
      "Haegang Lee",
      "Seongwan Kim",
      "Jaeho Lee"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1144": {
    "title": "How Real Are Synthetic Therapy Conversations? Evaluating Fidelity in Prolonged Exposure Dialogues",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suhas Bn",
      "Dominik O. Mattioli",
      "Andrew M. Sherrill",
      "Rosa I. Arriaga",
      "Christopher Wiese",
      "Saeed Abdullah"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1145": {
    "title": "Large Language Models for Controllable Multi-property Multi-objective Molecule Optimization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vishal Dey",
      "Xiao Hu",
      "Xia Ning"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1146": {
    "title": "Measuring Lexical Diversity of Synthetic Data Generated through Fine-Grained Persona Prompting",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gauri Kambhatla",
      "Chantal Shaib",
      "Venkata S Govindarajan"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1147": {
    "title": "Beyond Function-Level Search: Repository-Aware Dual-Encoder Code Retrieval with Adversarial Verification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aofan Liu",
      "Song Shiyuan",
      "Haoxuan Li",
      "Cehao Yang",
      "Yiyan Qi"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1148": {
    "title": "Watermark under Fire: A Robustness Evaluation of LLM Watermarking",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiacheng Liang",
      "Zian Wang",
      "Spencer Hong",
      "Shouling Ji",
      "Ting Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1149": {
    "title": "PEPE: Long-context Extension for Large Language Models via Periodic Extrapolation Positional Encodings",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jikun Hu",
      "Dongsheng Guo",
      "Yuli Liu",
      "Qingyao Ai",
      "Lixuan Wang",
      "Xuebing Sun",
      "Qilei Zhang",
      "Quan Zhou",
      "Cheng Luo"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1150": {
    "title": "Beyond Self-Reports: Multi-Observer Agents for Personality Assessment in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yin Jou Huang",
      "Rafik Hadfi"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1151": {
    "title": "Controlled Retrieval-augmented Context Evaluation for Long-form RAG",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jia-Huei Ju",
      "Suzan Verberne",
      "Maarten de Rijke",
      "Andrew Yates"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1152": {
    "title": "Humanity's Last Code Exam: Can Advanced LLMs Conquer Human's Hardest Code Competition?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangyang Li",
      "Xiaopeng Li",
      "Kuicai Dong",
      "Zhangquanhu",
      "Rongju Ruan",
      "Xinyi Dai",
      "Yasheng Wang",
      "Ruiming Tang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1153": {
    "title": "False Friends Are Not Foes: Investigating Vocabulary Overlap in Multilingual Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julie Kallini",
      "Dan Jurafsky",
      "Christopher Potts",
      "Martijn Bartelds"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1154": {
    "title": "Rule-Guided Extraction: A Hierarchical Rule Optimization Framework for Document-Level Event Argument Extraction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Zuo",
      "Yuxiao Fei",
      "Wanting Ning",
      "Jiayi Huang",
      "Yubo Feng",
      "Lishuang Li"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1155": {
    "title": "SOPL: A Sequential Optimal Learning Approach to Automated Prompt Engineering in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuyang Wang",
      "Somayeh Moazeni",
      "Diego Klabjan"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1156": {
    "title": "CLIP-UP: A Simple and Efficient Mixture-of-Experts CLIP Training Recipe with Sparse Upcycling",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinze Wang",
      "Chen Chen",
      "Yinfei Yang",
      "Hong-You Chen",
      "Bowen Zhang",
      "Aditya Pal",
      "Xiangxin Zhu",
      "Xianzhi Du"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1157": {
    "title": "A Category-Theoretic Approach to Neural-Symbolic Task Planning with Bidirectional Search",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuhui Qu",
      "Jie Wang",
      "Kincho Law"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1158": {
    "title": "HEAL: An Empirical Study on Hallucinations in Embodied Agents Driven by Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Trishna Chakraborty",
      "Udita Ghosh",
      "Xiaopan Zhang",
      "Fahim Faisal Niloy",
      "Yue Dong",
      "Jiachen Li",
      "Amit Roy-Chowdhury",
      "Chengyu Song"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1159": {
    "title": "Can LLMs Judge Debates? Evaluating Non-Linear Reasoning via Argumentation Theory Semantics",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reza Sanayei",
      "Srdjan Vesic",
      "Eduardo Blanco",
      "Mihai Surdeanu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1160": {
    "title": "How Jailbreak Defenses Work and Ensemble? A Mechanistic Investigation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuohan Long",
      "Siyuan Wang",
      "Shujun Liu",
      "Yuhang Lai"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1161": {
    "title": "Visual Self-Refinement for Autoregressive Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiamian Wang",
      "Ziqi Zhou",
      "Chaithanya Kumar Mummadi",
      "Sohail Dianat",
      "Majid Rabbani",
      "Raghuveer Rao",
      "Chen Qiu",
      "Zhiqiang Tao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1162": {
    "title": "Retrieval-Augmented Language Models are Mimetic Theorem Provers",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjie Yang",
      "Ruiyuan Huang",
      "Jiaxing Guo",
      "Zicheng Lyu",
      "Tongshan Xu",
      "Shengzhong Zhang",
      "Lun Du",
      "Da Zheng",
      "Zengfeng Huang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1163": {
    "title": "LORE: Continual Logit Rewriting Fosters Faithful Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Charles Yu",
      "Qingyun Wang",
      "Yuting Hu",
      "Jinjun Xiong",
      "Heng Ji"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1164": {
    "title": "PRINCIPLES: Synthetic Strategy Memory for Proactive Dialogue Agents",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Namyoung Kim",
      "Kai Tzu-iunn Ong",
      "Yeonjun Hwang",
      "Minseok Kang",
      "Iiseo Jihn",
      "Gayoung Kim",
      "Minju Kim",
      "Jinyoung Yeo"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1165": {
    "title": "SLM-Bench: A Comprehensive Benchmark of Small Language Models on Environmental Impacts",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nghiem Thanh Pham",
      "Tung Kieu",
      "Duc Manh Nguyen",
      "Son Ha Xuan",
      "Nghia Duong-Trung",
      "Danh Le-Phuoc"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1166": {
    "title": "A Decoupled Multi-Agent Framework for Complex Text Style Transfer",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingxi Zhang",
      "Yu-Neng Chuang",
      "Guanchu Wang",
      "Ruixiang Tang",
      "Xuanting Cai",
      "Rajesh Shenoy",
      "Xia Hu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1167": {
    "title": "Mamba Drafters for Speculative Decoding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daewon Choi",
      "Seunghyuk Oh",
      "Saket Dingliwal",
      "Jihoon Tack",
      "Kyuyoung Kim",
      "Woomin Song",
      "Seojin Kim",
      "Insu Han",
      "Jinwoo Shin",
      "Aram Galstyan",
      "Shubham Katiyar",
      "Sravan Babu Bodapati"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1168": {
    "title": "LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via a Hybrid Architecture",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xidong Wang",
      "Dingjie Song",
      "Shunian Chen",
      "Junying Chen",
      "Zhenyang Cai",
      "Chen Zhang",
      "Lichao Sun",
      "Benyou Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1169": {
    "title": "Think Clearly: Improving Reasoning via Redundant Token Pruning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daewon Choi",
      "Jimin Lee",
      "Jihoon Tack",
      "Woomin Song",
      "Saket Dingliwal",
      "Sai Muralidhar Jayanthi",
      "Bhavana Ganesh",
      "Jinwoo Shin",
      "Aram Galstyan",
      "Sravan Babu Bodapati"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1170": {
    "title": "A Systematic Survey of Claim Verification: Corpora, Systems, and Case Studies",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaxi Zerong",
      "Chenxi Li",
      "Xinyi Liu",
      "Ju-hui Chen",
      "Fei Xia"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1171": {
    "title": "Automated Creativity Evaluation for Large Language Models: A Reference-Based Approach",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruizhe Li",
      "Chiwei Zhu",
      "Benfeng Xu",
      "Xiaorui Wang",
      "Zhendong Mao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1172": {
    "title": "LangProBe: a Language Program Benchmark",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shangyin Tan",
      "Lakshya A Agrawal",
      "Arnav Singhvi",
      "Liheng Lai",
      "Michael J Ryan",
      "Dan Klein",
      "Omar Khattab",
      "Koushik Sen",
      "Matei Zaharia"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1173": {
    "title": "Exploring and Detecting Self-disclosure in Multi-modal posts on Chinese Social Media",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingbao Luo",
      "Ming Liu",
      "Aoli Huo",
      "Fujing Hu",
      "Gang Li",
      "Wupeng Njust"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1174": {
    "title": "MV-CLAM: Multi-View Molecular Interpretation with Cross-Modal Projection via Language Model",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sumin Ha",
      "Jun Hyeong Kim",
      "Yinhua Piao",
      "Changyun Cho",
      "Sun Kim"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1175": {
    "title": "Mind the Style Gap: Meta-Evaluation of Style and Attribute Transfer Metrics",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amalie Brogaard Pauli",
      "Isabelle Augenstein",
      "Ira Assent"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1176": {
    "title": "ExtremeAIGC: Benchmarking LMM Vulnerability to AI-Generated Extremist Content",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bhavik Chandna",
      "Mariam Aboujenane",
      "Usman Naseem"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1177": {
    "title": "Data Augmentation for Maltese NLP using Transliterated and Machine Translated Arabic Data",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kurt Micallef",
      "Nizar Habash",
      "Claudia Borg"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1178": {
    "title": "Do LLMs Align Human Values Regarding Social Biases? Judging and Explaining Social Biases with LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Liu",
      "Chenhui Chu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1179": {
    "title": "CoEx – Co-evolving World-model and Exploration",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minsoo Kim",
      "Seung-won Hwang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1180": {
    "title": "BrainLoc: Brain Signal-Based Object Detection with Multi-modal Alignment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi Duan",
      "Xiaoda Yang",
      "Kaixuan Luan",
      "Hongshun Qiu",
      "Weicai Yan",
      "Xueyi Zhang",
      "Youliang Zhang",
      "Zhaoyang Li",
      "Donglin Huang",
      "JunYu Lu",
      "Ziyue Jiang",
      "Xifeng Yang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1181": {
    "title": "PVTNL: Prompting Vision Transformers with Natural Language for Generalizable Person Re-identification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wangning",
      "Lei Xie",
      "Sanglu Lu",
      "Shiwei Gan"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1182": {
    "title": "RingFormer: Rethinking Recurrent Transformer with Adaptive Level Signals",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaemu Heo",
      "Eldor Fozilov",
      "Hyunmin Song",
      "Taehwan Kim"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1183": {
    "title": "TriSPrompt: A Hierarchical Soft Prompt Model for Multimodal Rumor Detection with Incomplete Modalities",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiajun Chen",
      "Yangyang Wu",
      "Xiaoye Miao",
      "Mengying Zhu",
      "Meng Xi"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1184": {
    "title": "Evaluating Uncertainty Quantification Methods in Argumentative Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kevin Zhou",
      "Adam Dejl",
      "Gabriel Freedman",
      "Lihu Chen",
      "Antonio Rago",
      "Francesca Toni"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1185": {
    "title": "CLAIMCHECK: How Grounded are LLM Critiques of Scientific Papers?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiefu Ou",
      "William Gantt Walden",
      "Kate Sanders",
      "Zhengping Jiang",
      "Kaiser Sun",
      "Jeffrey Cheng",
      "William Jurayj",
      "Miriam Wanner",
      "Shaobo Liang",
      "Candice Morgan",
      "Seunghoon Han",
      "Weiqi Wang",
      "Chandler May",
      "Hannah Recknor",
      "Daniel Khashabi",
      "Benjamin Van Durme"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1186": {
    "title": "From Noise to Clarity: Filtering Real and LLM-Generated Samples for Enhanced Intent Detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junbao Huang",
      "Weizhen Li",
      "Peijie Huang",
      "Yuhong Xu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1187": {
    "title": "Improving Language Model Personas via Rationalization with Psychological Scaffolds",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brihi Joshi",
      "Xiang Ren",
      "Swabha Swayamdipta",
      "Rik Koncel-Kedziorski",
      "Tim Paek"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1188": {
    "title": "KBM: Delineating Knowledge Boundary for Adaptive Retrieval in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Zhang",
      "Xinyu Wang",
      "Yong Jiang",
      "Zile Qiao",
      "Zhuo Chen",
      "Guangyu Li",
      "Feiteng Mu",
      "Mengting Hu",
      "Pengjun Xie",
      "Fei Huang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1189": {
    "title": "TABARD: A Novel Benchmark for Tabular Anomaly Analysis, Reasoning and Detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manan Roy Choudhury",
      "Anirudh Iyengar Kaniyar Narayana Iyengar",
      "Shikhhar Siingh",
      "Sugeeth Puranam",
      "Vivek Gupta"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1190": {
    "title": "Aspect-based Sentiment Analysis via Synthetic Image Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ge Chen",
      "Zhongqing Wang",
      "Guodong Zhou"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1191": {
    "title": "IntrEx: A Dataset for Modeling Engagement in Educational Conversations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingwei Tan",
      "Mahathi Parvatham",
      "Chiara Gambi",
      "Gabriele Pergola"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1192": {
    "title": "Bridging the Capability Gap: Joint Alignment Tuning for Harmonizing LLM-based Multi-Agent Systems",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minghang Zhu",
      "Zhengliang Shi",
      "Zhiwei Xu",
      "Shiguang Wu",
      "Lingjie Wang",
      "Pengjie Ren",
      "Zhaochun Ren",
      "Zhumin Chen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1193": {
    "title": "Safety Through Reasoning: An Empirical Study of Reasoning Guardrail Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Makesh Narsimhan Sreedhar",
      "Traian Rebedea",
      "Christopher Parisien"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1194": {
    "title": "Context-Aware Reasoning On Parametric Knowledge for Inferring Causal Variables",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ivaxi Sheth",
      "Sahar Abdelnabi",
      "Mario Fritz"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1195": {
    "title": "LoRE-Merging: Exploring Low-Rank Estimation For Large Language Model Merging",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zehua Liu",
      "Han Wu",
      "Yuxuan Yao",
      "Xiaojin Fu",
      "Ruifeng She",
      "Xiongwei Han",
      "Tao Zhong",
      "Mingxuan Yuan"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1196": {
    "title": "Benchmarking Foundation Models with Retrieval-Augmented Generation in Olympic-Level Physics Problem Solving",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shunfeng Zheng",
      "Yudi Zhang",
      "Meng Fang",
      "Zihan Zhang",
      "Zhitan Wu",
      "Mykola Pechenizkiy",
      "Ling Chen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1197": {
    "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive Latency Reduction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akriti Jain",
      "Saransh Sharma",
      "Koyel Mukherjee",
      "Soumyabrata Pal"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1198": {
    "title": "PolitiSky24: U.S. Political Bluesky Dataset with User Stance Labels",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peyman Rostami",
      "Vahid Rahimzadeh",
      "Ali Adibi",
      "Azadeh Shakery"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1199": {
    "title": "From Ground Trust to Truth: Disparities in Offensive Language Judgments on Contemporary Korean Political Discourse",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seunguk Yu",
      "JungMin Yun",
      "Jinhee Jang",
      "YoungBin Kim"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1200": {
    "title": "Misalignment Attack on Text-to-Image Models via Text Embedding Optimization and Inversion",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhijie Du",
      "Daizong Liu",
      "Pan Zhou"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1201": {
    "title": "Domain Pre-training Impact on Representations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cesar Gonzalez-Gutierrez",
      "Ariadna Quattoni"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1202": {
    "title": "KoACD: The First Korean Adolescent Dataset for Cognitive Distortion Analysis via Role-Switching Multi-LLM Negotiation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Seo Kim",
      "Hye Hyeon Kim"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1203": {
    "title": "Refined Assessment for Translation Evaluation: Rethinking Machine Translation Evaluation in the Era of Human-Level Systems",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dmitry Popov",
      "Vladislav Negodin",
      "Ekaterina Enikeeva",
      "Iana Matrosova",
      "Nikolay Karpachev",
      "Max Ryabinin"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1204": {
    "title": "Pre-Storage Reasoning for Episodic Memory: Shifting Inference Burden to Memory for Personalized Dialogue",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sangyeop Kim",
      "Yohan Lee",
      "Sanghwa Kim",
      "Hyunjong Kim",
      "Sungzoon Cho"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1205": {
    "title": "Temporal Consistency for LLM Reasoning Process Error Identification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiacheng Guo",
      "Yue Wu",
      "Jiahao Qiu",
      "Kaixuan Huang",
      "Xinzhe Juan",
      "Ling Yang",
      "Mengdi Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1206": {
    "title": "Quantifying Compositionality of Classic and State-of-the-Art Embeddings",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhijin Guo",
      "Chenhao Xue",
      "Zhaozhen Xu",
      "Hongbo Bo",
      "Yuxuan Ye",
      "Janet B. Pierrehumbert",
      "Martha Lewis"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1207": {
    "title": "Presumed Cultural Identity: How Names Shape LLM Responses",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siddhesh Milind Pawar",
      "Arnav Arora",
      "Lucie-Aimée Kaffee",
      "Isabelle Augenstein"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1208": {
    "title": "I-GUARD: Interpretability-Guided Parameter Optimization for Adversarial Defense",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mamta Mamta",
      "Oana Cocarascu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1209": {
    "title": "DecoupledESC: Enhancing Emotional Support Generation via Strategy-Response Decoupled Preference Optimization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Zhang",
      "Xin Shi",
      "Xueqiao Zhang",
      "Yifan Zhu",
      "Yi Yang",
      "Yawei Luo"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1210": {
    "title": "Local Normalization Distortion and the Thermodynamic Formalism of Decoding Strategies for Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tom Kempton",
      "Stuart Burrell"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1211": {
    "title": "BRIT: Bidirectional Retrieval over Unified Image-Text Graph",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ainulla Khan",
      "Moyuru Yamada",
      "Srinidhi Akella"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1212": {
    "title": "ReTAG: Retrieval-Enhanced, Topic-Augmented Graph-Based Global Sensemaking",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boyoung Kim",
      "Dosung Lee",
      "Sumin An",
      "Jinseong Jeong",
      "Paul Hongsuck Seo"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1213": {
    "title": "Capturing Latent Modal Association For Multimodal Entity Alignment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongquan Ji",
      "Jingwei Cheng",
      "Fu Zhang",
      "Chenglong Lu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1214": {
    "title": "Explaining novel senses using definition generation with open language models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mariia Fedorova",
      "Andrey Kutuzov",
      "Francesco Periti",
      "Yves Scherrer"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1215": {
    "title": "Can Code-Switched Texts Activate a Knowledge Switch in LLMs? A Case Study on English-Korean Code-Switching",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seoyeon Kim",
      "Huiseo Kim",
      "Chanjun Park",
      "Jinyoung Yeo",
      "Dongha Lee"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1216": {
    "title": "Compositional Translation: A Novel LLM-based Approach for Low-resource Machine Translation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Armel Randy Zebaze",
      "Benoît Sagot",
      "Rachel Bawden"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1217": {
    "title": "TopXGen: Topic-Diverse Parallel Data Generation for Low-Resource Machine Translation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Armel Randy Zebaze",
      "Benoît Sagot",
      "Rachel Bawden"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1218": {
    "title": "Fast, Not Fancy: Rethinking G2P with Rich Data and Statistical Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mahta Fetrat Qharabagh",
      "Zahra Dehghanian",
      "Hamid R. Rabiee"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1219": {
    "title": "Personalized open world plan generation for safety-critical human centered autonomous systems: A case study on Artificial Pancreas",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ayan Banerjee",
      "Sandeep Gupta"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1220": {
    "title": "CaMMT: Benchmarking Culturally Aware Multimodal Machine Translation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emilio Villa-Cueva",
      "Sholpan Bolatzhanova",
      "Diana Turmakhan",
      "Kareem Elzeky",
      "Henok Biadglign Ademtew",
      "Alham Fikri Aji",
      "Vladimir Araujo",
      "Israel Abebe Azime",
      "Jinheon Baek",
      "Frederico Belcavello",
      "Fermin Cristobal",
      "Jan Christian Blaise Cruz",
      "Mary Dabre",
      "Raj Dabre",
      "Toqeer Ehsan",
      "Naome A Etori",
      "Fauzan Farooqui",
      "Jiahui Geng",
      "Guido Ivetta",
      "Thanmay Jayakumar",
      "Soyeong Jeong",
      "Zheng Wei Lim",
      "Aishik Mandal",
      "Sofía Martinelli",
      "Mihail Minkov Mihaylov",
      "Daniil Orel",
      "Aniket Pramanick",
      "Sukannya Purkayastha",
      "Israfel Salazar",
      "Haiyue Song",
      "Tiago Timponi Torrent",
      "Debela Desalegn Yadeta",
      "Injy Hamed",
      "Atnafu Lambebo Tonja",
      "Thamar Solorio"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1221": {
    "title": "Training Text-to-Molecule Models with Context-Aware Tokenization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seojin Kim",
      "Hyeontae Song",
      "Jaehyun Nam",
      "Jinwoo Shin"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1222": {
    "title": "Challenging the Evaluator: LLM Sycophancy Under User Rebuttal",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sung Won Kim",
      "Daniel Khashabi"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1223": {
    "title": "Perspective-driven Preference Optimization with Entropy Maximization for Diverse Argument Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilin Cao",
      "Ruike Zhang",
      "Penghui Wei",
      "Qingchao Kong",
      "Wenji Mao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1224": {
    "title": "Spoken Document Retrieval for an Unwritten Language: A Case Study on Gormati",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanjay Booshanam",
      "Kelly Chen",
      "Ondrej Klejch",
      "Thomas Reitmaier",
      "Dani Kalarikalayil Raju",
      "Electra Wallington",
      "Nina Markl",
      "Jennifer Pearson",
      "Matt Jones",
      "Simon Robinson",
      "Peter Bell"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1225": {
    "title": "M-Help: Using Social Media Data to Detect Mental Health Help-Seeking Signals",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Msvpj Sathvik",
      "Zuhair Hasan Shaik",
      "Vivek Gupta"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1226": {
    "title": "Brittle Minds, Fixable Activations: Understanding Belief Representations in Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matteo Bortoletto",
      "Constantin Ruhdorfer",
      "Lei Shi",
      "Andreas Bulling"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1227": {
    "title": "Golden Touchstone: A Comprehensive Bilingual Benchmark for Evaluating Financial Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaojun Wu",
      "Junxi Liu",
      "Huan-Yi Su",
      "Zhouchi Lin",
      "Yiyan Qi",
      "Chengjin Xu",
      "Jiajun Su",
      "Jiajie Zhong",
      "Fuwei Wang",
      "Saizhuo Wang",
      "Fengrui Hua",
      "Jia Li",
      "Jian Guo"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1228": {
    "title": "Quantifying the Risks of LLM- and Tool-assisted Rephrasing to Linguistic Diversity",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengying Wang",
      "Andreas Spitz"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1229": {
    "title": "NUMINA: A Natural Understanding Benchmark for Multi-dimensional Intelligence and Numerical Reasoning Abilities",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changyu Zeng",
      "Yifan Wang",
      "Zimu Wang",
      "Wei Wang",
      "Zhengni Yang",
      "Muyi Bao",
      "Jimin Xiao",
      "Anh Nguyen",
      "Yutao Yue"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1230": {
    "title": "MoMentS: A Comprehensive Multimodal Benchmark for Theory of Mind",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emilio Villa-Cueva",
      "S M Masrur Ahmed",
      "Rendi Chevi",
      "Jan Christian Blaise Cruz",
      "Kareem Elzeky",
      "Fermin Cristobal",
      "Alham Fikri Aji",
      "Skyler Wang",
      "Rada Mihalcea",
      "Thamar Solorio"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1231": {
    "title": "Code Like Humans: A Multi-Agent Solution for Medical Coding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andreas Geert Motzfeldt",
      "Joakim Edin",
      "Casper L. Christensen",
      "Christian Hardmeier",
      "Lars Maaløe",
      "Anna Rogers"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1232": {
    "title": "Can Out-of-Distribution Evaluations Uncover Reliance on Prediction Shortcuts? A Case Study in Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michal Štefánik",
      "Timothee Mickus",
      "Michal Spiegel",
      "Marek Kadlčík",
      "Josef Kuchař"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1233": {
    "title": "MEXA: Towards General Multimodal Reasoning with Dynamic Multi-Expert Aggregation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shoubin Yu",
      "Yue Zhang",
      "Ziyang Wang",
      "Jaehong Yoon",
      "Mohit Bansal"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1234": {
    "title": "Lifelong Knowledge Editing requires Better Regularization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akshat Gupta",
      "Phudish Prateepamornkul",
      "Maochuan Lu",
      "Ahmed Alaa",
      "Thomas Hartvigsen",
      "Gopala Anumanchipalli"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1235": {
    "title": "Lost in Embeddings: Information Loss in Vision–Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenyan Li",
      "Raphael Tang",
      "Chengzu Li",
      "Caiqi Zhang",
      "Ivan Vulić",
      "Anders Søgaard"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1236": {
    "title": "Assessing the Role of Data Quality in Training Bilingual Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Skyler Seto",
      "Maartje Ter Hoeve",
      "Maureen de Seyssel",
      "David Grangier"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1237": {
    "title": "DORM: Preference Data Weights Optimization for Reward Modeling in LLM Alignment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rongzhi Zhang",
      "Chenwei Zhang",
      "Xinyang Zhang",
      "Liang Qiu",
      "Haoming Jiang",
      "Yuchen Zhuang",
      "Qingru Zhang",
      "Hyokun Yun",
      "Xian Li",
      "Bing Yin",
      "Tuo Zhao",
      "Chao Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1238": {
    "title": "Enhancing Domain-Specific Encoder Models with LLM-Generated Data: How to Leverage Ontologies, and How to Do Without Them",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marc Felix Brinner",
      "Tarek Al Mustafa",
      "Sina Zarrieß"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1239": {
    "title": "Aligning Dialogue Agents with Global Feedback via Large Language Model Multimodal Reward Decomposition",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong Won Lee",
      "Hae Won Park",
      "Cynthia Breazeal",
      "Louis-Philippe Morency"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1240": {
    "title": "UrduFactCheck: An Agentic Fact-Checking Framework for Urdu with Evidence Boosting and Benchmarking",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sarfraz Ahmad",
      "Hasan Iqbal",
      "Momina Ahsan",
      "Numaan Naeem",
      "Muhammad Ahsan Riaz Khan",
      "Arham Riaz",
      "Muhammad Arslan Manzoor",
      "Yuxia Wang",
      "Preslav Nakov"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1241": {
    "title": "Echoes of Agreement: Argument Driven Sycophancy in Large Language models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Avneet Kaur"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1242": {
    "title": "Rethinking NLP for Chemistry: A Critical Look at the USPTO Benchmark",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Derin Ozer",
      "Nicolas Gutowski",
      "Benoit Da Mota",
      "Thomas Cauchy",
      "Sylvain Lamprier"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1243": {
    "title": "Investigating Dictionary Expansion for Video-based Sign Language Dictionaries",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aashaka Desai",
      "Daniela Massiceti",
      "Richard Ladner",
      "Hal Daumé Iii",
      "Danielle Bragg",
      "Alex Xijie Lu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1244": {
    "title": "From Insight to Exploit: Leveraging LLM Collaboration for Adaptive Adversarial Text Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Najrin Sultana",
      "Md Rafi Ur Rashid",
      "Kang Gu",
      "Shagufta Mehnaz"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1245": {
    "title": "Beyond Contrastive Learning: Synthetic Data Enables List-wise Training with Multiple Levels of Relevance",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reza Esfandiarpoor",
      "George Zerveas",
      "Ruochen Zhang",
      "Macton Mgonzo",
      "Carsten Eickhoff",
      "Stephen Bach"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1246": {
    "title": "Instability in Downstream Task Performance During LLM Pretraining",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuto Nishida",
      "Masaru Isonuma",
      "Yusuke Oda"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1247": {
    "title": "A Comparison of Independent and Joint Fine-tuning Strategies for Retrieval-Augmented Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Neal Gregory Lawton",
      "Alfy Samuel",
      "Anoop Kumar",
      "Daben Liu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1248": {
    "title": "mrCAD: Multimodal Communication to Refine Computer-aided Designs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "William P McCarthy",
      "Saujas Vaduguru",
      "Karl D.d. Willis",
      "Justin Matejka",
      "Judith E Fan",
      "Daniel Fried",
      "Yewen Pu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1249": {
    "title": "MOCHA: Are Code Language Models Robust Against Multi-Turn Malicious Coding Prompts?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muntasir Wahed",
      "Xiaona Zhou",
      "Kiet A. Nguyen",
      "Tianjiao Yu",
      "Nirav Diwan",
      "Gang Wang",
      "Dilek Hakkani-Tür",
      "Ismini Lourentzou"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1250": {
    "title": "How Can Input Reformulation Improve Tool Usage Accuracy in a Complex Dynamic Environment? A Study on tau-bench",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Venkatesh Mishra",
      "Amir Saeidi",
      "Satyam Raj",
      "Mutsumi Nakamura",
      "Gaowen Liu",
      "Ali Payani",
      "Jayanth Srinivasa",
      "Chitta Baral"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1251": {
    "title": "Evaluating Fairness in Large Vision-Language Models Across Diverse Demographic Attributes and Prompts",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuyang Wu",
      "Yuan Wang",
      "Hsin-Tai Wu",
      "Zhiqiang Tao",
      "Yi Fang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1252": {
    "title": "VIBE: Can a VLM Read the Room?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tania Chakraborty",
      "Eylon Caplan",
      "Dan Goldwasser"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1253": {
    "title": "LoRATK: LoRA Once, Backdoor Everywhere in the Share-and-Play Ecosystem",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyi Liu",
      "Shaochen Zhong",
      "Xintong Sun",
      "Minghao Tian",
      "Mohsen Hariri",
      "Zirui Liu",
      "Ruixiang Tang",
      "Zhimeng Jiang",
      "Jiayi Yuan",
      "Yu-Neng Chuang",
      "Li Li",
      "Soo-Hyun Choi",
      "Rui Chen",
      "Vipin Chaudhary",
      "Xia Hu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1254": {
    "title": "Pearl: A Multimodal Culturally-Aware Arabic Instruction Dataset",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fakhraddin Alwajih",
      "Samar M. Magdy",
      "Abdellah El Mekki",
      "Omer Nacar",
      "Youssef Nafea",
      "Safaa Taher Abdelfadil",
      "Abdulfattah Mohammed Yahya",
      "Hamzah Luqman",
      "Nada Almarwani",
      "Samah Aloufi",
      "Baraah Qawasmeh",
      "Houdaifa Atou",
      "Serry Sibaee",
      "Hamzah A. Alsayadi",
      "Walid Al-Dhabyani",
      "Maged S. Al-shaibani",
      "Aya El aatar",
      "Nour Qandos",
      "Rahaf Alhamouri",
      "Samar Ahmad",
      "Mohammed Anwar AL-Ghrawi",
      "Aminetou Yacoub",
      "Ruwa AbuHweidi",
      "Vatimetou Mohamed Lemin",
      "Reem Abdel-Salam",
      "Ahlam Bashiti",
      "Adel Ammar",
      "Aisha Alansari",
      "Ahmed Ashraf",
      "Nora Alturayeif",
      "Alcides Alcoba Inciarte",
      "AbdelRahim A. Elmadany",
      "Mohamedou Cheikh Tourad",
      "Ismail Berrada",
      "Mustafa Jarrar",
      "Shady Shehata",
      "Muhammad Abdul-Mageed"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1255": {
    "title": "Protein Large Language Models: A Comprehensive Survey",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yijia Xiao",
      "Wanjia Zhao",
      "Junkai Zhang",
      "Yiqiao Jin",
      "Han Zhang",
      "Zhicheng Ren",
      "Renliang Sun",
      "Haixin Wang",
      "Guancheng Wan",
      "Pan Lu",
      "Xiao Luo",
      "Yu Zhang",
      "James Zou",
      "Yizhou Sun",
      "Wei Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1256": {
    "title": "MAKIEval: A Multilingual Automatic WiKidata-based Framework for Cultural Awareness Evaluation for LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raoyuan Zhao",
      "Beiduo Chen",
      "Barbara Plank",
      "Michael A. Hedderich"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1257": {
    "title": "Looking Beyond the Pixels: Evaluating Visual Metaphor Understanding in VLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manishit Kundu",
      "Sumit Shekhar",
      "Pushpak Bhattacharyya"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1258": {
    "title": "AGENTVIGIL: Automatic Black-Box Red-teaming for Indirect Prompt Injection against LLM Agents",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhun Wang",
      "Vincent Siu",
      "Zhe Ye",
      "Tianneng Shi",
      "Yuzhou Nie",
      "Xuandong Zhao",
      "Chenguang Wang",
      "Wenbo Guo",
      "Dawn Song"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1259": {
    "title": "Improving LLM-as-a-Judge Inference with the Judgment Distribution",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Victor Wang",
      "Michael JQ Zhang",
      "Eunsol Choi"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1260": {
    "title": "Learning Is Not A Race: Improving Retrieval in Language Models via Equal Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wanqian Yang",
      "Aahlad Manas Puli",
      "Rajesh Ranganath"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1261": {
    "title": "The Prompt Makes the Person(a): A Systematic Evaluation of Sociodemographic Persona Prompting for Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marlene Lutz",
      "Indira Sen",
      "Georg Ahnert",
      "Elisa Rogers",
      "Markus Strohmaier"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1262": {
    "title": "Spiral of Silence in Large Language Model Agents",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingze Zhong",
      "Meng Fang",
      "Zijing Shi",
      "Yuxuan Huang",
      "Shunfeng Zheng",
      "Yali Du",
      "Ling Chen",
      "Jun Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1263": {
    "title": "Do We Know What LLMs Don't Know? A Study of Consistency in Knowledge Probing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raoyuan Zhao",
      "Abdullatif Köksal",
      "Ali Modarressi",
      "Michael A. Hedderich",
      "Hinrich Schuetze"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1264": {
    "title": "Context Length Alone Hurts LLM Performance Despite Perfect Retrieval",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufeng Du",
      "Minyang Tian",
      "Srikanth Ronanki",
      "Subendhu Rongali",
      "Sravan Babu Bodapati",
      "Aram Galstyan",
      "Azton Wells",
      "Roy Schwartz",
      "Eliu A Huerta",
      "Hao Peng"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1265": {
    "title": "DebUnc: Improving Large Language Model Agent Communication With Uncertainty Metrics",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luke Yoffe",
      "Alfonso Amayuelas",
      "William Yang Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1266": {
    "title": "ProcVQA: Benchmarking the Effects of Structural Properties in Mined Process Visualizations on Vision–Language Model Performance",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kazi Tasnim Zinat",
      "Saad Mohammad Abrar",
      "Shoumik Saha",
      "Sharmila Duppala",
      "Saimadhav Naga Sakhamuri",
      "Zhicheng Liu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1267": {
    "title": "Probing Political Ideology in Large Language Models: How Latent Political Representations Generalize Across Tasks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyi Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1268": {
    "title": "Understanding GUI Agent Localization Biases through Logit Sharpness",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingjian Tao",
      "Yiwei Wang",
      "Yujun Cai",
      "Zhicheng Yang",
      "Jing Tang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1269": {
    "title": "The Language of Interoception: Examining Embodiment and Emotion Through a Corpus of Body Part Mentions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sophie Wu",
      "Jan Philip Wahle",
      "Saif M. Mohammad"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1270": {
    "title": "HomoGraphAdapter: A Homogeneous Graph Neural Network as an Effective Adapter for Vision-Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuan He",
      "Zhuozhao Li",
      "Song Guo",
      "Xiaocheng Lu",
      "Jinxiang Lai"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1271": {
    "title": "No Black Boxes: Interpretable and Interactable Predictive Healthcare with Knowledge-Enhanced Agentic Causal Discovery",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoxue Han",
      "Pengfei Hu",
      "Chang Lu",
      "Jun-En Ding",
      "Feng Liu",
      "Yue Ning"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1272": {
    "title": "PROOD: A Simple LLM Out-of-Distribution Guardrail Leveraging Response Semantics",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joshua Tint"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1273": {
    "title": "ICL-Bandit: Relevance Labeling in Advertisement Recommendation Systems via LLM",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lu Wang",
      "Chiming Duan",
      "Pu Zhao",
      "Fangkai Yang",
      "Yong Shi",
      "Xuefeng Luo",
      "Bingjing Xu",
      "Weiwei Deng",
      "Qingwei Lin",
      "Dongmei Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1274": {
    "title": "Intent-aware Schema Generation and Refinement for Literature Review Tables",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vishakh Padmakumar",
      "Joseph Chee Chang",
      "Kyle Lo",
      "Doug Downey",
      "Aakanksha Naik"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1275": {
    "title": "NLP Needs Diversity outside of ‘Diversity",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joshua Tint"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1276": {
    "title": "Anatomy of a Feeling: Narrating Embodied Emotions via Large Vision-Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Saim",
      "Phan Anh Duong",
      "Cat Luong",
      "Aniket Bhanderi",
      "Tianyu Jiang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1277": {
    "title": "Towards Universal Debiasing for Language Models-based Tabular Data Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianchun Li",
      "Tianci Liu",
      "Xingchen Wang",
      "Rongzhe Wei",
      "Pan Li",
      "Lu Su",
      "Jing Gao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1278": {
    "title": "Beyond Linear Steering: Unified Multi-Attribute Control for Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Narmeen Fatimah Oozeer",
      "Luke Marks",
      "Fazl Barez",
      "Amir Abdullah"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1279": {
    "title": "Unequal Scientific Recognition in the Age of LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixuan Liu",
      "Abel Elekes",
      "Jianglin Lu",
      "Rodrigo Dorantes-Gilardi",
      "Albert-Laszlo Barabasi"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1280": {
    "title": "Zero-Shot Fine-Grained Image Classification Using Large Vision-Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md. Atabuzzaman",
      "Andrew Zhang",
      "Chris Thomas"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1281": {
    "title": "Using tournaments to calculate AUROC for zero-shot classification with LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "WonJin Yoon",
      "Ian Bulovic",
      "Timothy A. Miller"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1282": {
    "title": "Exploration-Driven Reinforcement Learning for Expert Routing Improvement in Mixture-of-Experts Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gyunyeop Kim",
      "Sangwoo Kang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1283": {
    "title": "D2CS - Documents Graph Clustering using LLM supervision",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yoel Ashkenazi",
      "Etzion Harari",
      "Regev Yehezkel Imra",
      "Naphtali Abudarham",
      "Dekel Cohen",
      "Yoram Louzoun"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1284": {
    "title": "GeoChain: Multimodal Chain-of-Thought for Geographic Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sahiti Yerramilli",
      "Nilay Pande",
      "Rynaa Grover",
      "Jayant Sravan Tamarapalli"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1285": {
    "title": "SteerVLM: Robust Model Control through Lightweight Activation Steering for Vision Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anushka Sivakumar",
      "Andrew Zhang",
      "Zaber Ibn Abdul Hakim",
      "Chris Thomas"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1286": {
    "title": "FractalLLM: Lossless Self-Speculative Decoding with Layer Embedded Self-Compression",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juhyeong Kim",
      "Sangyeon Yu",
      "Gyunyeop Kim",
      "Sangwoo Kang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1287": {
    "title": "Saten: Sparse Augmented Tensor Networks for Post-Training Compression of Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryan Solgi",
      "Kai Zhen",
      "Rupak Vignesh Swaminathan",
      "Nathan Susanj",
      "Athanasios Mouchtaris",
      "Siegfried Kunzmann",
      "Zheng Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1288": {
    "title": "Third-Person Appraisal Agent: Simulating Human Emotional Reasoning in Text with Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simin Hong",
      "Jun Sun",
      "Hongyang Chen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1289": {
    "title": "Source-primed Multi-turn Conversation Helps Large Language Models Translate Documents",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanxu Hu",
      "Jannis Vamvas",
      "Rico Sennrich"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1290": {
    "title": "Mitigating Spurious Correlations via Counterfactual Contrastive Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fengxiang Cheng",
      "Chuan Zhou",
      "Xiang Li",
      "Alina Leidinger",
      "Haoxuan Li",
      "Mingming Gong",
      "Fenrong Liu",
      "Robert Van Rooij"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1291": {
    "title": "The RAG Paradox: A Black-Box Attack Exploiting Unintentional Vulnerabilities in Retrieval-Augmented Generation Systems",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chanwoo Choi",
      "Jinsoo Kim",
      "Sukmin Cho",
      "Soyeong Jeong",
      "Buru Chang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1292": {
    "title": "Guiding Large Language Models for Biomedical Entity Linking via Restrictive and Contrastive Decoding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenxi Lin",
      "Ziheng Zhang",
      "Jian Wu",
      "Yefeng Zheng",
      "Xian Wu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1293": {
    "title": "Cut the Deadwood Out: Backdoor Purification via Guided Module Substitution",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yao Tong",
      "Weijun Li",
      "Xuanli He",
      "Haolan Zhan",
      "Qiongkai Xu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1294": {
    "title": "RepoDebug: Repository-Level Multi-Task and Multi-Language Debugging Evaluation of Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingjing Liu",
      "Zeming Liu",
      "Zihao Cheng",
      "Mengliang He",
      "Xiaoming Shi",
      "Yuhang Guo",
      "Xiangrong Zhu",
      "Yuanfang Guo",
      "Yunhong Wang",
      "Haifeng Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1295": {
    "title": "FaStFact: Faster, Stronger Long-Form Factuality Evaluations in LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingjia Wan",
      "Haochen Tan",
      "Xiao Zhu",
      "Xinyu Zhou",
      "Zhiwei Li",
      "Qingsong Lv",
      "Changxuan Sun",
      "Jiaqi Zeng",
      "Yi Xu",
      "Jianqiao Lu",
      "Yinhong Liu",
      "Zhijiang Guo"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1296": {
    "title": "PropXplain: Can LLMs Enable Explainable Propaganda Detection?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maram Hasanain",
      "Md Arid Hasan",
      "Mohamed Bayan Kmainasi",
      "Elisa Sartori",
      "Ali Ezzat Shahroor",
      "Giovanni Da San Martino",
      "Firoj Alam"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1297": {
    "title": "EoT: Evolution of Thoughts for Complex Reasoning Tasks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qin Hua",
      "Jiaqi Sun",
      "Shiyou Qian",
      "Dingyu Yang",
      "Jian Cao",
      "Guangtao Xue"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1298": {
    "title": "Reveal and Release: Iterative LLM Unlearning with Self-generated Data",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linxi Xie",
      "Xin Teng",
      "Shichang Ke",
      "Hongyi Wen",
      "Shenji Wan"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1299": {
    "title": "An Evaluation Resource for Grounding Translation Errors",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sujin Chen",
      "Kang Wang",
      "Zixuan Zhou",
      "Xiangyu Duan",
      "Wanqun Zhang",
      "Hao Yang",
      "Jinsong Su",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1300": {
    "title": "Enhancing Time Awareness in Generative Recommendation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sunkyung Lee",
      "Seongmin Park",
      "Jonghyo Kim",
      "Mincheol Yoon",
      "Jongwuk Lee"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1301": {
    "title": "Adaptive LLM Routing under Budget Constraints",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pranoy Panda",
      "Raghav Magazine",
      "Chaitanya Devaguptapu",
      "Sho Takemori",
      "Vishal Sharma"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1302": {
    "title": "Promptception: How Sensitive Are Large Multimodal Models to Prompts?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohamed Insaf Ismithdeen",
      "Muhammad Uzair Khattak",
      "Salman Khan"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1303": {
    "title": "Can Federated Learning Safeguard Private Data in LLM Training? Vulnerabilities, Attacks, and Defense Evaluation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenkai Guo",
      "Xuefeng Liu",
      "Haolin Wang",
      "Jianwei Niu",
      "Shaojie Tang",
      "Jing Yuan"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1304": {
    "title": "Runaway is Ashamed, But Helpful: On the Early-Exit Behavior of Large Language Model-based Agents in Embodied Environments",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingyu Lu",
      "Liang Ding",
      "Siyi Cao",
      "Xuebo Liu",
      "Kanjian Zhang",
      "Jinxia Zhang",
      "Dacheng Tao"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1305": {
    "title": "AutoMIR: Effective Zero-Shot Medical Information Retrieval without Relevance Labels",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Li",
      "Xiangxu Zhang",
      "Xiao Zhou",
      "Zheng Liu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1306": {
    "title": "RG-VQA: Leveraging Retriever-Generator Pipelines for Knowledge Intensive Visual Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Settaluri Lakshmi Sravanthi",
      "Pulkit Agarwal",
      "Debjyoti Mondal",
      "Rituraj Singh",
      "Subhadarshi Panda",
      "Ankit Mishra",
      "Kiran Pradeep",
      "Srihari K B",
      "Godawari Sudhakar Rao",
      "Pushpak Bhattacharyya"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1307": {
    "title": "Enhancing RAG Efficiency with Adaptive Context Compression",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuyu Guo",
      "Shuo Zhang",
      "Zhaochun Ren"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1308": {
    "title": "Revealing the impact of synthetic native samples and multi-tasking strategies in Hindi-English code-mixed humour and sarcasm detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Debajyoti Mazumder",
      "Aakash Kumar",
      "Jasabanta Patro"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1309": {
    "title": "CogAtom: From Cognitive Atoms to Olympiad-level Mathematical Reasoning in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuofan Chen",
      "Jiyuan He",
      "Yichi Zhang",
      "Xing Hu",
      "Haoxing Wen",
      "Jun Bai",
      "Wenge Rong"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1310": {
    "title": "Efficient Latent Semantic Clustering for Scaling Test-Time Computation of LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sungjae Lee",
      "Hoyoung Kim",
      "Jeongyeon Hwang",
      "Eunhyeok Park",
      "Jungseul Ok"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1311": {
    "title": "BannerBench: Benchmarking Vision Language Models for Multi-Ad Selection with Human Preferences",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hiroto Otake",
      "Peinan Zhang",
      "Yusuke Sakai",
      "Masato Mita",
      "Hiroki Ouchi",
      "Taro Watanabe"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1312": {
    "title": "DeKeyNLU: Enhancing Natural Language to SQL Generation through Task Decomposition and Keyword Extraction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Chen",
      "Zhenyan Chen",
      "Xuming Hu",
      "Peilin Zhou",
      "Yining Hua",
      "Han Fang",
      "Cissy Hing Yee Choy",
      "Xinmei Ke",
      "Jingfeng Luo",
      "Zixuan Yuan"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1313": {
    "title": "Facilitating Cross-lingual Transfer of Empathy through Language-independent Latent Diffusion: A Case Study in Chinese",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junlin Li",
      "Peng Bo",
      "Yu-Yin Hsu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1314": {
    "title": "Evaluating Compound AI Systems through Behaviors, Not Benchmarks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pranav Bhagat",
      "K N Ajay Shastry",
      "Pranoy Panda",
      "Chaitanya Devaguptapu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1315": {
    "title": "SciCompanion: Graph-Grounded Reasoning for Structured Evaluation of Scientific Arguments",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joshua Alan Flashner",
      "Adithya Kulkarni",
      "Dawei Zhou"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1316": {
    "title": "From Generation to Detection: A Multimodal Multi-Task Dataset for Benchmarking Health Misinformation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihao Zhang",
      "Yiran Zhang",
      "Xiyue Zhou",
      "Liting Huang",
      "Imran Razzak",
      "Preslav Nakov",
      "Usman Naseem"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1317": {
    "title": "Estimating Machine Translation Difficulty",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lorenzo Proietti",
      "Stefano Perrella",
      "Vilém Zouhar",
      "Roberto Navigli",
      "Tom Kocmi"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1318": {
    "title": "TIU-Bench: A Benchmark for Evaluating Large Multimodal Models on Text-rich Image Understanding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kun Zhang",
      "Liqiang Niu",
      "Zhen Cao",
      "Fandong Meng",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1319": {
    "title": "Breaking Token Into Concepts: Exploring Extreme Compression in Token Representation Via Compositional Shared Semantics",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kavin R V",
      "Pawan Goyal"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1320": {
    "title": "ExeSQL: Self-Taught Text-to-SQL Models with Execution-Driven Bootstrapping for SQL Dialects",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jipeng Zhang",
      "Haolin Yang",
      "Kehao Miao",
      "Ruiyuan Zhang",
      "Renjie Pi",
      "Jiahui Gao",
      "Xiaofang Zhou"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1321": {
    "title": "Under the Shadow of Babel: How Language Shapes Reasoning in LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenxi Wang",
      "Yixuan Zhang",
      "Lang Gao",
      "Zixiang Xu",
      "Zirui Song",
      "Yanbo Wang",
      "Xiuying Chen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1322": {
    "title": "Think Right, Not More: Test-Time Scaling for Numerical Claim Verification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Primakov Chungkham",
      "Venktesh V",
      "Vinay Setty",
      "Avishek Anand"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1323": {
    "title": "Nexus: Adaptive Upcycling to Efficiently Pretrain Mixture of Experts",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikolas Gritsch",
      "Qizhen Zhang",
      "Acyr Locatelli",
      "Sara Hooker",
      "Ahmet Üstün"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1324": {
    "title": "Exploring Context Strategies in LLMs for Discourse-Aware Machine Translation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ritvik Choudhary",
      "Rem Hida",
      "Masaki Hamada",
      "Hayato Futami",
      "Toshiyuki Sekiya"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1325": {
    "title": "Insights into using temporal coordinated behaviour to explore connections between social media posts and influence",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elisa Sartori",
      "Serena Tardelli",
      "Maurizio Tesconi",
      "Mauro Conti",
      "Alessandro Galeazzi",
      "Stefano Cresci",
      "Giovanni Da San Martino"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1326": {
    "title": "SpecCoT: Accelerating Chain-of-Thought Reasoning through Speculative Exploration",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junhan Shi",
      "Yijia Zhu",
      "Zhenning Shi",
      "Dan Zhao",
      "Qing Li",
      "Yong Jiang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1327": {
    "title": "A Similarity Measure for Comparing Conversational Dynamics",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sang Min Jung",
      "Kaixiang Zhang",
      "Cristian Danescu-Niculescu-Mizil"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1328": {
    "title": "AgentDrug: Utilizing Large Language Models in an Agentic Workflow for Zero-Shot Molecular Optimization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Le Huy Khiem",
      "Ting Hua",
      "Nitesh V Chawla"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1329": {
    "title": "Improving Preference Alignment of LLM with Inference-Free Self-Refinement",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fukun Ma",
      "Kaibin Tian",
      "Jieting Xue",
      "Xiaoyi Wang",
      "Ye Ma",
      "Quan Chen",
      "Peng Jiang",
      "Lijie Wen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1330": {
    "title": "Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC Transpilation with Testing Guarantees",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahmed Heakl",
      "Sarim Hashmi",
      "Chaimaa Abi",
      "Celine Lee",
      "Abdulrahman Mahmoud"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1331": {
    "title": "StructuThink: Reasoning with Task Transition Knowledge for Autonomous LLM-Based Agents",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiyu Zhao",
      "Zhenyu Guo",
      "Chunhong Zhang",
      "Ziyu Zhou",
      "Zheng Hu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1332": {
    "title": "Leveraging Unpaired Feedback for Long-Term LLM-based Recommendation Tuning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jizhi Zhang",
      "Chongming Gao",
      "Wentao Shi",
      "Xin Chen",
      "Jingang Wang",
      "Xunliang Cai",
      "Fuli Feng"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1333": {
    "title": "Investigating Multi-layer Representations for Dense Passage Retrieval",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongbin Xie",
      "Thomas Lukasiewicz"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1334": {
    "title": "KELE: Residual Knowledge Erasure for Enhanced Multi-hop Reasoning in Knowledge Editing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengqi Zhang",
      "Bowen Fang",
      "Qiang Liu",
      "Xiaotian Ye",
      "Shu Wu",
      "Pengjie Ren",
      "Zhumin Chen",
      "Liang Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1335": {
    "title": "Dissecting Persona-Driven Reasoning in Language Models via Activation Patching",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ansh Poonia",
      "Maeghal Jain"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1336": {
    "title": "PUER: Boosting Few-shot Positive-Unlabeled Entity Resolution with Reinforcement Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaoshu Wang",
      "Mengyi Yan",
      "Wei Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1337": {
    "title": "Toward the Automatic Detection of Word Meaning Negotiation Indicators in Conversation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aina Garí Soler",
      "Matthieu Labeau",
      "Chloé Clavel"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1338": {
    "title": "Forget the Unneeded: Backdooring Large Language Models via Contrastive-enhanced Machine Unlearning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiji Yang",
      "Shu Zhao",
      "Congyao Mei",
      "Zhen Yang",
      "Jie Chen",
      "Fulan Qian",
      "Zhen Duan",
      "Yanping Zhang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1339": {
    "title": "Equipping Retrieval-Augmented Large Language Models with Document Structure Awareness",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingnan Xu",
      "Chong Feng",
      "Kaiyuan Zhang",
      "Liu Zhengyong",
      "Wenqiang Xu",
      "Fanqing Meng"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1340": {
    "title": "QEVA: A Reference-Free Evaluation Metric for Narrative Video Summarization with Multimodal Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Woojun Jung",
      "Junyeong Kim"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1341": {
    "title": "Thinking Before You Speak: A Proactive Test-time Scaling Approach",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cong Liu",
      "Wenchang Chai",
      "Hejun Wu",
      "Yan Pan",
      "Pengxu Wei",
      "Liang Lin"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1342": {
    "title": "Do Before You Judge: Self-Reference as a Pathway to Better LLM Evaluation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei-Hsiang Lin",
      "Sheng-Lun Wei",
      "Hen-Hsen Huang",
      "Hsin-Hsi Chen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1343": {
    "title": "Beyond Content: How Grammatical Gender Shapes Visual Representation in Text-to-Image Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammed Saeed",
      "Shaina Raza",
      "Ashmal Vayani",
      "Muhammad Abdul-Mageed",
      "Ali Emami",
      "Shady Shehata"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1344": {
    "title": "ToolHaystack: Stress-Testing Tool-Augmented Language Models in Realistic Long-Term Interactions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Beong-woo Kwak",
      "Minju Kim",
      "Dongha Lim",
      "Hyungjoo Chae",
      "Dongjin Kang",
      "Sunghwan Kim",
      "Dongil Yang",
      "Jinyoung Yeo"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1345": {
    "title": "GraphCheck: Multipath Fact-Checking with Entity-Relationship Graphs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyewon Jeon",
      "Jay-Yoon Lee"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1346": {
    "title": "FLAMES: Improving LLM Math Reasoning via a Fine-Grained Analysis of the Data Synthesis Pipeline",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Parker Seegmiller",
      "Kartik Mehta",
      "Soumya Saha",
      "Chenyang Tao",
      "Shereen Oraby",
      "Arpit Gupta",
      "Tagyoung Chung",
      "Mohit Bansal",
      "Nanyun Peng"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1347": {
    "title": "POW: Political Overton Windows of Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leif Azzopardi",
      "Yashar Moshfeghi"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1348": {
    "title": "Columbo: Expanding Abbreviated Column Names for Tabular Data Using Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ting Cai",
      "Stephen Sheen",
      "AnHai Doan"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1349": {
    "title": "RTTC: Reward-Guided Collaborative Test-Time Compute",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juan Pablo Munoz",
      "Jinjie Yuan"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1350": {
    "title": "AMANDA: Agentic Medical Knowledge Augmentation for Data-Efficient Medical Visual Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziqing Wang",
      "Chengsheng Mao",
      "Xiaole Wen",
      "Yuan Luo",
      "Kaize Ding"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1351": {
    "title": "Mixed Signals: Decoding VLMs' Reasoning and Underlying Bias in Vision-Language Conflict",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pouya Pezeshkpour",
      "Moin Aminnaseri",
      "Estevam Hruschka"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1352": {
    "title": "Mitigating Hallucination in Large Vision-Language Models through Aligning Attention Distribution to Information Flow",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianfei Zhao",
      "Feng Zhang",
      "Xin Sun",
      "Chong Feng"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1353": {
    "title": "OptiSeq: Ordering Examples On-The-Fly for In-Context Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rahul Atul Bhope",
      "Praveen Venkateswaran",
      "K. R. Jayaram",
      "Vatche Isahagian",
      "Vinod Muthusamy",
      "Nalini Venkatasubramanian"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1354": {
    "title": "Dependency Parsing-Based Syntactic Enhancement of Relation Extraction in Scientific Texts",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Devvrat Joshi",
      "Islem Rekik"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1355": {
    "title": "DIPLomA: Efficient Adaptation of Instructed LLMs to Low-Resource Languages via Post-Training Delta Merging",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ixak Sarasua Antero",
      "Ander Corral",
      "Xabier Saralegi"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1356": {
    "title": "Reliability Crisis of Reference-free Metrics for Grammatical Error Correction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Takumi Goto",
      "Yusuke Sakai",
      "Taro Watanabe"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1357": {
    "title": "Who Speaks Matters: Analysing the Influence of the Speaker's Linguistic Identity on Hate Classification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ananya Malik",
      "Kartik Sharma",
      "Shaily Bhatt",
      "Lynnette Hui Xian Ng"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1358": {
    "title": "Are LLMs Empathetic to All? Investigating the Influence of Multi-Demographic Personas on a Model's Empathy",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ananya Malik",
      "Nazanin Sabri",
      "Melissa M. Karnaze",
      "Mai ElSherief"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1359": {
    "title": "Active Learning for Multidialectal Arabic POS Tagging",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Diyam Akra",
      "Mohammed Khalilia",
      "Mustafa Jarrar"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1360": {
    "title": "Embedding-Free RAG",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jessica Maghakian",
      "Raunak Sinha",
      "Max Schettewi",
      "Gunkirat Kaur"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1361": {
    "title": "Rating Roulette: Self-Inconsistency in LLM-As-A-Judge Frameworks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rajarshi Haldar",
      "Julia Hockenmaier"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1362": {
    "title": "Quantifying Uncertainty in Natural Language Explanations of Large Language Models for Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangyi Li",
      "Mengdi Huai"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1363": {
    "title": "Real-World Summarization: When Evaluation Reaches Its Limits",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Patrícia Schmidtová",
      "Ondrej Dusek",
      "Saad Mahamood"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1364": {
    "title": "Open-DeBias: Toward Mitigating Open-Set Bias in Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arti Rani",
      "Shweta Singh",
      "Nihar Ranjan Sahoo",
      "Gaurav Kumar Nayak"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1365": {
    "title": "SACL: Understanding and Combating Textual Bias in Code Retrieval with Semantic-Augmented Reranking and Localization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dhruv Gupta",
      "Gayathri Ganesh Lakshmy",
      "Yiqing Xie"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1366": {
    "title": "Jailbreak Distillation: Renewable Safety Benchmarking",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyu Zhang",
      "Ahmed Elgohary",
      "Xiawei Wang",
      "A S M Iftekhar",
      "Ahmed Magooda",
      "Benjamin Van Durme",
      "Daniel Khashabi",
      "Kyle Jackson"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1367": {
    "title": "Uncertainty-Aware Answer Selection for Improved Reasoning in Multi-LLM Systems",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aakriti Agrawal",
      "Rohith Aralikatti",
      "Anirudh Satheesh",
      "Souradip Chakraborty",
      "Amrit Singh Bedi",
      "Furong Huang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1368": {
    "title": "GreekBarBench: A Challenging Benchmark for Free-Text Legal Reasoning and Citations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Odysseas S. Chlapanis",
      "Dimitris Galanis",
      "Nikolaos Aletras",
      "Ion Androutsopoulos"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1369": {
    "title": "Pi-SQL: Enhancing Text-to-SQL with Fine-Grained Guidance from Pivot Programming Languages",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongdong Chi",
      "Hanqing Wang",
      "Yun Chen",
      "Yan Yang",
      "Jian Yang",
      "Zonghan Yang",
      "Xiao Yan",
      "Guanhua Chen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1370": {
    "title": "RAC: Efficient LLM Factuality Correction with Retrieval Augmentation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changmao Li",
      "Jeffrey Flanigan"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1371": {
    "title": "Does It Run and Is That Enough? Revisiting Text-to-Chart Generation with a Multi-Agent Approach",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "James Ford",
      "Anthony Rios"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1372": {
    "title": "GeLoRA: Geometric Adaptive Ranks For Efficient LoRA Fine-tuning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abdessalam Ed-dib",
      "Zhanibek Datbayev",
      "Amine M. Aboussalah"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1373": {
    "title": "Uncovering Scaling Laws for Large Language Models via Inverse Problems",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arun Verma",
      "Zhaoxuan Wu",
      "Zijian Zhou",
      "Xiaoqiang Lin",
      "Zhiliang Chen",
      "Rachael Hwee Ling Sim",
      "Rui Qiao",
      "Jingtan Wang",
      "Nhung Bui",
      "Xinyuan Niu",
      "Wenyang Hu",
      "Gregory Kang Ruey Lau",
      "Zi-Yu Khoo",
      "Zitong Zhao",
      "Xinyi Xu",
      "Apivich Hemachandra",
      "See-Kiong Ng",
      "Bryan Kian Hsiang Low"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1374": {
    "title": "UIPE: Enhancing LLM Unlearning by Removing Knowledge Related to Forgetting Targets",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenyu Wang",
      "Mengqi Zhang",
      "Xiaotian Ye",
      "Zhaochun Ren",
      "Pengjie Ren",
      "Zhumin Chen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1375": {
    "title": "FicSim: A Dataset for Multi-Faceted Semantic Similarity in Long-Form Fiction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Natasha Johnson",
      "Amanda Bertsch",
      "Maria-Emil Deal",
      "Emma Strubell"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1376": {
    "title": "Masked Diffusion Captioning for Visual Feature Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Feng",
      "Zihao Wei",
      "Andrew Owens"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1377": {
    "title": "Diverse Multi-tool Aggregation with Large Language Models for Enhanced Math Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bohan Yao",
      "Vikas Yadav"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1378": {
    "title": "Enhancing Goal-oriented Proactive Dialogue Systems via Dynamic Multi-dimensional Consistency Optimization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Didi Zhang",
      "Yaxin Fan",
      "Peifeng Li",
      "Qiaoming Zhu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1379": {
    "title": "Injecting Domain-Specific Knowledge into Large Language Models: A Comprehensive Survey",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zirui Song",
      "Bin Yan",
      "Yuhan Liu",
      "Miao Fang",
      "Mingzhe Li",
      "Rui Yan",
      "Xiuying Chen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1380": {
    "title": "Who's the Author? How Explanations Impact User Reliance in AI-Assisted Authorship Attribution",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Calvin Bao",
      "Connor Baumler",
      "Hal Daumé Iii",
      "Marine Carpuat"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1381": {
    "title": "UniSpeaker: A Unified Approach for Multimodality-driven Speaker Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengyan Sheng",
      "Zhihao Du",
      "Heng Lu",
      "ShiLiang Zhang",
      "Zhen-Hua Ling"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1382": {
    "title": "On the Fine-Grained Planning Abilities of VLM Web Agents",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Surgan Jandial",
      "Yinong Oliver Wang",
      "Andrea Bajcsy",
      "Fernando De la Torre"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1383": {
    "title": "InterFeedback: Unveiling Interactive Intelligence of Large Multimodal Models with Human Feedback",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Henry Hengyuan Zhao",
      "Wenqi Pei",
      "Yifei Tao",
      "Haiyang Mei",
      "Mike Zheng Shou"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1384": {
    "title": "ReFLAIR: Enhancing Multimodal Reasoning via Structured Reflection and Reward-Guided Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiazhou Ji",
      "Xinru Lu"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1385": {
    "title": "ControlText: Unlocking Controllable Fonts in Multilingual Text Rendering without Font Annotations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowen Jiang",
      "Yuan Yuan",
      "Xinyi Bai",
      "Zhuoqun Hao",
      "Alyson Yin",
      "Yaojie Hu",
      "Wenyu Liao",
      "Lyle Ungar",
      "Camillo Jose Taylor"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1386": {
    "title": "STA-CoT: Structured Target-Centric Agentic Chain-of-Thought for Consistent Multi-Image Geological Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Beibei Yu",
      "Tao Shen",
      "Ling Chen"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1387": {
    "title": "Can Language Models Follow Multiple Turns of Entangled Instructions?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chi Han",
      "Xin Liu",
      "Haodong Wang",
      "Shiyang Li",
      "Jingfeng Yang",
      "Haoming Jiang",
      "Zhengyang Wang",
      "Qingyu Yin",
      "Liang Qiu",
      "Changlong Yu",
      "Yifan Gao",
      "Zheng Li",
      "Bing Yin",
      "Jingbo Shang",
      "Heng Ji"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1388": {
    "title": "How to Generalize the Detection of AI-Generated Text: Confounding Neurons",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Claudio Borile",
      "Carlo Abrate"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1389": {
    "title": "SparsePO: Controlling Preference Alignment of LLMs via Sparse Token Masks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fenia Christopoulou",
      "Ronald Cardenas",
      "Gerasimos Lampouras",
      "Haitham Bou Ammar",
      "Jun Wang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1390": {
    "title": "We Argue to Agree: Towards Personality-Driven Argumentation-Based Negotiation Dialogue Systems for Tourism",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Priyanshu Priya",
      "Saurav Dudhate",
      "Desai Vishesh Yasheshbhai",
      "Asif Ekbal"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1391": {
    "title": "Towards the Roots of the Negation Problem: A Multilingual NLI Dataset and Model Scaling Analysis",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tereza Vrabcová",
      "Marek Kadlčík",
      "Petr Sojka",
      "Michal Štefánik",
      "Michal Spiegel"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1392": {
    "title": "Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sai Ashish Somayajula",
      "Bokai Hu",
      "Qi Cao",
      "Xin Pan",
      "Pengtao Xie"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1393": {
    "title": "HATECAT-TR: A Hate Speech Span Detection and Categorization Dataset for Turkish",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hasan Kerem Şeker",
      "Gökçe Uludoğan",
      "Pelin Önal",
      "Arzucan Özgür"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1394": {
    "title": "DM-Codec: Distilling Multimodal Representations for Speech Tokenization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Mubtasim Ahasan",
      "Md Fahim",
      "Tasnim Mohiuddin",
      "Akmmahbubur Rahman",
      "Aman Chadha",
      "Tariq Iqbal",
      "M Ashraful Amin",
      "Md Mofijul Islam",
      "Amin Ahsan Ali"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1395": {
    "title": "LCAN: A Label-Aware Contrastive Attention Network for Multi-Intent Recognition and Slot Filling in Task-Oriented Dialogue Systems",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuli Zhang",
      "Zhiqiang You",
      "Xiao Xiang Qi",
      "Peng Liu",
      "Gaode Wu",
      "Kan Xia",
      "Shenguang Huang"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1396": {
    "title": "Low-Resource Languages LLM Disinformation is Within Reach: The Case of Walliserdeutsch",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrei Kucharavy",
      "Sherine Seppey",
      "Cyril Vallez",
      "Dimitri Percia David",
      "Ljiljana Dolamic"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1397": {
    "title": "Exploring and Controlling Diversity in LLM-Agent Conversation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "KuanChao Chu",
      "Yi-Pei Chen",
      "Hideki Nakayama"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1398": {
    "title": "Agentic-ToM: Cognition-Inspired Agentic Processing For Enhancing Theory of Mind Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sneheel Sarangi",
      "Chetan Talele",
      "Hanan Salam"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1399": {
    "title": "Can We Edit LLMs for Long-Tail Biomedical Knowledge?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinhao Yi",
      "Jake Lever",
      "Kevin Bryson",
      "Zaiqiao Meng"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1400": {
    "title": "GeoPQA: Bridging the Visual Perception Gap in MLLMs for Geometric Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guizhen Chen",
      "Weiwen Xu",
      "Hao Zhang",
      "Hou Pong Chan",
      "Deli Zhao",
      "Anh Tuan Luu",
      "Yu Rong"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1401": {
    "title": "CM-Align: Consistency-based Multilingual Alignment for Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xue Zhang",
      "Yunlong Liang",
      "Fandong Meng",
      "Songming Zhang",
      "Yufeng Chen",
      "Jinan Xu",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1402": {
    "title": "Cache Saver: A Modular Framework for Efficient, Affordable, and Reproducible LLM Inference",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nearchos Potamitis",
      "Lars Henning Klein",
      "Bardia Mohammadi",
      "Chongyang Xu",
      "Attreyee Mukherjee",
      "Niket Tandon",
      "Laurent Bindschaedler",
      "Akhil Arora"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1403": {
    "title": "Evaluating Cultural Knowledge and Reasoning in LLMs Through Persian Allusions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Melika Nobakhtian",
      "Yadollah Yaghoobzadeh",
      "Mohammad Taher Pilehvar"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1404": {
    "title": "Evolving Stances on Reproducibility: A Longitudinal Study of NLP and ML Researchers' Views and Experience of Reproducibility",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Craig Thomson",
      "Ehud Reiter",
      "João Sedoc",
      "Anya Belz"
    ]
  },
  "https://aclanthology.org/2025.findings-emnlp.1405": {
    "title": "KAHAN: Knowledge-Augmented Hierarchical Analysis and Narration for Financial Data Narration",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yajing Yang",
      "Tony Deng",
      "Min-Yen Kan"
    ]
  }
}