{
  "https://aclanthology.org/2025.tacl-1.2": {
    "title": "SpiRit-LM: Interleaved Spoken and Written Language Model",
    "volume": "main",
    "abstract": "We introduce SpiRit-LM, a foundation multimodal language model that freely mixes text and speech. Our model is based on a 7B pretrained text language model that we extend to the speech modality by continuously training it on text and speech units. Speech and text sequences are concatenated as a single stream of tokens, and trained with a word-level interleaving method using a small automatically curated speech-text parallel corpus. SpiRit-LM comes in two versions: a Base version that uses speech phonetic units (HuBERT) and an Expressive version that models expressivity using pitch and style units in addition to the phonetic units. For both versions, the text is encoded with subword BPE tokens. The resulting model displays both the semantic abilities of text models and the expressive abilities of speech models. Additionally, we demonstrate that SpiRit-LM can learn new tasks in a few-shot fashion across modalities (i.e., ASR, TTS, Speech Classification). We make available model weights and inference code.1,2",
    "checked": true,
    "id": "7547e30ba98a0217f07a6bb9fc393902bbc89269",
    "semantic_title": "spirit-lm: interleaved spoken and written language model",
    "citation_count": 46,
    "authors": [
      "Tu Anh Nguyen",
      "Benjamin Muller",
      "Bokai Yu",
      "Marta R. Costa-jussa",
      "Maha Elbayad",
      "Sravya Popuri",
      "Christophe Ropers",
      "Paul-Ambroise Duquenne",
      "Robin Algayres",
      "Ruslan Mavlyutov",
      "Itai Gat",
      "Mary Williamson",
      "Gabriel Synnaeve",
      "Juan Pino",
      "Benoît Sagot",
      "Emmanuel Dupoux"
    ]
  },
  "https://aclanthology.org/2025.tacl-1.3": {
    "title": "CLAPnq: Cohesive Long-form Answers from Passages in Natural Questions for RAG systems",
    "volume": "main",
    "abstract": "Retrieval Augmented Generation (RAG) has become a popular application for large language models. It is preferable that successful RAG systems provide accurate answers that are supported by being grounded in a passage without any hallucinations. While considerable work is required for building a full RAG pipeline, being able to benchmark performance is also necessary. We present CLAPnq, a benchmark Long-form Question Answering dataset for the full RAG pipeline. CLAPnq includes long answers with grounded gold passages from Natural Questions (NQ) and a corpus to perform either retrieval, generation, or the full RAG pipeline. The CLAPnq answers are concise, 3x smaller than the full passage, and cohesive, meaning that the answer is composed fluently, often by integrating multiple pieces of the passage that are not contiguous. RAG models must adapt to these properties to be successful at CLAPnq. We present baseline experiments and analysis for CLAPnq that highlight areas where there is still significant room for improvement in grounded RAG. CLAPnq is publicly available at https://github.com/primeqa/clapnq",
    "checked": true,
    "id": "62390c0002c6de5e9252e12e2eec2e78ebc1c3d4",
    "semantic_title": "clapnq: cohesive long-form answers from passages in natural questions for rag systems",
    "citation_count": 13,
    "authors": [
      "Sara Rosenthal",
      "Avirup Sil",
      "Radu Florian",
      "Salim Roukos"
    ]
  },
  "https://aclanthology.org/2025.tacl-1.4": {
    "title": "Salute the Classic: Revisiting Challenges of Machine Translation in the Age of Large Language Models",
    "volume": "main",
    "abstract": "The evolution of Neural Machine Translation (NMT) has been significantly influenced by six core challenges (Koehn and Knowles, 2017) that have acted as benchmarks for progress in this field. This study revisits these challenges, offering insights into their ongoing relevance in the context of advanced Large Language Models (LLMs): domain mismatch, amount of parallel data, rare word prediction, translation of long sentences, attention model as word alignment, and sub-optimal beam search. Our empirical findings show that LLMs effectively reduce reliance on parallel data for major languages during pretraining and significantly improve translation of long sentences containing approximately 80 words, even translating documents up to 512 words. Despite these improvements, challenges in domain mismatch and rare word prediction persist. While NMT-specific challenges like word alignment and beam search may not apply to LLMs, we identify three new challenges in LLM-based translation: inference efficiency, translation of low-resource languages during pretraining, and human-aligned evaluation",
    "checked": true,
    "id": "d1679160211bcbedd46bf1826caad5a7e1877315",
    "semantic_title": "salute the classic: revisiting challenges of machine translation in the age of large language models",
    "citation_count": 6,
    "authors": [
      "Jianhui Pang",
      "Fanghua Ye",
      "Derek Fai Wong",
      "Dian Yu",
      "Shuming Shi",
      "Zhaopeng Tu",
      "Longyue Wang"
    ]
  },
  "https://aclanthology.org/2025.tacl-1.5": {
    "title": "Investigating Critical Period Effects in Language Acquisition through Neural Language Models",
    "volume": "main",
    "abstract": "Humans appear to have a critical period (CP) for language acquisition: Second language (L2) acquisition becomes harder after early childhood, and ceasing exposure to a first language (L1) after this period (but not before) typically does not lead to substantial loss of L1 proficiency. It is unknown whether these CP effects result from innately determined brain maturation or as a stabilization of neural connections naturally induced by experience. In this study, we use language models (LMs) to test the extent to which these phenomena are peculiar to humans, or shared by a broader class of language learners. We vary the age of exposure by training LMs on language pairs in various experimental conditions, and find that LMs, which lack any direct analog to innate maturational stages, do not show CP effects when the age of exposure of L2 is delayed. Our results contradict the claim that CP effects are an inevitable result of statistical learning, and they are consistent with an innate mechanism for CP effects. We show that we can reverse-engineer the CP by introducing a regularizer partway through training to simulate a maturational decrease in plasticity. All in all, our results suggest that L1 learning on its own may not be enough to induce a CP, and additional engineering is necessary to make language models more cognitively plausible",
    "checked": true,
    "id": "232ba6c42e6f38147210d16337f0a0826fc4b900",
    "semantic_title": "investigating critical period effects in language acquisition through neural language models",
    "citation_count": 1,
    "authors": [
      "Ionut Constantinescu",
      "Tiago Pimentel",
      "Ryan Cotterell",
      "Alex Warstadt"
    ]
  },
  "https://aclanthology.org/2025.tacl-1.6": {
    "title": "Learning Syntax Without Planting Trees: Understanding Hierarchical Generalization in Transformers",
    "volume": "main",
    "abstract": "Transformers trained on natural language data have been shown to exhibit hierarchical generalization without explicitly encoding any structural bias. In this work, we investigate sources of inductive bias in transformer models and their training that could cause such preference for hierarchical generalization. We extensively experiment with transformers trained on five synthetic, controlled datasets using several training objectives and show that, while objectives such as sequence-to-sequence modeling, classification, etc., often fail to lead to hierarchical generalization, the language modeling objective consistently leads to transformers generalizing hierarchically. We then study how different generalization behaviors emerge during the training by conducting pruning experiments that reveal the joint existence of subnetworks within the model implementing different generalizations. Finally, we take a Bayesian perspective to understand transformers' preference for hierarchical generalization: We establish a correlation between whether transformers generalize hierarchically on a dataset and if the simplest explanation of that dataset is provided by a hierarchical grammar compared to regular grammars exhibiting linear generalization. Overall, our work presents new insights on the origins of hierarchical generalization in transformers and provides a theoretical framework for studying generalization in language models",
    "checked": true,
    "id": "bfd1ad01ffa6adaf2625f90d0e6fe57a6d4a87b4",
    "semantic_title": "learning syntax without planting trees: understanding hierarchical generalization in transformers",
    "citation_count": 7,
    "authors": [
      "Kabir Ahuja",
      "Vidhisha Balachandran",
      "Madhur Panwar",
      "Tianxing He",
      "Noah A. Smith",
      "Navin Goyal",
      "Yulia Tsvetkov"
    ]
  },
  "https://aclanthology.org/2025.tacl-1.7": {
    "title": "A Confidence-based Acquisition Model for Self-supervised Active Learning and Label Correction",
    "volume": "main",
    "abstract": "Supervised neural approaches are hindered by their dependence on large, meticulously annotated datasets, a requirement that is particularly cumbersome for sequential tasks. The quality of annotations tends to deteriorate with the transition from expert-based to crowd-sourced labeling. To address these challenges, we present CAMEL (Confidence-based Acquisition Model for Efficient self-supervised active Learning), a pool-based active learning framework tailored to sequential multi-output problems. CAMEL possesses two core features: (1) it requires expert annotators to label only a fraction of a chosen sequence, and (2) it facilitates self-supervision for the remainder of the sequence. By deploying a label correction mechanism, CAMEL can also be utilized for data cleaning. We evaluate CAMEL on two sequential tasks, with a special emphasis on dialogue belief tracking, a task plagued by the constraints of limited and noisy datasets. Our experiments demonstrate that CAMEL significantly outperforms the baselines in terms of efficiency. Furthermore, the data corrections suggested by our method contribute to an overall improvement in the quality of the resulting datasets.1",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Carel van Niekerk",
      "Christian Geishauser",
      "Michael Heck",
      "Shutong Feng",
      "Hsien-chin Lin",
      "Nurul Lubis",
      "Benjamin Ruppik",
      "Renato Vukovic",
      "Milica Gašić"
    ]
  },
  "https://aclanthology.org/2025.tacl-1.8": {
    "title": "OPT-Tree: Speculative Decoding with Adaptive Draft Tree Structure",
    "volume": "main",
    "abstract": "Autoregressive language models demonstrate excellent performance in various scenarios. However, the inference efficiency is limited by its one-step-one-word generation mode, which has become a pressing problem recently as the models become increasingly larger. Speculative decoding employs a \"draft and then verify\" mechanism to allow multiple tokens to be generated in one step, realizing lossless acceleration. Existing methods mainly adopt fixed heuristic draft structures, which do not adapt to different situations to maximize the acceptance length during verification. To alleviate this dilemma, we propose OPT-Tree, an algorithm to construct adaptive and scalable draft trees, which can be applied to any autoregressive draft model. It searches the optimal tree structure that maximizes the mathematical expectation of the acceptance length in each decoding step. Experimental results reveal that OPT-Tree outperforms the existing draft structures and achieves a speed-up ratio of up to 3.2 compared with autoregressive decoding. If the draft model is powerful enough and the node budget is sufficient, it can generate more than ten tokens in a single step. Our code is available at https://github.com/Jikai0Wang/OPT-Tree",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jikai Wang",
      "Yi Su",
      "Juntao Li",
      "Qingrong Xia",
      "Zi Ye",
      "Xinyu Duan",
      "Zhefeng Wang",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2025.tacl-1.9": {
    "title": "Transformers as Transducers",
    "volume": "main",
    "abstract": "We study the sequence-to-sequence mapping capacity of transformers by relating them to finite transducers, and find that they can express surprisingly large classes of (total functional) transductions. We do so using variants of RASP, a programming language designed to help people \"think like transformers,\" as an intermediate representation. We extend the existing Boolean variant B-RASP to sequence-to-sequence transductions and show that it computes exactly the first-order rational transductions (such as string rotation). Then, we introduce two new extensions. B-RASP[pos] enables calculations on positions (such as copying the first half of a string) and contains all first-order regular transductions. S-RASP adds prefix sum, which enables additional arithmetic operations (such as squaring a string) and contains all first-order polyregular transductions. Finally, we show that masked average-hard attention transformers can simulate S-RASP",
    "checked": true,
    "id": "86dd67744f7e8b4ce8e8cf7449e13459ccf3d1c7",
    "semantic_title": "transformers as transducers",
    "citation_count": 5,
    "authors": [
      "Lena Strobl",
      "Dana Angluin",
      "David Chiang",
      "Jonathan Rawski",
      "Ashish Sabharwal"
    ]
  }
}