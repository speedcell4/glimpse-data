{
  "https://aclanthology.org/2025.tacl-1.2": {
    "title": "SpiRit-LM: Interleaved Spoken and Written Language Model",
    "volume": "main",
    "abstract": "We introduce SpiRit-LM, a foundation multimodal language model that freely mixes text and speech. Our model is based on a 7B pretrained text language model that we extend to the speech modality by continuously training it on text and speech units. Speech and text sequences are concatenated as a single stream of tokens, and trained with a word-level interleaving method using a small automatically curated speech-text parallel corpus. SpiRit-LM comes in two versions: a Base version that uses speech phonetic units (HuBERT) and an Expressive version that models expressivity using pitch and style units in addition to the phonetic units. For both versions, the text is encoded with subword BPE tokens. The resulting model displays both the semantic abilities of text models and the expressive abilities of speech models. Additionally, we demonstrate that SpiRit-LM can learn new tasks in a few-shot fashion across modalities (i.e., ASR, TTS, Speech Classification). We make available model weights and inference code.1,2",
    "checked": true,
    "id": "7547e30ba98a0217f07a6bb9fc393902bbc89269",
    "semantic_title": "spirit-lm: interleaved spoken and written language model",
    "citation_count": 67,
    "authors": [
      "Tu Anh Nguyen",
      "Benjamin Muller",
      "Bokai Yu",
      "Marta R. Costa-jussa",
      "Maha Elbayad",
      "Sravya Popuri",
      "Christophe Ropers",
      "Paul-Ambroise Duquenne",
      "Robin Algayres",
      "Ruslan Mavlyutov",
      "Itai Gat",
      "Mary Williamson",
      "Gabriel Synnaeve",
      "Juan Pino",
      "Beno√Æt Sagot",
      "Emmanuel Dupoux"
    ]
  },
  "https://aclanthology.org/2025.tacl-1.3": {
    "title": "CLAPnq: Cohesive Long-form Answers from Passages in Natural Questions for RAG systems",
    "volume": "main",
    "abstract": "Retrieval Augmented Generation (RAG) has become a popular application for large language models. It is preferable that successful RAG systems provide accurate answers that are supported by being grounded in a passage without any hallucinations. While considerable work is required for building a full RAG pipeline, being able to benchmark performance is also necessary. We present CLAPnq, a benchmark Long-form Question Answering dataset for the full RAG pipeline. CLAPnq includes long answers with grounded gold passages from Natural Questions (NQ) and a corpus to perform either retrieval, generation, or the full RAG pipeline. The CLAPnq answers are concise, 3x smaller than the full passage, and cohesive, meaning that the answer is composed fluently, often by integrating multiple pieces of the passage that are not contiguous. RAG models must adapt to these properties to be successful at CLAPnq. We present baseline experiments and analysis for CLAPnq that highlight areas where there is still significant room for improvement in grounded RAG. CLAPnq is publicly available at https://github.com/primeqa/clapnq",
    "checked": true,
    "id": "62390c0002c6de5e9252e12e2eec2e78ebc1c3d4",
    "semantic_title": "clapnq: cohesive long-form answers from passages in natural questions for rag systems",
    "citation_count": 17,
    "authors": [
      "Sara Rosenthal",
      "Avirup Sil",
      "Radu Florian",
      "Salim Roukos"
    ]
  },
  "https://aclanthology.org/2025.tacl-1.4": {
    "title": "Salute the Classic: Revisiting Challenges of Machine Translation in the Age of Large Language Models",
    "volume": "main",
    "abstract": "The evolution of Neural Machine Translation (NMT) has been significantly influenced by six core challenges (Koehn and Knowles, 2017) that have acted as benchmarks for progress in this field. This study revisits these challenges, offering insights into their ongoing relevance in the context of advanced Large Language Models (LLMs): domain mismatch, amount of parallel data, rare word prediction, translation of long sentences, attention model as word alignment, and sub-optimal beam search. Our empirical findings show that LLMs effectively reduce reliance on parallel data for major languages during pretraining and significantly improve translation of long sentences containing approximately 80 words, even translating documents up to 512 words. Despite these improvements, challenges in domain mismatch and rare word prediction persist. While NMT-specific challenges like word alignment and beam search may not apply to LLMs, we identify three new challenges in LLM-based translation: inference efficiency, translation of low-resource languages during pretraining, and human-aligned evaluation",
    "checked": true,
    "id": "d1679160211bcbedd46bf1826caad5a7e1877315",
    "semantic_title": "salute the classic: revisiting challenges of machine translation in the age of large language models",
    "citation_count": 6,
    "authors": [
      "Jianhui Pang",
      "Fanghua Ye",
      "Derek Fai Wong",
      "Dian Yu",
      "Shuming Shi",
      "Zhaopeng Tu",
      "Longyue Wang"
    ]
  },
  "https://aclanthology.org/2025.tacl-1.5": {
    "title": "Investigating Critical Period Effects in Language Acquisition through Neural Language Models",
    "volume": "main",
    "abstract": "Humans appear to have a critical period (CP) for language acquisition: Second language (L2) acquisition becomes harder after early childhood, and ceasing exposure to a first language (L1) after this period (but not before) typically does not lead to substantial loss of L1 proficiency. It is unknown whether these CP effects result from innately determined brain maturation or as a stabilization of neural connections naturally induced by experience. In this study, we use language models (LMs) to test the extent to which these phenomena are peculiar to humans, or shared by a broader class of language learners. We vary the age of exposure by training LMs on language pairs in various experimental conditions, and find that LMs, which lack any direct analog to innate maturational stages, do not show CP effects when the age of exposure of L2 is delayed. Our results contradict the claim that CP effects are an inevitable result of statistical learning, and they are consistent with an innate mechanism for CP effects. We show that we can reverse-engineer the CP by introducing a regularizer partway through training to simulate a maturational decrease in plasticity. All in all, our results suggest that L1 learning on its own may not be enough to induce a CP, and additional engineering is necessary to make language models more cognitively plausible",
    "checked": true,
    "id": "232ba6c42e6f38147210d16337f0a0826fc4b900",
    "semantic_title": "investigating critical period effects in language acquisition through neural language models",
    "citation_count": 1,
    "authors": [
      "Ionut Constantinescu",
      "Tiago Pimentel",
      "Ryan Cotterell",
      "Alex Warstadt"
    ]
  },
  "https://aclanthology.org/2025.tacl-1.6": {
    "title": "Learning Syntax Without Planting Trees: Understanding Hierarchical Generalization in Transformers",
    "volume": "main",
    "abstract": "Transformers trained on natural language data have been shown to exhibit hierarchical generalization without explicitly encoding any structural bias. In this work, we investigate sources of inductive bias in transformer models and their training that could cause such preference for hierarchical generalization. We extensively experiment with transformers trained on five synthetic, controlled datasets using several training objectives and show that, while objectives such as sequence-to-sequence modeling, classification, etc., often fail to lead to hierarchical generalization, the language modeling objective consistently leads to transformers generalizing hierarchically. We then study how different generalization behaviors emerge during the training by conducting pruning experiments that reveal the joint existence of subnetworks within the model implementing different generalizations. Finally, we take a Bayesian perspective to understand transformers' preference for hierarchical generalization: We establish a correlation between whether transformers generalize hierarchically on a dataset and if the simplest explanation of that dataset is provided by a hierarchical grammar compared to regular grammars exhibiting linear generalization. Overall, our work presents new insights on the origins of hierarchical generalization in transformers and provides a theoretical framework for studying generalization in language models",
    "checked": true,
    "id": "bfd1ad01ffa6adaf2625f90d0e6fe57a6d4a87b4",
    "semantic_title": "learning syntax without planting trees: understanding hierarchical generalization in transformers",
    "citation_count": 8,
    "authors": [
      "Kabir Ahuja",
      "Vidhisha Balachandran",
      "Madhur Panwar",
      "Tianxing He",
      "Noah A. Smith",
      "Navin Goyal",
      "Yulia Tsvetkov"
    ]
  },
  "https://aclanthology.org/2025.tacl-1.10": {
    "title": "Navigating Cultural Chasms: Exploring and Unlocking the Cultural POV of Text-To-Image Models",
    "volume": "main",
    "abstract": "Text-To-Image (TTI) models, such as DALL-E and StableDiffusion, have demonstrated remarkable prompt-based image generation capabilities. Multilingual encoders may have a substantial impact on the cultural agency of these models, as language is a conduit of culture. In this study, we explore the cultural perception embedded in TTI models by characterizing culture across three tiers: cultural dimensions, cultural domains, and cultural concepts. Based on this ontology, we derive prompt templates to unlock the cultural knowledge in TTI models, and propose a comprehensive suite of evaluation techniques, including intrinsic evaluations using the CLIP space, extrinsic evaluations with a Visual-Question-Answer models and human assessments, to evaluate the cultural content of TTI-generated images. To bolster our research, we introduce the CulText2I dataset, based on six diverse TTI models and spanning ten languages. Our experiments provide insights regarding Do, What, Which, and How research questions about the nature of cultural encoding in TTI models, paving the way for cross-cultural applications of these models.1",
    "checked": true,
    "id": "c811b7e98b755ab7d34baa466796d00a93f662e7",
    "semantic_title": "navigating cultural chasms: exploring and unlocking the cultural pov of text-to-image models",
    "citation_count": 14,
    "authors": [
      "Mor Ventura",
      "Eyal Ben-David",
      "Anna Korhonen",
      "Roi Reichart"
    ]
  },
  "https://aclanthology.org/2025.tacl-1.7": {
    "title": "A Confidence-based Acquisition Model for Self-supervised Active Learning and Label Correction",
    "volume": "main",
    "abstract": "Supervised neural approaches are hindered by their dependence on large, meticulously annotated datasets, a requirement that is particularly cumbersome for sequential tasks. The quality of annotations tends to deteriorate with the transition from expert-based to crowd-sourced labeling. To address these challenges, we present CAMEL (Confidence-based Acquisition Model for Efficient self-supervised active Learning), a pool-based active learning framework tailored to sequential multi-output problems. CAMEL possesses two core features: (1) it requires expert annotators to label only a fraction of a chosen sequence, and (2) it facilitates self-supervision for the remainder of the sequence. By deploying a label correction mechanism, CAMEL can also be utilized for data cleaning. We evaluate CAMEL on two sequential tasks, with a special emphasis on dialogue belief tracking, a task plagued by the constraints of limited and noisy datasets. Our experiments demonstrate that CAMEL significantly outperforms the baselines in terms of efficiency. Furthermore, the data corrections suggested by our method contribute to an overall improvement in the quality of the resulting datasets.1",
    "checked": true,
    "id": "ecd6fe307df6eb82195b3d20a99ce0c40e67998e",
    "semantic_title": "a confidence-based acquisition model for self-supervised active learning and label correction",
    "citation_count": 1,
    "authors": [
      "Carel van Niekerk",
      "Christian Geishauser",
      "Michael Heck",
      "Shutong Feng",
      "Hsien-chin Lin",
      "Nurul Lubis",
      "Benjamin Ruppik",
      "Renato Vukovic",
      "Milica Ga≈°iƒá"
    ]
  },
  "https://aclanthology.org/2025.tacl-1.8": {
    "title": "OPT-Tree: Speculative Decoding with Adaptive Draft Tree Structure",
    "volume": "main",
    "abstract": "Autoregressive language models demonstrate excellent performance in various scenarios. However, the inference efficiency is limited by its one-step-one-word generation mode, which has become a pressing problem recently as the models become increasingly larger. Speculative decoding employs a \"draft and then verify\" mechanism to allow multiple tokens to be generated in one step, realizing lossless acceleration. Existing methods mainly adopt fixed heuristic draft structures, which do not adapt to different situations to maximize the acceptance length during verification. To alleviate this dilemma, we propose OPT-Tree, an algorithm to construct adaptive and scalable draft trees, which can be applied to any autoregressive draft model. It searches the optimal tree structure that maximizes the mathematical expectation of the acceptance length in each decoding step. Experimental results reveal that OPT-Tree outperforms the existing draft structures and achieves a speed-up ratio of up to 3.2 compared with autoregressive decoding. If the draft model is powerful enough and the node budget is sufficient, it can generate more than ten tokens in a single step. Our code is available at https://github.com/Jikai0Wang/OPT-Tree",
    "checked": true,
    "id": "ab16bf2223fcb8c749a52ee3cc0496dc5bb0b4ec",
    "semantic_title": "opt-tree: speculative decoding with adaptive draft tree structure",
    "citation_count": 23,
    "authors": [
      "Jikai Wang",
      "Yi Su",
      "Juntao Li",
      "Qingrong Xia",
      "Zi Ye",
      "Xinyu Duan",
      "Zhefeng Wang",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2025.tacl-1.9": {
    "title": "Transformers as Transducers",
    "volume": "main",
    "abstract": "We study the sequence-to-sequence mapping capacity of transformers by relating them to finite transducers, and find that they can express surprisingly large classes of (total functional) transductions. We do so using variants of RASP, a programming language designed to help people \"think like transformers,\" as an intermediate representation. We extend the existing Boolean variant B-RASP to sequence-to-sequence transductions and show that it computes exactly the first-order rational transductions (such as string rotation). Then, we introduce two new extensions. B-RASP[pos] enables calculations on positions (such as copying the first half of a string) and contains all first-order regular transductions. S-RASP adds prefix sum, which enables additional arithmetic operations (such as squaring a string) and contains all first-order polyregular transductions. Finally, we show that masked average-hard attention transformers can simulate S-RASP",
    "checked": true,
    "id": "86dd67744f7e8b4ce8e8cf7449e13459ccf3d1c7",
    "semantic_title": "transformers as transducers",
    "citation_count": 6,
    "authors": [
      "Lena Strobl",
      "Dana Angluin",
      "David Chiang",
      "Jonathan Rawski",
      "Ashish Sabharwal"
    ]
  },
  "https://aclanthology.org/2025.tacl-1.11": {
    "title": "Benchmarking Uncertainty Quantification Methods for Large Language Models with LM-Polygraph",
    "volume": "main",
    "abstract": "The rapid proliferation of large language models (LLMs) has stimulated researchers to seek effective and efficient approaches to deal with LLM hallucinations and low-quality outputs. Uncertainty quantification (UQ) is a key element of machine learning applications in dealing with such challenges. However, research to date on UQ for LLMs has been fragmented in terms of techniques and evaluation methodologies. In this work, we address this issue by introducing a novel benchmark that implements a collection of state-of-the-art UQ baselines and offers an environment for controllable and consistent evaluation of novel UQ techniques over various text generation tasks. Our benchmark also supports the assessment of confidence normalization methods in terms of their ability to provide interpretable scores. Using our benchmark, we conduct a large-scale empirical investigation of UQ and normalization techniques across eleven tasks, identifying the most effective approaches",
    "checked": true,
    "id": "cc0c6f4dbbfc163cfae15724da1d7e3042fa099c",
    "semantic_title": "benchmarking uncertainty quantification methods for large language models with lm-polygraph",
    "citation_count": 34,
    "authors": [
      "Roman Vashurin",
      "Ekaterina Fadeeva",
      "Artem Vazhentsev",
      "Lyudmila Rvanova",
      "Daniil Vasilev",
      "Akim Tsvigun",
      "Sergey Petrakov",
      "Rui Xing",
      "Abdelrahman Sadallah",
      "Kirill Grishchenkov",
      "Alexander Panchenko",
      "Timothy Baldwin",
      "Preslav Nakov",
      "Maxim Panov",
      "Artem Shelmanov"
    ]
  },
  "https://aclanthology.org/2025.tacl-1.12": {
    "title": "Supervised Neural Topic Modeling with Label Alignment",
    "volume": "main",
    "abstract": "Neural topic modeling is a scalable automated technique for text data mining. In various downstream tasks of topic modeling, it is preferred that the discovered topics well align with labels. However, due to the lack of guidance from labels, unsupervised neural topic models are less powerful in this situation. Existing supervised neural topic models often adopt a label-free prior to generate the latent document-topic distributions and use them to predict the labels and thus achieve label-topic alignment indirectly. Such a mechanism faces the following issues: 1) The label-free prior leads to topics blending the latent patterns of multiple labels; and 2) One is unable to intuitively identify the explicit relationships between labels and the discovered topics. To tackle these problems, we develop a novel supervised neural topic model which utilizes a chain-structured graphical model with a label-conditioned prior. Soft indicators are introduced to explicitly construct the label-topic relationships. To obtain well-organized label-topic relationships, we formalize an entropy-regularized optimal transport problem on the embedding space and model them as the transport plan. Moreover, our proposed method can be flexibly integrated with most existing unsupervised neural topic models. Experimental results on multiple datasets demonstrate that our model can greatly enhance the alignment between labels and topics while maintaining good topic quality",
    "checked": true,
    "id": "31b7d8982b5a147a267434f25f0ba907dede97b0",
    "semantic_title": "supervised neural topic modeling with label alignment",
    "citation_count": 0,
    "authors": [
      "Ruihao Chen",
      "Hegang Chen",
      "Yuyin Lu",
      "Yanghui Rao",
      "Chunjiang Zhu"
    ]
  },
  "https://aclanthology.org/2025.tacl-1.13": {
    "title": "From Robustness to Improved Generalization and Calibration in Pre-trained Language Models",
    "volume": "main",
    "abstract": "Enforcing representation smoothness in pre-trained language models (PLMs) through Jacobian and Hessian regularization provides an effective approach for enhancing both robustness and generalization. Although such regularization methods have proven effective in computer vision, their application in natural language processing, where PLM inputs are derived from a discrete domain, poses unique challenges. We introduce JacHess, a regularization approach for PLMs that minimizes the norms of the Jacobian and Hessian matrices in intermediate representations, using embeddings as substitutes for discrete token inputs. JacHess supports dual-mode regularization, alternating between fine-tuning with labeled data and regularization with unlabeled data. We evaluate JacHess on the GLUE benchmark and demonstrate that it consistently and significantly improves in-distribution generalization and enhances performance under domain shift. Across diverse PLMs, JacHess outperforms comparable representation-based regularization methods and unregularized fine-tuning, while also improving model calibration. Our findings, coupled with a computationally efficient estimator for the Jacobian and Hessian norms, position JacHess as a robust and widely applicable solution for enhancing PLM performance",
    "checked": true,
    "id": "020b7ed6a07c60125902a527a3a8d064a842e725",
    "semantic_title": "from robustness to improved generalization and calibration in pre-trained language models",
    "citation_count": 0,
    "authors": [
      "Josip Jukiƒá",
      "Jan ≈†najder"
    ]
  },
  "https://aclanthology.org/2025.tacl-1.14": {
    "title": "How \"Real\" is Your Real-Time Simultaneous Speech-to-Text Translation System?",
    "volume": "main",
    "abstract": "Simultaneous speech-to-text translation (SimulST) translates source-language speech into target-language text concurrently with the speaker's speech, ensuring low latency for better user comprehension. Despite its intended application to unbounded speech, most research has focused on human pre-segmented speech, simplifying the task and overlooking significant challenges. This narrow focus, coupled with widespread terminological inconsistencies, is limiting the applicability of research outcomes to real-world applications, ultimately hindering progress in the field. Our extensive literature review of 110 papers not only reveals these critical issues in current research but also serves as the foundation for our key contributions. We: 1) define the steps and core components of a SimulST system, proposing a standardized terminology and taxonomy; 2) conduct a thorough analysis of community trends; and 3) offer concrete recommendations and future directions to bridge the gaps in existing literature, from evaluation frameworks to system architectures, for advancing the field towards more realistic and effective SimulST solutions",
    "checked": true,
    "id": "1e3f72a19e32f6e28b6ed2c438cfea296e9ffebb",
    "semantic_title": "how \"real\" is your real-time simultaneous speech-to-text translation system?",
    "citation_count": 6,
    "authors": [
      "Sara Papi",
      "Peter Pol√°k",
      "Dominik Mach√°ƒçek",
      "Ond≈ôej Bojar"
    ]
  },
  "https://aclanthology.org/2025.tacl-1.15": {
    "title": "Self-Rationalization in the Wild: A Large-scale Out-of-Distribution Evaluation on NLI-related tasks",
    "volume": "main",
    "abstract": "Free-text explanations are expressive and easy to understand, but many datasets lack annotated explanation data, making it challenging to train models for explainable predictions. To address this, we investigate how to use existing explanation datasets for self-rationalization and evaluate models' out-of-distribution (OOD) performance. We fine-tune T5-Large and OLMo-7B models and assess the impact of fine-tuning data quality, the number of fine-tuning samples, and few-shot selection methods. The models are evaluated on 19 diverse OOD datasets across three tasks: natural language inference (NLI), fact-checking, and hallucination detection in abstractive summarization. For the generated explanation evaluation, we conduct a human study on 13 selected models and study its correlation with the Acceptability score (T5-11B) and three other LLM-based reference-free metrics. Human evaluation shows that the Acceptability score correlates most strongly with human judgments, demonstrating its effectiveness in evaluating free-text explanations. Our findings reveal: 1) few annotated examples effectively adapt models for OOD explanation generation; 2) compared to sample selection strategies, fine-tuning data source has a larger impact on OOD performance; and 3) models with higher label prediction accuracy tend to produce better explanations, as reflected by higher Acceptability scores.1",
    "checked": false,
    "id": "9ccfabde6a99c5e270e951ee494737b15ec781c4",
    "semantic_title": "self-rationalization in the wild: a large scale out-of-distribution evaluation on nli-related tasks",
    "citation_count": 1,
    "authors": [
      "Jing Yang",
      "Max Glockner",
      "Anderson Rocha",
      "Iryna Gurevych"
    ]
  },
  "https://aclanthology.org/2025.tacl-1.16": {
    "title": "DEAR: Disentangled Event-Agnostic Representation Learning for Early Fake News Detection",
    "volume": "main",
    "abstract": "Detecting fake news early is challenging due to the absence of labeled articles for emerging events in training data. To address this, we propose a Disentangled Event-Agnostic Representation (DEAR) learning approach. Our method begins with a BERT-based adaptive multi-grained semantic encoder that captures hierarchical and comprehensive textual representations of the input news content. To effectively separate latent authenticity-related and event-specific knowledge within the news content, we employ a disentanglement architecture. To further enhance the decoupling effect, we introduce a cross-perturbation mechanism that perturbs authenticity-related representation with the event-specific one, and vice versa, deriving a robust and discerning authenticity-related signal. Additionally, we implement a refinement learning scheme to minimize potential interactions between two decoupled representations, ensuring that the authenticity signal remains strong and unaffected by event-specific details. Experimental results demonstrate that our approach effectively mitigates the impact of event-specific influence, outperforming state-of-the-art methods. In particular, it achieves a 6.0% improvement in accuracy on the PHEME dataset over MDDA, a similar approach that decouples latent content and style knowledge, in scenarios involving articles from unseen events different from the topics of the training set",
    "checked": true,
    "id": "0aafec8147aaf5767adb52c0ca5d8bba0d91888c",
    "semantic_title": "dear: disentangled event-agnostic representation learning for early fake news detection",
    "citation_count": 0,
    "authors": [
      "Xiao Pu",
      "Hao Wu",
      "Xiuli Bi",
      "Yu Wu",
      "Xinbo Gao"
    ]
  },
  "https://aclanthology.org/2025.tacl-1.17": {
    "title": "LLM Reading Tea Leaves: Automatically Evaluating Topic Models with Large Language Models",
    "volume": "main",
    "abstract": "Topic modeling has been a widely used tool for unsupervised text analysis. However, comprehensive evaluations of a topic model remain challenging. Existing evaluation methods are either less comparable across different models (e.g., perplexity) or focus on only one specific aspect of a model (e.g., topic quality or document representation quality) at a time, which is insufficient to reflect the overall model performance. In this paper, we propose WALM (Word Agreement with Language Model), a new evaluation method for topic modeling that considers the semantic quality of document representations and topics in a joint manner, leveraging the power of Large Language Models (LLMs). With extensive experiments involving different types of topic models, WALM is shown to align with human judgment and can serve as a complementary evaluation method to the existing ones, bringing a new perspective to topic modeling. Our software package is available at https://github.com/Xiaohao-Yang/Topic_Model_Evaluation",
    "checked": true,
    "id": "11cf9d5b836bbed2dfa44d659e1978e8804b184f",
    "semantic_title": "llm reading tea leaves: automatically evaluating topic models with large language models",
    "citation_count": 3,
    "authors": [
      "Xiaohao Yang",
      "He Zhao",
      "Dinh Phung",
      "Wray Buntine",
      "Lan Du"
    ]
  },
  "https://aclanthology.org/2025.tacl-1.18": {
    "title": "The Thai Universal Dependency Treebank",
    "volume": "main",
    "abstract": "Automatic dependency parsing of Thai sentences has been underexplored, as evidenced by the lack of large Thai dependency treebanks with complete dependency structures and the lack of a published evaluation of state-of-the-art models, especially transformer-based parsers. In this work, we addressed these gaps by introducing the Thai Universal Dependency Treebank (TUD), a new Thai treebank consisting of 3,627 trees annotated according to the Universal Dependencies (UD) framework. We then benchmarked 92 dependency parsing models that incorporate pretrained transformers on Thai-PUD and our TUD, achieving state-of-the-art results and shedding light on the optimal model components for Thai dependency parsing. Our error analysis of the models also reveals that polyfunctional words, serial verb construction, and lack of rich morphosyntactic features present main challenges for Thai dependency parsing",
    "checked": true,
    "id": "275537a57669dee578efe89841a01d46653e97c7",
    "semantic_title": "the thai universal dependency treebank",
    "citation_count": 0,
    "authors": [
      "Panyut Sriwirote",
      "Wei Qi Leong",
      "Charin Polpanumas",
      "Santhawat Thanyawong",
      "William Chandra Tjhi",
      "Wirote Aroonmanakun",
      "Attapol T. Rutherford"
    ]
  },
  "https://aclanthology.org/2025.tacl-1.19": {
    "title": "Diverse AI Feedback For Large Language Model Alignment",
    "volume": "main",
    "abstract": "Recent advances in large language models (LLMs) focus on aligning models with human values to minimize harmful content. However, existing methods often rely on a single type of feedback, such as preferences, annotated labels, or critiques, which can lead to overfitting and suboptimal performance. In this paper, we propose Diverse AIFeedback (DAIF), a novel approach that integrates three types of feedback‚Äîcritique, refinement, and preference‚Äîtailored to tasks of varying uncertainty levels. Through an analysis of information gain, we show that critique feedback is most effective for low-uncertainty tasks, refinement feedback for medium-uncertainty tasks, and preference feedback for high-uncertainty tasks. Training with this diversified feedback reduces overfitting and improves alignment. Experimental results across three tasks‚Äîquestion answering, dialog generation, and text summarization‚Äìdemonstrate that DAIF outperforms traditional methods relying on a single feedback type.1",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianshu Yu",
      "Ting-En Lin",
      "Yuchuan Wu",
      "Min Yang",
      "Fei Huang",
      "Yongbin Li"
    ]
  },
  "https://aclanthology.org/2025.tacl-1.20": {
    "title": "How Much Semantic Information is Available in Large Language Model Tokens?",
    "volume": "main",
    "abstract": "Large language models segment many words into multiple tokens, and companies that make those models claim that meaningful subword tokens are essential. To investigate whether subword tokens bear meaning, we segmented tens of thousands of words from each of 41 languages according to three generations of GPT tokenizers. We found that words sharing tokens are more semantically similar than expected by chance or expected from length alone, that tokens capture morphological information even when they don't look like morphemes, and that tokens capture more information than is explained by morphology. In languages that use a script other than the Latin alphabet, GPT-4 tokens are uninformative, but GPT-4o has improved this situation. These results suggest that comparing tokens to morphemes overlooks the wider variety of semantic information available in word form and that standard tokenization methods successfully capture much of that information",
    "checked": true,
    "id": "fd3e54a34ee3516fe29de9c0c713fe4a9cff2e43",
    "semantic_title": "how much semantic information is available in large language model tokens?",
    "citation_count": 2,
    "authors": [
      "David A. Haslett",
      "Zhenguang G. Cai"
    ]
  },
  "https://aclanthology.org/2025.tacl-1.21": {
    "title": "Phonetic Reconstruction of the Consonant System of Middle Chinese via Mixed Integer Optimization",
    "volume": "main",
    "abstract": "This paper is concerned with phonetic reconstruction of the consonant system of Middle Chinese. We propose to cast the problem as a Mixed Integer Programming problem, which is able to automatically explore homophonic information from ancient rhyme dictionaries and phonetic information from modern Chinese dialects, the descendants of Middle Chinese. Numerical evaluation on a wide range of synthetic and real data demonstrates the effectiveness and robustness of the new method. We apply the method to information from Gu«éngy√πn and 20 modern Chinese dialects to obtain a new phonetic reconstruction result. A linguistically motivated discussion of this result is also provided.1",
    "checked": true,
    "id": "2ad8e3376b493eaa413de0a52df06cecd222e7be",
    "semantic_title": "phonetic reconstruction of the consonant system of middle chinese via mixed integer optimization",
    "citation_count": 1,
    "authors": [
      "Xiaoxi Luo",
      "Weiwei Sun"
    ]
  },
  "https://aclanthology.org/2025.tacl-1.22": {
    "title": "Anchored Preference Optimization and Contrastive Revisions: Addressing Underspecification in Alignment",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) are often aligned using contrastive alignment objectives and preference pair datasets. The interaction between model, paired data, and objective makes alignment a complicated procedure, sometimes producing subpar results. We study this and find that (i) preference data gives a better learning signal when the underlying responses are contrastive, and (ii) alignment objectives lead to better performance when they specify more control over the model during training. Based on these insights, we introduce Contrastive Learning from AI Revisions (CLAIR), a data-creation method which leads to more contrastive preference pairs, and Anchored Preference Optimization (APO), a controllable and more stable alignment objective. We align Llama-3-8B-Instruct using various comparable datasets and alignment objectives and measure MixEval-Hard scores, which correlate highly with human judgments. The CLAIR preferences lead to the strongest performance out of all datasets, and APO consistently outperforms less controllable objectives. Our best model, trained on 32K CLAIR preferences with APO, improves Llama-3-8B-Instruct by 7.65%, closing the gap with GPT4-turbo by 45%. Our code and datasets are available",
    "checked": true,
    "id": "f9b6af0ef6393c73c32ffe1122b28634a8fb6367",
    "semantic_title": "anchored preference optimization and contrastive revisions: addressing underspecification in alignment",
    "citation_count": 23,
    "authors": [
      "Karel D‚ÄôOosterlinck",
      "Winnie Xu",
      "Chris Develder",
      "Thomas Demeester",
      "Amanpreet Singh",
      "Christopher Potts",
      "Douwe Kiela",
      "Shikib Mehri"
    ]
  },
  "https://aclanthology.org/2025.tacl-1.23": {
    "title": "TANQ: An Open Domain Dataset of Table Answered Questions",
    "volume": "main",
    "abstract": "Language models, potentially augmented with tool usage such as retrieval, are becoming the go-to means of answering questions. Understanding and answering questions in real-world settings often requires retrieving information from different sources, processing and aggregating data to extract insights, and presenting complex findings in form of structured artifacts such as novel tables, charts, or infographics. In this paper, we introduce TANQ,1 the first open-domain question answering dataset where the answers require building tables from information across multiple sources. We release the full source attribution for every cell in the resulting table and benchmark state-of-the-art language models in open, oracle, and closed book setups. Our best-performing baseline, Gemini Flash, reaches an overall F1 score of 60.7, lagging behind human performance by 12.3 points. We analyze baselines' performance across different dataset attributes such as different skills required for this task, including multi-hop reasoning, math operations, and unit conversions. We further discuss common failures in model-generated answers, suggesting that TANQ is a complex task with many challenges ahead",
    "checked": true,
    "id": "e16d43263b40910b35c34533e2d095d48a29ea14",
    "semantic_title": "tanq: an open domain dataset of table answered questions",
    "citation_count": 2,
    "authors": [
      "Mubashara Akhtar",
      "Chenxi Pang",
      "Andreea Marzoca",
      "Yasemin Altun",
      "Julian Martin Eisenschlos"
    ]
  },
  "https://aclanthology.org/2025.tacl-1.24": {
    "title": "Few-Shot Multilingual Open-Domain QA from Five Examples",
    "volume": "main",
    "abstract": "Recent approaches to multilingual open- domain question answering (MLODQA) have achieved promising results given abundant language-specific training data. However, the considerable annotation cost limits the application of these methods for underrepresented languages. We introduce a few-shot learning approach to synthesize large-scale multilingual data from large language models (LLMs). Our method begins with large-scale self-supervised pre-training using WikiData, followed by training on high-quality synthetic multilingual data generated by prompting LLMs with few-shot supervision. The final model, FsModQA, significantly outperforms existing few-shot and supervised baselines in MLODQA and cross-lingual and monolingual retrieval. We further show our method can be extended for effective zero-shot adaptation to new languages through a cross-lingual prompting strategy with only English-supervised data, making it a general and applicable solution for MLODQA tasks without costly large-scale annotation",
    "checked": true,
    "id": "959f4a20239fef4a397ebfbdbdc81a04b9fcdbf2",
    "semantic_title": "few-shot multilingual open-domain qa from five examples",
    "citation_count": 0,
    "authors": [
      "Fan Jiang",
      "Tom Drummond",
      "Trevor Cohn"
    ]
  },
  "https://aclanthology.org/2025.tacl-1.25": {
    "title": "Navigating the Landscape of Hint Generation Research: From the Past to the Future",
    "volume": "main",
    "abstract": "Digital education has gained popularity in the last decade, especially after the COVID-19 pandemic. With the improving capabilities of large language models to reason and communicate with users, envisioning intelligent tutoring systems that can facilitate self-learning is not very far-fetched. One integral component to fulfill this vision is the ability to give accurate and effective feedback via hints to scaffold the learning process. In this survey article, we present a comprehensive review of prior research on hint generation, aiming to bridge the gap between research in education and cognitive science, and research in AI and Natural Language Processing. Informed by our findings, we propose a formal definition of the hint generation task, and discuss the roadmap of building an effective hint generation system aligned with the formal definition, including open challenges, future directions and ethical considerations",
    "checked": true,
    "id": "a0e210b79e96ccb5384e90dc0f7809302d67858f",
    "semantic_title": "navigating the landscape of hint generation research: from the past to the future",
    "citation_count": 2,
    "authors": [
      "Anubhav Jangra",
      "Jamshid Mozafari",
      "Adam Jatowt",
      "Smaranda Muresan"
    ]
  },
  "https://aclanthology.org/2025.tacl-1.26": {
    "title": "Know Your Limits: A Survey of Abstention in Large Language Models",
    "volume": "main",
    "abstract": "Abstention, the refusal of large language models (LLMs) to provide an answer, is increasingly recognized for its potential to mitigate hallucinations and enhance safety in LLM systems. In this survey, we introduce a framework to examine abstention from three perspectives: the query, the model, and human values. We organize the literature on abstention methods, benchmarks, and evaluation metrics using this framework, and discuss merits and limitations of prior work. We further identify and motivate areas for future research, such as whether abstention can be achieved as a meta-capability that transcends specific tasks or domains, and opportunities to optimize abstention abilities in specific contexts. In doing so, we aim to broaden the scope and impact of abstention methodologies in AI systems.1",
    "checked": true,
    "id": "314a471e8fa6788054e98aca1a74622f90fccbb9",
    "semantic_title": "know your limits: a survey of abstention in large language models",
    "citation_count": 16,
    "authors": [
      "Bingbing Wen",
      "Jihan Yao",
      "Shangbin Feng",
      "Chenjun Xu",
      "Yulia Tsvetkov",
      "Bill Howe",
      "Lucy Lu Wang"
    ]
  },
  "https://aclanthology.org/2025.tacl-1.27": {
    "title": "TaxoPro: A Plug-In LoRA-based Cross-Domain Method for Low-Resource Taxonomy Completion",
    "volume": "main",
    "abstract": "Low-resource taxonomy completion aims to automatically insert new concepts into the existing taxonomy, in which only a few in-domain training samples are available. Recent studies have achieved considerable progress by incorporating prior knowledge from pre-trained language models (PLMs). However, these studies tend to overly rely on such knowledge and neglect the shareable knowledge across different taxonomies. In this paper, we propose TaxoPro, a plug-in LoRA-based cross-domain method, that captures shareable knowledge from the high- resource taxonomy to improve PLM-based low-resource taxonomy completion techniques. To prevent negative interference between domain-specific and domain-shared knowledge, TaxoPro decomposes cross- domain knowledge into domain-shared and domain-specific components, storing them using low-rank matrices (LoRA). Additionally, TaxoPro employs two auxiliary losses to regulate the flow of shareable knowledge. Experimental results demonstrate that TaxoPro improves PLM-based techniques, achieving state-of-the-art performance in completing low-resource taxonomies. Code is available at https://github.com/cyclexu/TaxoPro",
    "checked": true,
    "id": "488838225c9daf0cbd0d99650a54424725592cff",
    "semantic_title": "taxopro: a plug-in lora-based cross-domain method for low-resource taxonomy completion",
    "citation_count": 0,
    "authors": [
      "Hongyuan Xu",
      "Yuhang Niu",
      "Ciyi Liu",
      "Yanlong Wen",
      "Xiaojie Yuan"
    ]
  },
  "https://aclanthology.org/2025.tacl-1.28": {
    "title": "Exploring Practical Gaps in Using Cross Entropy to Implement Maximum Mutual Information Criterion for Rationalization",
    "volume": "main",
    "abstract": "Rationalization is a framework that aims to build self-explanatory NLP models by extracting a subset of human-intelligible pieces of their inputting texts. It involves a cooperative game where a selector selects the most human-intelligible parts of the input as the rationale, followed by a predictor that makes predictions based on these selected rationales. Existing literature uses the cross-entropy between the model's predictions and the ground-truth labels to measure the informativeness of the selected rationales, guiding the selector to choose better ones. In this study, we first theoretically analyze the objective of rationalization by decomposing it into two parts: the model-agnostic informativeness of the rationale candidates and the predictor's degree of fit. We then provide various empirical evidence to support that, under this framework, the selector tends to sample from a limited small region, causing the predictor to overfit these localized areas. This results in a significant mismatch between the cross-entropy objective and the informativeness of the rationale candidates, leading to suboptimal solutions. To address this issue, we propose a simple yet effective method that introduces random vicinal1 perturbations to the selected rationale candidates. This approach broadens the predictor's assessment to a vicinity around the selected rationale candidate. Compared to recent competitive methods, our method significantly improves rationale quality (by up to 6.6%) across six widely used classification datasets. The term \"vicinal\" is borrowed from vicinal risk minimization (Chapelle et al., 2000); \"vicinal\" means neighboring or adjacent",
    "checked": true,
    "id": "ea21c82e83f1af7edf3e1b07565be6ac8efb9688",
    "semantic_title": "exploring practical gaps in using cross entropy to implement maximum mutual information criterion for rationalization",
    "citation_count": 2,
    "authors": [
      "Wei Liu",
      "Zhiying Deng",
      "Zhongyu Niu",
      "Jun Wang",
      "Haozhao Wang",
      "Ruixuan Li"
    ]
  },
  "https://aclanthology.org/2025.tacl-1.29": {
    "title": "A Comparative Approach for Auditing Multilingual Phonetic Transcript Archives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Farhan Samir",
      "Emily P. Ahn",
      "Shreya Prakash",
      "M√°rton Soskuthy",
      "Vered Shwartz",
      "Jian Zhu"
    ]
  },
  "https://aclanthology.org/2025.tacl-1.30": {
    "title": "Understanding Epistemic Language with a Language-augmented Bayesian Theory of Mind",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lance Ying",
      "Tan Zhi-Xuan",
      "Lionel Wong",
      "Vikash Mansinghka",
      "Joshua B. Tenenbaum"
    ]
  },
  "https://aclanthology.org/2025.tacl-1.31": {
    "title": "Culturally Aware and Adapted NLP: A Taxonomy and a Survey of the State of the Art",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Cecilia Liu",
      "Iryna Gurevych",
      "Anna Korhonen"
    ]
  },
  "https://aclanthology.org/2025.tacl-1.32": {
    "title": "Sense-specific Historical Word Usage Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pierluigi Cassotti",
      "Nina Tahmasebi"
    ]
  },
  "https://aclanthology.org/2025.tacl-1.33": {
    "title": "NLP Security and Ethics, in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heather Lent",
      "Erick Galinkin",
      "Yiyi Chen",
      "Jens Myrup Pedersen",
      "Leon Derczynski",
      "Johannes Bjerva"
    ]
  }
}