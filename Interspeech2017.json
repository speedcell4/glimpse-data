{
  "https://www.isca-speech.org/archive/interspeech_2017/li17_interspeech.html": {
    "title": "ISCA Medal for Scientific Achievement",
    "volume": "main",
    "abstract": "The ISCA Medal for Scientific Achievement 2017 will be awarded to Professor Fumitada Itakura by the President of ISCA during the opening ceremony",
    "checked": true,
    "id": "f9caa3560e50318f82b850d9c254cbe89fd52d93",
    "semantic_title": "isca medal for scientific achievement",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kinnunen17_interspeech.html": {
    "title": "The ASVspoof 2017 Challenge: Assessing the Limits of Replay Spoofing Attack Detection",
    "volume": "main",
    "abstract": "The ASVspoof initiative was created to promote the development of countermeasures which aim to protect automatic speaker verification (ASV) from spoofing attacks. The first community-led, common evaluation held in 2015 focused on countermeasures for speech synthesis and voice conversion spoofing attacks. Arguably, however, it is replay attacks which pose the greatest threat. Such attacks involve the replay of recordings collected from enrolled speakers in order to provoke false alarms and can be mounted with greater ease using everyday consumer devices. ASVspoof 2017, the second in the series, hence focused on the development of replay attack countermeasures. This paper describes the database, protocols and initial findings. The evaluation entailed highly heterogeneous acoustic recording and replay conditions which increased the equal error rate (EER) of a baseline ASV system from 1.76% to 31.46%. Submissions were received from 49 research teams, 20 of which improved upon a baseline replay spoofing detector EER of 24.77%, in terms of replay/non-replay discrimination. While largely successful, the evaluation indicates that the quest for countermeasures which are resilient in the face of variable replay attacks remains very much alive",
    "checked": true,
    "id": "66e1dff1be7ad92104ef33a031b3392517a19b58",
    "semantic_title": "the asvspoof 2017 challenge: assessing the limits of replay spoofing attack detection",
    "citation_count": 443,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/font17_interspeech.html": {
    "title": "Experimental Analysis of Features for Replay Attack Detection — Results on the ASVspoof 2017 Challenge",
    "volume": "main",
    "abstract": "This paper presents an experimental comparison of different features for the detection of replay spoofing attacks in Automatic Speaker Verification systems. We evaluate the proposed countermeasures using two recently introduced databases, including the dataset provided for the ASVspoof 2017 challenge. This challenge provides researchers with a common framework for the evaluation of replay attack detection systems, with a particular focus on the generalization to new, unknown conditions (for instance, replay devices different from those used during system training). Our cross-database experiments show that, although achieving this level of generalization is indeed a challenging task, it is possible to train classifiers that exhibit stable and consistent results across different experiments. The proposed approach for the ASVspoof 2017 challenge consists in the score-level fusion of several base classifiers using logistic regression. These base classifiers are 2-class Gaussian Mixture Models (GMMs) representing genuine and spoofed speech respectively. Our best system achieves an Equal Error Rate of 10.52% on the challenge evaluation set. As a result of this set of experiments, we provide some general conclusions regarding feature extraction for replay attack detection and identify which features show the most promising results",
    "checked": true,
    "id": "2e61d0f2a57365a703ad647d017709c0d1193d2d",
    "semantic_title": "experimental analysis of features for replay attack detection - results on the asvspoof 2017 challenge",
    "citation_count": 115,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/patil17_interspeech.html": {
    "title": "Novel Variable Length Teager Energy Separation Based Instantaneous Frequency Features for Replay Detection",
    "volume": "main",
    "abstract": "Replay attacks presents a great risk for Automatic Speaker Verification (ASV) system. In this paper, we propose a novel replay detector based on Variable length Teager Energy Operator-Energy Separation Algorithm-Instantaneous Frequency Cosine Coefficients (VESA-IFCC) for the ASV spoof 2017 challenge. The key idea here is to exploit the contribution of IF in each subband energy via ESA to capture possible changes in spectral envelope (due to transmission and channel characteristics of replay device) of replayed speech. The IF is computed from narrowband components of speech signal, and DCT is applied in IF to get proposed feature set. We compare the performance of the proposed VESA-IFCC feature set with the features developed for detecting synthetic and voice converted speech. This includes the CQCC, CFCCIF and prosody-based features. On the development set, the proposed VESA-IFCC features when fused at score-level with a variant of CFCCIF and prosody-based features gave the least EER of 0.12%. On the evaluation set, this combination gave an EER of 18.33%. However, post-evaluation results of challenge indicate that VESA-IFCC features alone gave the relatively least EER of 14.06% (i.e., relatively 16.11% less compared to baseline CQCC) and hence, is a very useful countermeasure to detect replay attacks",
    "checked": true,
    "id": "aae57915d9b43f9fdb1286f1b8286b37704d70ea",
    "semantic_title": "novel variable length teager energy separation based instantaneous frequency features for replay detection",
    "citation_count": 89,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/cai17_interspeech.html": {
    "title": "Countermeasures for Automatic Speaker Verification Replay Spoofing Attack : On Data Augmentation, Feature Representation, Classification and Fusion",
    "volume": "main",
    "abstract": "The ongoing ASVspoof 2017 challenge aims to detect replay attacks for text dependent speaker verification. In this paper, we propose multiple replay spoofing countermeasure systems, with some of them boosting the CQCC-GMM baseline system after score level fusion. We investigate different steps in the system building pipeline, including data augmentation, feature representation, classification and fusion. First, in order to augment training data and simulate the unseen replay conditions, we converted the raw genuine training data into replay spoofing data with parametric sound reverberator and phase shifter. Second, we employed the original spectrogram rather than CQCC as input to explore the end-to-end feature representation learning methods. The spectrogram is randomly cropped into fixed size segments, and then fed into a deep residual network (ResNet). Third, upon the CQCC features, we replaced the subsequent GMM classifier with deep neural networks including fully-connected deep neural network (FDNN) and Bidirectional Long Short Term Memory neural network (BLSTM). Experiments showed that data augmentation strategy can significantly improve the system performance. The final fused system achieves to 16.39% EER on the test set of ASVspoof 2017 for the common task",
    "checked": true,
    "id": "3da78ce05b00b16aae951563bb9ec30831d3cb65",
    "semantic_title": "countermeasures for automatic speaker verification replay spoofing attack : on data augmentation, feature representation, classification and fusion",
    "citation_count": 67,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jelil17_interspeech.html": {
    "title": "Spoof Detection Using Source, Instantaneous Frequency and Cepstral Features",
    "volume": "main",
    "abstract": "This work describes the techniques used for spoofed speech detection for the ASVspoof 2017 challenge. The main focus of this work is on exploiting the differences in the speech-specific nature of genuine speech signals and spoofed speech signals generated by replay attacks. This is achieved using glottal closure instants, epoch strength, and the peak to side lobe ratio of the Hilbert envelope of linear prediction residual. Apart from these source features, the instantaneous frequency cosine coefficient feature, and two cepstral features namely, constant Q cepstral coefficients and mel frequency cepstral coefficients are used. A combination of all these features is performed to obtain a high degree of accuracy for spoof detection. Initially, efficacy of these features are tested on the development set of the ASVspoof 2017 database with Gaussian mixture model based systems. The systems are then fused at score level which acts as the final combined system for the challenge. The combined system is able to outperform the individual systems by a significant margin. Finally, the experiments are repeated on the evaluation set of the database and the combined system results in an equal error rate of 13.95%",
    "checked": true,
    "id": "c298f364ec1cc2e81aba3462d3a90751bc6b94ba",
    "semantic_title": "spoof detection using source, instantaneous frequency and cepstral features",
    "citation_count": 92,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/witkowski17_interspeech.html": {
    "title": "Audio Replay Attack Detection Using High-Frequency Features",
    "volume": "main",
    "abstract": "This paper presents our contribution to the ASVspoof 2017 Challenge. It addresses a replay spoofing attack against a speaker recognition system by detecting that the analysed signal has passed through multiple analogue-to-digital (AD) conversions. Specifically, we show that most of the cues that enable to detect the replay attacks can be found in the high-frequency band of the replayed recordings. The described anti-spoofing countermeasures are based on (1) modelling the subband spectrum and (2) using the proposed features derived from the linear prediction (LP) analysis. The results of the investigated methods show a significant improvement in comparison to the baseline system of the ASVspoof 2017 Challenge. A relative equal error rate (EER) reduction by 70% was achieved for the development set and a reduction by 30% was obtained for the evaluation set",
    "checked": true,
    "id": "7e41a8535e4fa2f9a88b0c59c602c2343eaf085d",
    "semantic_title": "audio replay attack detection using high-frequency features",
    "citation_count": 141,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wang17_interspeech.html": {
    "title": "Feature Selection Based on CQCCs for Automatic Speaker Verification Spoofing",
    "volume": "main",
    "abstract": "The ASVspoof 2017 challenge aims to assess spoofing and countermeasures attack detection accuracy for automatic speaker verification. It has been proven that constant Q cepstral coefficients (CQCCs) processes speech in different frequencies with variable resolution and performs much better than traditional features. When coupled with a Gaussian mixture model (GMM), it is an excellently effective spoofing countermeasure. The baseline CQCC+GMM system considers short-term impacts while ignoring the whole influence of channel. In the meanwhile, dimension of the feature is relatively higher than the traditional feature and usually with a higher variance. This paper explores different features for ASVspoof 2017 challenge. The mean and variance of the CQCC features of an utterance is used as the representation of the whole utterance. Feature selection method is introduced to avoid high variance and overfitting for spoofing detection. Experimental results on ASVspoof 2017 dataset show that feature selection followed by Support Vector Machine (SVM) gets an improvement compared to the baseline. It is also shown that pitch feature contributes to the performance improvement, and it obtains a relative improvement of 37.39% over the baseline CQCC+GMM system",
    "checked": true,
    "id": "2b37e26aa92c4433bc4ea54c8d41426e3c6242a5",
    "semantic_title": "feature selection based on cqccs for automatic speaker verification spoofing",
    "citation_count": 33,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ylmaz17_interspeech.html": {
    "title": "Longitudinal Speaker Clustering and Verification Corpus with Code-Switching Frisian-Dutch Speech",
    "volume": "main",
    "abstract": "In this paper, we present a new longitudinal and bilingual broadcast database designed for speaker clustering and text-independent verification research. The broadcast data is extracted from the archives of Omrop Fryslân which is the regional broadcaster in the province of Fryslân, located in the north of the Netherlands. Two speaker verification tasks are provided in a standard enrollment-test setting with language consistent trials. The first task contains target trials from all speakers available appearing in at least two different programs, while the second task contains target trials from a subgroup of speakers appearing in programs recorded in multiple years. The second task is designed to investigate the effects of ageing on the accuracy of speaker verification systems. This database also contains unlabeled spoken segments from different radio programs for speaker clustering research. We provide the output of an existing speaker diarization system for baseline verification experiments. Finally, we present the baseline speaker verification results using the Kaldi GMM- and DNN-UBM speaker verification system. This database will be an extension to the recently presented open source Frisian data collection and it is publicly available for research purposes",
    "checked": true,
    "id": "efd7f517a1f1f61b39e57a8582fd692ae8c97d6b",
    "semantic_title": "longitudinal speaker clustering and verification corpus with code-switching frisian-dutch speech",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ylmaz17b_interspeech.html": {
    "title": "Exploiting Untranscribed Broadcast Data for Improved Code-Switching Detection",
    "volume": "main",
    "abstract": "We have recently presented an automatic speech recognition (ASR) system operating on Frisian-Dutch code-switched speech. This type of speech requires careful handling of unexpected language switches that may occur in a single utterance. In this paper, we extend this work by using some raw broadcast data to improve multilingually trained deep neural networks (DNN) that have been trained on 11.5 hours of manually annotated bilingual speech. For this purpose, we apply the initial ASR to the untranscribed broadcast data and automatically create transcriptions based on the recognizer output using different language models for rescoring. Then, we train new acoustic models on the combined data, i.e., the manually and automatically transcribed bilingual broadcast data, and investigate the automatic transcription quality based on the recognition accuracies on a separate set of development and test data. Finally, we report code-switching detection performance elaborating on the correlation between the ASR and the code-switching detection performance",
    "checked": true,
    "id": "0f93bff2437383e29bd1c2f14de1d5377d6d35f3",
    "semantic_title": "exploiting untranscribed broadcast data for improved code-switching detection",
    "citation_count": 12,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ramanarayanan17_interspeech.html": {
    "title": "Jee haan, I'd like both, por favor: Elicitation of a Code-Switched Corpus of Hindi–English and Spanish–English Human–Machine Dialog",
    "volume": "main",
    "abstract": "We present a database of code-switched conversational human–machine dialog in English–Hindi and English–Spanish. We leveraged HALEF, an open-source standards-compliant cloud-based dialog system to capture audio and video of bilingual crowd workers as they interacted with the system. We designed conversational items with intra-sentential code-switched machine prompts, and examine its efficacy in eliciting code-switched speech in a total of over 700 dialogs. We analyze various characteristics of the code-switched corpus and discuss some considerations that should be taken into account while collecting and processing such data. Such a database can be leveraged for a wide range of potential applications, including automated processing, recognition and understanding of code-switched speech and language learning applications for new language learners",
    "checked": true,
    "id": "bce75eb27fada3cb4e3777da0f9d4e35631b51a7",
    "semantic_title": "jee haan, i'd like both, por favor: elicitation of a code-switched corpus of hindi-english and spanish-english human-machine dialog",
    "citation_count": 17,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/rallabandi17_interspeech.html": {
    "title": "On Building Mixed Lingual Speech Synthesis Systems",
    "volume": "main",
    "abstract": "Codemixing — phenomenon where lexical items from one language are embedded in the utterance of another — is relatively frequent in multilingual communities. However, TTS systems today are not fully capable of effectively handling such mixed content despite achieving high quality in the monolingual case. In this paper, we investigate various mechanisms for building mixed lingual systems which are built using a mixture of monolingual corpora and are capable of synthesizing such content. First, we explore the possibility of manipulating the phoneme representation: using target word to source phone mapping with the aim of emulating the native speaker intuition. We then present experiments at the acoustic stage investigating training techniques at both spectral and prosodic levels. Subjective evaluation shows that our systems are capable of generating high quality synthesis in codemixed scenarios",
    "checked": true,
    "id": "02a20ed2182475b40a4e7744aa6555607adffa62",
    "semantic_title": "on building mixed lingual speech synthesis systems",
    "citation_count": 19,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chandu17_interspeech.html": {
    "title": "Speech Synthesis for Mixed-Language Navigation Instructions",
    "volume": "main",
    "abstract": "Text-to-Speech (TTS) systems that can read navigation instructions are one of the most widely used speech interfaces today. Text in the navigation domain may contain named entities such as location names that are not in the language that the TTS database is recorded in. Moreover, named entities can be compound words where individual lexical items belong to different languages. These named entities may be transliterated into the script that the TTS system is trained on. This may result in incorrect pronunciation rules being used for such words. We describe experiments to extend our previous work in generating code-mixed speech to synthesize navigation instructions, with a mixed-lingual TTS system. We conduct subjective listening tests with two sets of users, one being students who are native speakers of an Indian language and very proficient in English, and the other being drivers with low English literacy, but familiarity with location names. We find that in both sets of users, there is a significant preference for our proposed system over a baseline system that synthesizes instructions in English",
    "checked": true,
    "id": "99f07e194197a55fd017657d4cd1a8d9c349de05",
    "semantic_title": "speech synthesis for mixed-language navigation instructions",
    "citation_count": 19,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/amazouz17_interspeech.html": {
    "title": "Addressing Code-Switching in French/Algerian Arabic Speech",
    "volume": "main",
    "abstract": "This study focuses on code-switching (CS) in French/Algerian Arabic bilingual communities and investigates how speech technologies, such as automatic data partitioning, language identification and automatic speech recognition (ASR) can serve to analyze and classify this type of bilingual speech. A preliminary study carried out using a corpus of Maghrebian broadcast data revealed a relatively high presence of CS Algerian Arabic as compared to the neighboring countries Morocco and Tunisia. Therefore this study focuses on code switching produced by bilingual Algerian speakers who can be considered native speakers of both Algerian Arabic and French. A specific corpus of four hours of speech from 8 bilingual French Algerian speakers was collected. This corpus contains read speech and conversational speech in both languages and includes stretches of code-switching. We provide a linguistic description of the code-switching stretches in terms of intra-sentential and inter-sentential switches, the speech duration in each language. We report on some initial studies to locate French, Arabic and the code-switched stretches, using ASR system word posteriors for this pair of languages",
    "checked": true,
    "id": "c935bcc748c4e9b87420deaf23074535592950d0",
    "semantic_title": "addressing code-switching in french/algerian arabic speech",
    "citation_count": 48,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/guzman17_interspeech.html": {
    "title": "Metrics for Modeling Code-Switching Across Corpora",
    "volume": "main",
    "abstract": "In developing technologies for code-switched speech, it would be desirable to be able to predict how much language mixing might be expected in the signal and the regularity with which it might occur. In this work, we offer various metrics that allow for the classification and visualization of multilingual corpora according to the ratio of languages represented, the probability of switching between them, and the time-course of switching. Applying these metrics to corpora of different languages and genres, we find that they display distinct probabilities and periodicities of switching, information useful for speech processing of mixed-language data",
    "checked": true,
    "id": "25a5cf5c7dc2269cf67d98b2fb46317a4d16b581",
    "semantic_title": "metrics for modeling code-switching across corpora",
    "citation_count": 55,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/westhuizen17_interspeech.html": {
    "title": "Synthesising isiZulu-English Code-Switch Bigrams Using Word Embeddings",
    "volume": "main",
    "abstract": "Code-switching is prevalent among South African speakers, and presents a challenge to automatic speech recognition systems. It is predominantly a spoken phenomenon, and generally does not occur in textual form. Therefore a particularly serious challenge is the extreme lack of training material for language modelling. We investigate the use of word embeddings to synthesise isiZulu-to-English code-switch bigrams with which to augment such sparse language model training data. A variety of word embeddings are trained on a monolingual English web text corpus, and subsequently queried to synthesise code-switch bigrams. Our evaluation is performed on language models trained on a new, although small, English-isiZulu code-switch corpus compiled from South African soap operas. This data is characterised by fast, spontaneously spoken speech containing frequent code-switching. We show that the augmentation of the training data with code-switched bigrams synthesised in this way leads to a reduction in perplexity",
    "checked": true,
    "id": "7b263493401762ede1f3a1f1e134e3c0e8584ed8",
    "semantic_title": "synthesising isizulu-english code-switch bigrams using word embeddings",
    "citation_count": 19,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/soto17_interspeech.html": {
    "title": "Crowdsourcing Universal Part-of-Speech Tags for Code-Switching",
    "volume": "main",
    "abstract": "Code-switching is the phenomenon by which bilingual speakers switch between multiple languages during communication. The importance of developing language technologies for code-switching data is immense, given the large populations that routinely code-switch. High-quality linguistic annotations are extremely valuable for any NLP task, and performance is often limited by the amount of high-quality labeled data. However, little such data exists for code-switching. In this paper, we describe crowd-sourcing universal part-of-speech tags for the Miami Bangor Corpus of Spanish-English code-switched speech. We split the annotation task into three subtasks: one in which a subset of tokens are labeled automatically, one in which questions are specifically designed to disambiguate a subset of high frequency words, and a more general cascaded approach for the remaining data in which questions are displayed to the worker following a decision tree structure. Each subtask is extended and adapted for a multilingual setting and the universal tagset. The quality of the annotation process is measured using hidden check questions annotated with gold labels. The overall agreement between gold standard labels and the majority vote is between 0.95 and 0.96 for just three labels and the average recall across part-of-speech tags is between 0.87 and 0.99, depending on the task",
    "checked": true,
    "id": "6a4e10bcbb86e5ea55f3dfbfd269dccba62b5db9",
    "semantic_title": "crowdsourcing universal part-of-speech tags for code-switching",
    "citation_count": 15,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lavrentyeva17_interspeech.html": {
    "title": "Audio Replay Attack Detection with Deep Learning Frameworks",
    "volume": "main",
    "abstract": "Nowadays spoofing detection is one of the priority research areas in the field of automatic speaker verification. The success of Automatic Speaker Verification Spoofing and Countermeasures (ASVspoof) Challenge 2015 confirmed the impressive perspective in detection of unforeseen spoofing trials based on speech synthesis and voice conversion techniques. However, there is a small number of researches addressed to replay spoofing attacks which are more likely to be used by non-professional impersonators. This paper describes the Speech Technology Center (STC) anti-spoofing system submitted for ASVspoof 2017 which is focused on replay attacks detection. Here we investigate the efficiency of a deep learning approach for solution of the mentioned-above task. Experimental results obtained on the Challenge corpora demonstrate that the selected approach outperforms current state-of-the-art baseline systems in terms of spoofing detection quality. Our primary system produced an EER of 6.73% on the evaluation part of the corpora which is 72% relative improvement over the ASVspoof 2017 baseline system",
    "checked": true,
    "id": "a2b4c396dc1064fb90bb5455525733733c761a7f",
    "semantic_title": "audio replay attack detection with deep learning frameworks",
    "citation_count": 248,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ji17_interspeech.html": {
    "title": "Ensemble Learning for Countermeasure of Audio Replay Spoofing Attack in ASVspoof2017",
    "volume": "main",
    "abstract": "To enhance the security and reliability of automatic speaker verification (ASV) systems, ASVspoof 2017 challenge focuses on the detection problem of known and unknown audio replay attacks. We proposed an ensemble learning classifier for CNCB team's submitted system scores, which across uses a variety of acoustic features and classifiers. An effective post-processing method is studied to improve the performance of Constant Q cepstral coefficients (CQCC) and to form a base feature set with some other classical acoustic features. We also proposed using an ensemble classifier set, which includes multiple Gaussian Mixture Model (GMM) based classifiers and two novel GMM mean supervector-Gradient Boosting Decision Tree (GSV-GBDT) and GSV-Random Forest (GSV-RF) classifiers. Experimental results have shown that the proposed ensemble learning system can provide substantially better performance than baseline. On common training condition of the challenge, Equal Error Rate (EER) of primary system on development set is 1.5%, compared to baseline 10.4%. EER of primary system (S02 in ASVspoof 2017 board) on evaluation data set are 12.3% (with only train dataset) and 10.8% (with train+dev dataset), which are also much better than baseline 30.6% and 24.8%, given by ASVSpoof 2017 organizer, with 59.7% and 56.4% relative performance improvement",
    "checked": true,
    "id": "8a862946febf154effe65d4ec94d8e6bd39f690d",
    "semantic_title": "ensemble learning for countermeasure of audio replay spoofing attack in asvspoof2017",
    "citation_count": 47,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/li17b_interspeech.html": {
    "title": "A Study on Replay Attack and Anti-Spoofing for Automatic Speaker Verification",
    "volume": "main",
    "abstract": "For practical automatic speaker verification (ASV) systems, replay attack poses a true risk. By replaying a pre-recorded speech signal of the genuine speaker, ASV systems tend to be easily fooled. An effective replay detection method is therefore highly desirable. In this study, we investigate a major difficulty in replay detection: the over-fitting problem caused by variability factors in speech signal. An F-ratio probing tool is proposed and three variability factors are investigated using this tool: speaker identity, speech content and playback & recording device. The analysis shows that device is the most influential factor that contributes the highest over-fitting risk. A frequency warping approach is studied to alleviate the over-fitting problem, as verified on the ASV-spoof 2017 database",
    "checked": true,
    "id": "466196986827959c10dfddce40202ad3e8341968",
    "semantic_title": "a study on replay attack and anti-spoofing for automatic speaker verification",
    "citation_count": 51,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/nagarsheth17_interspeech.html": {
    "title": "Replay Attack Detection Using DNN for Channel Discrimination",
    "volume": "main",
    "abstract": "Voice is projected to be the next input interface for portable devices. The increased use of audio interfaces can be mainly attributed to the success of speech and speaker recognition technologies. With these advances comes the risk of criminal threats where attackers are reportedly trying to access sensitive information using diverse voice spoofing techniques. Among them, replay attacks pose a real challenge to voice biometrics. This paper addresses the problem by proposing a deep learning architecture in tandem with low-level cepstral features. We investigate the use of a deep neural network (DNN) to discriminate between the different channel conditions available in the ASVSpoof 2017 dataset, namely recording, playback and session conditions. The high-level feature vectors derived from this network are used to discriminate between genuine and spoofed audio. Two kinds of low-level features are utilized: state-of-the-art constant-Q cepstral coefficients (CQCC), and our proposed high-frequency cepstral coefficients (HFCC) that derive from the high-frequency spectrum of the audio. The fusion of both features proved to be effective in generalizing well across diverse replay attacks seen in the evaluation of the ASVSpoof 2017 challenge, with an equal error rate of 11.5%, that is 53% better than the baseline Gaussian Mixture Model (GMM) applied on CQCC",
    "checked": true,
    "id": "9089817cb2a7eaf1d5835034fa5ba3f76f375cd4",
    "semantic_title": "replay attack detection using dnn for channel discrimination",
    "citation_count": 104,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17_interspeech.html": {
    "title": "ResNet and Model Fusion for Automatic Spoofing Detection",
    "volume": "main",
    "abstract": "Speaker verification systems have achieved great progress in recent years. Unfortunately, they are still highly prone to different kinds of spoofing attacks such as speech synthesis, voice conversion, and fake audio recordings etc. Inspired by the success of ResNet in image recognition, we investigated the effectiveness of using ResNet for automatic spoofing detection. Experimental results on the ASVspoof2017 data set show that ResNet performs the best among all the single-model systems. Model fusion is a good way to further improve the system performance. Nevertheless, we found that if the same feature is used for different fused models, the resulting system can hardly be improved. By using different features and models, our best fused model further reduced the Equal Error Rate (EER) by 18% relatively, compared with the best single-model system",
    "checked": true,
    "id": "6a7b88c8dc37850f8ffe48dcf7d839c6f0d47873",
    "semantic_title": "resnet and model fusion for automatic spoofing detection",
    "citation_count": 128,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/alluri17_interspeech.html": {
    "title": "SFF Anti-Spoofer: IIIT-H Submission for Automatic Speaker Verification Spoofing and Countermeasures Challenge 2017",
    "volume": "main",
    "abstract": "The ASVspoof 2017 challenge is about the detection of replayed speech from human speech. The proposed system makes use of the fact that when the speech signals are replayed, they pass through multiple channels as opposed to original recordings. This channel information is typically embedded in low signal to noise ratio regions. A speech signal processing method with high spectro-temporal resolution is required to extract robust features from such regions. The single frequency filtering (SFF) is one such technique, which we propose to use for replay attack detection. While SFF based feature representation was used at front-end, Gaussian mixture model and bi-directional long short-term memory models are investigated at the backend as classifiers. The experimental results on ASVspoof 2017 dataset reveal that, SFF based representation is very effective in detecting replay attacks. The score level fusion of back end classifiers further improved the performance of the system which indicates that both classifiers capture complimentary information",
    "checked": true,
    "id": "249e6da34b4218a3960e65f92c4c554ede62a412",
    "semantic_title": "sff anti-spoofer: iiit-h submission for automatic speaker verification spoofing and countermeasures challenge 2017",
    "citation_count": 57,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hartmann17_interspeech.html": {
    "title": "Improved Single System Conversational Telephone Speech Recognition with VGG Bottleneck Features",
    "volume": "main",
    "abstract": "On small datasets, discriminatively trained bottleneck features from deep networks commonly outperform more traditional spectral or cepstral features. While these features are typically trained with small, fully-connected networks, recent studies have used more sophisticated networks with great success. We use the recent deep CNN (VGG) network for bottleneck feature extraction — previously used only for low-resource tasks — and apply it to the Switchboard English conversational telephone speech task. Unlike features derived from traditional MLP networks, the VGG features outperform cepstral features even when used with BLSTM acoustic models trained on large amounts of data. We achieve the best BBN single system performance when combining the VGG features with a BLSTM acoustic model. When decoding with an n-gram language model, which are used for deployable systems, we have a realistic production system with a WER of 7.4%. This result is competitive with the current state-of-the-art in the literature. While our focus is on realistic single system performance, we further reduce the WER to 6.1% through system combination and using expensive neural network language model rescoring",
    "checked": true,
    "id": "850730adb72d900c9b484829c42f07c20e9af06d",
    "semantic_title": "improved single system conversational telephone speech recognition with vgg bottleneck features",
    "citation_count": 11,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wong17_interspeech.html": {
    "title": "Student-Teacher Training with Diverse Decision Tree Ensembles",
    "volume": "main",
    "abstract": "Student-teacher training allows a large teacher model or ensemble of teachers to be compressed into a single student model, for the purpose of efficient decoding. However, current approaches in automatic speech recognition assume that the state clusters, often defined by Phonetic Decision Trees (PDT), are the same across all models. This limits the diversity that can be captured within the ensemble, and also the flexibility when selecting the complexity of the student model output. This paper examines an extension to student-teacher training that allows for the possibility of having different PDTs between teachers, and also for the student to have a different PDT from the teacher. The proposal is to train the student to emulate the logical context dependent state posteriors of the teacher, instead of the frame posteriors. This leads to a method of mapping frame posteriors from one PDT to another. This approach is evaluated on three speech recognition tasks: the Tok Pisin and Javanese low resource conversational telephone speech tasks from the IARPA Babel programme, and the HUB4 English broadcast news task",
    "checked": true,
    "id": "15a1e3afa2e91477f21c3ddcb1fc58a58b6d772f",
    "semantic_title": "student-teacher training with diverse decision tree ensembles",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/cui17_interspeech.html": {
    "title": "Embedding-Based Speaker Adaptive Training of Deep Neural Networks",
    "volume": "main",
    "abstract": "An embedding-based speaker adaptive training (SAT) approach is proposed and investigated in this paper for deep neural network acoustic modeling. In this approach, speaker embedding vectors, which are a constant given a particular speaker, are mapped through a control network to layer-dependent element-wise affine transformations to canonicalize the internal feature representations at the output of hidden layers of a main network. The control network for generating the speaker-dependent mappings are jointly estimated with the main network for the overall speaker adaptive acoustic modeling. Experiments on large vocabulary continuous speech recognition (LVCSR) tasks show that the proposed SAT scheme can yield superior performance over the widely-used speaker-aware training using i-vectors with speaker-adapted input features",
    "checked": true,
    "id": "0a0820133e29e8039e9eff70aaeb4ecf76cdb25c",
    "semantic_title": "embedding-based speaker adaptive training of deep neural networks",
    "citation_count": 37,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ma17_interspeech.html": {
    "title": "Improving Deliverable Speech-to-Text Systems with Multilingual Knowledge Transfer",
    "volume": "main",
    "abstract": "This paper reports our recent progress on using multilingual data for improving speech-to-text (STT) systems that can be easily delivered. We continued the work BBN conducted on the use of multilingual data for improving Babel evaluation systems, but focused on training time-delay neural network (TDNN) based chain models. As done for the Babel evaluations, we used multilingual data in two ways: first, to train multilingual deep neural networks (DNN) for extracting bottle-neck (BN) features, and second, for initializing training on target languages Our results show that TDNN chain models trained on multilingual DNN bottleneck features yield significant gains over their counterparts trained on MFCC plus i-vector features. By initializing from models trained on multilingual data, TDNN chain models can achieve great improvements over random initializations of the network weights on target languages. Two other important findings are: 1) initialization with multilingual TDNN chain models produces larger gains on target languages that have less training data; 2) inclusion of target languages in multilingual training for either BN feature extraction or initialization have limited impact on performance measured on the target languages. Our results also reveal that for TDNN chain models, the combination of multilingual BN features and multilingual initialization achieves the best performance on all target languages",
    "checked": true,
    "id": "661da1a08bba29ec88c454c8913dbd828cf69f4c",
    "semantic_title": "improving deliverable speech-to-text systems with multilingual knowledge transfer",
    "citation_count": 12,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/saon17_interspeech.html": {
    "title": "English Conversational Telephone Speech Recognition by Humans and Machines",
    "volume": "main",
    "abstract": "Word error rates on the Switchboard conversational corpus that just a few years ago were 14% have dropped to 8.0%, then 6.6% and most recently 5.8%, and are now believed to be within striking range of human performance. This then raises two issues: what is human performance, and how far down can we still drive speech recognition error rates? In trying to assess human performance, we performed an independent set of measurements on the Switchboard and CallHome subsets of the Hub5 2000 evaluation and found that human accuracy may be considerably better than what was earlier reported, giving the community a significantly harder goal to achieve. We also report on our own efforts in this area, presenting a set of acoustic and language modeling techniques that lowered the WER of our system to 5.5%/10.3% on these subsets, which is a new performance milestone (albeit not at what we measure to be human performance). On the acoustic side, we use a score fusion of one LSTM with multiple feature inputs, a second LSTM trained with speaker-adversarial multi-task learning and a third convolutional residual net (ResNet). On the language modeling side, we use word and character LSTMs and convolutional WaveNet-style language models",
    "checked": true,
    "id": "c9bd15c7838c1d3cdd5f5113a2efd9440f86b3da",
    "semantic_title": "english conversational telephone speech recognition by humans and machines",
    "citation_count": 350,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/stolcke17_interspeech.html": {
    "title": "Comparing Human and Machine Errors in Conversational Speech Transcription",
    "volume": "main",
    "abstract": "Recent work in automatic recognition of conversational telephone speech (CTS) has achieved accuracy levels comparable to human transcribers, although there is some debate how to precisely quantify human performance on this task, using the NIST 2000 CTS evaluation set. This raises the question what systematic differences, if any, may be found differentiating human from machine transcription errors. In this paper we approach this question by comparing the output of our most accurate CTS recognition system to that of a standard speech transcription vendor pipeline. We find that the most frequent substitution, deletion and insertion error types of both outputs show a high degree of overlap. The only notable exception is that the automatic recognizer tends to confuse filled pauses (\"uh\") and backchannel acknowledgments (\"uhhuh\"). Human tend not to make this error, presumably due to the distinctive and opposing pragmatic functions attached to these words. Furthermore, we quantify the correlation between human and machine errors at the speaker level, and investigate the effect of speaker overlap between training and test data. Finally, we report on an informal \"Turing test\" asking humans to discriminate between automatic and human transcription error cases",
    "checked": true,
    "id": "002b9e38bf1f82a2b876082cb866c67e8ace2ba1",
    "semantic_title": "comparing human and machine errors in conversational speech transcription",
    "citation_count": 54,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/petukhova17_interspeech.html": {
    "title": "Multimodal Markers of Persuasive Speech: Designing a Virtual Debate Coach",
    "volume": "main",
    "abstract": "The study presented in this paper is carried out to support debate performance assessment in the context of debate skills training. The perception of good performance as a debater is influenced by how believable and convincing the debater's argumentation is. We identified a number of features that are useful for explaining perceived properties of persuasive speech and for defining rules and strategies to produce and assess debate performance. We collected and analysed multimodal and multisensory data of the trainees debate behaviour, and contrasted it with those of skilled professional debaters. Observational, correlation and machine learning studies were performed to identify multimodal markers of persuasive speech and link them to experts' assessments. A combination of multimodal in- and out-of-domain debate data, and various non-verbal, prosodic, lexical, linguistic and structural features has been computed based on our analysis and assessed used to , and several classification procedures has been applied achieving an accuracy of 0.79 on spoken debate data",
    "checked": true,
    "id": "814f31f7dbaf5f6341407cc9223d72ba4ff6c5ae",
    "semantic_title": "multimodal markers of persuasive speech: designing a virtual debate coach",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bone17_interspeech.html": {
    "title": "Acoustic-Prosodic and Physiological Response to Stressful Interactions in Children with Autism Spectrum Disorder",
    "volume": "main",
    "abstract": "Social anxiety is a prevalent condition affecting individuals to varying degrees. Research on autism spectrum disorder (ASD), a group of neurodevelopmental disorders marked by impairments in social communication, has found that social anxiety occurs more frequently in this population. Our study aims to further understand the multimodal manifestation of social stress for adolescents with ASD versus neurotypically developing (TD) peers. We investigate this through objective measures of speech behavior and physiology (mean heart rate) acquired during three tasks: a low-stress conversation, a medium-stress interview, and a high-stress presentation. Measurable differences are found to exist for speech behavior and heart rate in relation to task-induced stress. Additionally, we find the acoustic measures are particularly effective for distinguishing between diagnostic groups. Individuals with ASD produced higher prosodic variability, agreeing with previous reports. Moreover, the most informative features captured an individual's vocal changes between low and high social-stress, suggesting an interaction between vocal production and social stressors in ASD",
    "checked": true,
    "id": "4cec5b4afd01f59558942db3fe13e697f85a2c67",
    "semantic_title": "acoustic-prosodic and physiological response to stressful interactions in children with autism spectrum disorder",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/burmania17_interspeech.html": {
    "title": "A Stepwise Analysis of Aggregated Crowdsourced Labels Describing Multimodal Emotional Behaviors",
    "volume": "main",
    "abstract": "Affect recognition is a difficult problem that most often relies on human annotated data to train automated systems. As humans perceive emotion differently based on personality, cognitive state and past experiences, it is important to collect rankings from multiple individuals to assess the emotional content in corpora, which are later aggregated with rules such as majority vote. With the increased use of crowdsourcing services for perceptual evaluations, collecting large amount of data is now feasible. It becomes important to question the amount of data needed to create well-trained classifiers. How different are the aggregated labels collected from five raters compared to the ones obtained from twenty evaluators? Is it worthwhile to spend resources to increase the number of evaluators beyond those used in conventional/laboratory studies? This study evaluates the consensus labels obtained by incrementally adding new evaluators during perceptual evaluations. Using majority vote over categorical emotional labels, we compare the changes in the aggregated labels starting with one rater, and finishing with 20 raters. The large number of evaluators in a subset of the MSP-IMPROV database and the ability to filter annotators by quality allows us to better understand label aggregation as a function of the number of annotators",
    "checked": true,
    "id": "34a45c92139a9ffd0c76f8eee2201485232bdb20",
    "semantic_title": "a stepwise analysis of aggregated crowdsourced labels describing multimodal emotional behaviors",
    "citation_count": 12,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/fotedar17_interspeech.html": {
    "title": "An Information Theoretic Analysis of the Temporal Synchrony Between Head Gestures and Prosodic Patterns in Spontaneous Speech",
    "volume": "main",
    "abstract": "We analyze the temporal co-ordination between head gestures and prosodic patterns in spontaneous speech in a data-driven manner. For this study, we consider head motion and speech data from 24 subjects while they tell a fixed set of five stories. The head motion, captured using a motion capture system, is converted to Euler angles and translations in X, Y and Z-directions to represent head gestures. Pitch and short-time energy in voiced segments are used to represent the prosodic patterns. To capture the statistical relationship between head gestures and prosodic patterns, mutual information (MI) is computed at various delays between the two using data from 24 subjects in six native languages. The estimated MI, averaged across all subjects, is found to be maximum when the head gestures lag the prosodic patterns by 30msec. This is found to be true when subjects tell stories in English as well as in their native language. We observe a similar pattern in the root mean squared error of predicting head gestures from prosodic patterns using Gaussian mixture model. These results indicate that there could be an asynchrony between head gestures and prosody during spontaneous speech where head gestures follow the corresponding prosodic patterns",
    "checked": true,
    "id": "7292a91864be55f734a7088be70594db075e8d00",
    "semantic_title": "an information theoretic analysis of the temporal synchrony between head gestures and prosodic patterns in spontaneous speech",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/huang17_interspeech.html": {
    "title": "Multimodal Prediction of Affective Dimensions via Fusing Multiple Regression Techniques",
    "volume": "main",
    "abstract": "This paper presents a multimodal approach to predict affective dimensions, that makes full use of features from audio, video, Electrodermal Activity (EDA) and Electrocardiogram (ECG) using three regression techniques such as support vector regression (SVR), partial least squares regression (PLS), and a deep bidirectional long short-term memory recurrent neural network (DBLSTM-RNN) regression. Each of the three regression techniques performs multimodal affective dimension prediction followed by a fusion of different models on features of four modalities using a support vector regression. A support vector regression is also applied for a final fusion of the three regression systems. Experiments show that our proposed approach obtains promising results on the AVEC 2015 benchmark dataset for prediction of multimodal affective dimensions. For the development set, the concordance correlation coefficient (CCC) achieves results of 0.856 for arousal and 0.720 for valence, which increases 3.88% and 4.66% of the top-performer of AVEC 2015 in arousal and valence, respectively",
    "checked": true,
    "id": "dad58628834572e085c815b2601171187602e3b4",
    "semantic_title": "multimodal prediction of affective dimensions via fusing multiple regression techniques",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/dohen17_interspeech.html": {
    "title": "Co-Production of Speech and Pointing Gestures in Clear and Perturbed Interactive Tasks: Multimodal Designation Strategies",
    "volume": "main",
    "abstract": "Designation consists in attracting an interlocutor's attention on a specific object and/or location. It is most often achieved using both speech (e.g., demonstratives) and gestures (e.g., manual pointing). This study aims at analyzing how speech and pointing gestures are co-produced in a semi-directed interactive task involving designation. 20 native speakers of French were involved in a cooperative task in which they provided instructions to a partner for her to reproduce a model she could not see on a grid both of them saw. They had to use only sentences of the form ‘The [target word] goes there.'. They did this in two conditions: silence and noise. Their speech and articulatory/hand movements (motion capture) were recorded. The analyses show that the participants' speech features were modified in noise (Lombard effect). They also spoke slower and made more pauses and errors. Their pointing gestures lasted longer and started later showing an adaptation of gesture production to speech. The condition did not influence speech/gesture coordination. The apex (part of the gesture that shows) mainly occurred at the same time as the target word and not as the demonstrative showing that speakers group speech and gesture carrying complementary rather than redundant information",
    "checked": true,
    "id": "d80b346673bb9c4db02e8aca4935b218941af9a3",
    "semantic_title": "co-production of speech and pointing gestures in clear and perturbed interactive tasks: multimodal designation strategies",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/guzewich17_interspeech.html": {
    "title": "Improving Speaker Verification for Reverberant Conditions with Deep Neural Network Dereverberation Processing",
    "volume": "main",
    "abstract": "We present an improved method for training Deep Neural Networks for dereverberation and show that it can improve performance for the speech processing tasks of speaker verification and speech enhancement. We replicate recently proposed methods for dereverberation using Deep Neural Networks and present our improved method, highlighting important aspects that influence performance. We then experimentally evaluate the capabilities and limitations of the method with respect to speech quality and speaker verification to show that ours achieves better performance than other proposed methods",
    "checked": true,
    "id": "5fad2d973e1a1ec3903d38b999fbda27342560ef",
    "semantic_title": "improving speaker verification for reverberant conditions with deep neural network dereverberation processing",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bulling17_interspeech.html": {
    "title": "Stepsize Control for Acoustic Feedback Cancellation Based on the Detection of Reverberant Signal Periods and the Estimated System Distance",
    "volume": "main",
    "abstract": "A new approach for acoustic feedback cancellation is presented. The challenge in acoustic feedback cancellation is a strong correlation between the local speech and the loudspeaker signal. Due to this correlation, the convergence rate of adaptive algorithms is limited. Therefore, a novel stepsize control of the adaptive filter is presented. The stepsize control exploits reverberant signal periods to update the adaptive filter. As soon as local speech stops, the reverberation energy of the system decays exponentially. This means that during reverberation there is only excitation of the filter but no local speech. Thus, signals are not correlated and the filter can converge without correlation problems. Consequently, the stepsize control accelerates the adaption process during reverberation and slows it down at the beginning of speech activity. It is shown, that with a particular gain control, the reverb-based stepsize control can be interpreted as the theoretical optimum stepsize. However, for this purpose a precise estimation of the system distance is required. One estimation method is presented. The proposed estimator has a rescue mechanism to detect enclosure dislocations. Both, simulations and real world testing show that the acoustic feedback canceler is capable of improving stability and convergence rate, even at high system gains",
    "checked": true,
    "id": "36a39db98d31c8aabbd6852345edd63898a55c2e",
    "semantic_title": "stepsize control for acoustic feedback cancellation based on the detection of reverberant signal periods and the estimated system distance",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/franzen17_interspeech.html": {
    "title": "A Delay-Flexible Stereo Acoustic Echo Cancellation for DFT-Based In-Car Communication (ICC) Systems",
    "volume": "main",
    "abstract": "In-car communication (ICC) systems supporting speech communication in noise by reproducing amplified speech from the car cabin in the car cabin ask for low-delay acoustic echo cancellation (AEC). In this paper we propose a delay-flexible DFT-based stereo AEC capable of cancelling also the echoes stemming from the audio player or FM radio. For the price of a somewhat higher complexity we are able to reduce the 32 ms delay of the baseline down to 4 ms, loosing only 1 dB in ERLE while even preserving system distance properties",
    "checked": true,
    "id": "bc0f522aa0e225a2bc83ba46c4fd7570c9b51cd6",
    "semantic_title": "a delay-flexible stereo acoustic echo cancellation for dft-based in-car communication (icc) systems",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wang17b_interspeech.html": {
    "title": "Speech Enhancement Based on Harmonic Estimation Combined with MMSE to Improve Speech Intelligibility for Cochlear Implant Recipients",
    "volume": "main",
    "abstract": "In this paper, a speech enhancement algorithm is proposed to improve the speech intelligibility for cochlear implant recipients. Our method is based on combination of harmonic estimation and traditional statistical method. Traditional statistical based speech enhancement method is effective only for stationary noise suppression, but not non-stationary noise. To address more complex noise scenarios, we explore the harmonic structure of target speech to obtain a more accurate noise estimation. The estimated noise is then employed in the MMSE framework to obtain the gain function for recovering the target speech. Listening test experiments show a substantial speech intelligibility improvement for cochlear implant recipients in noisy environments",
    "checked": true,
    "id": "1e118317c8b26448b13559e0dd69010cab2aa954",
    "semantic_title": "speech enhancement based on harmonic estimation combined with mmse to improve speech intelligibility for cochlear implant recipients",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ayllon17_interspeech.html": {
    "title": "Improving Speech Intelligibility in Binaural Hearing Aids by Estimating a Time-Frequency Mask with a Weighted Least Squares Classifier",
    "volume": "main",
    "abstract": "An efficient algorithm for speech enhancement in binaural hearing aids is proposed. The algorithm is based on the estimation of a time-frequency mask using supervised machine learning. The standard least-squares linear classifier is reformulated to optimize a metric related to speech/noise separation. The method is energy-efficient in two ways: the computational complexity is limited and the wireless data transmission optimized. The ability of the algorithm to enhance speech contaminated with different types of noise and low SNR has been evaluated. Objective measures of speech intelligibility and speech quality demonstrate that the algorithm increments both the hearing comfort and speech understanding of the user. These results are supported by subjective listening tests",
    "checked": true,
    "id": "616054b72d505b539a3b9ce2a17e485123505fa2",
    "semantic_title": "improving speech intelligibility in binaural hearing aids by estimating a time-frequency mask with a weighted least squares classifier",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wu17_interspeech.html": {
    "title": "Simulations of High-Frequency Vocoder on Mandarin Speech Recognition for Acoustic Hearing Preserved Cochlear Implant",
    "volume": "main",
    "abstract": "Vocoder simulations are generally adopted to simulate the electrical hearing induced by the cochlear implant (CI). Our research group is developing a new four-electrode CI microsystem which induces high-frequency electrical hearing while preserving low-frequency acoustic hearing. To simulate the functionality of this CI, a previously developed hearing-impaired (HI) hearing model is combined with a 4-channel vocoder in this paper to respectively mimic the perceived acoustic hearing and electrical hearing. Psychoacoustic experiments are conducted on Mandarin speech recognition for determining parameters of electrodes for this CI. Simulation results show that initial consonants of Mandarin are more difficult to recognize than final vowels of Mandarin via acoustic hearing of HI patients. After electrical hearing being induced through logarithmic-frequency distributed electrodes, speech intelligibility of HI patients is boosted for all Mandarin phonemes, especially for initial consonants. Similar results are consistently observed in clean and noisy test conditions",
    "checked": true,
    "id": "6dfe77ccbea7e0eef743e5d6a4e8af58a2b2ac37",
    "semantic_title": "simulations of high-frequency vocoder on mandarin speech recognition for acoustic hearing preserved cochlear implant",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hermes17_interspeech.html": {
    "title": "Phonetic Correlates of Pharyngeal and Pharyngealized Consonants in Saudi, Lebanese, and Jordanian Arabic: An rt-MRI Study",
    "volume": "main",
    "abstract": "The phonemic inventory of Arabic includes sounds that involve a pharyngeal constriction. Sounds referred to as ‘pharyngeal' (/ʕ/ and /ħ/) are reported to have a primary constriction in the pharynx, while sounds referred to as ‘pharyngealized' (/s /, /t /, /d /, and /ð / or /z /) are reported to have a secondary constriction in the pharynx. Some studies propose grouping both types of sounds together, citing phonetic and phonological evidence. Phonetically, pharyngeal consonants are argued to have a primary constriction below the pharynx, and are thus posited to be pharyngealized laryngeals. Under this view, the pharyngeal constriction is secondary, not primary. Phonologically, it has been established that pharyngealized sounds trigger pharyngealization spread, and proposals for grouping pharyngeal and pharyngealized consonants together cite similar, but not identical, spread patterns triggered by pharyngeals. In this study, Real-time Magnetic Resonance Imaging is employed to investigate the phonetic correlates of the pharyngeal constriction in both pharyngeal and pharyngealized sounds in Saudi, Lebanese, and Jordanian Arabic as exemplified by one speaker from each dialect. Our findings demonstrate a difference in the location of constriction among both types of sounds. These distinctions in place possibly account for the differences in the spread patterns triggered by each type of sound",
    "checked": true,
    "id": "f9f4c903762e1102ad98f44444d6f5d9923da1fc",
    "semantic_title": "phonetic correlates of pharyngeal and pharyngealized consonants in saudi, lebanese, and jordanian arabic: an rt-mri study",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/elie17_interspeech.html": {
    "title": "Glottal Opening and Strategies of Production of Fricatives",
    "volume": "main",
    "abstract": "This work investigates the influence of the gradual opening of the glottis along its length during the production of fricatives in intervocalic contexts. Acoustic simulations reveal the existence of a transient zone in the articulatory space where the frication noise level is very sensitive to small perturbations of the glottal opening. This corresponds to the configurations where both frication noise and voiced contributions are present in the speech signal. To avoid this unstability, speakers may adopt different strategies to ensure the voiced/voiceless contrast of fricatives. This is evidenced by experimental data of simultaneous glottal opening measurements, performed with ePGG, and audio recordings of vowel-fricative-vowel pseudowords. Voiceless fricatives are usually longer, in order to maximize the number of voiceless time frames over voiced frames due to the crossing of the transient regime. For voiced fricatives, the speaker may avoid the unstable regime by keeping low frication noise level, and thus by favoring the voicing characteristic, or by doing very short crossings into the unstable regime. It is also shown that when speakers are asked to sustain voiced fricatives longer than in natural speech, they adopt the strategy of keeping low frication noise level to avoid the unstable regime",
    "checked": true,
    "id": "9bbb42a98fdd09c51c264b293e20590e684e4527",
    "semantic_title": "glottal opening and strategies of production of fricatives",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/frej17_interspeech.html": {
    "title": "Acoustics and Articulation of Medial versus Final Coronal Stop Gemination Contrasts in Moroccan Arabic",
    "volume": "main",
    "abstract": "This paper presents results of a simultaneous acoustic and articulatory investigation of word-medial and word-final geminate/singleton coronal stop contrasts in Moroccan Arabic (MA). The acoustic analysis revealed that, only for the word-medial contrast, the two MA speakers adopted comparable strategies in contrasting geminates with singletons, mainly by significantly lengthening closure duration in geminates, relative to singletons. In word-final position, two speaker-specific contrasting patterns emerged. While one speaker also lengthened the closure duration for final geminates, the other speaker instead lengthened only the release duration for final geminates, relative to singletons. Consonant closure and preceding vowel were significantly longer for the geminate only in medial position, not in final position. These temporal differences were even more clearly delineated in the articulatory signal, captured via ultrasound, to which we applied the novel approach of using TRACTUS [Temporally Resolved Articulatory Configuration Tracking of UltraSound: 15] to index temporal properties of closure gestures for these geminate/singleton contrasts",
    "checked": true,
    "id": "55e2565e1b3f996c114078e593b9cd8768e032d2",
    "semantic_title": "acoustics and articulation of medial versus final coronal stop gemination contrasts in moroccan arabic",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/turco17_interspeech.html": {
    "title": "How are Four-Level Length Distinctions Produced? Evidence from Moroccan Arabic",
    "volume": "main",
    "abstract": "We investigate the durational properties of Moroccan Arabic identical consonant sequences contrasting singleton (S) and geminate (G) dental fricatives, in six combinations of four-level length contrasts across word boundaries (#) (one timing slot for #S, two for #G and S#S, three for S#G and G#S, and four for G#G). The aim is to determine the nature of the mapping between discrete phonological timing units and phonetic durations. Acoustic results show that the largest and most systematic jump in duration is displayed between the singleton fricative on the one hand and the other sequences on the other hand. Looking at these sequences, S#S is shown to have the same duration as #G. When a geminate is within the sequence, a temporal reorganization is observed: G#S is not significantly longer than S#S and #G; and G#G is only slightly longer than S#G. Instead of a four-way hierarchy, our data point towards a possible upper limit of three-way length contrasts for consonants: S < G=S#S=G#S < S#G=G#G. The interplay of a number of factors resulting in this mismatch between phonological length and phonetic duration are discussed, and a working hypothesis is provided for why duration contrasts are rarely ternary, and almost never quaternary",
    "checked": true,
    "id": "c7a6ebd5e51b1cf995fec0eed7c3e2bebd04e1ad",
    "semantic_title": "how are four-level length distinctions produced? evidence from moroccan arabic",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jones17_interspeech.html": {
    "title": "Vowels in the Barunga Variety of North Australian Kriol",
    "volume": "main",
    "abstract": "North Australian Kriol is an English based creole spoken widely by Indigenous people in northern Australia in areas where the traditional languages are endangered or no longer spoken. This paper offers the first acoustic description of the vowel phonology of Roper Kriol, within a variety spoken at Barunga Community, east of the town of Katherine in the Northern Territory Drawing on a new corpus for Barunga Kriol, the paper presents analyses of the short and long monophthongs, as well as the diphthongs in the spontaneous speech of young adults. The results show the durations and spectral characteristics of the vowels, including major patterns of allophony (i.e. coarticulation and context effects). This updates the phonology over the previous description from the 1970s, showing that there is an additional front low vowel phoneme in the speech of young people today, as well as a vowel length contrast. Interestingly there are points of similarity with the vowel acoustics for traditional Aboriginal languages of the region, for example in a relatively compact vowel space and in the modest trajectories of diphthongs",
    "checked": true,
    "id": "5e38240b3a9425798125345ddec3c6b6a5033744",
    "semantic_title": "vowels in the barunga variety of north australian kriol",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/dutta17_interspeech.html": {
    "title": "Nature of Contrast and Coarticulation: Evidence from Mizo Tones and Assamese Vowel Harmony",
    "volume": "main",
    "abstract": "Tonal coarticulation is universally found to be greater in extent in the carryover direction compared to the anticipatory direction ([1], [2], [3], [4], [5]) leading to assimilatory processes. In general, carryover coarticulation has been understood to be due to intertio-mechanical forces, and, anticipatory effects are seen to be a consequence of parallel activation of articulatory plans ([6]). In this paper, we report on results from a set of Artificial Neural Networks (ANN) trained to predict adjacent tones in disyllabic sequences. Our results confirm the universal pattern of greater carryover effects in Mizo leading to tonal assimilation. In addition, we report on results from single-layered ANN models and Support Vector Machines (SVM) that predict the identity of V from V (anticipatory) consistently better than V from V (carryover) in Assamese non-harmonic #…V CV …# sequences. The directionality in the performance of the V and V models, help us conclude that the directionality effect of coarticulation in Assamese non-harmonic sequences is greater in the anticipatory direction, which is the same direction as in the harmonic sequences. We argue that coarticulatory propensity exhibits a great deal of sensitivity to the nature of contrast in a language",
    "checked": true,
    "id": "09a76f55dfd08a5515410c1ef5782ece9795f28b",
    "semantic_title": "nature of contrast and coarticulation: evidence from mizo tones and assamese vowel harmony",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/cabral17_interspeech.html": {
    "title": "The Influence of Synthetic Voice on the Evaluation of a Virtual Character",
    "volume": "main",
    "abstract": "Graphical realism and the naturalness of the voice used are important aspects to consider when designing a virtual agent or character. In this work, we evaluate how synthetic speech impacts people's perceptions of a rendered virtual character. Using a controlled experiment, we focus on the role that speech, in particular voice expressiveness in the form of personality, has on the assessment of voice level and character level perceptions. We found that people rated a real human voice as more expressive, understandable and likeable than the expressive synthetic voice we developed. Contrary to our expectations, we found that the voices did not have a significant impact on the character level judgments; people in the voice conditions did not significantly vary on their ratings of appeal, credibility, human-likeness and voice matching the character. The implications this has for character design and how this compares with previous work are discussed",
    "checked": true,
    "id": "ad48f676dc43d5e65035ed0d0d79f54243746e68",
    "semantic_title": "the influence of synthetic voice on the evaluation of a virtual character",
    "citation_count": 29,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gully17_interspeech.html": {
    "title": "Articulatory Text-to-Speech Synthesis Using the Digital Waveguide Mesh Driven by a Deep Neural Network",
    "volume": "main",
    "abstract": "Following recent advances in direct modeling of the speech waveform using a deep neural network, we propose a novel method that directly estimates a physical model of the vocal tract from the speech waveform, rather than magnetic resonance imaging data. This provides a clear relationship between the model and the size and shape of the vocal tract, offering considerable flexibility in terms of speech characteristics such as age and gender. Initial tests indicate that despite a highly simplified physical model, intelligible synthesized speech is obtained. This illustrates the potential of the combined technique for the control of physical models in general, and hence the generation of more natural-sounding synthetic speech",
    "checked": true,
    "id": "a09a29e6fc522420114314c024933d3dfe7345d3",
    "semantic_title": "articulatory text-to-speech synthesis using the digital waveguide mesh driven by a deep neural network",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/maguer17_interspeech.html": {
    "title": "An HMM/DNN Comparison for Synchronized Text-to-Speech and Tongue Motion Synthesis",
    "volume": "main",
    "abstract": "We present an end-to-end text-to-speech (TTS) synthesis system that generates audio and synchronized tongue motion directly from text. This is achieved by adapting a statistical shape space model of the tongue surface to an articulatory speech corpus and training a speech synthesis system directly on the tongue model parameter weights. We focus our analysis on the application of two standard methodologies, based on Hidden Markov Models (HMMs) and Deep Neural Networks (DNNs), respectively, to train both acoustic models and the tongue model parameter weights. We evaluate both methodologies at every step by comparing the predicted articulatory movements against the reference data. The results show that even with less than 2h of data, DNNs already outperform HMMs",
    "checked": true,
    "id": "a464a5a31c35a4cab1fc1eeed31177cdcf260b8b",
    "semantic_title": "an hmm/dnn comparison for synchronized text-to-speech and tongue motion synthesis",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/alexander17_interspeech.html": {
    "title": "VCV Synthesis Using Task Dynamics to Animate a Factor-Based Articulatory Model",
    "volume": "main",
    "abstract": "This paper presents an initial architecture for articulatory synthesis which combines a dynamical system for the control of vocal tract shaping with a novel MATLAB implementation of an articulatory synthesizer. The dynamical system controls a speaker-specific vocal tract model derived by factor analysis of mid-sagittal real-time MRI data and provides input to the articulatory synthesizer, which simulates the propagation of sound waves in the vocal tract. First, parameters of the dynamical system are estimated from real-time MRI data of human speech production. Second, vocal-tract dynamics is simulated for vowel-consonant-vowel utterances using a sequence of two dynamical systems: the first one starts from a vowel vocal-tract configuration and achieves a vocal-tract closure; the second one starts from the closure and achieves the target configuration of the second vowel. Third, vocal-tract dynamics is converted to area function dynamics and is input to the synthesizer to generate the acoustic signal. Synthesized vowel-consonant-vowel examples demonstrate the feasibility of the method",
    "checked": true,
    "id": "1d8e1de2cbd6b7c1e508f01d1a6a8f4bd6738556",
    "semantic_title": "vcv synthesis using task dynamics to animate a factor-based articulatory model",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mendelson17_interspeech.html": {
    "title": "Beyond the Listening Test: An Interactive Approach to TTS Evaluation",
    "volume": "main",
    "abstract": "Traditionally, subjective text-to-speech (TTS) evaluation is performed through audio-only listening tests, where participants evaluate unrelated, context-free utterances. The ecological validity of these tests is questionable, as they do not represent real-world end-use scenarios. In this paper, we examine a novel approach to TTS evaluation in an imagined end-use, via a complex interaction with an avatar. 6 different voice conditions were tested: Natural speech, Unit Selection and Parametric Synthesis, in neutral and expressive realizations. Results were compared to a traditional audio-only evaluation baseline. Participants in both studies rated the voices for naturalness and expressivity. The baseline study showed canonical results for naturalness: Natural speech scored highest, followed by Unit Selection, then Parametric synthesis. Expressivity was clearly distinguishable in all conditions. In the avatar interaction study, participants rated naturalness in the same order as the baseline, though with smaller effect size; expressivity was not distinguishable. Further, no significant correlations were found between cognitive or affective responses and any voice conditions. This highlights 2 primary challenges in designing more valid TTS evaluations: in real-world use-cases involving interaction, listeners generally interact with a single voice, making comparative analysis unfeasible, and in complex interactions, the context and content may confound perception of voice quality",
    "checked": true,
    "id": "e25869e27af783fc2943cd34586edd5aa476a4fe",
    "semantic_title": "beyond the listening test: an interactive approach to tts evaluation",
    "citation_count": 21,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/cao17_interspeech.html": {
    "title": "Integrating Articulatory Information in Deep Learning-Based Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "Articulatory information has been shown to be effective in improving the performance of hidden Markov model (HMM)-based text-to-speech (TTS) synthesis. Recently, deep learning-based TTS has outperformed HMM-based approaches. However, articulatory information has rarely been integrated in deep learning-based TTS. This paper investigated the effectiveness of integrating articulatory movement data to deep learning-based TTS. The integration of articulatory information was achieved in two ways: (1) direct integration, where articulatory and acoustic features were the output of a deep neural network (DNN), and (2) direct integration plus forward-mapping, where the output articulatory features were mapped to acoustic features by an additional DNN; These forward-mapped acoustic features were then combined with the output acoustic features to produce the final acoustic features. Articulatory (tongue and lip) and acoustic data collected from male and female speakers were used in the experiment. Both objective measures and subjective judgment by human listeners showed the approaches integrated articulatory information outperformed the baseline approach (without using articulatory information) in terms of naturalness and speaker voice identity (voice similarity)",
    "checked": true,
    "id": "bd3c26cb973503743829c4276b4eb6d3582eb2e7",
    "semantic_title": "integrating articulatory information in deep learning-based text-to-speech synthesis",
    "citation_count": 14,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ma17b_interspeech.html": {
    "title": "Approaches for Neural-Network Language Model Adaptation",
    "volume": "main",
    "abstract": "Language Models (LMs) for Automatic Speech Recognition (ASR) are typically trained on large text corpora from news articles, books and web documents. These types of corpora, however, are unlikely to match the test distribution of ASR systems, which expect spoken utterances. Therefore, the LM is typically adapted to a smaller held-out in-domain dataset that is drawn from the test distribution. We propose three LM adaptation approaches for Deep NN and Long Short-Term Memory (LSTM): (1) Adapting the softmax layer in the Neural Network (NN); (2) Adding a non-linear adaptation layer before the softmax layer that is trained only in the adaptation phase; (3) Training the extra non-linear adaptation layer in pre-training and adaptation phases. Aiming to improve upon a hierarchical Maximum Entropy (MaxEnt) second-pass LM baseline, which factors the model into word-cluster and word models, we build an NN LM that predicts only word clusters. Adapting the LSTM LM by training the adaptation layer in both training and adaptation phases (Approach 3), we reduce the cluster perplexity by 30% on a held-out dataset compared to an unadapted LSTM LM. Initial experiments using a state-of-the-art ASR system show a 2.3% relative reduction in WER on top of an adapted MaxEnt LM",
    "checked": true,
    "id": "8fb0151c211b6394fe6fda64abff62bbb0061fef",
    "semantic_title": "approaches for neural-network language model adaptation",
    "citation_count": 33,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/oualil17_interspeech.html": {
    "title": "A Batch Noise Contrastive Estimation Approach for Training Large Vocabulary Language Models",
    "volume": "main",
    "abstract": "Training large vocabulary Neural Network Language Models (NNLMs) is a difficult task due to the explicit requirement of the output layer normalization, which typically involves the evaluation of the full softmax function over the complete vocabulary. This paper proposes a Batch Noise Contrastive Estimation (B-NCE) approach to alleviate this problem. This is achieved by reducing the vocabulary, at each time step, to the target words in the batch and then replacing the softmax by the noise contrastive estimation approach, where these words play the role of targets and noise samples at the same time. In doing so, the proposed approach can be fully formulated and implemented using optimal dense matrix operations. Applying B-NCE to train different NNLMs on the Large Text Compression Benchmark (LTCB) and the One Billion Word Benchmark (OBWB) shows a significant reduction of the training time with no noticeable degradation of the models performance. This paper also presents a new baseline comparative study of different standard NNLMs on the large OBWB on a single Titan-X GPU",
    "checked": true,
    "id": "e2dc0acd574bfc5f956788bb517a714080c0b111",
    "semantic_title": "a batch noise contrastive estimation approach for training large vocabulary language models",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17b_interspeech.html": {
    "title": "Investigating Bidirectional Recurrent Neural Network Language Models for Speech Recognition",
    "volume": "main",
    "abstract": "Recurrent neural network language models (RNNLMs) are powerful language modeling techniques. Significant performance improvements have been reported in a range of tasks including speech recognition compared to n-gram language models. Conventional n-gram and neural network language models are trained to predict the probability of the next word given its preceding context history. In contrast, bidirectional recurrent neural network based language models consider the context from future words as well. This complicates the inference process, but has theoretical benefits for tasks such as speech recognition as additional context information can be used. However to date, very limited or no gains in speech recognition performance have been reported with this form of model. This paper examines the issues of training bidirectional recurrent neural network language models (bi-RNNLMs) for speech recognition. A bi-RNNLM probability smoothing technique is proposed, that addresses the very sharp posteriors that are often observed in these models. The performance of the bi-RNNLMs is evaluated on three speech recognition tasks: broadcast news; meeting transcription (AMI); and low-resource systems (Babel data). On all tasks gains are observed by applying the smoothing technique to the bi-RNNLM. In addition consistent performance gains can be obtained by combining bi-RNNLMs with n-gram and uni-directional RNNLMs",
    "checked": true,
    "id": "7fe37b79f80e8937ecba653b57ebc989a56b29f9",
    "semantic_title": "investigating bidirectional recurrent neural network language models for speech recognition",
    "citation_count": 40,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/huang17b_interspeech.html": {
    "title": "Fast Neural Network Language Model Lookups at N-Gram Speeds",
    "volume": "main",
    "abstract": "Feed forward Neural Network Language Models (NNLM) have shown consistent gains over backoff word n-gram models in a variety of tasks. However, backoff n-gram models still remain dominant in applications with real time decoding requirements as word probabilities can be computed orders of magnitude faster than the NNLM. In this paper, we present a combination of techniques that allows us to speed up the probability computation from a neural net language model to make it comparable to the word n-gram model without any approximations. We present results on state of the art systems for Broadcast news transcription and conversational speech which demonstrate the speed improvements in real time factor and probability computation while retaining the WER gains from NNLM",
    "checked": true,
    "id": "3dac8b53c0a43df06129986eed236d179f1d16a8",
    "semantic_title": "fast neural network language model lookups at n-gram speeds",
    "citation_count": 12,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kurata17_interspeech.html": {
    "title": "Empirical Exploration of Novel Architectures and Objectives for Language Models",
    "volume": "main",
    "abstract": "While recurrent neural network language models based on Long Short Term Memory (LSTM) have shown good gains in many automatic speech recognition tasks, Convolutional Neural Network (CNN) language models are relatively new and have not been studied in-depth. In this paper we present an empirical comparison of LSTM and CNN language models on English broadcast news and various conversational telephone speech transcription tasks. We also present a new type of CNN language model that leverages dilated causal convolution to efficiently exploit long range history. We propose a novel criterion for training language models that combines word and class prediction in a multi-task learning framework. We apply this criterion to train word and character based LSTM language models and CNN language models and show that it improves performance. Our results also show that CNN and LSTM language models are complementary and can be combined to obtain further gains",
    "checked": true,
    "id": "b0bd00f3a30bc5cbcb1a277b8ecd0bb9b4919f89",
    "semantic_title": "empirical exploration of novel architectures and objectives for language models",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/benes17_interspeech.html": {
    "title": "Residual Memory Networks in Language Modeling: Improving the Reputation of Feed-Forward Networks",
    "volume": "main",
    "abstract": "We introduce the Residual Memory Network (RMN) architecture to language modeling. RMN is an architecture of feed-forward neural networks that incorporates residual connections and time-delay connections that allow us to naturally incorporate information from a substantial time context. As this is the first time RMNs are applied for language modeling, we thoroughly investigate their behaviour on the well studied Penn Treebank corpus. We change the model slightly for the needs of language modeling, reducing both its time and memory consumption. Our results show that RMN is a suitable choice for small-sized neural language models: With test perplexity 112.7 and as few as 2.3M parameters, they out-perform both a much larger vanilla RNN (PPL 124, 8M parameters) and a similarly sized LSTM (PPL 115, 2.08M parameters), while being only by less than 3 perplexity points worse than twice as big LSTM",
    "checked": true,
    "id": "2ee7ee38745e9fcf89860dfb3d41c2155521e3a3",
    "semantic_title": "residual memory networks in language modeling: improving the reputation of feed-forward networks",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/poorjam17_interspeech.html": {
    "title": "Dominant Distortion Classification for Pre-Processing of Vowels in Remote Biomedical Voice Analysis",
    "volume": "main",
    "abstract": "Advances in speech signal analysis facilitate the development of techniques for remote biomedical voice assessment. However, the performance of these techniques is affected by noise and distortion in signals. In this paper, we focus on the vowel /a/ as the most widely-used voice signal for pathological voice assessments and investigate the impact of four major types of distortion that are commonly present during recording or transmission in voice analysis, namely: background noise, reverberation, clipping and compression, on Mel-frequency cepstral coefficients (MFCCs) — the most widely-used features in biomedical voice analysis. Then, we propose a new distortion classification approach to detect the most dominant distortion in such voice signals. The proposed method involves MFCCs as frame-level features and a support vector machine as classifier to detect the presence and type of distortion in frames of a given voice signal. Experimental results obtained from the healthy and Parkinson's voices show the effectiveness of the proposed approach in distortion detection and classification",
    "checked": true,
    "id": "6003f076ac577e2cd9e22d674b90f984fa7763fe",
    "semantic_title": "dominant distortion classification for pre-processing of vowels in remote biomedical voice analysis",
    "citation_count": 22,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/le17_interspeech.html": {
    "title": "Automatic Paraphasia Detection from Aphasic Speech: A Preliminary Study",
    "volume": "main",
    "abstract": "Aphasia is an acquired language disorder resulting from brain damage that can cause significant communication difficulties. Aphasic speech is often characterized by errors known as paraphasias, the analysis of which can be used to determine an appropriate course of treatment and to track an individual's recovery progress. Being able to detect paraphasias automatically has many potential clinical benefits; however, this problem has not previously been investigated in the literature. In this paper, we perform the first study on detecting phonemic and neologistic paraphasias from scripted speech samples in AphasiaBank. We propose a speech recognition system with task-specific language models to transcribe aphasic speech automatically. We investigate features based on speech duration, Goodness of Pronunciation, phone edit distance, and Dynamic Time Warping on phoneme posteriorgrams. Our results demonstrate the feasibility of automatic paraphasia detection and outline the path toward enabling this system in real-world clinical applications",
    "checked": true,
    "id": "a61f6c2cb8618fcf193f3de695f0a59cc719df9e",
    "semantic_title": "automatic paraphasia detection from aphasic speech: a preliminary study",
    "citation_count": 22,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/garcia17_interspeech.html": {
    "title": "Evaluation of the Neurological State of People with Parkinson's Disease Using i-Vectors",
    "volume": "main",
    "abstract": "The i-vector approach is used to model the speech of PD patients with the aim of assessing their condition. Features related to the articulation, phonation, and prosody dimensions of speech were used to train different i-vector extractors. Each i-vector extractor is trained using utterances from both PD patients and healthy controls. The i-vectors of the healthy control (HC) speakers are averaged to form a single i-vector that represents the HC group, i.e., the reference i-vector. A similar process is done to create a reference of the group with PD patients. Then the i-vectors of test speakers are compared to these reference i-vectors using the cosine distance. Three analyses are performed using this distance: classification between PD patients and HC, prediction of the neurological state of PD patients according to the MDS-UPDRS-III scale, and prediction of a modified version of the Frenchay Dysarthria Assessment. The Spearman's correlation between this cosine distance and the MDS-UPDRS-III scale was 0.63. These results show the suitability of this approach to monitor the neurological state of people with Parkinson's Disease",
    "checked": true,
    "id": "edad4b36bf583c4949dd2f1272b143309f300bcd",
    "semantic_title": "evaluation of the neurological state of people with parkinson's disease using i-vectors",
    "citation_count": 16,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chien17_interspeech.html": {
    "title": "Objective Severity Assessment from Disordered Voice Using Estimated Glottal Airflow",
    "volume": "main",
    "abstract": "In clinical practice, the severity of disordered voice is typically rated by a professional with auditory-perceptual judgment. The present study aims to automate this assessment procedure, in an attempt to make the assessment objective and less labor-intensive. In the automated analysis, glottal airflow is estimated from the analyzed voice signal with an inverse filtering algorithm. Automatic assessment is realized by a regressor that predicts from temporal and spectral features of the glottal airflow. A regressor trained on overtone amplitudes and harmonic richness factors extracted from a set of continuous-speech utterances was applied to a set of sustained-vowel utterances, giving severity predictions (on a scale of ratings from 0 to 100) with an average error magnitude of 14",
    "checked": true,
    "id": "eae51baf6bd12f879314b459e5d8ed502fb8e7c3",
    "semantic_title": "objective severity assessment from disordered voice using estimated glottal airflow",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/pokorny17_interspeech.html": {
    "title": "Earlier Identification of Children with Autism Spectrum Disorder: An Automatic Vocalisation-Based Approach",
    "volume": "main",
    "abstract": "Autism spectrum disorder (ASD) is a neurodevelopmental disorder usually diagnosed in or beyond toddlerhood. ASD is defined by repetitive and restricted behaviours, and deficits in social communication. The early speech-language development of individuals with ASD has been characterised as delayed. However, little is known about ASD-related characteristics of pre-linguistic vocalisations at the feature level. In this study, we examined pre-linguistic vocalisations of 10-month-old individuals later diagnosed with ASD and a matched control group of typically developing individuals (N = 20). We segmented 684 vocalisations from parent-child interaction recordings. All vocalisations were annotated and signal-analytically decomposed. We analysed ASD-related vocalisation specificities on the basis of a standardised set (eGeMAPS) of 88 acoustic features selected for clinical speech analysis applications. 54 features showed evidence for a differentiation between vocalisations of individuals later diagnosed with ASD and controls. In addition, we evaluated the feasibility of automated, vocalisation-based identification of individuals later diagnosed with ASD. We compared linear kernel support vector machines and a 1-layer bidirectional long short-term memory neural network. Both classification approaches achieved an accuracy of 75% for subject-wise identification in a subject-independent 3-fold cross-validation scheme. Our promising results may be an important contribution en-route to facilitate earlier identification of ASD",
    "checked": true,
    "id": "65616754cbd08935c0f17c2f34258670b61a5fc9",
    "semantic_title": "earlier identification of children with autism spectrum disorder: an automatic vocalisation-based approach",
    "citation_count": 44,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/vasquezcorrea17_interspeech.html": {
    "title": "Convolutional Neural Network to Model Articulation Impairments in Patients with Parkinson's Disease",
    "volume": "main",
    "abstract": "Speech impairments are one of the earliest manifestations in patients with Parkinson's disease. Particularly, articulation deficits related to the capability of the speaker to start/stop the vibration of the vocal folds have been observed in the patients. Those difficulties can be assessed by modeling the transitions between voiced and unvoiced segments from speech. A robust strategy to model the articulatory deficits related to the starting or stopping vibration of the vocal folds is proposed in this study. The transitions between voiced and unvoiced segments are modeled by a convolutional neural network that extracts suitable information from two time-frequency representations: the short time Fourier transform and the continuous wavelet transform. The proposed approach improves the results previously reported in the literature. Accuracies of up to 89% are obtained for the classification of Parkinson's patients vs. healthy speakers. This study is a step towards the robust modeling of the speech impairments in patients with neuro-degenerative disorders",
    "checked": true,
    "id": "1b316a46a9d0cd0cf8edd44367a888fd71429ce9",
    "semantic_title": "convolutional neural network to model articulation impairments in patients with parkinson's disease",
    "citation_count": 50,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bai17_interspeech.html": {
    "title": "Phone Classification Using a Non-Linear Manifold with Broad Phone Class Dependent DNNs",
    "volume": "main",
    "abstract": "Most state-of-the-art automatic speech recognition (ASR) systems use a single deep neural network (DNN) to map the acoustic space to the decision space. However, different phonetic classes employ different production mechanisms and are best described by different types of features. Hence it may be advantageous to replace this single DNN with several phone class dependent DNNs. The appropriate mathematical formalism for this is a manifold. This paper assesses the use of a non-linear manifold structure with multiple DNNs for phone classification. The system has two levels. The first comprises a set of broad phone class (BPC) dependent DNN-based mappings and the second level is a fusion network. Various ways of designing and training the networks in both levels are assessed, including varying the size of hidden layers, the use of the bottleneck or softmax outputs as input to the fusion network, and the use of different broad class definitions. Phone classification experiments are performed on TIMIT. The results show that using the BPC-dependent DNNs provides small but significant improvements in phone classification accuracy relative to a single global DNN. The paper concludes with visualisations of the structures learned by the local and global DNNs and discussion of their interpretations",
    "checked": true,
    "id": "2f21907b030e5ccf29ab3867841718bc98c74054",
    "semantic_title": "phone classification using a non-linear manifold with broad phone class dependent dnns",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17c_interspeech.html": {
    "title": "An Investigation of Crowd Speech for Room Occupancy Estimation",
    "volume": "main",
    "abstract": "Room occupancy estimation technology has been shown to reduce building energy cost significantly. However speech-based occupancy estimation has not been well explored. In this paper, we investigate energy mode and babble speaker count methods for estimating both small and large crowds in a party-mode room setting. We also examine how distance between speakers and microphone affects their estimation accuracies. Then we propose a novel entropy-based method, which is invariant to different speakers and their different positions in a room. Evaluations on synthetic crowd speech generated using the TIMIT corpus show that acoustic volume features are less affected by distance, and our proposed method outperforms existing methods across a range of different conditions",
    "checked": true,
    "id": "daf4097464bf07d4e53ad21a7a3a5cb3adba7a29",
    "semantic_title": "an investigation of crowd speech for room occupancy estimation",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/vijayan17_interspeech.html": {
    "title": "Time-Frequency Coherence for Periodic-Aperiodic Decomposition of Speech Signals",
    "volume": "main",
    "abstract": "Decomposing speech signals into periodic and aperiodic components is an important task, finding applications in speech synthesis, coding, denoising, etc. In this paper, we construct a time-frequency coherence function to analyze spectro-temporal signatures of speech signals for distinguishing between deterministic and stochastic components of speech. The narrowband speech spectrogram is segmented into patches, which are represented as 2-D cosine carriers modulated in amplitude and frequency. Separation of carrier and amplitude/frequency modulations is achieved by 2-D demodulation using Riesz transform, which is the 2-D extension of Hilbert transform. The demodulated AM component reflects contributions of the vocal tract to spectrogram. The frequency modulated carrier (FM-carrier) signal exhibits properties of the excitation. The time-frequency coherence is defined with respect to FM-carrier and a coherence map is constructed, in which highly coherent regions represent nearly periodic and deterministic components of speech, whereas the incoherent regions correspond to unstructured components. The coherence map shows a clear distinction between deterministic and stochastic components in speech characterized by jitter, shimmer, lip radiation, type of excitation, etc. Binary masks prepared from the time-frequency coherence function are used for periodic-aperiodic decomposition of speech. Experimental results are presented to validate the efficiency of the proposed method",
    "checked": true,
    "id": "93b97386a78e815f4612aa8378784dd9c442e59d",
    "semantic_title": "time-frequency coherence for periodic-aperiodic decomposition of speech signals",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/meireles17_interspeech.html": {
    "title": "Musical Speech: A New Methodology for Transcribing Speech Prosody",
    "volume": "main",
    "abstract": "Musical Speech is a new methodology for transcribing speech prosody using musical notation. The methodology presented in this paper is an updated version of our work [12]. Our work is situated in a historical context with a brief survey of the literature of speech melodies, in which we highlight the pioneering works of John Steele, Leoš Janávcek, Engelbert Humperdinck, and Arnold Schoenberg, followed by a linguistic view of musical notation in the analysis of speech. Finally, we present the current state-of-the-art of our innovative methodology that uses a quarter-tone scale for transcribing speech, and shows some initial results of the application of this methodology to prosodic transcription",
    "checked": true,
    "id": "3ef82fe149a24e1406e354c8b0b4ad35579853a2",
    "semantic_title": "musical speech: a new methodology for transcribing speech prosody",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/nataraj17_interspeech.html": {
    "title": "Estimation of Place of Articulation of Fricatives from Spectral Characteristics for Speech Training",
    "volume": "main",
    "abstract": "A visual feedback of the place of articulation is considered to be useful for speech training aids for hearing-impaired children and for learners of second languages in helping them in improving pronunciation. For such applications, the relation between place of articulation of fricatives and their spectral characteristics is investigated using English fricatives available in the XRMB database, which provides simultaneously acquired speech signal and articulogram. Place of articulation is estimated from the articulogram as the position of maximum constriction in the oral cavity, using an automated graphical technique. The magnitude spectrum is smoothed by critical band based median and mean filters for improving the consistency of the spectral parameters. Out of several spectral parameters investigated, spectral moments and spectral slope appear to be related to the place of articulation of the fricative segment of the utterances as measured from articulogram. The data are used to train and test a Gaussian mixture model to estimate the place of articulation with spectral parameters as the inputs. The estimated values showed a good match with those obtained from the articulograms",
    "checked": true,
    "id": "a4ff3e50abe2dc21fc1fe30acbfc5ab2fc744938",
    "semantic_title": "estimation of place of articulation of fricatives from spectral characteristics for speech training",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/backstrom17_interspeech.html": {
    "title": "Estimation of the Probability Distribution of Spectral Fine Structure in the Speech Source",
    "volume": "main",
    "abstract": "The efficiency of many speech processing methods rely on accurate modeling of the distribution of the signal spectrum and a majority of prior works suggest that the spectral components follow the Laplace distribution. To improve the probability distribution models based on our knowledge of speech source modeling, we argue that the model should in fact be a multiplicative mixture model, including terms for voiced and unvoiced utterances. While prior works have applied Gaussian mixture models, we demonstrate that a mixture of generalized Gaussian models more accurately follows the observations. The proposed estimation method is based on measuring the ratio of L -norms between spectral bands. Such ratios follow the Beta-distribution when the input signal is generalized Gaussian, whereby the estimated parameters can be used to determine the underlying parameters of the mixture of generalized Gaussian distributions",
    "checked": true,
    "id": "c3a964d36176261d54457b62e46c01d69733f0d1",
    "semantic_title": "estimation of the probability distribution of spectral fine structure in the speech source",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ghosh17_interspeech.html": {
    "title": "End-to-End Acoustic Feedback in Language Learning for Correcting Devoiced French Final-Fricatives",
    "volume": "main",
    "abstract": "This work aims at providing an end-to-end acoustic feedback framework to help learners of French to pronounce voiced fricatives. A classifier ensemble detects voiced/unvoiced utterances, then a correction method is proposed to improve the perception and production of voiced fricatives in a word-final position. Realizations of voiced fricatives contained in French sentences uttered by French and German speakers were analyzed to find out the deviations between the acoustic cues realized by the two groups of speakers. The correction method consists in substituting the erroneous devoiced fricative by TD-PSOLA concatenative synthesis that uses exemplars of voiced fricatives chosen from a French speaker corpus. To achieve a seamless concatenation the energy of the replacement fricative was adjusted with respect to the energy levels of the learner's and French speaker's preceding vowels. Finally, a perception experiment with the corrected stimuli has been carried out with French native speakers to check the appropriateness of the fricative revoicing. The results showed that the proposed revoicing strategy proved to be very efficient and can be used as an acoustic feedback",
    "checked": true,
    "id": "7b3da8ae724a767141026781403ed6b3ee9f11dc",
    "semantic_title": "end-to-end acoustic feedback in language learning for correcting devoiced french final-fricatives",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jacewicz17_interspeech.html": {
    "title": "Dialect Perception by Older Children",
    "volume": "main",
    "abstract": "The acquisition of regional dialect variation is an inherent part of the language learning process that takes place in the specific environments in which the child participates. This study examined dialect perception by 9–12-year-olds who grew up in two very diverse dialect regions in the United States, Western North Carolina (NC) and Southeastern Wisconsin (WI). In a dialect identification task, each group of children responded to 120 talkers from the same dialects representing three generations, ranging in age from old adults to children. There was a robust discrepancy in the children's dialect identification performance: WI children were able to identify talker dialect quite well (although still not as well as the adults) whereas NC children were at chance level. WI children were also more sensitive to cross-generational changes in both dialects as a function of diachronic sound change. It is concluded that both groups of children demonstrated their sociolinguistic awareness in very different ways, corresponding to relatively stable (WI) and changing (NC) socio-cultural environments in their respective speech communities",
    "checked": true,
    "id": "3399778ddace86ff24e28c9996188a90e7daf7c8",
    "semantic_title": "dialect perception by older children",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/yoneyama17_interspeech.html": {
    "title": "Perception of Non-Contrastive Variations in American English by Japanese Learners: Flaps are Less Favored Than Stops",
    "volume": "main",
    "abstract": "Alveolar flaps are non-contrastive allophonic variants of alveolar stops in American English. A lexical decision experiment was conducted with Japanese learners of English (JE) to investigate whether second-language (L2) learners are sensitive to such allophonic variations when recognizing words in L2. The stimuli consisted of 36 isolated bisyllabic English words containing word-medial /t/, half of which were flap-favored words, e.g. city, and the other half were [t]-favored words, e.g. faster. All stimuli were recorded with two surface forms: /t/ as a flap, e.g. city with a flap, or as [t], e.g. city with [t]. The stimuli were counterbalanced so that participants only heard one of the two surface forms of each word. The accuracy data indicated that flap-favored words pronounced with a flap, e.g. city with a flap, were recognized significantly less accurately than flap-favored words with [t], e.g. city with [t], and [t]-favored words with [t], e.g. faster with [t]. These results suggest that JE learners prefer canonical forms over frequent forms produced with context-dependent allophonic variations. These results are inconsistent with previous studies that found native speakers' preference for frequent forms, and highlight differences in the effect of allophonic variations on the perception of native-language and L2 speech",
    "checked": true,
    "id": "c102db8f188e0c90f09e9544a237411f06910a23",
    "semantic_title": "perception of non-contrastive variations in american english by japanese learners: flaps are less favored than stops",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/maastricht17_interspeech.html": {
    "title": "L1 Perceptions of L2 Prosody: The Interplay Between Intonation, Rhythm, and Speech Rate and Their Contribution to Accentedness and Comprehensibility",
    "volume": "main",
    "abstract": "This study investigates the cumulative effect of (non-)native intonation, rhythm, and speech rate in utterances produced by Spanish learners of Dutch on Dutch native listeners' perceptions. In order to assess the relative contribution of these language-specific properties to perceived accentedness and comprehensibility, speech produced by Spanish learners of Dutch was manipulated using transplantation and resynthesis techniques. Thus, eight manipulation conditions reflecting all possible combinations of L1 and L2 intonation, rhythm, and speech rate were created, resulting in 320 utterances that were rated by 50 Dutch natives on their degree of foreign accent and ease of comprehensibility Our analyses show that all manipulations result in lower accentedness and higher comprehensibility ratings. Moreover, both measures are not affected in the same way by different combinations of prosodic features: For accentedness, Dutch listeners appear most influenced by intonation, and intonation combined with speech rate. This holds for comprehensibility ratings as well, but here the combination of all three properties, including rhythm, also significantly affects ratings by native speakers. Thus, our study reaffirms the importance of differentiating between different aspects of perception and provides insight into those features that are most likely to affect how native speakers perceive second language learners",
    "checked": true,
    "id": "a928e082af0d19e51b51ea9b5688d0d4b87477c9",
    "semantic_title": "l1 perceptions of l2 prosody: the interplay between intonation, rhythm, and speech rate and their contribution to accentedness and comprehensibility",
    "citation_count": 23,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/takiguchi17_interspeech.html": {
    "title": "Effects of Pitch Fall and L1 on Vowel Length Identification in L2 Japanese",
    "volume": "main",
    "abstract": "This study investigated whether and how the role of pitch fall in the first language (L1) interacts with its use as a cue for Japanese phonological vowel length in the second language (L2). Native listeners of Japanese (NJ) and L2 learners of Japanese with L1 backgrounds in Mandarin Chinese (NC), Seoul Korean (NK), American English (NE), and French (NFr) participated in a perception experiment. The results showed that the proportion of \"long\" responses increased as a function of vowel duration for all groups, giving s-shaped curves. Meanwhile, the presence or absence of a pitch fall within a syllable affected only NJ and NC's perception. Their category boundary occurred at a shorter duration for vowels with a pitch fall than without a pitch fall. Among the four groups of L2 learners, only NC use pitch fall to distinguish words in the L1. Thus, it is possible to think that the role of pitch fall as an L1 cue relates to its use as a cue for L2 length identification. L2 learners tend to attend to an important phonetic feature as a cue for perceiving an L1 category differentiating L1 words even in the L2 as implied by the Feature Hypothesis",
    "checked": true,
    "id": "170d7b909363e5c47c7255b13e940ce6a618fcdb",
    "semantic_title": "effects of pitch fall and l1 on vowel length identification in l2 japanese",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhang17_interspeech.html": {
    "title": "A Preliminary Study of Prosodic Disambiguation by Chinese EFL Learners",
    "volume": "main",
    "abstract": "This study investigated whether Chinese learners of English as a foreign language (EFL learners hereafter) could use prosodic cues to resolve syntactically ambiguous sentences in English. 8 sentences with 3 types of syntactic ambiguity were adopted. They were far/near PP attachment, left/right word attachment and wide/narrow scope. In the production experiment, 15 Chinese college students who passed the annual national examination CET (College English Test) Band 4 and 5 native English speakers from America were recruited. They were asked to read the 8 target sentences after hearing the contexts spoken by a Native American speaker, which clarified the intended meaning of the ambiguous sentences. The preliminary results showed that, as the native speakers did, Chinese EFL learners employed different durational patterns to express the alternative meanings of the ambiguous sentences by altering prosodic phrasing. That is, the duration of the pre-boundary items were lengthened and pause were inserted at the boundary. But the perception experiment showed that the utterances produced by Chinese EFL learners couldn't be effectively perceived by the native speakers due to their different use of pre-boundary lengthening and pause. The conclusion is that Chinese EFL learners find prosodic disambiguation difficult",
    "checked": true,
    "id": "e30be62b4da615745ef3c9c2cccf772267901c96",
    "semantic_title": "a preliminary study of prosodic disambiguation by chinese efl learners",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kim17_interspeech.html": {
    "title": "Generation of Large-Scale Simulated Utterances in Virtual Rooms to Train Deep-Neural Networks for Far-Field Speech Recognition in Google Home",
    "volume": "main",
    "abstract": "We describe the structure and application of an acoustic room simulator to generate large-scale simulated data for training deep neural networks for far-field speech recognition. The system simulates millions of different room dimensions, a wide distribution of reverberation time and signal-to-noise ratios, and a range of microphone and sound source locations. We start with a relatively clean training set as the source and artificially create simulated data by randomly sampling a noise configuration for every new training example. As a result, the acoustic model is trained using examples that are virtually never repeated. We evaluate performance of this approach based on room simulation using a factored complex Fast Fourier Transform (CFFT) acoustic model introduced in our earlier work, which uses CFFT layers and LSTM AMs for joint multichannel processing and acoustic modeling. Results show that the simulator-driven approach is quite effective in obtaining large improvements not only in simulated test conditions, but also in real / rerecorded conditions. This room simulation system has been employed in training acoustic models including the ones for the recently released Google Home",
    "checked": true,
    "id": "49636d64a097f708ac131eb24c46719dfcd6d6b2",
    "semantic_title": "generation of large-scale simulated utterances in virtual rooms to train deep-neural networks for far-field speech recognition in google home",
    "citation_count": 222,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kinoshita17_interspeech.html": {
    "title": "Neural Network-Based Spectrum Estimation for Online WPE Dereverberation",
    "volume": "main",
    "abstract": "In this paper, we propose a novel speech dereverberation framework that utilizes deep neural network (DNN)-based spectrum estimation to construct linear inverse filters. The proposed dereverberation framework is based on the state-of-the-art inverse filter estimation algorithm called weighted prediction error (WPE) algorithm, which is known to effectively reduce reverberation and greatly boost the ASR performance in various conditions. In WPE, the accuracy of the inverse filter estimation, and thus the dereverberation performance, is largely dependent on the estimation of the power spectral density (PSD) of the target signal. Therefore, the conventional WPE iteratively performs the inverse filter estimation, actual dereverberation and the PSD estimation to gradually improve the PSD estimate. However, while such iterative procedure works well when sufficiently long acoustically-stationary observed signals are available, WPE's performance degrades when the duration of observed/accessible data is short, which typically is the case for real-time applications using online block-batch processing with small batches. To solve this problem, we incorporate the DNN-based spectrum estimator into the framework of WPE, because a DNN can estimate the PSD robustly even from very short observed data. We experimentally show that the proposed framework outperforms the conventional WPE, and improves the ASR performance in real noisy reverberant environments in both single-channel and multichannel cases",
    "checked": true,
    "id": "2b5fe0dceaf84e0ad0d2f06ce4a9865f065a6f63",
    "semantic_title": "neural network-based spectrum estimation for online wpe dereverberation",
    "citation_count": 85,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ichikawa17_interspeech.html": {
    "title": "Factorial Modeling for Effective Suppression of Directional Noise",
    "volume": "main",
    "abstract": "The assumed scenario is transcription of a face-to-face conversation, such as in the financial industry when an agent and a customer talk over a desk with microphones placed between the speakers and then it is transcribed. From the automatic speech recognition (ASR) perspective, one of the speakers is the target speaker, and the other speaker is a directional noise source. When the number of microphones is small, we often accept microphone intervals that are larger than the spatial aliasing limit because the performance of the beamformer is better. Unfortunately, such a configuration results in significant leakage of directional noise in certain frequency bands because the spatial aliasing makes the beamformer and post-filter inaccurate there. Thus, we introduce a factorial model to compensate only the degraded bands with information from the reliable bands in a probabilistic framework integrating our proposed metrics and speech model. In our experiments, the proposed method reduced the errors from 29.8% to 24.9%",
    "checked": true,
    "id": "58084137d1a8227433f9f113fb57cefd1262f839",
    "semantic_title": "factorial modeling for effective suppression of directional noise",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tu17_interspeech.html": {
    "title": "On Design of Robust Deep Models for CHiME-4 Multi-Channel Speech Recognition with Multiple Configurations of Array Microphones",
    "volume": "main",
    "abstract": "We design a novel deep learning framework for multi-channel speech recognition in two aspects. First, for the front-end, an iterative mask estimation (IME) approach based on deep learning is presented to improve the beamforming approach based on the conventional complex Gaussian mixture model (CGMM). Second, for the back-end, deep convolutional neural networks (DCNNs), with augmentation of both noisy and beamformed training data, are adopted for acoustic modeling while the forward and backward long short-term memory recurrent neural networks (LSTM-RNNs) are used for language modeling. The proposed framework can be quite effective to multi-channel speech recognition with random combinations of fixed microphones. Testing on the CHiME-4 Challenge speech recognition task with a single set of acoustic and language models, our approach achieves the best performance of all three tracks (1-channel, 2-channel, and 6-channel) among submitted systems",
    "checked": true,
    "id": "f8e7eed4f75a5ea89a9d7659fb0b784cf8f5ffd8",
    "semantic_title": "on design of robust deep models for chime-4 multi-channel speech recognition with multiple configurations of array microphones",
    "citation_count": 26,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/li17c_interspeech.html": {
    "title": "Acoustic Modeling for Google Home",
    "volume": "main",
    "abstract": "This paper describes the technical and system building advances made to the Google Home multichannel speech recognition system, which was launched in November 2016. Technical advances include an adaptive dereverberation frontend, the use of neural network models that do multichannel processing jointly with acoustic modeling, and Grid-LSTMs to model frequency variations. On the system level, improvements include adapting the model using Google Home specific data. We present results on a variety of multichannel sets. The combination of technical and system advances result in a reduction of WER of 8–28% relative compared to the current production system",
    "checked": true,
    "id": "13e30c5dccae82477ee5d38e4d9c96b504a13d29",
    "semantic_title": "acoustic modeling for google home",
    "citation_count": 150,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mirsamadi17_interspeech.html": {
    "title": "On Multi-Domain Training and Adaptation of End-to-End RNN Acoustic Models for Distant Speech Recognition",
    "volume": "main",
    "abstract": "Recognition of distant (far-field) speech is a challenge for ASR due to mismatch in recording conditions resulting from room reverberation and environment noise. Given the remarkable learning capacity of deep neural networks, there is increasing interest to address this problem by using a large corpus of reverberant far-field speech to train robust models. In this study, we explore how an end-to-end RNN acoustic model trained on speech from different rooms and acoustic conditions (different domains) achieves robustness to environmental variations. It is shown that the first hidden layer acts as a domain separator, projecting the data from different domains into different subspaces. The subsequent layers then use this encoded domain knowledge to map these features to final representations that are invariant to domain change. This mechanism is closely related to noise-aware or room-aware approaches which append manually-extracted domain signatures to the input features. Additionally, we demonstrate how this understanding of the learning procedure provides useful guidance for model adaptation to new acoustic conditions. We present results based on AMI corpus to demonstrate the propagation of domain information in a deep RNN, and perform recognition experiments which indicate the role of encoded domain knowledge on training and adaptation of RNN acoustic models",
    "checked": true,
    "id": "691e9714cca248f95f967d39328b23fcdaf8c040",
    "semantic_title": "on multi-domain training and adaptation of end-to-end rnn acoustic models for distant speech recognition",
    "citation_count": 22,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/morise17_interspeech.html": {
    "title": "Low-Dimensional Representation of Spectral Envelope Without Deterioration for Full-Band Speech Analysis/Synthesis System",
    "volume": "main",
    "abstract": "A speech coding for a full-band speech analysis/synthesis system is described. In this work, full-band speech is defined as speech with a sampling frequency above 40 kHz, whose Nyquist frequency covers the audible frequency range. In prior works, speech coding has generally focused on the narrow-band speech with a sampling frequency below 16 kHz. On the other hand, statistical parametric speech synthesis currently uses the full-band speech, and low-dimensional representation of speech parameters is being used. The purpose of this study is to achieve speech coding without deterioration for full-band speech. We focus on a high-quality speech analysis/synthesis system and mel-cepstral analysis using frequency warping. In the frequency warping function, we directly use three auditory scales. We carried out a subjective evaluation using the WORLD vocoder and found that the optimum number of dimensions was around 50. The kind of frequency warping did not significantly affect the sound quality in the dimensions",
    "checked": true,
    "id": "ce1e8d04fdff25e91399e2fbb8bc8159dd1ea58a",
    "semantic_title": "low-dimensional representation of spectral envelope without deterioration for full-band speech analysis/synthesis system",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/loweimi17_interspeech.html": {
    "title": "Robust Source-Filter Separation of Speech Signal in the Phase Domain",
    "volume": "main",
    "abstract": "In earlier work we proposed a framework for speech source-filter separation that employs phase-based signal processing. This paper presents a further theoretical investigation of the model and optimisations that make the filter and source representations less sensitive to the effects of noise and better matched to downstream processing. To this end, first, in computing the Hilbert transform, the log function is replaced by the generalised logarithmic function. This introduces a tuning parameter that adjusts both the dynamic range and distribution of the phase-based representation. Second, when computing the group delay, a more robust estimate for the derivative is formed by applying a regression filter instead of using sample differences. The effectiveness of these modifications is evaluated in clean and noisy conditions by considering the accuracy of the fundamental frequency extracted from the estimated source, and the performance of speech recognition features extracted from the estimated filter. In particular, the proposed filter-based front-end reduces Aurora-2 WERs by 6.3% (average 0–20 dB) compared with previously reported results. Furthermore, when tested in a LVCSR task (Aurora-4) the new features resulted in 5.8% absolute WER reduction compared to MFCCs without performance loss in the clean/matched condition",
    "checked": true,
    "id": "d73e9b415841f4eeb43a66a031027a30b5312312",
    "semantic_title": "source-filter separation of speech signal in the phase domain",
    "citation_count": 20,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/stone17_interspeech.html": {
    "title": "A Time-Warping Pitch Tracking Algorithm Considering Fast f0 Changes",
    "volume": "main",
    "abstract": "Accurately tracking the fundamental frequency (f ) or pitch in speech data is of great interest in numerous contexts. All currently available pitch tracking algorithms perform a short-term analysis of a speech signal to extract the f under the assumption that the pitch does not change within a single analysis frame, a simplification that introduces errors when the f changes rather quickly over time. This paper proposes a new algorithm that warps the time axis of an analysis frame to counteract intra-frame f changes and thus to improve the total tracking results. The algorithm was evaluated on a set of 4718 sentences from 20 speakers (10 male, 10 female) and with added white and babble noise. It was comparative in performance to the state-of-the-art algorithms RAPT and PRAAT to Pitch (ac) under clean conditions and outperformed both of them under noisy conditions",
    "checked": true,
    "id": "35cabd729c7aad08d2a591e767cbe2555642af76",
    "semantic_title": "a time-warping pitch tracking algorithm considering fast f0 changes",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kawahara17_interspeech.html": {
    "title": "A Modulation Property of Time-Frequency Derivatives of Filtered Phase and its Application to Aperiodicity and fo Estimation",
    "volume": "main",
    "abstract": "We introduce a simple and linear SNR (strictly speaking, periodic to random power ratio) estimator (0 dB to 80 dB without additional calibration/linearization) for providing reliable descriptions of aperiodicity in speech corpus. The main idea of this method is to estimate the background random noise level without directly extracting the background noise. The proposed method is applicable to a wide variety of time windowing functions with very low sidelobe levels. The estimate combines the frequency derivative and the time-frequency derivative of the mapping from filter center frequency to the output instantaneous frequency. This procedure can replace the periodicity detection and aperiodicity estimation subsystems of recently introduced open source vocoder, YANG vocoder. Source code of MATLAB implementation of this method will also be open sourced",
    "checked": true,
    "id": "dfad99fe6d9bbba16d089a0c8779894b3d749446",
    "semantic_title": "a modulation property of time-frequency derivatives of filtered phase and its application to aperiodicity and fo estimation",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kumar17_interspeech.html": {
    "title": "Non-Local Estimation of Speech Signal for Vowel Onset Point Detection in Varied Environments",
    "volume": "main",
    "abstract": "Vowel onset point (VOP) is an important information extensively employed in speech analysis and synthesis. Detecting the VOPs in a given speech sequence, independent of the text contexts and recording environments, is a challenging area of research. Performance of existing VOP detection methods have not yet been extensively studied in varied environmental conditions. In this paper, we have exploited the non-local means estimation to detect those regions in the speech sequence which are of high signal-to-noise ratio and exhibit periodicity. Mostly, those regions happen to be the vowel regions. This helps in overcoming the ill-effects of environmental degradations. Next, for each short-time frame of estimated speech sequence, we cumulatively sum the magnitude of the corresponding Fourier transform spectrum. The cumulative sum is then used as the feature to detect the VOPs. The experiments conducted on TIMIT database show that the proposed approach provides better results in terms of detection and spurious rate when compared to a few existing methods under clean and noisy test conditions",
    "checked": true,
    "id": "7de52c37d55db5532b31a336cea3149315d48024",
    "semantic_title": "non-local estimation of speech signal for vowel onset point detection in varied environments",
    "citation_count": 19,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/alradhi17_interspeech.html": {
    "title": "Time-Domain Envelope Modulating the Noise Component of Excitation in a Continuous Residual-Based Vocoder for Statistical Parametric Speech Synthesis",
    "volume": "main",
    "abstract": "In this paper, we present an extension of a novel continuous residual-based vocoder for statistical parametric speech synthesis. Previous work has shown the advantages of adding envelope modulated noise to the voiced excitation, but this has not been investigated yet in the context of continuous vocoders, i.e. of which all parameters are continuous. The noise component is often not accurately modeled in modern vocoders (e.g. STRAIGHT). For more natural sounding speech synthesis, four time-domain envelopes (Amplitude, Hilbert, Triangular and True) are investigated and enhanced, and then applied to the noise component of the excitation in our continuous vocoder. The performance evaluation is based on the study of time envelopes. In an objective experiment, we investigated the Phase Distortion Deviation of vocoded samples. A MUSHRA type subjective listening test was also conducted comparing natural and vocoded speech samples. Both experiments have shown that the proposed framework using Hilbert and True envelopes provides high-quality vocoding while outperforming the two other envelopes",
    "checked": true,
    "id": "aac5326c98894ef028f2ec2bb8f6383f83c91795",
    "semantic_title": "time-domain envelope modulating the noise component of excitation in a continuous residual-based vocoder for statistical parametric speech synthesis",
    "citation_count": 21,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wu17b_interspeech.html": {
    "title": "Wavelet Speech Enhancement Based on Robust Principal Component Analysis",
    "volume": "main",
    "abstract": "Most state-of-the-art speech enhancement (SE) techniques prefer to enhance utterances in the frequency domain rather than in the time domain. However, the overlap-add (OLA) operation in the short-time Fourier transform (STFT) for speech signal processing possibly distorts the signal and limits the performance of the SE techniques. In this study, a novel SE method that integrates the discrete wavelet packet transform (DWPT) and a novel subspace-based method, robust principal component analysis (RPCA), is proposed to enhance noise-corrupted signals directly in the time domain. We evaluate the proposed SE method on the Mandarin hearing in noise test (MHINT) sentences. The experimental results show that the new method reduces the signal distortions dramatically, thereby improving speech quality and intelligibility significantly. In addition, the newly proposed method outperforms the STFT-RPCA-based speech enhancement system",
    "checked": true,
    "id": "2f90d9735de7025cbc19815aacaa87fa5a75817e",
    "semantic_title": "wavelet speech enhancement based on robust principal component analysis",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sharma17_interspeech.html": {
    "title": "Vowel Onset Point Detection Using Sonority Information",
    "volume": "main",
    "abstract": "Vowel onset point (VOP) refers to the starting event of a vowel, that may be reflected in different aspects of the speech signal. The major issue in VOP detection using existing methods is the confusion among the vowels and other categories of sounds preceding them. This work explores the usefulness of sonority information to reduce this confusion and improve VOP detection. Vowels are the most sonorant sounds followed by semivowels, nasals, voiced fricatives, voiced stops. The sonority feature is derived from the vocal-tract system, excitation source and suprasegmental aspects. As this feature has the capability to discriminate among different sonorant sound units, it reduces the confusion among onset of vowels with that of other sonorant sounds. This results in improved detection and resolution of VOP detection for continuous speech. The performance of proposed sonority information based VOP detection is found to be 92.4%, compared to 85.2% by the existing method. Also the resolution of localizing VOP within 10 ms is significantly enhanced and a performance of 73.0% is achieved as opposed to 60.2% by the existing method",
    "checked": true,
    "id": "38274f465e50f0e74e38bbc2aad51af587096590",
    "semantic_title": "vowel onset point detection using sonority information",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/laine17_interspeech.html": {
    "title": "Analytic Filter Bank for Speech Analysis, Feature Extraction and Perceptual Studies",
    "volume": "main",
    "abstract": "Speech signal consists of events in time and frequency, and therefore its analysis with high-resolution time-frequency tools is often of importance. Analytic filter bank provides a simple, fast, and flexible method to construct time-frequency representations of signals. Its parameters can be easily adapted to different situations from uniform to any auditory frequency scale, or even to a focused resolution. Since the Hilbert magnitude values of the channels are obtained at every sample, it provides a practical tool for a high-resolution time-frequency analysis The present study describes the basic theory of analytic filters and tests their main properties. Applications of analytic filter bank to different speech analysis tasks including pitch period estimation and pitch synchronous analysis of formant frequencies and bandwidths are demonstrated. In addition, a new feature vector called group delay vector is introduced. It is shown that this representation provides comparable, or even better results, than those obtained by spectral magnitude feature vectors in the analysis and classification of vowels. The implications of this observation are discussed also from the speech perception point of view",
    "checked": true,
    "id": "800f5f51e48c9ecce30239b264884dd99715c401",
    "semantic_title": "analytic filter bank for speech analysis, feature extraction and perceptual studies",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kroos17_interspeech.html": {
    "title": "Learning the Mapping Function from Voltage Amplitudes to Sensor Positions in 3D-EMA Using Deep Neural Networks",
    "volume": "main",
    "abstract": "The first generation of three-dimensional Electromagnetic Articulography devices (Carstens AG500) suffered from occasional critical tracking failures. Although now superseded by new devices, the AG500 is still in use in many speech labs and many valuable data sets exist. In this study we investigate whether deep neural networks (DNNs) can learn the mapping function from raw voltage amplitudes to sensor positions based on a comprehensive movement data set. This is compared to arriving sample by sample at individual position values via direct optimisation as used in previous methods. We found that with appropriate hyperparameter settings a DNN was able to approximate the mapping function with good accuracy, leading to a smaller error than the previous methods, but that the DNN-based approach was not able to solve the tracking problem completely",
    "checked": true,
    "id": "63629b14ca0c4af978ef9b4df2fa32f90e2291ec",
    "semantic_title": "learning the mapping function from voltage amplitudes to sensor positions in 3d-ema using deep neural networks",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/dai17_interspeech.html": {
    "title": "Multilingual i-Vector Based Statistical Modeling for Music Genre Classification",
    "volume": "main",
    "abstract": "For music signal processing, compared with the strategy which models each short-time frame independently, when the long-time features are considered, the time-series characteristics of the music signal can be better presented. As a typical kind of long-time modeling strategy, the identification vector (i-vector) uses statistical modeling to model the audio signal in the segment level. It can better capture the important elements of the music signal, and these important elements may benefit to the classification of music signal. In this paper, the i-vector based statistical feature for music genre classification is explored. In addition to learn enough important elements for music signal, a new multilingual i-vector feature is proposed based on the multilingual model. The experimental results show that the multilingual i-vector based models can achieve better classification performances than conventional short-time modeling based methods",
    "checked": true,
    "id": "8c5c6522778d906045da87fc14da89515056ae28",
    "semantic_title": "multilingual i-vector based statistical modeling for music genre classification",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/khonglah17_interspeech.html": {
    "title": "Indoor/Outdoor Audio Classification Using Foreground Speech Segmentation",
    "volume": "main",
    "abstract": "The task of indoor/ outdoor audio classification using foreground speech segmentation is attempted in this work. Foreground speech segmentation is the use of features to segment between foreground speech and background interfering sources like noise. Initially, the foreground and background segments are obtained from foreground speech segmentation by using the normalized autocorrelation peak strength (NAPS) of the zero frequency filtered signal (ZFFS) as a feature. The background segments are then considered for determining whether a particular segment is an indoor or outdoor audio sample. The mel frequency cepstral coefficients are obtained from the background segments of both the indoor and outdoor audio samples and are used to train the Support Vector Machine (SVM) classifier. The use of foreground speech segmentation gives a promising performance for the indoor/ outdoor audio classification task",
    "checked": true,
    "id": "46393584d0aefd22fcc669439ddd5a6e1049a922",
    "semantic_title": "indoor/outdoor audio classification using foreground speech segmentation",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/guo17_interspeech.html": {
    "title": "Attention Based CLDNNs for Short-Duration Acoustic Scene Classification",
    "volume": "main",
    "abstract": "Recently, neural networks with deep architecture have been widely applied to acoustic scene classification. Both Convolutional Neural Networks (CNNs) and Long Short-Term Memory Networks (LSTMs) have shown improvements over fully connected Deep Neural Networks (DNNs). Motivated by the fact that CNNs, LSTMs and DNNs are complimentary in their modeling capability, we apply the CLDNNs (Convolutional, Long Short-Term Memory, Deep Neural Networks) framework to short-duration acoustic scene classification in a unified architecture. The CLDNNs take advantage of frequency modeling with CNNs, temporal modeling with LSTM, and discriminative training with DNNs. Based on the CLDNN architecture, several novel attention-based mechanisms are proposed and applied on the LSTM layer to predict the importance of each time step. We evaluate the proposed method on the truncated version of the 2016 TUT acoustic scenes dataset which consists of recordings from 15 different scenes. By using CLDNNs with bidirectional LSTM, we achieve higher performance compared to the conventional neural network architectures. Moreover, by combining the attention-weighted output with LSTM final time step output, significant improvement can be further achieved",
    "checked": true,
    "id": "9c80aa1dec0087fb754d0c3351f3d6803dbb5685",
    "semantic_title": "attention based cldnns for short-duration acoustic scene classification",
    "citation_count": 47,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/xia17_interspeech.html": {
    "title": "Frame-Wise Dynamic Threshold Based Polyphonic Acoustic Event Detection",
    "volume": "main",
    "abstract": "Acoustic event detection, the determination of the acoustic event type and the localisation of the event, has been widely applied in many real-world applications. Many works adopt multi-label classification techniques to perform the polyphonic acoustic event detection with a global threshold to detect the active acoustic events. However, the global threshold has to be set manually and is highly dependent on the database being tested. To deal with this, we replaced the fixed threshold method with a frame-wise dynamic threshold approach in this paper. Two novel approaches, namely contour and regressor based dynamic threshold approaches are proposed in this work. Experimental results on the popular TUT Acoustic Scenes 2016 database of polyphonic events demonstrated the superior performance of the proposed approaches",
    "checked": true,
    "id": "5db7c47f98bc91ba41f14c2d2e9cf25785df8359",
    "semantic_title": "frame-wise dynamic threshold based polyphonic acoustic event detection",
    "citation_count": 16,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jang17_interspeech.html": {
    "title": "Enhanced Feature Extraction for Speech Detection in Media Audio",
    "volume": "main",
    "abstract": "Speech detection is an important first step for audio analysis on media contents, whose goal is to discriminate the presence of speech from non-speech. It remains a challenge owing to various sound sources included in media audio. In this work, we present a novel audio feature extraction method to reflect the acoustic characteristic of the media audio in the time-frequency domain. Since the degree of combination of harmonic and percussive components varies depending on the type of sound source, the audio features which further distinguish between speech and non-speech can be obtained by decomposing the signal into both components. For the evaluation, we use over 20 hours of drama which manually annotated for speech detection as well as 4 full-length movies with annotations released for a research community, whose total length is over 8 hours. Experimental results with deep neural network show superior performance of the proposed in media audio condition",
    "checked": true,
    "id": "8f51d1738c4719e0c5cdd71c5f4cd84984bb657a",
    "semantic_title": "enhanced feature extraction for speech detection in media audio",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sonowal17_interspeech.html": {
    "title": "Audio Classification Using Class-Specific Learned Descriptors",
    "volume": "main",
    "abstract": "This paper presents a classification scheme for audio signals using high-level feature descriptors. The descriptor is designed to capture the relevance of each acoustic feature group (or feature set like mel-frequency cepstral coefficients, perceptual features etc.) in recognizing an audio class. For this, a bank of RVM classifiers are modeled for each ‘audio class'–‘feature group' pair. The response of an input signal to this bank of RVM classifiers forms the entries of the descriptor. Each entry of the descriptor thus measures the proximity of the input signal to an audio class based on a single feature group. This form of signal representation offers two-fold advantages. First, it helps to determine the effectiveness of each feature group in classifying a specific audio class. Second, the descriptor offers higher discriminability than the low-level feature groups and a simple SVM classifier trained on the descriptor produces better performance than several state-of-the-art methods",
    "checked": true,
    "id": "97d804c4e8341534232709cc633bae37e145e52f",
    "semantic_title": "audio classification using class-specific learned descriptors",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ebbers17_interspeech.html": {
    "title": "Hidden Markov Model Variational Autoencoder for Acoustic Unit Discovery",
    "volume": "main",
    "abstract": "Variational Autoencoders (VAEs) have been shown to provide efficient neural-network-based approximate Bayesian inference for observation models for which exact inference is intractable. Its extension, the so-called Structured VAE (SVAE) allows inference in the presence of both discrete and continuous latent variables. Inspired by this extension, we developed a VAE with Hidden Markov Models (HMMs) as latent models. We applied the resulting HMM-VAE to the task of acoustic unit discovery in a zero resource scenario. Starting from an initial model based on variational inference in an HMM with Gaussian Mixture Model (GMM) emission probabilities, the accuracy of the acoustic unit discovery could be significantly improved by the HMM-VAE. In doing so we were able to demonstrate for an unsupervised learning task what is well-known in the supervised learning case: Neural networks provide superior modeling power compared to GMMs",
    "checked": true,
    "id": "e8591d52f6054cc13c1f3603ce086bb83c7d4499",
    "semantic_title": "hidden markov model variational autoencoder for acoustic unit discovery",
    "citation_count": 45,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zohrer17_interspeech.html": {
    "title": "Virtual Adversarial Training and Data Augmentation for Acoustic Event Detection with Gated Recurrent Neural Networks",
    "volume": "main",
    "abstract": "In this paper, we use gated recurrent neural networks (GRNNs) for efficiently detecting environmental events of the IEEE Detection and Classification of Acoustic Scenes and Events challenge (DCASE2016). For this acoustic event detection task data is limited. Therefore, we propose data augmentation such as on-the-fly shuffling and virtual adversarial training for regularization of the GRNNs. Both improve the performance using GRNNs. We obtain a segment-based error rate of 0.59 and an F-score of 58.6%",
    "checked": true,
    "id": "c11170c830de3ea834d6d533f515240d6f060c53",
    "semantic_title": "virtual adversarial training and data augmentation for acoustic event detection with gated recurrent neural networks",
    "citation_count": 12,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mcauliffe17_interspeech.html": {
    "title": "Montreal Forced Aligner: Trainable Text-Speech Alignment Using Kaldi",
    "volume": "main",
    "abstract": "We present the Montreal Forced Aligner (MFA), a new open-source system for speech-text alignment. MFA is an update to the Prosodylab-Aligner, and maintains its key functionality of trainability on new data, as well as incorporating improved architecture (triphone acoustic models and speaker adaptation), and other features. MFA uses Kaldi instead of HTK, allowing MFA to be distributed as a stand-alone package, and to exploit parallel processing for computationally-intensive training and scaling to larger datasets. We evaluate MFA's performance on aligning word and phone boundaries in English conversational and laboratory speech, relative to human-annotated boundaries, focusing on the effects of aligner architecture and training on the data to be aligned. MFA performs well relative to two existing open-source aligners with simpler architecture (Prosodylab-Aligner and FAVE), and both its improved architecture and training on data to be aligned generally result in more accurate boundaries",
    "checked": true,
    "id": "9e8b06c60722fee06d7f01d4eeaf3ae81e0247d7",
    "semantic_title": "montreal forced aligner: trainable text-speech alignment using kaldi",
    "citation_count": 743,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/meenakshi17_interspeech.html": {
    "title": "A Robust Voiced/Unvoiced Phoneme Classification from Whispered Speech Using the ‘Color' of Whispered Phonemes and Deep Neural Network",
    "volume": "main",
    "abstract": "In this work, we propose a robust method to perform frame-level classification of voiced (V) and unvoiced (UV) phonemes from whispered speech, a challenging task due to its voiceless and noise-like nature. We hypothesize that a whispered speech spectrum can be represented as a linear combination of a set of colored noise spectra. A five-dimensional (5D) feature is computed by employing non-negative matrix factorization with a fixed basis dictionary, constructed using spectra of five colored noises. Deep Neural Network (DNN) is used as the classifier. We consider two baseline features-1) Mel Frequency Cepstral Coefficients (MFCC), 2) features computed from a data driven dictionary. Experiments reveal that the features from the colored noise dictionary perform better (on average) than that using the data driven dictionary, with a relative improvement in the average V/UV accuracy of 10.30%, within, and 10.41%, across, data from seven subjects. We also find that the MFCCs and 5D features carry complementary information regarding the nature of voicing decisions in whispered speech. Hence, across all subjects, we obtain a balanced frame-level V/UV classification performance, when MFCC and 5D features are combined, compared to a skewed performance when they are considered separately",
    "checked": true,
    "id": "9367699d3722a28986488933bc929006f05f14a6",
    "semantic_title": "a robust voiced/unvoiced phoneme classification from whispered speech using the 'color' of whispered phonemes and deep neural network",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/williams17_interspeech.html": {
    "title": "Rescoring-Aware Beam Search for Reduced Search Errors in Contextual Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Using context in automatic speech recognition allows the recognition system to dynamically task-adapt and bring gains to a broad variety of use-cases. An important mechanism of context-inclusion is on-the-fly rescoring of hypotheses with contextual language model content available only in real-time In systems where rescoring occurs on the lattice during its construction as part of beam search decoding, hypotheses eligible for rescoring may be missed due to pruning. This can happen for many reasons: the language model and rescoring model may assign significantly different scores, there may be a lot of noise in the utterance, or word prefixes with a high out-degree may necessitate aggressive pruning to keep the search tractable. This results in misrecognitions when contextually-relevant hypotheses are pruned before rescoring, even if a contextual rescoring model favors those hypotheses by a large margin We present a technique to adapt the beam search algorithm to preserve hypotheses when they may benefit from rescoring. We show that this technique significantly reduces the number of search pruning errors on rescorable hypotheses, without a significant increase in the search space size. This technique makes it feasible to use one base language model, but still achieve high-accuracy speech recognition results in all contexts",
    "checked": true,
    "id": "31fc9159ecb3a43c107a45e8fe32ab9f1c39cebb",
    "semantic_title": "rescoring-aware beam search for reduced search errors in contextual automatic speech recognition",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zenkel17_interspeech.html": {
    "title": "Comparison of Decoding Strategies for CTC Acoustic Models",
    "volume": "main",
    "abstract": "Connectionist Temporal Classification has recently attracted a lot of interest as it offers an elegant approach to building acoustic models (AMs) for speech recognition. The CTC loss function maps an input sequence of observable feature vectors to an output sequence of symbols. Output symbols are conditionally independent of each other under CTC loss, so a language model (LM) can be incorporated conveniently during decoding, retaining the traditional separation of acoustic and linguistic components in ASR For fixed vocabularies, Weighted Finite State Transducers provide a strong baseline for efficient integration of CTC AMs with n-gram LMs. Character-based neural LMs provide a straight forward solution for open vocabulary speech recognition and all-neural models, and can be decoded with beam search. Finally, sequence-to-sequence models can be used to translate a sequence of individual sounds into a word string We compare the performance of these three approaches, and analyze their error patterns, which provides insightful guidance for future research and development in this important area",
    "checked": true,
    "id": "7b8022a139564225ffd34d8edf8ee92af104ae20",
    "semantic_title": "comparison of decoding strategies for ctc acoustic models",
    "citation_count": 39,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hadian17_interspeech.html": {
    "title": "Phone Duration Modeling for LVCSR Using Neural Networks",
    "volume": "main",
    "abstract": "We describe our work on incorporating probabilities of phone durations, learned by a neural net, into an ASR system. Phone durations are incorporated via lattice rescoring. The input features are derived from the phone identities of a context window of phones, plus the durations of preceding phones within that window. Unlike some previous work, our network outputs the probability of different durations (in frames) directly, up to a fixed limit. We evaluate this method on several large vocabulary tasks, and while we consistently see improvements inWord Error Rates, the improvements are smaller when the lattices are generated with neural net based acoustic models",
    "checked": true,
    "id": "e5b181fe8c7711fb4bbe13990655a86309aba873",
    "semantic_title": "phone duration modeling for lvcsr using neural networks",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chorowski17_interspeech.html": {
    "title": "Towards Better Decoding and Language Model Integration in Sequence to Sequence Models",
    "volume": "main",
    "abstract": "The recently proposed Sequence-to-Sequence (seq2seq) framework advocates replacing complex data processing pipelines, such as an entire automatic speech recognition system, with a single neural network trained in an end-to-end fashion. In this contribution, we analyse an attention-based seq2seq speech recognition system that directly transcribes recordings into characters. We observe two shortcomings: overconfidence in its predictions and a tendency to produce incomplete transcriptions when language models are used. We propose practical solutions to both problems achieving competitive speaker independent word error rates on the Wall Street Journal dataset: without separate language models we reach 10.6% WER, while together with a trigram language model, we reach 6.7% WER, a state-of-the-art result for HMM-free methods",
    "checked": true,
    "id": "7dbb2d983ab95da04e5d47c87ddd2cd9a8f20786",
    "semantic_title": "towards better decoding and language model integration in sequence to sequence models",
    "citation_count": 341,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/li17d_interspeech.html": {
    "title": "Empirical Evaluation of Parallel Training Algorithms on Acoustic Modeling",
    "volume": "main",
    "abstract": "Deep learning models (DLMs) are state-of-the-art techniques in speech recognition. However, training good DLMs can be time consuming especially for production-size models and corpora. Although several parallel training algorithms have been proposed to improve training efficiency, there is no clear guidance on which one to choose for the task in hand due to lack of systematic and fair comparison among them. In this paper we aim at filling this gap by comparing four popular parallel training algorithms in speech recognition, namely asynchronous stochastic gradient descent (ASGD), blockwise model-update filtering (BMUF), bulk synchronous parallel (BSP) and elastic averaging stochastic gradient descent (EASGD), on 1000-hour LibriSpeech corpora using feed-forward deep neural networks (DNNs) and convolutional, long short-term memory, DNNs (CLDNNs). Based on our experiments, we recommend using BMUF as the top choice to train acoustic models since it is most stable, scales well with number of GPUs, can achieve reproducible results, and in many cases even outperforms single-GPU SGD. ASGD can be used as a substitute in some cases",
    "checked": true,
    "id": "9907faa68764e8c57aec05c1e3f6779cf84c7235",
    "semantic_title": "empirical evaluation of parallel training algorithms on acoustic modeling",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/xiang17_interspeech.html": {
    "title": "Binary Deep Neural Networks for Speech Recognition",
    "volume": "main",
    "abstract": "Deep neural networks (DNNs) are widely used in most current automatic speech recognition (ASR) systems. To guarantee good recognition performance, DNNs usually require significant computational resources, which limits their application to low-power devices. Thus, it is appealing to reduce the computational cost while keeping the accuracy. In this work, in light of the success in image recognition, binary DNNs are utilized in speech recognition, which can achieve competitive performance and substantial speed up. To our knowledge, this is the first time that binary DNNs have been used in speech recognition. For binary DNNs, network weights and activations are constrained to be binary values, which enables faster matrix multiplication based on bit operations. By exploiting the hardware population count instructions, the proposed binary matrix multiplication can achieve 5~7 times speed up compared with highly optimized floating-point matrix multiplication. This results in much faster DNN inference since matrix multiplication is the most computationally expensive operation. Experiments on both TIMIT phone recognition and a 50-hour Switchboard speech recognition show that, binary DNNs can run about 4 times faster than standard DNNs during inference, with roughly 10.0% relative accuracy reduction",
    "checked": true,
    "id": "8f1f128889f9236e83d4380a82ce7e96a89d85c3",
    "semantic_title": "binary deep neural networks for speech recognition",
    "citation_count": 24,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chandrashekaran17_interspeech.html": {
    "title": "Hierarchical Constrained Bayesian Optimization for Feature, Acoustic Model and Decoder Parameter Optimization",
    "volume": "main",
    "abstract": "We describe the implementation of a hierarchical constrained Bayesian Optimization algorithm and it's application to joint optimization of features, acoustic model structure and decoding parameters for deep neural network (DNN)-based large vocabulary continuous speech recognition (LVCSR) systems. Within our hierarchical optimization method we perform constrained Bayesian optimization jointly of feature hyper-parameters and acoustic model structure in the first-level, and then perform an iteration of constrained Bayesian optimization for the decoder hyper-parameters in the second. We show the the proposed hierarchical optimization method can generate a model with higher performance than a manually optimized system on a server platform. Furthermore, we demonstrate that the proposed framework can be used to automatically build real-time speech recognition systems for graphics processing unit (GPU)-enabled embedded platforms that retain similar accuracy to a server platform, while running with constrained computing resources",
    "checked": true,
    "id": "d74e5b93e243ae59cf36baf0fe5680edf0ae3f68",
    "semantic_title": "hierarchical constrained bayesian optimization for feature, acoustic model and decoder parameter optimization",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/toyama17_interspeech.html": {
    "title": "Use of Global and Acoustic Features Associated with Contextual Factors to Adapt Language Models for Spontaneous Speech Recognition",
    "volume": "main",
    "abstract": "In this study, we propose a new method of adapting language models for speech recognition using para-linguistic and extra-linguistic features in speech. When we talk with others, we often change the way of lexical choice and speaking style according to various contextual factors. This fact indicates that the performance of automatic speech recognition can be improved by taking the contextual factors into account, which can be estimated from speech acoustics. In this study, we attempt to find global and acoustic features that are associated with those contextual factors, then integrate those features into Recurrent Neural Network (RNN) language models for speech recognition. In experiments, using Japanese spontaneous speech corpora, we examine how i-vector and openSMILE are associated with contextual factors. Then, we use those features in the reranking process of RNN-based language models. Results show that perplexity is reduced by 16% relative and word error rate is reduced by 2.1% relative for highly emotional speech",
    "checked": true,
    "id": "abd4ec54d6feb7c96266667304968bf8b5d01cc3",
    "semantic_title": "use of global and acoustic features associated with contextual factors to adapt language models for spontaneous speech recognition",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/pahuja17_interspeech.html": {
    "title": "Joint Learning of Correlated Sequence Labeling Tasks Using Bidirectional Recurrent Neural Networks",
    "volume": "main",
    "abstract": "The stream of words produced by Automatic Speech Recognition (ASR) systems is typically devoid of punctuations and formatting. Most natural language processing applications expect segmented and well-formatted texts as input, which is not available in ASR output. This paper proposes a novel technique of jointly modeling multiple correlated tasks such as punctuation and capitalization using bidirectional recurrent neural networks, which leads to improved performance for each of these tasks. This method could be extended for joint modeling of any other correlated sequence labeling tasks",
    "checked": true,
    "id": "dcae7738f1dbb5148544fabdd8464946a9a51b17",
    "semantic_title": "joint learning of correlated sequence labeling tasks using bidirectional recurrent neural networks",
    "citation_count": 31,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/shen17_interspeech.html": {
    "title": "Estimation of Gap Between Current Language Models and Human Performance",
    "volume": "main",
    "abstract": "Language models (LMs) have gained dramatic improvement in the past years due to the wide application of neural networks. This raises the question of how far we are away from the perfect language model and how much more research is needed in language modelling. As for perplexity giving a value for human perplexity (as an upper bound of what is reasonably expected from an LM) is difficult. Word error rate (WER) has the disadvantage that it also measures the quality of other components of a speech recognizer like the acoustic model and the feature extraction. We therefore suggest evaluating LMs in a generative setting (which has been done before on selected hand-picked examples) and running a human evaluation on the generated sentences. The results imply that LMs need about 10 to 20 more years of research before human performance is reached. Moreover, we show that the human judgement scores on the generated sentences and perplexity are closely correlated. This leads to an estimated perplexity of 12 for an LM that would be able to pass the human judgement test in the setting we suggested",
    "checked": true,
    "id": "7fe4e308de5b2e5b509be8636c169e7928c242d9",
    "semantic_title": "estimation of gap between current language models and human performance",
    "citation_count": 24,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/moro17_interspeech.html": {
    "title": "A Phonological Phrase Sequence Modelling Approach for Resource Efficient and Robust Real-Time Punctuation Recovery",
    "volume": "main",
    "abstract": "For the automatic punctuation of Automatic Speech Recognition (ASR) output, both prosodic and text based features are used, often in combination. Pure prosody based approaches usually have low computation needs, introduce little latency (delay) and they are also more robust to ASR errors. Text based approaches usually yield better performance, they are however resource demanding (both regarding their training and computational needs), often introduce high time latency and are more sensitive to ASR errors. The present paper proposes a lightweight prosody based punctuation approach following a new paradigm: we argue in favour of an all-inclusive modelling of speech prosody instead of just relying on distinct acoustic markers: first, the entire phonological phrase structure is reconstructed, then its close correlation with punctuations is exploited in a sequence modelling approach with recurrent neural networks. With this tiny and easy to implement model we reach performance in Hungarian punctuation comparable to large, text based models for other languages by keeping resource requirements minimal and suitable for real-time operation with low latency",
    "checked": true,
    "id": "fd9c8e54db7fd691c490bca061181fdfbc82ba44",
    "semantic_title": "a phonological phrase sequence modelling approach for resource efficient and robust real-time punctuation recovery",
    "citation_count": 13,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wang17c_interspeech.html": {
    "title": "Factors Affecting the Intelligibility of Low-Pass Filtered Speech",
    "volume": "main",
    "abstract": "Frequency compression is an effective alternative to conventional hearing aids amplification for patients with severe-to-profound middle- and high-frequency hearing loss and with some low-frequency residual hearing. In order to develop novel frequency compression strategy, it is important to first understand the mechanism for recognizing low-pass filtered speech, which simulates high-frequency hearing loss. The present work investigated three factors affecting the intelligibility of low-pass filtered speech, i.e., vowels, temporal fine-structure, and fundamental frequency (F0) contour. Mandarin sentences were processed to generate three types (i.e., vowel-only, fine-structure-only, and F0-contour-flattened) of low-pass filtered stimuli. Listening experiments with normal-hearing listeners showed that among the three factors assessed, the vowel-only low-pass filtered speech was the most intelligible, which was followed by the fine-structure-based low-pass filtered speech. Flattening F0-contour significantly deteriorated the intelligibility of low-pass filtered speech",
    "checked": true,
    "id": "644ae1984221281054d387e84d8def4bde9f360f",
    "semantic_title": "factors affecting the intelligibility of low-pass filtered speech",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wang17d_interspeech.html": {
    "title": "Phonetic Restoration of Temporally Reversed Speech",
    "volume": "main",
    "abstract": "Early study showed that temporally reversed speech may still be very intelligible. The present work further assessed the role of acoustic cues accounting for the intelligibility of temporally reversed speech. Mandarin sentences were edited to be temporally reversed. Experiment 1 preserved the original consonant segments, and experiment 2 only preserved the temporally reversed fine-structure waveform. Experimental results with normal-hearing listeners showed that for Mandarin speech, listeners could still perfectly understand the temporally reversed speech with a reversion duration up to 50 ms. Preserving original consonant segments did not significantly improve the intelligibility of the temporally reversed speech, suggesting that the reversion processing applied to vowels largely affected the intelligibility of temporally reversed speech. When the local short-time envelope waveform was removed, listeners could still understand stimuli with primarily temporally reversed fine-structure waveform, suggesting the perceptual role of temporally reversed fine-structure to the intelligibility of temporally reversed speech",
    "checked": true,
    "id": "183cdbf28b760e85cf545688770f9b539fbed37e",
    "semantic_title": "phonetic restoration of temporally reversed speech",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ishida17_interspeech.html": {
    "title": "Simultaneous Articulatory and Acoustic Distortion in L1 and L2 Listening: Locally Time-Reversed \"Fast\" Speech",
    "volume": "main",
    "abstract": "The current study explores how native and non-native speakers cope with simultaneous articulatory and acoustic distortion in speech perception. The articulatory distortion was generated by asking a speaker to articulate target speech as fast as possible (fast speech). The acoustic distortion was created by dividing speech signals into small segments with equal time duration (e.g., 50 ms) from the onset of speech, and flipping every segment on a temporal axis, and putting them back together (locally time-reversed speech). This study explored how \"locally time-reversed fast speech\" was intelligible as compared to \"locally time-reversed normal speech\" measured in Ishida, Samuel, and Arai (2016). Participants were native English speakers and native Japanese speakers who spoke English as a second language. They listened to English words and pseudowords that contained a lot of stop consonants. These items were spoken fast and locally time-reversed at every 10, 20, 30, 40, 50, or 60 ms. In general, \"locally time-reversed fast speech\" became gradually unintelligible as the length of reversed segments increased. Native speakers generally understood locally time-reversed fast spoken words well but not pseudowords, while non-native speakers hardly understood both words and pseudowords. Language proficiency strongly supported the perceptual restoration of locally time-reversed fast speech",
    "checked": true,
    "id": "ba3cc5c345884802fc18d16f7b584b414d63c61b",
    "semantic_title": "simultaneous articulatory and acoustic distortion in l1 and l2 listening: locally time-reversed \"fast\" speech",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/burchfield17_interspeech.html": {
    "title": "Lexically Guided Perceptual Learning in Mandarin Chinese",
    "volume": "main",
    "abstract": "Lexically guided perceptual learning refers to the use of lexical knowledge to retune speech categories and thereby adapt to a novel talker's pronunciation. This adaptation has been extensively documented, but primarily for segmental-based learning in English and Dutch. In languages with lexical tone, such as Mandarin Chinese, tonal categories can also be retuned in this way, but segmental category retuning had not been studied. We report two experiments in which Mandarin Chinese listeners were exposed to an ambiguous mixture of [f] and [s] in lexical contexts favoring an interpretation as either [f] or [s]. Listeners were subsequently more likely to identify sounds along a continuum between [f] and [s], and to interpret minimal word pairs, in a manner consistent with this exposure. Thus lexically guided perceptual learning of segmental categories had indeed taken place, consistent with suggestions that such learning may be a universally available adaptation process",
    "checked": true,
    "id": "ba132ee084ecfcbc171d452d0f3b167e850d3962",
    "semantic_title": "lexically guided perceptual learning in mandarin chinese",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/davis17_interspeech.html": {
    "title": "The Effect of Spectral Profile on the Intelligibility of Emotional Speech in Noise",
    "volume": "main",
    "abstract": "The current study investigated why the intelligibility of expressive speech in noise varies as a function of the emotion expressed (e.g., happiness being more intelligible than sadness), even though the signal-to-noise ratio is the same. We tested the straightforward proposal that the expression of some emotions affect speech intelligibility by shifting spectral energy above the energy profile of the noise masker. This was done by determining how the spectral profile of speech is affected by different emotional expressions using three different expressive speech databases. We then examined if these changes were correlated with scores produced by an objective intelligibility metric. We found a relatively consistent shift in spectral energy for different emotions across the databases and a high correlation between the extent of these changes and the objective intelligibility scores. Moreover, the pattern of intelligibility scores is consistent with human perception studies (although there was considerable individual variation). We suggest that the intelligibility of emotion speech in noise is simply related to its audibility as conditioned by the effect that the expression of emotion has on its spectral profile",
    "checked": true,
    "id": "a8d7a2d2d3af19313fb5da98bfe7637c88970b49",
    "semantic_title": "the effect of spectral profile on the intelligibility of emotional speech in noise",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/maslowski17_interspeech.html": {
    "title": "Whether Long-Term Tracking of Speech Rate Affects Perception Depends on Who is Talking",
    "volume": "main",
    "abstract": "Speech rate is known to modulate perception of temporally ambiguous speech sounds. For instance, a vowel may be perceived as short when the immediate speech context is slow, but as long when the context is fast. Yet, effects of long-term tracking of speech rate are largely unexplored. Two experiments tested whether long-term tracking of rate influences perception of the temporal Dutch vowel contrast /ɑ/-/a:/. In Experiment 1, one low-rate group listened to ‘neutral' rate speech from talker A and to slow speech from talker B. Another high-rate group was exposed to the same neutral speech from A, but to fast speech from B. Between-group comparison of the ‘neutral' trials revealed that the low-rate group reported a higher proportion of /a:/ in A's ‘neutral' speech, indicating that A sounded faster when B was slow. Experiment 2 tested whether one's own speech rate also contributes to effects of long-term tracking of rate. Here, talker B's speech was replaced by playback of participants' own fast or slow speech. No evidence was found that one's own voice affected perception of talker A in larger speech contexts. These results carry implications for our understanding of the mechanisms involved in rate-dependent speech perception and of dialogue",
    "checked": true,
    "id": "4d5e5ee02b3bb51e208f028b06e27f009fd000f3",
    "semantic_title": "whether long-term tracking of speech rate affects perception depends on who is talking",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/peres17_interspeech.html": {
    "title": "Emotional Thin-Slicing: A Proposal for a Short- and Long-Term Division of Emotional Speech",
    "volume": "main",
    "abstract": "Human listeners are adept at successfully recovering linguistically- and socially-relevant information from very brief utterances. Studies using the ‘thin-slicing' approach show that accurate judgments of the speaker's emotional state can be made from minimal quantities of speech. The present experiment tested the performance of listeners exposed to thin-sliced samples of spoken Brazilian Portuguese selected to exemplify four emotions ( anger, fear, sadness, happiness). Rather than attaching verbal labels to the audio samples, participants were asked to pair the excerpts with averaged facial images illustrating the four emotion categories. Half of the listeners were native speakers of Brazilian Portuguese, while the others were native English speakers who knew no Portuguese. Both groups of participants were found to be accurate and consistent in assigning the audio samples to the expected emotion category, but some emotions were more reliably identified than others. Fear was misidentified most frequently. We conclude that the phonetic cues to speakers' emotional states are sufficiently salient and differentiated that listeners need only a few syllables upon which to base judgments, and that as a species we owe our perceptual sensitivity in this area to the survival value of being able to make rapid decisions concerning the psychological states of others",
    "checked": true,
    "id": "2b2119fc6b6bafd23f933914c4167530283c7f39",
    "semantic_title": "emotional thin-slicing: a proposal for a short- and long-term division of emotional speech",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/guevararukoz17_interspeech.html": {
    "title": "Predicting Epenthetic Vowel Quality from Acoustics",
    "volume": "main",
    "abstract": "Past research has shown that sound sequences not permitted in our native language may be distorted by our perceptual system. A well-documented example is vowel epenthesis, a phenomenon by which listeners hallucinate non-existent vowels within illegal consonantal sequences. As reported in previous work, this occurs for instance in Japanese (JP) and Brazilian Portuguese (BP), languages for which the ‘default' epenthetic vowels are /u/ and /i/, respectively. In a perceptual experiment, we corroborate the finding that the quality of this illusory vowel is language-dependent, but also that this default choice can be overridden by coarticulatory information present on the consonant cluster. In a second step, we analyse recordings of JP and BP speakers producing ‘epenthesized' versions of stimuli from the perceptual task. Results reveal that the default vowel corresponds to the vowel with the most reduced acoustic characteristics and whose formants are acoustically closest to formant transitions present in consonantal clusters. Lastly, we model behavioural responses from the perceptual experiment with an exemplar model using dynamic time warping (DTW)-based similarity measures on MFCCs",
    "checked": true,
    "id": "88dff104f0035574e0e9abee51e8d7124055e371",
    "semantic_title": "predicting epenthetic vowel quality from acoustics",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/matsui17_interspeech.html": {
    "title": "The Effect of Spectral Tilt on Size Discrimination of Voiced Speech Sounds",
    "volume": "main",
    "abstract": "A number of studies, with either voiced or unvoiced speech, have demonstrated that a speaker's geometric mean formant frequency (MFF) has a large effect on the perception of the speaker's size, as would be expected. One study with unvoiced speech showed that lifting the slope of the speech spectrum by 6 dB/octave also led to a reduction in the perceived size of the speaker. This paper reports an analogous experiment to determine whether lifting the slope of the speech spectrum by 6 dB/octave affects the perception of speaker size with voiced speech (words). The results showed that voiced speech with high-frequency enhancement was perceived to arise from smaller speakers. On average, the point of subjective equality in MFF discrimination was reduced by about 5%. However, there were large individual differences; some listeners were effectively insensitive to spectral enhancement of 6 dB/octave; others showed a consistent effect of the same enhancement. The results suggest that models of speaker size perception will need to include a listener specific parameter for the effect of spectral slope",
    "checked": true,
    "id": "564032b3709dc9539a64f5370e125ae7300950ef",
    "semantic_title": "the effect of spectral tilt on size discrimination of voiced speech sounds",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lorenzotrueba17_interspeech.html": {
    "title": "Misperceptions of the Emotional Content of Natural and Vocoded Speech in a Car",
    "volume": "main",
    "abstract": "This paper analyzes a) how often listeners interpret the emotional content of an utterance incorrectly when listening to vocoded or natural speech in adverse conditions; b) which noise conditions cause the most misperceptions; and c) which group of listeners misinterpret emotions the most. The long-term goal is to construct new emotional speech synthesizers that adapt to the environment and to the listener. We performed a large-scale listening test where over 400 listeners between the ages of 21 and 72 assessed natural and vocoded acted emotional speech stimuli. The stimuli had been artificially degraded using a room impulse response recorded in a car and various in-car noise types recorded in a real car. Experimental results show that the recognition rates for emotions and perceived emotional strength degrade as signal-to-noise ratio decreases. Interestingly, misperceptions seem to be more pronounced for negative and low-arousal emotions such as calmness or anger, while positive emotions such as happiness appear to be more robust to noise. An ANOVA analysis of listener meta-data further revealed that gender and age also influenced results, with elderly male listeners most likely to incorrectly identify emotions",
    "checked": true,
    "id": "64a5b2233094db9b082a3ff0eb16d8b217d6e7ed",
    "semantic_title": "misperceptions of the emotional content of natural and vocoded speech in a car",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/niebuhr17_interspeech.html": {
    "title": "The Relative Cueing Power of F0 and Duration in German Prominence Perception",
    "volume": "main",
    "abstract": "Previous studies showed for German and other (West) Germanic language, including English, that perceived syllable prominence is primarily controlled by changes in duration and F0, with the latter cue being more powerful than the former. Our study is an initial approach to develop this prominence hierarchy further by putting numbers on the interplay of duration and F0. German listeners indirectly judged through lexical identification the relative prominence levels of two neighboring syllables. Results show that an increase in F0 of between 0.49 and 0.76 st is required to outweigh the prominence effect of a 30% increase in duration of a neighboring syllable. These numbers are fairly stable across a large range of absolute F0 and duration levels and hence useful in speech technology",
    "checked": true,
    "id": "a7da20824d041ad054a52e880fb7edbdd16615ea",
    "semantic_title": "the relative cueing power of f0 and duration in german prominence perception",
    "citation_count": 12,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/marques17_interspeech.html": {
    "title": "Perception and Acoustics of Vowel Nasality in Brazilian Portuguese",
    "volume": "main",
    "abstract": "This study explores the relationship between identification, degree of nasality and vowel quality in oral, nasal and nasalized vowels in Brazilian Portuguese. Despite common belief that the language possesses contrastive nasal vowels, literature examination shows that nasal vowels may be followed by a nasal resonance, while nasalized vowels must be followed by a nasal consonant. It is argued that the nasal resonance may be the remains of a consonant that nasalizes the vowel, making nasal vowels simply coarticulatorily nasalized (e.g. [1]). If so, vowel nasality should not be more informative for the perception of a word containing a nasal vowel than for a word containing a nasalized vowel, as nasality is attributed to coarticulation. To test this hypothesis, randomized stimuli containing the first syllable of words with oral, nasal and nasalized vowels were presented to BP listeners who had to identify the stimuli original word. Preliminary results demonstrate that accuracy decreased for nasal and nasalized stimuli. A comparison between patterns of response to measured degrees of vowel acoustic nasality and formant values demonstrate that vowel quality differences may play a more relevant role in word identification than type of nasality in a vowel",
    "checked": true,
    "id": "68d0c63ebf60b539abab52a8e977d9b8da786b82",
    "semantic_title": "perception and acoustics of vowel nasality in brazilian portuguese",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kim17b_interspeech.html": {
    "title": "Sociophonetic Realizations Guide Subsequent Lexical Access",
    "volume": "main",
    "abstract": "Previous studies on spoken word recognition suggest that lexical access is facilitated when social information attributed to the voice is congruent with the social characteristics associated with the word. This paper builds on this work, presenting results from a lexical decision task in which target words associated with different age groups were preceded by sociophonetic primes. No age-related phonetic cues were provided within the target words; instead, the non-related prime words contained a sociophonetic variable involved in ongoing change. We found that age-associated words are recognized faster when preceded by an age-congruent phonetic variant in the prime word. The results demonstrate that lexical access is influenced by sociophonetic variation, a result which we argue arises from experience-based probabilities of covariation between sounds and words",
    "checked": true,
    "id": "458b5b6330fce26eeee7bd57d5330f736c0909e4",
    "semantic_title": "sociophonetic realizations guide subsequent lexical access",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/silva17_interspeech.html": {
    "title": "Critical Articulators Identification from RT-MRI of the Vocal Tract",
    "volume": "main",
    "abstract": "Several technologies, such as electromagnetic midsagittal articulography (EMA) or real-time magnetic resonance (RT-MRI), enable studying the static and dynamic aspects of speech production. The resulting knowledge can, in turn, inform the improvement of speech production models, e.g., for articulatory speech synthesis, by enabling the identification of which articulators and gestures are involved in producing specific sounds The amount of data available from these technologies, and the need for a systematic quantitative assessment, advise tackling these matters through data-driven approaches, preferably unsupervised, since annotated data is scarce. In this context, a method for statistical identification of critical articulators has been proposed, in the literature, and successfully applied to EMA data. However, the many differences regarding the data available from other technologies, such as RT-MRI, and language-specific aspects create a challenging setting for its direct and wider applicability In this article, we address the steps needed to extend the applicability of the proposed statistical analyses, initially applied to EMA, to an existing RT-MRI corpus and test it for a different language, European Portuguese. The obtained results, for three speakers, and considering 33 phonemes, provide phonologically meaningful critical articulator outcomes and show evidence of the applicability of the method to RT-MRI",
    "checked": true,
    "id": "cf9524ff4f764d2b277e9da7b7a338bc877e183d",
    "semantic_title": "critical articulators identification from rt-mri of the vocal tract",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/somandepalli17_interspeech.html": {
    "title": "Semantic Edge Detection for Tracking Vocal Tract Air-Tissue Boundaries in Real-Time Magnetic Resonance Images",
    "volume": "main",
    "abstract": "Recent developments in real-time magnetic resonance imaging (rtMRI) have enabled the study of vocal tract dynamics during production of running speech at high frame rates (e.g., 83 frames per second). Such large amounts of acquired data require scalable automated methods to identify different articulators (e.g., tongue, velum) for further analysis. In this paper, we propose a convolutional neural network with an encoder-decoder architecture to jointly detect the relevant air-tissue boundaries as well as to label them, which we refer to as ‘semantic edge detection'. We pose this as a pixel labeling problem, with the outline contour of each articulator of interest as positive class and the remaining tissue and airway as negative classes. We introduce a loss function modified with additional penalty for misclassification at air-tissue boundaries to account for class imbalance and improve edge localization. We then use a greedy search algorithm to draw contours from the probability maps of the positive classes predicted by the network. The articulator contours obtained by our method are comparable to the true labels generated by iteratively fitting a manually created subject-specific template. Our results generalize well across subjects and different vocal tract postures, demonstrating a significant improvement over the structured regression baseline",
    "checked": true,
    "id": "7c816a5babe0fa571450cedc0c1dc2265f82e0ba",
    "semantic_title": "semantic edge detection for tracking vocal tract air-tissue boundaries in real-time magnetic resonance images",
    "citation_count": 29,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/asadiabadi17_interspeech.html": {
    "title": "Vocal Tract Airway Tissue Boundary Tracking for rtMRI Using Shape and Appearance Priors",
    "volume": "main",
    "abstract": "Knowledge about the dynamic shape of the vocal tract is the basis of many speech production applications such as, articulatory analysis, modeling and synthesis. Vocal tract airway tissue boundary segmentation in the mid-sagittal plane is necessary as an initial step for extraction of the cross-sectional area function. This segmentation problem is however challenging due to poor resolution of real-time speech MRI, grainy noise and the rapidly varying vocal tract shape. We present a novel approach to vocal tract airway tissue boundary tracking by training a statistical shape and appearance model for human vocal tract. We manually segment a set of vocal tract profiles and utilize a statistical approach to train a shape and appearance model for the tract. An active contour approach is employed to segment the airway tissue boundaries of the vocal tract while restricting the curve movement to the trained shape and appearance model. Then the contours in subsequent frames are tracked using dense motion estimation methods. Experimental evaluations over the mean square error metric indicate significant improvements compared to the state-of-the-art",
    "checked": true,
    "id": "8d2b02ea41f3a1d497bcfa2c0dad3b75e3eed317",
    "semantic_title": "vocal tract airway tissue boundary tracking for rtmri using shape and appearance priors",
    "citation_count": 13,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ananthapadmanabha17_interspeech.html": {
    "title": "An Objective Critical Distance Measure Based on the Relative Level of Spectral Valley",
    "volume": "main",
    "abstract": "Spectral integration is a subjective phenomenon in which a vowel with two formants, spaced below a critical distance, is perceived to be of the same phonetic quality as that of a vowel with a single formant. It is tedious to conduct perceptual tests to determine the critical distance for various experimental conditions. To alleviate this difficulty, we propose an objective critical distance (OCD) that can be determined from the spectral envelope of a speech signal. OCD is defined as that spacing between the adjacent formants when the level of the spectral valley between them reaches the mean spectral value. The measured OCD lies in the same range of 3 to 3.5 Bark as the subjective critical distance for similar experimental conditions giving credibility to the definition. However, it is noted that OCD for front vowels is significantly different from that for the back vowels",
    "checked": true,
    "id": "c9a2596fe2aaf5cd10e1befffaa7d34bbf6263e1",
    "semantic_title": "an objective critical distance measure based on the relative level of spectral valley",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sorensen17_interspeech.html": {
    "title": "Database of Volumetric and Real-Time Vocal Tract MRI for Speech Science",
    "volume": "main",
    "abstract": "We present the USC Speech and Vocal Tract Morphology MRI Database, a 17-speaker magnetic resonance imaging database for speech research. The database consists of real-time magnetic resonance images (rtMRI) of dynamic vocal tract shaping, denoised audio recorded simultaneously with rtMRI, and 3D volumetric MRI of vocal tract shapes during sustained speech sounds. We acquired 2D real-time MRI of vocal tract shaping during consonant-vowel-consonant sequences, vowel-consonant-vowel sequences, read passages, and spontaneous speech. We acquired 3D volumetric MRI of the full set of vowels and continuant consonants of American English. Each 3D volumetric MRI was acquired in one 7-second scan in which the participant sustained the sound. This is the first database to combine rtMRI of dynamic vocal tract shaping and 3D volumetric MRI of the entire vocal tract. The database provides a unique resource with which to examine the relationship between vocal tract morphology and vocal tract function. The USC Speech and Vocal Tract Morphology MRI Database is provided free for research use at http://sail.usc.edu/span/morphdb",
    "checked": true,
    "id": "435217e38709ca2238ed0330fc01d70fdd59fc14",
    "semantic_title": "database of volumetric and real-time vocal tract mri for speech science",
    "citation_count": 21,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/cao17b_interspeech.html": {
    "title": "The Influence on Realization and Perception of Lexical Tones from Affricate's Aspiration",
    "volume": "main",
    "abstract": "Consonants in /CV/ syllables usually have potential influence on onset fundamental frequency (i.e., onset f0) of succeeding vowels. Previous studies showed such effect with respect to the aspiration of stops with evidence from Mandarin, a tonal language. While few studies investigated the effect on onset f0 from the aspiration of affricates. The differences between stops and affricates in aspiration leave space for further investigations. We examined the effect of affricate's aspiration on the realization of onset f0 of following vowels in the form of isolated syllables and continuous speech by reference to a minimal pair of syllables which differ only in aspiration. Besides, we conducted tone identification tests using two sets of tone continua based on the same minimal pair of syllables. Experimental results showed that the aspirated syllables increased the onset f0 of following vowels compared with unaspirated counterparts in both kinds of contexts. While the magnitude of differences varied with tones. And the perception results showed that aspirated syllables tended to be perceived as tones that have relative lower onset f0, which in turn supported the production result. The present study may have applications for speech identification and speech synthesis",
    "checked": true,
    "id": "28888dd79a2462d6429539e206ded49abd6b8ba8",
    "semantic_title": "the influence on realization and perception of lexical tones from affricate's aspiration",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/franken17_interspeech.html": {
    "title": "Audiovisual Recalibration of Vowel Categories",
    "volume": "main",
    "abstract": "One of the most daunting tasks of a listener is to map a continuous auditory stream onto known speech sound categories and lexical items. A major issue with this mapping problem is the variability in the acoustic realizations of sound categories, both within and across speakers. Past research has suggested listeners may use visual information (e.g., lip-reading) to calibrate these speech categories to the current speaker. Previous studies have focused on audiovisual recalibration of consonant categories. The present study explores whether vowel categorization, which is known to show less sharply defined category boundaries, also benefit from visual cues Participants were exposed to videos of a speaker pronouncing one out of two vowels, paired with audio that was ambiguous between the two vowels. After exposure, it was found that participants had recalibrated their vowel categories. In addition, individual variability in audiovisual recalibration is discussed. It is suggested that listeners' category sharpness may be related to the weight they assign to visual information in audiovisual speech perception. Specifically, listeners with less sharp categories assign more weight to visual information during audiovisual speech recognition",
    "checked": true,
    "id": "514fa68fb45771418b63ad021ca1b886bb607f47",
    "semantic_title": "audiovisual recalibration of vowel categories",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/peters17_interspeech.html": {
    "title": "The Effect of Gesture on Persuasive Speech",
    "volume": "main",
    "abstract": "Speech perception is multimodal, with not only speech, but also gesture presumably playing a role in how a message is perceived. However, there have not been many studies on the effect that hand gestures may have on speech perception in general, and on persuasive speech in particular. Moreover, we do not yet know whether an effect of gestures may be larger when addressees are not involved in the topic of the discourse, and are therefore more focused on peripheral cues, rather than the content of the message. In the current study participants were shown a speech with or without gestures. Some participants were involved in the topic of the speech, others were not. We studied five measures of persuasiveness. Results showed that for all but one measure, viewing the video with accompanying gestures made the speech more persuasive. In addition, there were several interactions, showing that the performance of the speaker and the factual accuracy of the speech scored high especially for those participants who not only saw gestures but were also not involved in the topic of the speech",
    "checked": true,
    "id": "d5f035bd94cc137960c922a0ebef42ec86ceffb4",
    "semantic_title": "the effect of gesture on persuasive speech",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lai17_interspeech.html": {
    "title": "Auditory-Visual Integration of Talker Gender in Cantonese Tone Perception",
    "volume": "main",
    "abstract": "This study investigated the auditory-visual integration of talker gender in the perception of tone variances. Two experiments were conducted to evaluate how listeners use the information of talker gender to adjust their expectation towards speakers' pitch range and uncover intended tonal targets in Cantonese tone perception. Results from an audio-only tone identification task showed that tone categorization along the same pitch continuum shifted under different conditions of voice gender. Listeners generally heard a tone of lower pitch when the word was produced by a female voice, while they heard a tone of higher pitch when the word was produced at the same pitch level by a male voice. Results from an audio-visual tone identification task showed that tone categorization along the same pitch continuum shifted under different conditions of face gender, despite the fact that the photos of different genders were disguised for the same set of stimuli in identical voices with identical pitch heights. These findings show that gender normalization plays a role in uncovering linguistic pitch targets, and lend support to a hypothesis according to which listeners make use of socially constructed stereotypes to facilitate their basic phonological categorization in speech perception and processing",
    "checked": true,
    "id": "63b50026b773732607a9e243925ff796909ab0b9",
    "semantic_title": "auditory-visual integration of talker gender in cantonese tone perception",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ito17_interspeech.html": {
    "title": "Event-Related Potentials Associated with Somatosensory Effect in Audio-Visual Speech Perception",
    "volume": "main",
    "abstract": "Speech perception often involves multisensory processing. Although previous studies have demonstrated visual [1, 2] and somatosensory interactions [3, 4] with auditory processing, it is not clear whether somatosensory information can contribute to the processing of audio-visual speech perception. This study explored the neural consequence of somatosensory interactions in audio-visual speech processing. We assessed whether somatosensory orofacial stimulation influenced event-related potentials (ERPs) in response to an audio-visual speech illusion (the McGurk Effect [1]). 64 scalp sites of ERPs were recorded in response to audio-visual speech stimulation and somatosensory stimulation. In the audio-visual condition, an auditory stimulus /ba/ was synchronized with the video of congruent facial motion (the production of /ba/) or incongruent facial motion (the production of the /da/: McGurk condition). These two audio-visual stimulations were randomly presented with and without somatosensory stimulation associated with facial skin deformation. We found ERPs differences associated with the McGurk effect in the presence of the somatosensory conditions. ERPs for the McGurk effect reliably diverge around 280 ms after auditory onset. The results demonstrate a change of cortical potential of audio-visual processing due to somatosensory inputs and suggest that somatosensory information encoding facial motion also influences speech processing",
    "checked": true,
    "id": "d4b6120e938858660d82e08233746d842ddbac49",
    "semantic_title": "event-related potentials associated with somatosensory effect in audio-visual speech perception",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/renner17_interspeech.html": {
    "title": "When a Dog is a Cat and How it Changes Your Pupil Size: Pupil Dilation in Response to Information Mismatch",
    "volume": "main",
    "abstract": "In the present study, we investigate pupil dilation as a measure of lexical retrieval. We captured pupil size changes in reaction to a match or a mismatch between a picture and an auditorily presented word in 120 trials presented to ten native speakers of Swedish. In each trial a picture was displayed for six seconds, and 2.5 seconds into the trial the word was played through loudspeakers. The picture and the word were matching in half of the trials, and all stimuli were common high-frequency monosyllabic Swedish words. The difference in pupil diameter trajectories across the two conditions was analyzed with Functional Data Analysis. In line with the expectations, the results indicate greater dilation in the mismatch condition starting from around 800 ms after the stimulus onset. Given that similar processes were observed in brain imaging studies, pupil dilation measurements seem to provide an appropriate tool to reveal lexical retrieval. The results suggest that pupillometry could be a viable alternative to existing methods in the field of speech and language processing, for instance across different ages and clinical groups",
    "checked": true,
    "id": "2eaf4130dccfd375cea7d1037f2215eceb0bb99b",
    "semantic_title": "when a dog is a cat and how it changes your pupil size: pupil dilation in response to information mismatch",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kyaw17_interspeech.html": {
    "title": "Cross-Modal Analysis Between Phonation Differences and Texture Images Based on Sentiment Correlations",
    "volume": "main",
    "abstract": "Motivated by the success of speech characteristics representation by color attributes, we analyzed the cross-modal sentiment correlations between voice source characteristics and textural image characteristics. For the analysis, we employed vowel sounds with representative three phonation differences (modal, creaky and breathy) and 36 texture images with 36 semantic attributes (e.g., banded, cracked and scaly) annotated one semantic attribute for each texture. By asking 40 subjects to select the most fitted textures from 36 figures with different textures after listening 30 speech samples with different phonations, we measured the correlations between acoustic parameters showing voice source variations and the parameters of selected textural image differences showing coarseness, contrast, directionality, busyness, complexity and strength. From the texture classifications, voice characteristics can be roughly characterized by textural differences: modal — gauzy, banded and smeared, creaky — porous, crystalline, cracked and scaly, breathy — smeared, freckled and stained. We have also found significant correlations between voice source acoustic parameters and textural parameters. These correlations suggest the possibility of cross-modal mapping between voice source characteristics and textural parameters, which enables visualization of speech information with source variations reflecting human sentiment perception",
    "checked": true,
    "id": "3abc020a625a5599bd5bb13043f7f22d52b0a487",
    "semantic_title": "cross-modal analysis between phonation differences and texture images based on sentiment correlations",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mehta17_interspeech.html": {
    "title": "Wireless Neck-Surface Accelerometer and Microphone on Flex Circuit with Application to Noise-Robust Monitoring of Lombard Speech",
    "volume": "main",
    "abstract": "Ambulatory monitoring of real-world voice characteristics and behavior has the potential to provide important assessment of voice and speech disorders and psychological and emotional state. In this paper, we report on the novel development of a lightweight, wireless voice monitor that synchronously records dual-channel data from an acoustic microphone and a neck-surface accelerometer embedded on a flex circuit. In this paper, Lombard speech effects were investigated in pilot data from four adult speakers with normal vocal function who read a phonetically balanced paragraph in the presence of different ambient acoustic noise levels. Whereas the signal-to-noise ratio (SNR) of the microphone signal decreased in the presence of increasing ambient noise level, the SNR of the accelerometer sensor remained high. Lombard speech properties were thus robustly computed from the accelerometer signal and observed in all four speakers who exhibited increases in average estimates of sound pressure level (+2.3 dB), fundamental frequency (+21.4 Hz), and cepstral peak prominence (+1.3 dB) from quiet to loud ambient conditions. Future work calls for ambulatory data collection in naturalistic environments, where the microphone acts as a sound level meter and the accelerometer functions as a noise-robust voicing sensor to assess voice disorders, neurological conditions, and cognitive load",
    "checked": true,
    "id": "1afc1f765d62b85cb8a82d0086ec1b95f2c5748a",
    "semantic_title": "wireless neck-surface accelerometer and microphone on flex circuit with application to noise-robust monitoring of lombard speech",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bandini17_interspeech.html": {
    "title": "Video-Based Tracking of Jaw Movements During Speech: Preliminary Results and Future Directions",
    "volume": "main",
    "abstract": "Facial (e.g., lips and jaw) movements can provide important information for the assessment, diagnosis and treatment of motor speech disorders. However, due to the high costs of the instrumentation used to record speech movements, such information is typically limited to research studies. With the recent development of depth sensors and efficient algorithms for facial tracking, clinical applications of this technology may be possible. Although lip tracking methods have been validated in the past, jaw tracking remains a challenge. In this study, we assessed the accuracy of tracking jaw movements with a video-based system composed of a face tracker and a depth sensor, specifically developed for short range applications (Intel RealSense SR300). The assessment was performed on healthy subjects during speech and non-speech tasks. Preliminary results showed that jaw movements can be tracked with reasonable accuracy (RMSE≈2mm), with better performance for slow movements. Further tests are needed in order to improve the performance of these systems and develop accurate methodologies that can reveal subtle changes in jaw movements for the assessment and treatment of motor speech disorders",
    "checked": true,
    "id": "f7428622667a1cc5b86fe83254136ad9db65da09",
    "semantic_title": "video-based tracking of jaw movements during speech: preliminary results and future directions",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sb17_interspeech.html": {
    "title": "Accurate Synchronization of Speech and EGG Signal Using Phase Information",
    "volume": "main",
    "abstract": "Synchronization of speech and corresponding Electroglottographic (EGG) signal is very helpful for speech processing research and development. During simultaneous recording of speech and EGG signals, the speech signal will be delayed by the duration corresponding to the speech wave propagation from the glottis to the microphone relative to the EGG signal. Even in same session of recording, the delay between the speech and the EGG signals is varying due to the natural movement of speaker's head and movement of microphone in case MIC is held by hand. To study and model the information within glottal cycles, precise synchronization of speech and EGG signals is of utmost necessity. In this work, we propose a method for synchronization of speech and EGG signals based on the glottal activity information present in the signals. The performance of the proposed method is demonstrated by estimation of delay between the two signals (speech signals and corresponding EGG signals) and synchronizing these signals by compensating the estimated delay. The CMU-Arctic database consist of simultaneous recording of the speech and the EGG signals is used for the evaluation of the proposed method",
    "checked": true,
    "id": "99f53c1117ead2974c5ff7729a77a6755809e133",
    "semantic_title": "accurate synchronization of speech and egg signal using phase information",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/romren17_interspeech.html": {
    "title": "The Acquisition of Focal Lengthening in Stockholm Swedish",
    "volume": "main",
    "abstract": "In order to be efficient communicators, children need to adapt their utterances to the common ground shared between themselves and their conversational partners. One way of doing this is by prosodically highlighting focal information. In this paper we look at one specific prosodic manipulation, namely word duration, asking whether Swedish-speaking children lengthen words to mark focus, as compared to adult controls. To the best of our knowledge, this is the first study on the relationship between focus and word duration in Swedish-speaking children",
    "checked": true,
    "id": "901106c767818b7405cf2f5dc0abf1c2f19504c8",
    "semantic_title": "the acquisition of focal lengthening in stockholm swedish",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhou17_interspeech.html": {
    "title": "Multilingual Recurrent Neural Networks with Residual Learning for Low-Resource Speech Recognition",
    "volume": "main",
    "abstract": "The shared-hidden-layer multilingual deep neural network (SHL-MDNN), in which the hidden layers of feed-forward deep neural network (DNN) are shared across multiple languages while the softmax layers are language dependent, has been shown to be effective on acoustic modeling of multilingual low-resource speech recognition. In this paper, we propose that the shared-hidden-layer with Long Short-Term Memory (LSTM) recurrent neural networks can achieve further performance improvement considering LSTM has outperformed DNN as the acoustic model of automatic speech recognition (ASR). Moreover, we reveal that shared-hidden-layer multilingual LSTM (SHL-MLSTM) with residual learning can yield additional moderate but consistent gain from multilingual tasks given the fact that residual learning can alleviate the degradation problem of deep LSTMs. Experimental results demonstrate that SHL-MLSTM can relatively reduce word error rate (WER) by 2.1–6.8% over SHL-MDNN trained using six languages and 2.6–7.3% over monolingual LSTM trained using the language specific data on CALLHOME datasets. Additional WER reduction, about relatively 2% over SHL-MLSTM, can be obtained through residual learning on CALLHOME datasets, which demonstrates residual learning is useful for SHL-MLSTM on multilingual low-resource ASR",
    "checked": true,
    "id": "da650416734a210981b4cdcda63d9dc0f920219a",
    "semantic_title": "multilingual recurrent neural networks with residual learning for low-resource speech recognition",
    "citation_count": 21,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/siohan17_interspeech.html": {
    "title": "CTC Training of Multi-Phone Acoustic Models for Speech Recognition",
    "volume": "main",
    "abstract": "Phone-sized acoustic units such as triphones cannot properly capture the long-term co-articulation effects that occur in spontaneous speech. For that reason, it is interesting to construct acoustic units covering a longer time-span such as syllables or words. Unfortunately, the frequency distribution of those units is such that a few high frequency units account for most of the tokens, while many units rarely occur. As a result, those units suffer from data sparsity and can be difficult to train. In this paper we propose a scalable data-driven approach to construct a set of salient units made of sequences of phones called M-phones. We illustrate that since the decomposition of a word sequence into a sequence of M-phones is ambiguous, those units are well suited to be used with a connectionist temporal classification (CTC) approach which does not rely on an explicit frame-level segmentation of the word sequence into a sequence of acoustic units. Experiments are presented on a Voice Search task using 12,500 hours of training data",
    "checked": true,
    "id": "15067b905682139b5c4d0f642bb019249923a56b",
    "semantic_title": "ctc training of multi-phone acoustic models for speech recognition",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tong17_interspeech.html": {
    "title": "An Investigation of Deep Neural Networks for Multilingual Speech Recognition Training and Adaptation",
    "volume": "main",
    "abstract": "Different training and adaptation techniques for multilingual Automatic Speech Recognition (ASR) are explored in the context of hybrid systems, exploiting Deep Neural Networks (DNN) and Hidden Markov Models (HMM). In multilingual DNN training, the hidden layers (possibly extracting bottleneck features) are usually shared across languages, and the output layer can either model multiple sets of language-specific senones or one single universal IPA-based multilingual senone set. Both architectures are investigated, exploiting and comparing different language adaptive training (LAT) techniques originating from successful DNN-based speaker-adaptation. More specifically, speaker adaptive training methods such as Cluster Adaptive Training (CAT) and Learning Hidden Unit Contribution (LHUC) are considered. In addition, a language adaptive output architecture for IPA-based universal DNN is also studied and tested Experiments show that LAT improves the performance and adaptation on the top layer further improves the accuracy. By combining state-level minimum Bayes risk (sMBR) sequence training with LAT, we show that a language adaptively trained IPA-based universal DNN outperforms a monolingually sequence trained model",
    "checked": true,
    "id": "190b71beb0b27fdcf596812b2d935fa3e10a1092",
    "semantic_title": "an investigation of deep neural networks for multilingual speech recognition training and adaptation",
    "citation_count": 42,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/karafiat17_interspeech.html": {
    "title": "2016 BUT Babel System: Multilingual BLSTM Acoustic Model with i-Vector Based Adaptation",
    "volume": "main",
    "abstract": "The paper provides an analysis of BUT automatic speech recognition systems (ASR) built for the 2016 IARPA Babel evaluation. The IARPA Babel program concentrates on building ASR system for many low resource languages, where only a limited amount of transcribed speech is available for each language. In such scenario, we found essential to train the ASR systems in a multilingual fashion. In this work, we report superior results obtained with pre-trained multilingual BLSTM acoustic models, where we used multi-task training with separate classification layer for each language. The results reported on three Babel Year 4 languages show over 3% absolute WER reductions obtained from such multilingual pre-training. Experiments with different input features show that the multilingual BLSTM performs the best with simple log-Mel-filter-bank outputs, which makes our previously successful multilingual stack bottleneck features with CMLLR adaptation obsolete. Finally, we experiment with different configurations of i-vector based speaker adaptation in the mono- and multi-lingual BLSTM architectures. This results in additional WER reductions over 1% absolute",
    "checked": true,
    "id": "ea174d36c2d3ebc42664455064c676b5df866235",
    "semantic_title": "2016 but babel system: multilingual blstm acoustic model with i-vector based adaptation",
    "citation_count": 17,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/matassoni17_interspeech.html": {
    "title": "Optimizing DNN Adaptation for Recognition of Enhanced Speech",
    "volume": "main",
    "abstract": "Speech enhancement directly using deep neural network (DNN) is of major interest due to the capability of DNN to tangibly reduce the impact of noisy conditions in speech recognition tasks. Similarly, DNN based acoustic model adaptation to new environmental conditions is another challenging topic. In this paper we present an analysis of acoustic model adaptation in presence of a disjoint speech enhancement component, identifying an optimal setting for improving the speech recognition performance. Adaptation is derived from a consolidated technique that introduces in the training process a regularization term to prevent overfitting. We propose to optimize the adaptation of the clean acoustic models towards the enhanced speech by tuning the regularization term based on the degree of enhancement. Experiments on a popular noisy dataset (e.g., AURORA-4) demonstrate the validity of the proposed approach",
    "checked": true,
    "id": "3c873111fbae4f11f6f2c0d817540dbb7b2d7409",
    "semantic_title": "optimizing dnn adaptation for recognition of enhanced speech",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kim17c_interspeech.html": {
    "title": "Deep Least Squares Regression for Speaker Adaptation",
    "volume": "main",
    "abstract": "Recently, speaker adaptation methods in deep neural networks (DNNs) have been widely studied for automatic speech recognition. However, almost all adaptation methods for DNNs have to consider various heuristic conditions such as mini-batch sizes, learning rate scheduling, stopping criteria, and initialization conditions because of the inherent property of a stochastic gradient descent (SGD)-based training process. Unfortunately, those heuristic conditions are hard to be properly tuned. To alleviate those difficulties, in this paper, we propose a least squares regression-based speaker adaptation method in a DNN framework utilizing posterior mean of each class. Also, we show how the proposed method can provide a unique solution which is quite easy and fast to calculate without SGD. The proposed method was evaluated in the TED-LIUM corpus. Experimental results showed that the proposed method achieved up to a 4.6% relative improvement against a speaker independent DNN. In addition, we report further performance improvement of the proposed method with speaker-adapted features",
    "checked": true,
    "id": "1dc883dbe0d5545fd3513f45fbc730cca2f9bbe8",
    "semantic_title": "deep least squares regression for speaker adaptation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/do17_interspeech.html": {
    "title": "Multi-Task Learning Using Mismatched Transcription for Under-Resourced Speech Recognition",
    "volume": "main",
    "abstract": "It is challenging to obtain large amounts of native (matched) labels for audio in under-resourced languages. This could be due to a lack of literate speakers of the language or a lack of universally acknowledged orthography. One solution is to increase the amount of labeled data by using mismatched transcription, which employs transcribers who do not speak the language (in place of native speakers), to transcribe what they hear as nonsense speech in their own language (e.g., Mandarin). This paper presents a multi-task learning framework where the DNN acoustic model is simultaneously trained using both a limited amount of native (matched) transcription and a larger set of mismatched transcription. We find that by using a multi-task learning framework, we achieve improvements over monolingual baselines and previously proposed mismatched transcription adaptation techniques. In addition, we show that using alignments provided by a GMM adapted by mismatched transcription further improves acoustic modeling performance. Our experiments on Georgian data from the IARPA Babel program show the effectiveness of the proposed method",
    "checked": true,
    "id": "0fdf5679df010da76a4510006e40d8742942952e",
    "semantic_title": "multi-task learning using mismatched transcription for under-resourced speech recognition",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/joy17_interspeech.html": {
    "title": "Generalized Distillation Framework for Speaker Normalization",
    "volume": "main",
    "abstract": "Generalized distillation framework has been shown to be effective in speech enhancement in the past. We extend this idea to speaker normalization without any explicit adaptation data in this paper. In the generalized distillation framework, we assume the presence of some \"privileged\" information to guide the training process in addition to the training data. In the proposed approach, the privileged information is obtained from a \"teacher\" model, trained on speaker-normalized FMLLR features. The \"student\" model is trained on un-normalized filterbank features and uses teacher's supervision for cross-entropy training. The proposed distillation method does not need first pass decode information during testing and imposes no constraints on the duration of the test data for computing speaker-specific transforms unlike in FMLLR or i-vector. Experiments done on Switchboard and AMI corpus show that the generalized distillation framework shows improvement over un-normalized features with or without i-vectors",
    "checked": true,
    "id": "2904a5c43940a2d1da84d4c4a387cb17de987ffb",
    "semantic_title": "generalized distillation framework for speaker normalization",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/samarakoon17_interspeech.html": {
    "title": "Learning Factorized Transforms for Unsupervised Adaptation of LSTM-RNN Acoustic Models",
    "volume": "main",
    "abstract": "Factorized Hidden Layer (FHL) adaptation has been proposed for speaker adaptation of deep neural network (DNN) based acoustic models. In FHL adaptation, a speaker-dependent (SD) transformation matrix and an SD bias are included in addition to the standard affine transformation. The SD transformation is a linear combination of rank-1 matrices whereas the SD bias is a linear combination of vectors. Recently, the Long Short-Term Memory (LSTM) Recurrent Neural Networks (RNNs) have shown to outperform DNN acoustic models in many Automatic Speech Recognition (ASR) tasks. In this work, we investigate the effectiveness of SD transformations for LSTM-RNN acoustic models. Experimental results show that when combined with scaling of LSTM cell states' outputs, SD transformations achieve 2.3% and 2.1% absolute improvements over the baseline LSTM systems for the AMI IHM and AMI SDM tasks respectively",
    "checked": true,
    "id": "9329958df2f1216af0235a0bca6c0e34b048b37a",
    "semantic_title": "learning factorized transforms for unsupervised adaptation of lstm-rnn acoustic models",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/fainberg17_interspeech.html": {
    "title": "Factorised Representations for Neural Network Adaptation to Diverse Acoustic Environments",
    "volume": "main",
    "abstract": "Adapting acoustic models jointly to both speaker and environment has been shown to be effective. In many realistic scenarios, however, either the speaker or environment at test time might be unknown, or there may be insufficient data to learn a joint transform. Generating independent speaker and environment transforms improves the match of an acoustic model to unseen combinations. Using i-vectors, we demonstrate that it is possible to factorise speaker or environment information using multi-condition training with neural networks. Specifically, we extract bottleneck features from networks trained to classify either speakers or environments. We perform experiments on the Wall Street Journal corpus combined with environment noise from the Diverse Environments Multichannel Acoustic Noise Database. Using the factorised i-vectors we show improvements in word error rates on perturbed versions of the eval92 and dev93 test sets, both when one factor is missing and when the factors are seen but not in the desired combination",
    "checked": true,
    "id": "3cb509620c4b69d014b5048b3fdf0f7952aebce9",
    "semantic_title": "factorised representations for neural network adaptation to diverse acoustic environments",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sproat17_interspeech.html": {
    "title": "An RNN Model of Text Normalization",
    "volume": "main",
    "abstract": "We present a recurrent neural net (RNN) model of text normalization — defined as the mapping of written text to its spoken form, and a description of the open-source dataset that we used in our experiments. We show that while the RNN model achieves very high overall accuracies, there remain errors that would be unacceptable in a speech application like TTS. We then show that a simple FST-based filter can help mitigate those errors. Even with that mitigation challenges remain, and we end the paper outlining some possible solutions. In releasing our data we are thereby inviting others to help solve this problem",
    "checked": true,
    "id": "499225b952b07877f3767415a0094406a4b9ffb8",
    "semantic_title": "an rnn model of text normalization",
    "citation_count": 42,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/rendel17_interspeech.html": {
    "title": "Weakly-Supervised Phrase Assignment from Text in a Speech-Synthesis System Using Noisy Labels",
    "volume": "main",
    "abstract": "The proper segmentation of an input text string into meaningful intonational phrase units is a fundamental task in the text-processing component of a text-to-speech (TTS) system that generates intelligible and natural synthesis. In this work we look at the creation of a symbolic, phrase-assignment model within the front end (FE) of a North American English TTS system when high-quality labels for supervised learning are unavailable and/or potentially mismatched to the target corpus and domain. We explore a labeling scheme that merges heuristics derived from (i) automatic high-quality phonetic alignments, (ii) linguistic rules, and (iii) a legacy acoustic phrase-labeling system to arrive at a ground truth that can be used to train a bidirectional recurrent neural network model. We evaluate the performance of this model in terms of objective metrics describing categorical phrase assignment within the FE proper, as well as on the effect that these intermediate labels carry onto the TTS back end for the task of continuous prosody prediction (i.e., intonation and duration contours, and pausing). For this second task, we rely on subjective listening tests and demonstrate that the proposed system significantly outperforms a linguistic rules-based baseline for two different synthetic voices",
    "checked": true,
    "id": "f7ffa465ab9980126d74a16c628a2235189861b7",
    "semantic_title": "weakly-supervised phrase assignment from text in a speech-synthesis system using noisy labels",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ijima17_interspeech.html": {
    "title": "Prosody Aware Word-Level Encoder Based on BLSTM-RNNs for DNN-Based Speech Synthesis",
    "volume": "main",
    "abstract": "Recent studies have shown the effectiveness of the use of word vectors in DNN-based speech synthesis. However, these word vectors trained from a large amount of text generally carry not prosodic information, which is important information for speech synthesis, but semantic information. Therefore, if word vectors that take prosodic information into account can be obtained, it would be expected to improve the quality of synthesized speech. In this paper, to obtain word-level vectors that take prosodic information into account, we propose a novel prosody aware word-level encoder. A novel point of the proposed technique is to train a word-level encoder by using a large speech corpus constructed for automatic speech recognition. A word-level encoder that estimates the F0 contour for each word from the input word sequence is trained. The outputs of the bottleneck layer in the trained encoder are used as the word-level vector. By training the relationship between words and their prosodic information by using large speech corpus, the outputs of the bottleneck layer would be expected to contain prosodic information. The results of objective and subjective experiments indicate the proposed technique can synthesize speech with improved naturalness",
    "checked": true,
    "id": "b0e758bd38511b13e2712ad3a013633af7fb682c",
    "semantic_title": "prosody aware word-level encoder based on blstm-rnns for dnn-based speech synthesis",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ni17_interspeech.html": {
    "title": "Global Syllable Vectors for Building TTS Front-End with Deep Learning",
    "volume": "main",
    "abstract": "Recent vector space representations of words have succeeded in capturing syntactic and semantic regularities. In the context of text-to-speech (TTS) synthesis, a front-end is a key component for extracting multi-level linguistic features from text, where syllable acts as a link between low- and high-level features. This paper describes the use of global syllable vectors as features to build a front-end, particularly evaluated in Chinese. The global syllable vectors directly capture global statistics of syllable-syllable co-occurrences in a large-scale text corpus. They are learned by a global log-bilinear regression model in an unsupervised manner, whilst the front-end is built using deep bidirectional recurrent neural networks in a supervised fashion. Experiments are conducted on large-scale Chinese speech and treebank text corpora, evaluating grapheme to phoneme (G2P) conversion, word segmentation, part of speech (POS) tagging, phrasal chunking, and pause break prediction. Results show that the proposed method is efficient for building a compact and robust front-end with high performance. The global syllable vectors can be acquired relatively cheaply from plain text resources, therefore, they are vital to develop multilingual speech synthesis, especially for under-resourced language modeling",
    "checked": true,
    "id": "4173177f47461dafca636777c60bd1e12aab6e62",
    "semantic_title": "global syllable vectors for building tts front-end with deep learning",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/fukuoka17_interspeech.html": {
    "title": "Prosody Control of Utterance Sequence for Information Delivering",
    "volume": "main",
    "abstract": "We propose a conversational speech synthesis system in which the prosodic features of each utterance are controlled throughout the entire input text. We have developed a \"news-telling system,\" which delivered news articles through spoken language. The speech synthesis system for the news-telling should be able to highlight utterances containing noteworthy information in the article with a particular way of speaking so as to impress them on the users. To achieve this, we introduced role and position features of the individual utterances in the article into the control parameters for prosody generation throughout the text. We defined three categories for the role feature: a nucleus (which is assigned to the utterance including the noteworthy information), a front satellite (which precedes the nucleus) and a rear satellite (which follows the nucleus). We investigated how the prosodic features differed depending on the role and position features through an analysis of news-telling speech data uttered by a voice actress. We designed the speech synthesis system on the basis of a deep neural network having the role and position features added to its input layer. Objective and subjective evaluation results showed that introducing those features was effective in the speech synthesis for the information delivering",
    "checked": true,
    "id": "0cb0cb06d8303499ad080a0f49c4e96c0f857093",
    "semantic_title": "prosody control of utterance sequence for information delivering",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/huang17c_interspeech.html": {
    "title": "Multi-Task Learning for Prosodic Structure Generation Using BLSTM RNN with Structured Output Layer",
    "volume": "main",
    "abstract": "Prosodic structure generation from text plays an important role in Chinese text-to-speech (TTS) synthesis, which greatly influences the naturalness and intelligibility of the synthesized speech. This paper proposes a multi-task learning method for prosodic structure generation using bidirectional long short-term memory (BLSTM) recurrent neural network (RNN) and structured output layer (SOL). Unlike traditional methods where prerequisites such as lexicon word or even syntactic tree are usually required as the input, the proposed method predicts prosodic boundary labels directly from Chinese characters. BLSTM RNN is used to capture the bidirectional contextual dependencies of prosodic boundary labels. SOL further models correlations between prosodic structures, lexicon words as well as part-of-speech (POS), where the prediction of prosodic boundary labels are conditioned upon word tokenization and POS tagging results. Experimental results demonstrate the effectiveness of the proposed method",
    "checked": true,
    "id": "1b8372912fd6bde2f0d213bc31bee12f0a40a417",
    "semantic_title": "multi-task learning for prosodic structure generation using blstm rnn with structured output layer",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zheng17_interspeech.html": {
    "title": "Investigating Efficient Feature Representation Methods and Training Objective for BLSTM-Based Phone Duration Prediction",
    "volume": "main",
    "abstract": "Accurate modeling and prediction of speech-sound durations are important in generating natural synthetic speech. This paper focuses on both feature and training objective aspects to improve the performance of the phone duration model for speech synthesis system. In feature aspect, we combine the feature representation from gradient boosting decision tree (GBDT) and phoneme identity embedding model (which is realized by the jointly training of phoneme embedded vector (PEV) and word embedded vector (WEV)) for BLSTM to predict the phone duration. The PEV is used to replace the one-hot phoneme identity, and GBDT is utilized to transform the traditional contextual features. In the training objective aspect, a new training objective function which taking into account of the correlation and consistency between the predicted utterance and the natural utterance is proposed. Perceptual tests indicate the proposed methods could improve the naturalness of the synthetic speech, which benefits from the proposed feature representation methods could capture more precise contextual features, and the proposed training objective function could tackle the over-averaged problem for the generated phone durations",
    "checked": true,
    "id": "036c8943a91ee5749dd4a4d8e6e44b3fb54420d5",
    "semantic_title": "investigating efficient feature representation methods and training objective for blstm-based phone duration prediction",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17d_interspeech.html": {
    "title": "Discrete Duration Model for Speech Synthesis",
    "volume": "main",
    "abstract": "The acoustic model and the duration model are the two major components in statistical parametric speech synthesis (SPSS) systems. The neural network based acoustic model makes it possible to model phoneme duration at phone-level instead of state-level in conventional hidden Markov model (HMM) based SPSS systems. Since the duration of phonemes is countable value, the distribution of the phone-level duration is discrete given the linguistic features, which means the Gaussian hypothesis is no longer necessary. This paper provides an investigation on the performance of LSTM-RNN duration model that directly models the probability of the countable duration values given linguistic features using cross entropy as criteria. The multi-task learning is also experimented at the same time, with a comparison to the standard LSTM-RNN duration model in objective and subjective measures. The result shows that directly modeling the discrete distribution has its benefit and multi-task model achieves better performance in phone-level duration modeling",
    "checked": true,
    "id": "4d5a73b160ed297050b0b388ae88012350cdd9ba",
    "semantic_title": "discrete duration model for speech synthesis",
    "citation_count": 11,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17e_interspeech.html": {
    "title": "Comparison of Modeling Target in LSTM-RNN Duration Model",
    "volume": "main",
    "abstract": "Speech duration is an important component in statistical parameter speech synthesis(SPSS). In LSTM-RNN based SPSS system, the speech duration affects the quality of synthesized speech in two aspects, the prosody of speech and the position features in acoustic model. This paper investigated the effects of duration in LSTM-RNN based SPSS system. The performance of the acoustic models with position features at different levels are compared. Also, duration models with different network architectures are presented. A method to utilize the priori knowledge that the sum of state duration of a phoneme should be equal to the phone duration is proposed and proved to have better performance in both state duration and phone duration modeling. The result shows that acoustic model with state-level position features has better performance in acoustic modeling (especially in voice/unvoice classification), which means state-level duration model still has its advantage and the duration models with the priori knowledge can result in better speech quality",
    "checked": true,
    "id": "6a58a50cef15d59925fb888390a4226d5d32c3d5",
    "semantic_title": "comparison of modeling target in lstm-rnn duration model",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ribeiro17_interspeech.html": {
    "title": "Learning Word Vector Representations Based on Acoustic Counts",
    "volume": "main",
    "abstract": "This paper presents a simple count-based approach to learning word vector representations by leveraging statistics of co-occurrences between text and speech. This type of representation requires two discrete sequences of units defined across modalities. Two possible methods for the discretization of an acoustic signal are presented, which are then applied to fundamental frequency and energy contours of a transcribed corpus of speech, yielding a sequence of textual objects (e.g. words, syllables) aligned with a sequence of discrete acoustic events. Constructing a matrix recording the co-occurrence of textual objects with acoustic events and reducing its dimensionality with matrix decomposition results in a set of context-independent representations of word types. These are applied to the task of acoustic modelling for speech synthesis; objective and subjective results indicate that these representations are useful for the generation of acoustic parameters in a text-to-speech (TTS) system. In general, we observe that the more discretization approaches, acoustic signals, and levels of linguistic analysis are incorporated into a TTS system via these count-based representations, the better that TTS system performs",
    "checked": true,
    "id": "c0d3d9f3c1f0e019663ab2c5c450355cd7f3152c",
    "semantic_title": "learning word vector representations based on acoustic counts",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/szekely17_interspeech.html": {
    "title": "Synthesising Uncertainty: The Interplay of Vocal Effort and Hesitation Disfluencies",
    "volume": "main",
    "abstract": "As synthetic voices become more flexible, and conversational systems gain more potential to adapt to the environmental and social situation, the question needs to be examined, how different modifications to the synthetic speech interact with each other and how their specific combinations influence perception. This work investigates how the vocal effort of the synthetic speech together with added disfluencies affect listeners' perception of the degree of uncertainty in an utterance. We introduce a DNN voice built entirely from spontaneous conversational speech data and capable of producing a continuum of vocal efforts, prolongations and filled pauses with a corpus-based method. Results of a listener evaluation indicate that decreased vocal effort, filled pauses and prolongation of function words increase the degree of perceived uncertainty of conversational utterances expressing the speaker's beliefs. We demonstrate that the effect of these three cues are not merely additive, but that interaction effects, in particular between the two types of disfluencies and between vocal effort and prolongations need to be considered when aiming to communicate a specific level of uncertainty. The implications of these findings are relevant for adaptive and incremental conversational systems using expressive speech synthesis and aspiring to communicate the attitude of uncertainty",
    "checked": true,
    "id": "6398aee06c840d0332fb90853bcaa3b80ab9f660",
    "semantic_title": "synthesising uncertainty: the interplay of vocal effort and hesitation disfluencies",
    "citation_count": 19,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/oktem17_interspeech.html": {
    "title": "Prosograph: A Tool for Prosody Visualisation of Large Speech Corpora",
    "volume": "main",
    "abstract": "This paper presents an open-source tool that has been developed to visualize a speech corpus with its transcript and prosodic features aligned at word level. In particular, the tool is aimed at providing a simple and clear way to visualize prosodic patterns along large segments of speech corpora, and can be applied in any research that involves prosody analysis",
    "checked": true,
    "id": "e33fc523c7677de80f610fd9920dcaab5ee393d4",
    "semantic_title": "prosograph: a tool for prosody visualisation of large speech corpora",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/vetchinnikova17_interspeech.html": {
    "title": "ChunkitApp: Investigating the Relevant Units of Online Speech Processing",
    "volume": "main",
    "abstract": "This paper presents a web-based application for tablets ‘ChunkitApp' developed to investigate chunking in online speech processing. The design of the app is based on recent theoretical developments in linguistics and cognitive science, and in particular on the suggestions of Linear Unit Grammar [1]. The data collected using the app provides evidence for the reality of online chunking in language processing and the validity of the construct. In addition to experimental uses, the app has potential applications in language education and speech recognition",
    "checked": true,
    "id": "4fb5c8bbfdacb088e44912fe6cf56493794d79b2",
    "semantic_title": "chunkitapp: investigating the relevant units of online speech processing",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jochim17_interspeech.html": {
    "title": "Extending the EMU Speech Database Management System: Cloud Hosting, Team Collaboration, Automatic Revision Control",
    "volume": "main",
    "abstract": "In this paper, we introduce a new component of the EMU Speech Database Management System [1, 2] to improve the team workflow of handling production data (both acoustic and physiological) in phonetics and the speech sciences. It is named emuDB Manager, and it facilitates the coordination of team efforts, possibly distributed over several nations, by introducing automatic revision control (based on Git), cloud hosting (in private clouds provided by the researchers themselves or a third party), by keeping track of which parts of the database have already been edited (and by whom), and by centrally collecting and making searchable the notes made during the edit process",
    "checked": true,
    "id": "9debe14abe9acb069f9998dbd73b44f7d965efa8",
    "semantic_title": "extending the emu speech database management system: cloud hosting, team collaboration, automatic revision control",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/warlaumont17_interspeech.html": {
    "title": "HomeBank: A Repository for Long-Form Real-World Audio Recordings of Children",
    "volume": "main",
    "abstract": "HomeBank is a new component of the TalkBank system, focused on long-form (i.e., multi-hour, typically daylong) real-world recordings of children's language experiences, and it is linked to a GitHub repository in which tools for analyzing those recordings can be shared. HomeBank constitutes not only a rich resource for researchers interested in early language acquisition specifically, but also for those seeking to study spontaneous speech, media exposure, and audio environments more generally. This Show and Tell describes the procedures for accessing and contributing HomeBank data and code. It also overviews the current contents of the repositories, and provides some examples of audio recordings, available transcriptions, and currently available analysis tools",
    "checked": true,
    "id": "dff69c0bf85eda41acd1a0e3f98e00a02914aba0",
    "semantic_title": "homebank: a repository for long-form real-world audio recordings of children",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bell17_interspeech.html": {
    "title": "A System for Real Time Collaborative Transcription Correction",
    "volume": "main",
    "abstract": "We present a system to enable efficient, collaborative human correction of ASR transcripts, designed to operate in real-time situations, for example, when post-editing live captions generated for news broadcasts. In the system, confusion networks derived from ASR lattices are used to highlight low-confident words and present alternatives to the user for quick correction. The system uses a client-server architecture, whereby information about each manual edit is posted to the server. Such information can be used to dynamically update the one-best ASR output for all utterances currently in the editing pipeline. We propose to make updates in three different ways; by finding a new one-best path through an existing ASR lattice consistent with the correction received; by identifying further instances of out-of-vocabulary terms entered by the user; and by adapting the language model on the fly. Updates are received asynchronously by the client",
    "checked": true,
    "id": "2b5322873c60424d7f8442951bff2471d3248e00",
    "semantic_title": "a system for real time collaborative transcription correction",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bhat17_interspeech.html": {
    "title": "MoPAReST — Mobile Phone Assisted Remote Speech Therapy Platform",
    "volume": "main",
    "abstract": "Through this paper, we present the Mobile Phone Assisted Remote Speech Therapy Platform for individuals with speech disabilities to avail the benefits of therapy remotely with minimal face-to-face sessions with the Speech Language Pathologist (SLP). The objective is to address the skewed ratio of SLP to patients as well increase the efficacy of the therapy by keeping the patient engaged more frequently albeit asynchronously and remotely. The platform comprises (1) A web-interface to be used by the SLP to monitor the progress of their patients at a time convenient to them and (2) A mobile application along with speech processing algorithms to provide instant feedback to the patient. We envision this platform to cut down the therapy time, especially for rural Indian patients. Evaluation of this platform is being done for five patients with mis-articulation in Marathi language",
    "checked": true,
    "id": "af5e8db3a5daa43fafe6ede33d9b7578b04e09b1",
    "semantic_title": "moparest - mobile phone assisted remote speech therapy platform",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jaumardhakoun17_interspeech.html": {
    "title": "An Apparatus to Investigate Western Opera Singing Skill Learning Using Performance and Result Biofeedback, and Measuring its Neural Correlates",
    "volume": "main",
    "abstract": "We present our preliminary developments on a biofeedback interface for Western operatic style training, combining performance and result biofeedback. Electromyographic performance feedbacks, as well as formant-tuning result feedbacks are displayed visually, using continuously scrolling displays, or discrete post-trial evaluations. Our final aim is to investigate electroencephalographic (EEG) measurements in order to identify neural correlates of feedback-based skill learning",
    "checked": true,
    "id": "502dd2534e6233fd0165e57fac527b3585a99818",
    "semantic_title": "an apparatus to investigate western opera singing skill learning using performance and result biofeedback, and measuring its neural correlates",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/draxler17_interspeech.html": {
    "title": "PercyConfigurator — Perception Experiments as a Service",
    "volume": "main",
    "abstract": "PercyConfigurator is an experiment editor that eliminates the need for programming; the experiment definition and content are simply dropped onto the PercyConfigurator web page for interactive editing and testing. When the editing is done, the experiment definition and content are uploaded to the server. The server returns a link to the experiment which is then distributed to potential participants The Bavarian Archive for Speech Signals (BAS) hosts PercyConfigurator as a free service to the academic community",
    "checked": true,
    "id": "fe61af3a82cc996b78e0f7bce4b02ce9c12985a0",
    "semantic_title": "percyconfigurator - perception experiments as a service",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/salimbajevs17_interspeech.html": {
    "title": "System for Speech Transcription and Post-Editing in Microsoft Word",
    "volume": "main",
    "abstract": "In this demonstration paper, we introduce a transcription service that can be used for transcription of different meetings, sessions etc. The service performs speaker diarization, automatic speech recognition, punctuation restoration and produces human-readable transcripts as special Microsoft Word documents that have audio and word alignments embedded. Thereby, a widely-used word processor is transformed into a transcription post-editing tool. Currently, Latvian and Lithuanian languages are supported, but other languages can be easily added",
    "checked": true,
    "id": "d0942abd9a3f1d27811d58b0330b8c2eea91fd86",
    "semantic_title": "system for speech transcription and post-editing in microsoft word",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/park17_interspeech.html": {
    "title": "Emojive! Collecting Emotion Data from Speech and Facial Expression Using Mobile Game App",
    "volume": "main",
    "abstract": "We developed Emojive!, a mobile game app to make emotion recognition from audio and image interactive and fun, motivating the users to play with the app. The game is to act out a specific emotion, among six emotion labels (happy, sad, anger, anxiety, loneliness, criticism), given by the system. Double player mode lets two people to compete their acting skills. The more users play the game, the more emotion-labelled data will be acquired. We are using deep Convolutional Neural Network (CNN) models to recognize emotion from audio and facial image in real-time with a mobile front-end client including intuitive user interface and simple data visualization",
    "checked": true,
    "id": "fea0f385a0023c7096fcd7c9e2109d9e3358b03a",
    "semantic_title": "emojive! collecting emotion data from speech and facial expression using mobile game app",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lennes17_interspeech.html": {
    "title": "Mylly — The Mill: A New Platform for Processing Speech and Text Corpora Easily and Efficiently",
    "volume": "main",
    "abstract": "Speech and language researchers need to manage and analyze increasing quantities of material. Various tools are available for various stages of the work, but they often require the researcher to use different interfaces and to convert the output from each tool into suitable input for the next one The Language Bank of Finland (Kielipankki) is developing an on-line platform called Mylly for processing speech and language data in a graphical user interface that integrates different tools into a single workflow. Mylly provides tools and computational resources for processing material and for the inspecting the results. The tools plugged into Mylly include a parser, morphological analyzers, generic finite-state technology, and a speech recognizer. Users can upload data and download any intermediate results in the tool chain. Mylly runs on CSC's Taito cluster and is an instance of the Chipster platform. Access rights to Mylly are given for academic use The Language Bank of Finland is a collection of corpora, tools and other services maintained by FIN-CLARIN, a consortium of Finnish universities and research organizations coordinated by the University of Helsinki. The technological infrastructure for the Language Bank of Finland is provided by CSC – IT Center for Science",
    "checked": true,
    "id": "b42c184106e4be7938df15c2f21afdb384917b28",
    "semantic_title": "mylly - the mill: a new platform for processing speech and text corpora easily and efficiently",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/suzuki17_interspeech.html": {
    "title": "Visual Learning 2: Pronunciation App Using Ultrasound, Video, and MRI",
    "volume": "main",
    "abstract": "We demonstrate Visual Learning 2, an English pronunciation app for second-language (L2) learners and phonetics students. This iOS app links together audio, front and side video, MRI and ultrasound movies of a native speaker reading a phonetically balanced text. Users can watch and shadow front and side video overlaid with an ultrasound tongue movie. They are able to play the video at three speeds and start the video from any word by tapping on it, with a choice of display in either English or IPA. Users can record their own audio/video and play it back in sync with the model for comparison",
    "checked": true,
    "id": "ea7de7b9ae7d06832913cdf671849969b9184a64",
    "semantic_title": "visual learning 2: pronunciation app using ultrasound, video, and mri",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/allen17_interspeech.html": {
    "title": "Dialogue as Collaborative Problem Solving",
    "volume": "main",
    "abstract": "I will describe the current status of a long-term effort at developing dialogue systems that go beyond simple task execution models to systems that involve collaborative problem solving. Such systems involve open-ended discussion and the tasks cannot be accomplished without extensive interaction (e.g., 10 turns or more). The key idea is that dialogue itself arises from an agent's ability for collaborative problem solving (CPS). In such dialogues, agents may introduce, modify and negotiate goals; propose and discuss the merits possible paths to solutions; explicitly discuss progress as the two agents work towards the goals; and evaluate how well a goal was accomplished. To complicate matters, user utterances in such settings are much more complex than seen in simple task execution dialogues and requires full semantic parsing. A key question we have been exploring in the past few years is how much of dialogue can be accounted for by domain-independent mechanisms. I will discuss these issues and draw examples from a dialogue system we have built that, except for the specialized domain reasoning required in each case, uses the same architecture to perform three different tasks: collaborative blocks world planning, when the system and user build structures and may have differing goals; biocuration, in which a biologist and the system interact in order to build executable causal models of biological pathways; and collaborative composition, where the user and system collaborate to compose simple pieces of music",
    "checked": true,
    "id": "c7c65d31949aacbc83921e280e5f36e6c808a808",
    "semantic_title": "dialogue as collaborative problem solving",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/stasak17_interspeech.html": {
    "title": "Elicitation Design for Acoustic Depression Classification: An Investigation of Articulation Effort, Linguistic Complexity, and Word Affect",
    "volume": "main",
    "abstract": "Assessment of neurological and psychiatric disorders like depression are unusual from a speech processing perspective, in that speakers can be prompted or instructed in what they should say (e.g. as part of a clinical assessment). Despite prior speech-based depression studies that have used a variety of speech elicitation methods, there has been little evaluation of the best elicitation mode. One approach to understand this better is to analyze an existing database from the perspective of articulation effort, word affect, and linguistic complexity measures as proxies for depression sub-symptoms (e.g. psychomotor retardation, negative stimulus suppression, cognitive impairment). Here a novel measure for quantifying articulation effort is introduced, and when applied experimentally to the DAIC corpus shows promise for identifying speech data that are more discriminative of depression. Interestingly, experiment results demonstrate that by selecting speech with higher articulation effort, linguistic complexity, or word-based arousal/valence, improvements in acoustic speech-based feature depression classification performance can be achieved, serving as a guide for future elicitation design",
    "checked": true,
    "id": "46daf35c87f68efafb6352926a99677104319d39",
    "semantic_title": "elicitation design for acoustic depression classification: an investigation of articulation effort, linguistic complexity, and word affect",
    "citation_count": 21,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/novoa17_interspeech.html": {
    "title": "Robustness Over Time-Varying Channels in DNN-HMM ASR Based Human-Robot Interaction",
    "volume": "main",
    "abstract": "This paper addresses the problem of time-varying channels in speech-recognition-based human-robot interaction using Locally-Normalized Filter-Bank features (LNFB), and training strategies that compensate for microphone response and room acoustics. Testing utterances were generated by re-recording the Aurora-4 testing database using a PR2 mobile robot, equipped with a Kinect audio interface while performing head rotations and movements toward and away from a fixed source. Three training conditions were evaluated called Clean, 1-IR and 33-IR. With Clean training, the DNN-HMM system was trained using the Aurora-4 clean training database. With 1-IR training, the same training data were convolved with an impulse response estimated at one meter from the source with no rotation of the robot head. With 33-IR training, the Aurora-4 training data were convolved with impulse responses estimated at one, two and three meters from the source and 11 angular positions of the robot head. The 33-IR training method produced reductions in WER greater than 50% when compared with Clean training using both LNFB and conventional Mel filterbank features. Nevertheless, LNFB features provided a WER 23% lower than MelFB using 33-IR training. The use of 33-IR training and LNFB features reduced WER by 64% compared to Clean training and MelFB features",
    "checked": true,
    "id": "2beda4bb9b72b15fb23453c25ebaf68ceab4e846",
    "semantic_title": "robustness over time-varying channels in dnn-hmm asr based human-robot interaction",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/turker17_interspeech.html": {
    "title": "Analysis of Engagement and User Experience with a Laughter Responsive Social Robot",
    "volume": "main",
    "abstract": "We explore the effect of laughter perception and response in terms of engagement in human-robot interaction. We designed two distinct experiments in which the robot has two modes: laughter responsive and laughter non-responsive. In responsive mode, the robot detects laughter using a multimodal real-time laughter detection module and invokes laughter as a backchannel to users accordingly. In non-responsive mode, robot has no utilization of detection, thus provides no feedback. In the experimental design, we use a straightforward question-answer based interaction scenario using a back-projected robot head. We evaluate the interactions with objective and subjective measurements of engagement and user experience",
    "checked": true,
    "id": "e97124ba9d246dc0e3b7487c849ed338a2ddcb96",
    "semantic_title": "analysis of engagement and user experience with a laughter responsive social robot",
    "citation_count": 17,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/baird17_interspeech.html": {
    "title": "Automatic Classification of Autistic Child Vocalisations: A Novel Database and Results",
    "volume": "main",
    "abstract": "Humanoid robots have in recent years shown great promise for supporting the educational needs of children on the autism spectrum. To further improve the efficacy of such interactions, user-adaptation strategies based on the individual needs of a child are required. In this regard, the proposed study assesses the suitability of a range of speech-based classification approaches for automatic detection of autism severity according to the commonly used Social Responsiveness Scale second edition (SRS-2). Autism is characterised by socialisation limitations including child language and communication ability. When compared to neurotypical children of the same age these can be a strong indication of severity. This study introduces a novel dataset of 803 utterances recorded from 14 autistic children aged between 4–10 years, during Wizard-of-Oz interactions with a humanoid robot. Our results demonstrate the suitability of support vector machines (SVMs) which use acoustic feature sets from multiple Interspeech COMPARE challenges. We also evaluate deep spectrum features, extracted via an image classification convolutional neural network (CNN) from the spectrogram of autistic speech instances. At best, by using SVMs on the acoustic feature sets, we achieved a UAR of 73.7% for the proposed 3-class task",
    "checked": true,
    "id": "8102aaeb504d51b8133a5acb18bc9dc0de24fd5b",
    "semantic_title": "automatic classification of autistic child vocalisations: a novel database and results",
    "citation_count": 36,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/oertel17_interspeech.html": {
    "title": "Crowd-Sourced Design of Artificial Attentive Listeners",
    "volume": "main",
    "abstract": "Feedback generation is an important component of human-human communication. Humans can choose to signal support, understanding, agreement or also scepticism by means of feedback tokens. Many studies have focused on the timing of feedback behaviours. In the current study, however, we keep the timing constant and instead focus on the lexical form and prosody of feedback tokens as well as their sequential patterns For this we crowdsourced participant's feedback behaviour in identical interactional contexts in order to model a virtual agent that is able to provide feedback as an attentive/supportive as well as attentive/sceptical listener. The resulting models were realised in a robot which was evaluated by third-party observers",
    "checked": true,
    "id": "e9b15b5b74f2d3591c20e0a699ed08ed84a01e15",
    "semantic_title": "crowd-sourced design of artificial attentive listeners",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lancia17_interspeech.html": {
    "title": "Studying the Link Between Inter-Speaker Coordination and Speech Imitation Through Human-Machine Interactions",
    "volume": "main",
    "abstract": "According to accounts of inter-speaker coordination based on internal predictive models, speakers tend to imitate each other each time they need to coordinate their behavior. According to accounts based on the notion of dynamical coupling, imitation should be observed only if it helps stabilizing the specific coordinative pattern produced by the interlocutors or if it is a direct consequence of inter-speaker coordination. To compare these accounts, we implemented an artificial agent designed to repeat a speech utterance while coordinating its behavior with that of a human speaker performing the same task. We asked 10 Italian speakers to repeat the utterance /topkop/ simultaneously with the agent during short time intervals. In some interactions, the agent was parameterized to cooperate with the speakers (by producing its syllables simultaneously with those of the human) while in others it was parameterized to compete with them (by producing its syllables in-between those of the human). A positive correlation between the stability of inter-speaker coordination and the degree of f0 imitation was observed only in cooperative interactions. However, in line with accounts based on prediction, speakers imitate the f0 of the agent regardless of whether this is parameterized to cooperate or to compete with them",
    "checked": true,
    "id": "127ca0b3b865019f7283826262676b1c5e5cd6fc",
    "semantic_title": "studying the link between inter-speaker coordination and speech imitation through human-machine interactions",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/delalez17_interspeech.html": {
    "title": "Adjusting the Frame: Biphasic Performative Control of Speech Rhythm",
    "volume": "main",
    "abstract": "Performative time and pitch scaling is a new research paradigm for prosodic analysis by synthesis. In this paper, a system for real-time recorded speech time and pitch scaling by the means of hands or feet gestures is designed and evaluated. Pitch is controlled with the preferred hand, using a stylus on a graphic tablet. Time is controlled using rhythmic frames, or constriction gestures, defined by pairs of control points. The \"Arsis\" corresponds to the constriction (weak beat of the syllable) and the \"Thesis\" corresponds to the vocalic nucleus (strong beat of the syllable). This biphasic control of rhythmic units is performed by the non-preferred hand using a button. Pitch and time scales are modified according to these gestural controls with the help of a real-time pitch synchronous overlap-add technique (RT-PSOLA). Rhythm and pitch control accuracy are assessed in a prosodic imitation experiment: the task is to reproduce intonation and rhythm of various sentences. The results show that inter-vocalic durations differ on average of only 20 ms. The system appears as a new and effective tool for performative speech and singing synthesis. Consequences and applications in speech prosody research are discussed",
    "checked": true,
    "id": "5686b901dc53803657687f0df335cdc1878f7c45",
    "semantic_title": "adjusting the frame: biphasic performative control of speech rhythm",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/saryazdi17_interspeech.html": {
    "title": "Attentional Factors in Listeners' Uptake of Gesture Cues During Speech Processing",
    "volume": "main",
    "abstract": "In conversation, speakers spontaneously produce manual gestures that can facilitate listeners' comprehension of speech. However, various factors may affect listeners' ability to use gesture cues. Here we examine a situation where a speaker is referring to physical objects in the contextual here-and-now. In this situation, objects for potential reference will compete with gestures for visual attention. In two experiments, a speaker provided instructions to pick up objects in the visual environment (\" Pick up the candy\"). On some trials, the speaker produced a \"pick up\" gesture that reflected the size/shape of the target object. Gaze position was recorded to evaluate how listeners allocated attention to scene elements. Experiment 1 showed that, although iconic gestures (when present) were rarely fixated directly, peripheral uptake of these cues speeded listeners' visual identification of intended referents as the instruction unfolded. However, the benefit was mild and occurred primarily for small/hard-to-identify objects. In Experiment 2, background noise was added to reveal whether challenging auditory environments lead listeners to allocate additional visual attention to gesture cues in a compensatory manner. Interestingly, background noise actually reduced listeners' use of gesture cues. Together the findings highlight how situational factors govern the use of visual cues during multimodal communication",
    "checked": true,
    "id": "3c4b29a7cecb7b7085f811708dcab527cedd422a",
    "semantic_title": "attentional factors in listeners' uptake of gesture cues during speech processing",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ishi17_interspeech.html": {
    "title": "Motion Analysis in Vocalized Surprise Expressions",
    "volume": "main",
    "abstract": "The background of our research is the generation of natural human-like motions during speech in android robots that have a highly human-like appearance. Mismatches in speech and motion are sources of unnaturalness, especially when emotion expressions are involved. Surprise expressions often occur in dialogue interactions, and they are often accompanied by verbal interjectional utterances. In this study, we analyze facial, head and body motions during several types of vocalized surprise expressions appearing in human-human dialogue interactions. The analysis results indicate an inter-dependence between motion types and different types of surprise expression (such as emotional, social or quoted) as well as different degrees of surprise expression. The synchronization between motion and surprise utterances is also analyzed",
    "checked": true,
    "id": "549e5ddb8053f43a829dfe0ed53b009dec174d53",
    "semantic_title": "motion analysis in vocalized surprise expressions",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ruede17_interspeech.html": {
    "title": "Enhancing Backchannel Prediction Using Word Embeddings",
    "volume": "main",
    "abstract": "Backchannel responses like \"uh-huh\", \"yeah\", \"right\" are used by the listener in a social dialog as a way to provide feedback to the speaker. In the context of human-computer interaction, these responses can be used by an artificial agent to build rapport in conversations with users. In the past, multiple approaches have been proposed to detect backchannel cues and to predict the most natural timing to place those backchannel utterances. Most of these are based on manually optimized fixed rules, which may fail to generalize. Many systems rely on the location and duration of pauses and pitch slopes of specific lengths. In the past, we proposed an approach by training artificial neural networks on acoustic features such as pitch and power and also attempted to add word embeddings via word2vec. In this work, we refined this approach by evaluating different methods to add timed word embeddings via word2vec. Comparing the performance using various feature combinations, we could show that adding linguistic features improves the performance over a prediction system that only uses acoustic features",
    "checked": true,
    "id": "6627a1e0f4ddeb593dcc0cfc6cbc95a402b746c6",
    "semantic_title": "enhancing backchannel prediction using word embeddings",
    "citation_count": 21,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/raveh17_interspeech.html": {
    "title": "A Computational Model for Phonetically Responsive Spoken Dialogue Systems",
    "volume": "main",
    "abstract": "This paper introduces a model for segment-level phonetic responsiveness. It is based on behavior observed in human-human interaction, and is designed to be integrated into spoken dialogue systems to capture potential phonetic variation and simulate convergence capabilities. Each step in the process is responsible for an aspect of the interaction, including monitoring the input speech and appropriately analyzing it. Various parameters can be tuned to configure the speech handling and adjust the response style. Evaluation was performed by simulating simple end-to-end dialogue scenarios, including analyzing the synthesized output of the model. The results show promising ground for further extensions",
    "checked": true,
    "id": "ce5569e53e775ce85f3ccf35add29a3f8d59b6ca",
    "semantic_title": "a computational model for phonetically responsive spoken dialogue systems",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ebhotemhen17_interspeech.html": {
    "title": "Incremental Dialogue Act Recognition: Token- vs Chunk-Based Classification",
    "volume": "main",
    "abstract": "This paper presents a machine learning based approach to incremental dialogue act classification with a focus on the recognition of communicative functions associated with dialogue segments in a multidimensional space, as defined in the ISO 24617-2 dialogue act annotation standard. The main goal is to establish the nature of an increment whose processing will result in a reliable overall system performance. We explore scenarios where increments are tokens or syntactically, semantically or prosodically motivated chunks. Combing local classification with meta-classifiers at a late fusion decision level we obtained state-of-the-art classification performance. Experiments were carried out on manually corrected transcriptions and on potentially erroneous ASR output. Chunk-based classification yields better results on the manual transcriptions, whereas token-based classification shows a more robust performance on the ASR output. It is also demonstrated that layered hierarchical and cascade training procedures result in better classification performance than the single-layered approach based on a joint classification predicting complex class labels",
    "checked": true,
    "id": "27f69b94a3f3d4a70d4bd2d448a6e55d826e9387",
    "semantic_title": "incremental dialogue act recognition: token- vs chunk-based classification",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/niebuhr17b_interspeech.html": {
    "title": "Clear Speech — Mere Speech? How Segmental and Prosodic Speech Reduction Shape the Impression That Speakers Create on Listeners",
    "volume": "main",
    "abstract": "Research on speech reduction is primarily concerned with analyzing, modeling, explaining, and, ultimately, predicting phonetic variation. That is, the focus is on the speech signal itself. The present paper adds a little side note to this fundamental line of research by addressing the question whether variation in the degree of reduction also has a systematic effect on the attributes we ascribe to the speaker who produces the speech signal. A perception experiment was carried out for German in which 46 listeners judged whether or not speakers showing 3 different combinations of segmental and prosodic reduction levels (unreduced, moderately reduced, strongly reduced) are appropriately described by 13 physical, social, and cognitive attributes. The experiment shows that clear speech is not mere speech, and less clear speech is not just reduced either. Rather, results revealed a complex interplay of reduction levels and perceived speaker attributes in which moderate reduction can make a better impression on listeners than no reduction. In addition to its relevance in reduction models and theories, this interplay is instructive for various fields of speech application from social robotics to charisma coaching",
    "checked": true,
    "id": "a11af0e71aa252dabf583c99cdb0f803f2a7fabb",
    "semantic_title": "clear speech - mere speech? how segmental and prosodic speech reduction shape the impression that speakers create on listeners",
    "citation_count": 13,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kouklia17_interspeech.html": {
    "title": "Relationships Between Speech Timing and Perceived Hostility in a French Corpus of Political Debates",
    "volume": "main",
    "abstract": "This study investigates the relationship between perceived hostility and speech timing features within extracts from Montreuil's City Council sessions in 2013, marked by a tense political context at this time. A dataset of 118 speech extracts from the mayor (Dominique Voynet) and four of her political opponents during the City Council has been analyzed through the combination of perception tests and speech timing phenomena, estimated from classical timing-related measurements and custom metrics. We also develop a methodological framework for the phonetic analysis of nonscripted speech: a double perceptive evaluation of the original dataset (22 participants) allowed us to measure the difference of hostility perceived (dHost) between the original audio extracts and their read transcriptions, and the five speakers produced the same utterances in a controlled reading task to make the direct comparison with original extracts possible. Correlations between dHost and speech timing features differences between each original utterance and its control counterpart show that perceived hostility is mainly influenced by local deviations to the expected accentuation pattern in French combined with the insertion of silent pauses. Moreover, a finer-grained analysis of rhythmic features reveals different strategies amongst speakers, especially regarding the realization of interpausal speech rate variation and final syllables lengthening",
    "checked": true,
    "id": "c5a538ea5bc6ca643ce9ccabfbb3506ad8208ec7",
    "semantic_title": "relationships between speech timing and perceived hostility in a french corpus of political debates",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gallardo17_interspeech.html": {
    "title": "Towards Speaker Characterization: Identifying and Predicting Dimensions of Person Attribution",
    "volume": "main",
    "abstract": "A great number of investigations on person characterization rely on the assessment of the Big-Five personality traits, a prevalent and widely accepted model with strong psychological foundation. However, in the context on characterizing unfamiliar individuals from their voices only, it may be hard for assessors to determine the Big-Five traits based on their first impression. In this study, a 28-item semantic differential rating scale has been completed by a total of 33 listeners who were presented with 15 male voice stimuli. A factor analysis on their responses enabled us to identify five perceptual factors of person attribution: (social and physical) attractiveness, confidence, apathy, serenity, and incompetence. A discussion on the relations of these dimensions of speaker attribution to the Big-Five factors is provided and speech features relevant to the automatic prediction of our dimensions are analyzed, together with SVM regression performance. Although more data are needed to validate our findings, we believe that our approach can lead to establish a space of person attributions with dimensions that can easily be detected from utterances in zero-acquaintance scenarios",
    "checked": true,
    "id": "d5f0aa9727a8224075eb0ffca5f8fe0f87331043",
    "semantic_title": "towards speaker characterization: identifying and predicting dimensions of person attribution",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ishi17b_interspeech.html": {
    "title": "Prosodic Analysis of Attention-Drawing Speech",
    "volume": "main",
    "abstract": "The term \"attention drawing\" refers to the action of sellers who call out to get the attention of people passing by in front of their stores or shops to invite them inside to buy or sample products. Since the speaking styles exhibited in such attention-drawing speech are clearly different from conversational speech, in this study, we focused on prosodic analyses of attention-drawing speech and collected the speech data of multiple people with previous attention-drawing experience by simulating several situations. We then investigated the effects of several factors, including background noise, interaction phases, and shop categories on the prosodic features of attention-drawing utterances. Analysis results indicate that compared to dialogue interaction utterances, attention-drawing utterances usually have higher power, higher mean F0s, smaller F0 ranges, and do not drop at the end of sentences, regardless of the presence or absence of background noise. Analysis of sentence-final syllable intonation indicates the presence of lengthened flat or rising tones in attention-drawing utterances",
    "checked": true,
    "id": "e041470ced09fc59ff5b40adaa152bfc9a629e6d",
    "semantic_title": "prosodic analysis of attention-drawing speech",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/simpson17_interspeech.html": {
    "title": "Perceptual and Acoustic CorreLates of Gender in the Prepubertal Voice",
    "volume": "main",
    "abstract": "This study investigates the perceptual and acoustic correlates of gender in the prepubertal voice. 23 German-speaking primary school pupils (13 female, 10 male) aged 8–9 years were recorded producing 10 sentences each. Two sentences from each speaker were presented in random order to a group of listeners who were asked to assign a gender to each stimulus. Single utterances from each of the three male and three female speakers whose gender was identified most reliably were played in a second experiment to two further groups of listeners who judged each stimulus against seven perceptual attribute pairs. Acoustic analysis of those parameters corresponding most directly to the perceptual attributes revealed a number of highly significant correlations, indicating some aspects of the voice and speech (f0, harmonics-to-noise ratio, tempo) that children use to construct and adults use to identify gender in the prepubertal voice",
    "checked": true,
    "id": "6e55a68c6499594414fa1385f36a85ae8a7e2886",
    "semantic_title": "perceptual and acoustic correlates of gender in the prepubertal voice",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/schweitzer17_interspeech.html": {
    "title": "To See or not to See: Interlocutor Visibility and Likeability Influence Convergence in Intonation",
    "volume": "main",
    "abstract": "In this paper we look at convergence and divergence in intonation in the context of social qualities. Specifically we examine pitch accent realisations in the GECO corpus of German conversations. Pitch accents are represented as 6-dimensional vectors where each dimension corresponds to a characteristic of the accent's shape. Convergence/divergence is then measured by calculating the distance between pitch accent realisations of conversational partners. A decrease of distance values over time indicates convergence, an increase divergence. The corpus comprises dialogue sessions in two modalities: partners either saw each other during the conversation or not. Linear mixed model analyses show convergence as well as divergence effects in the realisations of H*L accents. This convergence/divergence is strongly related to the modality and to how much speakers like their partners: generally, seeing the partner comes with divergence, whereas when the dialogue partners cannot see each other, there is convergence. The effect varies, however, depending on the extent to which a speaker likes their partner. Less liking entails a greater change in the realisations over time — stronger divergence when partners could see each other, and stronger convergence when they could not",
    "checked": true,
    "id": "c69a3010fabf8e8ebac0cef24670a790af5790cc",
    "semantic_title": "to see or not to see: interlocutor visibility and likeability influence convergence in intonation",
    "citation_count": 14,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/weirich17_interspeech.html": {
    "title": "Acoustic Correlates of Parental Role and Gender Identity in the Speech of Expecting Parents",
    "volume": "main",
    "abstract": "Differences between male and female speakers have been explained in terms of biological inevitabilities but also in terms of behavioral and socially motivated factors. The aim of this study is to investigate the latter by examining gender-specific variability within the same gender The speech of 29 German men and women — all of them expecting their first child but varying in the time they plan to stay at home during their child's first year (parental role) — is analyzed. Acoustic analyses comprise the vowel space size and the realization of the inter-sibilant contrast While the data is part of a larger longitudinal project investigating adult- and infant-directed speech during the infant's first year of life, this study concentrates on the recordings made before the birth of the child. Inter-speaker variability is investigated in relation to 1) the chosen parental role and 2) self-ascribed ratings on positive feminine attributes (gender identity) Results show that both factors (planned duration of parental leave and the femininity ratings) contribute to the variability found between, but also within the same gender. In particular, the vowel space size was found to be positively correlated with self-ascribed femininity ratings in male speakers",
    "checked": true,
    "id": "f706b059d82357f9a92be366471e942bac6db260",
    "semantic_title": "acoustic correlates of parental role and gender identity in the speech of expecting parents",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/soleraurena17_interspeech.html": {
    "title": "A Semi-Supervised Learning Approach for Acoustic-Prosodic Personality Perception in Under-Resourced Domains",
    "volume": "main",
    "abstract": "Automatic personality analysis has gained attention in the last years as a fundamental dimension in human-to-human and human-to-machine interaction. However, it still suffers from limited number and size of speech corpora for specific domains, such as the assessment of children's personality. This paper investigates a semi-supervised training approach to tackle this scenario. We devise an experimental setup with age and language mismatch and two training sets: a small labeled training set from the Interspeech 2012 Personality Sub-challenge, containing French adult speech labeled with personality OCEAN traits, and a large unlabeled training set of Portuguese children's speech. As test set, a corpus of Portuguese children's speech labeled with OCEAN traits is used. Based on this setting, we investigate a weak supervision approach that iteratively refines an initial model trained with the labeled data-set using the unlabeled data-set. We also investigate knowledge-based features, which leverage expert knowledge in acoustic-prosodic cues and thus need no extra data. Results show that, despite the large mismatch imposed by language and age differences, it is possible to attain improvements with these techniques, pointing both to the benefits of using a weak supervision and expert-based acoustic-prosodic features across age and language",
    "checked": true,
    "id": "9542bcf34857660b988b358712637ea7e2f04887",
    "semantic_title": "a semi-supervised learning approach for acoustic-prosodic personality perception in under-resourced domains",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tatman17_interspeech.html": {
    "title": "Effects of Talker Dialect, Gender & Race on Accuracy of Bing Speech and YouTube Automatic Captions",
    "volume": "main",
    "abstract": "This project compares the accuracy of two automatic speech recognition (ASR) systems — Bing Speech and YouTube's automatic captions — across gender, race and four dialects of American English. The dialects included were chosen for their acoustic dissimilarity. Bing Speech had differences in word error rate (WER) between dialects and ethnicities, but they were not statistically reliable. YouTube's automatic captions, however, did have statistically different WERs between dialects and races. The lowest average error rates were for General American and white talkers, respectively. Neither system had a reliably different WER between genders, which had been previously reported for YouTube's automatic captions [1]. However, the higher error rate non-white talkers is worrying, as it may reduce the utility of these systems for talkers of color",
    "checked": true,
    "id": "1080dc00733e010fdd6a9b999506a0d4d864519d",
    "semantic_title": "effects of talker dialect, gender & race on accuracy of bing speech and youtube automatic captions",
    "citation_count": 83,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/prabhavalkar17_interspeech.html": {
    "title": "A Comparison of Sequence-to-Sequence Models for Speech Recognition",
    "volume": "main",
    "abstract": "In this work, we conduct a detailed evaluation of various all-neural, end-to-end trained, sequence-to-sequence models applied to the task of speech recognition. Notably, each of these systems directly predicts graphemes in the written domain, without using an external pronunciation lexicon, or a separate language model. We examine several sequence-to-sequence models including connectionist temporal classification (CTC), the recurrent neural network (RNN) transducer, an attention-based model, and a model which augments the RNN transducer with an attention mechanism We find that the sequence-to-sequence models are competitive with traditional state-of-the-art approaches on dictation test sets, although the baseline, which uses a separate pronunciation and language model, outperforms these models on voice-search test sets",
    "checked": true,
    "id": "6cc68e8adf34b580f3f37d1bd267ee701974edde",
    "semantic_title": "a comparison of sequence-to-sequence models for speech recognition",
    "citation_count": 272,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zeyer17_interspeech.html": {
    "title": "CTC in the Context of Generalized Full-Sum HMM Training",
    "volume": "main",
    "abstract": "We formulate a generalized hybrid HMM-NN training procedure using the full-sum over the hidden state-sequence and identify CTC as a special case of it. We present an analysis of the alignment behavior of such a training procedure and explain the strong localization of label output behavior of full-sum training (also referred to as peaky or spiky behavior). We show how to avoid that behavior by using a state prior. We discuss the temporal decoupling between output label position/time-frame, and the corresponding evidence in the input observations when this is trained with BLSTM models. We also show a way how to overcome this by jointly training a FFNN. We implemented the Baum-Welch alignment algorithm in CUDA to be able to do fast soft realignments on GPU. We have published this code along with some of our experiments as part of RETURNN, RWTH's extensible training framework for universal recurrent neural networks. We finish with experimental validation of our study on WSJ and Switchboard",
    "checked": true,
    "id": "2ec4f0035b840b4d90a114b298aaa1f4551a93d1",
    "semantic_title": "ctc in the context of generalized full-sum hmm training",
    "citation_count": 35,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hori17_interspeech.html": {
    "title": "Advances in Joint CTC-Attention Based End-to-End Speech Recognition with a Deep CNN Encoder and RNN-LM",
    "volume": "main",
    "abstract": "We present a state-of-the-art end-to-end Automatic Speech Recognition (ASR) model. We learn to listen and write characters with a joint Connectionist Temporal Classification (CTC) and attention-based encoder-decoder network. The encoder is a deep Convolutional Neural Network (CNN) based on the VGG network. The CTC network sits on top of the encoder and is jointly trained with the attention-based decoder. During the beam search process, we combine the CTC predictions, the attention-based decoder predictions and a separately trained LSTM language model. We achieve a 5–10% error reduction compared to prior systems on spontaneous Japanese and Chinese speech, and our end-to-end model beats out traditional hybrid ASR systems",
    "checked": true,
    "id": "c102ac8c779ee0a53dc8e4ee20b4088ac2c7e186",
    "semantic_title": "advances in joint ctc-attention based end-to-end speech recognition with a deep cnn encoder and rnn-lm",
    "citation_count": 274,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lu17_interspeech.html": {
    "title": "Multitask Learning with CTC and Segmental CRF for Speech Recognition",
    "volume": "main",
    "abstract": "Segmental conditional random fields (SCRFs) and connectionist temporal classification (CTC) are two sequence labeling methods used for end-to-end training of speech recognition models. Both models define a transcription probability by marginalizing decisions about latent segmentation alternatives to derive a sequence probability: the former uses a globally normalized joint model of segment labels and durations, and the latter classifies each frame as either an output symbol or a \"continuation\" of the previous label. In this paper, we train a recognition model by optimizing an interpolation between the SCRF and CTC losses, where the same recurrent neural network (RNN) encoder is used for feature extraction for both outputs. We find that this multitask objective improves recognition accuracy when decoding with either the SCRF or CTC models. Additionally, we show that CTC can also be used to pretrain the RNN encoder, which improves the convergence rate when learning the joint model",
    "checked": true,
    "id": "3d2988f54c7600adb5489de2dfb2c0f8f1316b88",
    "semantic_title": "multitask learning with ctc and segmental crf for speech recognition",
    "citation_count": 24,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/audhkhasi17_interspeech.html": {
    "title": "Direct Acoustics-to-Word Models for English Conversational Speech Recognition",
    "volume": "main",
    "abstract": "Recent work on end-to-end automatic speech recognition (ASR) has shown that the connectionist temporal classification (CTC) loss can be used to convert acoustics to phone or character sequences. Such systems are used with a dictionary and separately-trained Language Model (LM) to produce word sequences. However, they are not truly end-to-end in the sense of mapping acoustics directly to words without an intermediate phone representation. In this paper, we present the first results employing direct acoustics-to-word CTC models on two well-known public benchmark tasks: Switchboard and CallHome. These models do not require an LM or even a decoder at run-time and hence recognize speech with minimal complexity. However, due to the large number of word output units, CTC word models require orders of magnitude more data to train reliably compared to traditional systems. We present some techniques to mitigate this issue. Our CTC word model achieves a word error rate of 13.0%/18.8% on the Hub5-2000 Switchboard/CallHome test sets without any LM or decoder compared with 9.6%/16.0% for phone-based CTC with a 4-gram LM. We also present rescoring results on CTC word model lattices to quantify the performance benefits of a LM, and contrast the performance of word and phone CTC models",
    "checked": true,
    "id": "cf0e9724e51b420bc51a1d0625410c86d36641db",
    "semantic_title": "building competitive direct acoustics-to-word models for english conversational speech recognition",
    "citation_count": 174,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/li17e_interspeech.html": {
    "title": "Reducing the Computational Complexity of Two-Dimensional LSTMs",
    "volume": "main",
    "abstract": "Long Short-Term Memory Recurrent Neural Networks (LSTMs) are good at modeling temporal variations in speech recognition tasks, and have become an integral component of many state-of-the-art ASR systems. More recently, LSTMs have been extended to model variations in the speech signal in two dimensions, namely time and frequency [1, 2]. However, one of the problems with two-dimensional LSTMs, such as Grid-LSTMs, is that the processing in both time and frequency occurs sequentially, thus increasing computational complexity. In this work, we look at minimizing the dependence of the Grid-LSTM with respect to previous time and frequency points in the sequence, thus reducing computational complexity. Specifically, we compare reducing computation using a bidirectional Grid-LSTM (biGrid-LSTM) with non-overlapping frequency sub-band processing, a PyraMiD-LSTM [3] and a frequency-block Grid-LSTM (fbGrid-LSTM) for parallel time-frequency processing. We find that the fbGrid-LSTM can reduce computation costs by a factor of four with no loss in accuracy, on a 12,500 hour Voice Search task",
    "checked": true,
    "id": "c8d1eeb3a278e09804476049fd86d45d3f3c82af",
    "semantic_title": "reducing the computational complexity of two-dimensional lstms",
    "citation_count": 12,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lucero17_interspeech.html": {
    "title": "Functional Principal Component Analysis of Vocal Tract Area Functions",
    "volume": "main",
    "abstract": "This paper shows the application of a functional version of principal component analysis to build a parametrization of vocal tract area functions for vowel production. Sets of measured area values for ten vowels are expressed as smooth functional data and next decomposed into a mean area function and a basis of orthogonal eigenfunctions. Interpretations of the first four eigenfunctions are provided in terms of tongue movements and vocal tract length variations. Also, an alternative set of eigenfunctions with closer association to specific regions of the vocal tract is obtained via a varimax rotation. The general intention of the paper is to show the benefits of a functional approach to analyze vocal tract shapes and motivate further applications",
    "checked": true,
    "id": "de4f8358b43236c755fa5cc2e3ecebd262da1348",
    "semantic_title": "functional principal component analysis of vocal tract area functions",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sivaraman17_interspeech.html": {
    "title": "Analysis of Acoustic-to-Articulatory Speech Inversion Across Different Accents and Languages",
    "volume": "main",
    "abstract": "The focus of this paper is estimating articulatory movements of the tongue and lips from acoustic speech data. While there are several potential applications of such a method in speech therapy and pronunciation training, performance of such acoustic-to-articulatory inversion systems is not very high due to limited availability of simultaneous acoustic and articulatory data, substantial speaker variability, and variable methods of data collection. This paper therefore evaluates the impact of speaker, language and accent variability on the performance of an acoustic-to-articulatory speech inversion system. The articulatory dataset used in this study consists of 21 Dutch speakers reading Dutch and English words and sentences, and 22 UK English speakers reading English words and sentences. We trained several acoustic-to-articulatory speech inversion systems both based on deep and shallow neural network architectures in order to estimate electromagnetic articulography (EMA) sensor positions, as well as vocal tract variables (TVs). Our results show that with appropriate feature and target normalization, a speaker-independent speech inversion system trained on data from one language is able to estimate sensor positions (or TVs) for the same language correlating at about r = 0.53 with the actual sensor positions (or TVs). Cross-language results show a reduced performance of r = 0.47",
    "checked": true,
    "id": "f8db55708164b21f1a79d84a682996a4194ee56e",
    "semantic_title": "analysis of acoustic-to-articulatory speech inversion across different accents and languages",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/arai17_interspeech.html": {
    "title": "Integrated Mechanical Model of [r]-[l] and [b]-[m]-[w] Producing Consonant Cluster [br]",
    "volume": "main",
    "abstract": "We have developed two types of mechanical models of the human vocal tract. The first model was designed for the retroflex approximant [r] and the alveolar lateral approximant [l]. It consisted of the main vocal tract and a flapping tongue, where the front half of the tongue can be rotated against the palate. When the tongue is short and rotated approximately 90 degrees, the retroflex approximant [r] is produced. The second model was designed for [b], [m], and [w]. Besides the main vocal tract, this model contains a movable lower lip for lip closure and a nasal cavity with a controllable velopharyngeal port. In the present study, we joined these two mechanical models to form a new model containing the main vocal tract, the flapping tongue, the movable lower lip, and the nasal cavity with the controllable velopharyngeal port. This integrated model now makes it possible to produce consonant sequences. Therefore, we examined the sequence [br], in particular, adjusting the timing of the lip and lingual gestures to produce the best sound. Because the gestures are visually observable from the outside of this model, the timing of the gestures were examined with the use of a high-speed video camera",
    "checked": true,
    "id": "da8165d055aff164679683c2dee089e9a2d8ee8f",
    "semantic_title": "integrated mechanical model of [r]-[l] and [b]-[m]-[w] producing consonant cluster [br]",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/badino17_interspeech.html": {
    "title": "A Speaker Adaptive DNN Training Approach for Speaker-Independent Acoustic Inversion",
    "volume": "main",
    "abstract": "We address the speaker-independent acoustic inversion (AI) problem, also referred to as acoustic-to-articulatory mapping. The scarce availability of multi-speaker articulatory data makes it difficult to learn a mapping which generalizes from a limited number of training speakers and reliably reconstructs the articulatory movements of unseen speakers. In this paper, we propose a Multi-task Learning (MTL)-based approach that explicitly separates the modeling of each training speaker AI peculiarities from the modeling of AI characteristics that are shared by all speakers. Our approach stems from the well known Regularized MTL approach and extends it to feed-forward deep neural networks (DNNs). Given multiple training speakers, we learn for each an acoustic-to-articulatory mapping represented by a DNN. Then, through an iterative procedure, we search for a canonical speaker-independent DNN that is \"similar\" to all speaker-dependent DNNs. The degree of similarity is controlled by a regularization parameter. We report experiments on the University of Wisconsin X-ray Microbeam Database under different training/testing experimental settings. The results obtained indicate that our MTL-trained canonical DNN largely outperforms a standardly trained (i.e., single task learning-based) speaker independent DNN",
    "checked": true,
    "id": "47449a33b7b0eac650349b905b5a107780d7ba46",
    "semantic_title": "a speaker adaptive dnn training approach for speaker-independent acoustic inversion",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/uchida17_interspeech.html": {
    "title": "Acoustic-to-Articulatory Mapping Based on Mixture of Probabilistic Canonical Correlation Analysis",
    "volume": "main",
    "abstract": "In this paper, we propose a novel acoustic-to-articulatory mapping model based on mixture of probabilistic canonical correlation analysis (mPCCA). In PCCA, it is assumed that two different kinds of data are observed as results from different linear transforms of a common latent variable. It is expected that this variable represents a common factor which is inherent in the different domains, such as acoustic and articulatory feature spaces. mPCCA is an expansion of PCCA and it can model a much more complex structure. In mPCCA, covariance matrices of a joint probabilistic distribution of acoustic-articulatory data are structuralized reasonably by using transformation coefficients of the linear transforms. Even if the number of components in mPCCA increases, the structuralized covariance matrices can be expected to avoid over-fitting. Training and mapping processes of the mPCCA-based mapping model are reasonably derived by using the EM algorithm. Experiments using MOCHA-TIMIT show that the proposed mapping method has achieved better mapping performance than the conventional GMM-based mapping",
    "checked": true,
    "id": "fdc90d55b26324a0c9cc02c0e81e68be87e15ad9",
    "semantic_title": "acoustic-to-articulatory mapping based on mixture of probabilistic canonical correlation analysis",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sorensen17b_interspeech.html": {
    "title": "Test-Retest Repeatability of Articulatory Strategies Using Real-Time Magnetic Resonance Imaging",
    "volume": "main",
    "abstract": "Real-time magnetic resonance imaging (rtMRI) provides information about the dynamic shaping of the vocal tract during speech production. This paper introduces and evaluates a method for quantifying articulatory strategies using rtMRI. The method decomposes the formation and release of a constriction in the vocal tract into the contributions of individual articulators such as the jaw, tongue, lips, and velum. The method uses an anatomically guided factor analysis and dynamical principles from the framework of Task Dynamics. We evaluated the method within a test-retest repeatability framework. We imaged healthy volunteers (n = 8, 4 females, 4 males) in two scans on the same day and quantified inter-study agreement with the intraclass correlation coefficient and mean within-subject standard deviation. The evaluation established a limit on effect size and intra-group differences in articulatory strategy which can be studied using the method",
    "checked": true,
    "id": "af69e971da2fba4f85f9ae2c8ab2d5d48991b076",
    "semantic_title": "test-retest repeatability of articulatory strategies using real-time magnetic resonance imaging",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/snyder17_interspeech.html": {
    "title": "Deep Neural Network Embeddings for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "This paper investigates replacing i-vectors for text-independent speaker verification with embeddings extracted from a feed-forward deep neural network. Long-term speaker characteristics are captured in the network by a temporal pooling layer that aggregates over the input speech. This enables the network to be trained to discriminate between speakers from variable-length speech segments. After training, utterances are mapped directly to fixed-dimensional speaker embeddings and pairs of embeddings are scored using a PLDA-based backend. We compare performance with a traditional i-vector baseline on NIST SRE 2010 and 2016. We find that the embeddings outperform i-vectors for short speech segments and are competitive on long duration test conditions. Moreover, the two representations are complementary, and their fusion improves on the baseline at all operating points. Similar systems have recently shown promising results when trained on very large proprietary datasets, but to the best of our knowledge, these are the best results reported for speaker-discriminative neural networks when trained and tested on publicly available corpora",
    "checked": true,
    "id": "369728d7576683a25de8890e4bc02fae6132fccb",
    "semantic_title": "deep neural network embeddings for text-independent speaker verification",
    "citation_count": 687,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/villalba17_interspeech.html": {
    "title": "Tied Variational Autoencoder Backends for i-Vector Speaker Recognition",
    "volume": "main",
    "abstract": "Probabilistic linear discriminant analysis (PLDA) is the de facto standard for backends in i-vector speaker recognition. If we try to extend the PLDA paradigm using non-linear models, e.g., deep neural networks, the posterior distributions of the latent variables and the marginal likelihood become intractable. In this paper, we propose to approach this problem using stochastic gradient variational Bayes. We generalize the PLDA model to let i-vectors depend non-linearly on the latent factors. We approximate the evidence lower bound (ELBO) by Monte Carlo sampling using the reparametrization trick. This enables us to optimize of the ELBO using backpropagation to jointly estimate the parameters that define the model and the approximate posteriors of the latent factors. We also present a reformulation of the likelihood ratio, which we call Q-scoring. Q-scoring makes possible to efficiently score the speaker verification trials for this model. Experimental results on NIST SRE10 suggest that more data might be required to exploit the potential of this method",
    "checked": true,
    "id": "4488cba0d06ae06b4b7b99cbb3639731c9eefe32",
    "semantic_title": "tied variational autoencoder backends for i-vector speaker recognition",
    "citation_count": 29,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ranjan17_interspeech.html": {
    "title": "Improved Gender Independent Speaker Recognition Using Convolutional Neural Network Based Bottleneck Features",
    "volume": "main",
    "abstract": "This paper proposes a novel framework to improve performance of gender independent i-Vector PLDA based speaker recognition using convolutional neural network (CNN). Convolutional layers of a CNN offer robustness to variations in input features including those due to gender. A CNN is trained for ASR with a linear bottleneck layer. Bottleneck features extracted using the CNN are then used to train a gender-independent UBM to obtain frame posteriors for training an i-Vector extractor matrix. To preserve speaker specific information, a hybrid approach to training the i-Vector extractor matrix using MFCC features with corresponding frame posteriors derived from bottleneck features is proposed. On the NIST SRE10 C5 condition pooled trials, our approach reduces the EER and minDCF 2010 by +14.62% and +14.42% respectively compared to a standard mfcc based gender-independent speaker recognition system",
    "checked": true,
    "id": "2a1882ea2c3c63bc0cfd7ec98ccabdba9beec452",
    "semantic_title": "improved gender independent speaker recognition using convolutional neural network based bottleneck features",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/shon17_interspeech.html": {
    "title": "Autoencoder Based Domain Adaptation for Speaker Recognition Under Insufficient Channel Information",
    "volume": "main",
    "abstract": "In real-life conditions, mismatch between development and test domain degrades speaker recognition performance. To solve the issue, many researchers explored domain adaptation approaches using matched in-domain dataset. However, adaptation would be not effective if the dataset is insufficient to estimate channel variability of the domain. In this paper, we explore the problem of performance degradation under such a situation of insufficient channel information. In order to exploit limited in-domain dataset effectively, we propose an unsupervised domain adaptation approach using Autoencoder based Domain Adaptation (AEDA). The proposed approach combines an autoencoder with a denoising autoencoder to adapt resource-rich development dataset to test domain. The proposed technique is evaluated on the Domain Adaptation Challenge 13 experimental protocols that is widely used in speaker recognition for domain mismatched condition. The results show significant improvements over baselines and results from other prior studies",
    "checked": true,
    "id": "6634ed2960aeb5f5c2495262d4dd1136b3ec35bf",
    "semantic_title": "autoencoder based domain adaptation for speaker recognition under insufficient channel information",
    "citation_count": 30,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/khosravani17_interspeech.html": {
    "title": "Nonparametrically Trained Probabilistic Linear Discriminant Analysis for i-Vector Speaker Verification",
    "volume": "main",
    "abstract": "In this paper we propose to estimate the parameters of the probabilistic linear discriminant analysis (PLDA) in text-independent i-vector speaker verification framework using a nonparametric form rather than maximum likelihood estimation (MLE) obtained by an EM algorithm. In this approach the between-speaker covariance matrix that represents global information about the speaker variability is replaced with a local estimation computed on a nearest neighbor basis for each target speaker. The nonparametric between- and within-speaker scatter matrices can better exploit the discriminant information in training data and is more adapted to sample distribution especially when it does not satisfy Gaussian assumption as in i-vectors without length-normalization. We evaluated this approach on the recent NIST 2016 speaker recognition evaluation (SRE) as well as NIST 2010 core condition and found significant performance improvement compared with a generatively trained PLDA model",
    "checked": true,
    "id": "8e909cec10684ebcf0c61a2b249ed321cd19f5c2",
    "semantic_title": "nonparametrically trained probabilistic linear discriminant analysis for i-vector speaker verification",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jorrin17_interspeech.html": {
    "title": "DNN Bottleneck Features for Speaker Clustering",
    "volume": "main",
    "abstract": "In this work, we explore deep neural network bottleneck features (BNF) in the context of speaker clustering. A straightforward manner to deal with speaker clustering is to reuse the bottleneck features extracted from speaker recognition. However, the selection of a bottleneck architecture or nonlinearity impacts the performance of both systems. In this work, we analyze the bottleneck features obtained for speaker recognition and test them in a speaker clustering scenario. We observe that there are deep neural network topologies that work better for both cases, even when their classification criteria (senone classification) is loosely met. We present results that outperform a traditional MFCC system by 21% for speaker recognition and between 20% and 37% in clustering using the same topology",
    "checked": true,
    "id": "1e1002c9d53f8566b50e27538ee764b18cf8f86a",
    "semantic_title": "dnn bottleneck features for speaker clustering",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/aare17_interspeech.html": {
    "title": "Creak as a Feature of Lexical Stress in Estonian",
    "volume": "main",
    "abstract": "In addition to typological, turn-taking or sociolinguistic factors, presence of creaky voice in spontaneous interaction is also influenced by the syntactic and phonological properties of speech. For example, creaky voice is reportedly more frequent in function words than content words, has been observed to accompany unstressed syllables and ends of phrases, and is associated with relaxation and reduced speech In Estonian, creaky voice is frequently used by all speakers. In this paper, we observe the use of creaky voice in spontaneous Estonian in connection to syllabic properties of words, lexical stress, word class, lengthening, and timing in phrases The results indicate that creak occurs less in syllables with primary stress than in unstressed syllables. However, syllables with secondary stress are most frequently creaky. In content words, the primary stressed syllables creak less frequently and unstressed syllables more frequently compared to function words. The stress-related pattern is similar in both function and content words, but more contrastive in content words. The probability of creakiness increases considerably with non-final lengthening within words, and for all syllables towards the end of the intonational phrase",
    "checked": true,
    "id": "4fd64f72132c91c237982ce36ed5e9414259bc1d",
    "semantic_title": "creak as a feature of lexical stress in estonian",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/yanushevskaya17_interspeech.html": {
    "title": "Cross-Speaker Variation in Voice Source Correlates of Focus and Deaccentuation",
    "volume": "main",
    "abstract": "This paper describes cross-speaker variation in the voice source correlates of focal accentuation and deaccentuation. A set of utterances with varied narrow focus placement as well as broad focus and deaccented renditions were produced by six speakers of English. These were manually inverse filtered and parameterized on a pulse-by-pulse basis using the LF source model. Z-normalized F0, EE, OQ and RD parameters (selected through correlation and factor analysis) were used to generate speaker specific baseline voice profiles and to explore cross-speaker variation in focal and non-focal (post- and prefocal) syllables. As expected, source parameter values were found to differ in the focal and postfocal portions of the utterance. For four of the six speakers the measures revealed a trend of tenser phonation on the focal syllable (an increase in EE and F0 and typically, a decrease in OQ and RD) as well as increased laxness in the postfocal part of the utterance. For two of the speakers, however, the measurements showed a different trend. These speakers had very high F0 and often high EE on the focal accent. In these cases, RD and OQ values tended to be raised rather than lowered. The possible reasons for these differences are discussed",
    "checked": true,
    "id": "6075220ef193248202d02a8802f973e7cc4f1e6d",
    "semantic_title": "cross-speaker variation in voice source correlates of focus and deaccentuation",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kalita17_interspeech.html": {
    "title": "Acoustic Characterization of Word-Final Glottal Stops in Mizo and Assam Sora",
    "volume": "main",
    "abstract": "The present work proposed an approach to characterize the word-final glottal stops in Mizo and Assam Sora language. Generally, glottal stops have more strong glottal and ventricular constriction at the coda position than at the onset. However, the primary source characteristics of glottal stops are irregular glottal cycles, abrupt glottal closing, and reduced open cycle. These changes will not only affect the vocal quality parameters but may also significantly affect the vocal tract characteristics due to changes in the subglottal coupling behavior. This motivates to analyze the dynamic vocal tract characteristics in terms of source behavior, apart from the excitation source features computed from the Linear Prediction (LP) residual for the acoustic characterization of the word-final glottal stops. The dominant resonance frequency (DRF) of the vocal tract using Hilbert Envelope of Numerator Group Delay (HNGD) are extracted at every sample instants as a cue to study this deviation. The gradual increase in the DRF and significantly lower duration for which subglottal coupling is occurring is observed for the glottal stop region for both the languages",
    "checked": true,
    "id": "a2007f7af4ee7b7f0320ccade66c5a2b40e553bf",
    "semantic_title": "acoustic characterization of word-final glottal stops in mizo and assam sora",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mokhtari17_interspeech.html": {
    "title": "Iterative Optimal Preemphasis for Improved Glottal-Flow Estimation by Iterative Adaptive Inverse Filtering",
    "volume": "main",
    "abstract": "Iterative adaptive inverse filtering (IAIF) [1] remains among the state-of-the-art algorithms for estimating glottal flow from the recorded speech signal. Here, we re-examine IAIF in light of its foundational, classical model of voiced (non-nasalized) speech, wherein the overall spectral tilt is caused only by lip-radiation and glottal effects, while the vocal-tract transfer function contains formant peaks but is otherwise not tilted. In contrast, IAIF initially models and cancels the formants after only a first-order preemphasis of the speech signal, which is generally not enough to completely remove spectral tilt Iterative optimal preemphasis (IOP) is therefore proposed to replace IAIF's initial step. IOP is a rapidly converging algorithm that models a signal (then inverse-filters it) with one real pole (zero) at a time, until spectral tilt is flattened. IOP-IAIF is evaluated on sustained /a/ in a range of voice qualities from weak-breathy to shouted-tense. Compared with standard IAIF, IOP-IAIF yields: (i) an acceptable glottal flow even for a weak breathy voice that the standard algorithm failed to handle; (ii) generally smoother glottal flows that nevertheless retain pulse shape and closed phase; and (iii) enhanced separation of voice qualities in both normalized amplitude quotient (NAQ) and glottal harmonic spectra",
    "checked": true,
    "id": "96acd5d8c7ccaaba570d8badacd2e357b61be71d",
    "semantic_title": "iterative optimal preemphasis for improved glottal-flow estimation by iterative adaptive inverse filtering",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sheena17_interspeech.html": {
    "title": "Automatic Measurement of Pre-Aspiration",
    "volume": "main",
    "abstract": "Pre-aspiration is defined as the period of glottal friction occurring in sequences of vocalic/consonantal sonorants and phonetically voiceless obstruents. We propose two machine learning methods for automatic measurement of pre-aspiration duration: a feedforward neural network, which works at the frame level; and a structured prediction model, which relies on manually designed feature functions, and works at the segment level. The input for both algorithms is a speech signal of an arbitrary length containing a single obstruent, and the output is a pair of times which constitutes the pre-aspiration boundaries. We train both models on a set of manually annotated examples. Results suggest that the structured model is superior to the frame-based model as it yields higher accuracy in predicting the boundaries and generalizes to new speakers and new languages. Finally, we demonstrate the applicability of our structured prediction algorithm by replicating linguistic analysis of pre-aspiration in Aberystwyth English with high correlation",
    "checked": true,
    "id": "c870b6e0d968fbf63f33ff366bf17f40f0945e7d",
    "semantic_title": "automatic measurement of pre-aspiration",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/nara17_interspeech.html": {
    "title": "Acoustic and Electroglottographic Study of Breathy and Modal Vowels as Produced by Heritage and Native Gujarati Speakers",
    "volume": "main",
    "abstract": "While all languages of the world use modal phonation, many also rely on other phonation types such as breathy or creaky voice. For example, Gujarati, an Indo-Aryan language, makes a distinction between breathy and modal phonation among consonants and vowels: /b aɾ/ ‘burden', /baɾ/ ‘twelve', and /ba̤ɾ/ ‘outside' [1, 2]. This study, which is a replication and an extension of Khan [3], aims to determine the acoustic and articulatory parameters that distinguish breathy and modal vowels. The participants of this study are heritage and native Gujarati speakers The materials consisted of 40 target words with the modal and breathy pairs of the three vowel qualities: /a/ vs /a̤/, /e/ vs /e̤/, and /o/ vs /o̤/. The participants uttered the words in the context of a sentence. Acoustic measurements such as H1-H2, H1-A1, harmonic-to-noise ratio and articulatory measurements such as contact quotient were calculated throughout the vowel duration The results of the Smoothing Spline ANOVA analyses indicated that measures such as H1-A1, harmonic to noise ratio, and contact quotient distinguished modal and breathy vowels for native speakers. Heritage speakers also had a contrast between breathy and modal vowels, however the contrast is not as robust as that of native speakers",
    "checked": true,
    "id": "02d679ffc224da71ea37122645440a799a63f628",
    "semantic_title": "acoustic and electroglottographic study of breathy and modal vowels as produced by heritage and native gujarati speakers",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wang17e_interspeech.html": {
    "title": "An RNN-Based Quantized F0 Model with Multi-Tier Feedback Links for Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "A recurrent-neural-network-based F0 model for text-to-speech (TTS) synthesis that generates F0 contours given textual features is proposed. In contrast to related F0 models, the proposed one is designed to learn the temporal correlation of F0 contours at multiple levels. The frame-level correlation is covered by feeding back the F0 output of the previous frame as the additional input of the current frame; meanwhile, the correlation over long-time spans is similarly modeled but by using F0 features aggregated over the phoneme and syllable. Another difference is that the output of the proposed model is not the interpolated continuous-valued F0 contour but rather a sequence of discrete symbols, including quantized F0 levels and a symbol for the unvoiced condition. By using the discrete F0 symbols, the proposed model avoids the influence of artificially interpolated F0 curves. Experiments demonstrated that the proposed F0 model, which was trained using a dropout strategy, generated smooth F0 contours with relatively better perceived quality than those from baseline RNN models",
    "checked": true,
    "id": "9f1d5d90262ec9cebba2055f234e3a197d7f6f78",
    "semantic_title": "an rnn-based quantized f0 model with multi-tier feedback links for text-to-speech synthesis",
    "citation_count": 35,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/klimkov17_interspeech.html": {
    "title": "Phrase Break Prediction for Long-Form Reading TTS: Exploiting Text Structure Information",
    "volume": "main",
    "abstract": "Phrasing structure is one of the most important factors in increasing the naturalness of text-to-speech (TTS) systems, in particular for long-form reading. Most existing TTS systems are optimized for isolated short sentences, and completely discard the larger context or structure of the text This paper presents how we have built phrasing models based on data extracted from audiobooks. We investigate how various types of textual features can improve phrase break prediction: part-of-speech (POS), guess POS (GPOS), dependency tree features and word embeddings. These features are fed into a bidirectional LSTM or a CART baseline. The resulting systems are compared using both objective and subjective evaluations. Using BiLSTM and word embeddings proves to be beneficial",
    "checked": true,
    "id": "3a51eb66b9785af6f53cfd19188eeb5ecf9544fd",
    "semantic_title": "phrase break prediction for long-form reading tts: exploiting text structure information",
    "citation_count": 24,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tanaka17_interspeech.html": {
    "title": "Physically Constrained Statistical F0 Prediction for Electrolaryngeal Speech Enhancement",
    "volume": "main",
    "abstract": "Electrolaryngeal (EL) speech produced by a laryngectomee using an electrolarynx to mechanically generate artificial excitation sounds severely suffers from unnatural fundamental frequency (F ) patterns caused by monotonic excitation sounds. To address this issue, we have previously proposed EL speech enhancement systems using statistical F pattern prediction methods based on a Gaussian Mixture Model (GMM), making it possible to predict the underlying F pattern of EL speech from its spectral feature sequence. Our previous work revealed that the naturalness of the predicted F pattern can be improved by incorporating a physically based generative model of F patterns into the GMM-based statistical F prediction system within a Product-of-Expert framework. However, one drawback of this method is that it requires an iterative procedure to obtain a predicted F pattern, making it difficult to realize a real-time system. In this paper, we propose yet another approach to physically based statistical F pattern prediction by using a HMM-GMM framework. This approach is noteworthy in that it allows to generate an F pattern that is both statistically likely and physically natural without iterative procedures. Experimental results demonstrated that the proposed method was capable of generating F patterns more similar to those in normal speech than the conventional GMM-based method",
    "checked": true,
    "id": "5dc88f622202ee2c7732e672bb0f546ade2c8f2c",
    "semantic_title": "physically constrained statistical f0 prediction for electrolaryngeal speech enhancement",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hojo17_interspeech.html": {
    "title": "DNN-SPACE: DNN-HMM-Based Generative Model of Voice F0 Contours for Statistical Phrase/Accent Command Estimation",
    "volume": "main",
    "abstract": "This paper proposes a method to extract prosodic features from a speech signal by leveraging auxiliary linguistic information. A prosodic feature extractor called the statistical phrase/accent command estimation (SPACE) has recently been proposed. This extractor is based on a statistical model formulated as a stochastic counterpart of the Fujisaki model, a well-founded mathematical model representing the control mechanism of vocal fold vibration. The key idea of this approach is that a phrase/accent command pair sequence is modeled as an output sequence of a path-restricted hidden Markov model (HMM) so that estimating the state transition amounts to estimating the phrase/accent commands. Since the phrase and accent commands are related to linguistic information, we may expect to improve the command estimation accuracy by using them as auxiliary information for the inference. To model the relationship between the phrase/accent commands and linguistic information, we construct a deep neural network (DNN) that maps the linguistic feature vectors to the state posterior probabilities of the HMM. Thus, given a pitch contour and linguistic information, we can estimate phrase/accent commands via state decoding. We call this method \"DNN-SPACE.\" Experimental results revealed that using linguistic information was effective in improving the command estimation accuracy",
    "checked": true,
    "id": "6b3f95382dd3acfd18461018700ea4cdb9512dea",
    "semantic_title": "dnn-space: dnn-hmm-based generative model of voice f0 contours for statistical phrase/accent command estimation",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/malisz17_interspeech.html": {
    "title": "Controlling Prominence Realisation in Parametric DNN-Based Speech Synthesis",
    "volume": "main",
    "abstract": "This work aims to improve text-to-speech synthesis for Wikipedia by advancing and implementing models of prosodic prominence. We propose a new system architecture with explicit prominence modeling and test the first component of the architecture. We automatically extract a phonetic feature related to prominence from the speech signal in the ARCTIC corpus. We then modify the label files and train an experimental TTS system based on the feature using Merlin, a statistical-parametric DNN-based engine. Test sentences with contrastive prominence on the word-level are synthesised and separate listening tests a) evaluating the level of prominence control in generated speech, and b) naturalness, are conducted. Our results show that the prominence feature-enhanced system successfully places prominence on the appropriate words and increases perceived naturalness relative to the baseline",
    "checked": true,
    "id": "783f6a1d6eb0e48a7abaaf6c64f7c59662e1acd1",
    "semantic_title": "controlling prominence realisation in parametric dnn-based speech synthesis",
    "citation_count": 11,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/betz17_interspeech.html": {
    "title": "Increasing Recall of Lengthening Detection via Semi-Automatic Classification",
    "volume": "main",
    "abstract": "Lengthening is the ideal hesitation strategy for synthetic speech and dialogue systems: it is unobtrusive and hard to notice, because it occurs frequently in everyday speech before phrase boundaries, in accentuation, and in hesitation. Despite its elusiveness, it allows valuable extra time for computing or information highlighting in incremental spoken dialogue systems. The elusiveness of the matter, however, poses a challenge for extracting lengthening instances from corpus data: we suspect a recall problem, as human annotators might not be able to consistently label lengthening instances. We address this issue by filtering corpus data for instances of lengthening, using a simple classification method, based on a threshold for normalized phone duration. The output is then manually labeled for disfluency. This is compared to an existing, fully manual disfluency annotation, showing that recall is significantly higher with semi-automatic pre-classification. This shows that it is inevitable to use semi-automatic pre-selection to gather enough candidate data points for manual annotation and subsequent lengthening analyses. Also, it is desirable to further increase the performance of the automatic classification. We evaluate in detail human versus semi-automatic annotation and train another classifier on the resulting dataset to check the integrity of the disfluent – non-disfluent distinction",
    "checked": true,
    "id": "d393408fed9d2e43baa4bb1bd2a0331c6144e4b0",
    "semantic_title": "increasing recall of lengthening detection via semi-automatic classification",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/satt17_interspeech.html": {
    "title": "Efficient Emotion Recognition from Speech Using Deep Learning on Spectrograms",
    "volume": "main",
    "abstract": "We present a new implementation of emotion recognition from the para-lingual information in the speech, based on a deep neural network, applied directly to spectrograms. This new method achieves higher recognition accuracy compared to previously published results, while also limiting the latency. It processes the speech input in smaller segments — up to 3 seconds, and splits a longer input into non-overlapping parts to reduce the prediction latency The deep network comprises common neural network tools — convolutional and recurrent networks — which are shown to effectively learn the information that represents emotions directly from spectrograms. Convolution-only lower-complexity deep network achieves a prediction accuracy of 66% over four emotions (tested on IEMOCAP — a common evaluation corpus), while a combined convolution-LSTM higher-complexity model achieves 68% The use of spectrograms in the role of speech-representing features enables effective handling of background non-speech signals such as music (excl. singing) and crowd noise, even at noise levels comparable with the speech signal levels. Using harmonic modeling to remove non-speech components from the spectrogram, we demonstrate significant improvement of the emotion recognition accuracy in the presence of unknown background non-speech signals",
    "checked": true,
    "id": "de47fc09bc8dcd032c8b3450a0b2a816c376e07e",
    "semantic_title": "efficient emotion recognition from speech using deep learning on spectrograms",
    "citation_count": 281,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhang17b_interspeech.html": {
    "title": "Interaction and Transition Model for Speech Emotion Recognition in Dialogue",
    "volume": "main",
    "abstract": "In this paper we propose a novel emotion recognition method modeling interaction and transition in dialogue. Conventional emotion recognition utilizes intra-features such as MFCCs or F0s within individual utterance. However, human perceive emotions not only through individual utterances but also by contextual information. The proposed method takes in account the contextual effect of utterance in dialogue, which the conventional method fails to. Proposed method introduces Emotion Interaction and Transition (EIT) models which is constructed by end-to-end LSTMs. The inputs of EIT model are the previous emotions of both target and opponent speaker, estimated by state-of-the-art utterance emotion recognition model. The experimental results show that the proposed method improves overall accuracy and average precision by a relative error reduction of 18.8% and 22.6% respectively",
    "checked": true,
    "id": "46e8d8da8fa7a90b35f354efd429c0aead610e79",
    "semantic_title": "interaction and transition model for speech emotion recognition in dialogue",
    "citation_count": 11,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gideon17_interspeech.html": {
    "title": "Progressive Neural Networks for Transfer Learning in Emotion Recognition",
    "volume": "main",
    "abstract": "Many paralinguistic tasks are closely related and thus representations learned in one domain can be leveraged for another. In this paper, we investigate how knowledge can be transferred between three paralinguistic tasks: speaker, emotion, and gender recognition. Further, we extend this problem to cross-dataset tasks, asking how knowledge captured in one emotion dataset can be transferred to another. We focus on progressive neural networks and compare these networks to the conventional deep learning method of pre-training and fine-tuning. Progressive neural networks provide a way to transfer knowledge and avoid the forgetting effect present when pre-training neural networks on different tasks. Our experiments demonstrate that: (1) emotion recognition can benefit from using representations originally learned for different paralinguistic tasks and (2) transfer learning can effectively leverage additional datasets to improve the performance of emotion recognition systems",
    "checked": true,
    "id": "9c753bb807a3abb3563e68c898bc0f2cd473d84c",
    "semantic_title": "progressive neural networks for transfer learning in emotion recognition",
    "citation_count": 111,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/parthasarathy17_interspeech.html": {
    "title": "Jointly Predicting Arousal, Valence and Dominance with Multi-Task Learning",
    "volume": "main",
    "abstract": "An appealing representation of emotions is the use of emotional attributes such as arousal (passive versus active), valence (negative versus positive) and dominance (weak versus strong). While previous studies have considered these dimensions as orthogonal descriptors to represent emotions, there are strong theoretical and practical evidences showing the interrelation between these emotional attributes. This observation suggests that predicting emotional attributes with a unified framework should outperform machine learning algorithms that separately predict each attribute. This study presents methods to jointly learn emotional attributes by exploiting their interdependencies. The framework relies on multi-task learning (MTL) implemented with deep neural networks (DNN) with shared hidden layers. The framework provides a principled approach to learn shared feature representations that maximize the performance of regression models. The results of within-corpus and cross-corpora evaluation show the benefits of MTL over single task learning (STL). MTL achieves gains on concordance correlation coefficient (CCC) as high as 4.7% for within-corpus evaluations, and 14.0% for cross-corpora evaluations. The visualization of the activations of the last hidden layers illustrates that MTL creates better feature representation. The best structure has shared layers followed by attribute-dependent layers, capturing better the relation between attributes",
    "checked": true,
    "id": "9e616407b71813d7426793ae88cf01b2ebecbd0b",
    "semantic_title": "jointly predicting arousal, valence and dominance with multi-task learning",
    "citation_count": 107,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/le17b_interspeech.html": {
    "title": "Discretized Continuous Speech Emotion Recognition with Multi-Task Deep Recurrent Neural Network",
    "volume": "main",
    "abstract": "Estimating continuous emotional states from speech as a function of time has traditionally been framed as a regression problem. In this paper, we present a novel approach that moves the problem into the classification domain by discretizing the training labels at different resolutions. We employ a multi-task deep bidirectional long-short term memory (BLSTM) recurrent neural network (RNN) trained with cost-sensitive Cross Entropy loss to model these labels jointly. We introduce an emotion decoding algorithm that incorporates long- and short-term temporal properties of the signal to produce more robust time series estimates. We show that our proposed approach achieves competitive audio-only performance on the RECOLA dataset, relative to previously published works as well as other strong regression baselines. This work provides a link between regression and classification, and contributes an alternative approach for continuous emotion recognition",
    "checked": true,
    "id": "7720940403cfb35b7226103ab6e1df1a2bf71df1",
    "semantic_title": "discretized continuous speech emotion recognition with multi-task deep recurrent neural network",
    "citation_count": 48,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kim17d_interspeech.html": {
    "title": "Towards Speech Emotion Recognition \"in the Wild\" Using Aggregated Corpora and Deep Multi-Task Learning",
    "volume": "main",
    "abstract": "One of the challenges in Speech Emotion Recognition (SER) \"in the wild\" is the large mismatch between training and test data (e.g. speakers and tasks). In order to improve the generalisation capabilities of the emotion models, we propose to use Multi-Task Learning (MTL) and use gender and naturalness as auxiliary tasks in deep neural networks. This method was evaluated in within-corpus and various cross-corpus classification experiments that simulate conditions \"in the wild\". In comparison to Single-Task Learning (STL) based state of the art methods, we found that our MTL method proposed improved performance significantly. Particularly, models using both gender and naturalness achieved more gains than those using either gender or naturalness separately. This benefit was also found in the high-level representations of the feature space, obtained from our method proposed, where discriminative emotional clusters could be observed",
    "checked": true,
    "id": "e0ba2e87f8209a075c4cda0554014159c476c1c8",
    "semantic_title": "towards speech emotion recognition \"in the wild\" using aggregated corpora and deep multi-task learning",
    "citation_count": 80,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tamamori17_interspeech.html": {
    "title": "Speaker-Dependent WaveNet Vocoder",
    "volume": "main",
    "abstract": "In this study, we propose a speaker-dependent WaveNet vocoder, a method of synthesizing speech waveforms with WaveNet, by utilizing acoustic features from existing vocoder as auxiliary features of WaveNet. It is expected that WaveNet can learn a sample-by-sample correspondence between speech waveform and acoustic features. The advantage of the proposed method is that it does not require (1) explicit modeling of excitation signals and (2) various assumptions, which are based on prior knowledge specific to speech. We conducted both subjective and objective evaluation experiments on CMU-ARCTIC database. From the results of the objective evaluation, it was demonstrated that the proposed method could generate high-quality speech with phase information recovered, which was lost by a mel-cepstrum vocoder. From the results of the subjective evaluation, it was demonstrated that the sound quality of the proposed method was significantly improved from mel-cepstrum vocoder, and the proposed method could capture source excitation information more accurately",
    "checked": true,
    "id": "487aa8076bf3c0edb4134759e1ddf09d64f21476",
    "semantic_title": "speaker-dependent wavenet vocoder",
    "citation_count": 256,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gu17_interspeech.html": {
    "title": "Waveform Modeling Using Stacked Dilated Convolutional Neural Networks for Speech Bandwidth Extension",
    "volume": "main",
    "abstract": "This paper presents a waveform modeling and generation method for speech bandwidth extension (BWE) using stacked dilated convolutional neural networks (CNNs) with causal or non-causal convolutional layers. Such dilated CNNs describe the predictive distribution for each wideband or high-frequency speech sample conditioned on the input narrowband speech samples. Distinguished from conventional frame-based BWE approaches, the proposed methods can model the speech waveforms directly and therefore avert the spectral conversion and phase estimation problems. Experimental results prove that the BWE methods proposed in this paper can achieve better performance than the state-of-the-art frame-based approach utilizing recurrent neural networks (RNNs) incorporating long short-term memory (LSTM) cells in subjective preference tests",
    "checked": true,
    "id": "773e95ec372046a3be8c0e98cf17f9f1bfcbea1a",
    "semantic_title": "waveform modeling using stacked dilated convolutional neural networks for speech bandwidth extension",
    "citation_count": 20,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/takaki17_interspeech.html": {
    "title": "Direct Modeling of Frequency Spectra and Waveform Generation Based on Phase Recovery for DNN-Based Speech Synthesis",
    "volume": "main",
    "abstract": "In statistical parametric speech synthesis (SPSS) systems using the high-quality vocoder, acoustic features such as mel-cepstrum coefficients and F0 are predicted from linguistic features in order to utilize the vocoder to generate speech waveforms. However, the generated speech waveform generally suffers from quality deterioration such as buzziness caused by utilizing the vocoder. Although several attempts such as improving an excitation model have been investigated to alleviate the problem, it is difficult to completely avoid it if the SPSS system is based on the vocoder. To overcome this problem, there have recently been attempts to directly model waveform samples. Superior performance has been demonstrated, but computation time and latency are still issues. With the aim to construct another type of DNN-based speech synthesizer with neither the vocoder nor computational explosion, we investigated direct modeling of frequency spectra and waveform generation based on phase recovery. In this framework, STFT spectral amplitudes that include harmonic information derived from F0 are directly predicted through a DNN-based acoustic model and we use Griffin and Lim's approach to recover phase and generate waveforms. The experimental results showed that the proposed system synthesized speech without buzziness and outperformed one generated from a conventional system using the vocoder",
    "checked": true,
    "id": "2ea59b2bcab7badfd028e92345ed3a273d6ee0b9",
    "semantic_title": "direct modeling of frequency spectra and waveform generation based on phase recovery for dnn-based speech synthesis",
    "citation_count": 37,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ronanki17_interspeech.html": {
    "title": "A Hierarchical Encoder-Decoder Model for Statistical Parametric Speech Synthesis",
    "volume": "main",
    "abstract": "Current approaches to statistical parametric speech synthesis using Neural Networks generally require input at the same temporal resolution as the output, typically a frame every 5ms, or in some cases at waveform sampling rate. It is therefore necessary to fabricate highly-redundant frame-level (or sample-level) linguistic features at the input. This paper proposes the use of a hierarchical encoder-decoder model to perform the sequence-to-sequence regression in a way that takes the input linguistic features at their original timescales, and preserves the relationships between words, syllables and phones. The proposed model is designed to make more effective use of suprasegmental features than conventional architectures, as well as being computationally efficient. Experiments were conducted on prosodically-varied audiobook material because the use of supra-segmental features is thought to be particularly important in this case. Both objective measures and results from subjective listening tests, which asked listeners to focus on prosody, show that the proposed method performs significantly better than a conventional architecture that requires the linguistic input to be at the acoustic frame rate We provide code and a recipe to enable our system to be reproduced using the Merlin toolkit",
    "checked": true,
    "id": "7042f7f82813021827beda34fb775fd6de6161a1",
    "semantic_title": "a hierarchical encoder-decoder model for statistical parametric speech synthesis",
    "citation_count": 13,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kobayashi17_interspeech.html": {
    "title": "Statistical Voice Conversion with WaveNet-Based Waveform Generation",
    "volume": "main",
    "abstract": "This paper presents a statistical voice conversion (VC) technique with theWaveNet-based waveform generation. VC based on a Gaussian mixture model (GMM) makes it possible to convert the speaker identity of a source speaker into that of a target speaker. However, in the conventional vocoding process, various factors such as F extraction errors, parameterization errors and over-smoothing effects of converted feature trajectory cause the modeling errors of the speech waveform, which usually bring about sound quality degradation of the converted voice. To address this issue, we apply a direct waveform generation technique based on a WaveNet vocoder to VC. In the proposed method, first, the acoustic features of the source speaker are converted into those of the target speaker based on the GMM. Then, the waveform samples of the converted voice are generated based on the WaveNet vocoder conditioned on the converted acoustic features. In this paper, to investigate the modeling accuracies of the converted speech waveform, we compare several types of the acoustic features for training and synthesizing based on the WaveNet vocoder. The experimental results confirmed that the proposed VC technique achieves higher conversion accuracy on speaker individuality with comparable sound quality compared to the conventional VC technique",
    "checked": true,
    "id": "ac7ad4a80e5b0aac54ede084f93d3c3252d1b988",
    "semantic_title": "statistical voice conversion with wavenet-based waveform generation",
    "citation_count": 89,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wan17_interspeech.html": {
    "title": "Google's Next-Generation Real-Time Unit-Selection Synthesizer Using Sequence-to-Sequence LSTM-Based Autoencoders",
    "volume": "main",
    "abstract": "A neural network model that significant improves unit-selection-based Text-To-Speech synthesis is presented. The model employs a sequence-to-sequence LSTM-based autoencoder that compresses the acoustic and linguistic features of each unit to a fixed-size vector referred to as an embedding. Unit-selection is facilitated by formulating the target cost as an L distance in the embedding space. In open-domain speech synthesis the method achieves a 0.2 improvement in the MOS, while for limited-domain it reaches the cap of 4.5 MOS. Furthermore, the new TTS system halves the gap between the previous unit-selection system and WaveNet in terms of quality while retaining low computational cost and latency",
    "checked": true,
    "id": "19c5f127671aaa6e418caf1f77ad20b7cefd13ad",
    "semantic_title": "google's next-generation real-time unit-selection synthesizer using sequence-to-sequence lstm-based autoencoders",
    "citation_count": 39,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kain17_interspeech.html": {
    "title": "A Comparison of Sentence-Level Speech Intelligibility Metrics",
    "volume": "main",
    "abstract": "We examine existing and novel automatically-derived acoustic metrics that are predictive of speech intelligibility. We hypothesize that the degree of variability in feature space is correlated with the extent of a speaker's phonemic inventory, their degree of articulatory displacements, and thus with their degree of perceived speech intelligibility. We begin by using fully-automatic F1/F2 formant frequency trajectories for both vowel space area calculation and as input to a proposed class-separability metric. We then switch to representing vowels by means of short-term spectral features, and measure vowel separability in that space. Finally, we consider the case where phonetic labeling is unavailable; here we calculate short-term spectral features for the entire speech utterance and then estimate their entropy based on the length of a minimum spanning tree. In an alternative approach, we propose to first segment the speech signal using a hidden Markov model, and then calculate spectral feature separability based on the automatically-derived classes. We apply all approaches to a database with healthy controls as well as speakers with mild dysarthria, and report the resulting coefficients of determination",
    "checked": true,
    "id": "8d4f24cb8ea51fcffe595f46b9e27116168df2c1",
    "semantic_title": "a comparison of sentence-level speech intelligibility metrics",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/irino17_interspeech.html": {
    "title": "An Auditory Model of Speaker Size Perception for Voiced Speech Sounds",
    "volume": "main",
    "abstract": "An auditory model was developed to explain the results of behavioral experiments on perception of speaker size with voiced speech sounds. It is based on the dynamic, compressive gammachirp (dcGC) filterbank and a weighting function (SSI weight) derived from a theory of size-shape segregation in the auditory system. Voiced words with and without high-frequency emphasis (+6 dB/octave) were produced using a speech vocoder (STRAIGHT). The SSI weighting function reduces the effect of glottal pulse excitation in voiced speech, which, in turn, makes it possible for the model to explain the individual subject variability in the data",
    "checked": true,
    "id": "1ad03b893a0ff3beceeed1bffd9bd8b040d78654",
    "semantic_title": "an auditory model of speaker size perception for voiced speech sounds",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bosch17_interspeech.html": {
    "title": "The Recognition of Compounds: A Computational Account",
    "volume": "main",
    "abstract": "This paper investigates the processes in comprehending spoken noun-noun compounds, using data from the BALDEY database. BALDEY contains lexicality judgments and reaction times (RTs) for Dutch stimuli for which also linguistic information is included. Two different approaches are combined. The first is based on regression by Dynamic Survival Analysis, which models decisions and RTs as a consequence of the fact that a cumulative density function exceeds some threshold. The parameters of that function are estimated from the observed RT data. The second approach is based on DIANA, a process-oriented computational model of human word comprehension, which simulates the comprehension process with the acoustic stimulus as input. DIANA gives the identity and the number of the word candidates that are activated at each 10 ms time step Both approaches show how the processes involved in comprehending compounds change during a stimulus. Survival Analysis shows that the impact of word duration varies during the course of a stimulus. The density of word and non-word hypotheses in DIANA shows a corresponding pattern with different regimes. We show how the approaches complement each other, and discuss additional ways in which data and process models can be combined",
    "checked": true,
    "id": "ccfc2fe007d19cfd70b45e67e9ba62f7b5bf927a",
    "semantic_title": "the recognition of compounds: a computational account",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jahromi17_interspeech.html": {
    "title": "Humans do not Maximize the Probability of Correct Decision When Recognizing DANTALE Words in Noise",
    "volume": "main",
    "abstract": "Inspired by the DANTALE II listening test paradigm, which is used for determining the intelligibility of noisy speech, we assess the hypothesis that humans maximize the probability of correct decision when recognizing words contaminated by additive Gaussian, speech-shaped noise. We first propose a statistical Gaussian communication and classification scenario, where word models are built from short term spectra of human speech, and optimal classifiers in the sense of maximum a posteriori estimation are derived. Then, we perform a listening test, where the participants are instructed to make their best guess of words contaminated with speech-shaped Gaussian noise. Comparing the human's performance to that of the optimal classifier reveals that at high SNR, humans perform comparable to the optimal classifier. However, at low SNR, the human performance is inferior to that of the optimal classifier. This shows that, at least in this specialized task, humans are generally not able to maximize the probability of correct decision, when recognizing words",
    "checked": true,
    "id": "fa2fec8495c269af2d29526b7c9da95496b49fdf",
    "semantic_title": "humans do not maximize the probability of correct decision when recognizing dantale words in noise",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/huber17_interspeech.html": {
    "title": "Single-Ended Prediction of Listening Effort Based on Automatic Speech Recognition",
    "volume": "main",
    "abstract": "A new, single-ended, i.e. reference-free measure for the prediction of perceived listening effort of noisy speech is presented. It is based on phoneme posterior probabilities (or posteriorgrams) obtained from a deep neural network of an automatic speech recognition system. Additive noisy or other distortions of speech tend to smear the posteriorgrams. The smearing is quantified by a performance measure, which is used as a predictor for the perceived listening effort required to understand the noisy speech. The proposed measure was evaluated using a database obtained from the subjective evaluation of noise reduction algorithms of commercial hearing aids. Listening effort ratings of processed noisy speech samples were gathered from 20 hearing-impaired subjects. Averaged subjective ratings were compared with corresponding predictions computed by the proposed new method, the ITU-T standard P.563 for single-ended speech quality assessment, the American National Standard ANIQUE+ for single-ended speech quality assessment, and a single-ended SNR estimator. The proposed method achieved a good correlation with mean subjective ratings and clearly outperformed the standard speech quality measures and the SNR estimator",
    "checked": true,
    "id": "05bfb1e2e98b8cc43aa0c6c4f91d30ed90708a05",
    "semantic_title": "single-ended prediction of listening effort based on automatic speech recognition",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/neufeld17_interspeech.html": {
    "title": "Modeling Categorical Perception with the Receptive Fields of Auditory Neurons",
    "volume": "main",
    "abstract": "This paper demonstrates that a low-level, linear description of the response properties of auditory neurons can exhibit some of the high-level properties of the categorical perception of human speech. In particular, it is shown that the non-linearities observed in the human perception of speech sounds which span a categorical boundaries can be understood as arising rather naturally from a low-level statistical description of phonemic contrasts in the time-frequency plane, understood here as the receptive field of auditory neurons. The TIMIT database was used to train a model auditory neuron which discriminates between /s/ and /sh/, and a computer simulation was conducted which demonstrates that the neuron responds categorically to a linear continuum of synthetic fricative sounds which span the /s/-/sh/ boundary. The response of the model provides a good fit to human labeling behavior, and in addition, is able to account for asymmetries in reaction time across the two categories",
    "checked": true,
    "id": "1b52e267392cc72b76a7bcf658acb88f38bf452c",
    "semantic_title": "modeling categorical perception with the receptive fields of auditory neurons",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wang17f_interspeech.html": {
    "title": "A Maximum Likelihood Approach to Deep Neural Network Based Nonlinear Spectral Mapping for Single-Channel Speech Separation",
    "volume": "main",
    "abstract": "In contrast to the conventional minimum mean squared error (MMSE) training criterion for nonlinear spectral mapping based on deep neural networks (DNNs), we propose a probabilistic learning framework to estimate the DNN parameters for single-channel speech separation. A statistical analysis of the prediction error vector at the DNN output reveals that it follows a unimodal density for each log power spectral component. By characterizing the prediction error vector as a multivariate Gaussian density with zero mean vector and an unknown covariance matrix, we present a maximum likelihood (ML) approach to DNN parameter learning. Our experiments on the Speech Separation Challenge (SSC) corpus show that the proposed learning approach can achieve a better generalization capability and a faster convergence than MMSE-based DNN learning. Furthermore, we demonstrate that the ML-trained DNN consistently outperforms MMSE-trained DNN in all the objective measures of speech quality and intelligibility in single-channel speech separation",
    "checked": true,
    "id": "6d7b2fbbc71a5824a28e8b64cb5144dda38efcec",
    "semantic_title": "a maximum likelihood approach to deep neural network based nonlinear spectral mapping for single-channel speech separation",
    "citation_count": 19,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/higuchi17_interspeech.html": {
    "title": "Deep Clustering-Based Beamforming for Separation with Unknown Number of Sources",
    "volume": "main",
    "abstract": "This paper extends a deep clustering algorithm for use with time-frequency masking-based beamforming and perform separation with an unknown number of sources. Deep clustering is a recently proposed single-channel source separation algorithm, which projects inputs into the embedding space and performs clustering in the embedding domain. In deep clustering, bi-directional long short-term memory (BLSTM) recurrent neural networks are trained to make embedding vectors orthogonal for different speakers and concurrent for the same speaker. Then, by clustering the embedding vectors at test time, we can estimate time-frequency masks for separation. In this paper, we extend the deep clustering algorithm to a multiple microphone setup and incorporate deep clustering-based time-frequency mask estimation into masking-based beamforming, which has been shown to be more effective than masking for automatic speech recognition. Moreover, we perform source counting by computing the rank of the covariance matrix of the embedding vectors. With our proposed approach, we can perform masking-based beamforming in a multiple-speaker case without knowing the number of speakers. Experimental results show that our proposed deep clustering-based beamformer achieves comparable source separation performance to that obtained with a complex Gaussian mixture model-based beamformer, which requires the number of sources in advance for mask estimation",
    "checked": true,
    "id": "469d81aa88e01205c3b3449b2d4010477acd56b1",
    "semantic_title": "deep clustering-based beamforming for separation with unknown number of sources",
    "citation_count": 38,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/pirhosseinloo17_interspeech.html": {
    "title": "Time-Frequency Masking for Blind Source Separation with Preserved Spatial Cues",
    "volume": "main",
    "abstract": "In this paper, we address the problem of speech source separation by relying on time-frequency binary masks to segregate binaural mixtures. We describe an algorithm which can tackle reverberant mixtures and can extract the original sources while preserving their original spatial locations. The performance of the proposed algorithm is evaluated objectively and subjectively, by assessing the estimated interaural time differences versus their theoretical values and by testing for localization acuity in normal-hearing listeners for different spatial locations in a reverberant room. Experimental results indicate that the proposed algorithm is capable of preserving the spatial information of the recovered source signals while keeping the signal-to-distortion and signal-to-interference ratios high",
    "checked": true,
    "id": "867d77c8cac059df025744c8d36ae67ac0cad93d",
    "semantic_title": "time-frequency masking for blind source separation with preserved spatial cues",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chien17b_interspeech.html": {
    "title": "Variational Recurrent Neural Networks for Speech Separation",
    "volume": "main",
    "abstract": "We present a new stochastic learning machine for speech separation based on the variational recurrent neural network (VRNN). This VRNN is constructed from the perspectives of generative stochastic network and variational auto-encoder. The idea is to faithfully characterize the randomness of hidden state of a recurrent neural network through variational learning. The neural parameters under this latent variable model are estimated by maximizing the variational lower bound of log marginal likelihood. An inference network driven by the variational distribution is trained from a set of mixed signals and the associated source targets. A novel supervised VRNN is developed for speech separation. The proposed VRNN provides a stochastic point of view which accommodates the uncertainty in hidden states and facilitates the analysis of model construction. The masking function is further employed in network outputs for speech separation. The benefit of using VRNN is demonstrated by the experiments on monaural speech separation",
    "checked": true,
    "id": "80d45f972fb9f9c07947edce2c90ac492b2e9789",
    "semantic_title": "variational recurrent neural networks for speech separation",
    "citation_count": 36,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/andrei17_interspeech.html": {
    "title": "Detecting Overlapped Speech on Short Timeframes Using Deep Learning",
    "volume": "main",
    "abstract": "The intent of this work is to demonstrate how deep learning techniques can be successfully used to detect overlapped speech on independent short timeframes. A secondary objective is to provide an understanding on how the duration of the signal frame influences the accuracy of the method. We trained a deep neural network with heterogeneous layers and obtained close to 80% inference accuracy on frames going as low as 25 milliseconds. The proposed system provides higher detection quality than existing work and can predict overlapped speech with up to 3 simultaneous speakers. The method exposes low response latency and does not require a high amount of computing power",
    "checked": true,
    "id": "96c5b7e95a83f763d9078f435dac88b607cd5dab",
    "semantic_title": "detecting overlapped speech on short timeframes using deep learning",
    "citation_count": 27,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/li17f_interspeech.html": {
    "title": "Ideal Ratio Mask Estimation Using Deep Neural Networks for Monaural Speech Segregation in Noisy Reverberant Conditions",
    "volume": "main",
    "abstract": "Monaural speech segregation is an important problem in robust speech processing and has been formulated as a supervised learning problem. In supervised learning methods, the ideal binary mask (IBM) is usually used as the target because of its simplicity and large speech intelligibility gains. Recently, the ideal ratio mask (IRM) has been found to improve the speech quality over the IBM. However, the IRM was originally defined in anechoic conditions and did not consider the effect of reverberation. In this paper, the IRM is extended to reverberant conditions where the direct sound and early reflections of target speech are regarded as the desired signal. Deep neural networks (DNNs) is employed to estimate the extended IRM in the noisy reverberant conditions. The estimated IRM is then applied to the noisy reverberant mixture for speech segregation. Experimental results show that the estimated IRM provides substantial improvements in speech intelligibility and speech quality over the unprocessed mixture signals under various noisy and reverberant conditions",
    "checked": true,
    "id": "4cc54596ef0d37d66b10d9ba8c5a8e2e755369fe",
    "semantic_title": "ideal ratio mask estimation using deep neural networks for monaural speech segregation in noisy reverberant conditions",
    "citation_count": 22,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/quiroz17_interspeech.html": {
    "title": "The Vocative Chant and Beyond: German Calling Melodies Under Routine and Urgent Contexts",
    "volume": "main",
    "abstract": "This paper investigates calling melodies produced by 21 Standard German native speakers on a discourse completion task across two contexts: (i) routine context — calling a child from afar to come in for dinner; (ii) urgent context — calling a child from afar for a chastising. The intent of this investigation is to bring attention to other calling melodies found in German beside the vocative chant and to give an insight to their acoustic profile Three major melodies were identified in the two contexts: vocative chant (100% of routine context productions), urgent call (100% of male urgent context productions, 52.2% female productions), and stern call (47.8% female urgent context productions). A subsequent quantitative analysis was carried out on these calls across these parameters: (i) tonal scaling at tonal landmarks; (ii) proportional alignment of selected tonal landmarks with respect to the stressed or last vowel; and (iii) amplitude (integral and RMS) and (iv) duration of the stressed vowel, stressed syllable, and word. The resulting data were analyzed using a linear mixed model approach The results point to significant differences in the contours produced in the aforementioned parameters. We also proposed a phonological description of the contours in the framework of Autosegmental-Metrical Phonology",
    "checked": true,
    "id": "ddd6c7503b0d1c4ee685284b3b10043a7061b70a",
    "semantic_title": "the vocative chant and beyond: german calling melodies under routine and urgent contexts",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/simko17_interspeech.html": {
    "title": "Comparing Languages Using Hierarchical Prosodic Analysis",
    "volume": "main",
    "abstract": "We present a novel, data-driven approach to assessing mutual similarities and differences among a group of languages, based on purely prosodic characteristics, namely f and energy envelope signals. These signals are decomposed using continuous wavelet transform; the components represent f and energy patterns on three levels of prosodic hierarchy roughly corresponding to syllables, words and phrases. Unigram language models with states derived from a combination of Δ-features obtained from these components are trained and compared using a mutual perplexity measure. In this pilot study we apply this approach to a small corpus of spoken material from seven languages (Estonian, Finnish, Hungarian, German, Swedish, Russian and Slovak) with a rich history of mutual language contacts. We present similarity trees (dendrograms) derived from the models using the hierarchically decomposed prosodic signals separately as well as combined, and compare them with patterns obtained from non-decomposed signals. We show that (1) plausible similarity patterns, reflecting language family relationships and the known contact history can be obtained even from a relatively small data set, and (2) the hierarchical decomposition approach using both f and energy provides the most comprehensive results",
    "checked": true,
    "id": "3b1d2e383d15f5d6c0ca8524c88449a98d725bf8",
    "semantic_title": "comparing languages using hierarchical prosodic analysis",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ip17_interspeech.html": {
    "title": "Intonation Facilitates Prediction of Focus Even in the Presence of Lexical Tones",
    "volume": "main",
    "abstract": "In English and Dutch, listeners entrain to prosodic contours to predict where focus will fall in an utterance. However, is this strategy universally available, even in languages with different phonological systems? In a phoneme detection experiment, we examined whether prosodic entrainment is also found in Mandarin Chinese, a tone language, where in principle the use of pitch for lexical identity may take precedence over the use of pitch cues to salience. Consistent with the results from Germanic languages, response times were facilitated when preceding intonation predicted accent on the target-bearing word. Acoustic analyses revealed greater F0 range in the preceding intonation of the predicted-accent sentences. These findings have implications for how universal and language-specific mechanisms interact in the processing of salience",
    "checked": true,
    "id": "4b279998d590949153043c9616a4abb46295c2e1",
    "semantic_title": "intonation facilitates prediction of focus even in the presence of lexical tones",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zahner17_interspeech.html": {
    "title": "Mind the Peak: When Museum is Temporarily Understood as Musical in Australian English",
    "volume": "main",
    "abstract": "Intonation languages signal pragmatic functions (e.g. information structure) by means of different pitch accent types. Acoustically, pitch accent types differ in the alignment of pitch peaks (and valleys) in regard to stressed syllables, which makes the position of pitch peaks an unreliable cue to lexical stress (even though pitch peaks and lexical stress often coincide in intonation languages). We here investigate the effect of pitch accent type on lexical activation in English. Results of a visual-world eye-tracking study show that Australian English listeners temporarily activate SWW-words ( musical) if presented with WSW-words ( museum) with early-peak accents (H+!H*), compared to medial-peak accents (L+H*). Thus, in addition to signalling pragmatic functions, the alignment of tonal targets immediately affects lexical activation in English",
    "checked": true,
    "id": "c70fa54ab94d38a2633a6833744a684ecf24e4a4",
    "semantic_title": "mind the peak: when museum is temporarily understood as musical in australian english",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/rognoni17_interspeech.html": {
    "title": "Pashto Intonation Patterns",
    "volume": "main",
    "abstract": "A hand-labelled Pashto speech data set containing spontaneous conversations is analysed in order to propose an intonational inventory of Pashto. Basic intonation patterns observed in the language are summarised. The relationship between pitch accent and part of speech (PoS), which was also annotated for each word in the data set, is briefly addressed The results are compared with the intonational literature on Persian, a better-described and closely-related language. The results show that Pashto intonation patterns are similar to Persian, as well as reflecting common intonation patterns such as falling tone for statements and WH-questions, and yes/no questions ending in a rising tone. The data also show that the most frequently used intonation pattern in Pashto is the so-called hat pattern. The distribution of pitch accent is quite free both in Persian and Pashto, but there is a stronger association of pitch accent with content than with function words, as is typical of stress-accent languages The phonetic realisation of focus appears to be conveyed with the same acoustic cues as in Persian, with a higher pitch excursion and longer duration of the stressed syllable of the word in focus. The data also suggest that post-focus compression (PFC) is present in Pashto",
    "checked": true,
    "id": "0f48be8067ed474b8f54c6d75fc8eaf621953372",
    "semantic_title": "pashto intonation patterns",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/maekawa17_interspeech.html": {
    "title": "A New Model of Final Lowering in Spontaneous Monologue",
    "volume": "main",
    "abstract": "F0 downtrend observed in spontaneous monologues in the Corpus of Spontaneous Japanese was analyzed with special attention to the modeling of final lowering. In addition to the previous finding that the domain of final lowering covers all tones in the final accentual phrase, it turned out that the last L tone in the penultimate accentual phrase played important role in the control of final lowering. It is this tone that first reached the bottom of the speaker's pitch range in the time course of utterance; it also turned out that the phonetic realization of this tone is the most stable of all tones in terms of the F0 variability. Regression model of F0 downtrends is generated by generalized linear mixed-effect modeling and evaluated by cross-validation. The mean prediction error of z-normalized F0 values in the best model was 0.25 standard deviation",
    "checked": true,
    "id": "d0f495e430b9311870601625fb7f8f0d2014e3c8",
    "semantic_title": "a new model of final lowering in spontaneous monologue",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ma17c_interspeech.html": {
    "title": "Speech Emotion Recognition with Emotion-Pair Based Framework Considering Emotion Distribution Information in Dimensional Emotion Space",
    "volume": "main",
    "abstract": "In this work, an emotion-pair based framework is proposed for speech emotion recognition, which constructs more discriminative feature subspaces for every two different emotions (emotion-pair) to generate more precise emotion bi-classification results. Furthermore, it is found that in the dimensional emotion space, the distances between some of the archetypal emotions are closer than the others. Motivated by this, a Naive Bayes classifier based decision fusion strategy is proposed, which aims at capturing such useful emotion distribution information in deciding the final emotion category for emotion recognition. We evaluated the classification framework on the USC IEMOCAP database. Experimental results demonstrate that the proposed method outperforms the hierarchical binary decision tree approach on both weighted accuracy (WA) and unweighted accuracy (UA). Moreover, our framework possesses the advantages that it can be fully automatically generated without empirical guidance and is easier to be parallelized",
    "checked": true,
    "id": "7539945b5b66760ff4b909b4dcc55ee9e261fd34",
    "semantic_title": "speech emotion recognition with emotion-pair based framework considering emotion distribution information in dimensional emotion space",
    "citation_count": 16,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sahu17_interspeech.html": {
    "title": "Adversarial Auto-Encoders for Speech Based Emotion Recognition",
    "volume": "main",
    "abstract": "Recently, generative adversarial networks and adversarial auto-encoders have gained a lot of attention in machine learning community due to their exceptional performance in tasks such as digit classification and face recognition. They map the auto-encoder's bottleneck layer output (termed as code vectors) to different noise Probability Distribution Functions (PDFs), that can be further regularized to cluster based on class information. In addition, they also allow a generation of synthetic samples by sampling the code vectors from the mapped PDFs. Inspired by these properties, we investigate the application of adversarial auto-encoders to the domain of emotion recognition. Specifically, we conduct experiments on the following two aspects: (i) their ability to encode high dimensional feature vector representations for emotional utterances into a compressed space (with a minimal loss of emotion class discriminability in the compressed space), and (ii) their ability to regenerate synthetic samples in the original feature space, to be later used for purposes such as training emotion recognition classifiers. We demonstrate promise of adversarial auto-encoders with regards to these aspects on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) corpus and present our analysis",
    "checked": true,
    "id": "73a646f211a43de989349123787dea58fd1eb9ec",
    "semantic_title": "adversarial auto-encoders for speech based emotion recognition",
    "citation_count": 62,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/dang17_interspeech.html": {
    "title": "An Investigation of Emotion Prediction Uncertainty Using Gaussian Mixture Regression",
    "volume": "main",
    "abstract": "Existing continuous emotion prediction systems implicitly assume that prediction certainty does not vary with time. However, perception differences among raters and other possible sources of variability suggest that prediction certainty varies with time, which warrants deeper consideration. In this paper, the correlation between the inter-rater variability and the uncertainty of predicted emotion is firstly studied. A new paradigm that estimates the uncertainty in prediction is proposed based on the strong correlation uncovered in the RECOLA database. This is implemented by including the inter-rater variability as a representation of the uncertainty information in a probabilistic Gaussian Mixture Regression (GMR) model. In addition, we investigate the correlation between the uncertainty and the performance of a typical emotion prediction system utilizing average rating as the ground truth, by comparing the prediction performance in the lower and higher uncertainty regions. As expected, it is observed that the performance in lower uncertainty regions is better than that in higher uncertainty regions, providing a path for improving emotion prediction systems",
    "checked": true,
    "id": "abe75f450830997f4e395f0816c08911faa5597a",
    "semantic_title": "an investigation of emotion prediction uncertainty using gaussian mixture regression",
    "citation_count": 19,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/khorram17_interspeech.html": {
    "title": "Capturing Long-Term Temporal Dependencies with Convolutional Networks for Continuous Emotion Recognition",
    "volume": "main",
    "abstract": "The goal of continuous emotion recognition is to assign an emotion value to every frame in a sequence of acoustic features. We show that incorporating long-term temporal dependencies is critical for continuous emotion recognition tasks. To this end, we first investigate architectures that use dilated convolutions. We show that even though such architectures outperform previously reported systems, the output signals produced from such architectures undergo erratic changes between consecutive time steps. This is inconsistent with the slow moving ground-truth emotion labels that are obtained from human annotators. To deal with this problem, we model a downsampled version of the input signal and then generate the output signal through upsampling. Not only does the resulting downsampling/upsampling network achieve good performance, it also generates smooth output trajectories. Our method yields the best known audio-only performance on the RECOLA dataset",
    "checked": true,
    "id": "4c3292c5e8a334c23b04f37a081023d80a1259c7",
    "semantic_title": "capturing long-term temporal dependencies with convolutional networks for continuous emotion recognition",
    "citation_count": 36,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chasaide17_interspeech.html": {
    "title": "Voice-to-Affect Mapping: Inferences on Language Voice Baseline Settings",
    "volume": "main",
    "abstract": "Modulations of the voice convey affect, and the precise mapping of voice-to-affect may vary for different languages. However, affect-related modulations occur relative to the baseline affect-neutral voice, which tends to differ from language to language. Little is known about the characteristic long-term voice settings for different languages, and how they influence the use of voice quality to signal affect. In this paper, data from a voice-to-affect perception test involving Russian, English, Spanish and Japanese subjects is re-examined to glean insights concerning likely baseline settings in these languages. The test used synthetic stimuli with different voice qualities (modelled on a male voice), with or without extreme f contours as might be associated with affect. Cross-language differences in affect ratings for modal and tense voice suggest that the baseline in Spanish and Japanese is inherently tenser than in Russian and English, and that as a corollary, tense voice serves as a more potent cue to high-activation affects in the latter languages. A relatively tenser baseline in Japanese and Spanish is further suggested by the fact that tense voice can be associated with intimate, a low activation state, just as readily as with the high-activation state interested",
    "checked": true,
    "id": "df4133d7bf11212bd995bbade6231f8788b7b6d9",
    "semantic_title": "voice-to-affect mapping: inferences on language voice baseline settings",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/neumann17_interspeech.html": {
    "title": "Attentive Convolutional Neural Network Based Speech Emotion Recognition: A Study on the Impact of Input Features, Signal Length, and Acted Speech",
    "volume": "main",
    "abstract": "Speech emotion recognition is an important and challenging task in the realm of human-computer interaction. Prior work proposed a variety of models and feature sets for training a system. In this work, we conduct extensive experiments using an attentive convolutional neural network with multi-view learning objective function. We compare system performance using different lengths of the input signal, different types of acoustic features and different types of emotion speech (improvised/scripted). Our experimental results on the Interactive Emotional Motion Capture (IEMOCAP) database reveal that the recognition performance strongly depends on the type of speech data independent of the choice of input features. Furthermore, we achieved state-of-the-art results on the improvised speech data of IEMOCAP",
    "checked": true,
    "id": "ca7fb00692339f9704b4a5201604889994825a42",
    "semantic_title": "attentive convolutional neural network based speech emotion recognition: a study on the impact of input features, signal length, and acted speech",
    "citation_count": 208,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/miyoshi17_interspeech.html": {
    "title": "Voice Conversion Using Sequence-to-Sequence Learning of Context Posterior Probabilities",
    "volume": "main",
    "abstract": "Voice conversion (VC) using sequence-to-sequence learning of context posterior probabilities is proposed. Conventional VC using shared context posterior probabilities predicts target speech parameters from the context posterior probabilities estimated from the source speech parameters. Although conventional VC can be built from non-parallel data, it is difficult to convert speaker individuality such as phonetic property and speaking rate contained in the posterior probabilities because the source posterior probabilities are directly used for predicting target speech parameters. In this work, we assume that the training data partly include parallel speech data and propose sequence-to-sequence learning between the source and target posterior probabilities. The conversion models perform non-linear and variable-length transformation from the source probability sequence to the target one. Further, we propose a joint training algorithm for the modules. In contrast to conventional VC, which separately trains the speech recognition that estimates posterior probabilities and the speech synthesis that predicts target speech parameters, our proposed method jointly trains these modules along with the proposed probability conversion modules. Experimental results demonstrate that our approach outperforms the conventional VC",
    "checked": true,
    "id": "641b1c0980c9d8c0d897261af608d18e6a9c7d3b",
    "semantic_title": "voice conversion using sequence-to-sequence learning of context posterior probabilities",
    "citation_count": 57,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hsu17_interspeech.html": {
    "title": "Learning Latent Representations for Speech Generation and Transformation",
    "volume": "main",
    "abstract": "An ability to model a generative process and learn a latent representation for speech in an unsupervised fashion will be crucial to process vast quantities of unlabelled speech data. Recently, deep probabilistic generative models such as Variational Autoencoders (VAEs) have achieved tremendous success in modeling natural images. In this paper, we apply a convolutional VAE to model the generative process of natural speech. We derive latent space arithmetic operations to disentangle learned latent representations. We demonstrate the capability of our model to modify the phonetic content or the speaker identity for speech segments using the derived operations, without the need for parallel supervisory data",
    "checked": true,
    "id": "4c20dff792bc447fab730e05f5e997694b67a11e",
    "semantic_title": "learning latent representations for speech generation and transformation",
    "citation_count": 139,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hashimoto17_interspeech.html": {
    "title": "Parallel-Data-Free Many-to-Many Voice Conversion Based on DNN Integrated with Eigenspace Using a Non-Parallel Speech Corpus",
    "volume": "main",
    "abstract": "This paper proposes a novel approach to parallel-data-free and many-to-many voice conversion (VC). As 1-to-1 conversion has less flexibility, researchers focus on many-to-many conversion, where speaker identity is often represented using speaker space bases. In this case, utterances of the same sentences have to be collected from many speakers. This study aims at overcoming this constraint to realize a parallel-data-free and many-to-many conversion. This is made possible by integrating deep neural networks (DNNs) with eigenspace using a non-parallel speech corpus. In our previous study, many-to-many conversion was implemented using DNN, whose training was assisted by EVGMM conversion. By realizing the function of EVGMM equivalently by constructing eigenspace with a non-parallel speech corpus, the desired conversion is made possible. A key technique here is to estimate covariance terms without given parallel data between source and target speakers. Experiments show that objective assessment scores are comparable to those of the baseline system trained with parallel data",
    "checked": true,
    "id": "2d57285227ac3f18b37c67210e6b594aecfa85ab",
    "semantic_title": "parallel-data-free many-to-many voice conversion based on dnn integrated with eigenspace using a non-parallel speech corpus",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kaneko17_interspeech.html": {
    "title": "Sequence-to-Sequence Voice Conversion with Similarity Metric Learned Using Generative Adversarial Networks",
    "volume": "main",
    "abstract": "We propose a training framework for sequence-to-sequence voice conversion (SVC). A well-known problem regarding a conventional VC framework is that acoustic-feature sequences generated from a converter tend to be over-smoothed, resulting in buzzy-sounding speech. This is because a particular form of similarity metric or distribution for parameter training of the acoustic model is assumed so that the generated feature sequence that averagely fits the training target example is considered optimal. This over-smoothing occurs as long as a manually constructed similarity metric is used. To overcome this limitation, our proposed SVC framework uses a similarity metric implicitly derived from a generative adversarial network, enabling the measurement of the distance in the high-level abstract space. This would enable the model to mitigate the over-smoothing problem caused in the low-level data space. Furthermore, we use convolutional neural networks to model the long-range context-dependencies. This also enables the similarity metric to have a shift-invariant property; thus, making the model robust against misalignment errors involved in the parallel data. We tested our framework on a non-native-to-native VC task. The experimental results revealed that the use of the proposed framework had a certain effect in improving naturalness, clarity, and speaker individuality",
    "checked": true,
    "id": "870459a595a3511462d56a4431949d9c33933979",
    "semantic_title": "sequence-to-sequence voice conversion with similarity metric learned using generative adversarial networks",
    "citation_count": 97,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ardaillon17_interspeech.html": {
    "title": "A Mouth Opening Effect Based on Pole Modification for Expressive Singing Voice Transformation",
    "volume": "main",
    "abstract": "Improving expressiveness in singing voice synthesis systems requires to perform realistic timbre transformations, e.g. for varying voice intensity. In order to sing louder, singers tend to open their mouth more widely, which changes the vocal tract's shape and resonances. This study shows, by means of signal analysis and simulations, that the main effect of mouth opening is an increase of the 1 formant's frequency (F ) and a decrease of its bandwidth (BW ). From these observations, we then propose a rule for producing a mouth opening effect, by modifying F and BW , and an approach to apply this effect on real voice sounds. This approach is based on pole modification, by changing the AR coefficients of an estimated all-pole model of the spectral envelope. Finally, listening tests have been conducted to evaluate the effectiveness of the proposed effect",
    "checked": true,
    "id": "35c50660911af169c37743747a997aa562e97a3f",
    "semantic_title": "a mouth opening effect based on pole modification for expressive singing voice transformation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mohammadi17_interspeech.html": {
    "title": "Siamese Autoencoders for Speech Style Extraction and Switching Applied to Voice Identification and Conversion",
    "volume": "main",
    "abstract": "We propose an architecture called siamese autoencoders for extracting and switching pre-determined styles of speech signals while retaining the content. We apply this architecture to a voice conversion task in which we define the content to be the linguistic message and the style to be the speaker's voice. We assume two or more data streams with the same content but unique styles. The architecture is composed of two or more separate but shared-weight autoencoders that are joined by loss functions at the hidden layers. A hidden vector is composed of style and content sub-vectors and the loss functions constrain the encodings to decompose style and content. We can select an intended target speaker either by supplying the associated style vector, or by extracting a new style vector from a new utterance, using a proposed style extraction algorithm. We focus on in-training speakers but perform some initial experiments for out-of-training speakers as well. We propose and study several types of loss functions. The experiment results show that the proposed many-to-many model is able to convert voices successfully; however, its performance does not surpass that of the state-of-the-art one-to-one model's",
    "checked": true,
    "id": "85c25573745df89e2934bd5fc1daf7335fef65a2",
    "semantic_title": "siamese autoencoders for speech style extraction and switching applied to voice identification and conversion",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sak17_interspeech.html": {
    "title": "Recurrent Neural Aligner: An Encoder-Decoder Neural Network Model for Sequence to Sequence Mapping",
    "volume": "main",
    "abstract": "We introduce an encoder-decoder recurrent neural network model called Recurrent Neural Aligner (RNA) that can be used for sequence to sequence mapping tasks. Like connectionist temporal classification (CTC) models, RNA defines a probability distribution over target label sequences including blank labels corresponding to each time step in input. The probability of a label sequence is calculated by marginalizing over all possible blank label positions. Unlike CTC, RNA does not make a conditional independence assumption for label predictions; it uses the predicted label at time t-1 as an additional input to the recurrent model when predicting the label at time t. We apply this model to end-to-end speech recognition. RNA is capable of streaming recognition since the decoder does not employ attention mechanism. The model is trained on transcribed acoustic data to predict graphemes and no external language and pronunciation models are used for decoding. We employ an approximate dynamic programming method to optimize negative log likelihood, and a sampling-based sequence discriminative training technique to fine-tune the model to minimize expected word error rate. We show that the model achieves competitive accuracy without using an external language model nor doing beam search decoding",
    "checked": true,
    "id": "7703a2c5468ecbee5b62c048339a03358ed5fe19",
    "semantic_title": "recurrent neural aligner: an encoder-decoder neural network model for sequence to sequence mapping",
    "citation_count": 120,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/pundak17_interspeech.html": {
    "title": "Highway-LSTM and Recurrent Highway Networks for Speech Recognition",
    "volume": "main",
    "abstract": "Recently, very deep networks, with as many as hundreds of layers, have shown great success in image classification tasks. One key component that has enabled such deep models is the use of \"skip connections\", including either residual or highway connections, to alleviate the vanishing and exploding gradient problems. While these connections have been explored for speech, they have mainly been explored for feed-forward networks. Since recurrent structures, such as LSTMs, have produced state-of-the-art results on many of our Voice Search tasks, the goal of this work is to thoroughly investigate different approaches to adding depth to recurrent structures. Specifically, we experiment with novel Highway-LSTM models with bottlenecks skip connections and show that a 10 layer model can outperform a state-of-the-art 5 layer LSTM model with the same number of parameters by 2% relative WER. In addition, we experiment with Recurrent Highway layers and find these to be on par with Highway-LSTM models, when given sufficient depth",
    "checked": true,
    "id": "f84a30bf0554ee3e1c06cad52fe66f072956849b",
    "semantic_title": "highway-lstm and recurrent highway networks for speech recognition",
    "citation_count": 28,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ravanelli17_interspeech.html": {
    "title": "Improving Speech Recognition by Revising Gated Recurrent Units",
    "volume": "main",
    "abstract": "Speech recognition is largely taking advantage of deep learning, showing that substantial benefits can be obtained by modern Recurrent Neural Networks (RNNs). The most popular RNNs are Long Short-Term Memory (LSTMs), which typically reach state-of-the-art performance in many tasks thanks to their ability to learn long-term dependencies and robustness to vanishing gradients. Nevertheless, LSTMs have a rather complex design with three multiplicative gates, that might impair their efficient implementation. An attempt to simplify LSTMs has recently led to Gated Recurrent Units (GRUs), which are based on just two multiplicative gates This paper builds on these efforts by further revising GRUs and proposing a simplified architecture potentially more suitable for speech recognition. The contribution of this work is two-fold. First, we suggest to remove the reset gate in the GRU design, resulting in a more efficient single-gate architecture. Second, we propose to replace tanh with ReLU activations in the state update equations. Results show that, in our implementation, the revised architecture reduces the per-epoch training time with more than 30% and consistently improves recognition performance across different tasks, input features, and noisy conditions when compared to a standard GRU",
    "checked": true,
    "id": "d9e75a0c9d0a6538e822e460f068ac3963fbecc1",
    "semantic_title": "improving speech recognition by revising gated recurrent units",
    "citation_count": 49,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chien17c_interspeech.html": {
    "title": "Stochastic Recurrent Neural Network for Speech Recognition",
    "volume": "main",
    "abstract": "This paper presents a new stochastic learning approach to construct a latent variable model for recurrent neural network (RNN) based speech recognition. A hybrid generative and discriminative stochastic network is implemented to build a deep classification model. In the implementation, we conduct stochastic modeling for hidden states of recurrent neural network based on the variational auto-encoder. The randomness of hidden neurons is represented by the Gaussian distribution with mean and variance parameters driven by neural weights and learned from variational inference. Importantly, the class labels of input speech frames are incorporated to regularize this deep model to sample the informative and discriminative features for reconstruction of classification outputs. We accordingly propose the stochastic RNN (SRNN) to reflect the probabilistic property in RNN classification system. A stochastic error backpropagation algorithm is implemented. The experiments on speech recognition using TIMIT and Aurora4 show the merit of the proposed SRNN",
    "checked": true,
    "id": "162efff03a6550a1e98bc81a0885007062f45738",
    "semantic_title": "stochastic recurrent neural network for speech recognition",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ratajczak17_interspeech.html": {
    "title": "Frame and Segment Level Recurrent Neural Networks for Phone Classification",
    "volume": "main",
    "abstract": "We introduce a simple and efficient frame and segment level RNN model (FS-RNN) for phone classification. It processes the input at frame level and segment level by bidirectional gated RNNs. This type of processing is important to exploit the (temporal) information more effectively compared to (i) models which solely process the input at frame level and (ii) models which process the input on segment level using features obtained by heuristic aggregation of frame level features. Furthermore, we incorporated the activations of the last hidden layer of the FS-RNN as an additional feature type in a neural higher-order CRF (NHO-CRF). In experiments, we demonstrated excellent performance on the TIMIT phone classification task, reporting a performance of 13.8% phone error rate for the FS-RNN model and 11.9% when combined with the NHO-CRF. In both cases we significantly exceeded the state-of-the-art performance",
    "checked": true,
    "id": "d652a7edda630654389eb23a0ed3fdeb4e79fca5",
    "semantic_title": "frame and segment level recurrent neural networks for phone classification",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/han17_interspeech.html": {
    "title": "Deep Learning-Based Telephony Speech Recognition in the Wild",
    "volume": "main",
    "abstract": "In this paper, we explore the effectiveness of a variety of Deep Learning-based acoustic models for conversational telephony speech, specifically TDNN, bLSTM and CNN-bLSTM models. We evaluated these models on both research testsets, such as Switchboard and CallHome, as well as recordings from a real-world call-center application. Our best single system, consisting of a single CNN-bLSTM acoustic model, obtained a WER of 5.7% on the Switchboard testset, and in combination with other models a WER of 5.3% was obtained. On the CallHome testset a WER of 10.1% was achieved with model combination. On the test data collected from real-world call-centers, even with model adaptation using application specific data, the WER was significantly higher at 15.0%. We performed an error analysis on the real-world data and highlight the areas where speech recognition still has challenges",
    "checked": true,
    "id": "d0eccd60d800308cd6e59810769b92b40961c09a",
    "semantic_title": "deep learning-based telephony speech recognition in the wild",
    "citation_count": 30,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lee17_interspeech.html": {
    "title": "The I4U Mega Fusion and Collaboration for NIST Speaker Recognition Evaluation 2016",
    "volume": "main",
    "abstract": "The 2016 speaker recognition evaluation (SRE'16) is the latest edition in the series of benchmarking events conducted by the National Institute of Standards and Technology (NIST). I4U is a joint entry to SRE'16 as the result from the collaboration and active exchange of information among researchers from sixteen Institutes and Universities across 4 continents. The joint submission and several of its 32 sub-systems were among top-performing systems. A lot of efforts have been devoted to two major challenges, namely, unlabeled training data and dataset shift from Switchboard-Mixer to the new Call My Net dataset. This paper summarizes the lessons learned, presents our shared view from the sixteen research groups on recent advances, major paradigm shift, and common tool chain used in speaker recognition as we have witnessed in SRE'16. More importantly, we look into the intriguing question of fusing a large ensemble of sub-systems and the potential benefit of large-scale collaboration",
    "checked": true,
    "id": "8bd21e0d011150e59a50fecb2e639f4338229643",
    "semantic_title": "the i4u mega fusion and collaboration for nist speaker recognition evaluation 2016",
    "citation_count": 15,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/torrescarrasquillo17_interspeech.html": {
    "title": "The MIT-LL, JHU and LRDE NIST 2016 Speaker Recognition Evaluation System",
    "volume": "main",
    "abstract": "In this paper, the NIST 2016 SRE system that resulted from the collaboration between MIT Lincoln Laboratory and the team at Johns Hopkins University is presented. The submissions for the 2016 evaluation consisted of three fixed condition submissions and a single system open condition submission. The primary submission on the fixed (and core) condition resulted in an actual DCF of .618. Details of the submissions are discussed along with some discussion and observations of the 2016 evaluation campaign",
    "checked": true,
    "id": "d6c6f46725f538cf5960d3a4a21eea2e9605f3a8",
    "semantic_title": "the mit-ll, jhu and lrde nist 2016 speaker recognition evaluation system",
    "citation_count": 14,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/colibro17_interspeech.html": {
    "title": "Nuance - Politecnico di Torino's 2016 NIST Speaker Recognition Evaluation System",
    "volume": "main",
    "abstract": "This paper describes the Nuance–Politecnico di Torino (NPT) speaker recognition system submitted to the NIST SRE16 evaluation campaign. Included are the results of post-evaluation tests, focusing on the analysis of the performance of generative and discriminative classifiers, and of score normalization. The submitted system combines the results of four GMM-IVector models, two DNN-IVector models and a GMM-SVM acoustic system. Each system exploits acoustic front-end parameters that differ by feature type and dimension. We analyze the main components of our submission, which contributed to obtaining 8.1% EER and 0.532 actual C in the challenging SRE16 Fixed condition",
    "checked": true,
    "id": "9c95c4cbb26555f1d5961fa477b2f9b0aab1cd74",
    "semantic_title": "nuance - politecnico di torino's 2016 nist speaker recognition evaluation system",
    "citation_count": 11,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhang17c_interspeech.html": {
    "title": "UTD-CRSS Systems for 2016 NIST Speaker Recognition Evaluation",
    "volume": "main",
    "abstract": "This study describes systems submitted by the Center for Robust Speech Systems (CRSS) from the University of Texas at Dallas (UTD) to the 2016 National Institute of Standards and Technology (NIST) Speaker Recognition Evaluation (SRE).We developed 4 UBM and DNN i-vector based speaker recognition systems with alternate data sets and feature representations. Given that the emphasis of the NIST SRE 2016 is on language mismatch between training and enrollment/test data, so-called domain mismatch, in our system development we focused on: (i) utilizing unlabeled in-domain data for centralizing i-vectors to alleviate the domain mismatch; (ii) selecting the proper data sets and optimizing configurations for training LDA/PLDA; (iii) introducing a newly proposed dimension reduction technique which incorporates unlabeled in-domain data before PLDA training; (iv) unsupervised speaker clustering of unlabeled data and using them alone or with previous SREs for PLDA training, and finally (v) score calibration using unlabeled data with \"pseudo\" speaker labels generated from speaker clustering. NIST evaluations show that our proposed methods were very successful for the given task",
    "checked": true,
    "id": "1108510e22449d47fd1235815b654c54025b1733",
    "semantic_title": "utd-crss systems for 2016 nist speaker recognition evaluation",
    "citation_count": 39,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/plchot17_interspeech.html": {
    "title": "Analysis and Description of ABC Submission to NIST SRE 2016",
    "volume": "main",
    "abstract": "We present a condensed description and analysis of the joint submission for NIST SRE 2016, by Agnitio, BUT and CRIM (ABC). We concentrate on challenges that arose during development and we analyze the results obtained on the evaluation data and on our development sets. We show that testing on mismatched, non-English and short duration data introduced in NIST SRE 2016 is a difficult problem for current state-of-the-art systems. Testing on this data brought back the issue of score normalization and it also revealed that the bottleneck features (BN), which are superior when used for telephone English, are lacking in performance against the standard acoustic features like Mel Frequency Cepstral Coefficients (MFCCs). We offer ABC's insights, findings and suggestions for building a robust system suitable for mismatched, non-English and relatively noisy data such as those in NIST SRE 2016",
    "checked": true,
    "id": "add8fa15a22edc86e2eb7edee350357f6adc350c",
    "semantic_title": "analysis and description of abc submission to nist sre 2016",
    "citation_count": 17,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sadjadi17_interspeech.html": {
    "title": "The 2016 NIST Speaker Recognition Evaluation",
    "volume": "main",
    "abstract": "In 2016, the National Institute of Standards and Technology (NIST) conducted the most recent in an ongoing series of speaker recognition evaluations (SRE) to foster research in robust text-independent speaker recognition, as well as measure performance of current state-of-the-art systems. Compared to previous NIST SREs, SRE16 introduced several new aspects including: an entirely online evaluation platform, a fixed training data condition, more variability in test segment duration (uniformly distributed between 10s and 60s), the use of non-English (Cantonese, Cebuano, Mandarin and Tagalog) conversational telephone speech (CTS) collected outside North America, and providing labeled and unlabeled development (a.k.a. validation) sets for system hyperparameter tuning and adaptation. The introduction of the new non-English CTS data made SRE16 more challenging due to domain/channel and language mismatches as compared to previous SREs. A total of 66 research organizations from industry and academia registered for SRE16, out of which 43 teams submitted 121 valid system outputs that produced scores. This paper presents an overview of the evaluation and analysis of system performance over all primary evaluation conditions. Initial results indicate that effective use of the development data was essential for the top performing systems, and that domain/channel, language, and duration mismatch had an adverse impact on system performance",
    "checked": true,
    "id": "1108510e22449d47fd1235815b654c54025b1733",
    "semantic_title": "utd-crss systems for 2016 nist speaker recognition evaluation",
    "citation_count": 39,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kawahara17b_interspeech.html": {
    "title": "A New Cosine Series Antialiasing Function and its Application to Aliasing-Free Glottal Source Models for Speech and Singing Synthesis",
    "volume": "main",
    "abstract": "We formulated and implemented a procedure to generate aliasing-free excitation source signals. It uses a new antialiasing filter in the continuous time domain followed by an IIR digital filter for response equalization. We introduced a cosine-series-based general design procedure for the new antialiasing function. We applied this new procedure to implement the antialiased Fujisaki-Ljungqvist model. We also applied it to revise our previous implementation of the antialiased Fant-Liljencrants model. A combination of these signals and a lattice implementation of the time varying vocal tract model provides a reliable and flexible basis to test f extractors and source aperiodicity analysis methods. MATLAB implementations of these antialiased excitation source models are available as part of our open source tools for speech science",
    "checked": true,
    "id": "82ac08209ef31bd25029e3007ff10696b490a009",
    "semantic_title": "a new cosine series antialiasing function and its application to aliasing-free glottal source models for speech and singing synthesis",
    "citation_count": 28,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lopez17_interspeech.html": {
    "title": "Speaking Style Conversion from Normal to Lombard Speech Using a Glottal Vocoder and Bayesian GMMs",
    "volume": "main",
    "abstract": "Speaking style conversion is the technology of converting natural speech signals from one style to another. In this study, we focus on normal-to-Lombard conversion. This can be used, for example, to enhance the intelligibility of speech in noisy environments. We propose a parametric approach that uses a vocoder to extract speech features. These features are mapped using Bayesian GMMs from utterances spoken in normal style to the corresponding features of Lombard speech. Finally, the mapped features are converted to a Lombard speech waveform with the vocoder. Two vocoders were compared in the proposed normal-to-Lombard conversion: a recently developed glottal vocoder that decomposes speech into glottal flow excitation and vocal tract, and the widely used STRAIGHT vocoder. The conversion quality was evaluated in two subjective listening tests measuring subjective similarity and naturalness. The similarity test results show that the system is able to convert normal speech into Lombard speech for the two vocoders. However, the subjective naturalness of the converted Lombard speech was clearly better using the glottal vocoder in comparison to STRAIGHT",
    "checked": true,
    "id": "4cecfeb43a2a6a0b325542387fa4a655f2cda783",
    "semantic_title": "speaking style conversion from normal to lombard speech using a glottal vocoder and bayesian gmms",
    "citation_count": 17,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/juvela17_interspeech.html": {
    "title": "Reducing Mismatch in Training of DNN-Based Glottal Excitation Models in a Statistical Parametric Text-to-Speech System",
    "volume": "main",
    "abstract": "Neural network-based models that generate glottal excitation waveforms from acoustic features have been found to give improved quality in statistical parametric speech synthesis. Until now, however, these models have been trained separately from the acoustic model. This creates mismatch between training and synthesis, as the synthesized acoustic features used for the excitation model input differ from the original inputs, with which the model was trained on. Furthermore, due to the errors in predicting the vocal tract filter, the original excitation waveforms do not provide perfect reconstruction of the speech waveform even if predicted without error. To address these issues and to make the excitation model more robust against errors in acoustic modeling, this paper proposes two modifications to the excitation model training scheme. First, the excitation model is trained in a connected manner, with inputs generated by the acoustic model. Second, the target glottal waveforms are re-estimated by performing glottal inverse filtering with the predicted vocal tract filters. The results show that both of these modifications improve performance measured in MSE and MFCC distortion, and slightly improve the subjective quality of the synthetic speech",
    "checked": true,
    "id": "1eb145cb66a84f5bc36ba06ba262e2b973b16a42",
    "semantic_title": "reducing mismatch in training of dnn-based glottal excitation models in a statistical parametric text-to-speech system",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sorin17_interspeech.html": {
    "title": "Semi Parametric Concatenative TTS with Instant Voice Modification Capabilities",
    "volume": "main",
    "abstract": "Recently, a glottal vocoder has been integrated in the IBM concatenative TTS system and certain configurable global voice transformations were defined in the vocoder parameter space. The vocoder analysis employs a novel robust glottal source parameter estimation strategy. The vocoder is applied to the voiced speech only, while unvoiced speech is kept unparameterized, thus contributing to the perceived naturalness of the synthesized speech The semi-parametric system enables independent modifications of the glottal source and vocal tract components on-the-fly by embedding the voice transformations in the synthesis process. The transformations effect ranges from slight voice altering to a complete change of the perceived speaker personality. Pitch modifications enhance these changes. At the same time, the voice transformations are simple enough to be easily controlled externally to the system. This allows the users either to fine tune the voice sound or to create instantly multiple distinct virtual voices. In both cases, the synthesis is based on a large and meticulously cleaned concatenative TTS voice with a broad phonetic coverage. In this paper we present the system and provide subjective evaluations of its voice modification capabilities The technology presented in this paper is implemented in IBM Watson TTS service",
    "checked": true,
    "id": "7b370a25a7b71501dff3fcfd80084b24262199d5",
    "semantic_title": "semi parametric concatenative tts with instant voice modification capabilities",
    "citation_count": 16,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/manriquez17_interspeech.html": {
    "title": "Modeling Laryngeal Muscle Activation Noise for Low-Order Physiological Based Speech Synthesis",
    "volume": "main",
    "abstract": "Physiological-based synthesis using low order lumped-mass models of phonation have been shown to mimic and predict complex physical phenomena observed in normal and pathological speech production, and have received significant attention due to their ability to efficiently perform comprehensive parametric investigations that are cost prohibitive with more advanced computational tools. Even though these numerical models have been shown to be useful research and clinical tools, several physiological aspects of them remain to be explored. One of the key components that has been neglected is the natural fluctuation of the laryngeal muscle activity that affects the configuration of the model parameters. In this study, a physiologically-based laryngeal muscle activation model that accounts for random fluctuations is proposed. The method is expected to improve the ability to model muscle related pathologies, such as muscle tension dysphonia and Parkinson's disease. The mathematical framework and underlying assumptions are described, and the effects of the added random muscle activity is tested in a well-known body-cover model of the vocal folds with acoustic propagation and interaction. Initial simulations illustrate that the random fluctuations in the muscle activity impact the resulting kinematics to varying degrees depending on the laryngeal configuration",
    "checked": true,
    "id": "0071037f870a5a85028d4eec224e50eed6df878e",
    "semantic_title": "modeling laryngeal muscle activation noise for low-order physiological based speech synthesis",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/espic17_interspeech.html": {
    "title": "Direct Modelling of Magnitude and Phase Spectra for Statistical Parametric Speech Synthesis",
    "volume": "main",
    "abstract": "We propose a simple new representation for the FFT spectrum tailored to statistical parametric speech synthesis. It consists of four feature streams that describe magnitude, phase and fundamental frequency using real numbers. The proposed feature extraction method does not attempt to decompose the speech structure (e.g., into source+filter or harmonics+noise). By avoiding the simplifications inherent in decomposition, we can dramatically reduce the \"phasiness\" and \"buzziness\" typical of most vocoders. The method uses simple and computationally cheap operations and can operate at a lower frame rate than the 200 frames-per-second typical in many systems. It avoids heuristics and methods requiring approximate or iterative solutions, including phase unwrapping Two DNN-based acoustic models were built — from male and female speech data — using the Merlin toolkit. Subjective comparisons were made with a state-of-the-art baseline, using the STRAIGHT vocoder. In all variants tested, and for both male and female voices, the proposed method substantially outperformed the baseline. We provide source code to enable our complete system to be replicated",
    "checked": true,
    "id": "86912603ea04354a9c23bea1a7a652c3cac7c996",
    "semantic_title": "direct modelling of magnitude and phase spectra for statistical parametric speech synthesis",
    "citation_count": 28,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kember17_interspeech.html": {
    "title": "Similar Prosodic Structure Perceived Differently in German and English",
    "volume": "main",
    "abstract": "English and German have similar prosody, but their speakers realize some pitch falls (not rises) in subtly different ways. We here test for asymmetry in perception. An ABX discrimination task requiring F0 slope or duration judgements on isolated vowels revealed no cross-language difference in duration or F0 fall discrimination, but discrimination of rises (realized similarly in each language) was less accurate for English than for German listeners. This unexpected finding may reflect greater sensitivity to rising patterns by German listeners, or reduced sensitivity by English listeners as a result of extensive exposure to phrase-final rises (\"uptalk\") in their language",
    "checked": true,
    "id": "179d7f4b91153b524fb36736098f506c5ed729fa",
    "semantic_title": "similar prosodic structure perceived differently in german and english",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hou17_interspeech.html": {
    "title": "Disambiguate or not? — The Role of Prosody in Unambiguous and Potentially Ambiguous Anaphora Production in Strictly Mandarin Parallel Structures",
    "volume": "main",
    "abstract": "It has been observed that the interpretation of pronouns can depend on their accentuation patterns in parallel sentences as \"John hit Bill and then George hit him\", in which ‘him' refers to Bill when unaccented but shifts to John when accented. While accentuation is widely regarded as a means of disambiguation, some studies have noticed that it also extends to unambiguous anaphors [7–10]. From the perspective of production, however, no strong experimental confirmation was found for the ‘shift' function of accented pronouns, which is due to the fact that production research has mainly focused on corpora [5, 6]. Hence, the nature of the accent on anaphors still remains obscure. By manipulating referential shift and ambiguity, this study explores the role of prosody in anaphora production in strictly Mandarin parallel structures. The results reveal a significantly higher F and longer duration for anaphors in referentially shifted conditions, suggesting that anaphoric accentuation signals a referential change in strictly parallel structures in Mandarin. No evidence was found that ambiguity plays a role in anaphoric accentuation. This finding challenges the general view on accented pronouns and will deepen our understanding on semantics-prosody relationship",
    "checked": true,
    "id": "61fecd3ba9d4d776d997ef6b08e167642198dba5",
    "semantic_title": "disambiguate or not? - the role of prosody in unambiguous and potentially ambiguous anaphora production in strictly mandarin parallel structures",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/athanasopoulou17_interspeech.html": {
    "title": "Acoustic Properties of Canonical and Non-Canonical Stress in French, Turkish, Armenian and Brazilian Portuguese",
    "volume": "main",
    "abstract": "Languages are often categorized as having either predictable (fixed or quantity-sensitive) or non-predictable stress. Despite their name, fixed stress languages may have exceptions, so in fact, their stress does not always appear in the same position. Since predictability has been shown to affect certain speech phenomena, with additional or redundant acoustic cues being provided when the linguistic content is less predictable (e.g., Smooth Signal Redundancy Hypothesis), we investigate whether, and to what extent, the predictability of stress position affects the manifestation of stress in different languages. We examine the acoustic properties of stress in three languages classified as having fixed stress (Turkish, French, Armenian), with exceptions, and in one language with non-predictable-stress, Brazilian Portuguese. Specifically, we compare the manifestation of stress in the canonical stress (typically \"fixed\") position with its manifestation in the non-canonical (exceptional) position, where it would potentially be less predictable. We also compare these patterns with the manifestation of stress in Portuguese, in both the \"default\" penultimate and the less common final position. Our results show that stress is manifested quite similarly in canonical and non-canonical positions in the \"fixed\" stress languages and stress is most clearly produced when it is least predictable",
    "checked": true,
    "id": "e08a1c15b5d59e5fd72093ec094f582f80731bd5",
    "semantic_title": "acoustic properties of canonical and non-canonical stress in french, turkish, armenian and brazilian portuguese",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/plug17_interspeech.html": {
    "title": "Phonological Complexity, Segment Rate and Speech Tempo Perception",
    "volume": "main",
    "abstract": "Studies of speech tempo commonly use syllable or segment rate as a proxy measure for perceived tempo. In languages whose phonologies allow substantial syllable complexity these measures can produce figures on quite different scales; however, little is known about the correlation between syllable and segment rate measurements on the one hand and naïve listeners' tempo judgements on the other We follow up on the findings of one relevant study on German [1], which suggest that listeners attend to both syllable and segment rates in making tempo estimates, through a weighted average of the rates in which syllable rate carries more weight. We report on an experiment in which we manipulate phonological complexity in English utterance pairs that are constant in syllable rate. Listeners decide for each pair which utterance sounds faster. Our results suggest that differences in segment rate that do not correspond to differences in syllable rate have little impact on perceived speech tempo in English",
    "checked": true,
    "id": "ba73fc540baaf55498187c388fc8ac2adbfdfdea",
    "semantic_title": "phonological complexity, segment rate and speech tempo perception",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/yang17_interspeech.html": {
    "title": "On the Duration of Mandarin Tones",
    "volume": "main",
    "abstract": "The present study compared the duration of Mandarin tones in three types of speech contexts: isolated monosyllables, formal text-reading passages, and casual conversations. A total of 156 adult speakers was recruited. The speech materials included 44 monosyllables recorded from each of 121 participants, 18 passages read by 2 participants, and 20 conversations conducted by 33 participants. The duration pattern of the four lexical tones in the isolated monosyllables was consistent with the pattern described in previous literature. However, the duration of the four lexical tones became much shorter and tended to converge to that of the neutral tone (i.e., tone 0) in the text-reading and conversational speech. The maximum-likelihood estimator revealed that the durational cue contributed to tone recognition in the isolated monosyllables. With a single speaker, the average tone recognition based on duration alone could reach approximately 65% correct. As the number of speakers increased (e.g., ≥ 4), tone recognition performance dropped to approximately 45% correct. In conversational speech, the maximum likelihood estimation of tones based on duration cues was only 23% correct. The tone duration provided little useful cue to differentiate Mandarin tonal identity in everyday situations",
    "checked": true,
    "id": "5bb0dd554dd188ef10ce6889d1466ac3ff954f73",
    "semantic_title": "on the duration of mandarin tones",
    "citation_count": 23,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ewald17_interspeech.html": {
    "title": "The Formant Dynamics of Long Close Vowels in Three Varieties of Swedish",
    "volume": "main",
    "abstract": "This study compares the acoustic realisation of /iː yː ʉː uː/ in three varieties of Swedish: Central Swedish, Estonian Swedish, and Finland Swedish. Vowel tokens were extracted from isolated words produced by six elderly female speakers from each variety. Trajectories of the first three formants were modelled with discrete cosine transform (DCT) coefficients, enabling the comparison of the formant means as well as the direction and magnitude of the formant movement. Cross-dialectal differences were found in all measures and in all vowels. The most noteworthy feature of the Estonian Swedish long close vowel inventory is the lack of /yː/. For Finland Swedish it was shown that /iː/ and /yː/ are more close than in Central Swedish. The realisation of /ʉː/ varies from front in Central Swedish, to central in Estonian Swedish, and back in Finland Swedish. On average, the Central Swedish vowels exhibited a higher degree of formant movement than the vowels in the other two varieties. In the present study, regional variation in Swedish vowels was for the first time investigated using DCT coefficients. The results stress the importance of taking formant dynamics into account even in the analysis of nominal monophthongs",
    "checked": true,
    "id": "f3caa781ff7fdd2fcd317348cfb47f9c57ec317c",
    "semantic_title": "the formant dynamics of long close vowels in three varieties of swedish",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/qian17_interspeech.html": {
    "title": "Bidirectional LSTM-RNN for Improving Automated Assessment of Non-Native Children's Speech",
    "volume": "main",
    "abstract": "Recent advances in ASR and spoken language processing have led to improved systems for automated assessment for spoken language. However, it is still challenging for automated scoring systems to achieve high performance in terms of the agreement with human experts when applied to non-native children's spontaneous speech. The subpar performance is mainly caused by the relatively low recognition rate on non-native children's speech. In this paper, we investigate different neural network architectures for improving non-native children's speech recognition and the impact of the features extracted from the corresponding ASR output on the automated assessment of speaking proficiency. Experimental results show that bidirectional LSTM-RNN can outperform feed-forward DNN in ASR, with an overall relative WER reduction of 13.4%. The improved speech recognition can then boost the language proficiency assessment performance. Correlations between the rounded automated scores and expert scores range from 0.66 to 0.70 for the three speaking tasks studied, similar to the human-human agreement levels for these tasks",
    "checked": true,
    "id": "c6f72739a51e0fccd6a08aeec667b948f57816ba",
    "semantic_title": "bidirectional lstm-rnn for improving automated assessment of non-native children's speech",
    "citation_count": 18,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/yue17_interspeech.html": {
    "title": "Automatic Scoring of Shadowing Speech Based on DNN Posteriors and Their DTW",
    "volume": "main",
    "abstract": "Shadowing has become a well-known method to improve learners' overall proficiency. Our previous studies realized automatic scoring of shadowing speech using HMM phoneme posteriors, called GOP (Goodness of Pronunciation) and learners' TOEIC scores were predicted adequately. In this study, we enhance our studies from multiple angles: 1) a much larger amount of shadowing speech is collected, 2) manual scoring of these utterances is done by two native teachers, 3) DNN posteriors are introduced instead of HMM ones, 4) language-independent shadowing assessment based on posteriors-based DTW (Dynamic Time Warping) is examined. Experiments suggest that, compared to HMM, DNN can improve teacher-machine correlation largely by 0.37 and DTW based on DNN posteriors shows as high correlation as 0.74 even when posterior calculation is done using a different language from the target language of learning",
    "checked": true,
    "id": "a56ad57da1e5a6892cdc60ae754e2acc63b8b675",
    "semantic_title": "automatic scoring of shadowing speech based on dnn posteriors and their dtw",
    "citation_count": 18,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lee17b_interspeech.html": {
    "title": "Off-Topic Spoken Response Detection Using Siamese Convolutional Neural Networks",
    "volume": "main",
    "abstract": "In this study, we developed an off-topic response detection system to be used in the context of the automated scoring of non-native English speakers' spontaneous speech. Based on transcriptions generated from an ASR system trained on non-native speakers' speech and various semantic similarity features, the system classified each test response as an on-topic or off-topic response. The recent success of deep neural networks (DNN) in text similarity detection led us to explore DNN-based document similarity features. Specifically, we used a siamese adaptation of the convolutional network, due to its efficiency in learning similarity patterns simultaneously from both responses and questions used to elicit responses. In addition, a baseline system was developed using a standard vector space model (VSM) trained on sample responses for each question. The accuracy of the siamese CNN-based system was 0.97 and there was a 50% relative error reduction compared to the standard VSM-based system. Furthermore, the accuracy of the siamese CNN-based system was consistent across different questions",
    "checked": true,
    "id": "95c07805e04cef09e4cb373d906d0eaadb1afd31",
    "semantic_title": "off-topic spoken response detection using siamese convolutional neural networks",
    "citation_count": 14,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/arora17_interspeech.html": {
    "title": "Phonological Feature Based Mispronunciation Detection and Diagnosis Using Multi-Task DNNs and Active Learning",
    "volume": "main",
    "abstract": "This paper presents a phonological feature based computer aided pronunciation training system for the learners of a new language (L2). Phonological features allow analysing the learners' mispronunciations systematically and rendering the feedback more effectively. The proposed acoustic model consists of a multi-task deep neural network, which uses a shared representation for estimating the phonological features and HMM state probabilities. Moreover, an active learning based scheme is proposed to efficiently deal with the cost of annotation, which is done by expert teachers, by selecting the most informative samples for annotation. Experimental evaluations are carried out for German and Italian native-speakers speaking English. For mispronunciation detection, the proposed feature-based system outperforms conventional GOP measure and classifier based methods, while providing more detailed diagnosis. Evaluations also demonstrate the advantage of active learning based sampling over random sampling",
    "checked": true,
    "id": "ad1df1a960176f429092b34bccea5d99fa556dc4",
    "semantic_title": "phonological feature based mispronunciation detection and diagnosis using multi-task dnns and active learning",
    "citation_count": 12,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/proenca17_interspeech.html": {
    "title": "Detection of Mispronunciations and Disfluencies in Children Reading Aloud",
    "volume": "main",
    "abstract": "To automatically evaluate the performance of children reading aloud or to follow a child's reading in reading tutor applications, different types of reading disfluencies and mispronunciations must be accounted for. In this work, we aim to detect most of these disfluencies in sentence and pseudoword reading. Detecting incorrectly pronounced words, and quantifying the quality of word pronunciations, is arguably the hardest task. We approach the challenge as a two-step process. First, a segmentation using task-specific lattices is performed, while detecting repetitions and false starts and providing candidate segments for words. Then, candidates are classified as mispronounced or not, using multiple features derived from likelihood ratios based on phone decoding and forced alignment, as well as additional meta-information about the word. Several classifiers were explored (linear fit, neural networks, support vector machines) and trained after a feature selection stage to avoid overfitting. Improved results are obtained using feature combination compared to using only the log likelihood ratio of the reference word (22% versus 27% miss rate at constant 5% false alarm rate)",
    "checked": true,
    "id": "db39304c600d9d9f0f4b5dc5a3adfe2137adbc46",
    "semantic_title": "detection of mispronunciations and disfluencies in children reading aloud",
    "citation_count": 11,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/escuderomancebo17_interspeech.html": {
    "title": "Automatic Assessment of Non-Native Prosody by Measuring Distances on Prosodic Label Sequences",
    "volume": "main",
    "abstract": "The aim of this paper is to investigate how automatic prosodic labeling systems contribute to the evaluation of non-native pronunciation. In particular, it examines the efficiency of a group of metrics to evaluate the prosodic competence of non-native speakers, based on the information provided by sequences of labels in the analysis of both native and non-native speech. A group of Sp_ToBI labels were obtained by means of an automatic labeling system for the speech of native and non-native speakers who read the same texts. The metrics assessed the differences in the prosodic labels for both speech samples. The results showed the efficiency of the metrics to set apart both groups of speakers. Furthermore, they exhibited how non-native speakers (American and Japanese speakers) improved their Spanish productions after doing a set of listening and repeating activities. Finally, this study also shows that the results provided by the metrics are correlated with the scores given by human evaluators on the productions of the different speakers",
    "checked": true,
    "id": "10a78a106182d8396dd61a32e6999361049c5e2c",
    "semantic_title": "automatic assessment of non-native prosody by measuring distances on prosodic label sequences",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ward17_interspeech.html": {
    "title": "Inferring Stance from Prosody",
    "volume": "main",
    "abstract": "Speech conveys many things beyond content, including aspects of stance and attitude that have not been much studied. Considering 14 aspects of stance as they occur in radio news stories, we investigated the extent to which they could be inferred from prosody. By using time-spread prosodic features and by aggregating local estimates, many aspects of stance were at least somewhat predictable, with results significantly better than chance for many stance aspects, including, across English, Mandarin and Turkish, good, typical, local, background, new information, and relevant to a large group",
    "checked": true,
    "id": "bb95d34ded54579a2d937c1b593b319b90c2baf1",
    "semantic_title": "inferring stance from prosody",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/levow17_interspeech.html": {
    "title": "Exploring Dynamic Measures of Stance in Spoken Interaction",
    "volume": "main",
    "abstract": "Stance-taking, the expression of opinions or attitudes, informs the process of negotiation, argumentation, and decision-making. While receiving significant attention in text materials in work on the related areas of subjectivity and sentiment analysis, the expression of stance in speech remains less explored. Prior analysis of the acoustics of stance-expression in conversational speech has identified some significant differences across dimensions of stance-related behavior. However, that analysis, as in much prior work, relied on simple functionals of pitch, energy, and duration, including maxima, minima, means, and ranges. In contrast, the current work focuses on exploiting measures that capture the dynamics of the pitch and energy contour. We employ features based on subband autocorrelation measures of pitch change and variants of the modulation spectrum. Using a corpus of conversational speech manually annotated for dimensions of stance-taking, we demonstrate that these measures of pitch and energy dynamics can help to characterize and distinguish among stance-related behaviors in speech",
    "checked": true,
    "id": "c9562ee14b07f3923def8d08f3ddf0c82f0cb2ae",
    "semantic_title": "exploring dynamic measures of stance in spoken interaction",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/barriere17_interspeech.html": {
    "title": "Opinion Dynamics Modeling for Movie Review Transcripts Classification with Hidden Conditional Random Fields",
    "volume": "main",
    "abstract": "In this paper, the main goal is to detect a movie reviewer's opinion using hidden conditional random fields. This model allows us to capture the dynamics of the reviewer's opinion in the transcripts of long unsegmented audio reviews that are analyzed by our system. High level linguistic features are computed at the level of inter-pausal segments. The features include syntactic features, a statistical word embedding model and subjectivity lexicons. The proposed system is evaluated on the ICT-MMMO corpus. We obtain a F1-score of 82%, which is better than logistic regression and recurrent neural network approaches. We also offer a discussion that sheds some light on the capacity of our system to adapt the word embedding model learned from general written texts data to spoken movie reviews and thus model the dynamics of the opinion",
    "checked": true,
    "id": "8caef345759113bbd3f70db6a9d9d98d86d63164",
    "semantic_title": "opinion dynamics modeling for movie review transcripts classification with hidden conditional random fields",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/luo17_interspeech.html": {
    "title": "Transfer Learning Between Concepts for Human Behavior Modeling: An Application to Sincerity and Deception Prediction",
    "volume": "main",
    "abstract": "Transfer learning (TL) involves leveraging information from sources outside the domain at hand for enhancing model performances. Popular TL methods either directly use the data or adapt the models learned on out-of-domain resources and incorporate them within in-domain models. TL methods have shown promise in several applications such as text classification, cross-domain language classification and emotion recognition. In this paper, we propose TL methods to computational human behavioral trait modeling. Many behavioral traits are abstract constructs (e.g., sincerity of an individual), and are often conceptually related to other constructs (e.g., level of deception) making TL methods an attractive option for their modeling. We consider the problem of automatically predicting human sincerity and deception from behavioral data while leveraging transfer of knowledge from each other. We compare our methods against baseline models trained only on in-domain data. Our best models achieve an Unweighted Average Recall (UAR) of 72.02% in classifying deception (baseline: 69.64%). Similarly, applied methods achieve Spearman's/Pearson's correlation values of 49.37%/48.52% between true and predicted sincerity scores (baseline: 46.51%/41.58%), indicating the success and the potential of TL for such human behavior tasks",
    "checked": true,
    "id": "8ec3bf06bc143d48767667e9e83ba7d45f1155c2",
    "semantic_title": "transfer learning between concepts for human behavior modeling: an application to sincerity and deception prediction",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/schroder17_interspeech.html": {
    "title": "The Sound of Deception — What Makes a Speaker Credible?",
    "volume": "main",
    "abstract": "The detection of deception in human speech is a difficult task but can be performed above chance level by human listeners even when only audio data is provided. Still, it is highly contested, which speech features could be used to help identify lies. In this study, we examined a set of phonetic and paralinguistic cues and their influence on the credibility of speech using an analysis-by-synthesis approach. 33 linguistically neutral utterances with different manipulated cues (unfilled pauses, phonation type, higher speech rate, tremolo and raised F0) were synthesized using articulatory synthesis. These utterances were presented to 50 subjects who were asked to choose the more credible utterance. From those choices, a credibility score was calculated for each cue. The results show a significant increase in credibility when a tremolo is inserted or the breathiness is increased, and a decrease in credibility when a pause is inserted or the F0 is raised. Other cues also had a significant, but less pronounced influence on the credibility while some only showed trends. In summary, the study showed that the credibility of a factually unverifiable utterance is in parts controlled by the presented paralinguistic cues",
    "checked": true,
    "id": "3dd5c272b2bcadfc06608c2fcd5ae1e0040eb63c",
    "semantic_title": "the sound of deception - what makes a speaker credible?",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mendels17_interspeech.html": {
    "title": "Hybrid Acoustic-Lexical Deep Learning Approach for Deception Detection",
    "volume": "main",
    "abstract": "Automatic deception detection is an important problem with far-reaching implications for many disciplines. We present a series of experiments aimed at automatically detecting deception from speech. We use the Columbia X-Cultural Deception (CXD) Corpus, a large-scale corpus of within-subject deceptive and non-deceptive speech, for training and evaluating our models. We compare the use of spectral, acoustic-prosodic, and lexical feature sets, using different machine learning models. Finally, we design a single hybrid deep model with both acoustic and lexical features trained jointly that achieves state-of-the-art results on the CXD corpus",
    "checked": true,
    "id": "fda72ce8b95866dd924326b09fca35a4a68ecab7",
    "semantic_title": "hybrid acoustic-lexical deep learning approach for deception detection",
    "citation_count": 51,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/swart17_interspeech.html": {
    "title": "A Generative Model for Score Normalization in Speaker Recognition",
    "volume": "main",
    "abstract": "We propose a theoretical framework for thinking about score normalization, which confirms that normalization is not needed under (admittedly fragile) ideal conditions. If, however, these conditions are not met, e.g. under data-set shift between training and runtime, our theory reveals dependencies between scores that could be exploited by strategies such as score normalization. Indeed, it has been demonstrated over and over experimentally, that various ad-hoc score normalization recipes do work. We present a first attempt at using probability theory to design a generative score-space normalization model which gives similar improvements to ZT-norm on the text-dependent RSR 2015 database",
    "checked": true,
    "id": "2d1094ae908b23b7b92d286f11e77dd0bc40fb85",
    "semantic_title": "a generative model for score normalization in speaker recognition",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/dey17_interspeech.html": {
    "title": "Content Normalization for Text-Dependent Speaker Verification",
    "volume": "main",
    "abstract": "Subspace based techniques, such as i-vector and Joint Factor Analysis (JFA) have shown to provide state-of-the-art performance for fixed phrase based text-dependent speaker verification. However, the error rates of such systems on the random digit task of RSR dataset are higher than that of Gaussian Mixture Model-Universal Background Model (GMM-UBM). In this paper, we aim at improving i-vector system by normalizing the content of the enrollment data to match the test data. We estimate i-vectors for each frames of a speech utterance (also called online i-vectors). The largest similarity scores across frames between enrollment and test are taken using these online i-vectors to obtain speaker verification scores. Experiments on Part3 of RSR corpora show that the proposed approach achieves 12% relative improvement in equal error rate over a GMM-UBM based baseline system",
    "checked": true,
    "id": "556517c3cb5671d551dd4eb19aec4a0e6dff17c4",
    "semantic_title": "content normalization for text-dependent speaker verification",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhang17d_interspeech.html": {
    "title": "End-to-End Text-Independent Speaker Verification with Triplet Loss on Short Utterances",
    "volume": "main",
    "abstract": "Text-independent speaker verification against short utterances is still challenging despite of recent advances in the field of speaker recognition with i-vector framework. In general, to get a robust i-vector representation, a satisfying amount of data is needed in the MAP adaptation step, which is hard to meet under short duration constraint. To overcome this, we present an end-to-end system which directly learns a mapping from speech features to a compact fixed length speaker discriminative embedding where the Euclidean distance is employed for measuring similarity within trials. To learn the feature mapping, a modified Inception Net with residual block is proposed to optimize the triplet loss function. The input of our end-to-end system is a fixed length spectrogram converted from an arbitrary length utterance. Experiments show that our system consistently outperforms a conventional i-vector system on short duration speaker verification tasks. To test the limit under various duration conditions, we also demonstrate how our end-to-end system behaves with different duration from 2s–4s",
    "checked": true,
    "id": "ef8cebdb9e05c4f12e5019531897ad5f2e29afaa",
    "semantic_title": "end-to-end text-independent speaker verification with triplet loss on short utterances",
    "citation_count": 221,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/yu17_interspeech.html": {
    "title": "Adversarial Network Bottleneck Features for Noise Robust Speaker Verification",
    "volume": "main",
    "abstract": "In this paper, we propose a noise robust bottleneck feature representation which is generated by an adversarial network (AN). The AN includes two cascade connected networks, an encoding network (EN) and a discriminative network (DN). Mel-frequency cepstral coefficients (MFCCs) of clean and noisy speech are used as input to the EN and the output of the EN is used as the noise robust feature. The EN and DN are trained in turn, namely, when training the DN, noise types are selected as the training labels and when training the EN, all labels are set as the same, i.e., the clean speech label, which aims to make the AN features invariant to noise and thus achieve noise robustness. We evaluate the performance of the proposed feature on a Gaussian Mixture Model-Universal Background Model based speaker verification system, and make comparison to MFCC features of speech enhanced by short-time spectral amplitude minimum mean square error (STSA-MMSE) and deep neural network-based speech enhancement (DNN-SE) methods. Experimental results on the RSR2015 database show that the proposed AN bottleneck feature (AN-BN) dramatically outperforms the STSA-MMSE and DNN-SE based MFCCs for different noise types and signal-to-noise ratios. Furthermore, the AN-BN feature is able to improve the speaker verification performance under the clean condition",
    "checked": true,
    "id": "7f70160a11d62b75009327664dd4532aad8faa7d",
    "semantic_title": "adversarial network bottleneck features for noise robust speaker verification",
    "citation_count": 30,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wang17g_interspeech.html": {
    "title": "What Does the Speaker Embedding Encode?",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "7717d8196c20659707cec12eb857acf1b06525f3",
    "semantic_title": "what does the speaker embedding encode?",
    "citation_count": 53,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ma17d_interspeech.html": {
    "title": "Incorporating Local Acoustic Variability Information into Short Duration Speaker Verification",
    "volume": "main",
    "abstract": "State-of-the-art speaker verification systems are based on the total variability model to compactly represent the acoustic space. However, short duration utterances only contain limited phonetic content, potentially resulting in an incomplete representation being captured by the total variability model thus leading to poor speaker verification performance. In this paper, a technique to incorporate component-wise local acoustic variability information into the speaker verification framework is proposed. Specifically, Gaussian Probabilistic Linear Discriminant Analysis (G-PLDA) of the supervector space, with a block diagonal covariance assumption, is used in conjunction with the traditional total variability model. Experimental results obtained using the NIST SRE 2010 dataset show that the incorporation of the proposed method leads to relative improvements of 20.48% and 18.99% in the 3 second condition for male and female speech respectively",
    "checked": true,
    "id": "598ac69849784409fe2197be1e9b9cb3b807c317",
    "semantic_title": "incorporating local acoustic variability information into short duration speaker verification",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhong17_interspeech.html": {
    "title": "DNN i-Vector Speaker Verification with Short, Text-Constrained Test Utterances",
    "volume": "main",
    "abstract": "We investigate how to improve the performance of DNN i-vector based speaker verification for short, text-constrained test utterances, e.g. connected digit strings. A text-constrained verification, due to its smaller, limited vocabulary, can deliver better performance than a text-independent one for a short utterance. We study the problem with \"phonetically aware\" Deep Neural Net (DNN) in its capability on \"stochastic phonetic-alignment\" in constructing supervectors and estimating the corresponding i-vectors with two speech databases: a large vocabulary, conversational, speaker independent database (Fisher) and a small vocabulary, continuous digit database (RSR2015 Part III). The phonetic alignment efficiency and resultant speaker verification performance are compared with differently sized senone sets which can characterize the phonetic pronunciations of utterances in the two databases. Performance on RSR2015 Part III evaluation shows a relative improvement of EER, i.e., 7.89% for male speakers and 3.54% for female speakers with only digit related senones. The DNN bottleneck features were also studied to investigate their capability of extracting phonetic sensitive information which is useful for text-independent or text-constrained speaker verifications. We found that by tandeming MFCC with bottleneck features, EERs can be further reduced",
    "checked": true,
    "id": "4e926383ad15245d4c60323c89daf5c4e35d9d79",
    "semantic_title": "dnn i-vector speaker verification with short, text-constrained test utterances",
    "citation_count": 25,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/vestman17_interspeech.html": {
    "title": "Time-Varying Autoregressions for Speaker Verification in Reverberant Conditions",
    "volume": "main",
    "abstract": "In poor room acoustics conditions, speech signals received by a microphone might become corrupted by the signals' delayed versions that are reflected from the room surfaces (e.g. wall, floor). This phenomenon, reverberation, drops the accuracy of automatic speaker verification systems by causing mismatch between the training and testing. Since reverberation causes temporal smearing to the signal, one way to tackle its effects is to study robust feature extraction, particularly based on long-time temporal feature extraction. This approach has been adopted previously in the form of 2-dimensional autoregressive (2DAR) feature extraction scheme by using frequency domain linear prediction (FDLP). In 2DAR, FDLP processing is followed by time domain linear prediction (TDLP). In the current study, we propose modifying the latter part of the 2DAR feature extraction scheme by replacing TDLP with time-varying linear prediction (TVLP) to add an extra layer of temporal processing. Our speaker verification experiments using the proposed features with the text-dependent RedDots corpus show small but consistent improvements in clean and reverberant conditions (up to 6.5%) over the 2DAR features and large improvements over the MFCC features in reverberant conditions (up to 46.5%)",
    "checked": true,
    "id": "625b8eae05f9f4575788233ec1ec02e05f8357ca",
    "semantic_title": "time-varying autoregressions for speaker verification in reverberant conditions",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bhattacharya17_interspeech.html": {
    "title": "Deep Speaker Embeddings for Short-Duration Speaker Verification",
    "volume": "main",
    "abstract": "The performance of a state-of-the-art speaker verification system is severely degraded when it is presented with trial recordings of short duration. In this work we propose to use deep neural networks to learn short-duration speaker embeddings. We focus on the 5s-5s condition, wherein both sides of a verification trial are 5 seconds long. In our previous work we established that learning a non-linear mapping from i-vectors to speaker labels is beneficial for speaker verification [1]. In this work we take the idea of learning a speaker classifier one step further — we apply deep neural networks directly to time-frequency speech representations. We propose two feed-forward network architectures for this task. Our best model is based on a deep convolutional architecture wherein recordings are treated as images. From our experimental findings we advocate treating utterances as images or ‘speaker snapshots', much like in face recognition. Our convolutional speaker embeddings perform significantly better than i-vectors when scoring is done using cosine distance, where the relative improvement is 23.5%. The proposed deep embeddings combined with cosine distance also outperform a state-of-the-art i-vector verification system by 1%, providing further empirical evidence in favor of our learned speaker features",
    "checked": true,
    "id": "1fc867b43092fe83c4e0bfa38a9a45ffaea86deb",
    "semantic_title": "deep speaker embeddings for short-duration speaker verification",
    "citation_count": 139,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/park17b_interspeech.html": {
    "title": "Using Voice Quality Features to Improve Short-Utterance, Text-Independent Speaker Verification Systems",
    "volume": "main",
    "abstract": "Due to within-speaker variability in phonetic content and/or speaking style, the performance of automatic speaker verification (ASV) systems degrades especially when the enrollment and test utterances are short. This study examines how different types of variability influence performance of ASV systems. Speech samples (< 2 sec) from the UCLA Speaker Variability Database containing 5 different read sentences by 200 speakers were used to study content variability. Other samples (about 5 sec) that contained speech directed towards pets, characterized by exaggerated prosody, were used to analyze style variability. Using the i-vector/PLDA framework, the ASV system error rate with MFCCs had a relative increase of at least 265% and 730% in content-mismatched and style-mismatched trials, respectively. A set of features that represents voice quality (F0, F1, F2, F3, H1-H2, H2-H4, H4-H2k, A1, A2, A3, and CPP) was also used. Using score fusion with MFCCs, all conditions saw decreases in error rates. In addition, using the NIST SRE10 database, score fusion provided relative improvements of 11.78% for 5-second utterances, 12.41% for 10-second utterances, and a small improvement for long utterances (about 5 min). These results suggest that voice quality features can improve short-utterance text-independent ASV system performance",
    "checked": true,
    "id": "6aab2715a57882924d1982116931622fe92ce69d",
    "semantic_title": "using voice quality features to improve short-utterance, text-independent speaker verification systems",
    "citation_count": 22,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lee17c_interspeech.html": {
    "title": "Gain Compensation for Fast i-Vector Extraction Over Short Duration",
    "volume": "main",
    "abstract": "I-vector is widely described as a compact and effective representation of speech utterances for speaker recognition. Standard i-vector extraction could be an expensive task for applications where computing resource is limited, for instance, on handheld devices. Fast approximate inference of i-vector aims to reduce the computational cost required in i-vector extraction where run-time requirement is critical. Most fast approaches hinge on certain assumptions to approximate the i-vector inference formulae with little loss of accuracy. In this paper, we analyze the uniform assumption that we had proposed earlier. We show that the assumption generally hold for long utterances but inadequate for utterances of short duration. We then propose to compensate for the negative effects by applying a simple gain factor on the i-vectors estimated from short utterances. The assertion is confirmed through analysis and experiments conducted on NIST SRE'08 and SRE'10 datasets",
    "checked": true,
    "id": "190290991d4017ad9a91456ffcec57db47d067bb",
    "semantic_title": "gain compensation for fast i-vector extraction over short duration",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/heo17_interspeech.html": {
    "title": "Joint Training of Expanded End-to-End DNN for Text-Dependent Speaker Verification",
    "volume": "main",
    "abstract": "We propose an expanded end-to-end DNN architecture for speaker verification based on b-vectors as well as d-vectors. We embedded the components of a speaker verification system such as modeling frame-level features, extracting utterance-level features, dimensionality reduction of utterance-level features, and trial-level scoring in an expanded end-to-end DNN architecture. The main contribution of this paper is that, instead of using DNNs as parts of the system trained independently, we train the whole system jointly with a fine-tune cost after pre-training each part. The experimental results show that the proposed system outperforms the baseline d-vector system and i-vector PLDA system",
    "checked": true,
    "id": "adbfc239889c7ed83123774c8b99b10ab011e109",
    "semantic_title": "joint training of expanded end-to-end dnn for text-dependent speaker verification",
    "citation_count": 20,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17f_interspeech.html": {
    "title": "Speaker Verification via Estimating Total Variability Space Using Probabilistic Partial Least Squares",
    "volume": "main",
    "abstract": "The i-vector framework is one of the most popular methods in speaker verification, and estimating a total variability space (TVS) is a key part in the i-vector framework. Current estimation methods pay less attention on the discrimination of TVS, but the discrimination is so important that it will influence the improvement of performance. So we focus on the discrimination of TVS to achieve a better performance. In this paper, a discriminative estimating method of TVS based on probabilistic partial least squares (PPLS) is proposed. In this method, the discrimination is improved by using the priori information (labels) of speaker, so both the correlation of intra-class and the discrimination of interclass are fully utilized. Meanwhile, it also introduces a probabilistic view of the partial least squares (PLS) method to overcome the disadvantage of high computational complexity and the inability of channel compensation. And also this proposed method can achieve a better performance than the traditional TVS estimation method as well as the PLS-based method",
    "checked": true,
    "id": "a1765f17ecea9f2a6506edbf1ff53a75c6aad4d2",
    "semantic_title": "speaker verification via estimating total variability space using probabilistic partial least squares",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/li17g_interspeech.html": {
    "title": "Deep Speaker Feature Learning for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "Recently deep neural networks (DNNs) have been used to learn speaker features. However, the quality of the learned features is not sufficiently good, so a complex back-end model, either neural or probabilistic, has to be used to address the residual uncertainty when applied to speaker verification. This paper presents a convolutional time-delay deep neural network structure (CT-DNN) for speaker feature learning. Our experimental results on the Fisher database demonstrated that this CT-DNN can produce high-quality speaker features: even with a single feature (0.3 seconds including the context), the EER can be as low as 7.68%. This effectively confirmed that the speaker trait is largely a deterministic short-time property rather than a long-time distributional pattern, and therefore can be extracted from just dozens of frames",
    "checked": true,
    "id": "ea1da06d076ca05a7227291897adc8d936f2f47a",
    "semantic_title": "deep speaker feature learning for text-independent speaker verification",
    "citation_count": 82,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bousquet17_interspeech.html": {
    "title": "Duration Mismatch Compensation Using Four-Covariance Model and Deep Neural Network for Speaker Verification",
    "volume": "main",
    "abstract": "Duration mismatch between enrollment and test utterances still remains a major concern for reliability of real-life speaker recognition applications. Two approaches are proposed here to deal with this case when using the i-vector representation. The first one is an adaptation of Gaussian Probabilistic Linear Discriminant Analysis (PLDA) modeling, which can be extended to the case of any shift between i-vectors drawn from two distinct distributions. The second one attempts to map i-vectors of truncated segments of an utterance to the i-vector of the full segment, by the use of deep neural networks (DNN). Our results show that both new approaches outperform the standard PLDA by about 10% relative, noting that these back-end methods could complement those quantifying the i-vector uncertainty during its extraction process, in the case of duration gap",
    "checked": true,
    "id": "436236f43d063539013d28bd813c3f9ce513afc4",
    "semantic_title": "duration mismatch compensation using four-covariance model and deep neural network for speaker verification",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mccree17_interspeech.html": {
    "title": "Extended Variability Modeling and Unsupervised Adaptation for PLDA Speaker Recognition",
    "volume": "main",
    "abstract": "Probabilistic Linear Discriminant Analysis (PLDA) continues to be the most effective approach for speaker recognition in the i-vector space. This paper extends the PLDA model to include both enrollment and test cut duration as well as to distinguish between session and channel variability. In addition, we address the task of unsupervised adaptation to unknown new domains in two ways: speaker-dependent PLDA parameters and cohort score normalization using Bayes rule. Experimental results on the NIST SRE16 task show that these principled techniques provide state-of-the-art performance with negligible increase in complexity over a PLDA baseline",
    "checked": true,
    "id": "3aba10082d9d8e9c82690dae91ef5d5a872d72b1",
    "semantic_title": "extended variability modeling and unsupervised adaptation for plda speaker recognition",
    "citation_count": 12,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/borgstrom17_interspeech.html": {
    "title": "Improving the Effectiveness of Speaker Verification Domain Adaptation with Inadequate In-Domain Data",
    "volume": "main",
    "abstract": "This paper addresses speaker verification domain adaptation with inadequate in-domain data. Specifically, we explore the cases where in-domain data sets do not include speaker labels, contain speakers with few samples, or contain speakers with low channel diversity. Existing domain adaptation methods are reviewed, and their shortcomings are discussed. We derive an unsupervised version of fully Bayesian adaptation which reduces the reliance on rich in-domain data. When applied to domain adaptation with inadequate in-domain data, the proposed approach yields competitive results when the samples per speaker are reduced, and outperforms existing supervised methods when the channel diversity is low, even without requiring speaker labels. These results are validated on the NIST SRE16, which uses a highly inadequate in-domain data set",
    "checked": true,
    "id": "fb9603732bfe851932214df19099411ca7095775",
    "semantic_title": "improving the effectiveness of speaker verification domain adaptation with inadequate in-domain data",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tan17_interspeech.html": {
    "title": "i-Vector DNN Scoring and Calibration for Noise Robust Speaker Verification",
    "volume": "main",
    "abstract": "This paper proposes applying multi-task learning to train deep neural networks (DNNs) for calibrating the PLDA scores of speaker verification systems under noisy environments. To facilitate the DNNs to learn the main task (calibration), several auxiliary tasks were introduced, including the prediction of SNR and duration from i-vectors and classifying whether an i-vector pair belongs to the same speaker or not. The possibility of replacing the PLDA model by a DNN during the scoring stage is also explored. Evaluations on noise contaminated speech suggest that the auxiliary tasks are important for the DNNs to learn the main calibration task and that the uncalibrated PLDA scores are an essential input to the DNNs. Without this input, the DNNs can only predict the score shifts accurately, suggesting that the PLDA model is indispensable",
    "checked": true,
    "id": "090390bdb38787ae125cb4c4a62c810006cba66b",
    "semantic_title": "i-vector dnn scoring and calibration for noise robust speaker verification",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/matejka17_interspeech.html": {
    "title": "Analysis of Score Normalization in Multilingual Speaker Recognition",
    "volume": "main",
    "abstract": "NIST Speaker Recognition Evaluation 2016 has revealed the importance of score normalization for mismatched data conditions. This paper analyzes several score normalization techniques for test conditions with multiple languages. The best performing one for a PLDA classifier is an adaptive s-norm with 30% relative improvement over the system without any score normalization. The analysis shows that the adaptive score normalization (using top scoring files per trial) selects cohorts that in 68% contain recordings from the same language and in 92% of the same gender as the enrollment and test recordings. Our results suggest that the data to select score normalization cohorts should be a pool of several languages and channels and if possible, its subset should contain data from the target domain",
    "checked": true,
    "id": "295b4823f8baa9b8c8b24ae7d28875344062731b",
    "semantic_title": "analysis of score normalization in multilingual speaker recognition",
    "citation_count": 101,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/silnova17_interspeech.html": {
    "title": "Alternative Approaches to Neural Network Based Speaker Verification",
    "volume": "main",
    "abstract": "Just like in other areas of automatic speech processing, feature extraction based on bottleneck neural networks was recently found very effective for the speaker verification task. However, better results are usually reported with more complex neural network architectures (e.g. stacked bottlenecks), which are difficult to reproduce. In this work, we experiment with the so called deep features, which are based on a simple feed-forward neural network architecture. We study various forms of applying deep features to i-vector/PDA based speaker verification. With proper settings, better verification performance can be obtained by means of this simple architecture as compared to the more elaborate bottleneck features. Also, we further experiment with multi-task training, where the neural network is trained for both speaker recognition and senone recognition objectives. Results indicate that, with a careful weighting of the two objectives, multi-task training can result in significantly better performing deep features",
    "checked": true,
    "id": "2f54aeebac612f5bcc56130862975df4676632fa",
    "semantic_title": "alternative approaches to neural network based speaker verification",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/travadi17_interspeech.html": {
    "title": "A Distribution Free Formulation of the Total Variability Model",
    "volume": "main",
    "abstract": "The Total Variability Model (TVM) [1] has been widely used in audio signal processing as a framework for capturing differences in feature space distributions across variable length sequences by mapping them into a fixed-dimensional representation. Its formulation requires making an assumption about the source data distribution being a Gaussian Mixture Model (GMM). In this paper, we show that it is possible to arrive at the same model formulation without requiring such an assumption about distribution of the data, by showing asymptotic normality of the statistics used to estimate the model. We highlight some connections between TVM and heteroscedastic Principal Component Analysis (PCA), as well as the matrix completion problem, which lead to a computationally efficient formulation of the Maximum Likelihood estimation problem for the model",
    "checked": true,
    "id": "88bfbef4d2b7631b9dd1cae7935560e8f90f5107",
    "semantic_title": "a distribution free formulation of the total variability model",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/rahman17_interspeech.html": {
    "title": "Domain Mismatch Modeling of Out-Domain i-Vectors for PLDA Speaker Verification",
    "volume": "main",
    "abstract": "The state-of-the-art i-vector based probabilistic linear discriminant analysis (PLDA) trained on non-target (or out-domain) data significantly affects the speaker verification performance due to the domain mismatch between training and evaluation data. To improve the speaker verification performance, sufficient amount of domain mismatch compensated out-domain data must be used to train the PLDA models successfully. In this paper, we propose a domain mismatch modeling (DMM) technique using maximum-a-posteriori (MAP) estimation to model and compensate the domain variability from the out-domain training i-vectors. From our experimental results, we found that the DMM technique can achieve at least a 24% improvement in EER over an out-domain only baseline when speaker labels are available. Further improvement of 3% is obtained when combining DMM with domain-invariant covariance normalization (DICN) approach. The DMM/DICN combined technique is shown to perform better than in-domain PLDA system with only 200 labeled speakers or 2,000 unlabeled i-vectors",
    "checked": true,
    "id": "74f6118b95467b239ee8b215fe6c41de384e7ff1",
    "semantic_title": "domain mismatch modeling of out-domain i-vectors for plda speaker verification",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/cheng17_interspeech.html": {
    "title": "An Exploration of Dropout with LSTMs",
    "volume": "main",
    "abstract": "Long Short-Term Memory networks (LSTMs) are a component of many state-of-the-art DNN-based speech recognition systems. Dropout is a popular method to improve generalization in DNN training. In this paper we describe extensive experiments in which we investigated the best way to combine dropout with LSTMs — specifically, projected LSTMs (LSTMP). We investigated various locations in the LSTM to place the dropout (and various combinations of locations), and a variety of dropout schedules. Our optimized recipe gives consistent improvements in WER across a range of datasets, including Switchboard, TED-LIUM and AMI",
    "checked": true,
    "id": "b0316d17fef2a42fba426426e5ea090a83205aaa",
    "semantic_title": "an exploration of dropout with lstms",
    "citation_count": 100,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kim17e_interspeech.html": {
    "title": "Residual LSTM: Design of a Deep Recurrent Architecture for Distant Speech Recognition",
    "volume": "main",
    "abstract": "In this paper, a novel architecture for a deep recurrent neural network, residual LSTM is introduced. A plain LSTM has an internal memory cell that can learn long term dependencies of sequential data. It also provides a temporal shortcut path to avoid vanishing or exploding gradients in the temporal domain. The residual LSTM provides an additional spatial shortcut path from lower layers for efficient training of deep networks with multiple LSTM layers. Compared with the previous work, highway LSTM, residual LSTM separates a spatial shortcut path with temporal one by using output layers, which can help to avoid a conflict between spatial and temporal-domain gradient flows. Furthermore, residual LSTM reuses the output projection matrix and the output gate of LSTM to control the spatial information flow instead of additional gate networks, which effectively reduces more than 10% of network parameters. An experiment for distant speech recognition on the AMI SDM corpus shows that 10-layer plain and highway LSTM networks presented 13.7% and 6.2% increase in WER over 3-layer baselines, respectively. On the contrary, 10-layer residual LSTM networks provided the lowest WER 41.0%, which corresponds to 3.3% and 2.8% WER reduction over plain and highway LSTM networks, respectively",
    "checked": true,
    "id": "097fb68f360195d438126a7cd9b988fac31b1228",
    "semantic_title": "residual lstm: design of a deep recurrent architecture for distant speech recognition",
    "citation_count": 168,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tran17_interspeech.html": {
    "title": "Unfolded Deep Recurrent Convolutional Neural Network with Jump Ahead Connections for Acoustic Modeling",
    "volume": "main",
    "abstract": "Recurrent neural networks (RNNs) with jump ahead connections have been used in the computer vision tasks. Still, they have not been investigated well for automatic speech recognition (ASR) tasks. In other words, unfolded RNN has been shown to be an effective model for acoustic modeling tasks. This paper investigates how to elaborate a sophisticated unfolded deep RNN architecture in which recurrent connections use a convolutional neural network (CNN) to model a short-term dependence between hidden states. In this study, our unfolded RNN architecture is a CNN that process a sequence of input features sequentially. Each time step, the CNN inputs a small block of the input features and the output of the hidden layer from the preceding block in order to compute the output of its hidden layer. In addition, by exploiting either one or multiple jump ahead connections between time steps, our network can learn long-term dependencies more effectively. We carried experiments on the CHiME 3 task showing the effectiveness of our proposed approach",
    "checked": true,
    "id": "3640a44aebed60570e7ab76bce5d7b681a1db218",
    "semantic_title": "unfolded deep recurrent convolutional neural network with jump ahead connections for acoustic modeling",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/karita17_interspeech.html": {
    "title": "Forward-Backward Convolutional LSTM for Acoustic Modeling",
    "volume": "main",
    "abstract": "An automatic speech recognition (ASR) performance has greatly improved with the introduction of convolutional neural network (CNN) or long-short term memory (LSTM) for acoustic modeling. Recently, a convolutional LSTM (CLSTM) has been proposed to directly use convolution operation within the LSTM blocks and combine the advantages of both CNN and LSTM structures into a single architecture. This paper presents the first attempt to use CLSTMs for acoustic modeling. In addition, we propose a new forward-backward architecture to exploit long-term left/right context efficiently. The proposed scheme combines forward and backward LSTMs at different time points of an utterance with the aim of modeling long term frame invariant information such as speaker characteristics, channel etc. Furthermore, the proposed forward-backward architecture can be trained with truncated back-propagation-through-time unlike conventional bidirectional LSTM (BLSTM) architectures. Therefore, we are able to train deeply stacked CLSTM acoustic models, which is practically challenging with conventional BLSTMs. Experimental results show that both CLSTM and forward-backward LSTM improve word error rates significantly compared to standard CNN and LSTM architectures",
    "checked": true,
    "id": "7cf26ee9541433356a3bcf46cc62df9505c9b297",
    "semantic_title": "forward-backward convolutional lstm for acoustic modeling",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ark17_interspeech.html": {
    "title": "Convolutional Recurrent Neural Networks for Small-Footprint Keyword Spotting",
    "volume": "main",
    "abstract": "Keyword spotting (KWS) constitutes a major component of human-technology interfaces. Maximizing the detection accuracy at a low false alarm (FA) rate, while minimizing the footprint size, latency and complexity are the goals for KWS. Towards achieving them, we study Convolutional Recurrent Neural Networks (CRNNs). Inspired by large-scale state-of-the-art speech recognition systems, we combine the strengths of convolutional layers and recurrent layers to exploit local structure and long-range context. We analyze the effect of architecture parameters, and propose training strategies to improve performance. With only ~230k parameters, our CRNN model yields acceptably low latency, and achieves 97.71% accuracy at 0.5 FA/hour for 5 dB signal-to-noise ratio",
    "checked": true,
    "id": "db46b939874e01fb861e7d0a8670cb83092f0ca5",
    "semantic_title": "convolutional recurrent neural networks for small-footprint keyword spotting",
    "citation_count": 164,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wu17c_interspeech.html": {
    "title": "Deep Activation Mixture Model for Speech Recognition",
    "volume": "main",
    "abstract": "Deep learning approaches achieve state-of-the-art performance in a range of applications, including speech recognition. However, the parameters of the deep neural network (DNN) are hard to interpret, which makes regularisation and adaptation to speaker or acoustic conditions challenging. This paper proposes the deep activation mixture model (DAMM) to address these problems. The output of one hidden layer is modelled as the sum of a mixture and residual models. The mixture model forms an activation function contour while the residual one models fluctuations around the contour. The use of the mixture model gives two advantages: First, it introduces a novel regularisation on the DNN. Second, it allows novel adaptation schemes. The proposed approach is evaluated on a large-vocabulary U.S. English broadcast news task. It yields a slightly better performance than the DNN baselines, and on the utterance-level unsupervised adaptation, the adapted DAMM acquires further performance gains",
    "checked": true,
    "id": "805adcbf223e4453c7c2ba9adff809581883427e",
    "semantic_title": "deep activation mixture model for speech recognition",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/heck17_interspeech.html": {
    "title": "Ensembles of Multi-Scale VGG Acoustic Models",
    "volume": "main",
    "abstract": "We present our work on constructing multi-scale deep convolutional neural networks for automatic speech recognition. Several VGG nets have been trained that differ solely in the kernel size of the convolutional layers. The general idea is that receptive fields of varying sizes match structures of different scales, thus supporting more robust recognition when combined appropriately. We construct a large multi-scale system by means of system combination. We use ROVER and the fusion of posterior predictions as examples of late combination, and knowledge distillation using soft labels from a model ensemble as a way of early combination. In this work, distillation is approached from the perspective of knowledge transfer pre-training, which is followed by a fine-tuning on the original hard labels. Our results show that it is possible to bundle the individual recognition strengths of the VGGs in a much simpler CNN architecture that yields equal performance with the best late combination",
    "checked": true,
    "id": "fc7117259ab273da111533f3fdf824b4ed19ddbd",
    "semantic_title": "ensembles of multi-scale vgg acoustic models",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/grosz17_interspeech.html": {
    "title": "Training Context-Dependent DNN Acoustic Models Using Probabilistic Sampling",
    "volume": "main",
    "abstract": "In current HMM/DNN speech recognition systems, the purpose of the DNN component is to estimate the posterior probabilities of tied triphone states. In most cases the distribution of these states is uneven, meaning that we have a markedly different number of training samples for the various states. This imbalance of the training data is a source of suboptimality for most machine learning algorithms, and DNNs are no exception. A straightforward solution is to re-sample the data, either by upsampling the rarer classes or by downsampling the more common classes. Here, we experiment with the so-called probabilistic sampling method that applies downsampling and upsampling at the same time. For this, it defines a new class distribution for the training data, which is a linear combination of the original and the uniform class distributions. As an extension to previous studies, we propose a new method to re-estimate the class priors, which is required to remedy the mismatch between the training and the test data distributions introduced by re-sampling. Using probabilistic sampling and the proposed modification we report 5% and 6% relative error rate reductions on the TED-LIUM and on the AMI corpora, respectively",
    "checked": true,
    "id": "7768c3bb97bbe8d37720b5089f8da5c3eb7f851c",
    "semantic_title": "training context-dependent dnn acoustic models using probabilistic sampling",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/grosz17b_interspeech.html": {
    "title": "A Comparative Evaluation of GMM-Free State Tying Methods for ASR",
    "volume": "main",
    "abstract": "Deep neural network (DNN) based speech recognizers have recently replaced Gaussian mixture (GMM) based systems as the state-of-the-art. While some of the modeling techniques developed for the GMM based framework may directly be applied to HMM/DNN systems, others may be inappropriate. One such example is the creation of context-dependent tied states, for which an efficient decision tree state tying method exists. The tied states used to train DNNs are usually obtained using the same tying algorithm, even though it is based on likelihoods of Gaussians, hence it is more appropriate for HMM/GMMs. Recently, however, several refinements have been published which seek to adapt the state tying algorithm to the HMM/DNN hybrid architecture. Unfortunately, these studies reported results on different (and sometimes very small) datasets, which does not allow their direct comparison. Here, we tested four of these methods on the same LVCSR task, and compared their performance under the same circumstances. We found that, besides changing the input of the context-dependent state tying algorithm, it is worth adjusting the tying criterion as well. The methods which utilized a decision criterion designed directly for neural networks consistently, and significantly, outperformed those which employed the standard Gaussian-based algorithm",
    "checked": true,
    "id": "76fa912b181a4ac6917a1fa0cca78a13dfcb081e",
    "semantic_title": "a comparative evaluation of gmm-free state tying methods for asr",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wang17h_interspeech.html": {
    "title": "Backstitch: Counteracting Finite-Sample Bias via Negative Steps",
    "volume": "main",
    "abstract": "In this paper we describe a modification to Stochastic Gradient Descent (SGD) that improves generalization to unseen data. It consists of doing two steps for each minibatch: a backward step with a small negative learning rate, followed by a forward step with a larger learning rate. The idea was initially inspired by ideas from adversarial training, but we show that it can be viewed as a crude way of canceling out certain systematic biases that come from training on finite data sets. The method gives ~ 10% relative improvement over our best acoustic models based on lattice-free MMI, across multiple datasets with 100–300 hours of data",
    "checked": true,
    "id": "aeec5e369572d5f2cd88e4f5166de439558af933",
    "semantic_title": "backstitch: counteracting finite-sample bias via negative steps",
    "citation_count": 27,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/takeda17_interspeech.html": {
    "title": "Node Pruning Based on Entropy of Weights and Node Activity for Small-Footprint Acoustic Model Based on Deep Neural Networks",
    "volume": "main",
    "abstract": "This paper describes a node-pruning method for an acoustic model based on deep neural networks (DNNs). Node pruning is a promising method to reduce the memory usage and computational cost of DNNs. A score function is defined to measure the importance of each node, and less important nodes are pruned. The entropy of the activity of each node has been used as a score function to find nodes with outputs that do not change at all. We introduce entropy of weights of each node to consider the number of weights and their patterns of each node. Because the number of weights and the patterns differ at each layer, the importance of the node should also be measured using the related weights of the target node. We then propose a score function that integrates the entropy of weights and node activity, which will prune less important nodes more efficiently. Experimental results showed that the proposed pruning method successfully reduced the number of parameters by about 6% without any accuracy loss compared with a score function based only on the entropy of node activity",
    "checked": true,
    "id": "fdd97f0260eabb3f6b28cdb0a8b35e71cd710a8f",
    "semantic_title": "node pruning based on entropy of weights and node activity for small-footprint acoustic model based on deep neural networks",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/variani17_interspeech.html": {
    "title": "End-to-End Training of Acoustic Models for Large Vocabulary Continuous Speech Recognition with TensorFlow",
    "volume": "main",
    "abstract": "This article discusses strategies for end-to-end training of state-of-the-art acoustic models for Large Vocabulary Continuous Speech Recognition (LVCSR), with the goal of leveraging TensorFlow components so as to make efficient use of large-scale training sets, large model sizes, and high-speed computation units such as Graphical Processing Units (GPUs). Benchmarks are presented that evaluate the efficiency of different approaches to batching of training data, unrolling of recurrent acoustic models, and device placement of TensorFlow variables and operations. An overall training architecture developed in light of those findings is then described. The approach makes it possible to take advantage of both data parallelism and high speed computation on GPU for state-of-the-art sequence training of acoustic models. The effectiveness of the design is evaluated for different training schemes and model sizes, on a 15,000 hour Voice Search task",
    "checked": true,
    "id": "fab90790ab3a3880e6fec274b3760f4417e76f0b",
    "semantic_title": "end-to-end training of acoustic models for large vocabulary continuous speech recognition with tensorflow",
    "citation_count": 26,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sim17_interspeech.html": {
    "title": "An Efficient Phone N-Gram Forward-Backward Computation Using Dense Matrix Multiplication",
    "volume": "main",
    "abstract": "The forward-backward algorithm is commonly used to train neural network acoustic models when optimizing a sequence objective like MMI and sMBR. Recent work on lattice-free MMI training of neural network acoustic models shows that the forward-backward algorithm can be computed efficiently in the probability domain as a series of sparse matrix multiplications using GPUs. In this paper, we present a more efficient way of computing forward-backward using a dense matrix multiplication approach. We do this by exploiting the block-diagonal structure of the n-gram state transition matrix; instead of multiplying large sparse matrices, the proposed method involves a series of smaller dense matrix multiplications, which can be computed in parallel. Efficient implementation can be easily achieved by leveraging on the optimized matrix multiplication routines provided by standard libraries, such as NumPy and TensorFlow. Runtime benchmarks show that the dense multiplication method is consistently faster than the sparse multiplication method (on both CPUs and GPUs), when applied to a 4-gram phone language model. This is still the case even when the sparse multiplication method uses a more compact finite state model representation by excluding unseen n-grams",
    "checked": true,
    "id": "ed527c5fe826d17aa578adc07c63c7b9d6f134e6",
    "semantic_title": "an efficient phone n-gram forward-backward computation using dense matrix multiplication",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tuske17_interspeech.html": {
    "title": "Parallel Neural Network Features for Improved Tandem Acoustic Modeling",
    "volume": "main",
    "abstract": "The combination of acoustic models or features is a standard approach to exploit various knowledge sources. This paper investigates the concatenation of different bottleneck (BN) neural network (NN) outputs for tandem acoustic modeling. Thus, combination of NN features is performed via Gaussian mixture models (GMM). Complementarity between the NN feature representations is attained by using various network topologies: LSTM recurrent, feed-forward, and hierarchical, as well as different non-linearities: hyperbolic tangent, sigmoid, and rectified linear units. Speech recognition experiments are carried out on various tasks: telephone conversations, Skype calls, as well as broadcast news and conversations. Results indicate that LSTM based tandem approach is still competitive, and such tandem model can challenge comparable hybrid systems. The traditional steps of tandem modeling, speaker adaptive and sequence discriminative GMM training, improve the tandem results further. Furthermore, these \"old-fashioned\" steps remain applicable after the concatenation of multiple neural network feature streams. Exploiting the parallel processing of input feature streams, it is shown that 2–5% relative improvement could be achieved over the single best BN feature set. Finally, we also report results after neural network based language model rescoring and examine the system combination possibilities using such complex tandem models",
    "checked": true,
    "id": "44ca9b3d070ea55056093f6fb928927a23405e29",
    "semantic_title": "parallel neural network features for improved tandem acoustic modeling",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tang17_interspeech.html": {
    "title": "Acoustic Feature Learning via Deep Variational Canonical Correlation Analysis",
    "volume": "main",
    "abstract": "We study the problem of acoustic feature learning in the setting where we have access to another (non-acoustic) modality for feature learning but not at test time. We use deep variational canonical correlation analysis (VCCA), a recently proposed deep generative method for multi-view representation learning. We also extend VCCA with improved latent variable priors and with adversarial learning. Compared to other techniques for multi-view feature learning, VCCA's advantages include an intuitive latent variable interpretation and a variational lower bound objective that can be trained end-to-end efficiently. We compare VCCA and its extensions with previous feature learning methods on the University of Wisconsin X-ray Microbeam Database, and show that VCCA-based feature learning improves over previous methods for speaker-independent phonetic recognition",
    "checked": true,
    "id": "47025a0f8f8d375479d98dc6340959360c52d74a",
    "semantic_title": "acoustic feature learning via deep variational canonical correlation analysis",
    "citation_count": 19,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/masumura17_interspeech.html": {
    "title": "Online End-of-Turn Detection from Speech Based on Stacked Time-Asynchronous Sequential Networks",
    "volume": "main",
    "abstract": "This paper presents a novel modeling called stacked time-asynchronous sequential networks (STASNs) for online end-of-turn detection. An online end-of-turn detection that determines turn-taking points in a real-time manner is an essential component for human-computer interaction systems. In this study, we use long-range sequential information of multiple time-asynchronous sequential features, such as prosodic, phonetic, and lexical sequential features, to enhance online end-of-turn detection performance. Our key idea is to embed individual sequential features in a fixed-length continuous representation by using sequential networks. This enables us to simultaneously handle multiple time-asynchronous sequential features for end-of-turn detection. STASNs can embed all of the sequential information between a start-of-conversation and the current end-of-utterance in a fixed-length continuous representation that can be directly used for classification by stacking multiple sequential networks. Experiments show that STASNs outperforms conventional modeling with limited sequential information. Furthermore, STASNs with senone bottleneck features extracted using senone-based deep neural networks have superior performance without requiring lexical features decoded by an automatic speech recognition process",
    "checked": true,
    "id": "a6d0c679deb1e951fba165a3ca45ff730ba94662",
    "semantic_title": "online end-of-turn detection from speech based on stacked time-asynchronous sequential networks",
    "citation_count": 28,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wodarczak17_interspeech.html": {
    "title": "Improving Prediction of Speech Activity Using Multi-Participant Respiratory State",
    "volume": "main",
    "abstract": "One consequence of situated face-to-face conversation is the co-observability of participants' respiratory movements and sounds. We explore whether this information can be exploited in predicting incipient speech activity. Using a methodology called stochastic turn-taking modeling, we compare the performance of a model trained on speech activity alone to one additionally trained on static and dynamic lung volume features. The methodology permits automatic discovery of temporal dependencies across participants and feature types. Our experiments show that respiratory information substantially lowers cross-entropy rates, and that this generalizes to unseen data",
    "checked": true,
    "id": "b8ed3daffef928d9bf5161a2dc5362953711f352",
    "semantic_title": "improving prediction of speech activity using multi-participant respiratory state",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/heeman17_interspeech.html": {
    "title": "Turn-Taking Offsets and Dialogue Context",
    "volume": "main",
    "abstract": "A number of researchers have studied turn-taking offsets in human-human dialogues. However, that work collapses over a wide number of different turn-taking contexts. In this work, we delve into the turn-taking delays based on different contexts. We show that turn-taking behavior, both who tends to take the turn next, and the turn-taking delays, are dependent on the previous speech act type, the upcoming speech act, and the nature of the dialogue. This strongly suggests that in studying turn-taking, all turn-taking events should not be grouped together. This also suggests that delays are due to cognitive processing of what to say, rather than whether a speaker should take the turn",
    "checked": true,
    "id": "74e3ab6dce67c71fe90006f2bba28327a5702181",
    "semantic_title": "turn-taking offsets and dialogue context",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/maier17_interspeech.html": {
    "title": "Towards Deep End-of-Turn Prediction for Situated Spoken Dialogue Systems",
    "volume": "main",
    "abstract": "We address the challenge of improving live end-of-turn detection for situated spoken dialogue systems. While traditionally silence thresholds have been used to detect the user's end-of-turn, such an approach limits the system's potential fluidity in interaction, restricting it to a purely reactive paradigm. By contrast, here we present a system which takes a predictive approach. The user's end-of-turn is predicted live as acoustic features and words are consumed by the system. We compare the benefits of live lexical and acoustic information by feature analysis and testing equivalent models with different feature sets with a common deep learning architecture, a Long Short-Term Memory (LSTM) network. We show the usefulness of incremental enriched language model features in particular. Training and testing onWizard-of-Oz data collected to train an agent in a simple virtual world, we are successful in improving over a reactive baseline in terms of reducing latency whilst minimising the cut-in rate",
    "checked": true,
    "id": "1a527fa6c5e97e13112d7503cde891de8260719c",
    "semantic_title": "towards deep end-of-turn prediction for situated spoken dialogue systems",
    "citation_count": 34,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ishimoto17_interspeech.html": {
    "title": "End-of-Utterance Prediction by Prosodic Features and Phrase-Dependency Structure in Spontaneous Japanese Speech",
    "volume": "main",
    "abstract": "This study is aimed at uncovering a way that participants in conversation predict end-of-utterance for spontaneous Japanese speech. In spontaneous everyday conversation, the participants must predict the ends of utterances of a speaker to perform smooth turn-taking without too much gap. We consider that they utilize not only syntactic factors but also prosodic factors for the end-of-utterance prediction because of the difficulty of prediction of a syntactic completion point in spontaneous Japanese. In previous studies, we found that prosodic features changed significantly in the final accentual phrase. However, it is not clear what prosodic features support the prediction. In this paper, we focused on dependency structure among bunsetsu-phrases as the syntactic factor, and investigated the relation between the phrase-dependency and prosodic features. The results showed that the average fundamental frequency and the average intensity for accentual phrases did not decline until the modified phrase appeared. Next, to predict the end of utterance from the syntactic and prosodic features, we constructed a generalized linear mixed model. The model provided higher accuracy than using the prosodic features only. These suggest the possibility that prosodic changes and phrase-dependency relations inform the hearer that the utterance is approaching its end",
    "checked": true,
    "id": "7a10cd0fdd32c093da38ada5e16adf149a66b5f5",
    "semantic_title": "end-of-utterance prediction by prosodic features and phrase-dependency structure in spontaneous japanese speech",
    "citation_count": 14,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/liu17_interspeech.html": {
    "title": "Turn-Taking Estimation Model Based on Joint Embedding of Lexical and Prosodic Contents",
    "volume": "main",
    "abstract": "A natural conversation involves rapid exchanges of turns while talking. Taking turns at appropriate timing or intervals is a requisite feature for a dialog system as a conversation partner. This paper proposes a model that estimates the timing of turn-taking during verbal interactions. Unlike previous studies, our proposed model does not rely on a silence region between sentences since a dialog system must respond without large gaps or overlaps. We propose a Recurrent Neural Network (RNN) based model that takes the joint embedding of lexical and prosodic contents as its input to classify utterances into turn-taking related classes and estimates the turn-taking timing. To this end, we trained a neural network to embed the lexical contents, the fundamental frequencies, and the speech power into a joint embedding space. To learn meaningful embedding spaces, the prosodic features from each single utterance are pre-trained using RNN and combined with utterance lexical embedding as the input of our proposed model. We tested this model on a spontaneous conversation dataset and confirmed that it outperformed the use of word embedding-based features",
    "checked": true,
    "id": "f87f58d4f0707f6d0d41827fc8888dd236cfcec5",
    "semantic_title": "turn-taking estimation model based on joint embedding of lexical and prosodic contents",
    "citation_count": 21,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/inaguma17_interspeech.html": {
    "title": "Social Signal Detection in Spontaneous Dialogue Using Bidirectional LSTM-CTC",
    "volume": "main",
    "abstract": "Non-verbal speech cues such as laughter and fillers, which are collectively called social signals, play an important role in human communication. Therefore, detection of them would be useful for dialogue systems to infer speaker's intentions, emotions and engagements. The conventional approaches are based on frame-wise classifiers, which require precise time-alignment of these events for training. This work investigates the Connectionist Temporal Classification (CTC) approach which can learn an alignment between the input and its target label sequence. This allows for robust detection of the events and efficient training without precise time information. Experimental evaluations with various settings demonstrate that CTC based on bidirectional LSTM outperforms the conventional DNN and HMM based methods",
    "checked": true,
    "id": "1fb43ff5f1292d54ef1aa307f76b59957c2d4ff3",
    "semantic_title": "social signal detection in spontaneous dialogue using bidirectional lstm-ctc",
    "citation_count": 17,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/rahimi17_interspeech.html": {
    "title": "Entrainment in Multi-Party Spoken Dialogues at Multiple Linguistic Levels",
    "volume": "main",
    "abstract": "Linguistic entrainment, the phenomena whereby dialogue partners speak more similarly to each other in a variety of dimensions, is key to the success and naturalness of interactions. While there is considerable evidence for both lexical and acoustic-prosodic entrainment, little work has been conducted to investigate the relationship between these two different modalities using the same measures in the same dialogues, specifically in multi-party dialogue. In this paper, we measure lexical and acoustic-prosodic entrainment for multi-party teams to explore whether entrainment occurs at multiple levels during conversation and to understand the relationship between these two modalities",
    "checked": true,
    "id": "3c75da1569689f4083f33aa3b5e30f4f960af9b0",
    "semantic_title": "entrainment in multi-party spoken dialogues at multiple linguistic levels",
    "citation_count": 24,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/reverdy17_interspeech.html": {
    "title": "Measuring Synchrony in Task-Based Dialogues",
    "volume": "main",
    "abstract": "In many contexts from casual everyday conversations to formal discussions, people tend to repeat their interlocutors, and themselves. This phenomenon not only yields random repetitions one might expect from a natural Zipfian distribution of linguistic forms, but also projects underlying discourse mechanisms and rhythms that researchers have suggested establishes conversational involvement and may support communicative progress towards mutual understanding. In this paper, advances in an automated method for assessing interlocutor synchrony in task-based Human-to-Human interactions are reported. The method focuses on dialogue structure, rather than temporal distance, measuring repetition between speakers and their interlocutors last n-turns (n = 1, however far back in the conversation that might have been) rather than utterances during a prior window fixed by duration. The significance of distinct linguistic levels of repetition are assessed by observing contrasts between actual and randomized dialogues, in order to provide a quantifying measure of communicative success. Definite patterns of repetitions where identified, notably in contrasting the role of participants (as information giver or follower). The extent to which those interacted sometime surprisingly with gender, eye-contact and familiarity is the principal contribution of this work",
    "checked": true,
    "id": "fc8d74d6579c7c7e23dedbddee463c6bb8d5de5d",
    "semantic_title": "measuring synchrony in task-based dialogues",
    "citation_count": 13,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/crook17_interspeech.html": {
    "title": "Sequence to Sequence Modeling for User Simulation in Dialog Systems",
    "volume": "main",
    "abstract": "User simulators are a principal offline method for training and evaluating human-computer dialog systems. In this paper, we examine simple sequence-to-sequence neural network architectures for training end-to-end, natural language to natural language, user simulators, using only raw logs of previous interactions without any additional human labelling. We compare the neural network-based simulators with a language model (LM)-based approach for creating natural language user simulators. Using both an automatic evaluation using LM perplexity and a human evaluation, we demonstrate that the sequence-to-sequence approaches outperform the LM-based method. We show correlation between LM perplexity and the human evaluation on this task, and discuss the benefits of different neural network architecture variations",
    "checked": true,
    "id": "7b4efd142ce80de9a485f2d5aede00354e64862d",
    "semantic_title": "sequence to sequence modeling for user simulation in dialog systems",
    "citation_count": 24,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ramanarayanan17b_interspeech.html": {
    "title": "Human and Automated Scoring of Fluency, Pronunciation and Intonation During Human–Machine Spoken Dialog Interactions",
    "volume": "main",
    "abstract": "We present a spoken dialog-based framework for the computer-assisted language learning (CALL) of conversational English. In particular, we leveraged the open-source HALEF dialog framework to develop a job interview conversational application. We then used crowdsourcing to collect multiple interactions with the system from non-native English speakers. We analyzed human-rated scores of the recorded dialog data on three different scoring dimensions critical to the delivery of conversational English — fluency, pronunciation and intonation/stress — and further examined the efficacy of automatically-extracted, hand-curated speech features in predicting each of these sub-scores. Machine learning experiments showed that trained scoring models generally perform at par with the human inter-rater agreement baseline in predicting human-rated scores of conversational proficiency",
    "checked": true,
    "id": "78912b8661473f4f1f24ed8db11da8dcf7569fbd",
    "semantic_title": "human and automated scoring of fluency, pronunciation and intonation during human-machine spoken dialog interactions",
    "citation_count": 15,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ando17_interspeech.html": {
    "title": "Hierarchical LSTMs with Joint Learning for Estimating Customer Satisfaction from Contact Center Calls",
    "volume": "main",
    "abstract": "This paper presents a joint modeling of both turn-level and call-level customer satisfaction in contact center dialogue. Our key idea is to directly apply turn-level estimation results to call-level estimation and optimize them jointly; previous work treated both estimations as being independent. Proposed joint modeling is achieved by stacking two types of long short-term memory recurrent neural networks (LSTM-RNNs). The lower layer employs LSTM-RNN for sequential labeling of turn-level customer satisfaction in which each label is estimated from context information extracted from not only the target turn but also the surrounding turns. The upper layer uses another LSTM-RNN to estimate call-level customer satisfaction labels from all information of estimated turn-level customer satisfaction. These two networks can be efficiently optimized by joint learning of both types of labels. Experiments show that the proposed method outperforms a conventional support vector machine based method in terms of both turn-level and call-level customer satisfaction with relative error reductions of over 20%",
    "checked": true,
    "id": "fafeddb84afc6e923f866a54bfc611408ddff9e0",
    "semantic_title": "hierarchical lstms with joint learning for estimating customer satisfaction from contact center calls",
    "citation_count": 21,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ultes17_interspeech.html": {
    "title": "Domain-Independent User Satisfaction Reward Estimation for Dialogue Policy Learning",
    "volume": "main",
    "abstract": "Learning suitable and well-performing dialogue behaviour in statistical spoken dialogue systems has been in the focus of research for many years. While most work which is based on reinforcement learning employs an objective measure like task success for modelling the reward signal, we propose to use a reward based on user satisfaction. We will show in simulated experiments that a live user satisfaction estimation model may be applied resulting in higher estimated satisfaction whilst achieving similar success rates. Moreover, we will show that one satisfaction estimation model which has been trained on one domain may be applied in many other domains which cover a similar task. We will verify our findings by employing the model to one of the domains for learning a policy from real users and compare its performance to policies using the user satisfaction and task success acquired directly from the users as reward",
    "checked": true,
    "id": "95c402e37f3d44c378b7af831712a6a5ccfbccf6",
    "semantic_title": "domain-independent user satisfaction reward estimation for dialogue policy learning",
    "citation_count": 30,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/nakamura17_interspeech.html": {
    "title": "Analysis of the Relationship Between Prosodic Features of Fillers and its Forms or Occurrence Positions",
    "volume": "main",
    "abstract": "Fillers are involved in the ease of understanding by listeners and turn-taking. However, the knowledge about its prosodic features is insufficient, and its modeling has not been done either. For these reasons, there is insufficient knowledge to generate natural and appropriate fillers in a dialog system at present. Therefore, for the purpose of clarifying the prosodic features of fillers, its relationship with occurrence positions or forms were analyzed in this research. ‘Ano' and ‘Eto' were used as forms, non-/boundary of Dialog Act and non-/turn-taking for occurrence positions. Duration, F0, and intensity were utilized as prosodic features. As a result, the followings were found out: the prosodic features are different depending on the difference of the occurrence positions even for fillers of the same form, and similar prosodic features are found between the same occurrence positions even in different forms",
    "checked": true,
    "id": "1900205d42d46f37b8afef2e9790500e73a5f242",
    "semantic_title": "analysis of the relationship between prosodic features of fillers and its forms or occurrence positions",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/fatima17_interspeech.html": {
    "title": "Cross-Subject Continuous Emotion Recognition Using Speech and Body Motion in Dyadic Interactions",
    "volume": "main",
    "abstract": "Dyadic interactions encapsulate rich emotional exchange between interlocutors suggesting a multimodal, cross-speaker and cross-dimensional continuous emotion dependency. This study explores the dynamic inter-attribute emotional dependency at the cross-subject level with implications to continuous emotion recognition based on speech and body motion cues. We propose a novel two-stage Gaussian Mixture Model mapping framework for the continuous emotion recognition problem. In the first stage, we perform continuous emotion recognition (CER) of both speakers from speech and body motion modalities to estimate activation, valence and dominance (AVD) attributes. In the second stage, we improve the first stage estimates by performing CER of the selected speaker using her/his speech and body motion modalities as well as using the estimated affective attribute(s) of the other speaker. Our experimental evaluations indicate that the second stage, cross-subject continuous emotion recognition (CSCER), provides complementary information to recognize the affective state, and delivers promising improvements for the continuous emotion recognition problem",
    "checked": true,
    "id": "bc8750893faa247bbdf5a0ce752cee8cf7a45b8e",
    "semantic_title": "cross-subject continuous emotion recognition using speech and body motion in dyadic interactions",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/elsner17_interspeech.html": {
    "title": "An Automatically Aligned Corpus of Child-Directed Speech",
    "volume": "main",
    "abstract": "Forced alignment would enable phonetic analyses of child directed speech (CDS) corpora which have existing transcriptions. But existing alignment systems are inaccurate due to the atypical phonetics of CDS. We adapt a Kaldi forced alignment system to CDS by extending the dictionary and providing it with heuristically-derived hints for vowel locations. Using this system, we present a new time-aligned CDS corpus with a million aligned segments. We manually correct a subset of the corpus and demonstrate that our system is 70% accurate. Both our automatic and manually corrected alignments are publically available at osf.io/ke44q",
    "checked": true,
    "id": "926547b837d2c424dd9fb39b9ec760452b1e5349",
    "semantic_title": "an automatically aligned corpus of child-directed speech",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bohn17_interspeech.html": {
    "title": "A Comparison of Danish Listeners' Processing Cost in Judging the Truth Value of Norwegian, Swedish, and English Sentences",
    "volume": "main",
    "abstract": "The present study used a sentence verification task to assess the processing cost involved in native Danish listeners' attempts to comprehend true/false statements spoken in Danish, Norwegian, Swedish, and English. Three groups of native Danish listeners heard 40 sentences each which were translation equivalents, and assessed the truth value of these statements. Group 1 heard sentences in Danish and Norwegian, Group 2 in Danish and Swedish, and Group 3 in Danish and English. Response time and proportion of correct responses were used as indices of processing cost. Both measures indicate that the processing cost for native Danish listeners in comprehending Danish and English statements is equivalent, whereas Norwegian and Swedish statements incur a much higher cost, both in terms of response time and correct assessments. The results are discussed with regard to the costs of inter-Scandinavian and English lingua franca communication",
    "checked": true,
    "id": "c14039a245d155a6f2bcd6e37b138d243e7f266b",
    "semantic_title": "a comparison of danish listeners' processing cost in judging the truth value of norwegian, swedish, and english sentences",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kleber17_interspeech.html": {
    "title": "On the Role of Temporal Variability in the Acquisition of the German Vowel Length Contrast",
    "volume": "main",
    "abstract": "This study is part of a larger project investigating the acquisition of stable vowel-plus-consonant timing patterns needed to convey the phonemic vowel length and the voicing contrast in German. The research is motivated by findings showing greater temporal variability in children until the age of 12. The specific aims of the current study were to test (1) whether temporal variability in the production of the vowel length contrast decreases with increasing age (in general and more so when the variability is speech rate induced) and (2) whether duration cues are perceived more categorically with increasing age. Production and perception data were obtained from eleven preschool, five school children and eleven adults. Results revealed that children produce the quantity contrast with temporal patterns that are similar to adults' patterns, although vowel duration was overall longer and variability slightly higher in faster speech and younger children. Apart from that, the two groups of children did not differ in production. In perception, however, school children's response patterns to a continuum from a long vowel to a short vowel word were in between those of adults and preschool children. Findings are discussed with respect to motor control and phonemic abstraction",
    "checked": true,
    "id": "9f16f29d3f043debab5c18cb83a51ffc1a09dcc2",
    "semantic_title": "on the role of temporal variability in the acquisition of the german vowel length contrast",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/reidy17_interspeech.html": {
    "title": "A Data-Driven Approach for Perceptually Validated Acoustic Features for Children's Sibilant Fricative Productions",
    "volume": "main",
    "abstract": "Both perceptual and acoustic studies of children's speech independently suggest that phonological contrasts are continuously refined during acquisition. This paper considers two traditional acoustic features for the ‘s'-vs.-‘sh' contrast (centroid and peak frequencies) and a novel feature learned from data, evaluating these features relative to perceptual ratings of children's productions Productions of sibilant fricatives were elicited from 16 adults and 69 preschool children. A second group of adults rated the children's productions on a visual analog scale (VAS). Each production was rated by multiple listeners; mean VAS score for each production was used as its perceptual goodness rating. For each production from the repetition task, a psychoacoustic spectrum was estimated by passing it through a filter bank that modeled the auditory periphery. From these spectra centroid and peak frequencies were computed, two traditional features for a sibilant fricative's place of articulation. A novel acoustic measure was derived by inputting the spectra to a graph-based dimensionality-reduction algorithm Simple regression analyses indicated that a greater amount of variance in the VAS scores was explained by the novel feature (adjusted R = 0.569) than by either centroid (adjusted R = 0.468) or peak frequency (adjusted R = 0.254)",
    "checked": true,
    "id": "ef173ac50b2f9e7dcba6d60615f4246550213168",
    "semantic_title": "a data-driven approach for perceptually validated acoustic features for children's sibilant fricative productions",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/xiao17_interspeech.html": {
    "title": "Proficiency Assessment of ESL Learner's Sentence Prosody with TTS Synthesized Voice as Reference",
    "volume": "main",
    "abstract": "We investigate how to assess the prosody quality of an ESL learner's spoken sentence against native speaker's natural recording or TTS synthesized voice. A spoken English utterance read by an ESL leaner is compared with the recording of a native speaker, or TTS voice. The corresponding F0 contours (with voicings) and breaks are compared at the mapped syllable level via a DTW. The correlations between the prosody patterns of learner and native speaker (or TTS voice) of the same sentence are computed after the speech rates and F0 distributions between speakers are equalized. Based upon collected native and non-native speakers' databases and correlation coefficients, we use Gaussian mixtures to model them as continuous distributions for training a two-class (native vs non-native) neural net classifier. We found that classification accuracy between using native speaker's and TTS reference is close, i.e., 91.2% vs 88.1%. To assess the prosody proficiency of an ESL learner with one sentence input, the prosody patterns of our high quality TTS is almost as effective as those of native speakers' recordings, which are more expensive and inconvenient to collect",
    "checked": true,
    "id": "4109eea7d8d456eaa5b9e2ab7841f58d6622415d",
    "semantic_title": "proficiency assessment of esl learner's sentence prosody with tts synthesized voice as reference",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17g_interspeech.html": {
    "title": "Mechanisms of Tone Sandhi Rule Application by Non-Native Speakers",
    "volume": "main",
    "abstract": "This study is the first to examine acquisition of two Mandarin tone sandhi rules by Cantonese speakers. It designs both real and different types of wug words to test whether learners may exploit a lexical or computation mechanism in tone sandhi rule application. We also statistically compared their speech production with Beijing Mandarin speakers. The results of functional data analysis showed that non-native speakers applied tone sandhi rules both to real and wug words in a similar manner, indicating that they might utilize a computation mechanism and compute the rules under phonological conditions. No significant differences in applying these two phonological rules on reading wug words also suggest no bias in the application of these two rules. However, their speech production differed from native speakers. The application of third tone sandhi rule was more categorical than native speakers in that Cantonese speakers tended to neutralize the sandhi Tone 3 more with Tone 2 produced in isolation compared to native speakers. Also, Cantonese speakers might not have applied half-third tone sandhi rule fully since they tended to raise f0 values more at the end of vowels",
    "checked": true,
    "id": "45ad0fbba59e0b1190d8bdd4d89c7800d2b8c00e",
    "semantic_title": "mechanisms of tone sandhi rule application by non-native speakers",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wiener17_interspeech.html": {
    "title": "Changes in Early L2 Cue-Weighting of Non-Native Speech: Evidence from Learners of Mandarin Chinese",
    "volume": "main",
    "abstract": "This study examined how cue-weighting of a non-native speech cue changes during early adult second language (L2) acquisition. Ten native English speaking learners of Mandarin Chinese performed a speeded AX-discrimination task during months 1, 2, and 3 of a first-year Chinese course. Results were compared to ten native Mandarin speakers. Learners' reaction time and d-prime results became more native-like after two months of classroom study but plateaued thereafter. Multidimensional scaling results showed a similar shift to more native-like cue-weighting as learners attended more to pitch direction and less to pitch height. Despite the improvements, learners' month 3 configuration of cue-weighting differed from that of native speakers; learners appeared to weight pitch end points rather than overall pitch directions. These results suggest that learners' warping of the weights of dimensions underlying the perceptual space changes rapidly during early acquisition and can plateau like other measures of L2 acquisition. Previous perceptual learning studies may have only captured initial L2 perception gains, not the learning plateau that often follows. New methods of perceptual learning, especially for tonal languages, are needed to advance learners off the plateau",
    "checked": true,
    "id": "df7023048370539ff052aa669cfa0389d58e4e5e",
    "semantic_title": "changes in early l2 cue-weighting of non-native speech: evidence from learners of mandarin chinese",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17h_interspeech.html": {
    "title": "Directing Attention During Perceptual Training: A Preliminary Study of Phonetic Learning in Southern Min by Mandarin Speakers",
    "volume": "main",
    "abstract": "Previous studies have shown that directing learners' attention during perceptual training facilitates detection and learning of unfamiliar consonant categories [1, 2]. The current study asks whether this attentional directing can also facilitate other types of phonetic learning. Monolingual Mandarin speakers were divided into two groups directed to learn either 1) the consonants or 2) the tones in an identification training task with the same set of Southern Min monosyllabic words containing the consonants /p , p, b, k , k, ɡ, tɕ , tɕ, ɕ/ and the tones (55, 33, 22, 24, 41). All subjects were also tested with an AXB discrimination task (with a distinct set of Southern Min words) before and after the training. Unsurprisingly, both groups improved accuracy in the sound type to which they attended. However, the consonant-attending group did not improve in discriminating tones after training and neither did the tone-attending group in discriminating consonants — despite both groups having equal exposure to the same training stimuli. When combined with previous results for consonant and vowel training, these results suggest that explicitly directing learners' attention has a broadly facilitative effect on phonetic learning including of tonal contrasts",
    "checked": true,
    "id": "8b0e749ea4b18f4e734e7a7e6c900ad051498771",
    "semantic_title": "directing attention during perceptual training: a preliminary study of phonetic learning in southern min by mandarin speakers",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/luo17b_interspeech.html": {
    "title": "Prosody Analysis of L2 English for Naturalness Evaluation Through Speech Modification",
    "volume": "main",
    "abstract": "This study investigates how different prosodic features affect native speakers' naturalness judgement of L2 English speech by Chinese students. Through subjective judgment by native speakers and objectively measured prosodic features, timing and pitch related prosodic features, as well as segmental goodness of pronunciation have been found to play key roles in native speakers' perception of naturalness. In order to eliminate segmental factors, we used accent conversion techniques that modify native reference speech with learners' erroneous prosodic cues without altering segmental properties. Experimental results show that without interference of segmental factors, both timing and pitch features affect naturalness of L2 speech. Timing plays a more crucial role in naturalness than pitch. Accent modification that corrects timing or pitch errors can improve naturalness of the speech",
    "checked": true,
    "id": "14c6a3f6b8d4244f0480635abbe1770289aec537",
    "semantic_title": "prosody analysis of l2 english for naturalness evaluation through speech modification",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/grigonyte17_interspeech.html": {
    "title": "Measuring Encoding Efficiency in Swedish and English Language Learner Speech Production",
    "volume": "main",
    "abstract": "We use n-gram language models to investigate how far language approximates an optimal code for human communication in terms of Information Theory [1], and what differences there are between Learner proficiency levels. Although the language of lower level learners is simpler, it is less optimal in terms of information theory, and as a consequence more difficult to process",
    "checked": true,
    "id": "8e65e010b7b401d782ca5fb1586af2c6fd27b3c9",
    "semantic_title": "measuring encoding efficiency in swedish and english language learner speech production",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hanulikova17_interspeech.html": {
    "title": "Lexical Adaptation to a Novel Accent in German: A Comparison Between German, Swedish, and Finnish Listeners",
    "volume": "main",
    "abstract": "Listeners usually adjust rapidly to unfamiliar regional and foreign accents in their native (L1) language. Non-native (L2) listeners, however, usually struggle when confronted with unfamiliar accents in their non-native language. The present study asks how native language background of L2 speakers influences lexical adjustments in a novel accent of German, in which several vowels were systematically lowered. We measured word judgments on a lexical decision task before and after exposure to a 15-min story in the novel dialect, and compared German, Swedish and Finnish listeners' performance. Swedish is a Germanic language and shares with German a number of lexical roots and a relatively large vowel inventory. Finnish is a Finno-Ugric language and differs substantially from Germanic languages in both lexicon and phonology. The results were as predicted: descriptively, all groups showed a similar pattern of adaptation to the accented speech, but only German and Swedish participants showed a significant effect. Lexical and phonological relatedness between the native and non-native languages may thus positively influence lexical adaptation in an unfamiliar accent",
    "checked": true,
    "id": "c0020847a583e8b8bb16a35f3f955d5511ad5a41",
    "semantic_title": "lexical adaptation to a novel accent in german: a comparison between german, swedish, and finnish listeners",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/fernandez17_interspeech.html": {
    "title": "Qualitative Differences in L3 Learners' Neurophysiological Response to L1 versus L2 Transfer",
    "volume": "main",
    "abstract": "Third language (L3) acquisition differs from first language (L1) and second language (L2) acquisition. There are different views on whether L1 or L2 is of primary influence on L3 acquisition in terms of transfer. This study examines differences in the event-related brain potentials (ERP) response to agreement incongruencies between L1 Spanish speakers and L3 Spanish learners, comparing response differences to incongruencies that are transferrable from the learners' L1 (Swedish), or their L2 (English). Whereas verb incongruencies, available in L3 learners' L2 but not their L1, engendered a similar response for L1 speakers and L3 learners, adjective incongruencies, available in L3 learners' L1 but not their L2, elicited responses that differed between groups: Adjective incongruencies engendered a negativity in the 450–550 ms time window for L1 speakers only. Both congruent and incongruent adjectives also engendered an enhanced P3 wave in L3 learners compared to L1 speakers. Since the P300 correlates with task-related, strategic processing, this indicates that L3 learners process grammatical features that are transferrable from their L1 in a less automatic mode than features that are transferrable from their L2. L3 learners therefore seem to benefit more from their knowledge of their L2 than their knowledge of their L1",
    "checked": true,
    "id": "916da0c79887d56210fb1390467d29e39b30a278",
    "semantic_title": "qualitative differences in l3 learners' neurophysiological response to l1 versus l2 transfer",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sjons17_interspeech.html": {
    "title": "Articulation Rate in Swedish Child-Directed Speech Increases as a Function of the Age of the Child Even When Surprisal is Controlled for",
    "volume": "main",
    "abstract": "In earlier work, we have shown that articulation rate in Swedish child-directed speech (CDS) increases as a function of the age of the child, even when utterance length and differences in articulation rate between subjects are controlled for. In this paper we show on utterance level in spontaneous Swedish speech that i) for the youngest children, articulation rate in CDS is lower than in adult-directed speech (ADS), ii) there is a significant negative correlation between articulation rate and surprisal (the negative log probability) in ADS, and iii) the increase in articulation rate in Swedish CDS as a function of the age of the child holds, even when surprisal along with utterance length and differences in articulation rate between speakers are controlled for. These results indicate that adults adjust their articulation rate to make it fit the linguistic capacity of the child",
    "checked": true,
    "id": "bfa370ba9818796cc5831e9511c2234a0de5b15b",
    "semantic_title": "articulation rate in swedish child-directed speech increases as a function of the age of the child even when surprisal is controlled for",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhang17e_interspeech.html": {
    "title": "The Relationship Between the Perception and Production of Non-Native Tones",
    "volume": "main",
    "abstract": "To further investigate the relationship between non-native tone perception and production, the present study trained Mandarin speakers to learn Cantonese lexical tones with a speech shadowing paradigm. After two weeks' training, both Mandarin speakers' Cantonese tone perception and their production had improved significantly. The overall performances in Cantonese tone perception and production are moderately correlated, but the degree of performance change after training among the two modalities shows no correlation, suggesting that non-native tone perception and production might be partially correlated, but that the improvement of the two modalities is not synchronous. A comparison between the present study and previous studies on non-native tone learning indicates that experience in lexical tone processing might be important in forming the correlation between tone perception and production. Mandarin speakers showed greater improvement in Cantonese tone perception than in production after training, indicating that second language (L2) perception might precede production. Besides, both the first language (L1) and L2 tonal systems showed an influence on Mandarin speakers' learning of Cantonese tones",
    "checked": true,
    "id": "e6048def3e9746e7768273d8a0455433cbd3de20",
    "semantic_title": "the relationship between the perception and production of non-native tones",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/marklund17_interspeech.html": {
    "title": "MMN Responses in Adults After Exposure to Bimodal and Unimodal Frequency Distributions of Rotated Speech",
    "volume": "main",
    "abstract": "The aim of the present study is to further the understanding of the relationship between perceptual categorization and exposure to different frequency distributions of sounds. Previous studies have shown that speech sound discrimination proficiency is influenced by exposure to different distributions of speech sound continua varying along one or several acoustic dimensions, both in adults and in infants. In the current study, adults were presented with either a bimodal or a unimodal frequency distribution of spectrally rotated sounds along a continuum (a vowel continuum before rotation). Categorization of the sounds, quantified as amplitude of the event-related potential (ERP) component mismatch negativity (MMN) in response to two of the sounds, was measured before and after exposure. It was expected that the bimodal group would have a larger MMN amplitude after exposure whereas the unimodal group would have a smaller MMN amplitude after exposure. Contrary to expectations, the MMN amplitude was smaller overall after exposure, and no difference was found between groups. This suggests that either the previously reported sensitivity to frequency distributions of speech sounds is not present for non-speech sounds, or the MMN amplitude is not a sensitive enough measure of categorization to detect an influence from passive exposure, or both",
    "checked": true,
    "id": "144e22740eb0e9e787c119f02486294bd196537f",
    "semantic_title": "mmn responses in adults after exposure to bimodal and unimodal frequency distributions of rotated speech",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/berisha17_interspeech.html": {
    "title": "Float Like a Butterfly Sting Like a Bee: Changes in Speech Preceded Parkinsonism Diagnosis for Muhammad Ali",
    "volume": "main",
    "abstract": "Early identification of the onset of neurological disease is critical for testing drugs or interventions to halt or slow progression. Speech production has been proposed as an early indicator of neurological impairment. However, for speech to be useful for early detection, speech changes should be measurable from uncontrolled conversational speech collected passively in natural recording environments over extended periods of time. Such longitudinal speech data sets for testing the robustness of algorithms are difficult to acquire. In this paper, we exploit YouTube interviews from Muhammad Ali from 1968 to 1981, before his 1984 diagnosis of parkinsonism. The interviews are unscripted, conversational in nature, and of varying fidelity. We measured changes in speech production from the Ali interviews and analyzed these changes relative to a coded registry of blows Mr. Ali received in each of his boxing matches over time. This provided a rich and unique opportunity to evaluate speech change as both a function of disease progression and as a function of fight history. Multivariate analyses revealed changes in prosody and articulation consistent with hypokinetic dysarthria over time, and a relationship between reduced speech intonation and the amount of time elapsed since the most recent fight preceding the interview",
    "checked": true,
    "id": "29029138c1d71d68d9d18f7a209ea032d1699bc7",
    "semantic_title": "float like a butterfly sting like a bee: changes in speech preceded parkinsonism diagnosis for muhammad ali",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/castellana17_interspeech.html": {
    "title": "Cepstral and Entropy Analyses in Vowels Excerpted from Continuous Speech of Dysphonic and Control Speakers",
    "volume": "main",
    "abstract": "There is a growing interest in Cepstral and Entropy analyses of voice samples for defining a vocal health indicator, due to their reliability in investigating both regular and irregular voice signals. The purpose of this study is to determine whether the Cepstral Peak Prominence Smoothed (CPPS) and Sample Entropy (SampEn) could differentiate dysphonic speakers from normal speakers in vowels excerpted from readings and to compare their discrimination power. Results are reported for 33 patients and 31 controls, who read a standardized phonetically balanced passage while wearing a head mounted microphone. Vowels were excerpted from recordings using Automatic Speech Recognition and, after obtaining a measure for each vowel, individual distributions and their descriptive statistics were considered for CPPS and SampEn. The Receiver Operating Curve analysis revealed that the mean of the distributions was the parameter with the highest discrimination power for both CPPS and SampEn. CPPS showed a higher diagnostic precision than SampEn, exhibiting an Area Under Curve (AUC) of 0.85 compared to 0.72. A negative correlation between the parameters was found (Spearman; ρ = -0.61), with higher SampEn corresponding to lower CPPS. The automatic method used in this study could provide support to voice monitorings in clinic and during individual's daily activities",
    "checked": true,
    "id": "a0c4adb474f7f27a14ea5623aa04ef0dac4bef56",
    "semantic_title": "cepstral and entropy analyses in vowels excerpted from continuous speech of dysphonic and control speakers",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bandini17b_interspeech.html": {
    "title": "Classification of Bulbar ALS from Kinematic Features of the Jaw and Lips: Towards Computer-Mediated Assessment",
    "volume": "main",
    "abstract": "Recent studies demonstrated that lip and jaw movements during speech may provide important information for the diagnosis of amyotrophic lateral sclerosis (ALS) and for understanding its progression. A thorough investigation of these movements is essential for the development of intelligent video- or optically-based facial tracking systems that could assist with early diagnosis and progress monitoring. In this paper, we investigated the potential for a novel and expanded set of kinematic features obtained from lips and jaw to classify articulatory data into three stages of bulbar disease progression (i.e., pre-symptomatic, early symptomatic, and late symptomatic). Feature selection methods (Relief-F and mRMR) and classification algorithm (SVM) were used for this purpose. Results showed that even with a limited number of kinematic features it was possible to obtain good classification accuracy (nearly 80%). Given the recent development of video-based markerless methods for tracking speech movements, these results provide strong rationale for supporting the development of portable and cheap systems for monitoring the orofacial function in ALS",
    "checked": true,
    "id": "c0223c0675825ff4fdbd746c34cd04a434e642eb",
    "semantic_title": "classification of bulbar als from kinematic features of the jaw and lips: towards computer-mediated assessment",
    "citation_count": 15,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/adiga17_interspeech.html": {
    "title": "Zero Frequency Filter Based Analysis of Voice Disorders",
    "volume": "main",
    "abstract": "Pitch period and amplitude perturbations are widely used parameters to discriminate normal and voice disorder speech. Instantaneous pitch period and amplitude of glottal vibrations directly from the speech waveform may not give an accurate estimation of jitter and shimmer. In this paper, the significance of epochs (glottal closure instants) and strength of excitation (SoE) derived from the zero-frequency filter (ZFF) are exploited to discriminate the voice disorder and normal speech. Pitch epoch derived from ZFF is used to compute the jitter, and SoE derived around each epoch is used compute the shimmer. The derived epoch-based features are analyzed on the some of the voice disorders like Parkinson's disease, vocal fold paralysis, cyst, and gastroesophageal reflux disease. The significance of proposed epoch-based features for discriminating normal and pathological voices is analyzed and compared with the state-of-the-art methods using a support vector machine classifier. The results show that epoch-based features performed significantly better than other methods both in clean and noisy conditions",
    "checked": true,
    "id": "5a11ab94317f32f55f875c1572137211a34d6534",
    "semantic_title": "zero frequency filter based analysis of voice disorders",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/k17_interspeech.html": {
    "title": "Hypernasality Severity Analysis in Cleft Lip and Palate Speech Using Vowel Space Area",
    "volume": "main",
    "abstract": "Vowel space area (VSA) refers to a two-dimensional area, which is bounded by lines joining F and F coordinates of vowels. In the speech of individuals with cleft lip and palate (CLP), the effect of hypernasality introduces the pole-zero pairs in the speech spectrum, which will shift the formants of a target sound. As a result, vowel space in hypernasal speech gets affected. In this work, analysis of vowel space area in normal, mild and moderate-severe hypernasality groups is analyzed and compared across the three groups. Also, the effect of hypernasality severity ratings across different phonetic contexts i.e, /p/, /t/, and /k/ is studied. The results revealed that VSA is reduced in CLP children, compared to control participants, across sustained vowels and different phonetic contexts. Compared to normal, the reduction in the vowel space is more for the moderate-severe hypernasality group than that of mild. The CLP group exhibited a trend of having larger VSA for /p/, followed by /t/, and lastly by /k/. The statistical analysis revealed overall significant difference among the three groups (p < 0.05)",
    "checked": true,
    "id": "b1cec368b31f715ba0097702bd674c24fce5a2d6",
    "semantic_title": "hypernasality severity analysis in cleft lip and palate speech using vowel space area",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/laaridh17_interspeech.html": {
    "title": "Automatic Prediction of Speech Evaluation Metrics for Dysarthric Speech",
    "volume": "main",
    "abstract": "During the last decades, automatic speech processing systems witnessed an important progress and achieved remarkable reliability. As a result, such technologies have been exploited in new areas and applications including medical practice. In disordered speech evaluation context, perceptual evaluation is still the most common method used in clinical practice for the diagnosing and the following of the condition progression of patients despite its well documented limits (such as subjectivity) In this paper, we propose an automatic approach for the prediction of dysarthric speech evaluation metrics (intelligibility, severity, articulation impairment) based on the representation of the speech acoustics in the total variability subspace based on the i-vectors paradigm. The proposed approach, evaluated on 129 French dysarthric speakers from the DesPhoAPady and VML databases, is proven to be efficient for the modeling of patient's production and capable of detecting the evolution of speech quality. Also, low RMSE and high correlation measures are obtained between automatically predicted metrics and perceptual evaluations",
    "checked": true,
    "id": "3d000e4ab1ded28447ce5e0bed818bb8e64163b7",
    "semantic_title": "automatic prediction of speech evaluation metrics for dysarthric speech",
    "citation_count": 32,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/klumpp17_interspeech.html": {
    "title": "Apkinson — A Mobile Monitoring Solution for Parkinson's Disease",
    "volume": "main",
    "abstract": "In this paper we want to present our work on a smartphone application which aims to provide a mobile monitoring solution for patients suffering from Parkinson's disease. By unobtrusively analyzing the speech signal during phone calls and with a dedicated speech test, we want to be able to determine the severity and the progression of Parkinson's disease for a patient much more frequently than it would be possible with regular check-ups The application consists of four major parts. There is a phone call detection which triggers the whole processing chain. Secondly, there is the phone call recording which has proven to be more challenging than expected. The signal analysis, another crucial component, is still in development for the phone call analysis. Additionally, the application collects several pieces of meta information about the calls to put the results into deeper context After describing how the speech signal is affected by Parkinson's disease, we sketch the overall application architecture and explain the four major parts of the current implementation in further detail. We then present the promising results achieved with the first version of a dedicated speech test. In the end, we outline how the project could receive further improvements in the future",
    "checked": true,
    "id": "6c1af28a286edc6d3671b916d017ac93df91f9a7",
    "semantic_title": "apkinson - a mobile monitoring solution for parkinson's disease",
    "citation_count": 19,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hlavnicka17_interspeech.html": {
    "title": "Dysprosody Differentiate Between Parkinson's Disease, Progressive Supranuclear Palsy, and Multiple System Atrophy",
    "volume": "main",
    "abstract": "Parkinson's disease (PD), progressive supranuclear palsy (PSP), and multiple system atrophy (MSA) are distinctive neurodegenerative disorders, which manifest similar motor features. Their differentiation is crucial but difficult. Dysfunctional speech, especially dysprosody, is a common symptom accompanying PD, PSP, and MSA from early stages. We hypothesized that automated analysis of monologue could provide speech patterns distinguishing PD, PSP, and MSA. We analyzed speech recordings of 16 patients with PSP, 20 patients with MSA, and 23 patients with PD. Our findings revealed that deviant pause production differentiated between PSP, MSA, and PD. In addition, PSP showed greater deficits in speech respiration when compared to MSA and PD. Automated analysis of connected speech is easy to administer and could provide valuable information about underlying pathology for differentiation between PSP, MSA, and PD",
    "checked": true,
    "id": "93981d25e5a4f1ea3b1b7e4dc28aafcbbc56bf32",
    "semantic_title": "dysprosody differentiate between parkinson's disease, progressive supranuclear palsy, and multiple system atrophy",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tu17b_interspeech.html": {
    "title": "Interpretable Objective Assessment of Dysarthric Speech Based on Deep Neural Networks",
    "volume": "main",
    "abstract": "Improved performance in speech applications using deep neural networks (DNNs) has come at the expense of reduced model interpretability. For consumer applications this is not a problem; however, for health applications, clinicians must be able to interpret why a predictive model made the decision that it did. In this paper, we propose an interpretable model for objective assessment of dysarthric speech for speech therapy applications based on DNNs. Our model aims to predict a general impression of the severity of the speech disorder; however, instead of directly generating a severity prediction from a high-dimensional input acoustic feature space, we add an intermediate interpretable layer that acts as a bottle-neck feature extractor and constrains the solution space of the DNNs. During inference, the model provides an estimate of severity at the output of the network and a set of explanatory features from the intermediate layer of the network that explain the final decision. We evaluate the performance of the model on a dysarthric speech dataset and show that the proposed model provides an interpretable output that is highly correlated with the subjective evaluation of Speech-Language Pathologists (SLPs)",
    "checked": true,
    "id": "5bc707d22bc491741ba43b8bbd842adf1f0efcd8",
    "semantic_title": "interpretable objective assessment of dysarthric speech based on deep neural networks",
    "citation_count": 38,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/vachhani17_interspeech.html": {
    "title": "Deep Autoencoder Based Speech Features for Improved Dysarthric Speech Recognition",
    "volume": "main",
    "abstract": "Dysarthria is a motor speech disorder, resulting in mumbled, slurred or slow speech that is generally difficult to understand by both humans and machines. Traditional Automatic Speech Recognizers (ASR) perform poorly on dysarthric speech recognition tasks. In this paper, we propose the use of deep autoencoders to enhance the Mel Frequency Cepstral Coefficients (MFCC) based features in order to improve dysarthric speech recognition. Speech from healthy control speakers is used to train an autoencoder which is in turn used to obtain improved feature representation for dysarthric speech. Additionally, we analyze the use of severity based tempo adaptation followed by autoencoder based speech feature enhancement. All evaluations were carried out on Universal Access dysarthric speech corpus. An overall absolute improvement of 16% was achieved using tempo adaptation followed by autoencoder based speech front end representation for DNN-HMM based dysarthric speech recognition",
    "checked": true,
    "id": "6d43b42317668c1ae9f2869bc1c01790fbb27820",
    "semantic_title": "deep autoencoder based speech features for improved dysarthric speech recognition",
    "citation_count": 22,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lilley17_interspeech.html": {
    "title": "Prediction of Speech Delay from Acoustic Measurements",
    "volume": "main",
    "abstract": "Speech delay is characterized by a difficulty with producing or perceiving the sounds of language in comparison to one's peers. It is a common problem in young children, occurring at a rate of about 5%. There are high rates of co-occurring problems with language, reading, learning, and social interactions, so intervention is needed for most. The Goldman-Fristoe Test of Articulation (GFTA) is a standardized tool for the assessment of consonant articulation in American English children. GFTA scores are normalized for age and can be used to help diagnose and assess speech delay. The GFTA was administered to 65 young children, a mixture of delayed children and controls. Their productions of the 39 GFTA words spoken in isolation were recorded and aligned to 3-state hidden Markov models. Seven measurements (state log likelihoods, state durations, and total duration) were extracted from each target segment in each word. From a subset of these measures, cross-validated statistical models were used to predict the children's GFTA scores and whether they were delayed. The measurements most useful for prediction came primarily from approximants /r, l/. An analysis of the predictors and discussion of the implications will be provided",
    "checked": true,
    "id": "a7e52334689b0fd6644012c54cd0e2195776361b",
    "semantic_title": "prediction of speech delay from acoustic measurements",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/li17h_interspeech.html": {
    "title": "The Frequency Range of \"The Ling Six Sounds\" in Standard Chinese",
    "volume": "main",
    "abstract": "The Ling Six Sounds\" are a range of speech sounds encompassing the speech frequencies that are widely used clinically to verify the effectiveness of hearing aid fitting in children. This study focused on the spectral features of the six sounds in Standard Chinese. We examined the frequency range of /m, u, a, i, ʂ, s/ as well as three consonants in syllables, i.e., /m(o)/, /ʂ(ʅ)/, and /s(ɿ)/. We presented the frequency distribution of these sounds. Based on this, we further proposed guidelines to improve \"the Ling Six-Sound Test\" regarding tones in Standard Chinese. We also suggested further studies in other dialects/languages spoken in China with regard to their phonological specifics",
    "checked": true,
    "id": "74756c3d2990f5ca413454274437a44d43e36d99",
    "semantic_title": "the frequency range of \"the ling six sounds\" in standard chinese",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gu17b_interspeech.html": {
    "title": "Production of Sustained Vowels and Categorical Perception of Tones in Mandarin Among Cochlear-Implanted Children",
    "volume": "main",
    "abstract": "This study investigated both production and perception of Mandarin speech, comparing two groups of 4-to-5-year-old children, a normal-hearing (NH) group and a cochlear-implanted (CI) hearing-impaired group; the perception ability of the CI group was tested under two conditions, with and without hearing aids. In the production study, the participants were asked to produce sustained vowels /a/, /i/ and /u/, on which a set of acoustic parameters were then measured. In comparison to the NH group, the CI group showed a higher F , a higher H1-H2, and a smaller acoustic space for vowels, demonstrating both phonatory and articulatory impairments. In the perception study, the identification tests of two tone-pairs in Mandarin (T1-T2 and T1-T4) were conducted, using two sets of synthetic speech stimuli varying only along F continua. All groups/conditions showed categorical effects in perception. The CI group in the unimodal condition showed little difference from normal, while in the bimodal condition the categorical effect became weaker in identifying the T1-T4 continuum, with the category boundary more biased to T4. This suggests that bimodal CI children may need more fine grain adjustments of hearing aids to take full advantage of the bimodal technology",
    "checked": true,
    "id": "fbc2f4e846616c52708e5963be13601aa318a07f",
    "semantic_title": "production of sustained vowels and categorical perception of tones in mandarin among cochlear-implanted children",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kumar17b_interspeech.html": {
    "title": "Audio Content Based Geotagging in Multimedia",
    "volume": "main",
    "abstract": "In this paper we propose methods to extract geographically relevant information in a multimedia recording using its audio content. Our method primarily is based on the fact that urban acoustic environment consists of a variety of sounds. Hence, location information can be inferred from the composition of sound events/classes present in the audio. More specifically, we adopt matrix factorization techniques to obtain semantic content of recording in terms of different sound classes. We use semi-NMF to for to do audio semantic content analysis using MFCCs. These semantic information are then combined to identify the location of recording. We show that these semantic content based geotagging can perform significantly better than state of art methods",
    "checked": true,
    "id": "c32079d7fbea75bb3844fb6d430f91edc63039b8",
    "semantic_title": "audio content based geotagging in multimedia",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/huang17d_interspeech.html": {
    "title": "Time Delay Histogram Based Speech Source Separation Using a Planar Array",
    "volume": "main",
    "abstract": "Bin-wise time delay is a valuable clue to form the time-frequency (TF) mask for speech source separation on the two-microphone array. On widely spaces microphones, however, the time delay estimation suffers from spatial aliasing. Although histogram is a simple and effective method to tackle the problem of spatial aliasing, it can not be directly applied on planar arrays. This paper proposes a histogram-based method to separate multiple speech sources on the arbitrary-size planar array, where the spatial aliasing is resisted. Time delay histogram is firstly utilized to estimate the delays of multiple sources on each microphone pair. The estimated delays on all pairs are then incorporated into an azimuth histogram by means of the pairwise combination test. From the azimuth histogram, the direction-of-arrivals (DOAs) and the number of sources are obtained. Eventually, the TF mask is determined based on the estimated DOAs. Some experiments were conducted under various conditions, confirming the superiority of the proposed method",
    "checked": true,
    "id": "150254087187331fcfaf95fed644a457842b12d1",
    "semantic_title": "time delay histogram based speech source separation using a planar array",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/pradhan17_interspeech.html": {
    "title": "Excitation Source Features for Improving the Detection of Vowel Onset and Offset Points in a Speech Sequence",
    "volume": "main",
    "abstract": "The task of detecting the vowel regions in a given speech signal is a challenging problem. Over the years, several works on accurate detection of vowel regions and the corresponding vowel onset points (VOPs) and vowel end points (VEPs) have been reported. A novel front-end feature extraction technique exploiting the temporal and spectral characteristics of the excitation source information in the speech signal is proposed in this paper to improve the detection of vowel regions, VOPs and VEPs. To do the same, a three-class classifiers (vowel, non-vowel and silence) is developed on the TIMIT database using the proposed features as well as mel-frequency cepstral coefficients (MFCC). Statistical modeling based on deep neural network has been employed for learning the parameters. Using the developed three-class classifier, a given speech sample is then forced aligned against the trained acoustic models to detect the vowel regions. The use of proposed feature results in detection of vowel regions quite different from those obtained through the MFCC. Exploiting the differences in the evidences obtained by using the two kinds of features, a technique to combine the evidences is also proposed in order to get a better estimate of the VOPs and VEPs",
    "checked": true,
    "id": "a9cddc8a99b22754bd66850063ec36d41dd161b3",
    "semantic_title": "excitation source features for improving the detection of vowel onset and offset points in a speech sequence",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gao17_interspeech.html": {
    "title": "A Contrast Function and Algorithm for Blind Separation of Audio Signals",
    "volume": "main",
    "abstract": "This paper presents a contrast function and associated algorithm for blind separation of audio signals. The contrast function is based on second-order statistics to minimize the ratio between the product of the diagonal entries and the determinant of the covariance matrix. The contrast function can be minimized by a batch and adaptive gradient descent method to formulate a blind source separation algorithm. Experimental results on realistic audio signals show that the proposed algorithm yielded comparable separation performance with benchmark algorithms for speech signals, and outperformed benchmark algorithms for music signals",
    "checked": true,
    "id": "ca9f12167aef709c67d81e8d2b0fdaf16586e653",
    "semantic_title": "a contrast function and algorithm for blind separation of audio signals",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/xu17_interspeech.html": {
    "title": "Weighted Spatial Covariance Matrix Estimation for MUSIC Based TDOA Estimation of Speech Source",
    "volume": "main",
    "abstract": "We study the estimation of time difference of arrival (TDOA) under noisy and reverberant conditions. Conventional TDOA estimation methods such as MUltiple SIgnal Classification (MUSIC) are not robust to noise and reverberation due to the distortion in the spatial covariance matrix (SCM). To address this issue, this paper proposes a robust SCM estimation method, called weighted SCM (WSCM). In the WSCM estimation, each time-frequency (TF) bin of the input signal is weighted by a TF mask which is 0 for non-speech TF bins and 1 for speech TF bins in ideal case. In practice, the TF mask takes values between 0 and 1 that are predicted by a long short term memory (LSTM) network trained from a large amount of simulated noisy and reverberant data. The use of mask weights significantly reduces the contribution of low SNR TF bins to the SCM estimation, hence improves the robustness of MUSIC. Experimental results on both simulated and real data show that we have significantly improved the robustness of MUSIC by using the weighted SCM",
    "checked": true,
    "id": "a60970e206602025ff8c3081c059f16de5fffc3d",
    "semantic_title": "weighted spatial covariance matrix estimation for music based tdoa estimation of speech source",
    "citation_count": 19,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/guo17b_interspeech.html": {
    "title": "Speaker Direction-of-Arrival Estimation Based on Frequency-Independent Beampattern",
    "volume": "main",
    "abstract": "The differential microphone array (DMA) becomes more and more popular recently. In this paper, we derive the relationship between the direction-of-arrival (DoA) and DMA's frequency-independent beampatterns. The derivation demonstrates that the DoA can be yielded by solving a trigonometric polynomial. Taking the dipoles as a special case of this relationship, we propose three methods to estimate the DoA based on the dipoles. However, we find these methods are vulnerable to the axial directions under the reverberation environment. Fortunately, they can complement each other owing to their robustness to different angles. Hence, to increase the robustness to the reverberation, we proposed another new approach by combining the advantages of these three dipole-based methods for the speaker DoA estimation. Both simulations and experiments show that the proposed method not only outperforms the traditional methods for small aperture array but also is much more computationally efficient with avoiding the spatial spectrum search",
    "checked": true,
    "id": "6ac0176a1765f605048d76cccb9df302b2ebc2ec",
    "semantic_title": "speaker direction-of-arrival estimation based on frequency-independent beampattern",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wang17i_interspeech.html": {
    "title": "A Mask Estimation Method Integrating Data Field Model for Speech Enhancement",
    "volume": "main",
    "abstract": "In most approaches based on computational auditory scene analysis (CASA), the ideal binary mask (IBM) is often used for noise reduction. However, it is almost impossible to obtain the IBM result. The error in IBM estimation may greatly violate smooth evolution nature of speech because of the energy absence in many speech-dominated time-frequency (T-F) units. To reduce the error, the ideal ratio mask (IRM) via modeling the spatial dependencies of speech spectrum is used as an optimal target mask because the predictive ratio mask is less sensitive to the error than the predictive binary mask. In this paper, we introduce a data field (DF) to model the spatial dependencies of the cochleagram for obtaining the ratio mask. Firstly, initial T-F units of noise and speech are obtained from noisy speech. Then we can calculate the forms of the potentials of noise and speech. Subsequently, their optimal potentials which reflect their respective distribution of potential field are obtained by the optimal influence factors of speech and noise. Finally, we exploit the potentials of speech and noise to obtain the ratio mask. Experimental results show that the proposed method can obtain a better performance than the reference methods in speech quality",
    "checked": true,
    "id": "b72704543d544437df050d470d4349bc7bb86c76",
    "semantic_title": "a mask estimation method integrating data field model for speech enhancement",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/shannon17_interspeech.html": {
    "title": "Improved End-of-Query Detection for Streaming Speech Recognition",
    "volume": "main",
    "abstract": "In many streaming speech recognition applications such as voice search it is important to determine quickly and accurately when the user has finished speaking their query. A conventional approach to this task is to declare end-of-query whenever a fixed interval of silence is detected by a voice activity detector (VAD) trained to classify each frame as speech or silence. However silence detection and end-of-query detection are fundamentally different tasks, and the criterion used during VAD training may not be optimal. In particular the conventional approach ignores potential acoustic cues such as filler sounds and past speaking rate which may indicate whether a given pause is temporary or query-final. In this paper we present a simple modification to make the conventional VAD training criterion more closely related to end-of-query detection. A unidirectional long short-term memory architecture allows the system to remember past acoustic events, and the training criterion incentivizes the system to learn to use any acoustic cues relevant to predicting future user intent. We show experimentally that this approach improves latency at a given accuracy by around 100 ms for end-of-query detection for voice search",
    "checked": true,
    "id": "5db8d3d370107da36241039a0b1996946594b135",
    "semantic_title": "improved end-of-query detection for streaming speech recognition",
    "citation_count": 36,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/he17_interspeech.html": {
    "title": "Using Approximated Auditory Roughness as a Pre-Filtering Feature for Human Screaming and Affective Speech AED",
    "volume": "main",
    "abstract": "Detecting human screaming, shouting, and other verbal manifestations of fear and anger are of great interest to security Audio Event Detection (AED) systems. The Internet of Things (IoT) approach allows wide-covering, powerful AED systems to be distributed across the Internet. But a good feature to pre-filter the audio is critical to these systems. This work evaluates the potential of detecting screaming and affective speech using Auditory Roughness and proposes a very light-weight approximation method. Our approximation uses a similar amount of Multiple Add Accumulate (MAA) compared to short-term energy (STE), and at least 10× less MAA than MFCC. We evaluated the performance of our approximated roughness on the Mandarin Affective Speech corpus and a subset of the Youtube AudioSet for screaming against other low-complexity features. We show that our approximated roughness returns higher accuracy",
    "checked": true,
    "id": "8d4575891b63e7b0442f1edd419893dd54f4825f",
    "semantic_title": "using approximated auditory roughness as a pre-filtering feature for human screaming and affective speech aed",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zegers17_interspeech.html": {
    "title": "Improving Source Separation via Multi-Speaker Representations",
    "volume": "main",
    "abstract": "Lately there have been novel developments in deep learning towards solving the cocktail party problem. Initial results are very promising and allow for more research in the domain. One technique that has not yet been explored in the neural network approach to this task is speaker adaptation. Intuitively, information on the speakers that we are trying to separate seems fundamentally important for the speaker separation task. However, retrieving this speaker information is challenging since the speaker identities are not known a priori and multiple speakers are simultaneously active. There is thus some sort of chicken and egg problem. To tackle this, source signals and i-vectors are estimated alternately. We show that blind multi-speaker adaptation improves the results of the network and that (in our case) the network is not capable of adequately retrieving this useful speaker information itself",
    "checked": true,
    "id": "36cea0cb9f0209fd13968b21deffb26ca9d694d8",
    "semantic_title": "improving source separation via multi-speaker representations",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/yang17b_interspeech.html": {
    "title": "Multiple Sound Source Counting and Localization Based on Spatial Principal Eigenvector",
    "volume": "main",
    "abstract": "Multiple sound source localization remains a challenging issue due to the interaction between sources. Although traditional approaches can locate multiple sources effectively, most of them require the number of sound sources as a priori knowledge. However, the number of sound sources is generally unknown in practical applications. To overcome this problem, a spatial principal eigenvector based approach is proposed to estimate the number and the direction of arrivals (DOAs) of multiple speech sources. Firstly, a time-frequency (TF) bin weighting scheme is utilized to select the TF bins dominated by single source. Then, for these selected bins, the spatial principal eigenvectors are extracted to construct a contribution function which is used to simultaneously estimate the number of sources and corresponding coarse DOAs. Finally, the coarse DOA estimations are refined by iteratively optimizing the assignment of selected TF bins to each source. Experimental results validate that the proposed approach yields favorable performance for multiple sound source counting and localization in the environment with different levels of noise and reverberation",
    "checked": true,
    "id": "80acfd55ff884060559685bc69e584e4271bd6ec",
    "semantic_title": "multiple sound source counting and localization based on spatial principal eigenvector",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/karthik17_interspeech.html": {
    "title": "Subband Selection for Binaural Speech Source Localization",
    "volume": "main",
    "abstract": "We consider the task of speech source localization using binaural cues, namely interaural time and level difference (ITD & ILD). A typical approach is to process binaural speech using gammatone filters and calculate frame-level ITD and ILD in each subband. The ITD, ILD and their combination (ITLD) in each subband are statistically modelled using Gaussian mixture models for every direction during training. Given a binaural test-speech, the source is localized using maximum likelihood criterion assuming that the binaural cues in each subband are independent. We, in this work, investigate the robustness of each subband for localization and compare their performance against the full-band scheme with 32 gammatone filters. We propose a subband selection procedure using the training data where subbands are rank ordered based on their localization performance. Experiments on Subject 003 from the CIPIC database reveal that, for high SNRs, the ITD and ITLD of just one subband centered at 296Hz is sufficient to yield localization accuracy identical to that of the full-band scheme with a test-speech of duration 1sec. At low SNRs, in case of ITD, the selected subbands are found to perform better than the full-band scheme",
    "checked": true,
    "id": "26f2c330f2de2a9b57bad9b268c85d9a77b1742d",
    "semantic_title": "subband selection for binaural speech source localization",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17i_interspeech.html": {
    "title": "Unmixing Convolutive Mixtures by Exploiting Amplitude Co-Modulation: Methods and Evaluation on Mandarin Speech Recordings",
    "volume": "main",
    "abstract": "This paper presents and evaluates two frequency-domain methods for multi-channel sound source separation. The sources are assumed to couple to the microphones with unknown room responses. Independent component analysis (ICA) is applied in the frequency domain to obtain maximally independent amplitude envelopes (AEs) at every frequency. Due to the nature of ICA, the AEs across frequencies need to be de-permuted. To this end, we seek to assign AEs to the same source solely based on the correlation in their magnitude variation against time. The resulted time-varying spectra are inverse Fourier transformed to synthesize separated signals. Objective evaluation showed that both methods achieve a signal-to-interference ratio (SIR) that is comparable to Mazur et al (2013). In addition, we created spoken Mandarin materials and recruited age-matched subjects to perform word-by-word transcription. Results showed that, first, speech intelligibility significantly improved after unmixing. Secondly, while both methods achieved similar SIR, the subjects preferred to listen to the results that were post-processed to ensure a speech-like spectral shape; the mean opinion scores were 2.9 vs. 4.3 (out of 5) between the two methods. The present results may provide suggestions regarding deployment of the correlation-based source separation algorithms into devices with limited computational resources",
    "checked": true,
    "id": "802b893538b19ab4b1654ce4633bb8deeffd56df",
    "semantic_title": "unmixing convolutive mixtures by exploiting amplitude co-modulation: methods and evaluation on mandarin speech recordings",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tao17_interspeech.html": {
    "title": "Bimodal Recurrent Neural Network for Audiovisual Voice Activity Detection",
    "volume": "main",
    "abstract": "Voice activity detection (VAD) is an important preprocessing step in speech-based systems, especially for emerging hand-free intelligent assistants. Conventional VAD systems relying on audio-only features are normally impaired by noise in the environment. An alternative approach to address this problem is audiovisual VAD (AV-VAD) systems. Modeling timing dependencies between acoustic and visual features is a challenge in AV-VAD. This study proposes a bimodal recurrent neural network (RNN) which combines audiovisual features in a principled, unified framework, capturing the timing dependency within modalities and across modalities. Each modality is modeled with separate bidirectional long short-term memory (BLSTM) networks. The output layers are used as input of another BLSTM network. The experimental evaluation considers a large audiovisual corpus with clean and noisy recordings to assess the robustness of the approach. The proposed approach outperforms audio-only VAD by 7.9% (absolute) under clean/ideal conditions (i.e., high definition (HD) camera, close-talk microphone). The proposed solution outperforms the audio-only VAD system by 18.5% (absolute) when the conditions are more challenging (i.e., camera and microphone from a tablet with noise in the environment). The proposed approach shows the best performance and robustness across a varieties of conditions, demonstrating its potential for real-world applications",
    "checked": true,
    "id": "5166596f739b20d59b2489910d635bea341e91c2",
    "semantic_title": "bimodal recurrent neural network for audiovisual voice activity detection",
    "citation_count": 24,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/maas17_interspeech.html": {
    "title": "Domain-Specific Utterance End-Point Detection for Speech Recognition",
    "volume": "main",
    "abstract": "The task of automatically detecting the end of a device-directed user request is particularly challenging in case of switching short command and long free-form utterances. While low-latency end-pointing configurations typically lead to good user experiences in the case of short requests, such as \"play music\", it can be too aggressive in domains with longer free-form queries, where users tend to pause noticeably between words and hence are easily cut off prematurely. We previously proposed an approach for accurate end-pointing by continuously estimating pause duration features over all active recognition hypotheses. In this paper, we study the behavior of these pause duration features and infer domain-dependent parametrizations. We furthermore propose to adapt the end-pointer aggressiveness on-the-fly by comparing the Viterbi scores of active short command vs. long free-form decoding hypotheses. The experimental evaluation evidences a 18% relative reduction in word error rate on free-form requests while maintaining low latency on short queries",
    "checked": true,
    "id": "0c546c954dc6f1656690e76237a4831796943c63",
    "semantic_title": "domain-specific utterance end-point detection for speech recognition",
    "citation_count": 14,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kothapally17_interspeech.html": {
    "title": "Speech Detection and Enhancement Using Single Microphone for Distant Speech Applications in Reverberant Environments",
    "volume": "main",
    "abstract": "It is well known that in reverberant environments, the human auditory system has the ability to pre-process reverberant signals to compensate for reflections and obtain effective cues for improved recognition. In this study, we propose such a preprocessing technique for combined detection and enhancement of speech using a single microphone in reverberant environments for distant speech applications. The proposed system employs a framework where the target speech is synthesized using continuous auditory masks estimated from sub-band signals. Linear gammatone analysis/synthesis filter banks are used as an auditory model for sub-band processing. The performance of the proposed system is evaluated on the UT-DistantReverb corpus which consists of speech recorded in a reverberant racquetball court (T60~9000 msec). The current system shows an average improvement of 15% STNR over an existing single-channel dereverberation algorithm and 17% improvement in detecting speech frames over G729B, SOHN & Combo-SAD unsupervised speech activity detectors on actual reverberant and noisy environments",
    "checked": true,
    "id": "391240d9922d6c65be7872b211089c0d9ea49406",
    "semantic_title": "speech detection and enhancement using single microphone for distant speech applications in reverberant environments",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wu17d_interspeech.html": {
    "title": "A Post-Filtering Approach Based on Locally Linear Embedding Difference Compensation for Speech Enhancement",
    "volume": "main",
    "abstract": "This paper presents a novel difference compensation post-filtering approach based on the locally linear embedding (LLE) algorithm for speech enhancement (SE). The main goal of the proposed post-filtering approach is to further suppress residual noises in SE-processed signals to attain improved speech quality and intelligibility. The proposed system can be divided into offline and online stages. In the offline stage, we prepare paired differences: the estimated difference of SE-processed speech; noisy speech and the ground-truth difference of clean speech; noisy speech. In the online stage, on the basis of estimated difference of a test utterance, we first predict the corresponding ground-truth difference based on the LLE algorithm, and then compensate the noisy speech with the predicted difference. In this study, we integrate a deep denoising autoencoder (DDAE) SE method with the proposed LLE-based difference compensation post-filtering approach. The experiment results reveal that the proposed post-filtering approach obviously enhanced the speech quality and intelligibility of the DDAE-based SE-processed speech in different noise types and signal-to-noise-ratio levels",
    "checked": true,
    "id": "baa5db3fb095404a2523c07e981330a78e08e861",
    "semantic_title": "a post-filtering approach based on locally linear embedding difference compensation for speech enhancement",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhang17f_interspeech.html": {
    "title": "Multi-Target Ensemble Learning for Monaural Speech Separation",
    "volume": "main",
    "abstract": "Speech separation can be formulated as a supervised learning problem where a machine is trained to cast the acoustic features of the noisy speech to a time-frequency mask, or the spectrum of the clean speech. These two categories of speech separation methods can be generally referred as the masking-based and the mapping-based methods, but none of them can perfectly estimate the clean speech, since any target can only describe a part of the characteristics of the speech. However, the estimated masks and speech spectrum can, sometimes, be complementary as the speech is described from different perspectives. In this paper, by adopting an ensemble framework, a multi-target deep neural network (DNN) based method is proposed, which combines the masking-based and the mapping-based strategies, and the DNN is trained to jointly estimate the time-frequency masks and the clean spectrum. We show that as expected the mask and speech spectrum based targets yield partly complementary estimates, and the separation performance can be improved by merging these estimates. Furthermore, a merging model trained jointly with the multi-target DNN is developed. Experimental results indicate that the proposed multi-target DNN based method outperforms the DNN based algorithm which optimizes a single target",
    "checked": true,
    "id": "2cc181929da88128a9648121414e65b74144d29f",
    "semantic_title": "multi-target ensemble learning for monaural speech separation",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ogawa17_interspeech.html": {
    "title": "Improved Example-Based Speech Enhancement by Using Deep Neural Network Acoustic Model for Noise Robust Example Search",
    "volume": "main",
    "abstract": "Example-based speech enhancement is a promising single-channel approach for coping with highly nonstationary noise. Given a noisy speech input, it first searches in a noisy speech corpus for the noisy speech examples that best match the input. Then, it concatenates the clean speech examples that are paired with the matched noisy examples to obtain an estimate of the underlying clean speech component in the input. The quality of the enhanced speech depends on how accurate an example search can be performed given a noisy speech input. The example search is conventionally performed using a Gaussian mixture model (GMM) with mel-frequency cepstral coefficient features (MFCCs). To improve the noise robustness of the GMM-based example search, instead of using noise sensitive MFCCs, we have proposed using bottleneck features (BNFs), which are extracted from a deep neural network-based acoustic model (DNN-AM) built for automatic speech recognition. In this paper, instead of using a GMM with noise robust BNFs, we propose the direct use of a DNN-AM in the example search to further improve its noise robustness. Experimental results on the Aurora4 corpus show that the DNN-AM-based example search steadily improves the enhanced speech quality compared with the GMM-based example search using BNFs",
    "checked": true,
    "id": "ba7b54c2ea8699e2dddd0c65536e868e6b169eb8",
    "semantic_title": "improved example-based speech enhancement by using deep neural network acoustic model for noise robust example search",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gelderblom17_interspeech.html": {
    "title": "Subjective Intelligibility of Deep Neural Network-Based Speech Enhancement",
    "volume": "main",
    "abstract": "Recent literature indicates increasing interest in deep neural networks for use in speech enhancement systems. Currently, these systems are mostly evaluated through objective measures of speech quality and/or intelligibility. Subjective intelligibility evaluations of these systems have so far not been reported. In this paper we report the results of a speech recognition test with 15 participants, where the participants were asked to pick out words in background noise before and after enhancement using a common deep neural network approach. We found that, although the objective measure STOI predicts that intelligibility should improve or at the very least stay the same, the speech recognition threshold, which is a measure of intelligibility, deteriorated by 4 dB. These results indicate that STOI is not a good predictor for the subjective intelligibility of deep neural network-based speech enhancement systems. We also found that the postprocessing technique of global variance normalisation does not significantly affect subjective intelligibility",
    "checked": true,
    "id": "b2cc31f0b31b83aa117e52cce5f9f3a308fd0fc1",
    "semantic_title": "subjective intelligibility of deep neural network-based speech enhancement",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/koutsogiannaki17_interspeech.html": {
    "title": "Real-Time Modulation Enhancement of Temporal Envelopes for Increasing Speech Intelligibility",
    "volume": "main",
    "abstract": "In this paper, a novel approach is introduced for performing real-time speech modulation enhancement to increase speech intelligibility in noise. The proposed modulation enhancement technique operates independently in the frequency and time domains. In the frequency domain, a compression function is used to perform energy reallocation within a frame. This compression function contains novel scaling operations to ensure speech quality. In the time domain, a mathematical equation is introduced to reallocate energy from the louder to the quieter parts of the speech. This proposed mathematical equation ensures that the long-term energy of the speech is preserved independently of the amount of compression, hence gaining full control of the time-energy reallocation in real-time. Evaluations on intelligibility and quality show that the suggested approach increases the intelligibility of speech while maintaining the overall energy and quality of the speech signal",
    "checked": true,
    "id": "1dfe88d2424116b38af11813fae2c547ab3e363f",
    "semantic_title": "real-time modulation enhancement of temporal envelopes for increasing speech intelligibility",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hirsch17_interspeech.html": {
    "title": "On the Influence of Modifying Magnitude and Phase Spectrum to Enhance Noisy Speech Signals",
    "volume": "main",
    "abstract": "Neural networks have proven their ability to be usefully applied as component of a speech enhancement system. This is based on the known feature of neural nets to map regions inside a feature space to other regions. It can be taken to map noisy magnitude spectra to clean spectra. This way the net can be used to substitute an adaptive filtering in the spectral domain. We set up such a system and compared its performance against a known adaptive filtering approach in terms of speech quality and in terms of recognition rate. It is a still not fully answered question how far the speech quality can be enhanced by modifying not only the magnitude but also the spectral phase and how this phase modification could be realized. Before trying to use a neural network for a possible modification of the phase spectrum we ran a set of oracle experiments to find out how far the quality can be improved by modifying the magnitude and/or the phase spectrum in voiced segments. It turns out that the simultaneous modification of magnitude and phase spectrum has the potential for a considerable improvement of the speech quality in comparison to modifying the magnitude or the phase only",
    "checked": true,
    "id": "8753d5ac25e686d4485c79e1981e21530205b3a4",
    "semantic_title": "on the influence of modifying magnitude and phase spectrum to enhance noisy speech signals",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/rehr17_interspeech.html": {
    "title": "MixMax Approximation as a Super-Gaussian Log-Spectral Amplitude Estimator for Speech Enhancement",
    "volume": "main",
    "abstract": "For single-channel speech enhancement, most commonly, the noisy observation is described as the sum of the clean speech signal and the noise signal. For machine learning based enhancement schemes where speech and noise are modeled in the log-spectral domain, however, the log-spectrum of the noisy observation can be described as the maximum of the speech and noise log-spectrum to simplify statistical inference. This approximation is referred to as MixMax model or log-max approximation. In this paper, we show how this approximation can be used in combination with non-trained, blind speech and noise power estimators derived in the spectral domain. Our findings allow to interpret the MixMax based clean speech estimator as a super-Gaussian log-spectral amplitude estimator. This MixMax based estimator is embedded in a pre-trained speech enhancement scheme and compared to a log-spectral amplitude estimator based on an additive mixing model. Instrumental measures indicate that the MixMax based estimator causes less musical tones while it virtually yields the same quality for the enhanced speech signal",
    "checked": true,
    "id": "aff1bce79a8703cca364d5abb35c42e254dabf28",
    "semantic_title": "mixmax approximation as a super-gaussian log-spectral amplitude estimator for speech enhancement",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/marxer17_interspeech.html": {
    "title": "Binary Mask Estimation Strategies for Constrained Imputation-Based Speech Enhancement",
    "volume": "main",
    "abstract": "In recent years, speech enhancement by analysis-resynthesis has emerged as an alternative to conventional noise filtering approaches. Analysis-resynthesis replaces noisy speech with a signal that has been reconstructed from a clean speech model. It can deliver high-quality signals with no residual noise, but at the expense of losing information from the original signal that is not well-represented by the model. A recent compromise solution, called constrained resynthesis, solves this problem by only resynthesising spectro-temporal regions that are estimated to be masked by noise (conditioned on the evidence in the unmasked regions). In this paper we first extend the approach by: i) introducing multi-condition training and a deep discriminative model for the analysis stage; ii) introducing an improved resynthesis model that captures within-state cross-frequency dependencies. We then extend the previous stationary-noise evaluation by using real domestic audio noise from the CHiME-2 evaluation. We compare various mask estimation strategies while varying the degree of constraint by tuning the threshold for reliable speech detection. PESQ and log-spectral distance measures show that although mask estimation remains a challenge, it is only necessary to estimate a few reliable signal regions in order to achieve performance close to that achieved with an optimal oracle mask",
    "checked": true,
    "id": "dd82e1ba42c60ffb749031a805bf075bdb1d2521",
    "semantic_title": "binary mask estimation strategies for constrained imputation-based speech enhancement",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/park17c_interspeech.html": {
    "title": "A Fully Convolutional Neural Network for Speech Enhancement",
    "volume": "main",
    "abstract": "The presence of babble noise degrades hearing intelligibility of human speech greatly. However, removing the babble without creating artifacts in human speech is a challenging task in a low SNR environment. Here, we sought to solve the problem by finding a ‘mapping' between noisy speech spectra and clean speech spectra via supervised learning. Specifically, we propose using fully Convolutional Neural Networks, which consist of lesser number of parameters than fully connected networks. The proposed network, Redundant Convolutional Encoder Decoder (R-CED), demonstrates that a convolutional network can be 12 times smaller than a recurrent network and yet achieves better performance, which shows its applicability for an embedded system",
    "checked": true,
    "id": "9ed8e2f6c338f4e0d1ab0d8e6ab8b836ea66ae95",
    "semantic_title": "a fully convolutional neural network for speech enhancement",
    "citation_count": 315,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/li17i_interspeech.html": {
    "title": "Speech Enhancement Using Non-Negative Spectrogram Models with Mel-Generalized Cepstral Regularization",
    "volume": "main",
    "abstract": "Spectral domain speech enhancement algorithms based on non-negative spectrogram models such as non-negative matrix factorization (NMF) and non-negative matrix factor deconvolution are powerful in terms of signal recovery accuracy, however they do not directly lead to an enhancement in the feature domain (e.g., cepstral domain) or in terms of perceived quality. We have previously proposed a method that makes it possible to enhance speech in the spectral and cepstral domains simultaneously. Although this method was shown to be effective, the devised algorithm was computationally demanding. This paper proposes yet another formulation that allows for a fast implementation by replacing the regularization term with a divergence measure between the NMF model and the mel-generalized cepstral (MGC) representation of the target spectrum. Since the MGC is an auditory-motivated representation of an audio signal widely used in parametric speech synthesis, we also expect the proposed method to have an effect in enhancing the perceived quality. Experimental results revealed the effectiveness of the proposed method in terms of both the signal-to-distortion ratio and the cepstral distance",
    "checked": true,
    "id": "3100d47952f7a02abf1748c6b8a916b9f7638737",
    "semantic_title": "speech enhancement using non-negative spectrogram models with mel-generalized cepstral regularization",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/websdale17_interspeech.html": {
    "title": "A Comparison of Perceptually Motivated Loss Functions for Binary Mask Estimation in Speech Separation",
    "volume": "main",
    "abstract": "This work proposes and compares perceptually motivated loss functions for deep learning based binary mask estimation for speech separation. Previous loss functions have focused on maximising classification accuracy of mask estimation but we now propose loss functions that aim to maximise the hit minus false-alarm (HIT-FA) rate which is known to correlate more closely to speech intelligibility. The baseline loss function is binary cross-entropy (CE), a standard loss function used in binary mask estimation, which maximises classification accuracy. We propose first a loss function that maximises the HIT-FA rate instead of classification accuracy. We then propose a second loss function that is a hybrid between CE and HIT-FA, providing a balance between classification accuracy and HIT-FA rate. Evaluations of the perceptually motivated loss functions with the GRID database show improvements to HIT-FA rate and ESTOI across babble and factory noises. Further tests then explore application of the perceptually motivated loss functions to a larger vocabulary dataset",
    "checked": true,
    "id": "999eab69e601fb63c06fbe3b17ddd6bc200f9a79",
    "semantic_title": "a comparison of perceptually motivated loss functions for binary mask estimation in speech separation",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/michelsanti17_interspeech.html": {
    "title": "Conditional Generative Adversarial Networks for Speech Enhancement and Noise-Robust Speaker Verification",
    "volume": "main",
    "abstract": "Improving speech system performance in noisy environments remains a challenging task, and speech enhancement (SE) is one of the effective techniques to solve the problem. Motivated by the promising results of generative adversarial networks (GANs) in a variety of image processing tasks, we explore the potential of conditional GANs (cGANs) for SE, and in particular, we make use of the image processing framework proposed by Isola et al. [1] to learn a mapping from the spectrogram of noisy speech to an enhanced counterpart. The SE cGAN consists of two networks, trained in an adversarial manner: a generator that tries to enhance the input noisy spectrogram, and a discriminator that tries to distinguish between enhanced spectrograms provided by the generator and clean ones from the database using the noisy spectrogram as a condition. We evaluate the performance of the cGAN method in terms of perceptual evaluation of speech quality (PESQ), short-time objective intelligibility (STOI), and equal error rate (EER) of speaker verification (an example application). Experimental results show that the cGAN method overall outperforms the classical short-time spectral amplitude minimum mean square error (STSA-MMSE) SE algorithm, and is comparable to a deep neural network-based SE approach (DNN-SE)",
    "checked": true,
    "id": "36b3865f944c74c6d782c26dfe7be04ef9664a67",
    "semantic_title": "conditional generative adversarial networks for speech enhancement and noise-robust speaker verification",
    "citation_count": 200,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/qian17b_interspeech.html": {
    "title": "Speech Enhancement Using Bayesian Wavenet",
    "volume": "main",
    "abstract": "In recent years, deep learning has achieved great success in speech enhancement. However, there are two major limitations regarding existing works. First, the Bayesian framework is not adopted in many such deep-learning-based algorithms. In particular, the prior distribution for speech in the Bayesian framework has been shown useful by regularizing the output to be in the speech space, and thus improving the performance. Second, the majority of the existing methods operate on the frequency domain of the noisy speech, such as spectrogram and its variations. The clean speech is then reconstructed using the approach of overlap-add, which is limited by its inherent performance upper bound. This paper presents a Bayesian speech enhancement framework, called BaWN (Bayesian WaveNet), which directly operates on raw audio samples. It adopts the recently announced WaveNet, which is shown to be effective in modeling conditional distributions of speech samples while generating natural speech. Experiments show that BaWN is able to recover clean and natural speech",
    "checked": true,
    "id": "5e7481a1254b02f220b3c2b61397309193de0fba",
    "semantic_title": "speech enhancement using bayesian wavenet",
    "citation_count": 83,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhang17g_interspeech.html": {
    "title": "Binaural Reverberant Speech Separation Based on Deep Neural Networks",
    "volume": "main",
    "abstract": "Supervised learning has exhibited great potential for speech separation in recent years. In this paper, we focus on separating target speech in reverberant conditions from binaural inputs using supervised learning. Specifically, deep neural network (DNN) is constructed to map from both spectral and spatial features to a training target. For spectral features extraction, we first convert binaural inputs into a single signal by applying a fixed beamformer. A new spatial feature is proposed and extracted to complement spectral features. The training target is the recently suggested ideal ratio mask (IRM). Systematic evaluations and comparisons show that the proposed system achieves good separation performance and substantially outperforms existing algorithms under challenging multi-source and reverberant environments",
    "checked": true,
    "id": "d8e845ab0c81ff033ca8b36f4e57617fe216fad1",
    "semantic_title": "binaural reverberant speech separation based on deep neural networks",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zorila17_interspeech.html": {
    "title": "On the Quality and Intelligibility of Noisy Speech Processed for Near-End Listening Enhancement",
    "volume": "main",
    "abstract": "Most current techniques for near-end speech intelligibility enhancement have focused on processing clean input signals, however, in realistic environments, the input is often noisy. Processing noisy speech for intelligibility enhancement using algorithms developed for clean signals can lower the perceptual quality of the samples when they are listened in quiet. Here we address the quality loss in these conditions by combining noise reduction with a multi-band version of a state-of-the-art intelligibility enhancer for clean speech that is based on spectral shaping and dynamic range compression (SSDRC). Subjective quality and intelligibility assessments with noisy input speech showed that: (a) In quiet near-end conditions, the proposed system outperformed the baseline SSDRC in terms of Mean Opinion Score (MOS); (b) In speech-shaped near-end noise, the proposed system improved the intelligibility of unprocessed speech by a factor larger than three at the lowest tested signal-to-noise ratio (SNR) however, overall, it yielded lower recognition scores than the standard SSDRC",
    "checked": true,
    "id": "e30c44c73f4fc930c39c0e1e921578069174d0d7",
    "semantic_title": "on the quality and intelligibility of noisy speech processed for near-end listening enhancement",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/meermeier17_interspeech.html": {
    "title": "Applications of the BBN Sage Speech Processing Platform",
    "volume": "main",
    "abstract": "As a follow-up to our paper at Interspeech 2016 [1], we propose to showcase various applications that now all use BBN's Sage Speech Processing Platform, demonstrating the platform's versatility and ease of integration In particular, we will showcase 1) BBN TransTalk: A turn-based speech-to-speech translation program running entirely on an Android smartphone, alongside a custom 3D-printed peripheral for it. 2) A continuous transcription and translation application running on a Raspberry Pi 3) An offline OCR application utilizing Sage, running on a COTS Windows laptop",
    "checked": true,
    "id": "b56568e0f0d4c79ffe190871c74ac2471e884735",
    "semantic_title": "applications of the bbn sage speech processing platform",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/cernak17_interspeech.html": {
    "title": "Bob Speaks Kaldi",
    "volume": "main",
    "abstract": "This paper introduces and demonstrates Kaldi integration into Bob signal-processing and machine learning toolbox. The motivation for this integration is two-fold. Firstly, Bob benefits from using advanced speech processing tools developed in Kaldi. Secondly, Kaldi benefits from using complementary Bob modules, such as modulation-based VAD with an adaptive thresholding. In addition, Bob is designed as an open science tool, and this integration might offer to the Kaldi speech community a framework for better reproducibility of state-of-the-art research results",
    "checked": true,
    "id": "bd12e13b0177bb36b784048d45bf3c406234f22f",
    "semantic_title": "bob speaks kaldi",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lenarczyk17_interspeech.html": {
    "title": "Real Time Pitch Shifting with Formant Structure Preservation Using the Phase Vocoder",
    "volume": "main",
    "abstract": "Pitch shifting in speech is presented based on the use of the phase vocoder in combination with spectral whitening and envelope reconstruction, applied respectively before and after the transformation. A band preservation technique is introduced to contain quality degradation when downscaling the pitch. The transposition ratio is fixed in advance by selecting analysis and synthesis window sizes. Real time performance is demonstrated for window sizes having adequate factorization required by fast Fourier transformation",
    "checked": true,
    "id": "a7aa8e9d101ba6a42e376cd71870c84f4846e821",
    "semantic_title": "real time pitch shifting with formant structure preservation using the phase vocoder",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chennupati17_interspeech.html": {
    "title": "A Signal Processing Approach for Speaker Separation Using SFF Analysis",
    "volume": "main",
    "abstract": "Multi-speaker separation is necessary to increase intelligibility of speech signals or to improve accuracy of speech recognition systems. Ideal binary mask (IBM) has set a gold standard for speech separation by suppressing the undesired speakers and also by increasing intelligibility of the desired speech. In this work, single frequency filtering (SFF) analysis is used to estimate the mask closer to IBM for speaker separation. The SFF analysis gives good temporal resolution for extracting features such as glottal closure instants (GCIs), and high spectral resolution for resolving harmonics. The temporal resolution in SFF gives impulse locations, which are used to calculate the time delay. The delay compensation between two microphone signals reinforces the impulses corresponding to one of the speakers. The spectral resolution of the SFF is exploited to estimate the masks using the SFF magnitude spectra on the enhanced impulse-like sequence corresponding to one of the speakers. The estimated mask is used to refine the SFF magnitude. The refined SFF magnitude along with the phase of the mixed microphone signal is used to obtain speaker separation. Performance of proposed algorithm is demonstrated using multi-speaker data collected in a real room environment",
    "checked": true,
    "id": "361ba6529e3f8e98d49a6b6c3734a1da4ce25940",
    "semantic_title": "a signal processing approach for speaker separation using sff analysis",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/stemmer17_interspeech.html": {
    "title": "Speech Recognition and Understanding on Hardware-Accelerated DSP",
    "volume": "main",
    "abstract": "A smart home controller that responds to natural language input is demonstrated on an Intel embedded processor. This device contains two DSP cores and a neural network co-processor which share 4MB SRAM. An embedded configuration of the Intel RealSpeech speech recognizer and intent extraction engine runs on the DSP cores with neural network operations offloaded to the co-processor. The prototype demonstrates that continuous speech recognition and understanding is possible on hardware with very low power consumption. As an example application, control of lights in a home via natural language is shown. An Intel development kit is demonstrated together with a set of tools. Conference attendees are encouraged to interact with the demo and development system",
    "checked": true,
    "id": "5378a660410838816067bba0643349c811d96b72",
    "semantic_title": "speech recognition and understanding on hardware-accelerated dsp",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tsuji17_interspeech.html": {
    "title": "MetaLab: A Repository for Meta-Analyses on Language Development, and More",
    "volume": "main",
    "abstract": "MetaLab is a growing database of meta-analyses, shared in a github repository and via an interactive website. This website contains interactive tools for community-augmented meta-analyses, power analyses, and experimental planning. It currently contains a dozen meta-analyses spanning a number of phenomena in early language acquisition research, including infants' vowel discrimination, acoustic wordform segmentation, and distributional learning in the laboratory. During the Show and Tell, we will demonstrate how to use the online visualization tools, download data, and re-use our analysis scripts for other research purposes. We expect MetaLab data to be particularly useful to researchers interested in early speech perception. Additionally, the infrastructure and tools can be adopted by speech scientists seeking to perform and utilize (meta-)meta-analyses in other fields",
    "checked": true,
    "id": "f061a41720ec10539cc8c426ecfe8ad0fca6ee42",
    "semantic_title": "metalab: a repository for meta-analyses on language development, and more",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/daniel17_interspeech.html": {
    "title": "Evolving Recurrent Neural Networks That Process and Classify Raw Audio in a Streaming Fashion",
    "volume": "main",
    "abstract": "The paper describes a neuroevolution-based novel approach to train recurrent neural networks that can process and classify audio directly from the raw waveform signal, without any assumption on the signal itself, on the features that should be extracted, or on the required network topology to perform the task. Resulting networks are relatively small in memory size, and their usage in a streaming fashion makes them particularly suited to embedded real-time applications",
    "checked": true,
    "id": "6bebddac84f7e664c858d35a331f439caa15b508",
    "semantic_title": "evolving recurrent neural networks that process and classify raw audio in a streaming fashion",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/milosevic17_interspeech.html": {
    "title": "Combining Gaussian Mixture Models and Segmental Feature Models for Speaker Recognition",
    "volume": "main",
    "abstract": "In most speaker recognition systems speech utterances are not constrained in content or language. In a text-dependent speaker recognition system lexical content of speech and language are known in advance. The goal of this paper is to show that this information can be used by a segmental features (SF) approach to improve a standard Gaussian mixture model with MFCC features (GMM-MFCC). Speech features such as mean energy, delta energy, pitch, delta pitch, the formants F1–F4 and their bandwidths B1–B4 and the difference between F2 and F1 are calculated on segments and are associated to phonemes and phoneme groups for each speaker. The SF and GMM-MFCC approaches are combined by multiplying the outputs of two classifiers. All the experiments are performed on the two versions of TEVOID: TEVOID16 with 16 and the upgraded TEVOID50 with 50 speakers. On TEVOID16, SF achieves 84.23%, GMM-MFCC 91.75%, and the combined approach gives 95.12% recognition rate. On TEVOID50, the SF approach gives 68.69%, while both GMM-MFCC and the combined model achieve 95.84% recognition rate. On both databases, the number of male/female confusions decreased for the combined model. These results are promising for using segmental features to improve the recognition rate of text-dependent systems",
    "checked": true,
    "id": "f2628bcc5dfb32cf45e4596362bcee5d87111ce1",
    "semantic_title": "combining gaussian mixture models and segmental feature models for speaker recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hagerer17_interspeech.html": {
    "title": "Did you laugh enough today?\" — Deep Neural Networks for Mobile and Wearable Laughter Trackers",
    "volume": "main",
    "abstract": "In this paper we describe a mobile and wearable devices app that recognises laughter from speech in real-time. The laughter detection is based on a deep neural network architecture, which runs smoothly and robustly, even natively on a smartwatch. Further, this paper presents results demonstrating that our approach achieves state-of-the-art laughter detection performance on the SSPNet Vocalization Corpus (SVC) from the 2013 Interspeech Computational Paralinguistics Challenge Social Signals Sub-Challenge. As this technology is tailored for mobile and wearable devices, it enables and motivates many new use cases, for example, deployment in health care settings such as laughter tracking for psychological coaching, depression monitoring, and therapies",
    "checked": true,
    "id": "d57132cd49370c5250efa48d6b0294d3a408c98a",
    "semantic_title": "did you laugh enough today?\" - deep neural networks for mobile and wearable laughter trackers",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jeon17_interspeech.html": {
    "title": "Low-Frequency Ultrasonic Communication for Speech Broadcasting in Public Transportation",
    "volume": "main",
    "abstract": "Speech broadcasting via loudspeakers is widely used in public transportation to send broadcast notifications. However, listeners often fail to catch spoken context from speech broadcasts due to excessive environmental noise. We propose an ultrasonic communication method that can be applied to loudspeaker-based speech broadcasting to cope with this issue. In other words, text notifications are modulated and carried over low-frequency ultrasonic waves through loudspeakers to the microphones of each potential listener's mobile device. Then, the received ultrasonic stream is demodulated back into the text and the listener hears the notification context by a text-to-speech engine embedded in each mobile device. Such a transmission system is realized with a 20 kHz carrier frequency because it is inaudible to most listeners but capable of being used in communication between a loudspeaker and microphone. In addition, the performance of the proposed ultrasonic communication method is evaluated by measuring the success rate of transmitted words under various signal-to-noise ratio conditions",
    "checked": true,
    "id": "6889b882aed8d4e6b804f42c14f0fbdee0c1b008",
    "semantic_title": "low-frequency ultrasonic communication for speech broadcasting in public transportation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wood17_interspeech.html": {
    "title": "Real-Time Speech Enhancement with GCC-NMF: Demonstration on the Raspberry Pi and NVIDIA Jetson",
    "volume": "main",
    "abstract": "We demonstrate a real-time, open source implementation of the online GCC-NMF stereo speech enhancement algorithm. While the system runs on a variety of operating systems and hardware platforms, we highlight its potential for real-world mobile use by presenting it on two embedded systems: the Raspberry Pi 3 and the NVIDIA Jetson TX1. The effect of various algorithm parameters on subjective enhancement quality may be explored interactively via a graphical user interface, with the results heard in real-time. The trade-off between interference suppression and target fidelity is controlled by manipulating the parameters of the coefficient masking function. Increasing the pre-learned dictionary size improves overall speech enhancement quality at increased computational cost. We show that real-time GCC-NMF has potential for real-world application, remaining purely unsupervised and retaining the simplicity and flexibility of offline GCC-NMF",
    "checked": true,
    "id": "298fcd2a23b28e8aacac301b9ed301034500f4f3",
    "semantic_title": "real-time speech enhancement with gcc-nmf: demonstration on the raspberry pi and nvidia jetson",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/rouhe17_interspeech.html": {
    "title": "Reading Validation for Pronunciation Evaluation in the Digitala Project",
    "volume": "main",
    "abstract": "We describe a recognition, validation and segmentation system as an intelligent preprocessor for automatic pronunciation evaluation. The system is developed for large-scale high stake foreign language tests, where it is necessary to reduce human workload and ensure fair evaluation",
    "checked": true,
    "id": "e79a2834a3fc0578b39eea10bd49bbb4fcf1db80",
    "semantic_title": "reading validation for pronunciation evaluation in the digitala project",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/pelachaud17_interspeech.html": {
    "title": "Conversing with Social Agents That Smile and Laugh",
    "volume": "main",
    "abstract": "Our aim is to create virtual conversational partners. As such we have developed computational models to enrich virtual characters with socio-emotional capabilities that are communicated through multimodal behaviors. The approach we follow to build interactive and expressive interactants relies on theories from human and social sciences as well as data analysis and user-perception-based design. We have explored specific social signals such as smile and laughter, capturing their variation in production but also their different communicative functions and their impact in human-agent interaction. Lately we have been interested in modeling agents with social attitudes. Our aim is to model how social attitudes color the multimodal behaviors of the agents. We have gathered a corpus of dyads that was annotated along two layers: social attitudes and nonverbal behaviors. By applying sequence mining methods we have extracted behavior patterns involved in the change of perception of an attitude. We are particularly interested in capturing the behaviors that correspond to a change of perception of an attitude. In this talk I will present the GRETA/VIB platform where our research is implemented",
    "checked": true,
    "id": "22a2fa6bf00f95bcc0a4fda3c9af50ef4aacc238",
    "semantic_title": "conversing with social agents that smile and laugh",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/papadopoulos17_interspeech.html": {
    "title": "Team ELISA System for DARPA LORELEI Speech Evaluation 2016",
    "volume": "main",
    "abstract": "In this paper, we describe the system designed and developed by team ELISA for DARPA's LORELEI (Low Resource Languages for Emergent Incidents) pilot speech evaluation. The goal of the LORELEI program is to guide rapid resource deployment for humanitarian relief (e.g. for natural disasters), with a focus on \"low-resource\" language locations, where the cost of developing technologies for automated human language tools can be prohibitive both in monetary terms and timewise. In this phase of the program, the speech evaluation consisted of three separate tasks: detecting presence of an incident, classifying incident type, and classifying incident type along with identifying the location where it occurs. The performance metric was area under curve of precision-recall curves. Team ELISA competed against five other teams and won all the subtasks",
    "checked": true,
    "id": "f67986944d9f7f4a02dcaf6cbe84df3c6621bfc9",
    "semantic_title": "team elisa system for darpa lorelei speech evaluation 2016",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mihajlik17_interspeech.html": {
    "title": "First Results in Developing a Medieval Latin Language Charter Dictation System for the East-Central Europe Region",
    "volume": "main",
    "abstract": "Latin had served as an official language across Europe from the Roman Empire until the 19 century. As a result, vast amount of Latin language historical documents (charters, account books) survived from the Middle Ages, waiting for recovery. In the digitization process, tremendous human efforts are needed for the transliteration of textual content, as the applicability of optical character recognition techniques is often limited. In the era of Digital Humanities our aim is to accelerate the transcription by using automatic speech recognition technology. We introduce the challenges and our initial results in developing a real-time, medieval Latin language LVCSR dictation system for East-Central Europe (ECE). In this region, the pronunciation and usage of medieval Latin is considered to be roughly uniform. At this phase of the research, therefore, Latin speech data was not collected for acoustic model training but only for test purposes — from a selection of ECE countries. Our experimental results, however, suggest that ECE Latin varies significantly depending on the primary national language on both acoustic-phonetic and grammatical levels. On the other hand, unexpectedly low word error rates are obtained for several speakers whose native language is completely uncovered by the applied training data",
    "checked": true,
    "id": "c0e0580b7abee1eb1f5c452938cfc66a2eb099ef",
    "semantic_title": "first results in developing a medieval latin language charter dictation system for the east-central europe region",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/watson17_interspeech.html": {
    "title": "The Motivation and Development of MPAi, a Māori Pronunciation Aid",
    "volume": "main",
    "abstract": "This paper outlines the motivation and development of a pronunciation aid ( MPAi) for the Māori language, the language of the indigenous people of New Zealand. Māori is threatened and after a break in transmission the language is currently undergoing revitalization. The data for the aid has come from a corpus of 60 speakers (men and women). The language aid allows users to model their speech against exemplars from young speakers or older speakers of Māori. This is important, because of the status of the elders in the Māori speaking community, but it also recognizes that Māori is undergoing substantial vowel change. The pronunciation aid gives feedback on vowel production via formant analysis, and selected words via speech recognition. The evaluation of the aid by 22 language teachers is presented and the resulting changes are discussed",
    "checked": true,
    "id": "4eca04f48aad11e26cd832b7c45bc72ec6cc163b",
    "semantic_title": "the motivation and development of mpai, a māori pronunciation aid",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/feng17_interspeech.html": {
    "title": "On the Linguistic Relevance of Speech Units Learned by Unsupervised Acoustic Modeling",
    "volume": "main",
    "abstract": "Unsupervised acoustic modeling is an important and challenging problem in spoken language technology development for low-resource languages. It aims at automatically learning a set of speech units from un-transcribed data. These learned units are expected to be related to fundamental linguistic units that constitute the concerned language. Formulated as a clustering problem, unsupervised acoustic modeling methods are often evaluated in terms of average purity or similar types of performance measures. They do not provide detailed insights on the fitness of individual learned units and the relation between them. This paper presents an investigation on the linguistic relevance of learned speech units based on Kullback-Leibler (KL) divergence. A symmetric KL divergence metric is used to measure the distance between each pair of learned unit and ground-truth phoneme of the target language. Experimental analysis on a multilingual database shows that KL divergence is consistent with purity in evaluating clustering results. The deviation between a learned unit and its closest ground-truth phoneme is comparable to the inherent variability of the phoneme. The learned speech units have a good coverage of linguistically defined phonemes. However, there are certain phonemes that can not be covered, for example, the retroflex final /er/ in Mandarin",
    "checked": true,
    "id": "fd8495e46aaeb0de53fba56b65dbcf7522cfc230",
    "semantic_title": "on the linguistic relevance of speech units learned by unsupervised acoustic modeling",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/das17_interspeech.html": {
    "title": "Deep Auto-Encoder Based Multi-Task Learning Using Probabilistic Transcriptions",
    "volume": "main",
    "abstract": "We examine a scenario where we have no access to native transcribers in the target language. This is typical of language communities that are under-resourced. However, turkers (online crowd workers) available in online marketplaces can serve as valuable alternative resources for providing transcripts in the target language. We assume that the turkers neither speak nor have any familiarity with the target language. Thus, they are unable to distinguish all phone pairs in the target language; their transcripts therefore specify, at best, a probability distribution called a probabilistic transcript (PT). Standard deep neural network (DNN) training using PTs do not necessarily improve error rates. Previously reported results have demonstrated some success by adopting the multi-task learning (MTL) approach. In this study, we report further improvements by introducing a deep auto-encoder based MTL. This method leverages large amounts of untranscribed data in the target language in addition to the PTs obtained from turkers. Furthermore, to encourage transfer learning in the feature space, we also examine the effect of using monophones from transcripts in well-resourced languages. We report consistent improvement in phone error rates (PER) for Swahili, Amharic, Dinka, and Mandarin",
    "checked": true,
    "id": "de97cffa4ed4a186a56a3a7597a25cccc0f15162",
    "semantic_title": "deep auto-encoder based multi-task learning using probabilistic transcriptions",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gutkin17_interspeech.html": {
    "title": "Areal and Phylogenetic Features for Multilingual Speech Synthesis",
    "volume": "main",
    "abstract": "We introduce phylogenetic and areal language features to the domain of multilingual text-to-speech synthesis. Intuitively, enriching the existing universal phonetic features with cross-lingual shared representations should benefit the multilingual acoustic models and help to address issues like data scarcity for low-resource languages. We investigate these representations using the acoustic models based on long short-term memory recurrent neural networks. Subjective evaluations conducted on eight languages from diverse language families show that sometimes phylogenetic and areal representations lead to significant multilingual synthesis quality improvements. To help better leverage these novel features, improving the baseline phonetic representation may be necessary",
    "checked": true,
    "id": "7a28293b5ddd1e398a988e9db0e0b6c02b2767f1",
    "semantic_title": "areal and phylogenetic features for multilingual speech synthesis",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hall17_interspeech.html": {
    "title": "SLPAnnotator: Tools for Implementing Sign Language Phonetic Annotation",
    "volume": "main",
    "abstract": "This paper introduces a new resource for building phonetically transcribed corpora of signed languages. The free, open-source software tool, SLPAnnotator, is designed to facilitate the transcription of hand configurations using a slightly modified version of the Sign Language Phonetic Annotation (SLPA) system ([1], [2], [3], [4]; see also [5]) While the SLPA system is extremely phonetically detailed, it can be seen as cumbersome and, perhaps, harder for humans to use and interpret than other transcription systems (e.g. Prosodic Model Handshape Coding, [6]). SLPAnnotator is designed to bridge the gap between such systems by automating some of the transcription process, providing users with informative references about possible configurations as they are coding, giving continuously updatable access to a visual model of the transcribed handshape, and allowing users to verify that transcribed handshapes are both phonologically and anatomically plausible. Finally, SLPAnnotator is designed to interface with other analysis tools, such as Phonological CorpusTools ([7], [8]), to allow for subsequent phonological analysis of the resulting sign language corpora",
    "checked": true,
    "id": "7ab7cbcae73c2fd764712ce3e6905c34787d7aa6",
    "semantic_title": "slpannotator: tools for implementing sign language phonetic annotation",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/schwarz17_interspeech.html": {
    "title": "The LENA System Applied to Swedish: Reliability of the Adult Word Count Estimate",
    "volume": "main",
    "abstract": "The Language Environment Analysis system LENA is used to capture day-long recordings of children's natural audio environment. The system performs automated segmentation of the recordings and provides estimates for various measures. One of those measures is Adult Word Count (AWC), an approximation of the number of words spoken by adults in close proximity to the child. The LENA system was developed for and trained on American English, but it has also been evaluated on its performance when applied to Spanish, Mandarin and French. The present study is the first evaluation of the LENA system applied to Swedish, and focuses on the AWC estimate. Twelve five-minute segments were selected at random from each of four day-long recordings of 30-month-old children. Each of these 48 segments was transcribed by two transcribers, and both number of words and number of vowels were calculated (inter-transcriber reliability for words: r = .95, vowels: r = .93). Both counts correlated with the LENA system's AWC estimate for the same segments (words: r = .67, vowels: r = .66). The reliability of the AWC as estimated by the LENA system when applied to Swedish is therefore comparable to its reliability for Spanish, Mandarin and French",
    "checked": true,
    "id": "0c39eb3ebde2344a247dc602c046df16a10ffbd3",
    "semantic_title": "the lena system applied to swedish: reliability of the adult word count estimate",
    "citation_count": 18,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/casillas17_interspeech.html": {
    "title": "What do Babies Hear? Analyses of Child- and Adult-Directed Speech",
    "volume": "main",
    "abstract": "Child-directed speech is argued to facilitate language development, and is found cross-linguistically and cross-culturally to varying degrees. However, previous research has generally focused on short samples of child-caregiver interaction, often in the lab or with experimenters present. We test the generalizability of this phenomenon with an initial descriptive analysis of the speech heard by young children in a large, unique collection of naturalistic, daylong home recordings. Trained annotators coded automatically-detected adult speech ‘utterances' from 61 homes across 4 North American cities, gathered from children (age 2–24 months) wearing audio recorders during a typical day. Coders marked the speaker gender (male/female) and intended addressee (child/adult), yielding 10,886 addressee and gender tags from 2,523 minutes of audio (cf. HB-CHAAC Interspeech ComParE challenge; Schuller et al., in press). Automated speaker-diarization (LENA) incorrectly gender-tagged 30% of male adult utterances, compared to manually-coded consensus. Furthermore, we find effects of SES and gender on child-directed and overall speech, increasing child-directed speech with child age, and interactions of speaker gender, child gender, and child age: female caretakers increased their child-directed speech more with age than male caretakers did, but only for male infants. Implications for language acquisition and existing classification algorithms are discussed",
    "checked": true,
    "id": "967ec54ff8aecbafa8de227cc4974c96c3b058ae",
    "semantic_title": "what do babies hear? analyses of child- and adult-directed speech",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/casillas17b_interspeech.html": {
    "title": "A New Workflow for Semi-Automatized Annotations: Tests with Long-Form Naturalistic Recordings of Childrens Language Environments",
    "volume": "main",
    "abstract": "Interoperable annotation formats are fundamental to the utility, expansion, and sustainability of collective data repositories. In language development research, shared annotation schemes have been critical to facilitating the transition from raw acoustic data to searchable, structured corpora. Current schemes typically require comprehensive and manual annotation of utterance boundaries and orthographic speech content, with an additional, optional range of tags of interest. These schemes have been enormously successful for datasets on the scale of dozens of recording hours but are untenable for long-format recording corpora, which routinely contain hundreds to thousands of audio hours. Long-format corpora would benefit greatly from (semi-)automated analyses, both on the earliest steps of annotation — voice activity detection, utterance segmentation, and speaker diarization — as well as later steps — e.g., classification-based codes such as child-vs-adult-directed speech, and speech recognition to produce phonetic/orthographic representations. We present an annotation workflow specifically designed for long-format corpora which can be tailored by individual researchers and which interfaces with the current dominant scheme for short-format recordings. The workflow allows semi-automated annotation and analyses at higher linguistic levels. We give one example of how the workflow has been successfully implemented in a large cross-database project",
    "checked": true,
    "id": "de01dd258f451a197bfb291c00456261f6439ccf",
    "semantic_title": "a new workflow for semi-automatized annotations: tests with long-form naturalistic recordings of childrens language environments",
    "citation_count": 14,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bergmann17_interspeech.html": {
    "title": "Top-Down versus Bottom-Up Theories of Phonological Acquisition: A Big Data Approach",
    "volume": "main",
    "abstract": "Recent work has made available a number of standardized meta-analyses bearing on various aspects of infant language processing. We utilize data from two such meta-analyses (discrimination of vowel contrasts and word segmentation, i.e., recognition of word forms extracted from running speech) to assess whether the published body of empirical evidence supports a bottom-up versus a top-down theory of early phonological development by leveling the power of results from thousands of infants. We predicted that if infants can rely purely on auditory experience to develop their phonological categories, then vowel discrimination and word segmentation should develop in parallel, with the latter being potentially lagged compared to the former. However, if infants crucially rely on word form information to build their phonological categories, then development at the word level must precede the acquisition of native sound categories. Our results do not support the latter prediction. We discuss potential implications and limitations, most saliently that word forms are only one top-down level proposed to affect phonological development, with other proposals suggesting that top-down pressures emerge from lexical (i.e., word-meaning pairs) development. This investigation also highlights general procedures by which standardized meta-analyses may be reused to answer theoretical questions spanning across phenomena",
    "checked": true,
    "id": "bf11611ae7d1a2561458c592a5f5a0e1bdf6e73c",
    "semantic_title": "top-down versus bottom-up theories of phonological acquisition: a big data approach",
    "citation_count": 11,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tsuji17b_interspeech.html": {
    "title": "Which Acoustic and Phonological Factors Shape Infants' Vowel Discrimination? Exploiting Natural Variation in InPhonDB",
    "volume": "main",
    "abstract": "A key research question in early language acquisition concerns the development of infants' ability to discriminate sounds, and the factors structuring discrimination abilities. Vowel discrimination, in particular, has been studied using a range of tasks, experimental paradigms, and stimuli over the past 40 years, work recently compiled in a meta-analysis. We use this meta-analysis to assess whether there is statistical evidence for the following factors affecting effect sizes across studies: (1) the order in which the two vowel stimuli are presented; and (2) the distance between the vowels, measured acoustically in terms of spectral and quantity differences. The magnitude of effect sizes analysis revealed order effects consistent with the Natural Referent Vowels framework, with greater effect sizes when the second vowel was more peripheral than the first. Additionally, we find that spectral acoustic distinctiveness is a consistent predictor of studies' effect sizes, while temporal distinctiveness did not predict effect size magnitude. None of these factors interacted significantly with age. We discuss implications of these results for language acquisition, and more generally developmental psychology, research",
    "checked": true,
    "id": "49be1c8bc6587b6d19cf31f501e73ab24d3becf4",
    "semantic_title": "which acoustic and phonological factors shape infants' vowel discrimination? exploiting natural variation in inphondb",
    "citation_count": 12,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chasaide17b_interspeech.html": {
    "title": "The ABAIR Initiative: Bringing Spoken Irish into the Digital Space",
    "volume": "main",
    "abstract": "The processes of language demise take hold when a language ceases to belong to the mainstream of life's activities. Digital communication technology increasingly pervades all aspects of modern life. Languages not digitally ‘available' are ever more marginalised, whereas a digital presence often yields unexpected opportunities to integrate the language into the mainstream. The ABAIR initiative embraces three central aspects of speech technology development for Irish (Gaelic): the provision of technology-oriented linguistic-phonetic resources; the building and perfecting of core speech technologies; and the development of technology applications, which exploit both the technologies and the linguistic resources. The latter enable the public, learners, and those with disabilities to integrate Irish into their day-to-day usage. This paper outlines some of the specific linguistic and sociolinguistic challenges and the approaches adopted to address them. Although machine-learning approaches are helping to speed up the process of technology provision, the ABAIR experience highlights how phonetic-linguistic resources are also crucial to the development process. For the endangered language, linguistic resources are central to many applications that impact on language usage. The sociolinguistic context and the needs of potential end users should be central considerations in setting research priorities and deciding on methods",
    "checked": true,
    "id": "db88373f0b213af9fbc13768fb32b7506d635b6a",
    "semantic_title": "the abair initiative: bringing spoken irish into the digital space",
    "citation_count": 16,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/saeb17_interspeech.html": {
    "title": "Very Low Resource Radio Browsing for Agile Developmental and Humanitarian Monitoring",
    "volume": "main",
    "abstract": "We present a radio browsing system developed on a very small corpus of annotated speech by using semi-supervised training of multilingual DNN/HMM acoustic models. This system is intended to support relief and developmental programmes by the United Nations (UN) in parts of Africa where the spoken languages are extremely under resourced. We assume the availability of 12 minutes of annotated speech in the target language, and show how this can best be used to develop an acoustic model. First, a multilingual DNN/HMM is trained using Acholi as the target language and Luganda, Ugandan English and South African English as source languages. We show that the lowest word error rates are achieved by using this model to label further untranscribed target language data and then developing SGMM acoustic model from the extended dataset. The performance of an ASR system trained in this way is sufficient for keyword detection that yields useful and actionable near real-time information to developmental organisations",
    "checked": true,
    "id": "1e35d982f3bbb1bbd8e2a56a701803d76bfaff04",
    "semantic_title": "very low resource radio browsing for agile developmental and humanitarian monitoring",
    "citation_count": 18,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/malandrakis17_interspeech.html": {
    "title": "Extracting Situation Frames from Non-English Speech: Evaluation Framework and Pilot Results",
    "volume": "main",
    "abstract": "This paper describes the first evaluation framework for the extraction of Situation Frames — structures describing humanitarian assistance needs — from non-English speech audio, conducted for the DARPA LORELEI (Low Resource Languages for Emergent Incidents) program. Participants in LORELEI had to process audio from a variety of sources, in non-English languages, and extract the information required to populate Situation Frames describing whether any need is mentioned, the type of need present and where the need exists. The evaluation was conducted over a period of 10 days and attracted submissions from 6 teams, each team spanning multiple organizations. Performance was evaluated using precision-recall curves. The results are encouraging, with most teams showing some capability to detect the type of situation discussed, but more work will be required to connect needs to specific locations",
    "checked": true,
    "id": "047ebaf46d648fea9e5c2c64873c629f7fd53a00",
    "semantic_title": "extracting situation frames from non-english speech: evaluation framework and pilot results",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kocharov17_interspeech.html": {
    "title": "Eliciting Meaningful Units from Speech",
    "volume": "main",
    "abstract": "Elicitation of information structure from speech is a crucial step in automatic speech understanding. In terms of both production and perception, we consider intonational phrase to be the basic meaningful unit of information structure in speech. The current paper presents a method of detecting these units in speech by processing both the recorded speech and its textual representation. Using syntactic information, we split text into small groups of words closely connected with each other. Assuming that intonational phrases are built from these small groups, we use acoustic information to reveal their actual boundaries. The procedure was initially developed for processing Russian speech, and we have achieved the best published results for this language with F1 equal to 0.91. We assume that it may be adapted for other languages that have some amount of read speech resources, including under-resourced languages. For comparison we have evaluated it on English material (Boston University Radio Speech Corpus). Our results, F1 of 0.76, are comparable with the top systems designed for English",
    "checked": true,
    "id": "22458e9ad5650eb403ad980b11bda8530de35abf",
    "semantic_title": "eliciting meaningful units from speech",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bhati17_interspeech.html": {
    "title": "Unsupervised Speech Signal to Symbol Transformation for Zero Resource Speech Applications",
    "volume": "main",
    "abstract": "Zero resource speech processing refers to a scenario where no or minimal transcribed data is available. In this paper, we propose a three-step unsupervised approach to zero resource speech processing, which does not require any other information/dataset. In the first step, we segment the speech signal into phoneme-like units, resulting in a large number of varying length segments. The second step involves clustering the varying-length segments into a finite number of clusters so that each segment can be labeled with a cluster index. The unsupervised transcriptions, thus obtained, can be thought of as a sequence of virtual phone labels. In the third step, a deep neural network classifier is trained to map the feature vectors extracted from the signal to its corresponding virtual phone label. The virtual phone posteriors extracted from the DNN are used as features in the zero resource speech processing. The effectiveness of the proposed approach is evaluated on both ABX and spoken term discovery tasks (STD) using spontaneous American English and Tsonga language datasets, provided as part of zero resource 2015 challenge. It is observed that the proposed system outperforms baselines, supplied along the datasets, in both the tasks without any task specific modifications",
    "checked": true,
    "id": "d9a527a7426b8bad27f552cec40b0dcc95de098a",
    "semantic_title": "unsupervised speech signal to symbol transformation for zero resource speech applications",
    "citation_count": 16,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gauthier17_interspeech.html": {
    "title": "Machine Assisted Analysis of Vowel Length Contrasts in Wolof",
    "volume": "main",
    "abstract": "Growing digital archives and improving algorithms for automatic analysis of text and speech create new research opportunities for fundamental research in phonetics. Such empirical approaches allow statistical evaluation of a much larger set of hypothesis about phonetic variation and its conditioning factors (among them geographical / dialectal variants). This paper illustrates this vision and proposes to challenge automatic methods for the analysis of a not easily observable phenomenon: vowel length contrast. We focus on Wolof, an under-resourced language from Sub-Saharan Africa. In particular, we propose multiple features to make a fine evaluation of the degree of length contrast under different factors such as: read vs semi-spontaneous speech; standard vs dialectal Wolof. Our measures made fully automatically on more than 20k vowel tokens show that our proposed features can highlight different degrees of contrast for each vowel considered. We notably show that contrast is weaker in semi-spontaneous speech and in a non standard semi-spontaneous dialect",
    "checked": true,
    "id": "bfdd0c6fbf2f556b0a5366f9db07d72a2c36bbd9",
    "semantic_title": "machine assisted analysis of vowel length contrasts in wolof",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/glarner17_interspeech.html": {
    "title": "Leveraging Text Data for Word Segmentation for Underresourced Languages",
    "volume": "main",
    "abstract": "In this contribution we show how to exploit text data to support word discovery from audio input in an underresourced target language. Given audio, of which a certain amount is transcribed at the word level, and additional unrelated text data, the approach is able to learn a probabilistic mapping from acoustic units to characters and utilize it to segment the audio data into words without the need of a pronunciation dictionary. This is achieved by three components: an unsupervised acoustic unit discovery system, a supervisedly trained acoustic unit-to-grapheme converter, and a word discovery system, which is initialized with a language model trained on the text data. Experiments for multiple setups show that the initialization of the language model with text data improves the word segmentation performance by a large margin",
    "checked": true,
    "id": "f63a76c8e9e36a6fd399e41c9a0b13384007800f",
    "semantic_title": "leveraging text data for word segmentation for underresourced languages",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhuang17_interspeech.html": {
    "title": "Improving DNN Bluetooth Narrowband Acoustic Models by Cross-Bandwidth and Cross-Lingual Initialization",
    "volume": "main",
    "abstract": "The success of deep neural network (DNN) acoustic models is partly owed to large amounts of training data available for different applications. This work investigates ways to improve DNN acoustic models for Bluetooth narrowband mobile applications when relatively small amounts of in-domain training data are available. To address the challenge of limited in-domain data, we use cross-bandwidth and cross-lingual transfer learning methods to leverage knowledge from other domains with more training data (different bandwidth and/or languages). Specifically, narrowband DNNs in a target language are initialized using the weights of DNNs trained on bandlimited wide-band data in the same language or those trained on a different (resource-rich) language. We investigate multiple recipes involving such methods with different data resources. For all languages in our experiments, these recipes achieve up to 45% relative WER reduction, compared to training solely on the Bluetooth narrowband data in the target language. Furthermore, these recipes are very beneficial even when over two hundred hours of manually transcribed in-domain data is available, and we can achieve better accuracy than the baselines with as little as 20 hours of in-domain data",
    "checked": true,
    "id": "6a524f062cd11a3522e3e987442d75797b339617",
    "semantic_title": "improving dnn bluetooth narrowband acoustic models by cross-bandwidth and cross-lingual initialization",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/abraham17_interspeech.html": {
    "title": "Joint Estimation of Articulatory Features and Acoustic Models for Low-Resource Languages",
    "volume": "main",
    "abstract": "Using articulatory features for speech recognition improves the performance of low-resource languages. One way to obtain articulatory features is by using an articulatory classifier (pseudo-articulatory features). The performance of the articulatory features depends on the efficacy of this classifier. But, training such a robust classifier for a low-resource language is constrained due to the limited amount of training data. We can overcome this by training the articulatory classifier using a high resource language. This classifier can then be used to generate articulatory features for the low-resource language. However, this technique fails when high and low-resource languages have mismatches in their environmental conditions. In this paper, we address both the aforementioned problems by jointly estimating the articulatory features and low-resource acoustic model. The experiments were performed on two low-resource Indian languages namely, Hindi and Tamil. English was used as the high-resource language. A relative improvement of 23% and 10% were obtained for Hindi and Tamil, respectively",
    "checked": true,
    "id": "130829a8e351261b9e6058c1fcd519021a176151",
    "semantic_title": "joint estimation of articulatory features and acoustic models for low-resource languages",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/abraham17b_interspeech.html": {
    "title": "Transfer Learning and Distillation Techniques to Improve the Acoustic Modeling of Low Resource Languages",
    "volume": "main",
    "abstract": "Deep neural networks (DNN) require large amount of training data to build robust acoustic models for speech recognition tasks. Our work is intended in improving the low-resource language acoustic model to reach a performance comparable to that of a high-resource scenario with the help of data/model parameters from other high-resource languages. We explore transfer learning and distillation methods, where a complex high resource model guides or supervises the training of low resource model. The techniques include (i) multi-lingual framework of borrowing data from high-resource language while training the low-resource acoustic model. The KL divergence based constraints are added to make the model biased towards low-resource language, (ii) distilling knowledge from the complex high-resource model to improve the low-resource acoustic model. The experiments were performed on three Indian languages namely Hindi, Tamil and Kannada. All the techniques gave improved performance and the multi-lingual framework with KL divergence regularization giving the best results. In all the three languages a performance close to or better than high-resource scenario was obtained",
    "checked": true,
    "id": "acafbfb269723e452f0f8ebd6f3e351df019e30b",
    "semantic_title": "transfer learning and distillation techniques to improve the acoustic modeling of low resource languages",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/helgadottir17_interspeech.html": {
    "title": "Building an ASR Corpus Using Althingi's Parliamentary Speeches",
    "volume": "main",
    "abstract": "Acoustic data acquisition for under-resourced languages is an important and challenging task. In the Icelandic parliament, Althingi, all performed speeches are transcribed manually and published as text on Althingi's web page. To reduce the manual work involved, an automatic speech recognition system is being developed for Althingi. In this paper the development of a speech corpus suitable for the training of a parliamentary ASR system is described. Text and audio data of manually transcribed speeches were processed to build an aligned, segmented corpus, whereby language specific tasks had to be developed specially for Icelandic. The resulting corpus of 542 hours of speech is freely available on http://www.malfong.is. First experiments with an ASR system trained on the Althingi corpus have been conducted, showing promising results. Word error rate of 16.38% was obtained using time-delay deep neural network (TD-DNN) and 14.76% was obtained using long-short term memory recurrent neural network (LSTM-RNN) architecture. The Althingi corpus is to our knowledge the largest speech corpus currently available in Icelandic. The corpus as well as the developed methods for corpus creation constitute a valuable resource for further developments within Icelandic language technology",
    "checked": true,
    "id": "d4e17e82896c6ccf12a0d9d229bcd8c706f48151",
    "semantic_title": "building an asr corpus using althingi's parliamentary speeches",
    "citation_count": 31,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/alumae17_interspeech.html": {
    "title": "Implementation of a Radiology Speech Recognition System for Estonian Using Open Source Software",
    "volume": "main",
    "abstract": "Speech recognition has become increasingly popular in radiology reporting in the last decade. However, developing a speech recognition system for a new language in a highly specific domain requires a lot of resources, expert knowledge and skills. Therefore, commercial vendors do not offer ready-made radiology speech recognition systems for less-resourced languages This paper describes the implementation of a radiology speech recognition system for Estonian, a language with less than one million native speakers. The system was developed in partnership with a hospital that provided a corpus of written reports for language modeling purposes. Rewrite rules for pre-processing training texts and postprocessing recognition results were created manually based on a small parallel corpus created by the hospital's radiologists, using the Thrax toolkit. Deep neural network based acoustic models were trained based on 216 hours of out-of-domain data and adapted on 14 hours of spoken radiology data, using the Kaldi toolkit. The current word error rate of the system is 5.4%. The system is in active use in real clinical environment",
    "checked": true,
    "id": "c4bf986f6ab1bda6d6a7764402839834c2b533ac",
    "semantic_title": "implementation of a radiology speech recognition system for estonian using open source software",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gunason17_interspeech.html": {
    "title": "Building ASR Corpora Using Eyra",
    "volume": "main",
    "abstract": "Building acoustic databases for speech recognition is very important for under-resourced languages. To build a speech recognition system, a large amount of speech data from a considerable number of participants needs to be collected. Eyra is a toolkit that can be used to gather acoustic data from a large number of participants in a relatively straight forward fashion. Predetermined prompts are downloaded onto a client, typically run on a smartphone, where the participant reads them aloud so that the recording and its corresponding prompt can be uploaded. This paper presents the Eyra toolkit, its quality control routines and annotation mechanism. The quality control relies on a forced-alignment module, which gives feedback to the participant, and an annotation module which allows data collectors to rate the read prompts after they are uploaded to the system. The paper presents an analysis of the performance of the quality control and describes two data collections for Icelandic and Javanese",
    "checked": true,
    "id": "637b56b6097e85b12217e14376c6b1575a629aaa",
    "semantic_title": "building asr corpora using eyra",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/niekerk17_interspeech.html": {
    "title": "Rapid Development of TTS Corpora for Four South African Languages",
    "volume": "main",
    "abstract": "This paper describes the development of text-to-speech corpora for four South African languages. The approach followed investigated the possibility of using low-cost methods including informal recording environments and untrained volunteer speakers. This objective and the additional future goal of expanding the corpus to increase coverage of South Africa's 11 official languages necessitated experimenting with multi-speaker and code-switched data. The process and relevant observations are detailed throughout. The latest version of the corpora are available for download under an open-source licence and will likely see further development and refinement in future",
    "checked": true,
    "id": "4a00807538df59174bc3264e72bdfe83bcd9c924",
    "semantic_title": "rapid development of tts corpora for four south african languages",
    "citation_count": 27,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gutkin17b_interspeech.html": {
    "title": "Uniform Multilingual Multi-Speaker Acoustic Model for Statistical Parametric Speech Synthesis of Low-Resourced Languages",
    "volume": "main",
    "abstract": "Acquiring data for text-to-speech (TTS) systems is expensive. This typically requires large amounts of training data, which is not available for low-resourced languages. Sometimes small amounts of data can be collected, while often no data may be available at all. This paper presents an acoustic modeling approach utilizing long short-term memory (LSTM) recurrent neural networks (RNN) aimed at partially addressing the language data scarcity problem. Unlike speaker-adaptation systems that aim to preserve speaker similarity across languages, the salient feature of the proposed approach is that, once constructed, the resulting system does not need retraining to cope with the previously unseen languages. This is due to language and speaker-agnostic model topology and universal linguistic feature set. Experiments on twelve languages show that the system is able to produce intelligible and sometimes natural output when a language is unseen. We also show that, when small amounts of training data are available, pooling the data sometimes improves the overall intelligibility and naturalness. Finally, we show that sometimes having a multilingual system with no prior exposure to the language is better than building single-speaker system from small amounts of data for that language",
    "checked": true,
    "id": "ab58a5d4925ae122ffdf60b320a072b774972e93",
    "semantic_title": "uniform multilingual multi-speaker acoustic model for statistical parametric speech synthesis of low-resourced languages",
    "citation_count": 22,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mendelson17b_interspeech.html": {
    "title": "Nativization of Foreign Names in TTS for Automatic Reading of World News in Swahili",
    "volume": "main",
    "abstract": "When a text-to-speech (TTS) system is required to speak world news, a large fraction of the words to be spoken will be proper names originating in a wide variety of languages. Phonetization of these names based on target language letter-to-sound rules will typically be inadequate. This is detrimental not only during synthesis, when inappropriate phone sequences are produced, but also during training, if the system is trained on data from the same domain. This is because poor phonetization during forced alignment based on hidden Markov models can pollute the whole model set, resulting in degraded alignment even of normal target-language words. This paper presents four techniques designed to address this issue in the context of a Swahili TTS system: automatic transcription of proper names based on a lexicon from a better-resourced language; the addition of a parallel phone set and special part-of-speech tag exclusively dedicated to proper names; a manually-crafted phone mapping which allows substitutions for potentially more accurate phones in proper names during forced alignment; the addition in proper names of a grapheme-derived frame-level feature, supplementing the standard phonetic inputs to the acoustic model. We present results from objective and subjective evaluations of systems built using these four techniques",
    "checked": true,
    "id": "3be31e84fb8f53b9587662d72137fe7ebc79855c",
    "semantic_title": "nativization of foreign names in tts for automatic reading of world news in swahili",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tong17b_interspeech.html": {
    "title": "Multi-Task Learning for Mispronunciation Detection on Singapore Children's Mandarin Speech",
    "volume": "main",
    "abstract": "Speech technology for children is more challenging than for adults, because there is a lack of children's speech corpora. Moreover, there is higher heterogeneity in children's speech due to variability in anatomy across age and gender, larger variance in speaking rate and vocal effort, and immature command of word usage, grammar, and linguistic structure. Speech productions from Singapore children possess even more variability due to the multilingual environment in the city-state, causing inter-influences from Chinese languages (e.g., Hokkien and Mandarin), English dialects (e.g., American and British), and Indian languages (e.g., Hindi and Tamil). In this paper, we show that acoustic modeling of children's speech can leverage on a larger set of adult data. We compare two data augmentation approaches for children's acoustic modeling. The first approach disregards the child and adult categories and consolidates the two datasets together as one entire set. The second approach is multi-task learning: during training the acoustic characteristics of adults and children are jointly learned through shared hidden layers of the deep neural network, yet they still retain their respective targets using two distinct softmax layers. We empirically show that the multi-task learning approach outperforms the baseline in both speech recognition and computer-assisted pronunciation training",
    "checked": true,
    "id": "f44f8006279ecd0c0a9ee9c7d2313893d1bc2766",
    "semantic_title": "multi-task learning for mispronunciation detection on singapore children's mandarin speech",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/larsen17_interspeech.html": {
    "title": "Relating Unsupervised Word Segmentation to Reported Vocabulary Acquisition",
    "volume": "main",
    "abstract": "A range of computational approaches have been used to model the discovery of word forms from continuous speech by infants. Typically, these algorithms are evaluated with respect to the ideal ‘gold standard' word segmentation and lexicon. These metrics assess how well an algorithm matches the adult state, but may not reflect the intermediate states of the child's lexical development. We set up a new evaluation method based on the correlation between word frequency counts derived from the application of an algorithm onto a corpus of child-directed speech, and the proportion of infants knowing those words, according to parental reports. We evaluate a representative set of 4 algorithms, applied to transcriptions of the Brent corpus, which have been phonologized using either phonemes or syllables as basic units. Results show remarkable variation in the extent to which these 8 algorithm-unit combinations predicted infant vocabulary, with some of these predictions surpassing those derived from the adult gold standard segmentation. We argue that infant vocabulary prediction provides a useful complement to traditional evaluation; for example, the best predictor model was also one of the worst in terms of segmentation score, and there was no clear relationship between token or boundary F-score and vocabulary prediction",
    "checked": true,
    "id": "c7bca532c331838e757dbadba790cd3ccfd94108",
    "semantic_title": "relating unsupervised word segmentation to reported vocabulary acquisition",
    "citation_count": 16,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wiren17_interspeech.html": {
    "title": "Modelling the Informativeness of Non-Verbal Cues in Parent-Child Interaction",
    "volume": "main",
    "abstract": "Non-verbal cues from speakers, such as eye gaze and hand positions, play an important role in word learning [1]. This is consistent with the notion that for meaning to be reconstructed, acoustic patterns need to be linked to time-synchronous patterns from at least one other modality [2]. In previous studies of a multimodally annotated corpus of parent-child interaction, we have shown that parents interacting with infants at the early word-learning stage (7–9 months) display a large amount of time-synchronous patterns, but that this behaviour tails off with increasing age of the children [3]. Furthermore, we have attempted to quantify the informativeness of the different non-verbal cues, that is, to what extent they actually help to discriminate between different possible referents, and how critical the timing of the cues is [4]. The purpose of this paper is to generalise our earlier model by quantifying informativeness resulting from non-verbal cues occurring both before and after their associated verbal references",
    "checked": true,
    "id": "cbc61632a14d3da6d49c062114b150280f80807f",
    "semantic_title": "modelling the informativeness of non-verbal cues in parent-child interaction",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/marklund17b_interspeech.html": {
    "title": "Computational Simulations of Temporal Vocalization Behavior in Adult-Child Interaction",
    "volume": "main",
    "abstract": "The purpose of the present study was to introduce a computational simulation of timing in child-adult interaction. The simulation uses temporal information from real adult-child interactions as default temporal behavior of two simulated agents. Dependencies between the agents' behavior are added, and how the simulated interactions compare to real interaction data as a result is investigated. In the present study, the real data consisted of transcriptions of a mother interacting with her 12-month-old child, and the data simulated was vocalizations. The first experiment shows that although the two agents generate vocalizations according to the temporal characteristics of the interlocutors in the real data, simulated interaction with no contingencies between the two agents' behavior differs from real interaction data. In the second experiment, a contingency was introduced to the simulation: the likelihood that the adult agent initiated a vocalization if the child agent was already vocalizing. Overall, the simulated data is more similar to the real interaction data when the adult agent is less likely to start speaking while the child agent vocalizes. The results are in line with previous studies on turn-taking in parent-child interaction at comparable ages. This illustrates that computational simulations are useful tools when investigating parent-child interactions",
    "checked": true,
    "id": "7a160eb3a39c2b88ae499b9a512d2ef39f56cedb",
    "semantic_title": "computational simulations of temporal vocalization behavior in adult-child interaction",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/strombergsson17_interspeech.html": {
    "title": "Approximating Phonotactic Input in Children's Linguistic Environments from Orthographic Transcripts",
    "volume": "main",
    "abstract": "Child-directed spoken data is the ideal source of support for claims about children's linguistic environments. However, phonological transcriptions of child-directed speech are scarce, compared to sources like adult-directed speech or text data. Acquiring reliable descriptions of children's phonological environments from more readily accessible sources would mean considerable savings of time and money. The first step towards this goal is to quantify the reliability of descriptions derived from such secondary sources We investigate how phonological distributions vary across different modalities (spoken vs. written), and across the age of the intended audience (children vs. adults). Using a previously unseen collection of Swedish adult- and child-directed spoken and written data, we combine lexicon look-up and grapheme-to-phoneme conversion to approximate phonological characteristics. The analysis shows distributional differences across datasets both for single phonemes and for longer phoneme sequences. Some of these are predictably attributed to lexical and contextual characteristics of text vs. speech The generated phonological transcriptions are remarkably reliable. The differences in phonological distributions between child-directed speech and secondary sources highlight a need for compensatory measures when relying on written data or on adult-directed spoken data, and/or for continued collection of actual child-directed speech in research on children's language environments",
    "checked": true,
    "id": "5c4824322e150658e0991211b545caadac9a7c9c",
    "semantic_title": "approximating phonotactic input in children's linguistic environments from orthographic transcripts",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chaabouni17_interspeech.html": {
    "title": "Learning Weakly Supervised Multimodal Phoneme Embeddings",
    "volume": "main",
    "abstract": "Recent works have explored deep architectures for learning multimodal speech representation (e.g. audio and images, articulation and audio) in a supervised way. Here we investigate the role of combining different speech modalities, i.e. audio and visual information representing the lips' movements, in a weakly supervised way using Siamese networks and lexical same-different side information. In particular, we ask whether one modality can benefit from the other to provide a richer representation for phone recognition in a weakly supervised setting. We introduce mono-task and multi-task methods for merging speech and visual modalities for phone recognition. The mono-task learning consists in applying a Siamese network on the concatenation of the two modalities, while the multi-task learning receives several different combinations of modalities at train time. We show that multi-task learning enhances discriminability for visual and multimodal inputs while minimally impacting auditory inputs. Furthermore, we present a qualitative analysis of the obtained phone embeddings, and show that cross-modal visual input can improve the discriminability of phonological features which are visually discernable (rounding, open/close, labial place of articulation), resulting in representations that are closer to abstract linguistic features than those based on audio only",
    "checked": true,
    "id": "83715a45d2680a55727034f682a3fc998a49c302",
    "semantic_title": "learning weakly supervised multimodal phoneme embeddings",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/obuchi17_interspeech.html": {
    "title": "Personalized Quantification of Voice Attractiveness in Multidimensional Merit Space",
    "volume": "main",
    "abstract": "Voice attractiveness is an indicator which is somehow objective and somehow subjective. It would be helpful to assume that each voice has its own attractiveness. However, the paired comparison results of human listeners sometimes include inconsistency. In this paper, we propose a multidimensional mapping scheme of voice attractiveness, which explains the existence of objective merit values of voices and subjective preference of listeners. Paired comparison is modeled in a probabilistic framework, and the optimal mapping is obtained from the paired comparison results on the maximum likelihood criterion The merit values can be estimated from the acoustic feature using the machine learning framework. We show how the estimation process works using real database consisting of common Japanese greeting utterances. Experiments using 1- and 2- dimensional merit spaces confirm that the comparison result prediction from the acoustic feature becomes more accurate in the 2-dimensional case",
    "checked": true,
    "id": "b76f1055f43b1c628e49793a073c3a76e1668e43",
    "semantic_title": "personalized quantification of voice attractiveness in multidimensional merit space",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bosker17_interspeech.html": {
    "title": "The Role of Temporal Amplitude Modulations in the Political Arena: Hillary Clinton vs. Donald Trump",
    "volume": "main",
    "abstract": "Speech is an acoustic signal with inherent amplitude modulations in the 1–9 Hz range. Recent models of speech perception propose that this rhythmic nature of speech is central to speech recognition. Moreover, rhythmic amplitude modulations have been shown to have beneficial effects on language processing and the subjective impression listeners have of the speaker. This study investigated the role of amplitude modulations in the political arena by comparing the speech produced by Hillary Clinton and Donald Trump in the three presidential debates of 2016 Inspection of the modulation spectra, revealing the spectral content of the two speakers' amplitude envelopes after matching for overall intensity, showed considerably greater power in Clinton's modulation spectra (compared to Trump's) across the three debates, particularly in the 1–9 Hz range. The findings suggest that Clinton's speech had a more pronounced temporal envelope with rhythmic amplitude modulations below 9 Hz, with a preference for modulations around 3 Hz. This may be taken as evidence for a more structured temporal organization of syllables in Clinton's speech, potentially due to more frequent use of preplanned utterances. Outcomes are interpreted in light of the potential beneficial effects of a rhythmic temporal envelope on intelligibility and speaker perception",
    "checked": true,
    "id": "8aabf2024e7aca1a7b2c864bfb72fb658cb285ee",
    "semantic_title": "the role of temporal amplitude modulations in the political arena: hillary clinton vs. donald trump",
    "citation_count": 12,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gallardo17b_interspeech.html": {
    "title": "Perceptual Ratings of Voice Likability Collected Through In-Lab Listening Tests vs. Mobile-Based Crowdsourcing",
    "volume": "main",
    "abstract": "Human perceptions of speaker characteristics, needed to perform automatic predictions from speech features, have generally been collected by conducting demanding in-lab listening tests under controlled conditions. Concurrently, crowdsourcing has emerged as a valuable approach for running user studies through surveys or quantitative ratings. Micro-task crowdsourcing markets enable the completion of small tasks (commonly of minutes or seconds), rewarding users with micro-payments. This paradigm permits effortless collection of user input from a large and diverse pool of participants at low cost. This paper presents different auditory tests for collecting perceptual voice likability ratings employing a common set of 30 male and female voices. These tests are based on direct scaling and on paired-comparisons, and were conducted in the laboratory and via crowdsourcing using micro-tasks. Design considerations are proposed for adapting the laboratory listening tests to a mobile-based crowdsourcing platform to obtain trustworthy listeners' answers. Our likability scores obtained by the different test approaches are highly correlated. This outcome motivates the use of crowdsourcing for future listening tests investigating e.g. speaker characterization, reducing the efforts involved in engaging participants and administering the tests on-site",
    "checked": true,
    "id": "e1c0b5a4d306a88b9ffe42529a4f1fa540b3f5a8",
    "semantic_title": "perceptual ratings of voice likability collected through in-lab listening tests vs. mobile-based crowdsourcing",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/trouvain17_interspeech.html": {
    "title": "Attractiveness of French Voices for German Listeners — Results from Native and Non-Native Read Speech",
    "volume": "main",
    "abstract": "This study investigated how the perceived attractiveness of voices was influenced by a foreign language, a foreign accent, and the level of fluency in the foreign language. Stimuli were taken from a French-German corpus of read speech with German native speakers as raters. Additional factors were stimulus length (syllable or entire sentence) and sex (of the raters and speakers). Results with German native raters reveal that stimuli spanning just a syllable were judged significantly less attractive than those containing a sentence, and that stimuli from French speakers were assessed as more attractive than those of German speakers. This backs the cliché that French has an attractive image for German listeners. An analysis of the best vs. the worst rated sentences suggest that an individual mix of voice quality, disfluency management, prosodic behaviour and pronunciation precision is responsible for the results",
    "checked": true,
    "id": "f67333ff9ef72fb599c8b0dbb1a0271b30e80738",
    "semantic_title": "attractiveness of french voices for german listeners - results from native and non-native read speech",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/schweitzer17b_interspeech.html": {
    "title": "Social Attractiveness in Dialogs",
    "volume": "main",
    "abstract": "This study investigates how acoustic and lexical properties of spontaneous speech in dialogs affect perceived social attractiveness in terms of speaker likeability, friendliness, competence, and self-confidence. We analyze a database of longer spontaneous dialogs between German female speakers and the mutual ratings that dialog partners assigned to one another after every conversation. Thus the ratings reflect long-term impressions based on dialog behavior. Using linear mixed models, we investigate both classical acoustic-prosodic and lexical parameters as well as parameters that capture the degree of speakers' adaptation, or \"convergence\", of these parameters to each other. Specifically we find that likeability is correlated with the speaker's lexical convergence as well as with her convergence in f peak height. Friendliness is significantly related to variation in intensity. For competence, the proportion of positive words in the dialog, variation in shimmer, and overall phonetic convergence are significant correlates. Self-confidence finally is related to several prosodic, phonetic, and lexical adaptation parameters. In some cases, the effect depends on whether interlocutors also had eye contact during their conversation. Taken together, these findings provide evidence that in addition to classical parameters, convergence parameters play an important role in the mutual perception of social attractiveness",
    "checked": true,
    "id": "ed18b2165873d140f568ad7572ab6967819ce1b8",
    "semantic_title": "social attractiveness in dialogs",
    "citation_count": 15,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/novaktot17_interspeech.html": {
    "title": "A Gender Bias in the Acoustic-Melodic Features of Charismatic Speech?",
    "volume": "main",
    "abstract": "Previous studies proved the immense importance of nonverbal skills when it comes to being persuasive and coming across as charismatic. It was also found that men sound more convincing and persuasive (i.e. altogether more charismatic) than women under otherwise comparable conditions. This gender bias is investigated in the present study by analyzing and comparing acoustic-melodic charisma features of male and female business executives. In line with the gender bias in perception, our results show that female CEOs who are judged to be similarly charismatic as their male counterpart(s) produce more and stronger acoustic charisma cues. This suggests that there is a gender bias which is compensated for by making a greater effort on the part of the female speakers",
    "checked": true,
    "id": "8c20b69735073fdba71ac52484760d1c1e84f8f6",
    "semantic_title": "a gender bias in the acoustic-melodic features of charismatic speech?",
    "citation_count": 30,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/michalsky17_interspeech.html": {
    "title": "Pitch Convergence as an Effect of Perceived Attractiveness and Likability",
    "volume": "main",
    "abstract": "While there is a growing body of research on which and how pitch features are perceived as attractive or likable, there are few studies investigating how the impression of a speaker as attractive or likable affects the speech behavior of his/her interlocutor. Recent studies have shown that perceived attractiveness and likability may not only have an effect on a speaker's pitch features in isolation but also on the prosodic entrainment. It has been shown that how speakers synchronize their pitch features relatively to their interlocutor is affected by such impressions. This study investigates pitch convergence, examining whether speakers become more similar over the course of a conversation depending on perceived attractiveness and/or likability. The expected pitch convergence is thereby investigated on two levels, over the entire conversation (globally) as well as turn-wise (locally). The results from a speed dating experiment with 98 mixed-sex dialogues of heterosexual singles show that speakers become more similar globally and locally over time both in register and range. Furthermore, the degree of pitch convergence is greatly affected by perceived attractiveness and likability with effects differing between attractiveness and likability as well as between the global and the local level",
    "checked": true,
    "id": "c1561694d59f5f975dd5163c1b270e94b6de003c",
    "semantic_title": "pitch convergence as an effect of perceived attractiveness and likability",
    "citation_count": 43,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jiao17_interspeech.html": {
    "title": "Does Posh English Sound Attractive?",
    "volume": "main",
    "abstract": "Poshness refers to how much a British English speaker sounds upper class when they talk. Popular descriptions of posh English mostly focus on vocabulary, accent and phonology. This study tests the hypothesis that, as a social index, poshness is also manifested via phonetic properties known to encode vocal attractiveness. Specifically, posh English, because of its impression of being detached, authoritative and condescending, would more closely resemble an attractive male voice than an attractive female voice. In four experiments, we tested this hypothesis by acoustically manipulating Cambridge-accented English utterances by a male and a female speaker through PSOLA resynthesis, and having native speakers of British English judge how posh or attractive each utterance sounds. The manipulated acoustic dimensions are formant dispersion, pitch shift and speech rate. Initial results from the first two experiments showed a trend in the hypothesized direction for the male speakers' utterances. But for the female utterances there was a ceiling effect due to the frequent alternation of speaker gender within the same test session. When the two speakers' utterances were separated by blocks in the third and fourth experiments, a clearer support for the main hypothesis was found",
    "checked": true,
    "id": "fd36cf121343f7bc62d5a129b5c5bdcab032e435",
    "semantic_title": "does posh english sound attractive?",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/baumann17_interspeech.html": {
    "title": "Large-Scale Speaker Ranking from Crowdsourced Pairwise Listener Ratings",
    "volume": "main",
    "abstract": "Speech quality and likability is a multi-faceted phenomenon consisting of a combination of perceptory features that cannot easily be computed nor weighed automatically. Yet, it is often easy to decide which of two voices one likes better, even though it would be hard to describe why, or to name the underlying basic perceptory features. Although likability is inherently subjective and individual preferences differ frequently, generalizations are useful and there is often a broad intersubjective consensus about whether one speaker is more likable than another. However, breaking down likability rankings into pairwise comparisons leads to a quadratic explosion of rating pairs. We present a methodology and software to efficiently create a likability ranking for many speakers from crowdsourced pairwise likability ratings. We collected pairwise likability ratings for many (>220) speakers from many raters (>160) and turn these ratings into one likability ranking. We investigate the resulting speaker ranking stability under different conditions: limiting the number of ratings and the dependence on rater and speaker characteristics. We also analyze the ranking wrt. acoustic correlates to find out what factors influence likability. We publish our ranking and the underlying ratings in order to facilitate further research",
    "checked": true,
    "id": "f37ec2bdfd6b2afabd0a9e9c116575fd0dcccfa5",
    "semantic_title": "large-scale speaker ranking from crowdsourced pairwise listener ratings",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/signorello17_interspeech.html": {
    "title": "Aerodynamic Features of French Fricatives",
    "volume": "main",
    "abstract": "The present research investigates the aerodynamic features of French fricative consonants using direct measurement of subglottal air pressure by tracheal puncture (Ps) synchronized with intraoral air pressure (Po), oral airflow (Oaf) and acoustic measurements. Data were collected from four Belgian French speakers' productions of CVCV pseudowords including voiceless and voiced fricatives [f, v, s, z, ʃ, ʒ]. The goals of this study are: (i) to predict the starting, central, and releasing points of frication based on the measurements of Ps, Po, and Oaf; (ii) to compare voiceless and voiced fricatives and their places of articulation; and (iii) to provide reference values for the aerodynamic features of fricatives for further linguistic, clinical, physical and computational modeling research",
    "checked": true,
    "id": "6a415f8d1ccd40f922f21e5e00cc96bab76447c5",
    "semantic_title": "aerodynamic features of french fricatives",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/serrurier17_interspeech.html": {
    "title": "Inter-Speaker Variability: Speaker Normalisation and Quantitative Estimation of Articulatory Invariants in Speech Production for French",
    "volume": "main",
    "abstract": "Speech production can be analysed in terms of universal articulatory-acoustic phonemic units shared between speakers. However, morphological differences between speakers and idiosyncratic articulatory strategies lead to large inter-speaker articulatory variability. Relationships between strategy and morphology have already been pinpointed in the literature. This study aims thus at generalising existing results on a larger database for the entire vocal tract (VT) and at quantifying phoneme-specific inter-speaker articulatory invariants. Midsagittal MRI of 11 French speakers for 62 vowels and consonants were recorded and VT contours manually edited. A procedure of normalisation of VT contours between speakers, based on the use of mean VT contours, led to an overall reduction of inter-speaker VT contours variance of 88%. On the opposite, the sagittal function (i.e. the transverse sagittal distance along the VT midline), which is the main determinant of the acoustic output, had an overall amplitude variance decrease of only 37%, suggesting that the speakers adapt their strategy to their morphology to achieve proper acoustic goals. Moreover, articulatory invariants were identified on the sagittal variance distribution along the VT as the regions with lower variability. These regions correspond to the classical places of articulation and are associated with higher acoustic sensitivity function levels",
    "checked": true,
    "id": "58f281e3d1af03d912e525e72970452b5ad9886b",
    "semantic_title": "inter-speaker variability: speaker normalisation and quantitative estimation of articulatory invariants in speech production for french",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/patil17b_interspeech.html": {
    "title": "Comparison of Basic Beatboxing Articulations Between Expert and Novice Artists Using Real-Time Magnetic Resonance Imaging",
    "volume": "main",
    "abstract": "Real-time Magnetic Resonance Imaging (rtMRI) was used to examine mechanisms of sound production in five beatboxers. rtMRI was found to be an effective tool with which to study the articulatory dynamics of this form of human vocal production; it provides a dynamic view of the entire midsagittal vocal tract and at a frame rate (83 fps) sufficient to observe the movement and coordination of critical articulators. The artists' repertoires included percussion elements generated using a wide range of articulatory and airstream mechanisms. Analysis of three common beatboxing sounds resulted in the finding that advanced beatboxers produce stronger ejectives and have greater control over different airstreams than novice beatboxers, to enhance the quality of their sounds. No difference in production mechanisms between males and females was observed. These data offer insights into the ways in which articulators can be trained and used to achieve specific acoustic goals",
    "checked": true,
    "id": "b3239878627718a480d3c0f944e4c669b47f72e5",
    "semantic_title": "comparison of basic beatboxing articulations between expert and novice artists using real-time magnetic resonance imaging",
    "citation_count": 13,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tang17b_interspeech.html": {
    "title": "Speaker-Specific Biomechanical Model-Based Investigation of a Simple Speech Task Based on Tagged-MRI",
    "volume": "main",
    "abstract": "We create two 3D biomechanical speaker models matched to medical image data of two healthy English speakers. We use a new, hybrid registration technique that morphs a generic 3D, biomechanical model to medical images. The generic model of the head and neck includes jaw, tongue, soft-palate, epiglottis, lips and face, and is capable of simulating upper-airway biomechanics. We use cine and tagged magnetic resonance (MR) images captured while our volunteers repeated a simple utterance (/ə-gis/) synchronized to a metronome. We simulate our models based on internal tongue tissue trajectories that we extract from tagged MR images, and use in an inverse solver. For areas without tracked data points, the registered generic model moves based on the computed muscle activations. Our modeling efforts include a wide range of speech organs illustrating the coupling complexity between the oral anatomy during simple speech utterances",
    "checked": true,
    "id": "efae80c64aca16924caee187da2a6fe58370cfce",
    "semantic_title": "speaker-specific biomechanical model-based investigation of a simple speech task based on tagged-mri",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/blaylock17_interspeech.html": {
    "title": "Sounds of the Human Vocal Tract",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "a27c32d48d606027333bd47b143ca5ddb030d6cc",
    "semantic_title": "sounds of the human vocal tract",
    "citation_count": 19,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/uezu17_interspeech.html": {
    "title": "A Simulation Study on the Effect of Glottal Boundary Conditions on Vocal Tract Formants",
    "volume": "main",
    "abstract": "In the source-filter theory, the complete closure of the glottis is assumed as a glottal boundary condition. However, such assumption of glottal closure in the source-filter theory is not strictly satisfied in actual utterance. Therefore, it is considered that acoustic features of the glottis and the subglottal region may affect vocal tract formants. In this study, we investigated how differences in the glottal boundary conditions affect vocal tract formants by speech synthesis simulation using speech production model. We synthesized five Japanese vowels using the speech production model in consideration of the source-filter interaction. This model consisted of the glottal area polynomial model and the acoustic tube model in the concatenation of the vocal tract, glottis, and the subglottis. From the results, it was found that the first formant frequency was affected more strongly by the boundary conditions, and also found that the open quotient may give the formant stronger effect than the maximum glottal width. In addition, formant frequencies were also affected more strongly by subglottal impedance when the maximum glottal area was wider",
    "checked": true,
    "id": "9b3923a780034ea4f14474dc0c869a78e74a366d",
    "semantic_title": "a simulation study on the effect of glottal boundary conditions on vocal tract formants",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gangamohan17_interspeech.html": {
    "title": "A Robust and Alternative Approach to Zero Frequency Filtering Method for Epoch Extraction",
    "volume": "main",
    "abstract": "During production of voiced speech, there exists impulse-like excitations due to abrupt closure of vocal folds. These impulse-like excitations are often referred as epochs or glottal closure instants (GCIs). The zero frequency filtering (ZFF) method exploits the properties of impulse-like excitation by passing a speech signal through the resonator whose pole pair is located at 0 Hz. As the resonator is unstable, the polynomial growth/decay is observed in the filtered signal, thus requiring a trend removal operation. It is observed that the length of the window for trend removal operation is critical in speech signals where there are more fluctuations in the fundamental frequency (F ). In this paper, a simple finite impulse response (FIR) implementation is proposed. The FIR filter is designed by placing large number of zeros at (f_s)/(2)] Hz (f represents the sampling frequency), closer to the unit circle, in the z-plane. Experimental results show that the proposed method is robust and computationally less complex when compared to the ZFF method",
    "checked": true,
    "id": "dde3fc2e86f105c77f3015b70a376ca9d78621f0",
    "semantic_title": "a robust and alternative approach to zero frequency filtering method for epoch extraction",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hua17_interspeech.html": {
    "title": "Improving YANGsaf F0 Estimator with Adaptive Kalman Filter",
    "volume": "main",
    "abstract": "We present improvements to the refinement stage of YANGsaf[1] (Yet ANother Glottal source analysis framework), a recently published F0 estimation algorithm by Kawahara et al., for noisy/breathy speech signals. The baseline system, based on time-warping and weighted average of multi-band instantaneous frequency estimates, is still sensitive to additive noise when none of the harmonic provide reliable frequency estimate at low SNR. We alleviate this problem by calibrating the weighted averaging process based on statistics gathered from a Monte-Carlo simulation, and applying Kalman filtering to refined F0 trajectory with time-varying measurement and process distributions. The improved algorithm, adYANGsaf (adaptive Yet ANother Glottal source analysis framework), achieves significantly higher accuracy and smoother F0 trajectory on noisy speech while retaining its accuracy on clean speech, with little computational overhead introduced",
    "checked": true,
    "id": "5e410b260dc498f8e146af97d76f1d9233e9b180",
    "semantic_title": "improving yangsaf f0 estimator with adaptive kalman filter",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/dhiman17_interspeech.html": {
    "title": "A Spectro-Temporal Demodulation Technique for Pitch Estimation",
    "volume": "main",
    "abstract": "We consider a two-dimensional demodulation framework for spectro-temporal analysis of the speech signal. We construct narrowband (NB) speech spectrograms, and demodulate them using the Riesz transform, which is a two-dimensional extension of the Hilbert transform. The demodulation results in time-frequency envelope (amplitude modulation or AM) and time-frequency carrier (frequency modulation or FM). The AM corresponds to the vocal tract and is referred to as the vocal tract spectrogram. The FM corresponds to the underlying excitation and is referred to as the carrier spectrogram. The carrier spectrogram exhibits a high degree of time-frequency consistency for voiced sounds. For unvoiced sounds, such a structure is lacking. In addition, the carrier spectrogram reflects the fundamental frequency (F0) variation of the speech signal. We develop a technique to determine the F0 from the carrier spectrogram. The time-frequency consistency is used to determine which time-frequency regions correspond to voiced segments. Comparisons with the state-of-the-art F0 estimation algorithms show that the proposed F0 estimator has high accuracy for telephone channel speech and is robust to noise",
    "checked": true,
    "id": "344e1b401293a8c547bed50c71c4d5e61d2fbecc",
    "semantic_title": "a spectro-temporal demodulation technique for pitch estimation",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/miwa17_interspeech.html": {
    "title": "Robust Method for Estimating F0 of Complex Tone Based on Pitch Perception of Amplitude Modulated Signal",
    "volume": "main",
    "abstract": "Estimating the fundamental frequency (F ) of a target sound in noisy reverberant environments is a challenging issue in not only sound analysis/synthesis but also sound enhancement. This paper proposes a method for robustly and accurately estimating the F of a time-variant complex tone on the basis of an amplitude modulation/demodulation technique. It is based on the mechanism of the pitch perception of amplitude modulated signal and the frame-work of power envelope restoration based on the concept of modulation transfer function. Computer simulations were carried out to discuss feasibility of the accuracy and robustness of the proposed method for estimating the F in heavy noisy reverberant environments. The comparative results revealed that the percentage correct rates of the estimated F s using five recent methods (TEMPO2, YIN, PHIA, CmpCep, and SWIPE') were drastically reduced as the SNR decreased and the reverberation time increased. The results also demonstrated that the proposed method robustly and accurately estimated the F in both heavy noisy and reverberant environments",
    "checked": true,
    "id": "4efb11e3efd53aeea3aa05263382c5f97adc289b",
    "semantic_title": "robust method for estimating f0 of complex tone based on pitch perception of amplitude modulated signal",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/graf17_interspeech.html": {
    "title": "Low-Complexity Pitch Estimation Based on Phase Differences Between Low-Resolution Spectra",
    "volume": "main",
    "abstract": "Detection of voiced speech and estimation of the pitch frequency are important tasks for many speech processing algorithms. Pitch information can be used, e.g., to reconstruct voiced speech corrupted by noise In automotive environments, driving noise especially affects voiced speech portions in the lower frequencies. Pitch estimation is therefore important, e.g., for in-car-communication systems. Such systems amplify the driver's voice and allow for convenient conversations with backseat passengers. Low latency is required for this application, which requires the use of short window lengths and short frame shifts between consecutive frames. Conventional pitch estimation techniques, however, rely on long windows that exceed the pitch period of human speech. In particular, male speakers' low pitch frequencies are difficult to resolve In this publication, we introduce a technique that approaches pitch estimation from a different perspective. The pitch information is extracted based on phase differences between multiple low-resolution spectra instead of a single long window. The technique benefits from the high temporal resolution provided by the short frame shift and is capable to deal with the low spectral resolution caused by short window lengths. Using the new approach, even very low pitch frequencies can be estimated very efficiently",
    "checked": true,
    "id": "c1251e4e7b0321c3621e4f97082989d0a9715d7a",
    "semantic_title": "low-complexity pitch estimation based on phase differences between low-resolution spectra",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/morise17b_interspeech.html": {
    "title": "Harvest: A High-Performance Fundamental Frequency Estimator from Speech Signals",
    "volume": "main",
    "abstract": "A fundamental frequency (F0) estimator named Harvest is described. The unique points of Harvest are that it can obtain a reliable F0 contour and reduce the error that the voiced section is wrongly identified as the unvoiced section. It consists of two steps: estimation of F0 candidates and generation of a reliable F0 contour on the basis of these candidates. In the first step, the algorithm uses fundamental component extraction by many band-pass filters with different center frequencies and obtains the basic F0 candidates from filtered signals. After that, basic F0 candidates are refined and scored by using the instantaneous frequency, and then several F0 candidates in each frame are estimated. Since the frame-by-frame processing based on the fundamental component extraction is not robust against temporally local noise, a connection algorithm using neighboring F0s is used in the second step. The connection takes advantage of the fact that the F0 contour does not precipitously change in a short interval. We carried out an evaluation using two speech databases with electroglottograph (EGG) signals to compare Harvest with several state-of-the-art algorithms. Results showed that Harvest achieved the best performance of all algorithms",
    "checked": true,
    "id": "7143d07db6e2a12d0119bcd7dd21ed981072e728",
    "semantic_title": "harvest: a high-performance fundamental frequency estimator from speech signals",
    "citation_count": 60,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/stehwien17_interspeech.html": {
    "title": "Prosodic Event Recognition Using Convolutional Neural Networks with Context Information",
    "volume": "main",
    "abstract": "This paper demonstrates the potential of convolutional neural networks (CNN) for detecting and classifying prosodic events on words, specifically pitch accents and phrase boundary tones, from frame-based acoustic features. Typical approaches use not only feature representations of the word in question but also its surrounding context. We show that adding position features indicating the current word benefits the CNN. In addition, this paper discusses the generalization from a speaker-dependent modelling approach to a speaker-independent setup. The proposed method is simple and efficient and yields strong results not only in speaker-dependent but also speaker-independent cases",
    "checked": true,
    "id": "593d3637c32847dcce5d37203ec9d86b8da1fbad",
    "semantic_title": "prosodic event recognition using convolutional neural networks with context information",
    "citation_count": 13,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/galvez17_interspeech.html": {
    "title": "Prosodic Facilitation and Interference While Judging on the Veracity of Synthesized Statements",
    "volume": "main",
    "abstract": "Two primary sources of information are provided in human speech. On the one hand, the verbal channel encodes linguistic content, while on the other hand, the vocal channel transmits paralinguistic information, mainly through prosody. In line with several studies that induce a conflict between these two channels to better understand the role of prosody, we conducted an experiment in which subjects had to listen to a series of statements synthesized with varying prosody and indicate if they believed them to be true or false. We find evidence suggesting that acoustic/prosodic (a/p) features of the synthesized statements affect response times (a well-known proxy for cognitive load). Our results suggest that prosody in synthesized speech may play a role of either facilitation or interference when subjects judge the truthfulness of a statement. Furthermore, we find that this pattern is amplified when the a/p features of the synthesized statements are analyzed relative to the subjects' own a/p features. This suggests that the entrainment of TTS voices has serious implications in the perceived trustworthiness of the system's skills",
    "checked": true,
    "id": "9101c7b8ae6976b83fdb567fdc02746af4321e20",
    "semantic_title": "prosodic facilitation and interference while judging on the veracity of synthesized statements",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zellers17_interspeech.html": {
    "title": "An Investigation of Pitch Matching Across Adjacent Turns in a Corpus of Spontaneous German",
    "volume": "main",
    "abstract": "Speakers in conversations may adapt their turn pitch relative to that of preceding turns to signal alignment with their interlocutor. However, the reference frame for pitch matching across turns is still unclear. Researchers studying pitch in the context of conversation have argued for an initializing approach, in which turn pitch must be judged relative to pitch in preceding turns. However, perceptual studies have indicated that listeners are able to reliably identify the location of pitch values within an individual speaker's range; that is, even without conversational context, they are able to normalize to speakers. This would imply that speakers might match normalized pitch instead of absolute pitch. Using a combined quantitative-qualitative approach, we investigate the relationship between pitch in adjacent turns in spontaneous German conversation. We use two different methods of evaluating pitch in adjacent turns, reflecting normalizing and initializing approaches respectively. We find that the results are well correlated with conversational participants' evaluation of the conversation. Furthermore, evaluating locations with matched or mismatched pitch can help distinguish between blind and face-to-face conversational situations, as well as identifying locations where specific discourse strategies (such as tag questions) have been deployed",
    "checked": true,
    "id": "684fa06d5d16e87dc646016f90f766a8c6fe6432",
    "semantic_title": "an investigation of pitch matching across adjacent turns in a corpus of spontaneous german",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mukherjee17_interspeech.html": {
    "title": "The Relationship Between F0 Synchrony and Speech Convergence in Dyadic Interaction",
    "volume": "main",
    "abstract": "Speech accommodation happens when two people engage in verbal conversation. In this paper two types of accommodation are investigated — one dependent on cognitive, physiological, functional and social constraints (Convergence), the other dependent on linguistic and paralinguistic factors (Synchrony). Convergence refers to the situation when two speakers' speech characteristics move towards a common point. Synchrony happens if speakers' prosodic features become correlated over time. Here we analyze relations between the two phenomena at the single word level. Although calculation of Synchrony is fairly straightforward, measuring Convergence is even more problematic as proved by a long history of debates on how to define it. In this paper we consider Convergence as an emergent behavior and investigate it by developing a robust and automatic method based on Gaussian Mixture Model (GMM). Our results show that high Synchrony of F0 between two speakers leads to greater amount of Convergence. This provides robust support for the idea that Synchrony and Convergence are interrelated processes, particularly in female participants",
    "checked": true,
    "id": "d609d389ef1788e4f9c9ec84876d3677b6ef3f50",
    "semantic_title": "the relationship between f0 synchrony and speech convergence in dyadic interaction",
    "citation_count": 13,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/luque17_interspeech.html": {
    "title": "The Role of Linguistic and Prosodic Cues on the Prediction of Self-Reported Satisfaction in Contact Centre Phone Calls",
    "volume": "main",
    "abstract": "Call Centre data is typically collected by organizations and corporations in order to ensure the quality of service, supporting for example mining capabilities for monitoring customer satisfaction. In this work, we analyze the significance of various acoustic features extracted from customer-agents' spoken interaction in predicting self-reported satisfaction by the customer. We also investigate whether speech prosodic features can deliver complementary information to speech transcriptions provided by an ASR. We explore the possibility of using a deep neural architecture to perform early feature fusion on both prosodic and linguistic information. Convolutional Neural Networks are trained on a combination of word embedding and acoustic features for the binary classification task of \"low\" and \"high\" satisfaction prediction. We conducted our experiments analysing real call-centre interactions of a large corporation in a Spanish spoken country. Our experiments show that linguistic features can predict self-reported satisfaction more accurately than those based on prosodic and conversational descriptors. We also find that dialog turn-level conversational features generally outperforms frame-level signal descriptors. Finally, the fusion of linguistic and prosodic features reports the best performance in our experiments, suggesting the complementarity of the information conveyed by each set of behavioral representation",
    "checked": true,
    "id": "3da8c39451c8a2725833d554c74e2678c8f1db8a",
    "semantic_title": "the role of linguistic and prosodic cues on the prediction of self-reported satisfaction in contact centre phone calls",
    "citation_count": 16,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/brusco17_interspeech.html": {
    "title": "Cross-Linguistic Study of the Production of Turn-Taking Cues in American English and Argentine Spanish",
    "volume": "main",
    "abstract": "We present the results of a series of machine learning experiments aimed at exploring the differences and similarities in the production of turn-taking cues in American English and Argentine Spanish. An analysis of prosodic features automatically extracted from 21 dyadic conversations (12 En, 9 Sp) revealed that, when signaling Holds, speakers of both languages tend to use roughly the same combination of cues, characterized by a sustained final intonation, a shorter duration of turn-final inter-pausal units, and a distinct voice quality. However, in speech preceding Smooth Switches or Backchannels, we observe the existence of the same set of prosodic turn-taking cues in both languages, although the ways in which these cues are combined together to form complex signals differ. Still, we find that these differences do not degrade below chance the performance of cross-linguistic systems for automatically detecting turn-taking signals. These results are relevant to the construction of multilingual spoken dialogue systems, which need to adapt not only their ASR modules but also the way prosodic turn-taking cues are synthesized and recognized",
    "checked": true,
    "id": "5094ad4eb0d45ecd555f3d5e1d6630366c7f7512",
    "semantic_title": "cross-linguistic study of the production of turn-taking cues in american english and argentine spanish",
    "citation_count": 12,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/egorow17_interspeech.html": {
    "title": "Emotional Features for Speech Overlaps Classification",
    "volume": "main",
    "abstract": "One interesting phenomenon of natural conversation is overlapping speech. Besides causing difficulties in automatic speech processing, such overlaps carry information on the state of the overlapper: competitive overlaps (i.e. \"interruptions\") can signal disagreement or the feeling of being overlooked, and cooperative overlaps (i.e. supportive interjections) can signal agreement and interest. These hints can be used to improve human-machine interaction. In this paper we present an approach for automatic classification of competitive and cooperative overlaps using the emotional content of the speakers' utterances before and after the overlap. For these experiments, we use real-world data from human-human interactions in call centres. We also compare our approach to standard acoustic classification on the same data and come to the conclusion, that emotional features are clearly superior to acoustic features for this task, resulting in an unweighted average f-measure of 71.9%. But we also find that acoustic features should not be entirely neglected: using a late fusion procedure, we can further improve the unweighted average f-measure by 2.6%",
    "checked": true,
    "id": "b6e7bc9b94ac4557ddae7f236d42e2cbc605590e",
    "semantic_title": "emotional features for speech overlaps classification",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17j_interspeech.html": {
    "title": "Computing Multimodal Dyadic Behaviors During Spontaneous Diagnosis Interviews Toward Automatic Categorization of Autism Spectrum Disorder",
    "volume": "main",
    "abstract": "Autism spectrum disorder (ASD) is a highly-prevalent neural developmental disorder often characterized by social communicative deficits and restricted repetitive interest. The heterogeneous nature of ASD in its behavior manifestations encompasses broad syndromes such as, Classical Autism (AD), High-functioning Autism (HFA), and Asperger syndrome (AS). In this work, we compute a variety of multimodal behavior features, including body movements, acoustic characteristics, and turn-taking events dynamics, of the participant, the investigator and the interaction between the two directly from audio-video recordings by leveraging the Autism Diagnostic Observational Schedule (ADOS) as a clinically-valid behavior data elicitation technique. Several of these signal-derived behavioral measures show statistically significant differences among the three syndromes. Our analyses indicate that these features may be pointing to the underlying differences in the behavior characterizations of social functioning between AD, AS, and HFA — corroborating some of the previous literature. Further, our signal-derived behavior measures achieve competitive, sometimes exceeding, recognition accuracies in discriminating between the three syndromes of ASD when compared to investigator's clinical-rating on participant's social and communicative behaviors during ADOS",
    "checked": true,
    "id": "6558a2b7f85af76f82ff6c8f48cba761bcf38447",
    "semantic_title": "computing multimodal dyadic behaviors during spontaneous diagnosis interviews toward automatic categorization of autism spectrum disorder",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lin17_interspeech.html": {
    "title": "Deriving Dyad-Level Interaction Representation Using Interlocutors Structural and Expressive Multimodal Behavior Features",
    "volume": "main",
    "abstract": "The overall interaction atmosphere is often a result of complex interplay between individual interlocutor's behavior expressions and joint manifestation of dyadic interaction dynamics. There is very limited work, if any, that has computationally analyzed a human interaction at the dyad-level. Hence, in this work, we propose to compute an extensive novel set of features representing multi-faceted aspects of a dyadic interaction. These features are grouped into two broad categories: expressive and structural behavior dynamics, where each captures information about within-speaker behavior manifestation, inter-speaker behavior dynamics, durational and transitional statistics providing holistic behavior quantifications at the dyad-level. We carry out an experiment of recognizing targeted affective atmosphere using the proposed expressive and structural behavior dynamics features derived from audio and video modalities. Our experiment shows that the inclusion of both expressive and structural behavior dynamics is essential in achieving promising recognition accuracies across six different classes (72.5%), where structural-based features improve the recognition rates on classes of sad and surprise. Further analyses reveal important aspects of multimodal behavior dynamics within dyadic interactions that are related to the affective atmospheric scene",
    "checked": true,
    "id": "b4465ad53049aa4fbbf947424adde3008c777390",
    "semantic_title": "deriving dyad-level interaction representation using interlocutors structural and expressive multimodal behavior features",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/brueckner17_interspeech.html": {
    "title": "Spotting Social Signals in Conversational Speech over IP: A Deep Learning Perspective",
    "volume": "main",
    "abstract": "The automatic detection and classification of social signals is an important task, given the fundamental role nonverbal behavioral cues play in human communication. We present the first cross-lingual study on the detection of laughter and fillers in conversational and spontaneous speech collected ‘in the wild' over IP (internet protocol). Further, this is the first comparison of LSTM and GRU networks to shed light on their performance differences. We report frame-based results in terms of the unweighted-average area-under-the-curve (UAAUC) measure and will shortly discuss its suitability for this task. In the mono-lingual setup our best deep BLSTM system achieves 87.0% and 86.3% UAAUC for English and German, respectively. Interestingly, the cross-lingual results are only slightly lower, yielding 83.7% for a system trained on English, but tested on German, and 85.0% in the opposite case. We show that LSTM and GRU architectures are valid alternatives for e. g., on-line and compute-sensitive applications, since their application incurs a relative UAAUC decrease of only approximately 5% with respect to our best systems. Finally, we apply additional smoothing to correct for erroneous spikes and drops in the posterior trajectories to obtain an additional gain in all setups",
    "checked": true,
    "id": "ad5ac1382a7762bb263c295e9fab9536e33e64a8",
    "semantic_title": "spotting social signals in conversational speech over ip: a deep learning perspective",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gosztolya17_interspeech.html": {
    "title": "Optimized Time Series Filters for Detecting Laughter and Filler Events",
    "volume": "main",
    "abstract": "Social signal detection, that is, the task of identifying vocalizations like laughter and filler events is a popular task within computational paralinguistics. Recent studies have shown that besides applying state-of-the-art machine learning methods, it is worth making use of the contextual information and adjusting the frame-level scores based on the local neighbourhood. In this study we apply a weighted average time series smoothing filter for laughter and filler event identification, and set the weights using a state-of-the-art optimization method, namely the Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Our results indicate that this is a viable way of improving the Area Under the Curve (AUC) scores: our resulting scores are much better than the accuracy scores of the raw likelihoods produced by Deep Neural Networks trained on three different feature sets, and we also significantly outperform standard time series filters as well as DNNs used for smoothing. Our score achieved on the test set of a public English database containing spontaneous mobile phone conversations is the highest one published so far that was realized by feed-forward techniques",
    "checked": true,
    "id": "ed00dfcb05902e22b002b0ee141ce13270838f75",
    "semantic_title": "optimized time series filters for detecting laughter and filler events",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/haider17_interspeech.html": {
    "title": "Visual, Laughter, Applause and Spoken Expression Features for Predicting Engagement Within TED Talks",
    "volume": "main",
    "abstract": "There is an enormous amount of audio-visual content available on-line in the form of talks and presentations. The prospective users of the content face difficulties in finding the right content for them. However, automatic detection of interesting (engaging vs. non-engaging) content can help users to find the videos according to their preferences. It can also be helpful for a recommendation and personalised video segmentation system. This paper presents a study of engagement based on TED talks (1338 videos) which are rated by on-line viewers (users). It proposes novel models to predict the user's (on-line viewers) engagement using high-level visual features (camera angles), the audience's laughter and applause, and the presenter's speech expressions. The results show that these features contribute towards the prediction of user engagement in these talks. However, finding the engaging speech expressions can also help a system in making summaries of TED Talks (video summarization) and creating feedback to presenters about their speech expressions during talks",
    "checked": true,
    "id": "4d4b03ee65de9c6f1d6afcf47b60f6057bb4f6de",
    "semantic_title": "visual, laughter, applause and spoken expression features for predicting engagement within ted talks",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/li17j_interspeech.html": {
    "title": "Large-Scale Domain Adaptation via Teacher-Student Learning",
    "volume": "main",
    "abstract": "High accuracy speech recognition requires a large amount of transcribed data for supervised training. In the absence of such data, domain adaptation of a well-trained acoustic model can be performed, but even here, high accuracy usually requires significant labeled data from the target domain. In this work, we propose an approach to domain adaptation that does not require transcriptions but instead uses a corpus of unlabeled parallel data, consisting of pairs of samples from the source domain of the well-trained model and the desired target domain. To perform adaptation, we employ teacher/student (T/S) learning, in which the posterior probabilities generated by the source-domain model can be used in lieu of labels to train the target-domain model. We evaluate the proposed approach in two scenarios, adapting a clean acoustic model to noisy speech and adapting an adults' speech acoustic model to children's speech. Significant improvements in accuracy are obtained, with reductions in word error rate of up to 44% over the original source model without the need for transcribed data in the target domain. Moreover, we show that increasing the amount of unlabeled data results in additional model robustness, which is particularly beneficial when using simulated training data in the target-domain",
    "checked": true,
    "id": "195d13a69e736024360878521124c3016ae9ef1f",
    "semantic_title": "large-scale domain adaptation via teacher-student learning",
    "citation_count": 123,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ahmad17_interspeech.html": {
    "title": "Improving Children's Speech Recognition Through Explicit Pitch Scaling Based on Iterative Spectrogram Inversion",
    "volume": "main",
    "abstract": "The task of transcribing children's speech using statistical models trained on adults' speech is very challenging. Large mismatch in the acoustic and linguistic attributes of the training and test data is reported to degrade the performance. In such speech recognition tasks, the differences in pitch (or fundamental frequency) between the two groups of speakers is one among several mismatch factors. To overcome the pitch mismatch, an existing pitch scaling technique based on iterative spectrogram inversion is explored in this work. Explicit pitch scaling is found to improve the recognition of children's speech under mismatched setup. In addition to that, we have also studied the effect of discarding the phase information during spectrum reconstruction. This is motivated by the fact that the dominant acoustic feature extraction techniques make use of the magnitude spectrum only. On evaluating the effectiveness under mismatched testing scenario, the existing as well as the modified pitch scaling techniques result in very similar recognition performances. Furthermore, we have explored the role of pitch scaling on another speech recognition system which is trained on speech data from both adult and child speakers. Pitch scaling is noted to be effective for children's speech recognition in this case as well",
    "checked": true,
    "id": "43fb385181f9cf1af9120a07039309533b700501",
    "semantic_title": "improving children's speech recognition through explicit pitch scaling based on iterative spectrogram inversion",
    "citation_count": 12,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/xie17_interspeech.html": {
    "title": "RNN-LDA Clustering for Feature Based DNN Adaptation",
    "volume": "main",
    "abstract": "Model based deep neural network (DNN) adaptation approaches often require multi-pass decoding in test time. Input feature based DNN adaptation, for example, based on latent Dirichlet allocation (LDA) clustering, provide a more efficient alternative. In conventional LDA clustering, the transition and correlation between neighboring clusters is ignored. In order to address this issue, a recurrent neural network (RNN) based clustering scheme is proposed to learn both the standard LDA cluster labels and their natural correlation over time in this paper. In addition to directly using the resulting RNN-LDA as input features during DNN adaptation, a range of techniques were investigated to condition the DNN hidden layer parameters or activation outputs on the RNN-LDA features. On a DARPA Gale Mandarin Chinese broadcast speech transcription task, the proposed RNN-LDA cluster features adapted DNN system outperformed both the baseline un-adapted DNN system and conventional LDA features adapted DNN system by 8% relative on the most difficult Phoenix TV subset. Consistent improvements were also obtained after further combination with model based adaptation approaches",
    "checked": true,
    "id": "4f43698bd9c03f373dbb3e8b47d0cf42d9c79777",
    "semantic_title": "rnn-lda clustering for feature based dnn adaptation",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/arsikere17_interspeech.html": {
    "title": "Robust Online i-Vectors for Unsupervised Adaptation of DNN Acoustic Models: A Study in the Context of Digital Voice Assistants",
    "volume": "main",
    "abstract": "Supplementing log filter-bank energies with i-vectors is a popular method for adaptive training of deep neural network acoustic models. While offline i-vectors (the target utterance or other relevant adaptation material is available for i-vector extraction prior to decoding) have been well studied, there is little analysis of online i-vectors and their robustness in multi-user scenarios where speaker changes can be frequent and unpredictable. The authors of [1] showed that online adaptation could be achieved through segmental i-vectors computed using the hidden Markov model (HMM) state alignments of utterances decoded in the recent past. While this approach works well in general, it could be rendered ineffective by speaker changes. In this paper, we study robust extensions of the ideas proposed in [1] by: (a) updating i-vectors on a per-frame basis based on the incoming target utterance, and (b) using lattice posteriors instead of one-best HMM state alignments. Experiments with different i-vector implementations show that: (a) when speaker changes occur, lattice-based frame-level i-vectors provide up to 6% word error rate reduction relative to the baseline [1], and (b) online i-vectors are more effective, in general, when the microphone characteristics of test utterances are not seen in training",
    "checked": true,
    "id": "11480a59acc95062ef37c300c30ac4bbc14d20da",
    "semantic_title": "robust online i-vectors for unsupervised adaptation of dnn acoustic models: a study in the context of digital voice assistants",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/srinivasamurthy17_interspeech.html": {
    "title": "Semi-Supervised Learning with Semantic Knowledge Extraction for Improved Speech Recognition in Air Traffic Control",
    "volume": "main",
    "abstract": "Automatic Speech Recognition (ASR) can introduce higher levels of automation into Air Traffic Control (ATC), where spoken language is still the predominant form of communication. While ATC uses standard phraseology and a limited vocabulary, we need to adapt the speech recognition systems to local acoustic conditions and vocabularies at each airport to reach optimal performance. Due to continuous operation of ATC systems, a large and increasing amount of untranscribed speech data is available, allowing for semi-supervised learning methods to build and adapt ASR models. In this paper, we first identify the challenges in building ASR systems for specific ATC areas and propose to utilize out-of-domain data to build baseline ASR models. Then we explore different methods of data selection for adapting baseline models by exploiting the continuously increasing untranscribed data. We develop a basic approach capable of exploiting semantic representations of ATC commands. We achieve relative improvement in both word error rate (23.5%) and concept error rates (7%) when adapting ASR models to different ATC conditions in a semi-supervised manner",
    "checked": true,
    "id": "a53810d0af1b7be7b146d74c1659d19e20061dee",
    "semantic_title": "semi-supervised learning with semantic knowledge extraction for improved speech recognition in air traffic control",
    "citation_count": 42,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kim17f_interspeech.html": {
    "title": "Dynamic Layer Normalization for Adaptive Neural Acoustic Modeling in Speech Recognition",
    "volume": "main",
    "abstract": "Layer normalization is a recently introduced technique for normalizing the activities of neurons in deep neural networks to improve the training speed and stability. In this paper, we introduce a new layer normalization technique called Dynamic Layer Normalization (DLN) for adaptive neural acoustic modeling in speech recognition. By dynamically generating the scaling and shifting parameters in layer normalization, DLN adapts neural acoustic models to the acoustic variability arising from various factors such as speakers, channel noises, and environments. Unlike other adaptive acoustic models, our proposed approach does not require additional adaptation data or speaker information such as i-vectors. Moreover, the model size is fixed as it dynamically generates adaptation parameters. We apply our proposed DLN to deep bidirectional LSTM acoustic models and evaluate them on two benchmark datasets for large vocabulary ASR experiments: WSJ and TED-LIUM release 2. The experimental results show that our DLN improves neural acoustic models in terms of transcription accuracy by dynamically adapting to various speakers and environments",
    "checked": true,
    "id": "11f9732e22bedf2a6d9fa710940545d36815403c",
    "semantic_title": "dynamic layer normalization for adaptive neural acoustic modeling in speech recognition",
    "citation_count": 59,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bosker17b_interspeech.html": {
    "title": "An Entrained Rhythm's Frequency, Not Phase, Influences Temporal Sampling of Speech",
    "volume": "main",
    "abstract": "Brain oscillations have been shown to track the slow amplitude fluctuations in speech during comprehension. Moreover, there is evidence that these stimulus-induced cortical rhythms may persist even after the driving stimulus has ceased. However, how exactly this neural entrainment shapes speech perception remains debated. This behavioral study investigated whether and how the frequency and phase of an entrained rhythm would influence the temporal sampling of subsequent speech In two behavioral experiments, participants were presented with slow and fast isochronous tone sequences, followed by Dutch target words ambiguous between as /ɑs/ \"ash\" (with a short vowel) and aas /a:s/ \"bait\" (with a long vowel). Target words were presented at various phases of the entrained rhythm. Both experiments revealed effects of the frequency of the tone sequence on target word perception: fast sequences biased listeners to more long /a:s/ responses. However, no evidence for phase effects could be discerned These findings show that an entrained rhythm's frequency, but not phase, influences the temporal sampling of subsequent speech. These outcomes are compatible with theories suggesting that sensory timing is evaluated relative to entrained frequency. Furthermore, they suggest that phase tracking of (syllabic) rhythms by theta oscillations plays a limited role in speech parsing",
    "checked": true,
    "id": "2a47198c74f1f37f1f1c2e7639f24add117f1937",
    "semantic_title": "an entrained rhythm's frequency, not phase, influences temporal sampling of speech",
    "citation_count": 17,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wang17j_interspeech.html": {
    "title": "Context Regularity Indexed by Auditory N1 and P2 Event-Related Potentials",
    "volume": "main",
    "abstract": "It is still a question of debate whether the N1-P2 complex is an index of low-level auditory processes or whether it can capture higher-order information encoded in the immediate context. To address this issue, the current study examined the morphology of the N1-P2 complex as a function of context regularities instantiated at the sublexical level. We presented two types of speech targets in isolation and in contexts comprising sequences of Cantonese words sharing either the entire rime units or just the rime segments (thus lacking lexical tone consistency). Results revealed a pervasive yet unequal attenuation of the N1 and P2 components: The degree of N1 attenuation tended to decrease while that of P2 increased due to enhanced detectability of more regular speech patterns, as well as their enhanced predictability in the immediate context. The distinct behaviors of N1 and P2 event-related potentials could be explained by the influence of perceptual experience and the hierarchical encoding of context regularities",
    "checked": true,
    "id": "8ddc341c03c57134e351d636cb39b409bf99e716",
    "semantic_title": "context regularity indexed by auditory n1 and p2 event-related potentials",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/verma17_interspeech.html": {
    "title": "Discovering Language in Marmoset Vocalization",
    "volume": "main",
    "abstract": "Various studies suggest that marmosets ( Callithrix jacchus) show behavior similar to that of humans in many aspects. Analyzing their calls would not only enable us to better understand these species but would also give insights into the evolution of human languages and vocal tract. This paper describes a technique to discover the patterns in marmoset vocalization in an unsupervised fashion. The proposed unsupervised clustering approach operates in two stages. Initially, voice activity detection (VAD) is applied to remove silences and non-voiced regions from the audio. This is followed by a group-delay based segmentation on the voiced regions to obtain smaller segments. In the second stage, a two-tier clustering is performed on the segments obtained. Individual hidden Markov models (HMMs) are built for each of the segments using a multiple frame size and multiple frame rate. The HMMs are then clustered until each cluster is made up of a large number of segments. Once all the clusters get enough number of segments, one Gaussian mixture model (GMM) is built for each of the clusters. These clusters are then merged using Kullback-Leibler (KL) divergence. The algorithm converges to the total number of distinct sounds in the audio, as evidenced by listening tests",
    "checked": true,
    "id": "7eb45222f4222bc40390244bf14b5dafaa0c7a67",
    "semantic_title": "discovering language in marmoset vocalization",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/watanabe17_interspeech.html": {
    "title": "Subject-Independent Classification of Japanese Spoken Sentences by Multiple Frequency Bands Phase Pattern of EEG Response During Speech Perception",
    "volume": "main",
    "abstract": "Recent speech perception models propose that neural oscillations in theta band show phase locking to speech envelope to extract syllabic information and rapid temporal information is processed by the corresponding higher frequency band (e.g., low gamma). It is suggested that phase-locked responses to acoustic features show consistent patterns across subjects. Previous magnetoencephalographic (MEG) experiment showed that subject-dependent template matching classification by theta phase patterns could discriminate three English spoken sentences. In this paper, we adopt electroencephalography (EEG) to the spoken sentence discrimination on Japanese language, and we investigate the performances in various different settings by using: (1) template matching and support vector machine (SVM) classifiers; (2) subject dependent and independent models; (3) multiple frequency bands including theta, alpha, beta, low gamma, and the combination of all frequency bands. The performances in almost settings were higher than the chance level. While performances of SVM and template matching did not differ, the performance with combination of multiple frequency bands outperformed the one that trained only on single frequency bands. Best accuracies in subject dependent and independent models achieved 55.2% by SVM on the combination of all frequency bands and 44.0% by template matching on the combination of all frequency bands, respectively",
    "checked": true,
    "id": "717a256f669526db893f83d3fa2bad337d8456dd",
    "semantic_title": "subject-independent classification of japanese spoken sentences by multiple frequency bands phase pattern of eeg response during speech perception",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/rietmolen17_interspeech.html": {
    "title": "The Phonological Status of the French Initial Accent and its Role in Semantic Processing: An Event-Related Potentials Study",
    "volume": "main",
    "abstract": "French accentuation is held to belong to the level of the phrase. Consequently French is considered ‘a language without accent' with speakers that are ‘deaf to stress'. Recent ERP-studies investigating the French initial accent (IA) however demonstrate listeners not only discriminate between different stress patterns, but also prefer words to be marked with IA early in the process of speech comprehension. Still, as words were presented in isolation, it remains unclear whether the preference applied to the lexical or to the phrasal level. In the current ERP-study, we address this ambiguity and manipulate IA on words embedded in a sentence. Furthermore, we orthogonally manipulate semantic congruity to investigate the interplay between accentuation and later speech processing stages. Preliminary results on 14 participants reveal a significant interaction effect: the centro-frontally located N400 was larger for words without IA, with a bigger effect for semantically incongruent sentences. This indicates that IA is encoded at a lexical level and facilitates semantic processing. Furthermore, as participants attended to the semantic content of the sentences, the finding underlines the automaticity of stress processing. In sum, we demonstrate accentuation plays an important role in French speech comprehension and call for the traditional view to be reconsidered",
    "checked": true,
    "id": "1fb882c53c58403ddf5ed294aaae406c67b8fdd7",
    "semantic_title": "the phonological status of the french initial accent and its role in semantic processing: an event-related potentials study",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhao17_interspeech.html": {
    "title": "A Neuro-Experimental Evidence for the Motor Theory of Speech Perception",
    "volume": "main",
    "abstract": "The somatotopic activation in the sensorimotor cortex during speech comprehension has been redundantly documented and largely explained by the notion of embodied semantics, which suggests that processing auditory words referring to body movements recruits the same somatotopic regions for that action execution. For this issue, the motor theory of speech perception provided another explanation, suggesting that the perception of speech sounds produced by a specific articulator movement may recruit the motor representation of that articulator in the precentral gyrus. To examine the latter theory, we used a set of Chinese synonyms with different articulatory features, involving lip gestures (LipR) or not (LipN), and recorded the electroencephalographic (EEG) signals while subjects passively listened to them. It was found that at about 200 ms post-onset, the event-related potential of LipR and LipN showed a significant polarity reversal near the precentral lip motor areas. EEG source reconstruction results also showed more obvious somatotopic activation in the lip region for the LipR than the LipN. Our results provide a positive support for the effect of articulatory simulation on speech comprehension and basically agree with the motor theory of speech perception",
    "checked": true,
    "id": "d3e35559d16e505ad7e8e91b6bffdc4cc1b84f9c",
    "semantic_title": "a neuro-experimental evidence for the motor theory of speech perception",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/agrawal17_interspeech.html": {
    "title": "Speech Representation Learning Using Unsupervised Data-Driven Modulation Filtering for Robust ASR",
    "volume": "main",
    "abstract": "The performance of an automatic speech recognition (ASR) system degrades severely in noisy and reverberant environments in part due to the lack of robustness in the underlying representations used in the ASR system. On the other hand, the auditory processing studies have shown the importance of modulation filtered spectrogram representations in robust human speech recognition. Inspired by these evidences, we propose a speech representation learning paradigm using data-driven 2-D spectro-temporal modulation filter learning. In particular, multiple representations are derived using the convolutional restricted Boltzmann machine (CRBM) model in an unsupervised manner from the input speech spectrogram. A filter selection criteria based on average number of active hidden units is also employed to select the representations for ASR. The experiments are performed on Wall Street Journal (WSJ) Aurora-4 database with clean and multi condition training setup. In these experiments, the ASR results obtained from the proposed modulation filtering approach shows significant robustness to noise and channel distortions compared to other feature extraction methods (average relative improvements of 19% over baseline features in clean training). Furthermore, the ASR experiments performed on reverberant speech data from the REVERB challenge corpus highlight the benefits of the proposed representation learning scheme for far field speech recognition",
    "checked": true,
    "id": "50278a4adecbc1b54449ec907ae21978c4a75362",
    "semantic_title": "speech representation learning using unsupervised data-driven modulation filtering for robust asr",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mimura17_interspeech.html": {
    "title": "Combined Multi-Channel NMF-Based Robust Beamforming for Noisy Speech Recognition",
    "volume": "main",
    "abstract": "We propose a novel acoustic beamforming method using blind source separation (BSS) techniques based on non-negative matrix factorization (NMF). In conventional mask-based approaches, hard or soft masks are estimated and beamforming is performed using speech and noise spatial covariance matrices calculated from masked noisy observations, but the phase information of the target speech is not adequately preserved. In the proposed method, we perform complex-domain source separation based on multi-channel NMF with rank-1 spatial model (rank-1 MNMF) to obtain a speech spatial covariance matrix for estimating a steering vector for the target speech utilizing the separated speech observation in each time-frequency bin. This accurate steering vector estimation is effectively combined with our novel noise mask prediction method using multi-channel robust NMF (MRNMF) to construct a Maximum Likelihood (ML) beamformer that achieved a better speech recognition performance than a state-of-the-art DNN-based beamformer with no environment-specific training. Superiority of the phase preserving source separation to real-valued masks in beamforming is also confirmed through ASR experiments",
    "checked": true,
    "id": "c5349d63b054c2f5fc75de622a86720c332e16b1",
    "semantic_title": "combined multi-channel nmf-based robust beamforming for noisy speech recognition",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/yu17b_interspeech.html": {
    "title": "Recognizing Multi-Talker Speech with Permutation Invariant Training",
    "volume": "main",
    "abstract": "In this paper, we propose a novel technique for direct recognition of multiple speech streams given the single channel of mixed speech, without first separating them. Our technique is based on permutation invariant training (PIT) for automatic speech recognition (ASR). In PIT-ASR, we compute the average cross entropy (CE) over all frames in the whole utterance for each possible output-target assignment, pick the one with the minimum CE, and optimize for that assignment. PIT-ASR forces all the frames of the same speaker to be aligned with the same output layer. This strategy elegantly solves the label permutation problem and speaker tracing problem in one shot. Our experiments on artificially mixed AMI data showed that the proposed approach is very promising",
    "checked": true,
    "id": "c7428d5e2687d082b0dfc4525cffb4a3d911026c",
    "semantic_title": "recognizing multi-talker speech with permutation invariant training",
    "citation_count": 78,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tachioka17_interspeech.html": {
    "title": "Coupled Initialization of Multi-Channel Non-Negative Matrix Factorization Based on Spatial and Spectral Information",
    "volume": "main",
    "abstract": "Multi-channel non-negative matrix factorization (MNMF) is a multi-channel extension of NMF and often outperforms NMF because it can deal with spatial and spectral information simultaneously. On the other hand, MNMF has a larger number of parameters and its performance heavily depends on the initial values. MNMF factorizes an observation matrix into four matrices: spatial correlation, basis, cluster-indicator latent variables, and activation matrices. This paper proposes effective initialization methods for these matrices. First, the spatial correlation matrix, which shows the largest initial value dependencies, is initialized using the cross-spectrum method from enhanced speech by binary masking. Second, when the target is speech, constructing bases from phonemes existing in an utterance can improve the performance: this paper proposes a speech bases selection by using automatic speech recognition (ASR). Third, we also propose an initialization method for the cluster-indicator latent variables that couple the spatial and spectral information, which can achieve the simultaneous optimization of above two matrices. Experiments on a noisy ASR task show that the proposed initialization significantly improves the performance of MNMF by reducing the initial value dependencies",
    "checked": true,
    "id": "e2f015bbddd7bade7caca693e37f84c4cf70a7f5",
    "semantic_title": "coupled initialization of multi-channel non-negative matrix factorization based on spatial and spectral information",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/loweimi17b_interspeech.html": {
    "title": "Channel Compensation in the Generalised Vector Taylor Series Approach to Robust ASR",
    "volume": "main",
    "abstract": "Vector Taylor Series (VTS) is a powerful technique for robust ASR but, in its standard form, it can only be applied to log-filter bank and MFCC features. In earlier work, we presented a generalised VTS (gVTS) that extends the applicability of VTS to front-ends which employ a power transformation non-linearity. gVTS was shown to provide performance improvements in both clean and additive noise conditions. This paper makes two novel contributions. Firstly, while the previous gVTS formulation assumed that noise was purely additive, we now derive gVTS formulae for the case of speech in the presence of both additive noise and channel distortion. Second, we propose a novel iterative method for estimating the channel distortion which utilises gVTS itself and converges after a few iterations. Since the new gVTS blindly assumes the existence of both additive noise and channel effects, it is important not to introduce extra distortion when either are absent. Experimental results conducted on LVCSR Aurora-4 database show that the new formulation passes this test. In the presence of channel noise only, it provides relative WER reductions of up to 30% and 26%, compared with previous gVTS and multi-style training with cepstral mean normalisation, respectively",
    "checked": true,
    "id": "c389a960f95a9e0895f3aa033c7c20a707a07e76",
    "semantic_title": "channel compensation in the generalised vector taylor series approach to robust asr",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/king17_interspeech.html": {
    "title": "Robust Speech Recognition via Anchor Word Representations",
    "volume": "main",
    "abstract": "A challenge for speech recognition for voice-controlled household devices, like the Amazon Echo or Google Home, is robustness against interfering background speech. Formulated as a far-field speech recognition problem, another person or media device in proximity can produce background speech that can interfere with the device-directed speech. We expand on our previous work on device-directed speech detection in the far-field speech setting and introduce two approaches for robust acoustic modeling. Both methods are based on the idea of using an anchor word taken from the device directed speech. Our first method employs a simple yet effective normalization of the acoustic features by subtracting the mean derived over the anchor word. The second method utilizes an encoder network projecting the anchor word onto a fixed-size embedding, which serves as an additional input to the acoustic model. The encoder network and acoustic model are jointly trained. Results on an in-house dataset reveal that, in the presence of background speech, the proposed approaches can achieve up to 35% relative word error rate reduction",
    "checked": true,
    "id": "3f9ef7cd48bc5bea7dbd7d9b022575487d836a23",
    "semantic_title": "robust speech recognition via anchor word representations",
    "citation_count": 30,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bapna17_interspeech.html": {
    "title": "Towards Zero-Shot Frame Semantic Parsing for Domain Scaling",
    "volume": "main",
    "abstract": "State-of-the-art slot filling models for goal-oriented human/ machine conversational language understanding systems rely on deep learning methods. While multi-task training of such models alleviates the need for large in-domain annotated datasets, bootstrapping a semantic parsing model for a new domain using only the semantic frame, such as the back-end API or knowledge graph schema, is still one of the holy grail tasks of language understanding for dialogue systems. This paper proposes a deep learning based approach that can utilize only the slot description in context without the need for any labeled or unlabeled in-domain examples, to quickly bootstrap a new domain. The main idea of this paper is to leverage the encoding of the slot names and descriptions within a multi-task deep learned slot filling model, to implicitly align slots across domains. The proposed approach is promising for solving the domain scaling problem and eliminating the need for any manually annotated data or explicit schema alignment. Furthermore, our experiments on multiple domains show that this approach results in significantly better slot-filling performance when compared to using only in-domain data, especially in the low data regime",
    "checked": true,
    "id": "39a8f7ec557819f602b6185017d8bc48edc5b8c0",
    "semantic_title": "towards zero-shot frame semantic parsing for domain scaling",
    "citation_count": 119,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/georgiadou17_interspeech.html": {
    "title": "ClockWork-RNN Based Architectures for Slot Filling",
    "volume": "main",
    "abstract": "A prevalent and challenging task in spoken language understanding is slot filling. Currently, the best approaches in this domain are based on recurrent neural networks (RNNs). However, in their simplest form, RNNs cannot learn long-term dependencies in the data. In this paper, we propose the use of ClockWork recurrent neural network (CW-RNN) architectures in the slot-filling domain. CW-RNN is a multi-timescale implementation of the simple RNN architecture, which has proven to be powerful since it maintains relatively small model complexity. In addition, CW-RNN exhibits a great ability to model long-term memory inherently. In our experiments on the ATIS benchmark data set, we also evaluate several novel variants of CW-RNN and we find that they significantly outperform simple RNNs and they achieve results among the state-of-the-art, while retaining smaller complexity",
    "checked": true,
    "id": "f714986325c7ace2601c38c22ff9682d380be31a",
    "semantic_title": "clockwork-rnn based architectures for slot filling",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jannet17_interspeech.html": {
    "title": "Investigating the Effect of ASR Tuning on Named Entity Recognition",
    "volume": "main",
    "abstract": "Information retrieval from speech is a key technology for many applications, as it allows access to large amounts of audio data. This technology requires two major components: an automatic speech recognizer (ASR) and a text-based information retrieval module such as a key word extractor or a named entity recognizer (NER). When combining the two components, the resulting final application needs to be globally optimized. However, ASR and information retrieval are usually developed and optimized separately. The ASR tends to be optimized to reduce the word error rate (WER), a metric which does not take into account the contextual and syntactic roles of the words, which are valuable information for information retrieval systems. In this paper we investigate different ways to tune the ASR for a speech-based NER system. In an end-to-end configuration we also tested several ASR metrics, including WER, NE-WER and ATENE, as well as the use of an oracle during the development step. Our results show that using a NER oracle to tune the system reduces the named entity recognition error rate by more than 1% absolute, and using the ATENE metric allows us to reduce it by more than 0.75%. We also show that these optimization approaches favor a higher ASR language model weight which entails an overall gain in NER performance, despite a local increase of the WER",
    "checked": true,
    "id": "f47b1c35f37aa1768934d4a2e83b0eff9e607657",
    "semantic_title": "investigating the effect of asr tuning on named entity recognition",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/dinarelli17_interspeech.html": {
    "title": "Label-Dependency Coding in Simple Recurrent Networks for Spoken Language Understanding",
    "volume": "main",
    "abstract": "Modeling target label dependencies is important for sequence labeling tasks. This may become crucial in the case of Spoken Language Understanding (SLU) applications, especially for the slot-filling task where models have to deal often with a high number of target labels. Conditional Random Fields (CRF) were previously considered as the most efficient algorithm in these conditions. More recently, different architectures of Recurrent Neural Networks (RNNs) have been proposed for the SLU slot-filling task. Most of them, however, have been successfully evaluated on the simple ATIS database, on which it is difficult to draw significant conclusions. In this paper we propose new variants of RNNs able to learn efficiently and effectively label dependencies by integrating label embeddings. We show first that modeling label dependencies is useless on the (simple) ATIS database and unstructured models can produce state-of-the-art results on this benchmark. On ATIS our new variants achieve the same results as state-of-the-art models, while being much simpler. On the other hand, on the MEDIA benchmark, we show that the modification introduced in the proposed RNN outperforms traditional RNNs and CRF models",
    "checked": true,
    "id": "b3e4d194ba3c376786b9992aa757e1f831b910d5",
    "semantic_title": "label-dependency coding in simple recurrent networks for spoken language understanding",
    "citation_count": 17,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/meng17_interspeech.html": {
    "title": "Minimum Semantic Error Cost Training of Deep Long Short-Term Memory Networks for Topic Spotting on Conversational Speech",
    "volume": "main",
    "abstract": "The topic spotting performance on spontaneous conversational speech can be significantly improved by operating a support vector machine with a latent semantic rational kernel (LSRK) on the decoded word lattices (i.e., weighted finite-state transducers) of the speech [1]. In this work, we propose the minimum semantic error cost (MSEC) training of a deep bidirectional long short-term memory (BLSTM)-hidden Markov model acoustic model for generating lattices that are semantically accurate and are better suited for topic spotting with LSRK. With the MSEC training, the expected semantic error cost of all possible word sequences on the lattices is minimized given the reference. The word-word semantic error cost is first computed from either the latent semantic analysis or distributed vector-space word representations learned from the recurrent neural networks and is then accumulated to form the expected semantic error cost of the hypothesized word sequences. The proposed method achieves 3.5%–4.5% absolute topic classification accuracy improvement over the baseline BLSTM trained with cross-entropy on Switchboard-1 Release 2 dataset",
    "checked": true,
    "id": "bd3543746c72b08e8570e4834ae711b073cd4443",
    "semantic_title": "minimum semantic error cost training of deep long short-term memory networks for topic spotting on conversational speech",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/liu17b_interspeech.html": {
    "title": "Topic Identification for Speech Without ASR",
    "volume": "main",
    "abstract": "Modern topic identification (topic ID) systems for speech use automatic speech recognition (ASR) to produce speech transcripts, and perform supervised classification on such ASR outputs. However, under resource-limited conditions, the manually transcribed speech required to develop standard ASR systems can be severely limited or unavailable. In this paper, we investigate alternative unsupervised solutions to obtaining tokenizations of speech in terms of a vocabulary of automatically discovered word-like or phoneme-like units, without depending on the supervised training of ASR systems. Moreover, using automatic phoneme-like tokenizations, we demonstrate that a convolutional neural network based framework for learning spoken document representations provides competitive performance compared to a standard bag-of-words representation, as evidenced by comprehensive topic ID evaluations on both single-label and multi-label classification tasks",
    "checked": true,
    "id": "018e05fc89727b8c2050a985995f8504296e27ae",
    "semantic_title": "topic identification for speech without asr",
    "citation_count": 20,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/liu17c_interspeech.html": {
    "title": "An End-to-End Trainable Neural Network Model with Belief Tracking for Task-Oriented Dialog",
    "volume": "main",
    "abstract": "We present a novel end-to-end trainable neural network model for task-oriented dialog systems. The model is able to track dialog state, issue API calls to knowledge base (KB), and incorporate structured KB query results into system responses to successfully complete task-oriented dialogs. The proposed model produces well-structured system responses by jointly learning belief tracking and KB result processing conditioning on the dialog history. We evaluate the model in a restaurant search domain using a dataset that is converted from the second Dialog State Tracking Challenge (DSTC2) corpus. Experiment results show that the proposed model can robustly track dialog state given the dialog history. Moreover, our model demonstrates promising results in producing appropriate system responses, outperforming prior end-to-end trainable neural network models using per-response accuracy evaluation metrics",
    "checked": true,
    "id": "d40a1975ae84ec937a41fd4956c34be5af6f17c0",
    "semantic_title": "an end-to-end trainable neural network model with belief tracking for task-oriented dialog",
    "citation_count": 95,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/cuayahuitl17_interspeech.html": {
    "title": "Deep Reinforcement Learning of Dialogue Policies with Less Weight Updates",
    "volume": "main",
    "abstract": "Deep reinforcement learning dialogue systems are attractive because they can jointly learn their feature representations and policies without manual feature engineering. But its application is challenging due to slow learning. We propose a two-stage method for accelerating the induction of single or multi-domain dialogue policies. While the first stage reduces the amount of weight updates over time, the second stage uses very limited minibatches (of as much as two learning experiences) sampled from experience replay memories. The former frequently updates the weights of the neural nets at early stages of training, and decreases the amount of updates as training progresses by performing updates during exploration and by skipping updates during exploitation. The learning process is thus accelerated through less weight updates in both stages. An empirical evaluation in three domains (restaurants, hotels and tv guide) confirms that the proposed method trains policies 5 times faster than a baseline without the proposed method. Our findings are useful for training larger-scale neural-based spoken dialogue systems",
    "checked": true,
    "id": "c77bca956cbb36e6cfe8bb240a49377da9a76124",
    "semantic_title": "deep reinforcement learning of dialogue policies with less weight updates",
    "citation_count": 17,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bayer17_interspeech.html": {
    "title": "Towards End-to-End Spoken Dialogue Systems with Turn Embeddings",
    "volume": "main",
    "abstract": "Training task-oriented dialogue systems requires significant amount of manual effort and integration of many independently built components; moreover, the pipeline is prone to error-propagation. End-to-end training has been proposed to overcome these problems by training the whole system over the utterances of both dialogue parties. In this paper we present an end-to-end spoken dialogue system architecture that is based on turn embeddings. Turn embeddings encode a robust representation of user turns with a local dialogue history and they are trained using sequence-to-sequence models. Turn embeddings are trained by generating the previous and the next turns of the dialogue and additionally perform spoken language understanding. The end-to-end spoken dialogue system is trained using the pre-trained turn embeddings in a stateful architecture that considers the whole dialogue history. We observe that the proposed spoken dialogue system architecture outperforms the models based on local-only dialogue history and it is robust to automatic speech recognition errors",
    "checked": true,
    "id": "2224509d98812d142680356aa1b84452fc3ee38f",
    "semantic_title": "towards end-to-end spoken dialogue systems with turn embeddings",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/akhtiamov17_interspeech.html": {
    "title": "Speech and Text Analysis for Multimodal Addressee Detection in Human-Human-Computer Interaction",
    "volume": "main",
    "abstract": "The necessity of addressee detection arises in multiparty spoken dialogue systems which deal with human-human-computer interaction. In order to cope with this kind of interaction, such a system is supposed to determine whether the user is addressing the system or another human. The present study is focused on multimodal addressee detection and describes three levels of speech and text analysis: acoustical, syntactical, and lexical. We define the connection between different levels of analysis and the classification performance for different categories of speech and determine the dependence of addressee detection performance on speech recognition accuracy. We also compare the obtained results with the results of the original research performed by the authors of the Smart Video Corpus which we use in our computations. Our most effective meta-classifier working with acoustical, syntactical, and lexical features reaches an unweighted average recall equal to 0.917 showing almost a nine percent advantage over the best baseline model, though this baseline classifier additionally uses head orientation data. We also propose a universal meta-model based on acoustical and syntactical analysis, which may theoretically be applied in different domains",
    "checked": true,
    "id": "f9f68bfb52907f9d5c09f1d923aec6c30f525d2d",
    "semantic_title": "speech and text analysis for multimodal addressee detection in human-human-computer interaction",
    "citation_count": 18,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ramanarayanan17c_interspeech.html": {
    "title": "Rushing to Judgement: How do Laypeople Rate Caller Engagement in Thin-Slice Videos of Human–Machine Dialog?",
    "volume": "main",
    "abstract": "We analyze the efficacy of a small crowd of naïve human raters in rating engagement during human–machine dialog interactions. Each rater viewed multiple 10 second, thin-slice videos of non-native English speakers interacting with a computer-assisted language learning (CALL) system and rated how engaged and disengaged those callers were while interacting with the automated agent. We observe how the crowd's ratings compared to callers' self ratings of engagement, and further study how the distribution of these rating assignments vary as a function of whether the automated system or the caller was speaking. Finally, we discuss the potential applications and pitfalls of such a crowdsourced paradigm in designing, developing and analyzing engagement-aware dialog systems",
    "checked": true,
    "id": "f83c4dda91ba498358d98be418f7931f11af4fde",
    "semantic_title": "rushing to judgement: how do laypeople rate caller engagement in thin-slice videos of human-machine dialog?",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kraljevski17_interspeech.html": {
    "title": "Hyperarticulation of Corrections in Multilingual Dialogue Systems",
    "volume": "main",
    "abstract": "This present paper aims at answering the question whether there are distinctive cross-linguistic differences associated with hyperarticulated speech in correction dialogue acts. The objective is to assess the effort for adaptation of a multilingual dialogue system in 9 different languages, regarding the recovery strategies, particularly corrections. If the presence of hyperarticulation significantly differs across languages, it will have a significant impact on the dialogue design and recovery strategies",
    "checked": true,
    "id": "2160e25b258f078ade7854f9b65417b70e0cbd84",
    "semantic_title": "hyperarticulation of corrections in multilingual dialogue systems",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/milde17_interspeech.html": {
    "title": "Multitask Sequence-to-Sequence Models for Grapheme-to-Phoneme Conversion",
    "volume": "main",
    "abstract": "Recently, neural sequence-to-sequence (Seq2Seq) models have been applied to the problem of grapheme-to-phoneme (G2P) conversion. These models offer a straightforward way of modeling the conversion by jointly learning the alignment and translation of input to output tokens in an end-to-end fashion. However, until now this approach did not show improved error rates on its own compared to traditional joint-sequence based n-gram models for G2P. In this paper, we investigate how multitask learning can improve the performance of Seq2Seq G2P models. A single Seq2Seq model is trained on multiple phoneme lexicon datasets containing multiple languages and phonetic alphabets. Although multi-language learning does not show improved error rates, combining standard datasets and crawled data with different phonetic alphabets of the same language shows promising error reductions on English and German Seq2Seq G2P conversion. Finally, combining Seq2seq G2P models with standard n-grams based models yields significant improvements over using either model alone",
    "checked": true,
    "id": "26d009959fa2b2e18cddb5783493738a1c1ede2f",
    "semantic_title": "multitask sequence-to-sequence models for grapheme-to-phoneme conversion",
    "citation_count": 29,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhang17h_interspeech.html": {
    "title": "Acoustic Data-Driven Lexicon Learning Based on a Greedy Pronunciation Selection Framework",
    "volume": "main",
    "abstract": "Speech recognition systems for irregularly-spelled languages like English normally require hand-written pronunciations. In this paper, we describe a system for automatically obtaining pronunciations of words for which pronunciations are not available, but for which transcribed data exists. Our method integrates information from the letter sequence and from the acoustic evidence. The novel aspect of the problem that we address is the problem of how to prune entries from such a lexicon (since, empirically, lexicons with too many entries do not tend to be good for ASR performance). Experiments on various ASR tasks show that, with the proposed framework, starting with an initial lexicon of several thousand words, we are able to learn a lexicon which performs close to a full expert lexicon in terms of WER performance on test data, and is better than lexicons built using G2P alone or with a pruning criterion based on pronunciation probability",
    "checked": true,
    "id": "0884c2ffd34af1c6d0b9cc5aa08f9efac604fd60",
    "semantic_title": "acoustic data-driven lexicon learning based on a greedy pronunciation selection framework",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/shinozaki17_interspeech.html": {
    "title": "Semi-Supervised Learning of a Pronunciation Dictionary from Disjoint Phonemic Transcripts and Text",
    "volume": "main",
    "abstract": "While the performance of automatic speech recognition systems has recently approached human levels in some tasks, the application is still limited to specific domains. This is because system development relies on extensive supervised training and expert tuning in the target domain. To solve this problem, systems must become more self-sufficient, having the ability to learn directly from speech and adapt to new tasks. One open question in this area is how to learn a pronunciation dictionary containing the appropriate vocabulary. Humans can recognize words, even ones they have never heard before, by reading text and understanding the context in which a word is used. However, this ability is missing in current speech recognition systems. In this work, we propose a new framework that automatically expands an initial pronunciation dictionary using independently sampled acoustic and textual data. While the task is very challenging and in its initial stage, we demonstrate that a model based on Bayesian learning of Dirichlet processes can acquire word pronunciations from phone transcripts and text of the WSJ data set",
    "checked": true,
    "id": "7f817600b612aab6039dfba576ae8e8e7460d8f1",
    "semantic_title": "semi-supervised learning of a pronunciation dictionary from disjoint phonemic transcripts and text",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/smit17_interspeech.html": {
    "title": "Improved Subword Modeling for WFST-Based Speech Recognition",
    "volume": "main",
    "abstract": "Because in agglutinative languages the number of observed word forms is very high, subword units are often utilized in speech recognition. However, the proper use of subword units requires careful consideration of details such as silence modeling, position-dependent phones, and combination of the units. In this paper, we implement subword modeling in the Kaldi toolkit by creating modified lexicon by finite-state transducers to represent the subword units correctly. We experiment with multiple types of word boundary markers and achieve the best results by adding a marker to the left or right side of a subword unit whenever it is not preceded or followed by a word boundary, respectively. We also compare three different toolkits that provide data-driven subword segmentations. In our experiments on a variety of Finnish and Estonian datasets, the best subword models do outperform word-based models and naive subword implementations. The largest relative reduction in WER is a 23% over word-based models for a Finnish read speech dataset. The results are also better than any previously published ones for the same datasets, and the improvement on all datasets is more than 5%",
    "checked": true,
    "id": "dec569d5b8fb3888c38e81f938207cbc2fc20847",
    "semantic_title": "improved subword modeling for wfst-based speech recognition",
    "citation_count": 49,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bruguier17_interspeech.html": {
    "title": "Pronunciation Learning with RNN-Transducers",
    "volume": "main",
    "abstract": "Most speech recognition systems rely on pronunciation dictionaries to provide accurate transcriptions. Typically, some pronunciations are carved manually, but many are produced using pronunciation learning algorithms. Successful algorithms must have the ability to generate rich pronunciation variants, e.g. to accommodate words of foreign origin, while being robust to artifacts of the training data, e.g. noise in the acoustic segments from which the pronunciations are learned if the method uses acoustic signals. We propose a general finite-state transducer (FST) framework to describe such algorithms. This representation is flexible enough to accommodate a wide variety of pronunciation learning algorithms, including approaches that rely on the availability of acoustic data, and methods that only rely on the spelling of the target words. In particular, we show that the pronunciation FST can be built from a recurrent neural network (RNN) and tuned to provide rich yet constrained pronunciations. This new approach reduces the number of incorrect pronunciations learned from Google Voice traffic by up to 25% relative",
    "checked": true,
    "id": "f7dfc8981901bd008133373f26ed33e3c1065839",
    "semantic_title": "pronunciation learning with rnn-transducers",
    "citation_count": 11,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/naaman17_interspeech.html": {
    "title": "Learning Similarity Functions for Pronunciation Variations",
    "volume": "main",
    "abstract": "A significant source of errors in Automatic Speech Recognition (ASR) systems is due to pronunciation variations which occur in spontaneous and conversational speech. Usually ASR systems use a finite lexicon that provides one or more pronunciations for each word. In this paper, we focus on learning a similarity function between two pronunciations. The pronunciations can be the canonical and the surface pronunciations of the same word or they can be two surface pronunciations of different words. This task generalizes problems such as lexical access (the problem of learning the mapping between words and their possible pronunciations), and defining word neighborhoods. It can also be used to dynamically increase the size of the pronunciation lexicon, or in predicting ASR errors. We propose two methods, which are based on recurrent neural networks, to learn the similarity function. The first is based on binary classification, and the second is based on learning the ranking of the pronunciations. We demonstrate the efficiency of our approach on the task of lexical access using a subset of the Switchboard conversational speech corpus. Results suggest that on this task our methods are superior to previous methods which are based on graphical Bayesian methods",
    "checked": true,
    "id": "4145ff2db3a3b8c8e46a8f078604ee4d59c172f0",
    "semantic_title": "learning similarity functions for pronunciation variations",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gelly17_interspeech.html": {
    "title": "Spoken Language Identification Using LSTM-Based Angular Proximity",
    "volume": "main",
    "abstract": "This paper describes the design of an acoustic language identification (LID) system based on LSTMs that directly maps a sequence of acoustic features to a vector in a vector space where the angular proximity corresponds to a measure of language/dialect similarity. A specific architecture for the LSTM-based language vector extractor is introduced along with the angular proximity loss function to train it. This new LSTM-based LID system is quicker to train than a standard RNN topology using stacked layers trained with the cross-entropy loss function and obtains significantly lower language error rates. Experiments compare this approach to our previous developments on the subject, as well as to two widely used LID techniques: a phonotactic system using DNN acoustic models and an i-vector system. Results are reported on two different data sets: the 14 languages of NIST LRE07 and the 20 closely related languages and dialects of NIST LRE15. In addition to reporting the NIST Cavg metric which served as the primary metric for the LRE07 and LRE15 evaluations, the average LER is provided",
    "checked": true,
    "id": "ab9f94965f2f41469019b8b70bb07be13d17bf01",
    "semantic_title": "spoken language identification using lstm-based angular proximity",
    "citation_count": 45,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jin17_interspeech.html": {
    "title": "End-to-End Language Identification Using High-Order Utterance Representation with Bilinear Pooling",
    "volume": "main",
    "abstract": "A key problem in spoken language identification (LID) is how to design effective representations which are specific to language information. Recent advances in deep neural networks have led to significant improvements in results, with deep end-to-end methods proving effective. This paper proposes a novel network which aims to model an effective representation for high (first and second)-order statistics of LID-senones, defined as being LID analogues of senones in speech recognition. The high-order information extracted through bilinear pooling is robust to speakers, channels and background noise. Evaluation with NIST LRE 2009 shows improved performance compared to current state-of-the-art DBF/i-vector systems, achieving over 33% and 20% relative equal error rate (EER) improvement for 3s and 10s utterances and over 40% relative C improvement for all durations",
    "checked": true,
    "id": "46347bbe3a534a3d6bd1698d3251b595f7e6e7eb",
    "semantic_title": "end-to-end language identification using high-order utterance representation with bilinear pooling",
    "citation_count": 19,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhang17i_interspeech.html": {
    "title": "Dialect Recognition Based on Unsupervised Bottleneck Features",
    "volume": "main",
    "abstract": "Recently, bottleneck features (BNF) with an i-Vector strategy has been used for state-of-the-art language/dialect identification. However, traditional bottleneck extraction requires an additional transcribed corpus which is used for acoustic modeling. Alternatively, an unsupervised BNF extraction diagram is proposed in our study, which is derived from the traditional structure but trained with an estimated phonetic label. The proposed method is evaluated on a 4-way Chinese dialect dataset and a 5-way closely spaced Pan-Arabic corpus. Compared to a baseline i-Vector system based on acoustic features MFCCs, the proposed unsupervised BNF consistently achieves better performance across two corpora. Specifically, the EER and overall performance C_avg * 100 are improved by a relative +48% and +52%, respectively. Even under the condition with limited training data, the proposed feature still achieves up to 24% relative improvement compared to baseline, all without the need of a secondary transcribed corpus",
    "checked": true,
    "id": "7ecc9c552e33ac227437caae56dce0c8fa53baf6",
    "semantic_title": "dialect recognition based on unsupervised bottleneck features",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/irtza17_interspeech.html": {
    "title": "Investigating Scalability in Hierarchical Language Identification System",
    "volume": "main",
    "abstract": "State-of-the-art language identification (LID) systems are not easily scalable to accommodate new languages. Specifically, as the number of target languages grows the error rate of these LID systems increases rapidly. This paper addresses such a challenge by adopting a hierarchical language identification (HLID) framework. We demonstrate the superior scalability of the HLID framework. In particular, HLID only requires the training of relevant nodes in a hierarchical structure instead of re-training the entire tree. Experiments conducted on a dataset that combined languages from the NIST LRE 2007, 2009, 2011 and 2015 databases show that as the number of target languages grows from 28 to 42, the performance of a single level (non-hierarchical) system deteriorates by around 11% while that of the hierarchical system only deteriorates by about 3.4% in terms of C Finally, experiments also suggest that SVM based systems are more scalable than GPLDA based systems",
    "checked": true,
    "id": "90469a7cf79de6d820a8f7a476bb838c86dc3095",
    "semantic_title": "investigating scalability in hierarchical language identification system",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/qian17c_interspeech.html": {
    "title": "Improving Sub-Phone Modeling for Better Native Language Identification with Non-Native English Speech",
    "volume": "main",
    "abstract": "Identifying a speaker's native language with his speech in a second language is useful for many human-machine voice interface applications. In this paper, we use a sub-phone-based i-vector approach to identify non-native English speakers' native languages by their English speech input. Time delay deep neural networks (TDNN) are trained on LVCSR corpora for improving the alignment of speech utterances with their corresponding sub-phonemic \"senone\" sequences. The phonetic variability caused by a speaker's native language can be better modeled with the sub-phone models than the conventional phone model based approach. Experimental results on the database released for the 2016 Interspeech ComParE Native Language challenge with 11 different L1s show that our system outperforms the best system by a large margin (87.2% UAR compared to 81.3% UAR for the best system from the 2016 ComParE challenge)",
    "checked": true,
    "id": "d6f6f4695ed5fff107b5dbd65e4b8af0b22809c0",
    "semantic_title": "improving sub-phone modeling for better native language identification with non-native english speech",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/khurana17_interspeech.html": {
    "title": "QMDIS: QCRI-MIT Advanced Dialect Identification System",
    "volume": "main",
    "abstract": "As a continuation of our efforts towards tackling the problem of spoken Dialect Identification (DID) for Arabic languages, we present the QCRI-MIT Advanced Dialect Identification System (QMDIS). QMDIS is an automatic spoken DID system for Dialectal Arabic (DA). In this paper, we report a comprehensive study of the three main components used in the spoken DID task: phonotactic, lexical and acoustic. We use Support Vector Machines (SVMs), Logistic Regression (LR) and Convolutional Neural Networks (CNNs) as backend classifiers throughout the study. We perform all our experiments on a publicly available dataset and present new state-of-the-art results. QMDIS discriminates between the five most widely used dialects of Arabic: namely Egyptian, Gulf, Levantine, North African, and Modern Standard Arabic (MSA).We report ≈73% accuracy for system combination. All the data and the code used in our experiments are publicly available for research",
    "checked": true,
    "id": "34de260c6db1e50b7543d1bc8eb8330aedc37f19",
    "semantic_title": "qmdis: qcri-mit advanced dialect identification system",
    "citation_count": 16,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/alluri17b_interspeech.html": {
    "title": "Detection of Replay Attacks Using Single Frequency Filtering Cepstral Coefficients",
    "volume": "main",
    "abstract": "Automatic speaker verification systems are vulnerable to spoofing attacks. Recently, various countermeasures have been developed for detecting high technology attacks such as speech synthesis and voice conversion. However, there is a wide gap in dealing with replay attacks. In this paper, we propose a new feature for replay attack detection based on single frequency filtering (SFF), which provides high temporal and spectral resolution at each instant. Single frequency filtering cepstral coefficients (SFFCC) with Gaussian mixture model classifier are used for the experimentation on the standard BTAS-2016 corpus. The previously reported best result, which is based on constant Q cepstral coefficients (CQCC) achieved a half total error rate of 0.67% on this data-set. Our proposed method outperforms the state of the art (CQCC) with a half total error rate of 0.0002%",
    "checked": true,
    "id": "0b9f57b0585e875602c768ce42d87c30a667060a",
    "semantic_title": "detection of replay attacks using single frequency filtering cepstral coefficients",
    "citation_count": 21,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sailor17_interspeech.html": {
    "title": "Unsupervised Representation Learning Using Convolutional Restricted Boltzmann Machine for Spoof Speech Detection",
    "volume": "main",
    "abstract": "Speech Synthesis (SS) and Voice Conversion (VC) presents a genuine risk of attacks for Automatic Speaker Verification (ASV) technology. In this paper, we use our recently proposed unsupervised filterbank learning technique using Convolutional Restricted Boltzmann Machine (ConvRBM) as a front-end feature representation. ConvRBM is trained on training subset of ASV spoof 2015 challenge database. Analyzing the filterbank trained on this dataset shows that ConvRBM learned more low-frequency subband filters compared to training on natural speech database such as TIMIT. The spoofing detection experiments were performed using Gaussian Mixture Models (GMM) as a back-end classifier. ConvRBM-based cepstral coefficients (ConvRBM-CC) perform better than hand crafted Mel Frequency Cepstral Coefficients (MFCC). On the evaluation set, ConvRBM-CC features give an absolute reduction of 4.76% in Equal Error Rate (EER) compared to MFCC features. Specifically, ConvRBM-CC features significantly perform better in both known attacks (1.93%) and unknown attacks (5.87%) compared to MFCC features",
    "checked": true,
    "id": "9b41020880817be69a53818c2d9e665c174e06af",
    "semantic_title": "unsupervised representation learning using convolutional restricted boltzmann machine for spoof speech detection",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/suthokumar17_interspeech.html": {
    "title": "Independent Modelling of High and Low Energy Speech Frames for Spoofing Detection",
    "volume": "main",
    "abstract": "Spoofing detection systems for automatic speaker verification have moved from only modelling voiced frames to modelling all speech frames. Unvoiced speech has been shown to carry information about spoofing attacks and anti-spoofing systems may further benefit by treating voiced and unvoiced speech differently. In this paper, we separate speech into low and high energy frames and independently model the distributions of both to form two spoofing detection systems that are then fused at the score level. Experiments conducted on the ASVspoof 2015, BTAS 2016 and Spoofing and Anti-Spoofing (SAS) corpora demonstrate that the proposed approach of fusing two independent high and low energy spoofing detection systems consistently outperforms the standard approach that does not distinguish between high and low energy frames",
    "checked": true,
    "id": "4116a936e1e2c02c75dd469f337464124c34bf1b",
    "semantic_title": "independent modelling of high and low energy speech frames for spoofing detection",
    "citation_count": 14,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sarkar17_interspeech.html": {
    "title": "Improving Speaker Verification Performance in Presence of Spoofing Attacks Using Out-of-Domain Spoofed Data",
    "volume": "main",
    "abstract": "Automatic speaker verification (ASV) systems are vulnerable to spoofing attacks using speech generated by voice conversion and speech synthesis techniques. Commonly, a countermeasure (CM) system is integrated with an ASV system for improved protection against spoofing attacks. But integration of the two systems is challenging and often leads to increased false rejection rates. Furthermore, the performance of CM severely degrades if in-domain development data are unavailable. In this study, therefore, we propose a solution that uses two separate background models — one from human speech and another from spoofed data. During test, the ASV score for an input utterance is computed as the difference of the log-likelihood against the target model and the combination of the log-likelihoods against two background models. Evaluation experiments are conducted using the joint ASV and CM protocol of ASVspoof 2015 corpus consisting of text-independent ASV tasks with short utterances. Our proposed system reduces error rates in the presence of spoofing attacks by using out-of-domain spoofed data for system development, while maintaining the performance for zero-effort imposter attacks compared to the baseline system",
    "checked": true,
    "id": "c324b3a178898f74252a3b1cf216d7423934993b",
    "semantic_title": "improving speaker verification performance in presence of spoofing attacks using out-of-domain spoofed data",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/nagrani17_interspeech.html": {
    "title": "VoxCeleb: A Large-Scale Speaker Identification Dataset",
    "volume": "main",
    "abstract": "Most existing datasets for speaker identification contain samples obtained under quite constrained conditions, and are usually hand-annotated, hence limited in size. The goal of this paper is to generate a large scale text-independent speaker identification dataset collected ‘in the wild' We make two contributions. First, we propose a fully automated pipeline based on computer vision techniques to create the dataset from open-source media. Our pipeline involves obtaining videos from YouTube; performing active speaker verification using a two-stream synchronization Convolutional Neural Network (CNN), and confirming the identity of the speaker using CNN based facial recognition. We use this pipeline to curate VoxCeleb which contains hundreds of thousands of ‘real world' utterances for over 1,000 celebrities Our second contribution is to apply and compare various state of the art speaker identification techniques on our dataset to establish baseline performance. We show that a CNN based architecture obtains the best performance for both identification and verification",
    "checked": true,
    "id": "8a26431833b0ea8659ef1d24bff3ac9e56dcfcd0",
    "semantic_title": "voxceleb: a large-scale speaker identification dataset",
    "citation_count": 1806,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jones17b_interspeech.html": {
    "title": "Call My Net Corpus: A Multilingual Corpus for Evaluation of Speaker Recognition Technology",
    "volume": "main",
    "abstract": "The Call My Net 2015 (CMN15) corpus presents a new resource for Speaker Recognition Evaluation and related technologies. The corpus includes conversational telephone speech recordings for a total of 220 speakers spanning 4 languages: Tagalog, Cantonese, Mandarin and Cebuano. The corpus includes 10 calls per speaker made under a variety of noise conditions. Calls were manually audited for language, speaker identity and overall quality. The resulting data has been used in the NIST 2016 SRE Evaluation and will be published in the Linguistic Data Consortium catalog. We describe the goals of the CMN15 corpus, including details of the collection protocol and auditing procedure and discussion of the unique properties of this corpus compared to prior NIST SRE evaluation corpora",
    "checked": true,
    "id": "211bce170485e8fd7faee89ebf40b8eb2eacb190",
    "semantic_title": "call my net corpus: a multilingual corpus for evaluation of speaker recognition technology",
    "citation_count": 17,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/weiss17_interspeech.html": {
    "title": "Sequence-to-Sequence Models Can Directly Translate Foreign Speech",
    "volume": "main",
    "abstract": "We present a recurrent encoder-decoder deep neural network architecture that directly translates speech in one language into text in another. The model does not explicitly transcribe the speech into text in the source language, nor does it require supervision from the ground truth source language transcription during training. We apply a slightly modified sequence-to-sequence with attention architecture that has previously been used for speech recognition and show that it can be repurposed for this more complex task, illustrating the power of attention-based models A single model trained end-to-end obtains state-of-the-art performance on the Fisher Callhome Spanish-English speech translation task, outperforming a cascade of independently trained sequence-to-sequence speech recognition and machine translation models by 1.8 BLEU points on the Fisher test set. In addition, we find that making use of the training data in both languages by multi-task training sequence-to-sequence speech translation and recognition models with a shared encoder network can improve performance by a further 1.4 BLEU points",
    "checked": true,
    "id": "dda047fd87610911c82778243f72f60d1c063383",
    "semantic_title": "sequence-to-sequence models can directly translate foreign speech",
    "citation_count": 302,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kano17_interspeech.html": {
    "title": "Structured-Based Curriculum Learning for End-to-End English-Japanese Speech Translation",
    "volume": "main",
    "abstract": "Sequence-to-sequence attentional-based neural network architectures have been shown to provide a powerful model for machine translation and speech recognition. Recently, several works have attempted to extend the models for end-to-end speech translation task. However, the usefulness of these models were only investigated on language pairs with similar syntax and word order (e.g., English-French or English-Spanish). In this work, we focus on end-to-end speech translation tasks on syntactically distant language pairs (e.g., English-Japanese) that require distant word reordering. To guide the encoder-decoder attentional model to learn this difficult problem, we propose a structured-based curriculum learning strategy. Unlike conventional curriculum learning that gradually emphasizes difficult data examples, we formalize learning strategies from easier network structures to more difficult network structures. Here, we start the training with end-to-end encoder-decoder for speech recognition or text-based machine translation task then gradually move to end-to-end speech translation task. The experiment results show that the proposed approach could provide significant improvements in comparison with the one without curriculum learning",
    "checked": true,
    "id": "c1a75d69b0b2a4eaf90a8724f1fc7d37468b8108",
    "semantic_title": "structured-based curriculum learning for end-to-end english-japanese speech translation",
    "citation_count": 42,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ruiz17_interspeech.html": {
    "title": "Assessing the Tolerance of Neural Machine Translation Systems Against Speech Recognition Errors",
    "volume": "main",
    "abstract": "Machine translation systems are conventionally trained on textual resources that do not model phenomena that occur in spoken language. While the evaluation of neural machine translation systems on textual inputs is actively researched in the literature, little has been discovered about the complexities of translating spoken language data with neural models. We introduce and motivate interesting problems one faces when considering the translation of automatic speech recognition (ASR) outputs on neural machine translation (NMT) systems. We test the robustness of sentence encoding approaches for NMT encoder-decoder modeling, focusing on word-based over byte-pair encoding. We compare the translation of utterances containing ASR errors in state-of-the-art NMT encoder-decoder systems against a strong phrase-based machine translation baseline in order to better understand which phenomena present in ASR outputs are better represented under the NMT framework than approaches that represent translation as a linear model",
    "checked": true,
    "id": "185b4f405a3d5aae927bacc5533f99e99ecd1c83",
    "semantic_title": "assessing the tolerance of neural machine translation systems against speech recognition errors",
    "citation_count": 21,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/do17b_interspeech.html": {
    "title": "Toward Expressive Speech Translation: A Unified Sequence-to-Sequence LSTMs Approach for Translating Words and Emphasis",
    "volume": "main",
    "abstract": "Emphasis is an important piece of paralinguistic information that is used to express different intentions, attitudes, or convey emotion. Recent works have tried to translate emphasis by developing additional emphasis estimation and translation components apart from an existing speech-to-speech translation (S2ST) system. Although these approaches can preserve emphasis, they introduce more complexity to the translation pipeline. The emphasis translation component has to wait for the target language sentence and word alignments derived from a machine translation system, resulting in a significant translation delay. In this paper, we proposed an approach that jointly trains and predicts words and emphasis in a unified architecture based on sequence-to-sequence models. The proposed model not only speeds up the translation pipeline but also allows us to perform joint training. Our experiments on the emphasis and word translation tasks showed that we could achieve comparable performance for both tasks compared with previous approaches while eliminating complex dependencies",
    "checked": true,
    "id": "dab7bb72f29b2b7921b44d4836713f89931b0f22",
    "semantic_title": "toward expressive speech translation: a unified sequence-to-sequence lstms approach for translating words and emphasis",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/cho17_interspeech.html": {
    "title": "NMT-Based Segmentation and Punctuation Insertion for Real-Time Spoken Language Translation",
    "volume": "main",
    "abstract": "Insertion of proper segmentation and punctuation into an ASR transcript is crucial not only for the performance of subsequent applications but also for the readability of the text. In a simultaneous spoken language translation system, the segmentation model has to fulfill real-time constraints and minimize latency as well In this paper, we show the successful integration of an attentional encoder-decoder-based segmentation and punctuation insertion model into a real-time spoken language translation system. The proposed technique can be easily integrated into the real-time framework and improve the punctuation performance on reference transcripts as well as on ASR outputs. Compared to the conventional language model and prosody-based model, our experiments on end-to-end spoken language translation show that translation performance is improved by 1.3 BLEU points by adopting the NMT-based punctuation model, maintaining low-latency",
    "checked": true,
    "id": "9e8dc00ee9d098d94a970beb19b92aa2f996e343",
    "semantic_title": "nmt-based segmentation and punctuation insertion for real-time spoken language translation",
    "citation_count": 56,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/drude17_interspeech.html": {
    "title": "Tight Integration of Spatial and Spectral Features for BSS with Deep Clustering Embeddings",
    "volume": "main",
    "abstract": "Recent advances in discriminatively trained mask estimation networks to extract a single source utilizing beamforming techniques demonstrate, that the integration of statistical models and deep neural networks (DNNs) are a promising approach for robust automatic speech recognition (ASR) applications. In this contribution we demonstrate how discriminatively trained embeddings on spectral features can be tightly integrated into statistical model-based source separation to separate and transcribe overlapping speech. Good generalization to unseen spatial configurations is achieved by estimating a statistical model at test time, while still leveraging discriminative training of deep clustering embeddings on a separate training set. We formulate an expectation maximization (EM) algorithm which jointly estimates a model for deep clustering embeddings and complex-valued spatial observations in the short time Fourier transform (STFT) domain at test time. Extensive simulations confirm, that the integrated model outperforms (a) a deep clustering model with a subsequent beamforming step and (b) an EM-based model with a beamforming step alone in terms of signal to distortion ratio (SDR) and perceptually motivated metric (PESQ) gains. ASR results on a reverberated dataset further show, that the aforementioned gains translate to reduced word error rates (WERs) even in reverberant environments",
    "checked": true,
    "id": "349dfa6de1aaa89dba7813f21ac43a913faa1ab1",
    "semantic_title": "tight integration of spatial and spectral features for bss with deep clustering embeddings",
    "citation_count": 43,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zmolikova17_interspeech.html": {
    "title": "Speaker-Aware Neural Network Based Beamformer for Speaker Extraction in Speech Mixtures",
    "volume": "main",
    "abstract": "In this work, we address the problem of extracting one target speaker from a multichannel mixture of speech. We use a neural network to estimate masks to extract the target speaker and derive beamformer filters using these masks, in a similar way as the recently proposed approach for extraction of speech in presence of noise. To overcome the permutation ambiguity of neural network mask estimation, which arises in presence of multiple speakers, we propose to inform the neural network about the target speaker so that it learns to follow the speaker characteristics through the utterance. We investigate and compare different methods of passing the speaker information to the network such as making one layer of the network dependent on speaker characteristics. Experiments on mixture of two speakers demonstrate that the proposed scheme can track and extract a target speaker for both closed and open speaker set cases",
    "checked": true,
    "id": "e6ef66fe090afff8dac09f8f318589eb3e75a56f",
    "semantic_title": "speaker-aware neural network based beamformer for speaker extraction in speech mixtures",
    "citation_count": 94,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/pfeifenberger17_interspeech.html": {
    "title": "Eigenvector-Based Speech Mask Estimation Using Logistic Regression",
    "volume": "main",
    "abstract": "In this paper, we use a logistic regression to learn a speech mask from the dominant eigenvector of the Power Spectral Density (PSD) matrix of a multi-channel speech signal corrupted by ambient noise. We employ this speech mask to construct the Generalized Eigenvalue (GEV) beamformer and a Wiener postfilter. Further, we extend the beamformer to compensate for speech distortions. We do not make any assumptions about the array geometry or the characteristics of the speech and noise sources. Those parameters are learned from training data. Our assumptions are that the speaker may move slowly in the near-field of the array, and that the noise is in the far-field. We compare our speech enhancement system against recent contributions using the CHiME4 corpus. We show that our approach yields superior results, both in terms of perceptual speech quality and speech mask estimation error",
    "checked": true,
    "id": "6b57f9c6883eb077ccf0515d29e40a1c18656e03",
    "semantic_title": "eigenvector-based speech mask estimation using logistic regression",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wood17b_interspeech.html": {
    "title": "Real-Time Speech Enhancement with GCC-NMF",
    "volume": "main",
    "abstract": "We develop an online variant of the GCC-NMF blind speech enhancement algorithm and study its performance on two-channel mixtures of speech and real-world noise from the SiSEC separation challenge. While GCC-NMF performs enhancement independently for each time frame, the NMF dictionary, its activation coefficients, and the target TDOA are derived using the entire mixture signal, thus precluding its use online. Pre-learning the NMF dictionary using the CHiME dataset and inferring its activation coefficients online yields similar overall PEASS scores to the mixture-learned method, thus generalizing to new speakers, acoustic environments, and noise conditions. Surprisingly, if we forgo coefficient inference altogether, this approach outperforms both the mixture-learned method and most algorithms from the SiSEC challenge to date. Furthermore, the trade-off between interference suppression and target fidelity may be controlled online by adjusting the target TDOA window width. Finally, integrating online target localization with max-pooled GCC-PHAT yields only somewhat decreased performance compared to offline localization. We test a real-time implementation of the online GCC-NMF blind speech enhancement system on a variety of hardware platforms, with performance made to degrade smoothly with decreasing computational power using smaller pre-learned dictionaries",
    "checked": true,
    "id": "decb022336029dc27332ae9d993b3cbed314ea14",
    "semantic_title": "real-time speech enhancement with gcc-nmf",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ji17b_interspeech.html": {
    "title": "Coherence-Based Dual-Channel Noise Reduction Algorithm in a Complex Noisy Environment",
    "volume": "main",
    "abstract": "In this paper, a coherence-based noise reduction algorithm is proposed for a dual-channel speech enhancement system operating in a complex noise environment. The spatial coherence between two omnidirectional microphones is one of the crucial information for the dual-channel speech enhancement system. In this paper, we introduce a new model of coherence function for the complex noise environment in which a target speech coexists with a coherent interference and diffuse noise around. From the coherence model, three numerical methods of computing the normalized signal to interference plus diffuse noise ratio (SINR), which is related to the Wiener filter gain, are derived. Objective parameters measured from the enhanced speech demonstrate superior performance of the proposed algorithm in terms of speech quality and intelligibility, over the conventional coherence-based noise reduction algorithm",
    "checked": true,
    "id": "cc848ce20945752b68e32eb7781d2f9e149d280d",
    "semantic_title": "coherence-based dual-channel noise reduction algorithm in a complex noisy environment",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhang17j_interspeech.html": {
    "title": "Glottal Model Based Speech Beamforming for ad-hoc Microphone Arrays",
    "volume": "main",
    "abstract": "We are interested in the task of speech beamforming in conference room meetings, with microphones built in the electronic devices brought and casually placed by meeting participants. This task is challenging because of the inaccuracy in position and interference calibration due to random microphone configuration, variance of microphone quality, reverberation etc. As a result, not many beamforming algorithms perform better than simply picking the closest microphone in this setting. We propose a beamforming called Glottal Residual Assisted Beamforming (GRAB). It does not rely on any position or interference calibration. Instead, it incorporates a source-filter speech model and minimizes the energy that cannot be accounted for by the model. Objective and subjective evaluations on both simulation and real-world data show that GRAB is able to suppress noise effectively while keeping the speech natural and dry. Further analyses reveal that GRAB can distinguish contaminated or reverberant channels and take appropriate action accordingly",
    "checked": true,
    "id": "d187e2c205146476aef1d223a95978b3394d3c6d",
    "semantic_title": "glottal model based speech beamforming for ad-hoc microphone arrays",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/liu17d_interspeech.html": {
    "title": "Acoustic Assessment of Disordered Voice with Continuous Speech Based on Utterance-Level ASR Posterior Features",
    "volume": "main",
    "abstract": "Most previous studies on acoustic assessment of disordered voice were focused on extracting perturbation features from isolated vowels produced with steady-state phonation. Natural speech, however, is considered to be more preferable in the aspects of flexibility, effectiveness and reliability for clinical practice. This paper presents an investigation on applying automatic speech recognition (ASR) technology to disordered voice assessment of Cantonese speakers. A DNN-based ASR system is trained using phonetically-rich continuous utterances from normal speakers. It was found that frame-level phone posteriors obtained from the ASR system are strongly correlated with the severity level of voice disorder. Phone posteriors in utterances with severe disorder exhibit significantly larger variation than those with mild disorder. A set of utterance-level posterior features are computed to quantify such variation for pattern recognition purpose. An SVM based classifier is used to classify an input utterance into the categories of mild, moderate and severe disorder. The two-class classification accuracy for mild and severe disorders is 90.3%, and significant confusion between mild and moderate disorders is observed. For some of the subjects with severe voice disorder, the classification results are highly inconsistent among individual utterances. Furthermore, short utterances tend to have more classification errors",
    "checked": true,
    "id": "450eb866817a9f72d39e67da1c16558414eb50e8",
    "semantic_title": "acoustic assessment of disordered voice with continuous speech based on utterance-level asr posterior features",
    "citation_count": 11,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ylmaz17c_interspeech.html": {
    "title": "Multi-Stage DNN Training for Automatic Recognition of Dysarthric Speech",
    "volume": "main",
    "abstract": "Incorporating automatic speech recognition (ASR) in individualized speech training applications is becoming more viable thanks to the improved generalization capabilities of neural network-based acoustic models. The main problem in developing applications for dysarthric speech is the relative in-domain data scarcity. Collecting representative amounts of dysarthric speech data is difficult due to rigorous ethical and medical permission requirements, problems in accessing patients who are generally vulnerable and often subject to altering health conditions and, last but not least, the high variability in speech resulting from different pathological conditions. Developing such applications is even more challenging for languages which in general have fewer resources, fewer speakers and, consequently, also fewer patients than English, as in the case of a mid-sized language like Dutch. In this paper, we investigate a multi-stage deep neural network (DNN) training scheme aimed at obtaining better modeling of dysarthric speech by using only a small amount of in-domain training data. The results show that the system employing the proposed training scheme considerably improves the recognition of Dutch dysarthric speech compared to a baseline system with single-stage training only on a large amount of normal speech or a small amount of in-domain data",
    "checked": true,
    "id": "87ccc7be710de75b23f0174294a95ec6af433e4d",
    "semantic_title": "multi-stage dnn training for automatic recognition of dysarthric speech",
    "citation_count": 17,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/smith17_interspeech.html": {
    "title": "Improving Child Speech Disorder Assessment by Incorporating Out-of-Domain Adult Speech",
    "volume": "main",
    "abstract": "This paper describes the continued development of a system to provide early assessment of speech development issues in children and better triaging to professional services. Whilst corpora of children's speech are increasingly available, recognition of disordered children's speech is still a data-scarce task. Transfer learning methods have been shown to be effective at leveraging out-of-domain data to improve ASR performance in similar data-scarce applications. This paper combines transfer learning, with previously developed methods for constrained decoding based on expert speech pathology knowledge and knowledge of the target text. Results of this study show that transfer learning with out-of-domain adult speech can improve phoneme recognition for disordered children's speech. Specifically, a Deep Neural Network (DNN) trained on adult speech and fine-tuned on a corpus of disordered children's speech reduced the phoneme error rate (PER) of a DNN trained on a children's corpus from 16.3% to 14.2%. Furthermore, this fine-tuned DNN also improved the performance of a Hierarchal Neural Network based acoustic model previously used by the system with a PER of 19.3%. We close with a discussion of our planned future developments of the system",
    "checked": true,
    "id": "b8bc9b5e2b3baead136af5f4c51c5919c10702cc",
    "semantic_title": "improving child speech disorder assessment by incorporating out-of-domain adult speech",
    "citation_count": 17,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/joy17b_interspeech.html": {
    "title": "On Improving Acoustic Models for TORGO Dysarthric Speech Database",
    "volume": "main",
    "abstract": "Assistive technologies based on speech have been shown to improve the quality of life of people affected with dysarthria, a motor speech disorder. Multiple ways to improve Gaussian mixture model-hidden Markov model (GMM-HMM) and deep neural network (DNN) based automatic speech recognition (ASR) systems for TORGO database for dysarthric speech are explored in this paper. Past attempts in developing ASR systems for TORGO database were limited to training just monophone models and doing speaker adaptation over them. Although a recent work attempted training triphone and neural network models, parameters like the number of context dependent states, dimensionality of the principal component features etc were not properly tuned. This paper develops speaker-specific ASR models for each dysarthric speaker in TORGO database by tuning parameters of GMM-HMM model, number of layers and hidden nodes in DNN. Employing dropout scheme and sequence discriminative training in DNN also gave significant gains. Speaker adapted features like feature-space maximum likelihood linear regression (FMLLR) are used to pass the speaker information to DNNs. To the best of our knowledge, this paper presents the best recognition accuracies for TORGO database till date",
    "checked": true,
    "id": "16155ac9c52a11f732a020adaad457c36655969c",
    "semantic_title": "improving acoustic models in torgo dysarthric speech database",
    "citation_count": 49,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/simantiraki17_interspeech.html": {
    "title": "Glottal Source Features for Automatic Speech-Based Depression Assessment",
    "volume": "main",
    "abstract": "Depression is one of the most prominent mental disorders, with an increasing rate that makes it the fourth cause of disability worldwide. The field of automated depression assessment has emerged to aid clinicians in the form of a decision support system. Such a system could assist as a pre-screening tool, or even for monitoring high risk populations. Related work most commonly involves multimodal approaches, typically combining audio and visual signals to identify depression presence and/or severity. The current study explores categorical assessment of depression using audio features alone. Specifically, since depression-related vocal characteristics impact the glottal source signal, we examine Phase Distortion Deviation which has previously been applied to the recognition of voice qualities such as hoarseness, breathiness and creakiness, some of which are thought to be features of depressed speech. The proposed method uses as features DCT-coefficients of the Phase Distortion Deviation for each frequency band. An automated machine learning tool, Just Add Data, is used to classify speech samples. The method is evaluated on a benchmark dataset (AVEC2014), in two conditions: read-speech and spontaneous-speech. Our findings indicate that Phase Distortion Deviation is a promising audio-only feature for automated detection and assessment of depressed speech",
    "checked": true,
    "id": "2890074c36485fa6c6d120f8c9a421bb868aaf20",
    "semantic_title": "glottal source features for automatic speech-based depression assessment",
    "citation_count": 19,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sadeghian17_interspeech.html": {
    "title": "Speech Processing Approach for Diagnosing Dementia in an Early Stage",
    "volume": "main",
    "abstract": "The clinical diagnosis of Alzheimer's disease and other dementias is very challenging, especially in the early stages. Our hypothesis is that any disease that affects particular brain regions involved in speech production and processing will also leave detectable finger prints in the speech. Computerized analysis of speech signals and computational linguistics have progressed to the point where an automatic speech analysis system is a promising approach for a low-cost non-invasive diagnostic tool for early detection of Alzheimer's disease We present empirical evidence that strong discrimination between subjects with a diagnosis of probable Alzheimer's versus matched normal controls can be achieved with a combination of acoustic features from speech, linguistic features extracted from an automatically determined transcription of the speech including punctuation, and results of a mini mental state exam (MMSE). We also show that discrimination is nearly as strong even if the MMSE is not used, which implies that a fully automated system is feasible. Since commercial automatic speech recognition (ASR) tools were unable to provide transcripts for about half of our speech samples, a customized ASR system was developed",
    "checked": true,
    "id": "a562433479ac31ba65a068d8847721bbfac99e42",
    "semantic_title": "speech processing approach for diagnosing dementia in an early stage",
    "citation_count": 28,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/biadsy17_interspeech.html": {
    "title": "Effectively Building Tera Scale MaxEnt Language Models Incorporating Non-Linguistic Signals",
    "volume": "main",
    "abstract": "Maximum Entropy (MaxEnt) language models are powerful models that can incorporate linguistic and non-linguistic contextual signals in a unified framework with a convex loss. MaxEnt models also have the advantage of scaling to large model and training data sizes We present the following two contributions to MaxEnt training: (1) By leveraging smaller amounts of transcribed data, we demonstrate that a MaxEnt LM trained on various types of corpora can be easily adapted to better match the test distribution of Automatic Speech Recognition (ASR); (2) A novel adaptive-training approach that efficiently models multiple types of non-linguistic features in a universal model. We evaluate the impact of these approaches on Google's state-of-the-art ASR for the task of voice-search transcription and dictation. Training 10B parameter models utilizing a corpus of up to 1T words, we show large reductions in word error rate from adaptation across multiple languages. Also, human evaluations show significant improvements on a wide range of domains from using non-linguistic features. For example, adapting to geographical domains (e.g., US States and cities) affects about 4% of test utterances, with 2:1 win to loss ratio",
    "checked": true,
    "id": "2bebb7ec0c5d49e61c9b8aba19f04f22ca6fde0c",
    "semantic_title": "effectively building tera scale maxent language models incorporating non-linguistic signals",
    "citation_count": 31,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/deena17_interspeech.html": {
    "title": "Semi-Supervised Adaptation of RNNLMs by Fine-Tuning with Domain-Specific Auxiliary Features",
    "volume": "main",
    "abstract": "Recurrent neural network language models (RNNLMs) can be augmented with auxiliary features, which can provide an extra modality on top of the words. It has been found that RNNLMs perform best when trained on a large corpus of generic text and then fine-tuned on text corresponding to the sub-domain for which it is to be applied. However, in many cases the auxiliary features are available for the sub-domain text but not for the generic text. In such cases, semi-supervised techniques can be used to infer such features for the generic text data such that the RNNLM can be trained and then fine-tuned on the available in-domain data with corresponding auxiliary features In this paper, several novel approaches are investigated for dealing with the semi-supervised adaptation of RNNLMs with auxiliary features as input. These approaches include: using zero features during training to mask the weights of the feature sub-network; adding the feature sub-network only at the time of fine-tuning; deriving the features using a parametric model and; back-propagating to infer the features on the generic text. These approaches are investigated and results are reported both in terms of PPL and WER on a multi-genre broadcast ASR task",
    "checked": true,
    "id": "3e8561c5f6a92008d972f739a5018618ee297a69",
    "semantic_title": "semi-supervised adaptation of rnnlms by fine-tuning with domain-specific auxiliary features",
    "citation_count": 14,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/singh17_interspeech.html": {
    "title": "Approximated and Domain-Adapted LSTM Language Models for First-Pass Decoding in Speech Recognition",
    "volume": "main",
    "abstract": "Traditionally, short-range Language Models (LMs) like the conventional n-gram models have been used for language model adaptation. Recent work has improved performance for such tasks using adapted long-span models like Recurrent Neural Network LMs (RNNLMs). With the first pass performed using a large background n-gram LM, the adapted RNNLMs are mostly used to rescore lattices or N-best lists, as a second step in the decoding process. Ideally, these adapted RNNLMs should be applied for first-pass decoding. Thus, we introduce two ways of applying adapted long-short-term-memory (LSTM) based RNNLMs for first-pass decoding. Using available techniques to convert LSTMs to approximated versions for first-pass decoding, we compare approximated LSTMs adapted in a Fast Marginal Adaptation framework (FMA) and an approximated version of architecture-based-adaptation of LSTM. On a conversational speech recognition task, these differently approximated and adapted LSTMs combined with a trigram LM outperform other adapted and unadapted LMs. Here, the architecture-adapted LSTM combination obtains a 35.9% word error rate (WER) and is outperformed by FMA-based LSTM combination obtaining the overall lowest WER of 34.4%",
    "checked": true,
    "id": "5ebc1d03c5834f9312bd663ad8c7c63591b9a992",
    "semantic_title": "approximated and domain-adapted lstm language models for first-pass decoding in speech recognition",
    "citation_count": 22,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chelba17_interspeech.html": {
    "title": "Sparse Non-Negative Matrix Language Modeling: Maximum Entropy Flexibility on the Cheap",
    "volume": "main",
    "abstract": "We present a new method for estimating the sparse non-negative model (SNM) by using a small amount of held-out data and the multinomial loss that is natural for language modeling; we validate it experimentally against the previous estimation method which uses leave-one-out on training data and a binary loss function and show that it performs equally well. Being able to train on held-out data is very important in practical situations where training data is mismatched from held-out/test data. We find that fairly small amounts of held-out data (on the order of 30–70 thousand words) are sufficient for training the adjustment model, which is the only model component estimated using gradient descent; the bulk of model parameters are relative frequencies counted on training data A second contribution is a comparison between SNM and the related class of Maximum Entropy language models. While much cheaper computationally, we show that SNM achieves slightly better perplexity results for the same feature set and same speech recognition accuracy on voice search and short message dictation",
    "checked": true,
    "id": "46906b5120bde5903bef59ae11dafc8a8495928e",
    "semantic_title": "sparse non-negative matrix language modeling: maximum entropy flexibility on the cheap",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kumar17c_interspeech.html": {
    "title": "Multi-Scale Context Adaptation for Improving Child Automatic Speech Recognition in Child-Adult Spoken Interactions",
    "volume": "main",
    "abstract": "The mutual influence of participant behavior in a dyadic interaction has been studied for different modalities and quantified by computational models. In this paper, we consider the task of automatic recognition for children's speech, in the context of child-adult spoken interactions during interviews of children suspected to have been maltreated. Our long-term goal is to provide insights within this immensely important, sensitive domain through large-scale lexical and paralinguistic analysis. We demonstrate improvement in child speech recognition accuracy by conditioning on both the domain and the interlocutor's (adult) speech. Specifically, we use information from the automatic speech recognizer outputs of the adult's speech, for which we have more reliable estimates, to modify the recognition system of child's speech in an unsupervised manner. By learning first at session level, and then at the utterance level, we demonstrate an absolute improvement of upto 28% WER and 55% perplexity over the baseline results. We also report results of a parallel human speech recognition (HSR) experiment where annotators are asked to transcribe child's speech under two conditions: with and without contextual speech information. Demonstrated ASR improvements and the HSR experiment illustrate the importance of context in aiding child speech recognition, whether by humans or computers",
    "checked": true,
    "id": "657a2058293ee2198245d82e7a5480599af947de",
    "semantic_title": "multi-scale context adaptation for improving child automatic speech recognition in child-adult spoken interactions",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhu17_interspeech.html": {
    "title": "Using Knowledge Graph and Search Query Click Logs in Statistical Language Model for Speech Recognition",
    "volume": "main",
    "abstract": "This paper demonstrates how Knowledge Graph (KG) and Search Query Click Logs (SQCL) can be leveraged in statistical language models to improve named entity recognition for online speech recognition systems. Due to the missing in the training data, some named entities may be recognized as other common words that have the similar pronunciation. KG and SQCL cover comprehensive and fresh named entities and queries that can be used to mitigate the wrong recognition. First, all the entities located in the same area in KG are clustered together, and the queries that contain the entity names are selected from SQCL as the training data of a geographical statistical language model for each entity cluster. These geographical language models make the unseen named entities less likely to occur during the model training, and can be dynamically switched according to the user location in the recognition phase. Second, if any named entities are identified in the previous utterances within a conversational dialog, the probability of the n-best word sequence paths that contain their related entities will be increased for the current utterance by utilizing the entity relationships from KG and SQCL. This way can leverage the long-term contexts within the dialog. Experiments for the proposed approach on voice queries from a spoken dialog system yielded a 12.5% relative perplexity reduction in the language model measurement, and a 1.1% absolute word error rate reduction in the speech recognition measurement",
    "checked": true,
    "id": "afcc82aba617b737873bc915d1d971359233fdbc",
    "semantic_title": "using knowledge graph and search query click logs in statistical language model for speech recognition",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/dimitriadis17_interspeech.html": {
    "title": "Developing On-Line Speaker Diarization System",
    "volume": "main",
    "abstract": "In this paper we describe the process of converting a research prototype system for Speaker Diarization into a fully deployed product running in real time and with low latency. The deployment is a part of the IBM Cloud Speech-to-Text (STT) Service. First, the prototype system is described and the requirements for the on-line, deployable system are introduced. Then we describe the technical approaches we took to satisfy these requirements and discuss some of the challenges we have faced. In particular, we present novel ideas for speeding up the system by using Automatic Speech Recognition (ASR) transcripts as an input to diarization, we introduce a concept of active window to keep the computational complexity linear, we improve the speaker model using a new speaker-clustering algorithm, we automatically keep track of the number of active speakers and we enable the users to set an operating point on a continuous scale between low latency and optimal accuracy. The deployed system has been tuned on real-life data reaching average Speaker Error Rates around 3% and improving over the prototype system by about 10% relative",
    "checked": true,
    "id": "6e08198a32c8cbe6e34205d56cc4e9d2464d5f62",
    "semantic_title": "developing on-line speaker diarization system",
    "citation_count": 78,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/seshadri17_interspeech.html": {
    "title": "Comparison of Non-Parametric Bayesian Mixture Models for Syllable Clustering and Zero-Resource Speech Processing",
    "volume": "main",
    "abstract": "Zero-resource speech processing (ZS) systems aim to learn structural representations of speech without access to labeled data. A starting point for these systems is the extraction of syllable tokens utilizing the rhythmic structure of a speech signal. Several recent ZS systems have therefore focused on clustering such syllable tokens into linguistically meaningful units. These systems have so far used heuristically set number of clusters, which can, however, be highly dataset dependent and cannot be optimized in actual unsupervised settings. This paper focuses on improving the flexibility of ZS systems using Bayesian non-parametric (BNP) mixture models that are capable of simultaneously learning the cluster models as well as their number based on the properties of the dataset. We also compare different model design choices, namely priors over the weights and the cluster component models, as the impact of these choices is rarely reported in the previous studies. Experiments are conducted using conversational speech from several languages. The models are first evaluated in a separate syllable clustering task and then as a part of a full ZS system in order to examine the potential of BNP methods and illuminate the relative importance of different model design choices",
    "checked": true,
    "id": "1be7bd38b46d4257dacd03a9aa264ecabeadf241",
    "semantic_title": "comparison of non-parametric bayesian mixture models for syllable clustering and zero-resource speech processing",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/proenca17b_interspeech.html": {
    "title": "Automatic Evaluation of Children Reading Aloud on Sentences and Pseudowords",
    "volume": "main",
    "abstract": "Reading aloud performance in children is typically assessed by teachers on an individual basis, manually marking reading time and incorrectly read words. A computational tool that assists with recording reading tasks, automatically analyzing them and providing performance metrics could be a significant help. Towards that goal, this work presents an approach to automatically predicting the overall reading aloud ability of primary school children (6–10 years old), based on the reading of sentences and pseudowords. The opinions of primary school teachers were gathered as ground truth of performance, who provided 0–5 scores closely related to the expectations at the end of each grade. To predict these scores automatically, features based on reading speed and number of disfluencies were extracted, after an automatic disfluency detection. Various regression models were trained, with Gaussian process regression giving best results for automatic features. Feature selection from both sentence and pseudoword reading tasks gave the closest predictions, with a correlation of 0.944. Compared to the use of manual annotation with the best correlation being 0.952, automatic annotation was only 0.8% worse. Furthermore, the error rate of predicted scores relative to ground truth was found to be smaller than the deviation of evaluators' opinion per child",
    "checked": true,
    "id": "c093873b4c71fa09d24f9ff9e217875034e1cc48",
    "semantic_title": "automatic evaluation of children reading aloud on sentences and pseudowords",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/yoon17_interspeech.html": {
    "title": "Off-Topic Spoken Response Detection with Word Embeddings",
    "volume": "main",
    "abstract": "In this study, we developed an automated off-topic response detection system as a supplementary module for an automated proficiency scoring system for non-native English speakers' spontaneous speech. Given a spoken response, the system first generates an automated transcription using an ASR system trained on non-native speech, and then generates a set of features to assess similarity to the question. In contrast to previous studies which required a large set of training responses for each question, the proposed system only requires the question text, thus increasing the practical impact of the system, since new questions can be added to a test dynamically. However, questions are typically short and the traditional approach based on exact word matching does not perform well. In order to address this issue, a set of features based on neural embeddings and a convolutional neural network (CNN) were used. A system based on the combination of all features achieved an accuracy of 87% on a balanced dataset, which was substantially higher than the accuracy of a baseline system using question-based vector space models (49%). Additionally, this system almost reached the accuracy of vector space based model using a large set of responses to test questions (93%)",
    "checked": true,
    "id": "cb677ee09d29b82d84569044e0909138dd25e0b2",
    "semantic_title": "off-topic spoken response detection with word embeddings",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/li17k_interspeech.html": {
    "title": "Improving Mispronunciation Detection for Non-Native Learners with Multisource Information and LSTM-Based Deep Models",
    "volume": "main",
    "abstract": "In this paper, we utilize manner and place of articulation features and deep neural network models (DNNs) with long short-term memory (LSTM) to improve the detection performance of phonetic mispronunciations produced by second language learners. First, we show that speech attribute scores are complementary to conventional phone scores, so they can be concatenated as features to improve a baseline system based only on phone information. Next, pronunciation representation, usually calculated by frame-level averaging in a DNN, is now learned by LSTM, which directly uses sequential context information to embed a sequence of pronunciation scores into a pronunciation vector to improve the performance of subsequent mispronunciation detectors. Finally, when both proposed techniques are incorporated into the baseline phone-based GOP (goodness of pronunciation) classifier system trained on the same data, the integrated system reduces the false acceptance rate (FAR) and false rejection rate (FRR) by 37.90% and 38.44% (relative), respectively, from the baseline system",
    "checked": true,
    "id": "1086389bb3276e24a37e05ce140673f4c635c8ab",
    "semantic_title": "improving mispronunciation detection for non-native learners with multisource information and lstm-based deep models",
    "citation_count": 27,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tsujimura17_interspeech.html": {
    "title": "Automatic Explanation Spot Estimation Method Targeted at Text and Figures in Lecture Slides",
    "volume": "main",
    "abstract": "Because of the spread of the Internet in recent years, e-learning, which is a form of learning through the Internet, has been used in school education. Many lecture videos delivered at The Open University of Japan show lecturers and lecture slides alternately. In such video style, it is hard to understand where on the slide the lecturer is explaining. In this paper, we examined methods to automatically estimate spots where the lecturer explains on the slide using lecture speech and slide data. This technology is expected to help learners to study the lectures. For itemized text slides, using DTW with word embedding based distance, we obtained higher estimation accuracy than a previous work. For slides containing figures, we estimated explanation spots using image classification results and text in the charts. In addition, we modified the lecture browsing system to indicate estimation results on slides, and investigated the usefulness of indicating explanation spots by subjective evaluation with the system",
    "checked": true,
    "id": "af69061e8f327a7531ea6b4a745cfa1e612336be",
    "semantic_title": "automatic explanation spot estimation method targeted at text and figures in lecture slides",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kim17g_interspeech.html": {
    "title": "Multiview Representation Learning via Deep CCA for Silent Speech Recognition",
    "volume": "main",
    "abstract": "Silent speech recognition (SSR) converts non-audio information such as articulatory (tongue and lip) movements to text. Articulatory movements generally have less information than acoustic features for speech recognition, and therefore, the performance of SSR may be limited. Multiview representation learning, which can learn better representations by analyzing multiple information sources simultaneously, has been recently successfully used in speech processing and acoustic speech recognition. However, it has rarely been used in SSR. In this paper, we investigate SSR based on multiview representation learning via canonical correlation analysis (CCA). When both acoustic and articulatory data are available during training, it is possible to effectively learn a representation of articulatory movements from the multiview data with CCA. To further represent the complex structure of the multiview data, we apply deep CCA, where the functional form of the feature mapping is a deep neural network. This approach was evaluated in a speaker-independent SSR task using a data set collected from seven English speakers using an electromagnetic articulograph (EMA). Experimental results showed the effectiveness of the multiview representation learning via deep CCA over the CCA-based multiview approach as well as baseline articulatory movement data on Gaussian mixture model and deep neural network-based SSR systems",
    "checked": true,
    "id": "b03df67c798ebc908758b4d57b8c935fc91f5faa",
    "semantic_title": "multiview representation learning via deep cca for silent speech recognition",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/knill17_interspeech.html": {
    "title": "Use of Graphemic Lexicons for Spoken Language Assessment",
    "volume": "main",
    "abstract": "Automatic systems for practice and exams are essential to support the growing worldwide demand for learning English as an additional language. Assessment of spontaneous spoken English is, however, currently limited in scope due to the difficulty of achieving sufficient automatic speech recognition (ASR) accuracy. \"Off-the-shelf\" English ASR systems cannot model the exceptionally wide variety of accents, pronunciations and recording conditions found in non-native learner data. Limited training data for different first languages (L1s), across all proficiency levels, often with (at most) crowd-sourced transcriptions, limits the performance of ASR systems trained on non-native English learner speech. This paper investigates whether the effect of one source of error in the system, lexical modelling, can be mitigated by using graphemic lexicons in place of phonetic lexicons based on native speaker pronunciations. Graphemic-based English ASR is typically worse than phonetic-based due to the irregularity of English spelling-to-pronunciation but here lower word error rates are consistently observed with the graphemic ASR. The effect of using graphemes on automatic assessment is assessed on different grader feature sets: audio and fluency derived features, including some phonetic level features; and phone/grapheme distance features which capture a measure of pronunciation ability",
    "checked": true,
    "id": "8d3655cf6458a0d13dc2ee1bccdb77972087e73a",
    "semantic_title": "use of graphemic lexicons for spoken language assessment",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/yi17_interspeech.html": {
    "title": "Distilling Knowledge from an Ensemble of Models for Punctuation Prediction",
    "volume": "main",
    "abstract": "This paper proposes an approach to distill knowledge from an ensemble of models to a single deep neural network (DNN) student model for punctuation prediction. This approach makes the DNN student model mimic the behavior of the ensemble. The ensemble consists of three single models. Kullback-Leibler (KL) divergence is used to minimize the difference between the output distribution of the DNN student model and the behavior of the ensemble. Experimental results on English IWSLT2011 dataset show that the ensemble outperforms the previous state-of-the-art model by up to 4.0% absolute in overall F -score. The DNN student model also achieves up to 13.4% absolute overall F -score improvement over the conventionally-trained baseline models",
    "checked": true,
    "id": "dd2aa580bd5e3af2dfdde04b48235dace0c477d7",
    "semantic_title": "distilling knowledge from an ensemble of models for punctuation prediction",
    "citation_count": 26,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/pusateri17_interspeech.html": {
    "title": "A Mostly Data-Driven Approach to Inverse Text Normalization",
    "volume": "main",
    "abstract": "For an automatic speech recognition system to produce sensibly formatted, readable output, the spoken-form token sequence produced by the core speech recognizer must be converted to a written-form string. This process is known as inverse text normalization (ITN). Here we present a mostly data-driven ITN system that leverages a set of simple rules and a few hand-crafted grammars to cast ITN as a labeling problem. To this labeling problem, we apply a compact bi-directional LSTM. We show that the approach performs well using practical amounts of training data",
    "checked": true,
    "id": "392c2b9f28849c88893fcdb97a32c0fe5710e33b",
    "semantic_title": "a mostly data-driven approach to inverse text normalization",
    "citation_count": 26,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17k_interspeech.html": {
    "title": "Mismatched Crowdsourcing from Multiple Annotator Languages for Recognizing Zero-Resourced Languages: A Nullspace Clustering Approach",
    "volume": "main",
    "abstract": "It is extremely challenging to create training labels for building acoustic models of zero-resourced languages, in which conventional resources required for model training — lexicons, transcribed audio, or in extreme cases even orthographic system or a viable phone set design for the language — are unavailable. Here, language mismatched transcripts, in which audio is transcribed in the orthographic system of a completely different language by possibly non-speakers of the target language may play a vital role. Such mismatched transcripts have recently been successfully obtained through crowdsourcing and shown to be beneficial to ASR performance. This paper further studies this problem of using mismatched crowdsourced transcripts in a tonal language for which we have no standard orthography, and in which we may not even know the phoneme inventory. It proposes methods to project the multilingual mismatched transcriptions of a tonal language to the target phone segments. The results tested on Cantonese and Singapore Hokkien have shown that the reconstructed phone sequences' accuracies have absolute increment of more than 3% from those of previously proposed monolingual probabilistic transcription methods",
    "checked": true,
    "id": "5af53dc6575ef82fd63ed1e5c8af506199e92415",
    "semantic_title": "mismatched crowdsourcing from multiple annotator languages for recognizing zero-resourced languages: a nullspace clustering approach",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gale17_interspeech.html": {
    "title": "Experiments in Character-Level Neural Network Models for Punctuation",
    "volume": "main",
    "abstract": "We explore character-level neural network models for inferring punctuation from text-only input. Punctuation inference is treated as a sequence tagging problem where the input is a sequence of un-punctuated characters, and the output is a corresponding sequence of punctuation tags. We experiment with six architectures, all of which use a long short-term memory (LSTM) network for sequence modeling. They differ in the way the context and lookahead for a given character is derived: from simple character embedding and delayed output to enable lookahead, to complex convolutional neural networks (CNN) to capture context. We demonstrate that the accuracy of proposed character-level models are competitive with the accuracy of a state-of-the-art word-level Conditional Random Field (CRF) baseline with carefully crafted features",
    "checked": true,
    "id": "618ca5438ae72664ebc4c2c3553176f014e03252",
    "semantic_title": "experiments in character-level neural network models for punctuation",
    "citation_count": 21,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kaushik17_interspeech.html": {
    "title": "Multi-Channel Apollo Mission Speech Transcripts Calibration",
    "volume": "main",
    "abstract": "NASA's Apollo program is a great achievement of mankind in the 20th century. Previously we had introduced UTD-CRSS Apollo data digitization initiative where we proposed to digitize Apollo mission speech data (~100,000 hours) and develop Spoken Language Technology based algorithms to analyze and understand various aspects of conversational speech[1]. A new 30 track analog audio decoder is designed to decode 30 track Apollo analog tapes and is mounted on to the NASA Soundscriber analog audio decoder (in place of single channel decoder). Using the new decoder all 30 channels of data can be decoded simultaneously thereby reducing the digitization time significantly. We have digitized 19,000 hours of data from Apollo missions (including entire Apollo-11, most of Apollo-13, Apollo-1, and Gemini-8 missions). Each audio track corresponds to a specific personnel/position in NASA mission control room or astronauts in space. Since many of the planned Apollo related spoken language technology approaches need transcripts we have developed an Apollo mission specific custom Deep Neural Networks (DNN) based Automatic Speech Recognition (ASR) system. Apollo specific language models are developed. Most audio channels are degraded due to high channel noise, system noise, attenuated signal bandwidth, transmission noise, cosmic noise, analog tape static noise, noise due to tape aging, etc,. In this paper we propose a novel method to improve the transcript quality by using Signal-to-Noise ratio of channels and N-Gram sentence similarity metrics across data channels. The proposed method shows significant improvement in transcript quality of noisy channels. The Word Error Rate (WER) analysis of transcripts across channels shows significant reduction",
    "checked": true,
    "id": "55eb9d066893706a48e02e09e2c32306d821636b",
    "semantic_title": "multi-channel apollo mission speech transcripts calibration",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mclaren17_interspeech.html": {
    "title": "Calibration Approaches for Language Detection",
    "volume": "main",
    "abstract": "To date, automatic spoken language detection research has largely been based on a closed-set paradigm, in which the languages to be detected are known prior to system application. In actual practice, such systems may face previously unseen languages (out-of-set (OOS) languages) which should be rejected, a common problem that has received limited attention from the research community. In this paper, we focus on situations in which either (1) the system-modeled languages are not observed during use or (2) the test data contains OOS languages that are unseen during modeling or calibration. In these situations, the common multi-class objective function for calibration of language-detection scores is problematic. We describe how the assumptions of multi-class calibration are not always fulfilled in a practical sense and explore applying global and language-dependent binary objective functions to relax system constraints. We contrast the benefits and sensitivities of the calibration approaches on practical scenarios by presenting results using both LRE09 data and 14 languages from the BABEL dataset. We show that the global binary approach is less sensitive to the characteristics of the training data and that OOS modeling with individual detectors is the best option when OOS test languages are not known to the system",
    "checked": true,
    "id": "01dc36156bc4c57329376072aa250011a8eddb4c",
    "semantic_title": "calibration approaches for language detection",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/fernando17_interspeech.html": {
    "title": "Bidirectional Modelling for Short Duration Language Identification",
    "volume": "main",
    "abstract": "Language identification (LID) systems typically employ i-vectors as fixed length representations of utterances. However, it may not be possible to reliably estimate i-vectors from short utterances, which in turn could lead to reduced language identification accuracy. Recently, Long Short Term Memory networks (LSTMs) have been shown to better model short utterances in the context of language identification. This paper explores the use of bidirectional LSTMs for language identification with the aim of modelling temporal dependencies between past and future frame based features in short utterances. Specifically, an end-to-end system for short duration language identification employing bidirectional LSTM models of utterances is proposed. Evaluations on both NIST 2007 and 2015 LRE show state-of-the-art performance",
    "checked": true,
    "id": "67b720a6c47bc28c1295f63c05395774735795e1",
    "semantic_title": "bidirectional modelling for short duration language identification",
    "citation_count": 41,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/shen17b_interspeech.html": {
    "title": "Conditional Generative Adversarial Nets Classifier for Spoken Language Identification",
    "volume": "main",
    "abstract": "The i-vector technique using deep neural network has been successfully applied in spoken language identification systems. Neural network modeling showed its effectiveness as both discriminant feature transformation and classification in many tasks, in particular with a large training data set. However, on a small data set, neural networks suffer from the overfitting problem which degrades the performance. Many strategies have been investigated and used to improve the regularization for deep neural networks, for example, weigh decay, dropout, data augmentation. In this paper, we study and use conditional generative adversarial nets as a classifier for the spoken language identification task. Unlike the previous works on GAN for image generation, our purpose is to focus on improving regularization of the neural network by jointly optimizing the \"Real/Fake\" objective function and the categorical objective function. Compared with dropout and data augmentation methods, the proposed method obtained 29.7% and 31.8% relative improvement on NIST 2015 i-vector challenge data set for spoken language identification",
    "checked": true,
    "id": "050512496557c439a9b31459b457ea1a57ab7208",
    "semantic_title": "conditional generative adversarial nets classifier for spoken language identification",
    "citation_count": 33,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/miguel17_interspeech.html": {
    "title": "Tied Hidden Factors in Neural Networks for End-to-End Speaker Recognition",
    "volume": "main",
    "abstract": "In this paper we propose a method to model speaker and session variability and able to generate likelihood ratios using neural networks in an end-to-end phrase dependent speaker verification system. As in Joint Factor Analysis, the model uses tied hidden variables to model speaker and session variability and a MAP adaptation of some of the parameters of the model. In the training procedure our method jointly estimates the network parameters and the values of the speaker and channel hidden variables. This is done in a two-step backpropagation algorithm, first the network weights and factor loading matrices are updated and then the hidden variables, whose gradients are calculated by aggregating the corresponding speaker or session frames, since these hidden variables are tied. The last layer of the network is defined as a linear regression probabilistic model whose inputs are the previous layer outputs. This choice has the advantage that it produces likelihoods and additionally it can be adapted during the enrolment using MAP without the need of a gradient optimization. The decisions are made based on the ratio of the output likelihoods of two neural network models, speaker adapted and universal background model. The method was evaluated on the RSR2015 database",
    "checked": true,
    "id": "04db9d6bd7d30c9d5a3820462088eaefdf6748dd",
    "semantic_title": "tied hidden factors in neural networks for end-to-end speaker recognition",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/yun17_interspeech.html": {
    "title": "Speaker Clustering by Iteratively Finding Discriminative Feature Space and Cluster Labels",
    "volume": "main",
    "abstract": "This paper presents a speaker clustering framework by iteratively performing two stages: a discriminative feature space is obtained given a cluster label set, and the cluster label set is updated using a clustering algorithm given the feature space. In the iterations of two stages, the cluster labels may be different from the true labels, and thus the obtained feature space based on the labels may be inaccurately discriminated. However, by iteratively performing above two stages, more accurate cluster labels and more discriminative feature space can be obtained, and finally they are converged. In this research, the linear discriminant analysis is used for discriminating the i-vector feature space, and the variational Bayesian expectation-maximization on Gaussian mixture model is used for clustering the i-vectors. Our iterative clustering framework was evaluated using the database of keyword utterances and compared with the recently-published approaches. In all experiments, the results show that our framework outperforms the other approaches and converges in a few iterations",
    "checked": true,
    "id": "ff5d4250a7b0deb9786564d9bfe0f58e0b31a0a7",
    "semantic_title": "speaker clustering by iteratively finding discriminative feature space and cluster labels",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/vinals17_interspeech.html": {
    "title": "Domain Adaptation of PLDA Models in Broadcast Diarization by Means of Unsupervised Speaker Clustering",
    "volume": "main",
    "abstract": "This work presents a new strategy to perform diarization dealing with high variability data, such as multimedia information in broadcast. This variability is highly noticeable among domains (inter-domain variability among chapters, shows, genres, etc.). Therefore, each domain requires its own specific model to obtain the optimal results. We propose to adapt the PLDA models of our diarization system with in-domain unlabeled data. To do it, we estimate pseudo-speaker labels by unsupervised speaker clustering. This new method has been included in a PLDA-based diarization system and evaluated on the Multi-Genre Broadcast 2015 Challenge data. Given an audio, the system computes short-time i-vectors and clusters them using a variational Bayesian PLDA model with hidden labels. The proposed method improves 25.41% relative w.r.t. the system without PLDA adaptation",
    "checked": true,
    "id": "e06d07369c017dc401b2cb5076a4891ad4d5d229",
    "semantic_title": "domain adaptation of plda models in broadcast diarization by means of unsupervised speaker clustering",
    "citation_count": 17,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/india17_interspeech.html": {
    "title": "LSTM Neural Network-Based Speaker Segmentation Using Acoustic and Language Modelling",
    "volume": "main",
    "abstract": "This paper presents a new speaker change detection system based on Long Short-Term Memory (LSTM) neural networks using acoustic data and linguistic content. Language modelling is combined with two different Joint Factor Analysis (JFA) acoustic approaches: i-vectors and speaker factors. Both of them are compared with a baseline algorithm that uses cosine distance to detect speaker turn changes. LSTM neural networks with both linguistic and acoustic features have been able to produce a robust speaker segmentation. The experimental results show that our proposal clearly outperforms the baseline system",
    "checked": true,
    "id": "c6d2aa70538d242fdf48d9fe789871f5ce31fab3",
    "semantic_title": "lstm neural network-based speaker segmentation using acoustic and language modelling",
    "citation_count": 26,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gresse17_interspeech.html": {
    "title": "Acoustic Pairing of Original and Dubbed Voices in the Context of Video Game Localization",
    "volume": "main",
    "abstract": "The aim of this research work is the development of an automatic voice recommendation system for assisted voice casting. In this article, we propose preliminary work on acoustic pairing of original and dubbed voices. The voice segments are taken from a video game released in two different languages. The paired voice segments come from different languages but belong to the same video game character. Our wish is to exploit the relationship between a set of paired segments in order to model the perceptual aspects of a given character depending on the target language. We use a state-of-the-art approach in speaker recognition ( i.e. based on the paradigm i-vector/PLDA). We first evaluate pairs of i-vectors using two different acoustic spaces, one for each of the targeted languages. Secondly, we perform a transformation in order to project the source-language i-vector into the target language. The results showed that this latest approach is able to improve significantly the accuracy. Finally, we challenge the system ability to model the latent information that holds the video-game character independently of the speaker, the linguistic content and the language",
    "checked": true,
    "id": "981409a8b579567af64084c55d640e527d592bc1",
    "semantic_title": "acoustic pairing of original and dubbed voices in the context of video game localization",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ajili17_interspeech.html": {
    "title": "Homogeneity Measure Impact on Target and Non-Target Trials in Forensic Voice Comparison",
    "volume": "main",
    "abstract": "It is common to see mobile recordings being presented as a forensic trace in a court. In such cases, a forensic expert is asked to analyze both suspect and criminal's voice samples in order to determine the strength-of-evidence. This process is known as Forensic Voice Comparison (FVC). The Likelihood ratio (LR) framework is commonly used by the experts and quite often required by the expert's associations \"best practice guides\". Nevertheless, the LR accepts some practical limitations due both to intrinsic aspects of its estimation process and the information used during the FVC process. These aspects are embedded in a more general one, the lack of knowledge on FVC reliability. The question of reliability remains a major challenge, particularly for FVC systems where numerous variation factors like duration, noise, linguistic content or… within-speaker variability are not taken into account. Recently, we proposed an information theory-based criterion able to estimate one of these factors, the homogeneity of information between the two sides of a FVC trial. Thanks to this new criterion, we wish to explore new aspects of homogeneity in this article. We wish to question the impact of homogeneity on reliability separately on target and non-target trials. The study is performed using FABIOLE, a publicly available database dedicated to this kind of studies with a large number of recordings per target speaker. Our experiments report large differences of homogeneity impact between FVC genuine and impostor trials. These results show clearly the importance of intra-speaker variability effects in FVC reliability estimation. This study confirms also the interest of homogeneity measure for FVC reliability",
    "checked": true,
    "id": "156e0fe005693a8a929913dd24365237a4e2b14e",
    "semantic_title": "homogeneity measure impact on target and non-target trials in forensic voice comparison",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/solewicz17_interspeech.html": {
    "title": "Null-Hypothesis LLR: A Proposal for Forensic Automatic Speaker Recognition",
    "volume": "main",
    "abstract": "A new method named Null-Hypothesis LLR (H LLR) is proposed for forensic automatic speaker recognition. The method takes into account the fact that forensically realistic data are difficult to collect and that inter-individual variation is generally better represented than intra-individual variation. According to the proposal, intra-individual variation is modeled as a projection from case-customized inter-individual variation. Calibrated log Likelihood Ratios (LLR) that are calculated on the basis of the H LLR method were tested on two corpora of forensically-founded telephone interception test sets, German-based GFS 2.0 and Dutch-based NFI-FRITS. Five automatic speaker recognition systems were tested based on the scores or the LLRs provided by these systems which form the input to H LLR. Speaker-discrimination and calibration performance of H LLR is comparable to the performance indices of the system-internal LLR calculation methods. This shows that external data and strategies that work with data outside the forensic domain and without case customization are not necessary. It is also shown that H LLR leads to a reduction in the diversity of LLR output patterns of different automatic systems. This is important for the credibility of the Likelihood Ratio framework in forensics, and its application in forensic automatic speaker recognition in particular",
    "checked": true,
    "id": "b95faf9dc2881a00eb69d96ca2a56a176f489a9e",
    "semantic_title": "null-hypothesis llr: a proposal for forensic automatic speaker recognition",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/liu17e_interspeech.html": {
    "title": "The Opensesame NIST 2016 Speaker Recognition Evaluation System",
    "volume": "main",
    "abstract": "Last two decades have witnessed a significant progress in speaker recognition, as evidenced by the improving performance in the speaker recognition evaluations (SRE) hosted by NIST. Despite the progress, only a few research is focused on speaker recognition with short duration and language mismatch condition, which often leads to poor recognition performance. In NIST SRE2016, these concerns were first systematically investigated by the speaker recognition community. In this study, we address these challenges from the viewpoint of feature extraction and modeling. In particular, we improve the robustness of features by combining GMM and DNN based iVector extraction approaches, and improve the reliability of the back-end model by exploiting symmetric SVM that can effectively leverage the unlabeled data. Finally, we introduce distance metric learning to improve the generalization capacity of the development data that is usually in limited size. Then a fusion strategy is adopted to collectively boost the performance. The effectiveness of the proposed scheme for speaker recognition is demonstrated on SRE2016 evaluation data: compared with DNN-iVector PLDA baseline system, our method yields 25.6% relative improvement in terms of min_Cprimary",
    "checked": true,
    "id": "3f6f05ed70a1927660ccca685c9ceb0774af7b0a",
    "semantic_title": "the opensesame nist 2016 speaker recognition evaluation system",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kumar17d_interspeech.html": {
    "title": "IITG-Indigo System for NIST 2016 SRE Challenge",
    "volume": "main",
    "abstract": "This paper describes the speaker verification (SV) system submitted to the NIST 2016 speaker recognition evaluation (SRE) challenge by Indian Institute of Technology Guwahati (IITG) under the fixed training condition task. Various SV systems are developed following the idea-level collaboration with two other Indian institutions. Unlike the previous SREs, this time the focus was on developing SV system using non-target language speech data and a small amount unlabeled data from target language/ dialects. For addressing these novel challenges, we tried exploring the fusion of systems created using different features, data conditioning, and classifiers. On NIST 2016 SRE evaluation data, the presented fused system resulted in actual detection cost function ( actDCF) and equal error rate ( EER) of 0.81 and 12.91%, respectively. Post-evaluation, we explored a recently proposed pairwise support vector machine classifier and applied adaptive S-norm to the decision scores before fusion. With these changes, the final system achieves the actDCF and EER of 0.67 and 11.63%, respectively",
    "checked": true,
    "id": "2ef8339b3dc94f6b96a70ce34e8928f030568659",
    "semantic_title": "iitg-indigo system for nist 2016 sre challenge",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/misra17_interspeech.html": {
    "title": "Locally Weighted Linear Discriminant Analysis for Robust Speaker Verification",
    "volume": "main",
    "abstract": "Channel compensation is an integral part for any state-of-the-art speaker recognition system. Typically, Linear Discriminant Analysis (LDA) is used to suppress directions containing channel information. LDA assumes a unimodal Gaussian distribution of the speaker samples to maximize the ratio of the between-speaker variance to within-speaker variance. However, when speaker samples have multi-modal non-Gaussian distributions due to channel or noise distortions, LDA fails to provide optimal performance. In this study, we propose Locally Weighted Linear Discriminant Analysis (LWLDA). LWLDA computes the within-speaker scatter in a pairwise manner and then scales it by an affinity matrix so as to preserve the within-class local structure. This is in contrast to another recently proposed non-parametric discriminant analysis method called NDA. We show that LWLDA not only performs better than NDA but also is computationally much less expensive. Experiments are performed using the DARPA Robust Automatic Transcription of Speech (RATS) corpus. Results indicate that LWLDA consistently outperforms both LDA and NDA on all trial conditions",
    "checked": true,
    "id": "1cd725c7717216ceb346f7583ab7a6601e18f1c5",
    "semantic_title": "locally weighted linear discriminant analysis for robust speaker verification",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/shon17b_interspeech.html": {
    "title": "Recursive Whitening Transformation for Speaker Recognition on Language Mismatched Condition",
    "volume": "main",
    "abstract": "Recently in speaker recognition, performance degradation due to the channel domain mismatched condition has been actively addressed. However, the mismatches arising from language is yet to be sufficiently addressed. This paper proposes an approach which employs recursive whitening transformation to mitigate the language mismatched condition. The proposed method is based on the multiple whitening transformation, which is intended to remove un-whitened residual components in the dataset associated with i-vector length normalization. The experiments were conducted on the Speaker Recognition Evaluation 2016 trials of which the task is non-English speaker recognition using development dataset consist of both a large scale out-of-domain (English) dataset and an extremely low-quantity in-domain (non-English) dataset. For performance comparison, we develop a state-of-the-art system using deep neural network and bottleneck feature, which is based on a phonetically aware model. From the experimental results, along with other prior studies, effectiveness of the proposed method on language mismatched condition is validated",
    "checked": true,
    "id": "5600112e18c7fde75475308d6aaea3831da8b2f7",
    "semantic_title": "recursive whitening transformation for speaker recognition on language mismatched condition",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/settle17_interspeech.html": {
    "title": "Query-by-Example Search with Discriminative Neural Acoustic Word Embeddings",
    "volume": "main",
    "abstract": "Query-by-example search often uses dynamic time warping (DTW) for comparing queries and proposed matching segments. Recent work has shown that comparing speech segments by representing them as fixed-dimensional vectors — acoustic word embeddings — and measuring their vector distance (e.g., cosine distance) can discriminate between words more accurately than DTW-based approaches. We consider an approach to query-by-example search that embeds both the query and database segments according to a neural model, followed by nearest-neighbor search to find the matching segments. Earlier work on embedding-based query-by-example, using template-based acoustic word embeddings, achieved competitive performance. We find that our embeddings, based on recurrent neural networks trained to optimize word discrimination, achieve substantial improvements in performance and run-time efficiency over the previous approaches",
    "checked": true,
    "id": "592b897d1460e7f1aa28279ec211b2cef7ef9a7c",
    "semantic_title": "query-by-example search with discriminative neural acoustic word embeddings",
    "citation_count": 73,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kaneko17b_interspeech.html": {
    "title": "Constructing Acoustic Distances Between Subwords and States Obtained from a Deep Neural Network for Spoken Term Detection",
    "volume": "main",
    "abstract": "The detection of out-of-vocabulary (OOV) query terms is a crucial problem in spoken term detection (STD), because OOV query terms are likely. To enable search of OOV query terms in STD systems, a query subword sequence is compared with subword sequences generated using an automatic speech recognizer against spoken documents. When comparing two subword sequences, the edit distance is a typical distance between any two subwords. We previously proposed an acoustic distance defined from statistics between states of the hidden Markov model (HMM) and showed its effectiveness in STD [4]. This paper proposes an acoustic distance between subwords and HMM states where the posterior probabilities output by a deep neural network are used to improve the STD accuracy for OOV query terms. Experiments are conducted to evaluate the performance of the proposed method, using the open test collections for the \"Spoken&Doc\" tasks of the NTCIR-9 [13] and NTCIR-10 [14] workshops. The proposed method shows improvements in mean average precision",
    "checked": true,
    "id": "06e713e32a8005bb98bdbe1787dbe0d2355a4d0b",
    "semantic_title": "constructing acoustic distances between subwords and states obtained from a deep neural network for spoken term detection",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/khokhlov17_interspeech.html": {
    "title": "Fast and Accurate OOV Decoder on High-Level Features",
    "volume": "main",
    "abstract": "This work proposes a novel approach to out-of-vocabulary (OOV) keyword search (KWS) task. The proposed approach is based on using high-level features from an automatic speech recognition (ASR) system, so called phoneme posterior based ( PPB) features, for decoding. These features are obtained by calculating time-dependent phoneme posterior probabilities from word lattices, followed by their smoothing. For the PPB features we developed a special novel very fast, simple and efficient OOV decoder. Experimental results are presented on the Georgian language from the IARPA Babel Program, which was the test language in the OpenKWS 2016 evaluation campaign. The results show that in terms of maximum term weighted value (MTWV) metric and computational speed, for single ASR systems, the proposed approach significantly outperforms the state-of-the-art approach based on using in-vocabulary proxies for OOV keywords in the indexed database. The comparison of the two OOV KWS approaches on the fusion results of the nine different ASR systems demonstrates that the proposed OOV decoder outperforms the proxy-based approach in terms of MTWV metric given the comparable processing speed. Other important advantages of the OOV decoder include extremely low memory consumption and simplicity of its implementation and parameter optimization",
    "checked": true,
    "id": "45b6a6c46971056e54e5e89df00745ee78091f3d",
    "semantic_title": "fast and accurate oov decoder on high-level features",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17l_interspeech.html": {
    "title": "Exploring the Use of Significant Words Language Modeling for Spoken Document Retrieval",
    "volume": "main",
    "abstract": "Owing to the rapid global access to tremendous amounts of multimedia associated with speech information on the Internet, spoken document retrieval (SDR) has become an emerging application recently. Apart from much effort devoted to developing robust indexing and modeling techniques for spoken documents, a recent line of research targets at enriching and reformulating query representations in an attempt to enhance retrieval effectiveness. In practice, pseudo-relevance feedback is by far the most prevalent paradigm for query reformulation, which assumes that top-ranked feedback documents obtained from the initial round of retrieval are potentially relevant and can be exploited to reformulate the original query. Continuing this line of research, the paper presents a novel modeling framework, which aims at discovering significant words occurring in the feedback documents, to infer an enhanced query language model for SDR. Formally, the proposed framework targets at extracting the essential words representing a common notion of relevance (i.e., the significant words which occur in almost all of the feedback documents), so as to deduce a new query language model that captures these significant words and meanwhile modulates the influence of both highly frequent words and too specific words. Experiments conducted on a benchmark SDR task demonstrate the performance merits of our proposed framework",
    "checked": true,
    "id": "f23397a53ac12029b3308620aa40b4779d7e5859",
    "semantic_title": "exploring the use of significant words language modeling for spoken document retrieval",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tasaki17_interspeech.html": {
    "title": "Incorporating Acoustic Features for Spontaneous Speech Driven Content Retrieval",
    "volume": "main",
    "abstract": "A speech-driven information retrieval system is expected to be useful for gathering information with greater ease. In a conventional system, users have to decide on the contents of their utterance before speaking, which takes quite a long time when their request is complicated. To overcome that problem, it is required for the retrieval system to handle a spontaneously spoken query directly. In this work, we propose an extension technique of spoken content retrieval (SCR) for effectively using spontaneously spoken queries. Acoustic features of meaningful terms in the retrieval may have prominence compared to other terms. Also, those terms will have linguistic specificity. From this assumption, we predict the contribution of terms included in spontaneously spoken queries using acoustic and linguistic features, and incorporate it in the query likelihood model (QLM) which is a probabilistic retrieval model. We verified the effectiveness of the proposed method through experiments. Our proposed method was successful in improving retrieval performance under various conditions",
    "checked": true,
    "id": "1e9e16125efad5119f9190bed764df9ff70fef3a",
    "semantic_title": "incorporating acoustic features for spontaneous speech driven content retrieval",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lu17b_interspeech.html": {
    "title": "Order-Preserving Abstractive Summarization for Spoken Content Based on Connectionist Temporal Classification",
    "volume": "main",
    "abstract": "Connectionist temporal classification (CTC) is a powerful approach for sequence-to-sequence learning, and has been popularly used in speech recognition. The central ideas of CTC include adding a label \"blank\" during training. With this mechanism, CTC eliminates the need of segment alignment, and hence has been applied to various sequence-to-sequence learning problems. In this work, we applied CTC to abstractive summarization for spoken content. The \"blank\" in this case implies the corresponding input data are less important or noisy; thus it can be ignored. This approach was shown to outperform the existing methods in term of ROUGE scores over Chinese Giga-word and MATBN corpora. This approach also has the nice property that the ordering of words or characters in the input documents can be better preserved in the generated summaries",
    "checked": true,
    "id": "a64b75c593980a7d88a6d6573f1bb2de79082713",
    "semantic_title": "order-preserving abstractive summarization for spoken content based on connectionist temporal classification",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tsuchiya17_interspeech.html": {
    "title": "Automatic Alignment Between Classroom Lecture Utterances and Slide Components",
    "volume": "main",
    "abstract": "Multimodal alignment between classroom lecture utterances and lecture slide components is one of the crucial problems to realize a multimodal e-Learning application. This paper proposes the new method for the automatic alignment, and formulates the alignment as the integer linear programming (ILP) problem to maximize the score function which consists of three factors: the similarity score between utterances and slide components, the consistency of the explanation order, and the explanation coverage of slide components. The experimental result on the Corpus of Japanese classroom Lecture Contents (CJLC) shows that the automatic alignment information acquired by the proposed method is effective to improve the performance of the automatic extraction of important utterances",
    "checked": true,
    "id": "7d66cd10e41dad216813e4dff91b16588e627b3a",
    "semantic_title": "automatic alignment between classroom lecture utterances and slide components",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lopezotero17_interspeech.html": {
    "title": "Compensating Gender Variability in Query-by-Example Search on Speech Using Voice Conversion",
    "volume": "main",
    "abstract": "The huge amount of available spoken documents has raised the need for tools to perform automatic searches within large audio databases. These collections usually consist of documents with a great variability regarding speaker, language or recording channel, among others. Reducing this variability would boost the performance of query-by-example search on speech systems, especially in zero-resource systems that use acoustic features for audio representation. Hence, in this work, a technique to compensate the variability caused by speaker gender is proposed. Given a data collection composed of documents spoken by both male and female voices, every time a spoken query has to be searched, an alternative version of the query on its opposite gender is generated using voice conversion. After that, the female version of the query is used to search within documents spoken by females and vice versa. Experimental validation of the proposed strategy shows an improvement of search on speech performance caused by the reduction of gender variability",
    "checked": true,
    "id": "cc47a42e376a7a559ebb39c3c6fdbb02a525b448",
    "semantic_title": "compensating gender variability in query-by-example search on speech using voice conversion",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kumar17e_interspeech.html": {
    "title": "Zero-Shot Learning Across Heterogeneous Overlapping Domains",
    "volume": "main",
    "abstract": "We present a zero-shot learning approach for text classification, predicting which natural language understanding domain can handle a given utterance. Our approach can predict domains at runtime that did not exist at training time. We achieve this extensibility by learning to project utterances and domains into the same embedding space while generating each domain-specific embedding from a set of attributes that characterize the domain. Our model is a neural network trained via ranking loss. We evaluate the performance of this zero-shot approach on a subset of a virtual assistant's third-party domains and show the effectiveness of the technique on new domains not observed during training. We compare to generative baselines and show that our approach requires less storage and performs better on new domains",
    "checked": true,
    "id": "349c50a22c9f5b46f4ed0f03912706b2c9d484d5",
    "semantic_title": "zero-shot learning across heterogeneous overlapping domains",
    "citation_count": 34,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tsunoo17_interspeech.html": {
    "title": "Hierarchical Recurrent Neural Network for Story Segmentation",
    "volume": "main",
    "abstract": "A broadcast news stream consists of a number of stories and each story consists of several sentences. We capture this structure using a hierarchical model based on a word-level Recurrent Neural Network (RNN) sentence modeling layer and a sentence-level bidirectional Long Short-Term Memory (LSTM) topic modeling layer. First, the word-level RNN layer extracts a vector embedding the sentence information from the given transcribed lexical tokens of each sentence. These sentence embedding vectors are fed into a bidirectional LSTM that models the sentence and topic transitions. A topic posterior for each sentence is estimated discriminatively and a Hidden Markov model (HMM) follows to decode the story sequence and identify story boundaries. Experiments on the topic detection and tracking (TDT2) task indicate that the hierarchical RNN topic modeling achieves the best story segmentation performance with a higher F1-measure compared to conventional state-of-the-art methods. We also compare variations of our model to infer the optimal structure for the story segmentation task",
    "checked": true,
    "id": "8cfb517a873554518334dc59aa08e505e67115b3",
    "semantic_title": "hierarchical recurrent neural network for story segmentation",
    "citation_count": 20,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bouchekif17_interspeech.html": {
    "title": "Evaluating Automatic Topic Segmentation as a Segment Retrieval Task",
    "volume": "main",
    "abstract": "Several evaluation metrics have been proposed for topic segmentation. Most of them rely on the paradigm that segmentation is mainly a task that detects boundaries, and thus are oriented on boundary detection evaluation. Nevertheless, this paradigm is not appropriate to get homogeneous chapters, which is one of the major applications of topic segmentation. For instance on Broadcast News, topic segmentation enables users to watch a chapter independently of the others We propose to consider segmentation as a task that detects homogeneous segments, and we propose evaluation metrics oriented on segment retrieval. The proposed metrics are experimented on various TV shows from different channels. Results are analysed and discussed, highlighting their relevance",
    "checked": true,
    "id": "22cf4039e43aa9e470ce5c05c56d2ddb81e84546",
    "semantic_title": "evaluating automatic topic segmentation as a segment retrieval task",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bang17_interspeech.html": {
    "title": "Improving Speech Recognizers by Refining Broadcast Data with Inaccurate Subtitle Timestamps",
    "volume": "main",
    "abstract": "This paper proposes an automatic method to refine broadcast data collected every week for efficient acoustic model training. For training acoustic models, we use only audio signals, subtitle texts, and subtitle timestamps accompanied by recorded broadcast programs. However, the subtitle timestamps are often inaccurate due to inherent characteristics of closed captioning. In the proposed method, we remove subtitle texts with low subtitle quality index, concatenate adjacent subtitle texts into a merged subtitle text, and correct the timestamp of the merged subtitle text by adding a margin. Then, a speech recognizer is used to obtain a hypothesis text from the speech segment corresponding to the merged subtitle text. Finally, the refined speech segments to be used for acoustic model training, are generated by selecting the subparts of the merged subtitle text that matches the hypothesis text. It is shown that the acoustic models trained by using refined broadcast data give significantly higher speech recognition accuracy than those trained by using raw broadcast data. Consequently, the proposed method can efficiently refine a large amount of broadcast data with inaccurate timestamps taking about half of the time, compared with the previous approaches",
    "checked": true,
    "id": "dea591425ab84a5bddbfdef24309d30313635ac5",
    "semantic_title": "improving speech recognizers by refining broadcast data with inaccurate subtitle timestamps",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/svec17_interspeech.html": {
    "title": "A Relevance Score Estimation for Spoken Term Detection Based on RNN-Generated Pronunciation Embeddings",
    "volume": "main",
    "abstract": "In this paper, we present a novel method for term score estimation. The method is primarily designed for scoring the out-of-vocabulary terms, however it could also estimate scores for in-vocabulary results. The term score is computed as a cosine distance of two pronunciation embeddings. The first one is generated from the grapheme representation of the searched term, while the second one is computed from the recognized phoneme confusion network. The embeddings are generated by specifically trained recurrent neural network built on the idea of Siamese neural networks. The RNN is trained from recognition results on word- and phone-level in an unsupervised fashion without need of any hand-labeled data. The method is evaluated on the MALACH data in two languages, English and Czech. The results are compared with two baseline methods for OOV term detection",
    "checked": true,
    "id": "a8ad654be9b7b1c3914ac69a697850fc4657473b",
    "semantic_title": "a relevance score estimation for spoken term detection based on rnn-generated pronunciation embeddings",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gallardo17c_interspeech.html": {
    "title": "Predicting Automatic Speech Recognition Performance Over Communication Channels from Instrumental Speech Quality and Intelligibility Scores",
    "volume": "main",
    "abstract": "The performance of automatic speech recognition based on coded-decoded speech heavily depends on the quality of the transmitted signals, determined by channel impairments. This paper examines relationships between speech recognition performance and measurements of speech quality and intelligibility over transmission channels. Different to previous studies, the effects of super-wideband transmissions are analyzed and compared to those of wideband and narrowband channels. Furthermore, intelligibility scores, gathered by conducting a listening test based on logatomes, are also considered for the prediction of automatic speech recognition results. The modern instrumental measurement techniques POLQA and POLQA-based intelligibility have been respectively applied to estimate the quality and the intelligibility of transmitted speech. Based on our results, polynomial models are proposed that permit the prediction of speech recognition accuracy from the subjective and instrumental measures, involving a number of channel distortions in the three bandwidths. This approach can save the costs of performing automatic speech recognition experiments and can be seen as a first step towards a useful tool for communication channel designers",
    "checked": true,
    "id": "720df5c273c492cd42d3faaa46b3bf92191cb978",
    "semantic_title": "predicting automatic speech recognition performance over communication channels from instrumental speech quality and intelligibility scores",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/botinhao17_interspeech.html": {
    "title": "Speech Intelligibility in Cars: The Effect of Speaking Style, Noise and Listener Age",
    "volume": "main",
    "abstract": "Intelligibility of speech in noise becomes lower as the listeners age increases, even when no apparent hearing impairment is present. The losses are, however, different depending on the nature of the noise and the characteristics of the voice. In this paper we investigate the effect that age, noise type and speaking style have on the intelligibility of speech reproduced by car loudspeakers. Using a binaural mannequin we recorded a variety of voices and speaking styles played from the audio system of a car while driving in different conditions. We used this material to create a listening test where participants were asked to transcribe what they could hear and recruited groups of young and older adults to take part in it. We found that intelligibility scores of older participants were lower for the competing speaker and background music conditions. Results also indicate that clear and Lombard speech was more intelligible than plain speech for both age groups. A mixed effect model revealed that the largest effect was the noise condition, followed by sentence type, speaking style, voice, age group and pure tone average",
    "checked": true,
    "id": "46487bff894575e415da7799cd7cd29a509d1302",
    "semantic_title": "speech intelligibility in cars: the effect of speaking style, noise and listener age",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/yamamoto17_interspeech.html": {
    "title": "Predicting Speech Intelligibility Using a Gammachirp Envelope Distortion Index Based on the Signal-to-Distortion Ratio",
    "volume": "main",
    "abstract": "A new intelligibility prediction measure, called \"Gammachirp Envelope Distortion Index (GEDI)\" is proposed for the evaluation of speech enhancement algorithms. This model calculates the signal-to-distortion ratio (SDR) in envelope responses SDRenv derived from the gammachirp filterbank outputs of clean and enhanced speech, and is an extension of the speech based envelope power spectrum model (sEPSM) to improve prediction and usability. An evaluation was performed by comparing human subjective results and model predictions for the speech intelligibility of noise-reduced sounds processed by spectral subtraction and a recent Wiener filtering technique. The proposed GEDI predicted the subjective results of the Wiener filtering better than those predicted by the original sEPSM and well-known conventional measures, i.e., STOI, CSII, and HASPI",
    "checked": true,
    "id": "d774c4b68d53fe3b3ba47f0c40fa2d2e7d6e10e0",
    "semantic_title": "predicting speech intelligibility using a gammachirp envelope distortion index based on the signal-to-distortion ratio",
    "citation_count": 11,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17m_interspeech.html": {
    "title": "Intelligibilities of Mandarin Chinese Sentences with Spectral \"Holes",
    "volume": "main",
    "abstract": "The speech intelligibility of Mandarin Chinese sentences of various spectral regions, regarding band-stop conditions (one or two \"holes\" in the spectrum), was investigated through subjective listening tests. Results demonstrated significant effects on Mandarin Chinese sentence intelligibilities when a single or a pair of spectral holes was introduced. Meanwhile, it revealed the importance of the first and second formant (F1, F2) frequencies for the comprehension of Mandarin sentences. More importantly, the first formant frequencies played a more primary role rather than those of the second formants. Sentence intelligibilities declined evidently with the lacking of F1 frequencies, but the effect became small when the spectrum holes covered more than 50% of F1 frequencies, and F2 frequencies came into a major play in the intelligibility of Mandarin sentence",
    "checked": true,
    "id": "0a80c06e82fcc9a3c0df1e92c7800a8b8f87d344",
    "semantic_title": "intelligibilities of mandarin chinese sentences with spectral \"holes",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ward17b_interspeech.html": {
    "title": "The Effect of Situation-Specific Non-Speech Acoustic Cues on the Intelligibility of Speech in Noise",
    "volume": "main",
    "abstract": "In everyday life, speech is often accompanied by a situation-specific acoustic cue; a hungry bark as you ask ‘Has anyone fed the dog?'. This paper investigates the effect such cues have on speech intelligibility in noise and evaluates their interaction with the established effect of situation-specific semantic cues. This work is motivated by the introduction of new object-based broadcast formats, which have the potential to optimise intelligibility by controlling the level of individual broadcast audio elements, at point of service. Results of this study show that situation-specific acoustic cues alone can improve word recognition in multi-talker babble by 69.5%, a similar amount to semantic cues. The combination of both semantic and acoustic cues provide further improvement of 106.0% compared with no cues, and 18.7% compared with semantic cues only. Interestingly, whilst increasing subjective intelligibility of the target word, the presence of acoustic cues degraded the objective intelligibility of the speech-based semantic cues by 47.0% (equivalent to reducing the speech level by 4.5 dB). This paper discusses the interactions between the two types of cues and the implications that these results have for assessing and improving the intelligibility of broadcast speech",
    "checked": true,
    "id": "9101596e1e26d9565cfa31d766b3029e5e817c55",
    "semantic_title": "the effect of situation-specific non-speech acoustic cues on the intelligibility of speech in noise",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/andersen17_interspeech.html": {
    "title": "On the Use of Band Importance Weighting in the Short-Time Objective Intelligibility Measure",
    "volume": "main",
    "abstract": "Speech intelligibility prediction methods are popular tools within the speech processing community for objective evaluation of speech intelligibility of e.g. enhanced speech. The Short-Time Objective Intelligibility (STOI) measure has become highly used due to its simplicity and high prediction accuracy. In this paper we investigate the use of Band Importance Functions (BIFs) in the STOI measure, i.e. of unequally weighting the contribution of speech information from each frequency band. We do so by fitting BIFs to several datasets of measured intelligibility, and cross evaluating the prediction performance. Our findings indicate that it is possible to improve prediction performance in specific situations. However, it has not been possible to find BIFs which systematically improve prediction performance beyond the data used for fitting. In other words, we find no evidence that the performance of the STOI measure can be improved considerably by extending it with a non-uniform BIF",
    "checked": true,
    "id": "02d378ab35c7679df01211d2b1d430c687bdfd30",
    "semantic_title": "on the use of band importance weighting in the short-time objective intelligibility measure",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/spille17_interspeech.html": {
    "title": "Listening in the Dips: Comparing Relevant Features for Speech Recognition in Humans and Machines",
    "volume": "main",
    "abstract": "In recent years, automatic speech recognition (ASR) systems gradually decreased (and for some tasks closed) the gap between human and automatic speech recognition. However, it is unclear if similar performance implies humans and ASR systems to rely on similar signal cues. In the current study, ASR and HSR are compared using speech material from a matrix sentence test mixed with either a stationary speech-shaped noise (SSN) or amplitude-modulated SSN. Recognition performance of HSR and ASR is measured in term of the speech recognition threshold (SRT), i.e., the signal-to-noise ratio with 50% recognition rate and by comparing psychometric functions. ASR results are obtained with matched-trained DNN-based systems that use FBank features as input and compared to results obtained from eight normal-hearing listeners and two established models of speech intelligibility. For both maskers, HSR and ASR achieve similar SRTs with an average deviation of only 0.4 dB. A relevance propagation algorithm is applied to identify features relevant for ASR. The analysis shows that relevant features coincide either with spectral peaks of the speech signal or with dips of the noise masker, indicating that similar cues are important in HSR and ASR",
    "checked": true,
    "id": "df364390217b9034758440511146b5392ddcf23b",
    "semantic_title": "listening in the dips: comparing relevant features for speech recognition in humans and machines",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sugai17_interspeech.html": {
    "title": "Mental Representation of Japanese Mora; Focusing on its Intrinsic Duration",
    "volume": "main",
    "abstract": "Japanese is one of the typical languages in which vowel quantity plays a key role. In Japanese, a phonological structure called \"mora\" is a fundamental rhythmic unit, and theoretically, each mora is supposed to have a similar duration (isochronicity). The rhythm of a native language has great importance on spoken language processing, including second language speaking; therefore, in order to get a clear picture of bottom-up speech processing, it is crucial to discern how morae are mentally represented. Various studies have been conducted to understand the nature of speech processing as a cognitive construct; however, most of this research was conducted with the target stimuli embedded in words or carrier sentences to clarify on specifically the relative duration of morae. In this study, two reaction-time experiments were conducted to investigate whether morae are mentally represented and how long the duration is. The isolated vowels /i/, /e/, /a/, /o/, /u/, and syllable /tan/ were chosen as target stimuli, and the first morae were digitally manipulated into 15 durations with 20 ms variations in length, from 150 ms to 330 ms. The results revealed the existence of a durational threshold between one and two morae, ranging around 250 ms",
    "checked": true,
    "id": "a87dcec338755ebf2567c28ddca92ee76ebe3c9d",
    "semantic_title": "mental representation of japanese mora; focusing on its intrinsic duration",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ying17_interspeech.html": {
    "title": "Temporal Dynamics of Lateral Channel Formation in /l/: 3D EMA Data from Australian English",
    "volume": "main",
    "abstract": "This study investigated the dynamics of lateral channel formation of /l/ in Australian-accented English (AusE) using 3D electromagnetic articulography (EMA). Coils were placed on the tongue both mid-sagitally and para-sagitally. We varied the vowel preceding /l/ between /ɪ/ and /æ/, e.g., filbert vs. talbot, and the syllable position of /l/, e.g., /'tæl.bət/ vs. /'tæb.lət/. The articulatory analyses of lateral /l/ show that: (1) the mid-sagittal delay (from the tongue tip gesture to the tongue middle/tongue back gesture) changes across different syllable positions and vowel contexts; (2) the para-sagittal lateralization duration remains the same across syllable positions and vowel contexts; (3) the lateral formation reaches its peak earlier than the mid-sagittal gesture peak; (4) the magnitude of tongue asymmetrical lateralization is greater than the magnitude of tongue curvature in the coronal plane. We discuss these results in light of the temporal dynamics of lateral channel formation. We interpret our results as evidence that the formation of the lateral channel is the primary goal of /l/ production",
    "checked": true,
    "id": "6d2e85eee4e9682b7a5f5f0d6f09057d1ea1cd5c",
    "semantic_title": "temporal dynamics of lateral channel formation in /l/: 3d ema data from australian english",
    "citation_count": 12,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/klingler17_interspeech.html": {
    "title": "Vowel and Consonant Sequences in three Bavarian Dialects of Austria",
    "volume": "main",
    "abstract": "In 1913, Anton Pfalz described a specific relation of vowel and consonant sequences for East Middle Bavarian dialects, located in the eastern parts of Austria. According to his observations, a long vowel is always followed by a lenis consonant, and a short vowel is always followed by a fortis consonant. Consequently, vowel duration depends on the quality of the following consonant. Phonetic examinations of what became to be known as the Pfalz's Law yielded different results. Specifically, the occurrence of a third category, namely a long vowel followed by a fortis consonant, seems to be firmly embedded in East Middle Bavarian Up till now, phonetic examinations concentrated on CVCV sequences. The analysis of monosyllables and of sequences including consonant clusters has been largely neglected so far. In the present contribution, we analyse the impact of initial and final consonant clusters in monosyllables on the assumed relationship of vowel + consonant sequences. Thus, we included 18 speakers from three Bavarian varieties. The results show that in all examined varieties long vowel + fortis consonants occur and that the cluster complexity has no influence on the absolute vowel duration, contradicting Pfalz's Law",
    "checked": true,
    "id": "5dfb1dd773f4721d2208715e1f17399b6bebcecf",
    "semantic_title": "vowel and consonant sequences in three bavarian dialects of austria",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/issa17_interspeech.html": {
    "title": "Acoustic Cues to the Singleton-Geminate Contrast: The Case of Libyan Arabic Sonorants",
    "volume": "main",
    "abstract": "This study examines the acoustic correlates of the singleton and geminate consonants in Tripolitanian Libyan Arabic (TLA). Several measurements were obtained including target segment duration, preceding vowel duration, RMS amplitude for the singleton and geminate consonants, and F1, F2 and F3 for the target consonants. The results confirm that the primary acoustic correlate that distinguishes singletons from geminates in TLA is duration regardless of sound type with the ratio of C to CC being 1 to 2.42. The duration of the preceding vowels is suggestive and may be considered as another cue to the distinction between them. There was no evidence of differences in RMS amplitude between singleton and geminate consonants of any type. F1, F2 and F3 frequencies are found to show similar patterns for singleton and geminate consonants for all sound types, suggesting no gestural effects of gemination in TLA. Preliminary results from the phonetic cues investigated here suggest that the acoustic distinction between singleton and geminate consonants in TLA is dependent mainly on durational correlates",
    "checked": true,
    "id": "96141c1853685d6bbe2165ab3275ade8dddf4a8d",
    "semantic_title": "acoustic cues to the singleton-geminate contrast: the case of libyan arabic sonorants",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/brandt17_interspeech.html": {
    "title": "Mel-Cepstral Distortion of German Vowels in Different Information Density Contexts",
    "volume": "main",
    "abstract": "This study investigated whether German vowels differ significantly from each other in mel-cepstral distortion (MCD) when they stand in different information density (ID) contexts. We hypothesized that vowels in the same ID contexts are more similar to each other than vowels that stand in different ID conditions. Read speech material from PhonDat2 of 16 German natives (m = 10, f = 6) was analyzed. Bi-phone and word language models were calculated based on DeWaC. To account for additional variability in the data, prosodic factors, as well as corpus-specific frequency values were also entered into the statistical models. Results showed that vowels in different ID conditions were significantly different in their MCD values. Unigram word probability and corpus-specific word frequency showed the expected effect on vowel similarity with a hierarchy between non-contrasting and contrasting conditions. However, these did not form a homogeneous group since there were group-internal significant differences. The largest distance can be found between vowels produced at fast speech rate, and between unstressed vowels",
    "checked": true,
    "id": "165def1782deb05cbc5bc664b828e60906f48fc7",
    "semantic_title": "mel-cepstral distortion of german vowels in different information density contexts",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/boril17_interspeech.html": {
    "title": "Effect of Formant and F0 Discontinuity on Perceived Vowel Duration: Impacts for Concatenative Speech Synthesis",
    "volume": "main",
    "abstract": "Unit selection systems of speech synthesis offer good overall quality, but this may be countervailed by a sporadic and unpredictable occurrence of audible artifacts, such as discontinuities in F0 and the spectrum. Informal observations suggested that such breaks may have an effect on perceived vowel duration. This study therefore investigates the effect of F0 and formant discontinuities on the perceived duration of vowels in Czech synthetic speech. Ten manipulations of F0, F1 and F2 were performed on target vowels in short synthesized phrases creating abrupt breaks in the contours at the midpoint of the vowels. Listeners decided in a 2AFC task in which phrase the last syllable was longer. The results showed that despite identical duration of the compared stimuli, vowels which were manipulated in the second part towards centralized values (i.e., less peripheral) were systematically considered to be shorter by the listeners than stimuli without such discontinuities, and vice versa. However, the influence seems to be distinct from an overall formant change (without a discontinuity) since a control stimulus in which the manipulation was performed within the entire vowel was not perceived as significantly shorter or longer. No effect of F0 manipulations was observed",
    "checked": true,
    "id": "07d946b88d9201cd20022941098fd581d3304582",
    "semantic_title": "effect of formant and f0 discontinuity on perceived vowel duration: impacts for concatenative speech synthesis",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tabain17_interspeech.html": {
    "title": "An Ultrasound Study of Alveolar and Retroflex Consonants in Arrernte: Stressed and Unstressed Syllables",
    "volume": "main",
    "abstract": "This study presents ultrasound data from six female speakers of the Central Australian language Arrernte. We focus on the apical stop contrast, alveolar /t/ versus retroflex /ʈ/, which may be considered phonemically marginal. We compare these sounds in stressed and unstressed position. Consistent with previous results on this apical contrast, we show that there are minimal differences between the retroflex and the alveolar at stop offset; however, at stop onset, the retroflex has a higher front portion of the tongue, and often a more forward posterior portion of the tongue. This difference between the alveolar and the retroflex is particularly remarked in unstressed prosodic context. This result confirms our previous EPG and EMA results from two of the speakers in the present study, which showed that the most prototypical retroflex consonant occurs in the unstressed prosodic position",
    "checked": true,
    "id": "78b125fb105c81a4bf566cb2cbe60a2011a7aab9",
    "semantic_title": "an ultrasound study of alveolar and retroflex consonants in arrernte: stressed and unstressed syllables",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gobl17_interspeech.html": {
    "title": "Reshaping the Transformed LF Model: Generating the Glottal Source from the Waveshape Parameter Rd",
    "volume": "main",
    "abstract": "Precise specification of the voice source would facilitate better modelling of expressive nuances in human spoken interaction. This paper focuses on the transformed version of the widely used LF voice source model, and proposes an algorithm which makes it possible to use the waveshape parameter R to directly control the LF pulse, for more effective analysis and synthesis of voice modulations. The R parameter, capturing much of the natural covariation between glottal parameters, is central to the transformed LF model. It is used to predict the standard R-parameters, which in turn are used to synthesise the LF waveform. However, the LF pulse that results from these predictions may have an R value noticeably different from the specified R , yielding undesirable artefacts, particularly when the model is used for detailed analysis and synthesis of non-modal voice. A further limitation is that only a subset of possible R values can be used, to avoid conflicting LF parameter settings. To eliminate these problems, a new iterative algorithm was developed based on the Newton-Raphson method for two variables, but modified to include constraints. This ensures that the correct R is always obtained and that the algorithm converges for effectively all permissible R values",
    "checked": true,
    "id": "1418a18b9c353de44b0c087419d08abae4d0e733",
    "semantic_title": "reshaping the transformed lf model: generating the glottal source from the waveshape parameter rd",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/benus17_interspeech.html": {
    "title": "Kinematic Signatures of Prosody in Lombard Speech",
    "volume": "main",
    "abstract": "Human spoken interactions are embodied and situated. Better understanding of the restrictions and affordances this embodiment and situational awareness has on human speech informs the quest for more natural models of human-machine spoken interactions. Here we examine the articulatory realization of communicative meanings expressed through f0 falling and rising prosodic boundaries in quiet and noisy conditions. Our data show that 1) the effect of environmental noise is more robustly present in the post-boundary than the pre-boundary movements, 2) f0 falls and rises are only weakly differentiated in supra-laryngeal articulation and differ minimally in their response to noise, 3) individual speakers find different solutions for achieving the communicative goals, and 4) lip movements are affected by noise and boundary type more than the tongue movements",
    "checked": true,
    "id": "bacdd1b71fb6106fade53699a09ea7857946b8da",
    "semantic_title": "kinematic signatures of prosody in lombard speech",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jochim17b_interspeech.html": {
    "title": "What do Finnish and Central Bavarian Have in Common? Towards an Acoustically Based Quantity Typology",
    "volume": "main",
    "abstract": "The aim of this study was to investigate vowel and consonant quantity in Finnish, a typical quantity language, and to set up a reference corpus for a large-scale project studying the diachronic development of quantity contrasts in German varieties. Although German is not considered a quantity language, both tense and lax vowels and voiced and voiceless stops are differentiated by vowel and closure duration, respectively. The role of these cues, however, has undergone different diachronic changes in various German varieties. To understand the conditions for such prosodic changes, the present study investigates the stability of quantity relations in an undisputed quantity language. To this end, recordings of words differing in vowel and stop length were obtained from seven older and six younger L1 Finnish speakers, both in a normal and a loud voice. We then measured vowel and stop duration and calculated the vowel to vowel-plus-consonant ratio (a measure known to differentiate German VC sequences) as well as the geminate-to-singleton ratio. Results show stability across age groups but variability across speech styles. Moreover, VC ratios were similar for Finnish and Bavarian German speakers. We discuss our findings against the background of a typology of vowel and consonant quantity",
    "checked": true,
    "id": "239e8ddab3f2aafe3e244b8175b61e1ead30020d",
    "semantic_title": "what do finnish and central bavarian have in common? towards an acoustically based quantity typology",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/nellore17_interspeech.html": {
    "title": "Locating Burst Onsets Using SFF Envelope and Phase Information",
    "volume": "main",
    "abstract": "Bursts are produced by closing the oral tract at a place of articulation and suddenly releasing the acoustic energy built-up behind the closure in the tract. The release of energy is an impulse-like behavior, and it is followed by a short duration of frication. The burst release is short and mostly weak in nature (compared to sonorant sounds), thus making it difficult to detect its presence in continuous speech. This paper attempts to identify burst onsets based on parameters derived from single frequency filtering (SFF) analysis of speech signals. The SFF envelope and phase information give good spectral and temporal resolutions of certain features of the signal. Signal reconstructed from the SFF phase information is shown to be useful in locating burst onsets. Entropy and spectral distance parameters from the SFF spectral envelopes are used to refine the burst onset candidate set. The identified burst onset locations are compared with manual annotations in the TIMIT database",
    "checked": true,
    "id": "f6a50fbe041fc28fff48eb78670d925185bc3a1b",
    "semantic_title": "locating burst onsets using sff envelope and phase information",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ding17_interspeech.html": {
    "title": "A Preliminary Phonetic Investigation of Alphabetic Words in Mandarin Chinese",
    "volume": "main",
    "abstract": "Chinese words written partly or fully in roman letters have gained popularity in Mandarin Chinese in the last few decades and an appendix of such Mandarin Alphabetical Words (MAWs) is included in the authoritative dictionary of Standard Mandarin. However, no transcription of MAWs has been provided because it is not clear whether we should keep the original English pronunciation or transcribe MAWs with Mandarin Pinyin system. This study aims to investigate the phonetic adaptation of several most frequent MAWs extracted from the corpus. We recruited eight students from Shanghai, 18 students from Shandong Province, and one student from the USA. All the subjects were asked to read both 24 Chinese sentences embedding the MAWs and all 26 letters of the English alphabet. The results showed that Letters A O N T were predominantly pronounced in Tone 1; H was often produced with vowel epenthesis after the final consonant; and B was usually produced in Tone 2 by Shanghai speakers and in Tone 4 by Shandong speakers. We conclude that the phonetic adaptation of MAWs is influenced by the dialects of the speakers, tones of other Chinese characters in the MAWs, as well as individual preferences",
    "checked": true,
    "id": "ec4e2252e1ce2a5a29b697342d5c1a3e257d3cda",
    "semantic_title": "a preliminary phonetic investigation of alphabetic words in mandarin chinese",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/schatz17_interspeech.html": {
    "title": "A Quantitative Measure of the Impact of Coarticulation on Phone Discriminability",
    "volume": "main",
    "abstract": "Acoustic realizations of a given phonetic segment are typically affected by coarticulation with the preceding and following phonetic context. While coarticulation has been extensively studied using descriptive phonetic measurements, little is known about the functional impact of coarticulation for speech processing. Here, we use DTW-based similarity defined on raw acoustic features and ABX scores to derive a measure of the effect of coarticulation on phonetic discriminability. This measure does not rely on defining segment-specific phonetic cues (formants, duration, etc.) and can be applied systematically and automatically to any segment in large scale corpora. We illustrate our method using stimuli in English and Japanese. We confirm some expected trends, i.e., stronger anticipatory than perseveratory coarticulation and stronger coarticulation for lax/short vowels than for tense/long vowels. We then quantify for the first time the impact of coarticulation across different segment types (like vowels and consonants). We discuss how our metric and its possible extensions can help addressing current challenges in the systematic study of coarticulation",
    "checked": true,
    "id": "1ee46563f4f5bab741e6a86afe6c8d95e00029a2",
    "semantic_title": "a quantitative measure of the impact of coarticulation on phone discriminability",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lin17b_interspeech.html": {
    "title": "Sinusoidal Partials Tracking for Singing Analysis Using the Heuristic of the Minimal Frequency and Magnitude Difference",
    "volume": "main",
    "abstract": "We present a simple heuristic-based Sinusoidal Partial Tracking (PT) algorithm for singing analysis. Our PT algorithm uses a heuristic of minimal frequency and magnitude difference to track sinusoidal partials in the popular music. An Ideal Binary Mask (IBM), which is created from the ground truth of the singing voice and the music accompaniment, is used to identify the sound source of the partials. In this justifiable way, we are able to assess the quality of the partials identified from the PT algorithm. Using the iKala dataset along with the IBM and BSS Eval 3.0 as a new method of quantifying the partials quality, the comparative results show that our PT algorithm can achieve 0.8746 ~ 1.7029 dB GNSDR gain, compared to two common benchmarks, namely the MQ algorithm and the SMS-PT algorithm. Thus, our PT algorithm can be considered as a new benchmark of the PT algorithm used in singing analysis",
    "checked": true,
    "id": "41f0973c0777f6da3b47fa035aa0bc071c8f02f8",
    "semantic_title": "sinusoidal partials tracking for singing analysis using the heuristic of the minimal frequency and magnitude difference",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/phan17_interspeech.html": {
    "title": "Audio Scene Classification with Deep Recurrent Neural Networks",
    "volume": "main",
    "abstract": "We introduce in this work an efficient approach for audio scene classification using deep recurrent neural networks. An audio scene is firstly transformed into a sequence of high-level label tree embedding feature vectors. The vector sequence is then divided into multiple subsequences on which a deep GRU-based recurrent neural network is trained for sequence-to-label classification. The global predicted label for the entire sequence is finally obtained via aggregation of subsequence classification outputs. We will show that our approach obtains an F1-score of 97.7% on the LITIS Rouen dataset, which is the largest dataset publicly available for the task. Compared to the best previously reported result on the dataset, our approach is able to reduce the relative classification error by 35.3%",
    "checked": true,
    "id": "6b2c770e4fa5d945db3057946aa6cd37cc20b8ba",
    "semantic_title": "audio scene classification with deep recurrent neural networks",
    "citation_count": 58,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sandsten17_interspeech.html": {
    "title": "Automatic Time-Frequency Analysis of Echolocation Signals Using the Matched Gaussian Multitaper Spectrogram",
    "volume": "main",
    "abstract": "High-resolution time-frequency (TF) images of multi-component signals are of great interest for visualization, feature extraction and estimation. The matched Gaussian multitaper spectrogram has been proposed to optimally resolve multi-component transient functions of Gaussian shape. Hermite functions are used as multitapers and the weights of the different spectrogram functions are optimized. For a fixed number of multitapers, the optimization gives the approximate Wigner distribution of the Gaussian shaped function. Increasing the number of multitapers gives a better approximation, i.e. a better resolution, but the cross-terms also become more prominent for close TF components. In this submission, we evaluate a number of different concentration measures to automatically estimate the number of multitapers resulting in the optimal spectrogram for TF images of dolphin echolocation signals. The measures are evaluated for different multi-component signals and noise levels and a suggestion of an automatic procedure for optimal TF analysis is given. The results are compared to other well known TF estimation algorithms and examples of real data measurements of echolocation signals from a beluga whale ( Delphinapterus leucas) are presented",
    "checked": true,
    "id": "f13f1cd807c53e1178a15ec7c5071d5bd982e6fb",
    "semantic_title": "automatic time-frequency analysis of echolocation signals using the matched gaussian multitaper spectrogram",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/matousek17_interspeech.html": {
    "title": "Classification-Based Detection of Glottal Closure Instants from Speech Signals",
    "volume": "main",
    "abstract": "In this paper a classification-based method for the automatic detection of glottal closure instants (GCIs) from the speech signal is proposed. Peaks in the speech waveforms are taken as candidates for GCI placements. A classification framework is used to train a classification model and to classify whether or not a peak corresponds to the GCI. We show that the detection accuracy in terms of F1 score is 97.27%. In addition, despite using the speech signal only, the proposed method behaves comparably to a method utilizing the glottal signal. The method is also compared with three existing GCI detection algorithms on publicly available databases",
    "checked": true,
    "id": "7733cc382ff4110debd11a188674f317a0e5dc89",
    "semantic_title": "classification-based detection of glottal closure instants from speech signals",
    "citation_count": 17,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/qi17_interspeech.html": {
    "title": "A Domain Knowledge-Assisted Nonlinear Model for Head-Related Transfer Functions Based on Bottleneck Deep Neural Network",
    "volume": "main",
    "abstract": "Many methods have been proposed for modeling head-related transfer functions (HRTFs) and yield a good performance level in terms of log-spectral distortion (LSD). However, most of them utilize linear weighting to reconstruct or interpolate HRTFs, but not consider the inherent nonlinearity relationship between the basis function and HRTFs. Motivated by this, a domain knowledge-assisted nonlinear modeling method is proposed based on bottleneck features. Domain knowledge is used in two aspects. One is to generate the input features derived from the solution to sound wave propagation equation at the physical level, and the other is to design the loss function for model training based on the knowledge of objective evaluation criterion, i.e., LSD. Furthermore, with utilizing the strong representation ability of the bottleneck features, the nonlinear model has the potential to achieve a more accurate mapping. The objective and subjective experimental results show that the proposed method gains less LSD when compared with linear model, and the interpolated HRTFs can generate a similar perception to those of the database",
    "checked": true,
    "id": "1ecd1cf043d4e3b55145ccf1ab72094d2289f01a",
    "semantic_title": "a domain knowledge-assisted nonlinear model for head-related transfer functions based on bottleneck deep neural network",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jesus17_interspeech.html": {
    "title": "Laryngeal Articulation During Trumpet Performance: An Exploratory Study",
    "volume": "main",
    "abstract": "Music teacher's reports suggest that the respiratory function and laryngeal control in wind instruments, stimulate muscular tension of the involved anatomical structure. However, the physiology and acoustics of the larynx during trumpet playing has seldom been studied. Therefore, the current paper describes the laryngeal articulation during trumpet performance with biomedical signals and auditory perception. The activation of laryngeal musculature of six professional trumpeters when playing a standard musical passage was analysed using audio, electroglottography (EGG), oxygen saturation and heart rate signals. Two University trumpet teachers listened to the audio recordings, to evaluate the participants' laryngeal effort (answers on a 100 mm Visual-Analogue-Scale (VAS): 0 \"no perceived effort\"; 100 \"extreme effort\"). Correlations between parameters extracted from the EGG data and the perception of the audio stimuli by the teachers were explored. Two hundred and fifty laryngeal articulations, where raising of the larynx and muscular effort were observed, were annotated and analysed. No correlation between the EGG data and the auditory evaluation was observed. However, both teachers perceived the laryngeal effort (VAS mean scores = 61±14). Our findings show that EGG and auditory perception data can provide new insights into laryngeal articulation and breathing control that are key to low muscular tension",
    "checked": true,
    "id": "3e4b758e53fd3a4517f77740f396fcc11b9eaf59",
    "semantic_title": "laryngeal articulation during trumpet performance: an exploratory study",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/guan17_interspeech.html": {
    "title": "Matrix of Polynomials Model Based Polynomial Dictionary Learning Method for Acoustic Impulse Response Modeling",
    "volume": "main",
    "abstract": "We study the problem of dictionary learning for signals that can be represented as polynomials or polynomial matrices, such as convolutive signals with time delays or acoustic impulse responses. Recently, we developed a method for polynomial dictionary learning based on the fact that a polynomial matrix can be expressed as a polynomial with matrix coefficients, where the coefficient of the polynomial at each time lag is a scalar matrix. However, a polynomial matrix can be also equally represented as a matrix with polynomial elements. In this paper, we develop an alternative method for learning a polynomial dictionary and a sparse representation method for polynomial signal reconstruction based on this model. The proposed methods can be used directly to operate on the polynomial matrix without having to access its coefficients matrices. We demonstrate the performance of the proposed method for acoustic impulse response modeling",
    "checked": true,
    "id": "ff461438573cc7ad441c8991ba2e924b701bf401",
    "semantic_title": "matrix of polynomials model based polynomial dictionary learning method for acoustic impulse response modeling",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hyder17_interspeech.html": {
    "title": "Acoustic Scene Classification Using a CNN-SuperVector System Trained with Auditory and Spectrogram Image Features",
    "volume": "main",
    "abstract": "Enabling smart devices to infer about the environment using audio signals has been one of the several long-standing challenges in machine listening. The availability of public-domain datasets, e.g., Detection and Classification of Acoustic Scenes and Events (DCASE) 2016, enabled researchers to compare various algorithms on standard predefined tasks. Most of the current best performing individual acoustic scene classification systems utilize different spectrogram image based features with a Convolutional Neural Network (CNN) architecture. In this study, we first analyze the performance of a state-of-the-art CNN system for different auditory image and spectrogram features, including Mel-scaled, logarithmically scaled, linearly scaled filterbank spectrograms, and Stabilized Auditory Image (SAI) features. Next, we benchmark an MFCC based Gaussian Mixture Model (GMM) SuperVector (SV) system for acoustic scene classification. Finally, we utilize the activations from the final layer of the CNN to form a SuperVector (SV) and use them as feature vectors for a Probabilistic Linear Discriminative Analysis (PLDA) classifier. Experimental evaluation on the DCASE 2016 database demonstrates the effectiveness of the proposed CNN-SV approach compared to conventional CNNs with a fully connected softmax output layer. Score fusion of individual systems provides up to 7% relative improvement in overall accuracy compared to the CNN baseline system",
    "checked": true,
    "id": "8764cc19cb5b2e1421dff219b05b62df6c9dc514",
    "semantic_title": "acoustic scene classification using a cnn-supervector system trained with auditory and spectrogram image features",
    "citation_count": 32,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/feng17b_interspeech.html": {
    "title": "An Environmental Feature Representation for Robust Speech Recognition and for Environment Identification",
    "volume": "main",
    "abstract": "In this paper we investigate environment feature representations, which we refer to as e-vectors, that can be used for environment adaption in automatic speech recognition (ASR), and for environment identification. Inspired by the fact that i-vectors in the total variability space capture both speaker and channel environment variability, our proposed e-vectors are extracted from i-vectors. Two extraction methods are proposed: one is via linear discriminant analysis (LDA) projection, and the other via a bottleneck deep neural network (BN-DNN). Our evaluations show that by augmenting DNN-HMM ASR systems with the proposed e-vectors for environment adaptation, ASR performance is significantly improved. We also demonstrate that the proposed e-vector yields promising results on environment identification",
    "checked": true,
    "id": "893384123a4e257186ad9a28d88eacb84af20df3",
    "semantic_title": "an environmental feature representation for robust speech recognition and for environment identification",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/xu17b_interspeech.html": {
    "title": "Attention and Localization Based on a Deep Convolutional Recurrent Model for Weakly Supervised Audio Tagging",
    "volume": "main",
    "abstract": "Audio tagging aims to perform multi-label classification on audio chunks and it is a newly proposed task in the Detection and Classification of Acoustic Scenes and Events 2016 (DCASE 2016) challenge. This task encourages research efforts to better analyze and understand the content of the huge amounts of audio data on the web. The difficulty in audio tagging is that it only has a chunk-level label without a frame-level label. This paper presents a weakly supervised method to not only predict the tags but also indicate the temporal locations of the occurred acoustic events. The attention scheme is found to be effective in identifying the important frames while ignoring the unrelated frames. The proposed framework is a deep convolutional recurrent model with two auxiliary modules: an attention module and a localization module. The proposed algorithm was evaluated on the Task 4 of DCASE 2016 challenge. State-of-the-art performance was achieved on the evaluation set with equal error rate (EER) reduced from 0.13 to 0.11, compared with the convolutional recurrent baseline system",
    "checked": true,
    "id": "fc81205f3580998d642d029a91be9ceb9d10ff4f",
    "semantic_title": "attention and localization based on a deep convolutional recurrent model for weakly supervised audio tagging",
    "citation_count": 67,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/pan17_interspeech.html": {
    "title": "An Audio Based Piano Performance Evaluation Method Using Deep Neural Network Based Acoustic Modeling",
    "volume": "main",
    "abstract": "In this paper, we propose an annotated piano performance evaluation dataset with 185 audio pieces and a method to evaluate the performance of piano beginners based on their audio recordings. The proposed framework includes three parts: piano key posterior probability extraction, Dynamic Time Warping (DTW) based matching and performance score regression. First, a deep neural network model is trained to extract 88 dimensional piano key features from Constant-Q Transform (CQT) spectrum. The proposed acoustic model shows high robustness to the recording environments. Second, we employ the DTW algorithm on the high-level piano key feature sequences to align the input with the template. Upon the alignment, we extract multiple global matching features that could reflect the similarity between the input and the template. Finally, we apply linear regression upon these matching features with the scores annotated by expertise in training data to estimate performance scores for test audio. Experimental results show that our automatic evaluation method achieves 2.64 average absolute score error in score range from 0 to 100, and 0.73 average correlation coefficient on our in-house collected YCU-MPPE-II dataset",
    "checked": true,
    "id": "9244562704e7204e65b518227634a71e24f67b0c",
    "semantic_title": "an audio based piano performance evaluation method using deep neural network based acoustic modeling",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chowdhury17_interspeech.html": {
    "title": "Music Tempo Estimation Using Sub-Band Synchrony",
    "volume": "main",
    "abstract": "Tempo estimation aims at estimating the pace of a musical piece measured in beats per minute. This paper presents a new tempo estimation method that utilizes coherent energy changes across multiple frequency sub-bands to identify the onsets. A new measure, called the sub-band synchrony, is proposed to detect and quantify the coherent amplitude changes across multiple sub-bands. Given a musical piece, our method first detects the onsets using the sub-band synchrony measure. The periodicity of the resulting onset curve, measured using the autocorrelation function, is used to estimate the tempo value. The performance of the sub-band synchrony based tempo estimation method is evaluated on two music databases. Experimental results indicate a reasonable improvement in performance when compared to conventional methods of tempo estimation",
    "checked": true,
    "id": "bd5e0b001d909977c39415df21629378aa9e3cf6",
    "semantic_title": "music tempo estimation using sub-band synchrony",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wang17k_interspeech.html": {
    "title": "A Transfer Learning Based Feature Extractor for Polyphonic Sound Event Detection Using Connectionist Temporal Classification",
    "volume": "main",
    "abstract": "Sound event detection is the task of detecting the type, onset time, and offset time of sound events in audio streams. The mainstream solution is recurrent neural networks (RNNs), which usually predict the probability of each sound event at every time step. Connectionist temporal classification (CTC) has been applied in order to relax the need for exact annotations of onset and offset times; the CTC output layer is expected to generate a peak for each event boundary where the acoustic signal is most salient. However, with limited training data, the CTC network has been found to train slowly, and generalize poorly to new data In this paper, we try to introduce knowledge learned from a much larger corpus into the CTC network. We train two variants of SoundNet, a deep convolutional network that takes the audio tracks of videos as input, and tries to approximate the visual information extracted by an image recognition network. A lower part of SoundNet or its variants is then used as a feature extractor for the CTC network to perform sound event detection. We show that the new feature extractor greatly accelerates the convergence of the CTC network, and slightly improves the generalization",
    "checked": true,
    "id": "f1b35a675017b9eabd70a4bb4ec90a61117e4ad2",
    "semantic_title": "a transfer learning based feature extractor for polyphonic sound event detection using connectionist temporal classification",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mostafa17_interspeech.html": {
    "title": "A Note Based Query By Humming System Using Convolutional Neural Network",
    "volume": "main",
    "abstract": "In this paper, we propose a note-based query by humming (QBH) system with Hidden Markov Model (HMM) and Convolutional Neural Network (CNN) since note-based systems are much more efficient than the traditional frame-based systems. A note-based QBH system has two main components: humming transcription and candidate melody retrieval For humming transcription, we are the first to use a hybrid model using HMM and CNN. We use CNN for its ability to learn the features directly from raw audio data and for being able to model the locality and variability often present in a note and we use HMM for handling the variability across the time-axis For candidate melody retrieval, we use locality sensitive hashing to narrow down the candidates for retrieval and dynamic time warping and earth mover's distance for the final ranking of the selected candidates We show that our HMM-CNN humming transcription system outperforms other state of the art humming transcription systems by ~2% using the transcription evaluation framework by Molina et. al and our overall query by humming system has a Mean Reciprocal Rank of 0.92 using the standard MIREX dataset, which is higher than other state of the art note-based query by humming systems",
    "checked": true,
    "id": "fb63903f9c7f84c484a6f3a1cec4c2f1929b850c",
    "semantic_title": "a note based query by humming system using convolutional neural network",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sailor17b_interspeech.html": {
    "title": "Unsupervised Filterbank Learning Using Convolutional Restricted Boltzmann Machine for Environmental Sound Classification",
    "volume": "main",
    "abstract": "In this paper, we propose to use Convolutional Restricted Boltzmann Machine (ConvRBM) to learn filterbank from the raw audio signals. ConvRBM is a generative model trained in an unsupervised way to model the audio signals of arbitrary lengths. ConvRBM is trained using annealed dropout technique and parameters are optimized using Adam optimization. The subband filters of ConvRBM learned from the ESC-50 database resemble Fourier basis in the mid-frequency range while some of the low-frequency subband filters resemble Gammatone basis. The auditory-like filterbank scale is nonlinear w.r.t. the center frequencies of the subband filters and follows the standard auditory scales. We have used our proposed model as a front-end for the Environmental Sound Classification (ESC) task with supervised Convolutional Neural Network (CNN) as a back-end. Using CNN classifier, the ConvRBM filterbank (ConvRBM-BANK) and its score-level fusion with the Mel filterbank energies (FBEs) gave an absolute improvement of 10.65%, and 18.70% in the classification accuracy, respectively, over FBEs alone on the ESC-50 database. This shows that the proposed ConvRBM filterbank also contains highly complementary information over the Mel filterbank, which is helpful in the ESC task",
    "checked": true,
    "id": "f6fd1be38a2d764d900b11b382a379efe88b3ed6",
    "semantic_title": "unsupervised filterbank learning using convolutional restricted boltzmann machine for environmental sound classification",
    "citation_count": 87,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/soni17_interspeech.html": {
    "title": "Novel Shifted Real Spectrum for Exact Signal Reconstruction",
    "volume": "main",
    "abstract": "Retrieval of the phase of a signal is one of the major problems in signal processing. For an exact signal reconstruction, both magnitude, and phase spectrum of the signal is required. In many speech-based applications, only the magnitude spectrum is processed and the phase is ignored, which leads to degradation in the performance. Here, we propose a novel technique that enables the reconstruction of the speech signal from magnitude spectrum only. We consider the even-odd part decomposition of a causal sequence and process only on the real part of the DTFT of the signal. We propose the shifting of the real part of DTFT of the sequence to make it non-negative. By adding a constant of sufficient value to the real part of the DTFT, the exact signal reconstruction is possible from the magnitude or power spectrum alone. Moreover, we have compared our proposed approach with recently proposed phase retrieval method from magnitude spectrum of the Causal Delta Dominant (CDD) signal. We found that the method of phase retrieval from CDD signal and proposed method are identical under certain approximation. However, proposed method involves the less computational cost for the exact processing of the signal",
    "checked": true,
    "id": "e520db0a7541514f051c93cdec6616a50453a40a",
    "semantic_title": "novel shifted real spectrum for exact signal reconstruction",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/weiner17_interspeech.html": {
    "title": "Manual and Automatic Transcriptions in Dementia Detection from Speech",
    "volume": "main",
    "abstract": "As the population in developed countries is aging, larger numbers of people are at risk of developing dementia. In the near future there will be a need for time- and cost-efficient screening methods. Speech can be recorded and analyzed in this manner, and as speech and language are affected early on in the course of dementia, automatic speech processing can provide valuable support for such screening methods We present two pipelines of feature extraction for dementia detection: the manual pipeline uses manual transcriptions while the fully automatic pipeline uses transcriptions created by automatic speech recognition (ASR). The acoustic and linguistic features that we extract need no language specific tools other than the ASR system. Using these two different feature extraction pipelines we automatically detect dementia. Our results show that the ASR system's transcription quality is a good single feature and that the features extracted from automatic transcriptions perform similar or slightly better than the features extracted from the manual transcriptions",
    "checked": true,
    "id": "3ce917438bba77b3c73ee865512e6d66930b7090",
    "semantic_title": "manual and automatic transcriptions in dementia detection from speech",
    "citation_count": 42,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gupta17_interspeech.html": {
    "title": "An Affect Prediction Approach Through Depression Severity Parameter Incorporation in Neural Networks",
    "volume": "main",
    "abstract": "Humans use emotional expressions to communicate their internal affective states. These behavioral expressions are often multi-modal (e.g. facial expression, voice and gestures) and researchers have proposed several schemes to predict the latent affective states based on these expressions. The relationship between the latent affective states and their expression is hypothesized to be affected by several factors; depression disorder being one of them. Despite a wide interest in affect prediction, and several studies linking the effect of depression on affective expressions, only a limited number of affect prediction models account for the depression severity. In this work, we present a novel scheme that incorporates depression severity as a parameter in Deep Neural Networks (DNNs). In order to predict affective dimensions for an individual at hand, our scheme alters the DNN activation function based on the subject's depression severity. We perform experiments on affect prediction in two different sessions of the Audio-Visual Depressive language Corpus, which involves patients with varying degree of depression. Our results show improvements in arousal and valence prediction on both the sessions using the proposed DNN modeling. We also present analysis of the impact of such an alteration in DNNs during training and testing",
    "checked": true,
    "id": "eee47c79d087b36030a64e3a4410a56d5209ba20",
    "semantic_title": "an affect prediction approach through depression severity parameter incorporation in neural networks",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gillespie17_interspeech.html": {
    "title": "Cross-Database Models for the Classification of Dysarthria Presence",
    "volume": "main",
    "abstract": "Dysarthria is a motor speech disorder that impacts verbal articulation and co-ordination, resulting in slow, slurred and imprecise speech. Automated classification of dysarthria subtypes and severities could provide a useful clinical tool in assessing the onset and progress in treatment. This study represents a pilot project to train models to detect the presence of dysarthria in continuous speech. Subsets of the Universal Access Research Dataset (UA-Speech) and the Atlanta Motor Speech Disorders Corpus (AMSDC) database were utilized in a cross-database training strategy (training on UA-Speech / testing on AMSDC) to distinguish speech with and without dysarthria. In addition to traditional spectral and prosodic features, the current study also includes features based on the Teager Energy Operator (TEO) and the glottal waveform. Baseline results on the UA-Speech dataset maximize word- and participant-level accuracies at 75.3% and 92.9% using prosodic features. However, the cross-training of UA-Speech tested on the AMSDC maximize word- and participant-level accuracies at 71.3% and 90% based on a TEO feature. The results of this pilot study reinforce consideration of dysarthria subtypes in cross-dataset training as well as highlight additional features that may be sensitive to the presence of dysarthria in continuous speech",
    "checked": true,
    "id": "dd36e54e27a5a7f888e6f545c9deab90fed39f90",
    "semantic_title": "cross-database models for the classification of dysarthria presence",
    "citation_count": 26,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/novotny17_interspeech.html": {
    "title": "Acoustic Evaluation of Nasality in Cerebellar Syndromes",
    "volume": "main",
    "abstract": "Although previous studies have reported the occurrence of velopharyngeal incompetence connected with ataxic dysarthria, there is a lack of evidence related to nasality assessment in cerebellar disorders. This is partly due to the limited reliability of challenging analyses and partly due to nasality being a less pronounced manifestation of ataxic dysarthria. Therefore, we employed 1/3-octave spectra analysis as an objective measurement of nasality disturbances. We analyzed 20 subjects with multiple system atrophy (MSA), 13 subjects with cerebellar ataxia (CA), 20 subjects with multiple sclerosis (MS) and 20 healthy (HC) speakers. Although we did not detect the presence of hypernasality, our results showed increased nasality fluctuation in 65% of MSA, 43% of CA and 30% of MS subjects compared to 15% of HC speakers, suggesting inconsistent velopharyngeal motor control. Furthermore, we found a statistically significant difference between MSA and HC participants (p<0.001), and significant correlation between the natural history cerebellar subscore and neuroprotection in Parkinson plus syndromes — Parkinson plus scale and nasality fluctuations in MSA (r=0.51, p<0.05). In conclusion, acoustic analysis showed an increased presence of abnormal nasality fluctuations in all ataxic groups and revealed that nasality fluctuation is associated with distortion of cerebellar functions",
    "checked": true,
    "id": "7b326d07af2510e5af889316543c3ba22742a796",
    "semantic_title": "acoustic evaluation of nasality in cerebellar syndromes",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hantke17_interspeech.html": {
    "title": "Emotional Speech of Mentally and Physically Disabled Individuals: Introducing the EmotAsS Database and First Findings",
    "volume": "main",
    "abstract": "The automatic recognition of emotion from speech is a mature research field with a large number of publicly available corpora. However, to the best of the authors knowledge, none of these datasets consist solely of emotional speech samples from individuals with mental, neurological and/or physical disabilities. Yet, such individuals could benefit from speech-based assistive technologies to enhance their communication with their environment and to manage their daily work process. With the aim of advancing these technologies, we fill this void in emotional speech resources by introducing the EmotAsS (Emotional Sensitivity Assistance System for People with Disabilities) corpus consisting of spontaneous emotional German speech data recorded from 17 mentally, neurologically and/or physically disabled participants in their daily work environment, resulting in just under 11 hours of total speech time and featuring approximately 12.7 k utterances after segmentation. Transcription was performed and labelling was carried out in seven emotional categories, as well as for the intelligibility of the speaker. We present a set of baseline results, based on using standard acoustic and linguistic features, for arousal and valence emotion recognition",
    "checked": true,
    "id": "42e640fc7d37c51b157e7007117eacb78d7789a9",
    "semantic_title": "emotional speech of mentally and physically disabled individuals: introducing the emotass database and first findings",
    "citation_count": 16,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/agurto17_interspeech.html": {
    "title": "Phonological Markers of Oxytocin and MDMA Ingestion",
    "volume": "main",
    "abstract": "Speech data has the potential to become a powerful tool to provide quantitative information about emotion beyond that achieved by subjective assessments. Based on this concept, we investigate the use of speech to identify effects in subjects under the influence of two different drugs: Oxytocin (OT) and 3,4-methylenedioxymethamphetamine (MDMA), also known as ecstasy. We extract a set of informative phonological features that can characterize emotion. Then, we perform classification to detect if the subject is under the influence of a drug. Our best results show low error rates of 13% and 17% for the subject classification of OT and MDMA vs. placebo, respectively. We also analyze the performance of the features to differentiate the two levels of MDMA doses, obtaining an error rate of 19%. The results indicate that subtle emotional changes can be detected in the context of drug use",
    "checked": true,
    "id": "5b02311b8e505a0f2625e8603558da95c4b3c344",
    "semantic_title": "phonological markers of oxytocin and mdma ingestion",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mirheidari17_interspeech.html": {
    "title": "An Avatar-Based System for Identifying Individuals Likely to Develop Dementia",
    "volume": "main",
    "abstract": "This paper presents work on developing an automatic dementia screening test based on patients' ability to interact and communicate — a highly cognitively demanding process where early signs of dementia can often be detected. Such a test would help general practitioners, with no specialist knowledge, make better diagnostic decisions as current tests lack specificity and sensitivity. We investigate the feasibility of basing the test on conversations between a ‘talking head' (avatar) and a patient and we present a system for analysing such conversations for signs of dementia in the patient's speech and language. Previously we proposed a semi-automatic system that transcribed conversations between patients and neurologists and extracted conversation analysis style features in order to differentiate between patients with progressive neurodegenerative dementia (ND) and functional memory disorders (FMD). Determining who talks when in the conversations was performed manually. In this study, we investigate a fully automatic system including speaker diarisation, and the use of additional acoustic and lexical features. Initial results from a pilot study are presented which shows that the avatar conversations can successfully classify ND/FMD with around 91% accuracy, which is in line with previous results for conversations that were led by a neurologist",
    "checked": true,
    "id": "d4b5344c959ea05a880bd816cc4b1ed7ef46262b",
    "semantic_title": "an avatar-based system for identifying individuals likely to develop dementia",
    "citation_count": 28,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhang17k_interspeech.html": {
    "title": "Cross-Domain Classification of Drowsiness in Speech: The Case of Alcohol Intoxication and Sleep Deprivation",
    "volume": "main",
    "abstract": "In this work, we study the drowsy state of a speaker, induced by alcohol intoxication or sleep deprivation. In particular, we investigate the coherence between the two pivotal causes of drowsiness, as featured in the Intoxication and Sleepiness tasks of the INTERSPEECH Speaker State Challenge. In this way, we aim to exploit the interrelations between these different, yet highly correlated speaker states, which need to be reliably recognised in safety and security critical environments. To this end, we perform cross-domain classification of alcohol intoxication and sleepiness, thus leveraging the acoustic similarities of these speech phenomena for transfer learning. Further, we conducted in-depth feature analysis to quantitatively assess the task relatedness and to determine the most relevant features for both tasks. To test our methods in realistic contexts, we use the Alcohol Language Corpus and the Sleepy Language Corpus containing in total 60 hours of genuine intoxicated and sleepy speech. In the result, cross-domain classification combined with feature selection yields up to 60.3% unweighted average recall, which is significantly above-chance (50%) and highly notable given the mismatch in the training and validation data. Finally, we show that an effective, general drowsiness classifier can be obtained by aggregating the training data from both domains",
    "checked": true,
    "id": "d1a12dbf85c91d7484fc56db4be089f719e4ee7a",
    "semantic_title": "cross-domain classification of drowsiness in speech: the case of alcohol intoxication and sleep deprivation",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lopezotero17b_interspeech.html": {
    "title": "Depression Detection Using Automatic Transcriptions of De-Identified Speech",
    "volume": "main",
    "abstract": "Depression is a mood disorder that is usually addressed by outpatient treatments in order to favour patient's inclusion in society. This leads to a need for novel automatic tools exploiting speech processing approaches that can help to monitor the emotional state of patients via telephone or the Internet. However, the transmission, processing and subsequent storage of such sensitive data raises several privacy concerns. Speech de-identification can be used to protect the patients' identity. Nevertheless, these techniques modify the speech signal, eventually affecting the performance of depression detection approaches based on either speech characteristics or automatic transcriptions. This paper presents a study on the influence of speech de-identification when using transcription-based approaches for depression detection. To this effect, a system based on the global vectors method for natural language processing is proposed. In contrast to previous works, two main sources of nuisance have been considered: the de-identification process itself and the transcription errors introduced by the automatic recognition of the patients' speech. Experimental validation on the DAIC-WOZ corpus reveals very promising results, obtaining only a slight performance degradation with respect to the use of manual transcriptions",
    "checked": true,
    "id": "79c6cd9bde1150e59d935bb6d285fa38254f12c9",
    "semantic_title": "depression detection using automatic transcriptions of de-identified speech",
    "citation_count": 16,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wankerl17_interspeech.html": {
    "title": "An N-Gram Based Approach to the Automatic Diagnosis of Alzheimer's Disease from Spoken Language",
    "volume": "main",
    "abstract": "Alzheimer's disease (AD) is the most common cause of dementia and affects wide parts of the elderly population. Since there exists no cure for this illness, it is of particular interest to develop reliable and easy-to-use diagnostic methods to alleviate its effects. Speech can be a useful indicator to reach this goal. We propose a purely statistical approach towards the automatic diagnosis of AD which is solely based on n-gram models with subsequent evaluation of the perplexity and does not incorporate any further linguistic features. Hence, it works independently of a concrete language. We evaluate our approach on the DementiaBank which contains spontaneous speech of test subjects describing a picture. Using the Equal-Error-Rate as classification threshold, we achieve an accuracy of 77.1%. In addition to that, we studied the correlation between the calculated perplexities and the Mini-Mental State Examination (MMSE) scores of the test subjects. While there is little correlation for the healthy control group, a higher correlation could be found when considering the demented speakers. This makes it reasonable to conclude that our approach reveals some of the cognitive limitations of AD patients and can help to better diagnose the disease based on speech",
    "checked": true,
    "id": "c940a209c47594443c5f352890b37a8d3fa9525f",
    "semantic_title": "an n-gram based approach to the automatic diagnosis of alzheimer's disease from spoken language",
    "citation_count": 45,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mundnich17_interspeech.html": {
    "title": "Exploiting Intra-Annotator Rating Consistency Through Copeland's Method for Estimation of Ground Truth Labels in Couples' Therapy",
    "volume": "main",
    "abstract": "Behavioral and mental health research and its clinical applications widely rely on quantifying human behavioral expressions. This often requires human-derived behavioral annotations, which tend to be noisy, especially when the psychological objects of interest are latent and subjective in nature. This paper focuses on exploiting multiple human annotations toward improving reliability of the ensemble decision, by creating a ranking of the evaluated objects. To create this ranking, we employ an adapted version of Copeland's counting method, which results in robust inter-annotator rankings and agreement. We use a simple mapping between the ranked objects and the scale of evaluation, which preserves the original distribution of ratings, based on maximum likelihood estimation. We apply the algorithm to ratings that lack a ground truth. Therefore, we assess our algorithm in two ways: (1) by corrupting the annotations with different distributions of noise, and computing the inter-annotator agreement between the ensemble estimates derived from the original and corrupted data using Krippendorff's α; and (2) by replacing one annotator at a time with the ensemble estimate. Our results suggest that the proposed method provides a robust alternative that suffers less from individual annotator preferences/biases and scale misuse",
    "checked": true,
    "id": "d217508375e08c6ab23851361c238ea3c983ab40",
    "semantic_title": "exploiting intra-annotator rating consistency through copeland's method for estimation of ground truth labels in couples' therapy",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/pettorino17_interspeech.html": {
    "title": "Rhythmic Characteristics of Parkinsonian Speech: A Study on Mandarin and Polish",
    "volume": "main",
    "abstract": "Previous studies on Italian speech showed that the percentage of vocalic portion in the utterance (%V) and the duration of the interval between two consecutive vowel onset points (VtoV) were larger for parkinsonian (PD) than for healthy controls (HC). Especially, the values of %V were distinctly separated between PD and HC. The present study aimed to further test the finding on Mandarin and Polish. Twenty-five Mandarin speakers (13 PD and 12 HC matched on age) and thirty-one Polish speakers (18 PD and 13 HC matched on age) read aloud a passage of story. The recorded speeches were segmented into vocalic and consonantal intervals, and then %V and VtoV were calculated. For both languages, VtoV overlapped between HC and PD. For Polish, %V was distinctly higher in PD than in HC, while for Mandarin there was no significant difference. It suggests that %V could be used for automatic diagnosis of PD for Italian and Polish, but not for Mandarin. The effectiveness of the rhythmic metric appears to be language-dependent, varying with the rhythmic typology of the language",
    "checked": true,
    "id": "008a7adf743f2e4fe25cab5ededba0747cef08be",
    "semantic_title": "rhythmic characteristics of parkinsonian speech: a study on mandarin and polish",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tu17c_interspeech.html": {
    "title": "Trisyllabic Tone 3 Sandhi Patterns in Mandarin Produced by Cantonese Speakers",
    "volume": "main",
    "abstract": "The third tone sandhi in Mandarin is a well-studied rule, where a Tone 3 followed by another Tone 3 is changed as a rising tone, similar to Tone 2. This Tone 3 sandhi rule is straightforward in disyllabic words, which is phonetically driven for the ease of production. In three or more than three syllables with Tone 3, however, the Tone 3 sandhi application is more complicated and involves both the prosodic and morph-syntactic domains, which makes it difficult for L2 learners. This study aims to understand how L2 learners with another tone language experience could master the Mandarin Tone 3 sandhi rule. Specifically, the study investigates the production of Tone 3 sandhi in trisyllabic Mandarin words by Cantonese speakers. In the current study, 30 Cantonese speakers were requested to produce 15 trisyllabic words (\"1+[2+3]\" and \"[1+2]+3\" sandhi patterns) and 5 hexasyllabic sentences with Tone 3 in sequences. The analyses of results center on three major types of error patterns: overgeneralization, under application, and combination. The findings are discussed with regard to the phono-syntactic interactions of Tone 3 sandhi at the lexical and phrasal levels as well as the influence of the Cantonese tonal system",
    "checked": true,
    "id": "8dea5bfe979d2cf1cde4ef354ccd784274f13379",
    "semantic_title": "trisyllabic tone 3 sandhi patterns in mandarin produced by cantonese speakers",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sahkai17_interspeech.html": {
    "title": "Intonation of Contrastive Topic in Estonian",
    "volume": "main",
    "abstract": "Contrastive topic is an information structural category that is usually associated with a specific intonation, which tends to be similar across languages (a rising pitch accent). The aim of the present study is to examine whether this also true of Estonian. Three potential prosodic correlates of contrastive topics are examined: marking with a particular pitch accent type, an emphatic realization of the pitch accent, and a following prosodic boundary. With respect to pitch accent types, it is found that only two subjects out of eight distinguish sentences with a contrastive topic from other types of information structure; the contour bears resemblance to contrastive topic intonation in other languages (consisting of an H* accent on the contrastive topic and an HL* accent on the focus), but is not restricted to sentences with contrastive topics. A more consistent correlate turns out to be an emphatic realization of the pitch accent carried by the contrastive topic constituent. No evidence is found of a tendency to produce contrastive topics as separate prosodic phrases",
    "checked": true,
    "id": "53658add051825f4b42fcac2688538bad3d80d03",
    "semantic_title": "intonation of contrastive topic in estonian",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hao17_interspeech.html": {
    "title": "Reanalyze Fundamental Frequency Peak Delay in Mandarin",
    "volume": "main",
    "abstract": "In Mandarin, Fundamental Frequency (F0) peak delay has been reported to occur frequently in the rising (R) tone or high (H) tone succeeding by a low (L) tone. Its occurrence was ascribed to articulatory constraints within a conflicting tonal context: a high offset target followed by a low onset target. To further examine the underlying mechanism of the phenomenon, the current study tests the possibility that valley delay, as opposed to peak delay, may occur in an L+H tonal context; and peak or valley delay may also occur within a compatible tonal context where adjacent tonal values are identical or similar. An experiment was done on Annotated Speech Corpus of Chinese Discourse to investigate the frequency of occurrence and amount of peak and valley delay. The results indicated that: F0 peak and valley delay frequently occurred in both conflicting and compatible tonal contexts; the phenomenon was found extensively in R tone and F (falling) tone, but barely in H tone and L tone. The findings suggest that while peak or valley delay is partially due to articulatory constraints in certain tonal contexts, the speakers' active effort-distribution strategy based on economical principle is also behind the phenomenon",
    "checked": true,
    "id": "190a6d1429d147b70a9869c96baa56e6940c4c45",
    "semantic_title": "reanalyze fundamental frequency peak delay in mandarin",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/michelas17_interspeech.html": {
    "title": "How Does the Absence of Shared Knowledge Between Interlocutors Affect the Production of French Prosodic Forms?",
    "volume": "main",
    "abstract": "We examine the hypothesis that modelling the addressee in spoken interaction affects the production of prosodic forms by the speaker. This question was tested in an interactive paradigm that enabled us to measure prosodic variations at two levels: the global/acoustic level and the phonological one. We used a semi-spontaneous task in which French speakers gave instructions to addressees about where to place a cross between different objects (e.g., Tu mets la croix entre la souris bordeau et la maison bordeau; ‘You put the cross between the red mouse and the red house'). Each trial was composed of two noun-adjective fragments and the target was the second fragment. We manipulated (i) whether the two interlocutors shared or didn't share the same objects and (ii) the informational status of targets to obtain variations in abstract prosodic phrasing. We found that the absence of shared knowledge between interlocutors affected the speaker's production of prosodic forms at the global/acoustic level (i.e., pitch range and speech rate) but not at the phonological one (i.e., prosodic phrasing). These results are consistent with a mechanism in which global prosodic variations are influenced by audience design because they reflect the way that speakers help addressees to understand speech",
    "checked": true,
    "id": "68eaa10db2b8f4cdc9ce939ea11c0d16600ae745",
    "semantic_title": "how does the absence of shared knowledge between interlocutors affect the production of french prosodic forms?",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wagner17_interspeech.html": {
    "title": "Three Dimensions of Sentence Prosody and Their (Non-)Interactions",
    "volume": "main",
    "abstract": "Prosody simultaneously encodes different kinds of information, including the type of speech act of an utterance (e.g., falling declarative vs. rising interrogative intonational tunes), the location of semantic focus (via prosodic prominence), and syntactic constituent structure (via prosodic phrasing). The syntactic/ semantic functional dimensions (speech act, focus, constituency) are orthogonal to each other, but to which extent their prosodic correlates (tune, prominence, phrasing) are remains controversial. This paper takes a ‘bottom up' approach to test for interactions, and reports evidence that contrary to many current theories of sentence intonation, the cues to the three dimensions are often orthogonal where interactions are predicted",
    "checked": true,
    "id": "aa25f81d57515c3ca8e651765272c4c0a3a042f7",
    "semantic_title": "three dimensions of sentence prosody and their (non-)interactions",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kleinhans17_interspeech.html": {
    "title": "Using Prosody to Classify Discourse Relations",
    "volume": "main",
    "abstract": "This work aims to explore the correlation between the discourse structure of a spoken monologue and its prosody by predicting discourse relations from different prosodic attributes. For this purpose, a corpus of semi-spontaneous monologues in English has been automatically annotated according to the Rhetorical Structure Theory, which models coherence in text via rhetorical relations. From corresponding audio files, prosodic features such as pitch, intensity, and speech rate have been extracted from different contexts of a relation. Supervised classification tasks using Support Vector Machines have been performed to find relationships between prosodic features and rhetorical relations. Preliminary results show that intensity combined with other features extracted from intra- and intersegmental environments is the feature with the highest predictability for a discourse relation. The prediction of rhetorical relations from prosodic features and their combinations is straightforwardly applicable to several tasks such as speech understanding or generation. Moreover, the knowledge of how rhetorical relations should be marked in terms of prosody will serve as a basis to improve speech synthesis applications and make voices sound more natural and expressive",
    "checked": true,
    "id": "d07d8bcc851e67d386f7de63aa8a56918b20ea1f",
    "semantic_title": "using prosody to classify discourse relations",
    "citation_count": 13,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/godoy17_interspeech.html": {
    "title": "Canonical Correlation Analysis and Prediction of Perceived Rhythmic Prominences and Pitch Tones in Speech",
    "volume": "main",
    "abstract": "Speech prosody encodes information about language and communicative intent as well as speaker identity and state. Consequently, a host of speech technologies could benefit from increased understanding of prosodic phenomena and corresponding acoustics. A recently developed comprehensive prosodic transcription system called RaP (Rhythm-and-Pitch) annotates both perceived rhythmic prominences and pitch tones in speech. Using RaP-annotated speech corpora, the present work analyzes relationships between perceived prosodic events and acoustic features including syllable duration and novel measures of intensity and fundamental frequency. Canonical Correlation Analysis (CCA) reveals two dominant prosodic dimensions relating the acoustic features and RaP annotations. The first captures perceived prosodic emphasis of syllables indicated by strong metrical beats and significant pitch variability (i.e. presence of either high or low pitch tones). Acoustically, this dimension is described most by syllable duration followed by the mean intensity and fundamental frequency measures. The second CCA dimension then primarily discriminates pitch tone level (high versus low), indicated mainly by the mean fundamental frequency measure. Finally, within a leave-one-out cross-validation framework, RaP prosodic events are well-predicted from acoustic features (AUC between 0.78 and 0.84). Future work will exploit automated RaP labelling in contexts ranging from language learning to neurological disorder recognition",
    "checked": true,
    "id": "998ff256232750b40b307549424932726f252ca1",
    "semantic_title": "canonical correlation analysis and prediction of perceived rhythmic prominences and pitch tones in speech",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kakouros17_interspeech.html": {
    "title": "Evaluation of Spectral Tilt Measures for Sentence Prominence Under Different Noise Conditions",
    "volume": "main",
    "abstract": "Spectral tilt has been suggested to be a correlate of prominence in speech, although several studies have not replicated this empirically. This may be partially due to the lack of a standard method for tilt estimation from speech, rendering interpretations and comparisons between studies difficult. In addition, little is known about the performance of tilt estimators for prominence detection in the presence of noise. In this work, we investigate and compare several standard tilt measures on quantifying prominence in spoken Dutch and under different levels of additive noise. We also compare these measures with other acoustic correlates of prominence, namely, energy, F0, and duration. Our results provide further empirical support for the finding that tilt is a systematic correlate of prominence, at least in Dutch, even though energy, F0, and duration appear still to be more robust features for the task. In addition, our results show that there are notable differences between different tilt estimators in their ability to discriminate prominent words from non-prominent ones in different levels of noise",
    "checked": true,
    "id": "940990db0e29777d03f9bd0169fc0fe0a123381f",
    "semantic_title": "evaluation of spectral tilt measures for sentence prominence under different noise conditions",
    "citation_count": 16,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kuang17_interspeech.html": {
    "title": "Creaky Voice as a Function of Tonal Categories and Prosodic Boundaries",
    "volume": "main",
    "abstract": "This study looks into the distribution of creaky voice in Mandarin in continuous speech. A creaky voice detector was used to automatically detect the appearance of creaky voice in a large-scale Mandarin corpus (Sinica COSPRO corpus). As the prosodic information has been annotated in the corpus, we were able to look at the distribution of creaky voice as a function of the interaction between tone and prosodic structures. As expected, among the five tonal categories (four lexical tones and one neutral tone), creaky voice is most likely to occur with Tone 3 and the neutral tone, followed by Tone 2 and Tone 4. Prosodic boundaries also play important roles, as the likelihood of creak increases when the prosodic boundaries are larger, regardless of the tonal categories. It is also confirmed that the pitch range for the occurrence of creaky voice is 110 Hz for male speakers and 170 Hz for female speakers, consistent with previous small-scale studies. Finally, male speakers have a higher overall rate of creaky voice than female speakers. Altogether, this study validates the hypotheses from previous studies, and provides a better understanding of voice-source variation in different prosodic conditions",
    "checked": true,
    "id": "79d0b177b77acca5aa10f4382bbcd71d7246a58b",
    "semantic_title": "creaky voice as a function of tonal categories and prosodic boundaries",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/skarnitzl17_interspeech.html": {
    "title": "The Acoustics of Word Stress in Czech as a Function of Speaking Style",
    "volume": "main",
    "abstract": "The study is part of a series of studies which examine the acoustic correlates of lexical stress in several typologically different languages, in three speech styles: spontaneous speech, phrase reading, and wordlist reading. This study focuses on Czech, a language with stress fixed on the first syllable of a prosodic word, with no contrastive function at the level of individual words. The acoustic parameters examined here are F0-level, F0-variation, Duration, Sound Pressure Level, and Spectral Emphasis. Values for over 6,000 vowels were analyzed Unlike the other languages examined so far, lexical stress in Czech is not manifested by clear prominence markings on the first, stressed syllable: the stressed syllable is neither higher, realized with greater F0 variation, longer; nor does it have a higher SPL or higher Spectral Emphasis. There are slight, but insignificant tendencies pointing to a delayed rise, that is, to higher values of some of the acoustic parameters on the second, post-stressed syllable. Since lexical stress does not serve a contrastive function in Czech, the absence of acoustic marking on the stressed syllable is not surprising",
    "checked": true,
    "id": "1cae543b65668711e316c75327d0b4722ffc48b5",
    "semantic_title": "the acoustics of word stress in czech as a function of speaking style",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wagner17b_interspeech.html": {
    "title": "What You See is What You Get Prosodically Less — Visibility Shapes Prosodic Prominence Production in Spontaneous Interaction",
    "volume": "main",
    "abstract": "We investigated the expression of prosodic prominence related to unpredictability and relevance in spontaneous dyadic interactions in which interlocutors could or could not see each other. Interactions between visibility and prominence were analyzed in a verbal version of the game TicTacToe. This setting allows for disentangling different types of information structure: early moves tend to be unpredictable, but are typically irrelevant for the immediate outcome of the game, while late moves tend to be predictable but relevant, as they usually prevent an opponent's winning move or constitute a winning move by themselves Our analyses on German reveal that prominence expression is affected globally by visibility conditions: speech becomes overall softer and faster when interlocutors can see each other. However, speakers differentiate unpredictability and relevance-related accents rather consistently using intensity cues both under visibility and invisibility conditions. We also find that pitch excursions related to prosodic information structure are not affected by visibility. Our findings support effort-optimization models of speech production, but also models that regard speech production as an integrated bimodal process with a high degree of congruency across domains",
    "checked": true,
    "id": "a50b17248029d67a2dee0bb110e86127ac2fb074",
    "semantic_title": "what you see is what you get prosodically less - visibility shapes prosodic prominence production in spontaneous interaction",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hsu17b_interspeech.html": {
    "title": "Focus Acoustics in Mandarin Nominals",
    "volume": "main",
    "abstract": "In addition to deciding what to say, interlocutors have to decide how to say it. One of the important tasks of linguists is then to model how differences in acoustic patterns influence the interpretation of a sentence. In light of previous studies on how prosodic structure convey discourse-level of information in a sentence, this study makes use of a speech production experiment to investigate how expressions related to different information packaging, such as information focus, corrective focus, and old information, are prosodically realized within a complex nominal. Special attention was paid to the sequence of \"numeral-classifier-noun\" in Mandarin, which consists of closely related sub-syntactic units internally, and provides a phonetically controlled environment comparable to previous phonetic studies on focus prominence at the sentential level. The result shows that a multi-dimensional strategy is used in focus-marking, and that focus prosody is sensitive to the size of focus domain and is observable in various lexical tonal environments in Mandarin",
    "checked": true,
    "id": "9ec11ebb495c39b7d57f1261fc3d9939b3d91921",
    "semantic_title": "focus acoustics in mandarin nominals",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lundmark17_interspeech.html": {
    "title": "Exploring Multidimensionality: Acoustic and Articulatory Correlates of Swedish Word Accents",
    "volume": "main",
    "abstract": "This study investigates acoustic and articulatory correlates of South Swedish word accents (Accent 1 vs. 2) — a tonal distinction traditionally associated with F0 timing. The study is motivated by previous findings on (i) the acoustic complexity of tonal prosody and (ii) tonal-articulatory interplay in other languages Acoustic and articulatory (EMA) data from two controlled experiments are reported (14 speakers in total; pilot EMA recordings with 2 speakers). Apart from the well-established F0 timing pattern, results of Experiment 1 reveal a longer duration of a post-stress consonant in Accent 2 than in Accent 1, a higher degree of creaky voice in Accent 1, as well as a deviant (two-peak) pitch pattern in Accent 2 for one of eight discourse conditions used in the experiment. Experiment 2 reveals an effect of word accent on vowel articulation, as the tongue body gesture target is reached earlier in Accent 2. It also suggests slight but (marginally) significant word-accent effects on word-initial gestural coordination, taking slightly different forms in the two speakers, as well as corresponding differences in word-initial formant patterns. Results are discussed concerning their potential perceptual relevance, as well as with reference to the c-center effect discussed within Articulatory Phonology",
    "checked": true,
    "id": "409885406587c77009154902a333ce6e408d2144",
    "semantic_title": "exploring multidimensionality: acoustic and articulatory correlates of swedish word accents",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/puga17_interspeech.html": {
    "title": "The Perception of English Intonation Patterns by German L2 Speakers of English",
    "volume": "main",
    "abstract": "Previous research suggests that intonation is a particularly challenging aspect of L2 speech learning. While most research focuses on speech production, we widen the focus and study the perception of intonation by L2 learners. We investigate whether advanced German learners of English have knowledge of the appropriate English intonation patterns in a narrative context with different sentence types (e.g. statements, questions). The results of a tonal pattern selection task indicate that learners (n=20) performed similar to British English controls (n=25) for some sentence types (e.g. statements, yes/no-questions), but performed significantly worse than the control group in the case of open and closed tag questions and the expression of sarcasm. The results can be explained by the fact that tag questions are the only sentence type investigated that does not exist in the learners' L1, and sarcasm is not represented syntactically. This suggests that L1 influence can partly account for why some intonation patterns are more challenging than others, and that contextualized knowledge of the intonation patterns of the target language rather than knowledge of intonation patterns in isolation is crucial for the successful L2 learning of intonation",
    "checked": true,
    "id": "428e36272800512ad5b462090c3c1d3f0ab51580",
    "semantic_title": "the perception of english intonation patterns by german l2 speakers of english",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/paradacabaleiro17_interspeech.html": {
    "title": "The Perception of Emotions in Noisified Nonsense Speech",
    "volume": "main",
    "abstract": "Noise pollution is part of our daily life, affecting millions of people, particularly those living in urban environments. Noise alters our perception and decreases our ability to understand others. Considering this, speech perception in background noise has been extensively studied, showing that especially white noise can damage listener perception. However, the perception of emotions in noisified speech has not been explored with as much depth. In the present study, we use artificial background noise conditions, by applying noise to a subset of the GEMEP corpus (emotions expressed in nonsense speech). Noises were at varying intensities and ‘colours'; white, pink, and brownian. The categorical and dimensional perceptual test was completed by 26 listeners. The results indicate that background noise conditions influence the perception of emotion in speech — pink noise most, brownian least. Worsened perception invokes higher confusion, especially with sadness, an emotion with less pronounced prosodic characteristics. Yet, all this does not lead to a break-down of the ‘cognitive-emotional space' in a Non-metric MultiDimensional Scaling representation. The gender of speakers and the cultural background of listeners do not seem to play a role",
    "checked": true,
    "id": "aa43a3720ac5716c8907695883836ffd605b1400",
    "semantic_title": "the perception of emotions in noisified nonsense speech",
    "citation_count": 15,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gibson17_interspeech.html": {
    "title": "Attention Networks for Modeling Behaviors in Addiction Counseling",
    "volume": "main",
    "abstract": "In psychotherapy interactions there are several desirable and undesirable behaviors that give insight into the efficacy of the counselor and the progress of the client. It is important to be able to identify when these target behaviors occur and what aspects of the interaction signal their occurrence. Manual observation and annotation of these behaviors is costly and time intensive. In this paper, we use long short term memory networks equipped with an attention mechanism to process transcripts of addiction counseling sessions and predict prominent counselor and client behaviors. We demonstrate that this approach gives competitive performance while also providing additional interpretability",
    "checked": true,
    "id": "e7cb1ffc76948344074630ce123a6ae3200ca8af",
    "semantic_title": "attention networks for modeling behaviors in addiction counseling",
    "citation_count": 20,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wortwein17_interspeech.html": {
    "title": "Computational Analysis of Acoustic Descriptors in Psychotic Patients",
    "volume": "main",
    "abstract": "Various forms of psychotic disorders, including schizophrenia, can influence how we speak. Therefore, clinicians assess speech and language behaviors of their patients. While it is difficult for humans to quantify speech behaviors precisely, acoustic descriptors, such as tenseness of voice and speech rate, can be quantified automatically. In this work, we identify previously unstudied acoustic descriptors related to the severity of psychotic symptoms within a clinical population (N=29). Our dataset consists of semi-structured interviews between patients and clinicians. Psychotic disorders are often characterized by two groups of symptoms: negative and positive. While negative symptoms are also prevalent in disorders such as depression, positive symptoms in psychotic disorders have rarely been studied from an acoustic and computational perspective. Our experiments show relationships between psychotic symptoms and acoustic descriptors related to voice quality consistency, variation of speech rate and volume, vowel space, and a parameter of glottal flow. Further, we show that certain acoustic descriptors can track a patient's state from admission to discharge. Finally, we demonstrate that measures from the Brief Psychiatric Rating Scale (BPRS) can be estimated with acoustic descriptors",
    "checked": true,
    "id": "3502ac020e251ad28b386cf0ac63fa54e03dff93",
    "semantic_title": "computational analysis of acoustic descriptors in psychotic patients",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wu17e_interspeech.html": {
    "title": "Modeling Perceivers Neural-Responses Using Lobe-Dependent Convolutional Neural Network to Improve Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Developing automatic emotion recognition by modeling expressive behaviors is becoming crucial in enabling the next generation design of human-machine interface. Also, with the availability of functional magnetic resonance imaging (fMRI), researchers have also conducted studies into quantitative understanding of vocal emotion perception mechanism. In this work, our aim is two folds: 1) investigating whether the neural-responses can be used to automatically decode the emotion labels of vocal stimuli, and 2) combining acoustic and fMRI features to improve the speech emotion recognition accuracies. We introduce a novel framework of lobe-dependent convolutional neural network (LD-CNN) to provide better modeling of perceivers neural-responses on vocal emotion. Furthermore, by fusing LD-CNN with acoustic features, we demonstrate an overall 63.17% accuracies in a four-class emotion recognition task (9.89% and 14.42% relative improvement compared to the acoustic-only and the fMRI-only features). Our analysis further shows that temporal lobe possess the most information in decoding emotion labels; the fMRI and the acoustic information are complementary to each other, where neural-responses and acoustic features are better at discriminating along the valence and activation dimensions, respectively",
    "checked": true,
    "id": "297a40c0f831b54b97028d45568f791c64052f24",
    "semantic_title": "modeling perceivers neural-responses using lobe-dependent convolutional neural network to improve speech emotion recognition",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/vlasenko17_interspeech.html": {
    "title": "Implementing Gender-Dependent Vowel-Level Analysis for Boosting Speech-Based Depression Recognition",
    "volume": "main",
    "abstract": "Whilst studies on emotion recognition show that gender-dependent analysis can improve emotion classification performance, the potential differences in the manifestation of depression between male and female speech have yet to be fully explored. This paper presents a qualitative analysis of phonetically aligned acoustic features to highlight differences in the manifestation of depression. Gender-dependent analysis with phonetically aligned gender-dependent features are used for speech-based depression recognition. The presented experimental study reveals gender differences in the effect of depression on vowel-level features. Considering the experimental study, we also show that a small set of knowledge-driven gender-dependent vowel-level features can outperform state-of-the-art turn-level acoustic features when performing a binary depressed speech recognition task. A combination of these preselected gender-dependent vowel-level features with turn-level standardised openSMILE features results in additional improvement for depression recognition",
    "checked": true,
    "id": "a6d225dd3762dc2254c24b5d17f021d12e7ce79f",
    "semantic_title": "implementing gender-dependent vowel-level analysis for boosting speech-based depression recognition",
    "citation_count": 21,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/siddique17_interspeech.html": {
    "title": "Bilingual Word Embeddings for Cross-Lingual Personality Recognition Using Convolutional Neural Nets",
    "volume": "main",
    "abstract": "We propose a multilingual personality classifier that uses text data from social media and Youtube Vlog transcriptions, and maps them into Big Five personality traits using a Convolutional Neural Network (CNN). We first train unsupervised bilingual word embeddings from an English-Chinese parallel corpus, and use these trained word representations as input to our CNN. This enables our model to yield relatively high cross-lingual and multilingual performance on Chinese texts, after training on the English dataset for example. We also train monolingual Chinese embeddings from a large Chinese text corpus and then train our CNN model on a Chinese dataset consisting of conversational dialogue labeled with personality. We achieve an average F-score of 66.1 in our multilingual task compared to 63.3 F-score in cross-lingual, and 63.2 F-score in the monolingual performance",
    "checked": true,
    "id": "b3775a38a09fb0a047d0fdfcea87669444a4f8cb",
    "semantic_title": "bilingual word embeddings for cross-lingual personality recognition using convolutional neural nets",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/arimoto17_interspeech.html": {
    "title": "Emotion Category Mapping to Emotional Space by Cross-Corpus Emotion Labeling",
    "volume": "main",
    "abstract": "The psychological classification of emotion has two main approaches. One is emotion category, in which emotions are classified into discrete and fundamental groups; the other is emotion dimension, in which emotions are characterized by multiple continuous scales. The cognitive classification of emotion by humans perceived from speech is not sufficiently established. Although there have been several studies on such classification, they did not discuss it deeply. Moreover, the relationship between emotion category and emotion dimension perceived from speech is not well studied. Aiming to establish common emotion labels for emotional speech, this study elucidated the relationship between the emotion category and the emotion dimension perceived by speech by conducting an experiment of cross-corpus emotion labeling with two different Japanese dialogue corpora (Online Gaming Voice Chat Corpus with Emotional Label (OGVC) and Utsunomiya University Spoken Dialogue Database for Paralinguistic Information Studies (UUDB)). A likelihood ratio test was conducted to assess the independency of one emotion category from the others in three-dimensional emotional space. This experiment revealed that many emotion categories exhibited independency from the other emotion categories. Only the neutral states did not exhibit independency from the three emotions of sadness, disgust, and surprise",
    "checked": true,
    "id": "7ed6d82e3544dc25f73c9657ba4cfd9356ac10e3",
    "semantic_title": "emotion category mapping to emotional space by cross-corpus emotion labeling",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/fayet17_interspeech.html": {
    "title": "Big Five vs. Prosodic Features as Cues to Detect Abnormality in SSPNET-Personality Corpus",
    "volume": "main",
    "abstract": "This paper presents an attempt to evaluate three different sets of features extracted from prosodic descriptors and Big Five traits for building an anomaly detector. The Big Five model enables to capture personality information. Big Five traits are extracted from a manual annotation while Prosodic features are extracted directly from the speech signal. Two different anomaly detection methods are evaluated: Gaussian Mixture Model (GMM) and One-Class SVM (OC-SVM), each one combined with a threshold classification to decide the \"normality\" of a sample. The different combinations of models and feature sets are evaluated on the SSPNET-Personality corpus which has already been used in several experiments, including a previous work on separating two types of personality profiles in a supervised way. In this work, we propose the above mentioned unsupervised or semi-supervised methods, and discuss their performance, to detect particular audio-clips produced by a speaker with an abnormal personality. Results show that using automatically extracted prosodic features competes with the Big Five traits. The overall detection performance achieved by the best model is around 0.8 (F1-measure)",
    "checked": true,
    "id": "94d06bc8cf46fa66573da04ebbed3ce69281b4e9",
    "semantic_title": "big five vs. prosodic features as cues to detect abnormality in sspnet-personality corpus",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/akira17_interspeech.html": {
    "title": "Speech Rate Comparison When Talking to a System and Talking to a Human: A Study from a Speech-to-Speech, Machine Translation Mediated Map Task",
    "volume": "main",
    "abstract": "This study focuses on the adaptation of subjects in Human-to-Human (H2H) communication in spontaneous dialogues in two different settings. The speech rate of sixteen dialogues from the HCRC Map Task corpus have been analyzed as direct H2H communication, while fifteen dialogues from the ILMT-s2s corpus have been analyzed as a Speech-to-Speech Machine Translation (S2S-MT) mediated H2H communication comparison. The analysis shows that while the mean speech rate of the subjects in the two task oriented corpora differ, in both corpora the role of the subject causes a significant difference in the speech rate with the Information Giver using a slower speech rate than the Information Follower. Also the different settings of the dialogue recordings (with or without eye contact in the HCRC corpus and with or without live video streaming in the ILMT-s2s corpus) only show a negligible difference in the speech rate. However, the gender of the subjects have provided an interesting difference with the female subjects of the ILMT-s2s corpus using a slower speech rate than the male subjects, gender does not show any difference in the HCRC corpus. This indicates that the difference is not from performing the map task, but a result of their adaptation strategy to the S2S-MT system",
    "checked": true,
    "id": "230315f2c5315e96531c13f0111380db28163455",
    "semantic_title": "speech rate comparison when talking to a system and talking to a human: a study from a speech-to-speech, machine translation mediated map task",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tseng17_interspeech.html": {
    "title": "Approaching Human Performance in Behavior Estimation in Couples Therapy Using Deep Sentence Embeddings",
    "volume": "main",
    "abstract": "Identifying complex behavior in human interactions for observational studies often involves the tedious process of transcribing and annotating large amounts of data. While there is significant work towards accurate transcription in Automatic Speech Recognition, automatic Natural Language Understanding of high-level human behaviors from the transcribed text is still at an early stage of development. In this paper we present a novel approach for modeling human behavior using sentence embeddings and propose an automatic behavior annotation framework. We explore unsupervised methods of extracting semantic information, using seq2seq models, into deep sentence embeddings and demonstrate that these embeddings capture behaviorally meaningful information. Our proposed framework utilizes LSTM Recurrent Neural Networks to estimate behavior trajectories from these sentence embeddings. Finally, we employ fusion to compare our high-resolution behavioral trajectories with the coarse, session-level behavioral ratings of human annotators in Couples Therapy. Our experiments show that behavior annotation using this framework achieves better results than prior methods and approaches or exceeds human performance in terms of annotator agreement",
    "checked": true,
    "id": "b3150dcdfd5bb2ecaf1cecea6e0d656411cf1142",
    "semantic_title": "approaching human performance in behavior estimation in couples therapy using deep sentence embeddings",
    "citation_count": 18,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/nasir17_interspeech.html": {
    "title": "Complexity in Speech and its Relation to Emotional Bond in Therapist-Patient Interactions During Suicide Risk Assessment Interviews",
    "volume": "main",
    "abstract": "In this paper, we analyze a 53-hour speech corpus of interactions of soldiers who had recently attempted suicide or had strong suicidal ideation conversing with their therapists. In particular, we study the complexity in therapist-patient speech as a marker of their emotional bond. Emotional bond is the extent to which the patient feels understood by and connected to the therapist. First, we extract speech features from audio recordings of their interactions. Then, we consider the nonlinear time series representation of those features and compute complexity measures based on the Lyapunov coefficient and correlation dimension. For the majority of the subjects, we observe that speech complexity in therapist-patient pairs is higher for the interview sessions, when compared to that of the rest of their interactions (intervention and post-interview follow-up). This indicates that entrainment (adapting to each other's speech) between the patient and the therapist is lower during the interview than regular interactions. This observation is consistent with prior studies in clinical psychology, considering that assessment interviews typically involve the therapist asking routine questions to enquire about the patient's suicidal thoughts and feelings. In addition, we find that complexity is negatively correlated with the patient's perceived emotional bond with the therapist",
    "checked": true,
    "id": "699e81867f4c5035f5d7f1a480c2a51ec8fc5199",
    "semantic_title": "complexity in speech and its relation to emotional bond in therapist-patient interactions during suicide risk assessment interviews",
    "citation_count": 13,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/huang17e_interspeech.html": {
    "title": "An Investigation of Emotion Dynamics and Kalman Filtering for Speech-Based Emotion Prediction",
    "volume": "main",
    "abstract": "Despite recent interest in continuous prediction of dimensional emotions, the dynamical aspect of emotions has received less attention in automated systems. This paper investigates how emotion change can be effectively incorporated to improve continuous prediction of arousal and valence from speech. Significant correlations were found between emotion ratings and their dynamics during investigations on the RECOLA database, and here we examine how to best exploit them using a Kalman filter. In particular, we investigate the correlation between predicted arousal and valence dynamics with arousal and valence ground truth; the Kalman filter internal delay for estimating the state transition matrix; the use of emotion dynamics as a measurement input to a Kalman filter; and how multiple probabilistic Kalman filter outputs can be effectively fused. Evaluation results show that correct dynamics estimation and internal delay settings allow up to 5% and 58% relative improvement in arousal and valence prediction respectively over existing Kalman filter implementations. Fusion based on probabilistic Kalman filter outputs yields further gains",
    "checked": true,
    "id": "b69dc1a42b33bb7c4e3967af9882cec9d8840856",
    "semantic_title": "an investigation of emotion dynamics and kalman filtering for speech-based emotion prediction",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sadamitsu17_interspeech.html": {
    "title": "Zero-Shot Learning for Natural Language Understanding Using Domain-Independent Sequential Structure and Question Types",
    "volume": "main",
    "abstract": "Natural language understanding (NLU) is an important module of spoken dialogue systems. One of the difficulties when it comes to adapting NLU to new domains is the high cost of constructing new training data for each domain. To reduce this cost, we propose a zero-shot learning of NLU that takes into account the sequential structures of sentences together with general question types across different domains. Experimental results show that our methods achieve higher accuracy than baseline methods in two completely different domains (insurance and sightseeing)",
    "checked": true,
    "id": "b8aa20b9fd9e2b68c7a579c8d59f5d22de6dcf25",
    "semantic_title": "zero-shot learning for natural language understanding using domain-independent sequential structure and question types",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sawada17_interspeech.html": {
    "title": "Parallel Hierarchical Attention Networks with Shared Memory Reader for Multi-Stream Conversational Document Classification",
    "volume": "main",
    "abstract": "This paper describes a novel classification method for multi-stream conversational documents. Documents of contact center dialogues or meetings are often composed of multiple source documents that are transcriptions of the recordings of each speaker's channel. To enhance the classification performance of such multi-stream conversational documents, three main advances over the previous method are introduced. The first is a parallel hierarchical attention network (PHAN) for multi-stream conversational document modeling. PHAN can precisely capture word and sentence structures of individual source documents and efficiently integrate them. The second is a shared memory reader that can yield a shared attention mechanism. The shared memory reader highlights common important information in a conversation. Our experiments on a call category classification in contact center dialogues show that PHAN together with the shared memory reader outperforms the single document modeling method and previous multi-stream document modeling method",
    "checked": true,
    "id": "b34455b4f6ed3be97545f64b0c76a4343031a052",
    "semantic_title": "parallel hierarchical attention networks with shared memory reader for multi-stream conversational document classification",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/morchid17_interspeech.html": {
    "title": "Internal Memory Gate for Recurrent Neural Networks with Application to Spoken Language Understanding",
    "volume": "main",
    "abstract": "Long Short-Term Memory (LSTM) Recurrent Neural Networks (RNN) require 4 gates to learn short- and long-term dependencies for a given sequence of basic elements. Recently, \"Gated Recurrent Unit\" (GRU) has been introduced and requires fewer gates than LSTM (reset and update gates), to code short- and long-term dependencies and reaches equivalent performances to LSTM, with less processing time during the learning. The \"Leaky integration Unit\" (LU) is a GRU with a single gate (update) that codes mostly long-term dependencies quicker than LSTM or GRU (small number of operations for learning). This paper proposes a novel RNN that takes advantage of LSTM, GRU (short- and long-term dependencies) and the LU (fast learning) called \"Internal Memory Gate\" (IMG). The effectiveness and the robustness of the proposed IMG-RNN is evaluated during a classification task of a small corpus of spoken dialogues from the DECODA project that allows us to evaluate the capability of each RNN to code short-term dependencies. The experiments show that IMG-RNNs reach better accuracies with a gain of 0.4 points compared to LSTM- and GRU-RNNs and 0.7 points compared to the LU-RNN. Moreover, IMG-RNN requires less processing time than GRU or LSTM with a gain of 19% and 50% respectively",
    "checked": true,
    "id": "6108ca73e2bbb5952bb4d0a379c50094cb121b23",
    "semantic_title": "internal memory gate for recurrent neural networks with application to spoken language understanding",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/korpusik17_interspeech.html": {
    "title": "Character-Based Embedding Models and Reranking Strategies for Understanding Natural Language Meal Descriptions",
    "volume": "main",
    "abstract": "Character-based embedding models provide robustness for handling misspellings and typos in natural language. In this paper, we explore convolutional neural network based embedding models for handling out-of-vocabulary words in a meal description food ranking task. We demonstrate that character-based models combined with a standard word-based model improves the top-5 recall of USDA database food items from 26.3% to 30.3% on a test set of all USDA foods with typos simulated in 10% of the data. We also propose a new reranking strategy for predicting the top USDA food matches given a meal description, which significantly outperforms our prior method of n-best decoding with a finite state transducer, improving the top-5 recall on the all USDA foods task from 20.7% to 63.8%",
    "checked": true,
    "id": "bcd54081150ce8f72ab194ce06f37695dee6e419",
    "semantic_title": "character-based embedding models and reranking strategies for understanding natural language meal descriptions",
    "citation_count": 12,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/parcollet17_interspeech.html": {
    "title": "Quaternion Denoising Encoder-Decoder for Theme Identification of Telephone Conversations",
    "volume": "main",
    "abstract": "In the last decades, encoder-decoders or autoencoders (AE) have received a great interest from researchers due to their capability to construct robust representations of documents in a low dimensional subspace. Nonetheless, autoencoders reveal little in way of spoken document internal structure by only considering words or topics contained in the document as an isolate basic element, and tend to overfit with small corpus of documents. Therefore, Quaternion Multi-layer Perceptrons (QMLP) have been introduced to capture such internal latent dependencies, whereas denoising autoencoders (DAE) are composed with different stochastic noises to better process small set of documents. This paper presents a novel autoencoder based on both hitherto-proposed DAE (to manage small corpus) and the QMLP (to consider internal latent structures) called \"Quaternion denoising encoder-decoder\" (QDAE). Moreover, the paper defines an original angular Gaussian noise adapted to the specificity of hyper-complex algebra. The experiments, conduced on a theme identification task of spoken dialogues from the DECODA framework, show that the QDAE obtains the promising gains of 3% and 1.5% compared to the standard real valued denoising autoencoder and the QMLP respectively",
    "checked": true,
    "id": "bc620a47312b0c016633a9865255d8b2e6c9647e",
    "semantic_title": "quaternion denoising encoder-decoder for theme identification of telephone conversations",
    "citation_count": 18,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/simonnet17_interspeech.html": {
    "title": "ASR Error Management for Improving Spoken Language Understanding",
    "volume": "main",
    "abstract": "This paper addresses the problem of automatic speech recognition (ASR) error detection and their use for improving spoken language understanding (SLU) systems. In this study, the SLU task consists in automatically extracting, from ASR transcriptions, semantic concepts and concept/values pairs in a e.g touristic information system. An approach is proposed for enriching the set of semantic labels with error specific labels and by using a recently proposed neural approach based on word embeddings to compute well calibrated ASR confidence measures. Experimental results are reported showing that it is possible to decrease significantly the Concept/Value Error Rate with a state of the art system, outperforming previously published results performance on the same experimental data. It also shown that combining an SLU approach based on conditional random fields with a neural encoder/decoder attention based architecture, it is possible to effectively identifying confidence islands and uncertain semantic output segments useful for deciding appropriate error handling actions by the dialogue manager strategy",
    "checked": true,
    "id": "c3421879409ce6997fb6b016e865d7faac347bfc",
    "semantic_title": "asr error management for improving spoken language understanding",
    "citation_count": 33,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ma17e_interspeech.html": {
    "title": "Jointly Trained Sequential Labeling and Classification by Sparse Attention Neural Networks",
    "volume": "main",
    "abstract": "Sentence-level classification and sequential labeling are two fundamental tasks in language understanding. While these two tasks are usually modeled separately, in reality, they are often correlated, for example in intent classification and slot filling, or in topic classification and named-entity recognition. In order to utilize the potential benefits from their correlations, we propose a jointly trained model for learning the two tasks simultaneously via Long Short-Term Memory (LSTM) networks. This model predicts the sentence-level category and the word-level label sequence from the stepwise output hidden representations of LSTM. We also introduce a novel mechanism of \"sparse attention\" to weigh words differently based on their semantic relevance to sentence-level classification. The proposed method outperforms baseline models on ATIS and TREC datasets",
    "checked": true,
    "id": "8cc515fc057a7e8422cbba88ffffaf07f06a1fa4",
    "semantic_title": "jointly trained sequential labeling and classification by sparse attention neural networks",
    "citation_count": 15,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/nayak17_interspeech.html": {
    "title": "To Plan or not to Plan? Discourse Planning in Slot-Value Informed Sequence to Sequence Models for Language Generation",
    "volume": "main",
    "abstract": "Natural language generation for task-oriented dialogue systems aims to effectively realize system dialogue actions. All natural language generators (NLGs) must realize grammatical, natural and appropriate output, but in addition, generators for task-oriented dialogue must faithfully perform a specific dialogue act that conveys specific semantic information, as dictated by the dialogue policy of the system dialogue manager. Most previous work on deep learning methods for task-oriented NLG assumes that generation output can be an utterance skeleton. Utterances are delexicalized, with variable names for slots, which are then replaced with actual values as part of post-processing. However, the value of slots do, in fact, influence the lexical selection in the surrounding context as well as the overall sentence plan. To model this effect, we investigate sequence-to-sequence (seq2seq) models in which slot values are included as part of the input sequence and the output surface form. Furthermore, we study whether a separate sentence planning module that decides on grouping of slot value mentions as input to the seq2seq model results in more natural sentences than a seq2seq model that aims to jointly learn the plan and the surface realization",
    "checked": true,
    "id": "a18335e5b95bd42be728a048b96ffd9cae7bd87d",
    "semantic_title": "to plan or not to plan? discourse planning in slot-value informed sequence to sequence models for language generation",
    "citation_count": 49,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/riou17_interspeech.html": {
    "title": "Online Adaptation of an Attention-Based Neural Network for Natural Language Generation",
    "volume": "main",
    "abstract": "Following some recent propositions to handle natural language generation in spoken dialog systems with long short-term memory recurrent neural network models [1] we first investigate a variant thereof with the objective of a better integration of the attention subnetwork. Then our main objective is to propose and evaluate a framework to adapt the NLG module online through direct interactions with the users. When doing so the basic way is to ask the user to utter an alternative sentence to express a particular dialog act. But then the system has to decide between using an automatic transcription or to ask for a manual transcription. To do so a reinforcement learning approach based on an adversarial bandit scheme is retained. We show that by defining appropriately the rewards as a linear combination of expected payoffs and costs of acquiring the new data provided by the user, a system design can balance between improving the system's performance towards a better match with the user's preferences and the burden associated with it",
    "checked": true,
    "id": "0aba418754ff7753ab30c98844fb653585cb859b",
    "semantic_title": "online adaptation of an attention-based neural network for natural language generation",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/martinezhinarejos17_interspeech.html": {
    "title": "Spanish Sign Language Recognition with Different Topology Hidden Markov Models",
    "volume": "main",
    "abstract": "Natural language recognition techniques can be applied not only to speech signals, but to other signals that represent natural language units (e.g., words and sentences). This is the case of sign language recognition, which is usually employed by deaf people to communicate. The use of recognition techniques may allow this language users to communicate more independently with non-signal users. Several works have been done for different variants of sign languages, but in most cases their vocabulary is quite limited and they only recognise gestures corresponding to isolated words. In this work, we propose gesture recognisers which make use of typical Continuous Density Hidden Markov Model. They solve not only the isolated word problem, but also the recognition of basic sentences using the Spanish Sign Language with a higher vocabulary than in other approximations. Different topologies and Gaussian mixtures are studied. Results show that our proposal provides promising results that are the first step to obtain a general automatic recognition of Spanish Sign Language",
    "checked": true,
    "id": "871c38984be2094e0f5a1111c004e72cdd7cfaf7",
    "semantic_title": "spanish sign language recognition with different topology hidden markov models",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/morales17_interspeech.html": {
    "title": "OpenMM: An Open-Source Multimodal Feature Extraction Tool",
    "volume": "main",
    "abstract": "The primary use of speech is in face-to-face interactions and situational context and human behavior therefore intrinsically shape and affect communication. In order to usefully model situational awareness, machines must have access to the same streams of information humans have access to. In other words, we need to provide machines with features that represent each communicative modality: face and gesture, voice and speech, and language. This paper presents OpenMM: an open-source multimodal feature extraction tool. We build upon existing open-source repositories to present the first publicly available tool for multimodal feature extraction. The tool provides a pipeline for researchers to easily extract visual and acoustic features. In addition, the tool also performs automatic speech recognition (ASR) and then uses the transcripts to extract linguistic features. We evaluate the OpenMM's multimodal feature set on deception, depression and sentiment classification tasks and show its performance is very promising. This tool provides researchers with a simple way of extracting multimodal features and consequently a richer and more robust feature representation for machine learning tasks",
    "checked": true,
    "id": "b99f136402d006967f834c469770e90e912ac706",
    "semantic_title": "openmm: an open-source multimodal feature extraction tool",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/huang17f_interspeech.html": {
    "title": "Speaker Dependency Analysis, Audiovisual Fusion Cues and a Multimodal BLSTM for Conversational Engagement Recognition",
    "volume": "main",
    "abstract": "Conversational engagement is a multimodal phenomenon and an essential cue to assess both human-human and human-robot communication. Speaker-dependent and speaker-independent scenarios were addressed in our engagement study. Handcrafted audio-visual features were used. Fixed window sizes for feature fusion method were analysed. Novel dynamic window size selection and multimodal bi-directional long short term memory (Multimodal BLSTM) approaches were proposed and evaluated for engagement level recognition",
    "checked": true,
    "id": "c05cb3266e8f0e682db396a9c7aa9d308cbe0ffc",
    "semantic_title": "speaker dependency analysis, audiovisual fusion cues and a multimodal blstm for conversational engagement recognition",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hsu17c_interspeech.html": {
    "title": "Voice Conversion from Unaligned Corpora Using Variational Autoencoding Wasserstein Generative Adversarial Networks",
    "volume": "main",
    "abstract": "Building a voice conversion (VC) system from non-parallel speech corpora is challenging but highly valuable in real application scenarios. In most situations, the source and the target speakers do not repeat the same texts or they may even speak different languages. In this case, one possible, although indirect, solution is to build a generative model for speech. Generative models focus on explaining the observations with latent variables instead of learning a pairwise transformation function, thereby bypassing the requirement of speech frame alignment. In this paper, we propose a non-parallel VC framework with a variational autoencoding Wasserstein generative adversarial network (VAW-GAN) that explicitly considers a VC objective when building the speech model. Experimental results corroborate the capability of our framework for building a VC system from unaligned data, and demonstrate improved conversion quality",
    "checked": true,
    "id": "2c9139b546c5403def3bd75e437b84e4f541727e",
    "semantic_title": "voice conversion from unaligned corpora using variational autoencoding wasserstein generative adversarial networks",
    "citation_count": 298,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/nakashika17_interspeech.html": {
    "title": "CAB: An Energy-Based Speaker Clustering Model for Rapid Adaptation in Non-Parallel Voice Conversion",
    "volume": "main",
    "abstract": "In this paper, a new energy-based probabilistic model, called CAB (Cluster Adaptive restricted Boltzmann machine), is proposed for voice conversion (VC) that does not require parallel data during the training and requires only a small amount of speech data during the adaptation. Most of the existing VC methods require parallel data for training. Recently, VC methods that do not require parallel data (called non-parallel VCs) have been also proposed and are attracting much attention because they do not require prepared or recorded parallel speech data, unlike conventional approaches. The proposed CAB model is aimed at statistical non-parallel VC based on cluster adaptive training (CAT). This extends the VC method used in our previous model, ARBM (adaptive restricted Boltzmann machine). The ARBM approach assumes that any speech signals can be decomposed into speaker-invariant phonetic information and speaker-identity information using the ARBM adaptation matrices of each speaker. VC is achieved by switching the source speaker's identity into those of the target speaker while retaining the phonetic information obtained by decomposition of the source speaker's speech. In contrast, CAB speaker identities are represented as cluster vectors that determine the adaptation matrices. As the number of clusters is generally smaller than the number of speakers, the number of model parameters can be reduced compared to ARBM, which enables rapid adaptation of a new speaker. Our experimental results show that the proposed method especially performed better than the ARBM approach, particularly in adaptation",
    "checked": true,
    "id": "d94dae8b892c953cd94ec42a1b83f9ae34712ff1",
    "semantic_title": "cab: an energy-based speaker clustering model for rapid adaptation in non-parallel voice conversion",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/aihara17_interspeech.html": {
    "title": "Phoneme-Discriminative Features for Dysarthric Speech Conversion",
    "volume": "main",
    "abstract": "We present in this paper a Voice Conversion (VC) method for a person with dysarthria resulting from athetoid cerebral palsy. VC is being widely researched in the field of speech processing because of increased interest in using such processing in applications such as personalized Text-To-Speech systems. A Gaussian Mixture Model (GMM)-based VC method has been widely researched and Partial Least Square (PLS)-based VC has been proposed to prevent the over-fitting problems associated with the GMM-based VC method. In this paper, we present phoneme-discriminative features, which are associated with PLS-based VC. Conventional VC methods do not consider the phonetic structure of spectral features although phonetic structures are important for speech analysis. Especially for dysarthric speech, their phonetic structures are difficult to discriminate and discriminative learning will improve the conversion accuracy. This paper employs discriminative manifold learning. Spectral features are projected into a subspace in which a near point with the same phoneme label is close to another and a near point with a different phoneme label is apart. Our proposed method was evaluated on dysarthric speaker conversion task which converts dysarthric voice into non-dysarthric speech",
    "checked": true,
    "id": "fa1179a8b8bbc34f0d20f0734c3e883b2bb8f2d9",
    "semantic_title": "phoneme-discriminative features for dysarthric speech conversion",
    "citation_count": 20,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wu17f_interspeech.html": {
    "title": "Denoising Recurrent Neural Network for Deep Bidirectional LSTM Based Voice Conversion",
    "volume": "main",
    "abstract": "The paper studies the post processing in deep bidirectional Long Short-Term Memory (DBLSTM) based voice conversion, where the statistical parameters are optimized to generate speech that exhibits similar properties to target speech. However, there always exists residual error between converted speech and target one. We reformulate the residual error problem as speech restoration, which aims to recover the target speech samples from the converted ones. Specifically, we propose a denoising recurrent neural network (DeRNN) by introducing regularization during training to shape the distribution of the converted data in latent space. We compare the proposed approach with global variance (GV), modulation spectrum (MS) and recurrent neural network (RNN) based postfilters, which serve a similar purpose. The subjective test results show that the proposed approach significantly outperforms these conventional approaches in terms of quality and similarity",
    "checked": true,
    "id": "7f69a87287286a577e408511afd170e7445ea35f",
    "semantic_title": "denoising recurrent neural network for deep bidirectional lstm based voice conversion",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tanaka17b_interspeech.html": {
    "title": "Speaker Dependent Approach for Enhancing a Glossectomy Patient's Speech via GMM-Based Voice Conversion",
    "volume": "main",
    "abstract": "In this paper, using GMM-based voice conversion algorithm, we propose to generate speaker-dependent mapping functions to improve the intelligibility of speech uttered by patients with a wide glossectomy. The speaker-dependent approach enables to generate the mapping functions that reconstruct missing spectrum features of speech uttered by a patient without having influences of a speaker's factor. The proposed idea is simple, i.e., to collect speech uttered by a patient before and after the glossectomy, but in practice it is hard to ask patients to utter speech just for developing algorithms. To confirm the performance of the proposed approach, in this paper, in order to simulate glossectomy patients, we fabricated an intraoral appliance which covers lower dental arch and tongue surface to restrain tongue movements. In terms of the Mel-frequency cepstrum (MFC) distance, by applying the voice conversion, the distances were reduced by 25% and 42% for speaker-dependent case and speaker-independent case, respectively. In terms of phoneme intelligibility, dictation tests revealed that speech reconstructed by speaker-dependent approach almost always showed better performance than the original speech uttered by simulated patients, while speaker-independent approach did not",
    "checked": true,
    "id": "47eefdfa9fd489fb9463cf51c36623f1221964d4",
    "semantic_title": "speaker dependent approach for enhancing a glossectomy patient's speech via gmm-based voice conversion",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kaneko17c_interspeech.html": {
    "title": "Generative Adversarial Network-Based Postfilter for STFT Spectrograms",
    "volume": "main",
    "abstract": "We propose a learning-based postfilter to reconstruct the high-fidelity spectral texture in short-term Fourier transform (STFT) spectrograms. In speech-processing systems, such as speech synthesis, conversion, enhancement, separation, and coding, STFT spectrograms have been widely used as key acoustic representations. In these tasks, we normally need to precisely generate or predict the representations from inputs; however, generated spectra typically lack the fine structures that are close to those of the true data. To overcome these limitations and reconstruct spectra having finer structures, we propose a generative adversarial network (GAN)-based postfilter that is implicitly optimized to match the true feature distribution in adversarial learning. The challenge with this postfilter is that a GAN cannot be easily trained for very high-dimensional data such as STFT spectra. We take a simple divide-and-concatenate strategy. Namely, we first divide the spectrograms into multiple frequency bands with overlap, reconstruct the individual bands using the GAN-based postfilter trained for each band, and finally connect the bands with overlap. We tested our proposed postfilter on a deep neural network-based text-to-speech task and confirmed that it was able to reduce the gap between synthesized and target spectra, even in the high-dimensional STFT domain",
    "checked": true,
    "id": "40be84525faa6495a9e0e3eae79d5104ff56a65d",
    "semantic_title": "generative adversarial network-based postfilter for stft spectrograms",
    "citation_count": 62,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bollepalli17_interspeech.html": {
    "title": "Generative Adversarial Network-Based Glottal Waveform Model for Statistical Parametric Speech Synthesis",
    "volume": "main",
    "abstract": "Recent studies have shown that text-to-speech synthesis quality can be improved by using glottal vocoding. This refers to vocoders that parameterize speech into two parts, the glottal excitation and vocal tract, that occur in the human speech production apparatus. Current glottal vocoders generate the glottal excitation waveform by using deep neural networks (DNNs). However, the squared error-based training of the present glottal excitation models is limited to generating conditional average waveforms, which fails to capture the stochastic variation of the waveforms. As a result, shaped noise is added as post-processing. In this study, we propose a new method for predicting glottal waveforms by generative adversarial networks (GANs). GANs are generative models that aim to embed the data distribution in a latent space, enabling generation of new instances very similar to the original by randomly sampling the latent distribution. The glottal pulses generated by GANs show a stochastic component similar to natural glottal pulses. In our experiments, we compare synthetic speech generated using glottal waveforms produced by both DNNs and GANs. The results show that the newly proposed GANs achieve synthesis quality comparable to that of widely-used DNNs, without using an additive noise component",
    "checked": true,
    "id": "ce4d0824c06ea68a00379a2a6bd073567feb66f9",
    "semantic_title": "generative adversarial network-based glottal waveform model for statistical parametric speech synthesis",
    "citation_count": 38,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/luo17c_interspeech.html": {
    "title": "Emotional Voice Conversion with Adaptive Scales F0 Based on Wavelet Transform Using Limited Amount of Emotional Data",
    "volume": "main",
    "abstract": "Deep learning techniques have been successfully applied to speech processing. Typically, neural networks (NNs) are very effective in processing nonlinear features, such as mel cepstral coefficients (MCC), which represent the spectrum features in voice conversion (VC) tasks. Despite these successes, the approach is restricted to problems with moderate dimension and sufficient data. Thus, in emotional VC tasks, it is hard to deal with a simple representation of fundamental frequency (F0), which is the most important feature in emotional voice representation, Another problem is that there are insufficient emotional data for training. To deal with these two problems, in this paper, we propose the adaptive scales continuous wavelet transform (AS-CWT) method to systematically capture the F0 features of different temporal scales, which can represent different prosodic levels ranging from micro-prosody to sentence levels. Meanwhile, we also use the pre-trained conversion functions obtained from other emotional datasets to synthesize new emotional data as additional training samples for target emotional voice conversion. Experimental results indicate that our proposed method achieves the best performance in both objective and subjective evaluations",
    "checked": true,
    "id": "abe0dd136b11208fc721c98b00e883de7c4b7d2f",
    "semantic_title": "emotional voice conversion with adaptive scales f0 based on wavelet transform using limited amount of emotional data",
    "citation_count": 27,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/doddipatla17_interspeech.html": {
    "title": "Speaker Adaptation in DNN-Based Speech Synthesis Using d-Vectors",
    "volume": "main",
    "abstract": "The paper presents a mechanism to perform speaker adaptation in speech synthesis based on deep neural networks (DNNs). The mechanism extracts speaker identification vectors, so-called d-vectors, from the training speakers and uses them jointly with the linguistic features to train a multi-speaker DNN-based text-to-speech synthesizer (DNN-TTS). The d-vectors are derived by applying principal component analysis (PCA) on the bottle-neck features of a speaker classifier network. At the adaptation stage, three variants are explored: (1) d-vectors calculated using data from the target speaker, or (2) d-vectors calculated as a weighted sum of d-vectors from training speakers, or (3) d-vectors calculated as an average of the above two approaches. The proposed method of unsupervised adaptation using the d-vector is compared with the commonly used i-vector based approach for speaker adaptation. Listening tests show that: (1) for speech quality, the d-vector based approach is significantly preferred over the i-vector based approach. All the d-vector variants perform similar for speech quality; (2) for speaker similarity, both d-vector and i-vector based adaptation were found to perform similar, except a small significant preference for the d-vector calculated as an average over the i-vector",
    "checked": true,
    "id": "3a06659240d82058301c4de8257650f52e29b0c9",
    "semantic_title": "speaker adaptation in dnn-based speech synthesis using d-vectors",
    "citation_count": 36,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/li17l_interspeech.html": {
    "title": "Spectro-Temporal Modelling with Time-Frequency LSTM and Structured Output Layer for Voice Conversion",
    "volume": "main",
    "abstract": "From speech, speaker identity can be mostly characterized by the spectro-temporal structures of spectrum. Although recent researches have demonstrated the effectiveness of employing long short-term memory (LSTM) recurrent neural network (RNN) in voice conversion, traditional LSTM-RNN based approaches usually focus on temporal evolutions of speech features only. In this paper, we improve the conventional LSTM-RNN method for voice conversion by employing the two-dimensional time-frequency LSTM (TFLSTM) to model spectro-temporal warping along both time and frequency axes. A multi-task learned structured output layer (SOL) is afterward adopted to capture the dependencies between spectral and pitch parameters for further improvement, where spectral parameter targets are conditioned upon pitch parameters prediction. Experimental results show the proposed approach outperforms conventional systems in speech quality and speaker similarity",
    "checked": true,
    "id": "37f375ee40f45572d9167d6c850b1e2d80860ef7",
    "semantic_title": "spectro-temporal modelling with time-frequency lstm and structured output layer for voice conversion",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ramos17_interspeech.html": {
    "title": "Segment Level Voice Conversion with Recurrent Neural Networks",
    "volume": "main",
    "abstract": "Voice conversion techniques aim to modify a subject's voice characteristics in order to mimic the one's of another person. Due to the difference in utterance length between source and target speaker, state of the art voice conversion systems often rely on a frame alignment pre-processing step. This step aligns the entire utterances with algorithms such as dynamic time warping (DTW) that introduce errors, hindering system performance. In this paper we present a new technique that avoids the alignment of entire utterances at frame level, while keeping the local context during training. For this purpose, we combine an RNN model with the use of phoneme or syllable-level information, obtained from a speech recognition system. This system segments the utterances into segments which then can be grouped into overlapping windows, providing the needed context for the model to learn the temporal dependencies. We show that with this approach, notable improvements can be attained over a state of the art RNN voice conversion system on the CMU ARCTIC database. It is also worth noting that with this technique it is possible to halve the training data size and still outperform the baseline",
    "checked": true,
    "id": "33b854e0fae559dc5be63e6fcd671e612f538a5c",
    "semantic_title": "segment level voice conversion with recurrent neural networks",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/moore17_interspeech.html": {
    "title": "Creating a Voice for MiRo, the World's First Commercial Biomimetic Robot",
    "volume": "main",
    "abstract": "This paper introduces MiRo — the world's first commercial biomimetic robot — and describes how its vocal system was designed using a real-time parametric general-purpose mammalian vocal synthesiser tailored to the specific physical characteristics of the robot. MiRo's capabilities will be demonstrated live during the hands-on interactive ‘Show & Tell' session at INTERSPEECH-2017",
    "checked": true,
    "id": "9b241b4b15618aa4c5281d327736aef813060e0c",
    "semantic_title": "creating a voice for miro, the world's first commercial biomimetic robot",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/dominguez17_interspeech.html": {
    "title": "A Thematicity-Based Prosody Enrichment Tool for CTS",
    "volume": "main",
    "abstract": "This paper presents a demonstration of a stochastic prosody tool for enrichment of synthesized speech using SSML prosody tags applied over hierarchical thematicity spans in the context of a CTS application. The motivation for using hierarchical thematicity is exemplified, together with the capabilities of the module to generate a variety of SSML prosody tags within a controlled range of values depending on the input thematicity label",
    "checked": true,
    "id": "b9019e4bb05705e5f07ec0a153e1646122ed429a",
    "semantic_title": "a thematicity-based prosody enrichment tool for cts",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gruber17_interspeech.html": {
    "title": "WebSubDub — Experimental System for Creating High-Quality Alternative Audio Track for TV Broadcasting",
    "volume": "main",
    "abstract": "This paper deals with a presentation of an experimental system (called WebSubDub) for creating a high-quality alternative audio track for TV broadcasting. The system is used to create subtitles for TV shows in such a format which allows to automatically generate an alternative audio track with multiple voices employing a specially adapted TTS system. This alternative audio track is intended for televiewers with slight hearing impairments, i.e. for a group of televiewers who encounter issues when perceiving the original audio track — especially dialogues with background music, background noise or emotional speech. The system was developed in cooperation with Czech television, the public service broadcaster in the Czech Republic",
    "checked": true,
    "id": "3fcea84ee46794463408858a4b49028410369c45",
    "semantic_title": "websubdub - experimental system for creating high-quality alternative audio track for tv broadcasting",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/juzova17_interspeech.html": {
    "title": "Voice Conservation and TTS System for People Facing Total Laryngectomy",
    "volume": "main",
    "abstract": "The presented paper is focused on the building of personalized text-to-speech (TTS) synthesis for people who are losing their voices due to fatal diseases. The special conditions of this issue make the process different from preparing professional synthetic voices for commercial TTS systems and make it also more difficult. The whole process is described in this paper and the first results of the personalized voice building are presented here as well",
    "checked": true,
    "id": "9e754517bbea651259bf60d9693cd510f26d89d4",
    "semantic_title": "voice conservation and tts system for people facing total laryngectomy",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ghone17_interspeech.html": {
    "title": "TBT (Toolkit to Build TTS): A High Performance Framework to Build Multiple Language HTS Voice",
    "volume": "main",
    "abstract": "With the development of high quality TTS systems, application area of synthetic speech is increasing rapidly. Beyond the communication aids for the visually impaired and vocally handicap, TTS voices are being used in various educational, telecommunication and multimedia applications. All around the world people are trying to build TTS voice for their regional languages. TTS voice building requires a number of steps to follow and involves use of multiple tools, which makes it time consuming, tedious and perplexing to a user. This paper describes a Toolkit developed for HMM-based TTS voice building that makes the process much easier and handy. The toolkit uses all required tools, viz. HTS, Festival, Festvox, Hybrid Segmentation Tool, etc. and handles each and every step starting from phone set creation, then prompt generation, hybrid segmentation, F0 range finding, voice building, and finally putting the built voice into Synthesis framework. Wherever possible it does parallel processing to reduce time. It saves manual effort and time to a large extent and enable a person to build TTS voice very easily. This toolkit is made available under Open Source license",
    "checked": true,
    "id": "a1fd2144b1a42d432d1a07ecedb0994c871cfa42",
    "semantic_title": "tbt (toolkit to build tts): a high performance framework to build multiple language hts voice",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/karhila17_interspeech.html": {
    "title": "SIAK — A Game for Foreign Language Pronunciation Learning",
    "volume": "main",
    "abstract": "We introduce a digital game for children's foreign-language learning that uses automatic speech recognition (ASR) for evaluating children's utterances. Our first prototype focuses on the learning of English words and their pronunciation. The game connects to a network server, which handles the recognition and pronunciation grading of children's foreign-language speech. The server is reusable for different applications. Given suitable acoustic models, it can be used for grading pronunciations in any language",
    "checked": true,
    "id": "f89dda8850cbe5ff1a89959c81476cb3db5e2857",
    "semantic_title": "siak - a game for foreign language pronunciation learning",
    "citation_count": 13,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/larsson17_interspeech.html": {
    "title": "Integrating the Talkamatic Dialogue Manager with Alexa",
    "volume": "main",
    "abstract": "This paper describes the integration of Amazon Alexa with the Talkamatic Dialogue Manager (TDM), and shows how flexible dialogue skills and rapid prototyping of dialogue apps can be brought to the Alexa platform",
    "checked": true,
    "id": "23a48f6d688d6e931da8e8e256a3576eefd1f6cf",
    "semantic_title": "integrating the talkamatic dialogue manager with alexa",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ahmed17_interspeech.html": {
    "title": "A Robust Medical Speech-to-Speech/Speech-to-Sign Phraselator",
    "volume": "main",
    "abstract": "We present BabelDr, a web-enabled spoken-input phraselator for medical domains, which has been developed at Geneva University in a collaboration between a human language technology group and a group at the University hospital. The current production version of the system translates French into Arabic, using exclusively rule-based methods, and has performed credibly in simulated triaging tests with standardised patients. We also present an experimental version which combines large-vocabulary recognition with the main rule-based recogniser; offline tests on unseen data suggest that the new architecture adds robustness while more than halving the 2-best semantic error rate. The experimental version translates from spoken English into spoken French and also two sign languages",
    "checked": true,
    "id": "0134d7b7c5c57dc682555be65f04622a26d1f9b6",
    "semantic_title": "a robust medical speech-to-speech/speech-to-sign phraselator",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/duckhorn17_interspeech.html": {
    "title": "Towards an Autarkic Embedded Cognitive User Interface",
    "volume": "main",
    "abstract": "With this paper we present an overview of an autarkic embedded cognitive user interface. It is realized in form of an integrated device able to communicate with the user over speech & gesture recognition, speech synthesis and a touch display. Semantic processing and cognitive behaviour control support intuitive interaction and help controlling arbitrary electronic devices. To ensure user privacy and to operate autonomously of network access all information processing is done on the device",
    "checked": true,
    "id": "3b54e96f7aba1fac21bfec8f756bf047c16919a7",
    "semantic_title": "towards an autarkic embedded cognitive user interface",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/winata17_interspeech.html": {
    "title": "Nora the Empathetic Psychologist",
    "volume": "main",
    "abstract": "Nora is a new dialog system that mimics a conversation with a psychologist by screening for stress, anxiety, and depression. She understands, empathizes, and adapts to users using emotional intelligence modules trained via statistical modelling such as Convolutional Neural Networks. These modules also enable her to personalize the content of each conversation",
    "checked": true,
    "id": "a770ead81c84ec2f0c845dcc821a3f7763765808",
    "semantic_title": "nora the empathetic psychologist",
    "citation_count": 23,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/alam17_interspeech.html": {
    "title": "Modifying Amazon's Alexa ASR Grammar and Lexicon — A Case Study",
    "volume": "main",
    "abstract": "In this proof-of-concept study we build a tool that modifies the grammar and the dictionary of an Automatic Speech Recognition (ASR) engine. We evaluated our tool using Amazon's Alexa ASR engine. The experiments show that with our grammar and dictionary modification algorithms in the military domain, the accuracy of the modified ASR went up significantly — from 20/100 correct to 80/100 correct",
    "checked": true,
    "id": "2f8f940b0f05581ed8615a677bd851c5f38d57ec",
    "semantic_title": "modifying amazon's alexa asr grammar and lexicon - a case study",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lindblom17_interspeech.html": {
    "title": "Re-Inventing Speech — The Biological Way",
    "volume": "main",
    "abstract": "The mapping of the Speech Chain has so far been focused on the experimentally more accessible links — e.g., acoustics — whereas the brain's activity during speaking and listening has understandably received less attention. That state of affairs is about to change now thanks to the new sophisticated tools offered by brain imaging technology At present many key questions concerning human speech processes remain incompletely understood despite the significant research efforts of the past half century. As speech research goes neuro, we could do with some better answers In this paper I will attempt to shed some light on some of the issues. I will do so by heeding the advice that Tinbergen once gave his fellow biologists on explaining behavior. I paraphrase: Nothing in biology makes sense unless you simultaneously look at it with the following questions at the back of your mind: How did it evolve? How is it acquired? How does it work here and now? Applying the Tinbergen strategy to speech I will, in broad strokes, trace a path from the small and fixed innate repertoires of non-human primates to the open-ended vocal systems that humans learn today Such an agenda will admittedly identify serious gaps in our present knowledge but, importantly, it will also bring an overarching possibility: It will strongly suggest the feasibility of bypassing the traditional linguistic operational approach to speech units and replacing it by a first-principles account anchored in biology I will argue that this is the road-map we need for a more profound understanding of the fundamental nature spoken language and for educational, medical and technological applications",
    "checked": true,
    "id": "d7652b25f5a16f76c73299b0388f32472debfd31",
    "semantic_title": "re-inventing speech - the biological way",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/schuller17_interspeech.html": {
    "title": "The INTERSPEECH 2017 Computational Paralinguistics Challenge: Addressee, Cold & Snoring",
    "volume": "main",
    "abstract": "The INTERSPEECH 2017 Computational Paralinguistics Challenge addresses three different problems for the first time in research competition under well-defined conditions: In the Addressee sub-challenge, it has to be determined whether speech produced by an adult is directed towards another adult or towards a child; in the Cold sub-challenge, speech under cold has to be told apart from ‘healthy' speech; and in the Snoring sub-challenge, four different types of snoring have to be classified. In this paper, we describe these sub-challenges, their conditions, and the baseline feature extraction and classifiers, which include data-learnt feature representations by end-to-end learning with convolutional and recurrent neural networks, and bag-of-audio-words for the first time in the challenge series",
    "checked": true,
    "id": "58c87d2d678aab8bccd5cb20d04bc867682b07f2",
    "semantic_title": "the interspeech 2017 computational paralinguistics challenge: addressee, cold & snoring",
    "citation_count": 144,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/krajewski17_interspeech.html": {
    "title": "Description of the Upper Respiratory Tract Infection Corpus (URTIC)",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "c26386f9ef42accaec5331169fd789bc62e437f9",
    "semantic_title": "description of the upper respiratory tract infection corpus (urtic)",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/janott17_interspeech.html": {
    "title": "Description of the Munich-Passau Snore Sound Corpus (MPSSC)",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "82164e5672aff5b58cf5dc9f2709ab004829004b",
    "semantic_title": "description of the munich-passau snore sound corpus (mpssc)",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bergelson17_interspeech.html": {
    "title": "Description of the Homebank Child/Adult Addressee Corpus (HB-CHAAC)",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "4c5e83afa17c4bc216978caf94a34da956363eed",
    "semantic_title": "description of the homebank child/adult addressee corpus (hb-chaac)",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/huckvale17_interspeech.html": {
    "title": "It Sounds Like You Have a Cold! Testing Voice Features for the Interspeech 2017 Computational Paralinguistics Cold Challenge",
    "volume": "main",
    "abstract": "This paper describes an evaluation of four different voice feature sets for detecting symptoms of the common cold in speech as part of the Interspeech 2017 Computational Paralinguistics Challenge. The challenge corpus consists of 630 speakers in three partitions, of which approximately one third had a \"severe\" cold at the time of recording. Success on the task is measured in terms of unweighted average recall of cold/not-cold classification from short extracts of the recordings. In this paper we review previous voice features used for studying changes in health and devise four basic types of features for evaluation: voice quality features, vowel spectra features, modulation spectra features, and spectral distribution features. The evaluation shows that each feature set provides some useful information to the task, with features from the modulation spectrogram being most effective. Feature-level fusion of the feature sets shows small performance improvements on the development test set. We discuss the results in terms of the most suitable features for detecting symptoms of cold and address issues arising from the design of the challenge",
    "checked": true,
    "id": "4e0a5e08f66e88ee71a89e278c29a684641e97af",
    "semantic_title": "it sounds like you have a cold! testing voice features for the interspeech 2017 computational paralinguistics cold challenge",
    "citation_count": 14,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/cai17b_interspeech.html": {
    "title": "End-to-End Deep Learning Framework for Speech Paralinguistics Detection Based on Perception Aware Spectrum",
    "volume": "main",
    "abstract": "In this paper, we propose an end-to-end deep learning framework to detect speech paralinguistics using perception aware spectrum as input. Existing studies show that speech under cold has distinct variations of energy distribution on low frequency components compared with the speech under ‘healthy' condition. This motivates us to use perception aware spectrum as the input to an end-to-end learning framework with small scale dataset. In this work, we try both Constant Q Transform (CQT) spectrum and Gammatone spectrum in different end-to-end deep learning networks, where both spectrums are able to closely mimic the human speech perception and transform it into 2D images. Experimental results show the effectiveness of the proposed perception aware spectrum with end-to-end deep learning approach on Interspeech 2017 Computational Paralinguistics Cold sub-Challenge. The final fusion result of our proposed method is 8% better than that of the provided baseline in terms of UAR",
    "checked": true,
    "id": "f7c0c67d206570cca07b196037f5e2088a84ebc7",
    "semantic_title": "end-to-end deep learning framework for speech paralinguistics detection based on perception aware spectrum",
    "citation_count": 15,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wagner17c_interspeech.html": {
    "title": "Infected Phonemes: How a Cold Impairs Speech on a Phonetic Level",
    "volume": "main",
    "abstract": "The realization of language through vocal sounds involves a complex interplay between the lungs, the vocal cords, and a series of resonant chambers (e.g. mouth and nasal cavities). Due to their connection to the outside world, these body parts are popular spots for viruses and bacteria to enter the human organism. Affected people may suffer from an upper respiratory tract infection (URTIC) and consequently their voice often sounds breathy, raspy or sniffly. In this paper, we investigate the audible effects of a cold on a phonetic level. Results on a German corpus show that the articulation of consonants is more impaired than that of vowels. Surprisingly, nasal sounds do not follow this trend in our experiments. We finally try to predict a speaker's health condition by fusing decisions we derive from single phonemes. The presented work is part of the INTERSPEECH 2017 Computational Paralinguistics Challenge",
    "checked": true,
    "id": "e45506c70dfc3659fa5f09b2af650be9140d1be9",
    "semantic_title": "infected phonemes: how a cold impairs speech on a phonetic level",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/suresh17_interspeech.html": {
    "title": "Phoneme State Posteriorgram Features for Speech Based Automatic Classification of Speakers in Cold and Healthy Condition",
    "volume": "main",
    "abstract": "We consider the problem of automatically detecting if a speaker is suffering from common cold from his/her speech. When a speaker has symptoms of cold, his/her voice quality changes compared to the normal one. We hypothesize that such a change in voice quality could be reflected in lower likelihoods from a model built using normal speech. In order to capture this, we compute a 120-dimensional posteriorgram feature in each frame using Gaussian mixture model from 120 states of 40 three-states phonetic hidden Markov models trained on approximately 16.4 hours of normal English speech. Finally, a fixed 5160-dimensional phoneme state posteriorgram (PSP) feature vector for each utterance is obtained by computing statistics from the posteriorgram feature trajectory. Experiments on the 2017-Cold sub-challenge data show that when the decisions from bag-of-audio-words (BoAW) and end-to-end (e2e) are combined with those from PSP features with unweighted majority rule, the UAR on the development set becomes 69% which is 2.9% (absolute) better than the best of the UARs obtained by the baseline schemes. When the decisions from ComParE, BoAW and PSP features are combined with simple majority rule, it results in a UAR of 68.52% on the test set",
    "checked": true,
    "id": "6765f6341af9d067152c4f82ef7bdc2583606a53",
    "semantic_title": "phoneme state posteriorgram features for speech based automatic classification of speakers in cold and healthy condition",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/nwe17_interspeech.html": {
    "title": "An Integrated Solution for Snoring Sound Classification Using Bhattacharyya Distance Based GMM Supervectors with SVM, Feature Selection with Random Forest and Spectrogram with CNN",
    "volume": "main",
    "abstract": "Snoring is caused by the narrowing of the upper airway and it is excited by different locations within the upper airways. This irregularity could lead to the presence of Obstructive Sleep Apnea Syndrome (OSAS). Diagnosis of OSAS could therefore be made by snoring sound analysis. This paper proposes the novel method to automatically classify snoring sounds by their excitation locations for ComParE2017 challenge. We propose 3 sub-systems for classification. In the first system, we propose to integrate Bhattacharyya distance based Gaussian Mixture Model (GMM) supervectors to a set of static features provided by ComParE2017 challenge. The Bhattacharyya distance based GMM supervectors characterize the spectral dissimilarity measure among snore sounds excited by different locations. And, we employ Support Vector Machine (SVM) for classification. In the second system, we perform feature selection on static features provided by the challenge and conduct classification using Random Forest. In the third system, we extract spectrogram from audio and employ Convolutional Neural Network (CNN) for snore sound classification. Then, we fuse 3 sub-systems to produce final classification results. The experimental results show that the proposed system performs better than the challenge baseline",
    "checked": true,
    "id": "bfe45a1a95538caeb003e6a3613cc1572763fc88",
    "semantic_title": "an integrated solution for snoring sound classification using bhattacharyya distance based gmm supervectors with svm, feature selection with random forest and spectrogram with cnn",
    "citation_count": 14,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kitamura17_interspeech.html": {
    "title": "Acoustic Analysis of Detailed Three-Dimensional Shape of the Human Nasal Cavity and Paranasal Sinuses",
    "volume": "main",
    "abstract": "The nasal and paranasal cavities have a labyrinthine shape and their acoustic properties affect speech sounds. In this study, we explored the transfer function of the nasal and paranasal cavities, as well as the contribution of each paranasal cavity, using acoustical and numerical methods. A physical model of the nasal and paranasal cavities was formed using data from a high-resolution 3D X-ray CT and a 3D printer. The data was acquired from a female subject during silent nasal breathing. The transfer function of the physical model was then measured by introducing a white noise signal at the glottis and measuring its acoustic response at a point 20 mm away from the nostrils. We also calculated the transfer function of the 3D model using a finite-difference time-domain or FDTD method. The results showed that the gross shape and the frequency of peaks and dips of the measured and calculated transfer functions were similar, suggesting that both methods used in this study were reliable. The results of FDTD simulations evaluating the paranasal sinuses individually suggested that they contribute not only to spectral dips but also to peaks, which is contrary to the traditional theories regarding the production of speech sounds",
    "checked": true,
    "id": "f420857a72b35160ab48a4b1c26c9005c9514934",
    "semantic_title": "acoustic analysis of detailed three-dimensional shape of the human nasal cavity and paranasal sinuses",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/arnela17_interspeech.html": {
    "title": "A Semi-Polar Grid Strategy for the Three-Dimensional Finite Element Simulation of Vowel-Vowel Sequences",
    "volume": "main",
    "abstract": "Three-dimensional computational acoustic models need very detailed 3D vocal tract geometries to generate high quality sounds. Static geometries can be obtained from Magnetic Resonance Imaging (MRI), but it is not currently possible to capture dynamic MRI-based geometries with sufficient spatial and time resolution. One possible solution consists in interpolating between static geometries, but this is a complex task. We instead propose herein to use a semi-polar grid to extract 2D cross-sections from the static 3D geometries, and then interpolate them to obtain the vocal tract dynamics. Other approaches such as the adaptive grid have also been explored. In this method, cross-sections are defined perpendicular to the vocal tract midline, as typically done in 1D to obtain the vocal tract area functions. However, intersections between adjacent cross-sections may occur during the interpolation process, especially when the vocal tract midline quickly changes its orientation. In contrast, the semi-polar grid prevents these intersections because the plane orientations are fixed over time. Finite element simulations of static vowels are first conducted, showing that 3D acoustic wave propagation is not significantly altered when the semi-polar grid is used instead of the adaptive grid. The vowel-vowel sequence [ɑi] is finally simulated to demonstrate the method",
    "checked": true,
    "id": "f58534dccd16129ffd366039bdf7d7a199677669",
    "semantic_title": "a semi-polar grid strategy for the three-dimensional finite element simulation of vowel-vowel sequences",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/vasudevan17_interspeech.html": {
    "title": "A Fast Robust 1D Flow Model for a Self-Oscillating Coupled 2D FEM Vocal Fold Simulation",
    "volume": "main",
    "abstract": "A balance between the simplicity and speed of lumped-element vocal fold models and the completeness and complexity of continuum-models is required to achieve fast high-quality articulatory speech synthesis. We develop and implement a novel self-oscillating vocal-fold model, composed of a 1D unsteady fluid model loosely coupled with a 2D FEM structural model. The flow model is capable of robustly handling irregular geometries, different boundary conditions, closure of the glottis and unsteady flow states. A method for a fast decoupled solution of the flow equations that does not require the computation of the Jacobian is provided. The model is coupled with a 2D real-time finite-difference wave-solver for simulating vocal tract acoustics and a 1D wave-reflection analog representation of the trachea. The simulation results are shown to agree with existing data in literature, and give realistic pressure-velocity distributions, glottal width and glottal flow values. In addition, the model is more than an order of magnitude faster to run than comparable 2D Navier-Stokes fluid solvers, while better capturing transitional flow than simple Bernoulli-based flow models. The vocal fold model provides an alternative to simple lumped-element models for faster higher-quality articulatory speech synthesis",
    "checked": true,
    "id": "03042b49af345cde7ad9c634d8d7fe6cbc72c9ba",
    "semantic_title": "a fast robust 1d flow model for a self-oscillating coupled 2d fem vocal fold simulation",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/murtola17_interspeech.html": {
    "title": "Waveform Patterns in Pitch Glides Near a Vocal Tract Resonance",
    "volume": "main",
    "abstract": "A time-domain model of vowel production is used to simulate fundamental frequency glides over the first vocal tract resonance. A vocal tract geometry extracted from MRI data of a female speaker pronouncing [i] is used. The model contains direct feedback from the acoustic loads to vocal fold tissues and the inertial effect of the full air column on the glottal flow. The simulations reveal that a perturbation pattern in the fundamental frequency, namely, a jump and locking to the vocal tract resonance, is accompanied by a specific pattern of glottal waveform changes",
    "checked": true,
    "id": "5483a3e2a459ffd48143f1437dd3130910eda358",
    "semantic_title": "waveform patterns in pitch glides near a vocal tract resonance",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/degirmenci17_interspeech.html": {
    "title": "A Unified Numerical Simulation of Vowel Production That Comprises Phonation and the Emitted Sound",
    "volume": "main",
    "abstract": "A unified approach for the numerical simulation of vowels is presented, which accounts for the self-oscillations of the vocal folds including contact, the generation of acoustic waves and their propagation through the vocal tract, and the sound emission outwards the mouth. A monolithic incompressible fluid-structure interaction model is used to simulate the interaction between the glottal jet and the vocal folds, whereas the contact model is addressed by means of a level set application of the Eikonal equation. The coupling with acoustics is done through an acoustic analogy stemming from a simplification of the acoustic perturbation equations. This coupling is one-way in the sense that there is no feedback from the acoustics to the flow and mechanical fields All the involved equations are solved together at each time step and in a single computational run, using the finite element method (FEM). As an application, the production of vowel [i] has been addressed. Despite the complexity of all physical phenomena to be simulated simultaneously, which requires resorting to massively parallel computing, the formant locations of vowel [i] have been well recovered",
    "checked": true,
    "id": "0c219e11d9aae01d698f40fdaefdc8e120cbe6e7",
    "semantic_title": "a unified numerical simulation of vowel production that comprises phonation and the emitted sound",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/dabbaghchian17_interspeech.html": {
    "title": "Synthesis of VV Utterances from Muscle Activation to Sound with a 3D Model",
    "volume": "main",
    "abstract": "We propose a method to automatically generate deformable 3D vocal tract geometries from the surrounding structures in a biomechanical model. This allows us to couple 3D biomechanics and acoustics simulations. The basis of the simulations is muscle activation trajectories in the biomechanical model, which move the articulators to the desired articulatory positions. The muscle activation trajectories for a vowel-vowel utterance are here defined through interpolation between the determined activations of the start and end vowel. The resulting articulatory trajectories of flesh points on the tongue surface and jaw are similar to corresponding trajectories measured using Electromagnetic Articulography, hence corroborating the validity of interpolating muscle activation. At each time step in the articulatory transition, a 3D vocal tract tube is created through a cavity extraction method based on first slicing the geometry of the articulators with a semi-polar grid to extract the vocal tract contour in each plane and then reconstructing the vocal tract through a smoothed 3D mesh-generation using the extracted contours. A finite element method applied to these changing 3D geometries simulates the acoustic wave propagation. We present the resulting acoustic pressure changes on the vocal tract boundary and the formant transitions for the utterance [ɑi]",
    "checked": true,
    "id": "65cb239e6c9237818f48bd756fdfb289920bfa4f",
    "semantic_title": "synthesis of vv utterances from muscle activation to sound with a 3d model",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mv17_interspeech.html": {
    "title": "A Dual Source-Filter Model of Snore Audio for Snorer Group Classification",
    "volume": "main",
    "abstract": "Snoring is a common symptom of serious chronic disease known as obstructive sleep apnea (OSA). Knowledge about the location of obstruction site (V—Velum, O—Oropharyngeal lateral walls, T—Tongue, E—Epiglottis) in the upper airways is necessary for proper surgical treatment. In this paper we propose a dual source-filter model similar to the source-filter model of speech to approximate the generation process of snore audio. The first filter models the vocal tract from lungs to the point of obstruction with white noise excitation from the lungs. The second filter models the vocal tract from the obstruction point to the lips/nose with impulse train excitation which represents vibrations at the point of obstruction. The filter coefficients are estimated using the closed and open phases of the snore beat cycle. VOTE classification is done by using SVM classifier and filter coefficients as features. The classification experiments are performed on the development set (283 snore audios) of the MUNICH-PASSAU SNORE SOUND CORPUS (MPSSC).We obtain an unweighted average recall (UAR) of 49.58%, which is higher than the INTERSPEECH-2017 snoring sub-challenge baseline technique by ~3% (absolute)",
    "checked": true,
    "id": "cf924069757916654ae74d2322d9bcb5a8987733",
    "semantic_title": "a dual source-filter model of snore audio for snorer group classification",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/freitag17_interspeech.html": {
    "title": "An ‘End-to-Evolution' Hybrid Approach for Snore Sound Classification",
    "volume": "main",
    "abstract": "Whilst snoring itself is usually not harmful to a person's health, it can be an indication of Obstructive Sleep Apnoea (OSA), a serious sleep-related disorder. As a result, studies into using snoring as acoustic based marker of OSA are gaining in popularity. Motivated by this, the INTERSPEECH 2017 ComParE Snoring sub-challenge requires classification from which areas in the upper airways different snoring sounds originate. This paper explores a hybrid approach combining evolutionary feature selection based on competitive swarm optimisation and deep convolutional neural networks (CNN). Feature selection is applied to novel deep spectrum features extracted directly from spectrograms using pre-trained image classification CNN. Key results presented demonstrate that our hybrid approach can substantially increase the performance of a linear support vector machine on a set of low-level features extracted from the Snoring sub-challenge data. Even without subset selection, the deep spectrum features are sufficient to outperform the challenge baseline, and competitive swarm optimisation further improves system performance. In comparison to the challenge baseline, unweighted average recall is increased from 40.6% to 57.6% on the development partition, and from 58.5% to 66.5% on the test partition, using 2246 of the 4096 deep spectrum features",
    "checked": true,
    "id": "1e0ae9272a571804f6b3cbcee38a654423ad6193",
    "semantic_title": "an 'end-to-evolution' hybrid approach for snore sound classification",
    "citation_count": 25,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/amiriparian17_interspeech.html": {
    "title": "Snore Sound Classification Using Image-Based Deep Spectrum Features",
    "volume": "main",
    "abstract": "In this paper, we propose a method for automatically detecting various types of snore sounds using image classification convolutional neural network (CNN) descriptors extracted from audio file spectrograms. The descriptors, denoted as deep spectrum features, are derived from forwarding spectrograms through very deep task-independent pre-trained CNNs. Specifically, activations of fully connected layers from two common image classification CNNs, AlexNet and VGG19, are used as feature vectors. Moreover, we investigate the impact of differing spectrogram colour maps and two CNN architectures on the performance of the system. Results presented indicate that deep spectrum features extracted from the activations of the second fully connected layer of AlexNet using a viridis colour map are well suited to the task. This feature space, when combined with a support vector classifier, outperforms the more conventional knowledge-based features of 6 373 acoustic functionals used in the INTERSPEECH ComParE 2017 Snoring sub-challenge baseline system. In comparison to the baseline, unweighted average recall is increased from 40.6% to 44.8% on the development partition, and from 58.5% to 67.0% on the test partition",
    "checked": true,
    "id": "4bd0f5bddf31961361cc5b16cca991e5c6aa8f5c",
    "semantic_title": "snore sound classification using image-based deep spectrum features",
    "citation_count": 243,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tavarez17_interspeech.html": {
    "title": "Exploring Fusion Methods and Feature Space for the Classification of Paralinguistic Information",
    "volume": "main",
    "abstract": "This paper introduces the different systems developed by Aholab Signal Processing Laboratory for The INTERSPEECH 2017 Computational Paralinguistics Challenge, which includes three different subtasks: Addressee, Cold and Snoring classification. Several classification strategies and features related with the spectrum, prosody and phase have been tested separately and further combined by using different fusion techniques, such as early fusion by means of multi-feature vectors, late fusion of the standalone classifier scores and label fusion via weighted voting. The obtained results show that the applied fusion methods improve the performance of the standalone detectors and provide systems capable of outperforming the baseline systems in terms of UAR",
    "checked": true,
    "id": "8ff0ddcc645e456bd60dfab2a4d0befbaff03233",
    "semantic_title": "exploring fusion methods and feature space for the classification of paralinguistic information",
    "citation_count": 14,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gosztolya17b_interspeech.html": {
    "title": "DNN-Based Feature Extraction and Classifier Combination for Child-Directed Speech, Cold and Snoring Identification",
    "volume": "main",
    "abstract": "In this study we deal with the three sub-challenges of the Interspeech ComParE Challenge 2017, where the goal is to identify child-directed speech, speakers having a cold, and different types of snoring sounds. For the first two sub-challenges we propose a simple, two-step feature extraction and classification scheme: first we perform frame-level classification via Deep Neural Networks (DNNs), and then we extract utterance-level features from the DNN outputs. By utilizing these features for classification, we were able to match the performance of the standard paralinguistic approach (which involves extracting thousands of features, many of them being completely irrelevant to the actual task). As for the Snoring Sub-Challenge, we divided the recordings into segments, and averaged out some frame-level features segment-wise, which were then used for utterance-level classification. When combining the predictions of the proposed approaches with those got by the standard paralinguistic approach, we managed to outperform the baseline values of the Cold and Snoring sub-challenges on the hidden test sets",
    "checked": true,
    "id": "498973c5c2255bc02a2e274f0c3b96d188b95e04",
    "semantic_title": "dnn-based feature extraction and classifier combination for child-directed speech, cold and snoring identification",
    "citation_count": 35,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kaya17_interspeech.html": {
    "title": "Introducing Weighted Kernel Classifiers for Handling Imbalanced Paralinguistic Corpora: Snoring, Addressee and Cold",
    "volume": "main",
    "abstract": "The field of paralinguistics is growing rapidly with a wide range of applications that go beyond recognition of emotions, laughter and personality. The research flourishes in multiple directions such as signal representation and classification, addressing the issues of the domain. Apart from the noise robustness, an important issue with real life data is the imbalanced nature: some classes of states/traits are under-represented. Combined with the high dimensionality of the feature vectors used in the state-of-the-art analysis systems, this issue poses the threat of over-fitting. While the kernel trick can be employed to handle the dimensionality issue, regular classifiers inherently aim to minimize the misclassification error and hence are biased towards the majority class. A solution to this problem is over-sampling of the minority class(es). However, this brings increased memory/computational costs, while not bringing any new information to the classifier. In this work, we propose a new weighting scheme on instances of the original dataset, employing Weighted Kernel Extreme Learning Machine, and inspired from that, introducing the Weighted Partial Least Squares Regression based classifier. The proposed methods are applied on all three INTERSPEECH ComParE 2017 challenge corpora, giving better or competitive results compared to the challenge baselines",
    "checked": true,
    "id": "3f230fed7b640513088832e4e234f49821782f06",
    "semantic_title": "introducing weighted kernel classifiers for handling imbalanced paralinguistic corpora: snoring, addressee and cold",
    "citation_count": 29,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/steidl17_interspeech.html": {
    "title": "The INTERSPEECH 2017 Computational Paralinguistics Challenge: A Summary of Results",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "6bd5ab9a85e593b95bfb79cad94fd809b6baa2f9",
    "semantic_title": "the interspeech 2017 computational paralinguistics challenge: a summary of results",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/schuller17b_interspeech.html": {
    "title": "Discussion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": "49ae02af2ed131ca826f3d7da3c456a1db47a6bf",
    "semantic_title": "discussion papers",
    "citation_count": 632,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/toshniwal17_interspeech.html": {
    "title": "Multitask Learning with Low-Level Auxiliary Tasks for Encoder-Decoder Based Speech Recognition",
    "volume": "main",
    "abstract": "End-to-end training of deep learning-based models allows for implicit learning of intermediate representations based on the final task loss. However, the end-to-end approach ignores the useful domain knowledge encoded in explicit intermediate-level supervision. We hypothesize that using intermediate representations as auxiliary supervision at lower levels of deep networks may be a good way of combining the advantages of end-to-end training and more traditional pipeline approaches. We present experiments on conversational speech recognition where we use lower-level tasks, such as phoneme recognition, in a multitask training approach with an encoder-decoder model for direct character transcription. We compare multiple types of lower-level tasks and analyze the effects of the auxiliary tasks. Our results on the Switchboard corpus show that this approach improves recognition accuracy over a standard encoder-decoder model on the Eval2000 test set",
    "checked": true,
    "id": "4fc645df0046a6d7bdeaae8cb20f36b2542bac86",
    "semantic_title": "multitask learning with low-level auxiliary tasks for encoder-decoder based speech recognition",
    "citation_count": 111,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/shannon17b_interspeech.html": {
    "title": "Optimizing Expected Word Error Rate via Sampling for Speech Recognition",
    "volume": "main",
    "abstract": "State-level minimum Bayes risk (sMBR) training has become the de facto standard for sequence-level training of speech recognition acoustic models. It has an elegant formulation using the expectation semiring, and gives large improvements in word error rate (WER) over models trained solely using cross-entropy (CE) or connectionist temporal classification (CTC). sMBR training optimizes the expected number of frames at which the reference and hypothesized acoustic states differ. It may be preferable to optimize the expected WER, but WER does not interact well with the expectation semiring, and previous approaches based on computing expected WER exactly involve expanding the lattices used during training. In this paper we show how to perform optimization of the expected WER by sampling paths from the lattices used during conventional sMBR training. The gradient of the expected WER is itself an expectation, and so may be approximated using Monte Carlo sampling. We show experimentally that optimizing WER during acoustic model training gives 5% relative improvement in WER over a well-tuned sMBR baseline on a 2-channel query recognition task (Google Home)",
    "checked": true,
    "id": "b2c06c1bdcc1be8548a9821bcf8a2d30f9efd43e",
    "semantic_title": "optimizing expected word error rate via sampling for speech recognition",
    "citation_count": 54,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sainath17_interspeech.html": {
    "title": "Annealed f-Smoothing as a Mechanism to Speed up Neural Network Training",
    "volume": "main",
    "abstract": "In this paper, we describe a method to reduce the overall number of neural network training steps, during both cross-entropy and sequence training stages. This is achieved through the interpolation of frame-level CE and sequence level SMBR criteria, during the sequence training stage. This interpolation is known as f-smoothing and has previously been just used to prevent overfitting during sequence training. However, in this paper, we investigate its application to reduce the training time. We explore different interpolation strategies to reduce the overall training steps; and achieve a reduction of up to 25% with almost no degradation in word error rate (WER). Finally, we explore the generalization of f-smoothing to other tasks",
    "checked": true,
    "id": "19e57d9e64cb460ea147386b6f075ff59b094eba",
    "semantic_title": "annealed f-smoothing as a mechanism to speed up neural network training",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/meng17b_interspeech.html": {
    "title": "Non-Uniform MCE Training of Deep Long Short-Term Memory Recurrent Neural Networks for Keyword Spotting",
    "volume": "main",
    "abstract": "It has been shown in [1, 2] that improved performance can be achieved by formulating the keyword spotting as a non-uniform error automatic speech recognition problem. In this work, we discriminatively train a deep bidirectional long short-term memory (BLSTM) — hidden Markov model (HMM) based acoustic model with non-uniform boosted minimum classification error (BMCE) criterion which imposes more significant error cost on the keywords than those on the non-keywords. By introducing the BLSTM, the context information in both the past and the future are stored and updated to predict the desired output and the long-term dependencies within the speech signal are well captured. With non-uniform BMCE objective, the BLSTM is trained so that the recognition errors related to the keywords are remarkably reduced. The BLSTM is optimized using back-propagation through time and stochastic gradient descent. The keyword spotting system is implemented within weighted finite state transducer framework. The proposed method achieves 5.49% and 7.37% absolute figure-of-merit improvements respectively over the BLSTM and the feedforward deep neural network baseline systems trained with cross-entropy criterion for the keyword spotting task on Switchboard-1 Release 2 dataset",
    "checked": true,
    "id": "74a4184a8aab82914e78b2c7434b373607a7926d",
    "semantic_title": "non-uniform mce training of deep long short-term memory recurrent neural networks for keyword spotting",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/dighe17_interspeech.html": {
    "title": "Exploiting Eigenposteriors for Semi-Supervised Training of DNN Acoustic Models with Sequence Discrimination",
    "volume": "main",
    "abstract": "Deep neural network (DNN) acoustic models yield posterior probabilities of senone classes. Recent studies support the existence of low-dimensional subspaces underlying senone posteriors. Principal component analysis (PCA) is applied to identify eigenposteriors and perform low-dimensional projection of the training data posteriors. The resulted enhanced posteriors are applied as soft targets for training better DNN acoustic model under the student-teacher framework. The present work advances this approach by studying incorporation of sequence discriminative training. We demonstrate how to combine the gains from eigenposterior based enhancement with sequence discrimination to improve ASR using semi-supervised training. Evaluation on AMI meeting corpus yields nearly 4% absolute reduction in word error rate (WER) compared to the baseline DNN trained with cross entropy objective. In this context, eigenposterior enhancement of the soft targets is crucial to enable additive improvement using out-of-domain untranscribed data",
    "checked": true,
    "id": "483e8e8d19a56405d303cd3dc4b1dc16d1e9fa90",
    "semantic_title": "exploiting eigenposteriors for semi-supervised training of dnn acoustic models with sequence discrimination",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/yang17c_interspeech.html": {
    "title": "Discriminative Autoencoders for Acoustic Modeling",
    "volume": "main",
    "abstract": "Speech data typically contain information irrelevant to automatic speech recognition (ASR), such as speaker variability and channel/environmental noise, lurking deep within acoustic features. Such unwanted information is always mixed together to stunt the development of an ASR system. In this paper, we propose a new framework based on autoencoders for acoustic modeling in ASR. Unlike other variants of autoencoder neural networks, our framework is able to isolate phonetic components from a speech utterance by simultaneously taking two kinds of objectives into consideration. The first one relates to the minimization of reconstruction errors and benefits to learn most salient and useful properties of the data. The second one functions in the middlemost code layer, where the categorical distribution of the context-dependent phone states is estimated for phoneme discrimination and the derivation of acoustic scores, the proximity relationship among utterances spoken by the same speaker are preserved, and the intra-utterance noise is modeled and abstracted away. We describe the implementation of the discriminative autoencoders for training tri-phone acoustic models and present TIMIT phone recognition results, which demonstrate that our proposed method outperforms the conventional DNN-based approach",
    "checked": true,
    "id": "2b3ea6f48e33ba59eaff27f559f067f1af16fa1b",
    "semantic_title": "discriminative autoencoders for acoustic modeling",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zajic17_interspeech.html": {
    "title": "Speaker Diarization Using Convolutional Neural Network for Statistics Accumulation Refinement",
    "volume": "main",
    "abstract": "The aim of this paper is to investigate the benefit of information from a speaker change detection system based on Convolutional Neural Network (CNN) when applied to the process of accumulation of statistics for an i-vector generation. The investigation is carried out on the problem of diarization. In our system, the output of the CNN is a probability value of a speaker change in a conversation for a given time segment. According to this probability, we cut the conversation into short segments that are then represented by the i-vector (to describe a speaker in it). We propose a technique to utilize the information from the CNN for the weighting of the acoustic data in a segment to refine the statistics accumulation process. This technique enables us to represent the speaker better in the final i-vector. The experiments on the English part of the CallHome corpus show that our proposed refinement of the statistics accumulation is beneficial with the relative improvement of Diarization Error Rate almost by 16% when compared to the speaker diarization system without statistics refinement",
    "checked": true,
    "id": "35c40fde977932d8a3cd24f5a1724c9dbca8b38d",
    "semantic_title": "speaker diarization using convolutional neural network for statistics accumulation refinement",
    "citation_count": 29,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jati17_interspeech.html": {
    "title": "Speaker2Vec: Unsupervised Learning and Adaptation of a Speaker Manifold Using Deep Neural Networks with an Evaluation on Speaker Segmentation",
    "volume": "main",
    "abstract": "This paper presents a novel approach, we term Speaker2Vec, to derive a speaker-characteristics manifold learned in an unsupervised manner. The proposed representation can be employed in different applications such as diarization, speaker identification or, as in our evaluation test case, speaker segmentation. Speaker2Vec exploits large amounts of unlabeled training data and the assumption of short-term active-speaker stationarity to derive a speaker embedding using Deep Neural Networks (DNN). We assume that temporally-near speech segments belong to the same speaker, and as such a joint representation connecting these nearby segments can encode their common information. Thus, this bottleneck representation will be capturing mainly speaker-specific information. Such training can take place in a completely unsupervised manner. For testing, our trained model generates the embeddings for the test audio, and applies a simple distance metric to detect speaker-change points. The paper also proposes a strategy for unsupervised adaptation of the DNN models to the application domain. The proposed method outperforms the state-of-the-art speaker segmentation algorithms and MFCC based baseline methods on four evaluation datasets, while it allows for further improvements by employing this embedding into supervised training methods",
    "checked": true,
    "id": "b93eb486b3093712d99fde30b81d657228d64a62",
    "semantic_title": "speaker2vec: unsupervised learning and adaptation of a speaker manifold using deep neural networks with an evaluation on speaker segmentation",
    "citation_count": 21,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lan17_interspeech.html": {
    "title": "A Triplet Ranking-Based Neural Network for Speaker Diarization and Linking",
    "volume": "main",
    "abstract": "This paper investigates a novel neural scoring method, based on conventional i-vectors, to perform speaker diarization and linking of large collections of recordings. Using triplet loss for training, the network projects i-vectors in a space that better separates speakers in terms of cosine similarity. Experiments are run on two French TV collections built from REPERE [1] and ETAPE [2] campaigns corpora, the system being trained on French Radio data. Results indicate that the proposed approach outperforms conventional cosine and Probabilistic Linear Discriminant Analysis scoring methods on both within- and cross-recording diarization tasks, with a Diarization Error Rate reduction of 14% in average",
    "checked": true,
    "id": "edf67c0ea711d64472e763b948125d75827bd242",
    "semantic_title": "a triplet ranking-based neural network for speaker diarization and linking",
    "citation_count": 11,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/cohen17_interspeech.html": {
    "title": "Estimating Speaker Clustering Quality Using Logistic Regression",
    "volume": "main",
    "abstract": "This paper focuses on estimating clustering validity by using logistic regression. For many applications it might be important to estimate the quality of the clustering, e.g. in case of speech segments' clustering, make a decision whether to use the clustered data for speaker verification. In the case of short segments speakers clustering, the common criteria for cluster validity are average cluster purity (ACP), average speaker purity (ASP) and K — the geometric mean between the two measures. As in practice, true labels are not available for evaluation, hence they have to be estimated from the clustering itself. In this paper, mean-shift clustering with PLDA score is applied in order to cluster short speaker segments represented as i-vectors. Different statistical parameters are then estimated on the clustered data and are used to train logistic regression to estimate ACP, ASP and K. It was found that logistic regression can be a good predictor of the actual ACP, ASP and K, and yields reasonable information regarding the clustering quality",
    "checked": true,
    "id": "b7c21c2bbefaa2c49f1a5c5161c8f7b267dfd3ce",
    "semantic_title": "estimating speaker clustering quality using logistic regression",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wisniewksi17_interspeech.html": {
    "title": "Combining Speaker Turn Embedding and Incremental Structure Prediction for Low-Latency Speaker Diarization",
    "volume": "main",
    "abstract": "Real-time speaker diarization has many potential applications, including public security, biometrics or forensics. It can also significantly speed up the indexing of increasingly large multimedia archives. In this paper, we address the issue of low-latency speaker diarization that consists in continuously detecting new or reoccurring speakers within an audio stream, and determining when each speaker is active with a low latency ( e.g. every second). This is in contrast with most existing approaches in speaker diarization that rely on multiple passes over the complete audio recording. The proposed approach combines speaker turn neural embeddings with an incremental structure prediction approach inspired by state-of-the-art Natural Language Processing models for Part-of-Speech tagging and dependency parsing. It can therefore leverage both information describing the utterance and the inherent temporal structure of interactions between speakers to learn, in supervised framework, to identify speakers. Experiments on the Etape broadcast news benchmark validate the approach",
    "checked": true,
    "id": "3addc795cd508b0992cdd28f75894f6a01ce4933",
    "semantic_title": "combining speaker turn embedding and incremental structure prediction for low-latency speaker diarization",
    "citation_count": 18,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bredin17_interspeech.html": {
    "title": "pyannote.metrics: A Toolkit for Reproducible Evaluation, Diagnostic, and Error Analysis of Speaker Diarization Systems",
    "volume": "main",
    "abstract": "pyannote.metrics is an open-source Python library aimed at researchers working in the wide area of speaker diarization. It provides a command line interface (CLI) to improve reproducibility and comparison of speaker diarization research results. Through its application programming interface (API), a large set of evaluation metrics is available for diagnostic purposes of all modules of typical speaker diarization pipelines (speech activity detection, speaker change detection, clustering, and identification). Finally, thanks to visualization capabilities, we show that it can also be used for detailed error analysis purposes. pyannote.metrics can be downloaded from http://pyannote.github.io",
    "checked": true,
    "id": "e83c26cced73f7c5177695997e38b8b68d33fb68",
    "semantic_title": "pyannote.metrics: a toolkit for reproducible evaluation, diagnostic, and error analysis of speaker diarization systems",
    "citation_count": 64,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17n_interspeech.html": {
    "title": "A Rescoring Approach for Keyword Search Using Lattice Context Information",
    "volume": "main",
    "abstract": "In this paper we present a rescoring approach for keyword search (KWS) based on neural networks (NN). This approach exploits only the lattice context in a detected time interval instead of its corresponding audio. The most informative arcs in lattice context are selected and represented as a matrix, where words on arcs are represented in an embedding space with respect to their pronunciations. Then convolutional neural networks (CNNs) are employed to capture distinctive features from this matrix. A rescoring model is trained to minimize term-weighted sigmoid cross entropy so as to match the evaluation metric. Experiments on single-word queries show that lattice context brings complementary gains over normalized posterior scores. Performance on both in-vocabulary (IV) and out-of-vocabulary (OOV) queries are improved by combining NN-based scores with standard posterior scores",
    "checked": true,
    "id": "0b2219ff4ea1dc277276b6db8dced5adf415070a",
    "semantic_title": "a rescoring approach for keyword search using lattice context information",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/trmal17_interspeech.html": {
    "title": "The Kaldi OpenKWS System: Improving Low Resource Keyword Search",
    "volume": "main",
    "abstract": "The IARPA BABEL program has stimulated worldwide research in keyword search technology for low resource languages, and the NIST OpenKWS evaluations are the de facto benchmark test for such capabilities. The 2016 OpenKWS evaluation featured Georgian speech, and had 10 participants from across the world. This paper describes the Kaldi system developed to assist IARPA in creating a competitive baseline against which participants were evaluated, and to provide a truly open source system to all participants to support their research. This system handily met the BABEL program goals of 0.60 ATWV and 50% WER, achieving 0.70 ATWV and 38% WER with a single ASR system, i.e. without ASR system combination. All except one OpenKWS participant used Kaldi components in their submissions, typically in conjunction with system combination. This paper therefore complements all other OpenKWS-based papers",
    "checked": true,
    "id": "3119267d581fb65c3866ded0c194cfac76cc349a",
    "semantic_title": "the kaldi openkws system: improving low resource keyword search",
    "citation_count": 37,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/khokhlov17b_interspeech.html": {
    "title": "The STC Keyword Search System for OpenKWS 2016 Evaluation",
    "volume": "main",
    "abstract": "This paper describes the keyword search system developed by the STC team in the framework of OpenKWS 2016 evaluation. The acoustic modeling techniques included i-vectors based speaker adaptation, multilingual speaker-dependent bottleneck features, and a combination of feedforward and recurrent neural networks. To improve the language model, we augmented the training data provided by the organizers with texts generated by the character-level recurrent neural networks trained on different data sets. This led to substantial reductions in the out-of-vocabulary (OOV) and word error rates. The OOV search problem was solved with the help of a novel approach based on lattice generated phone posteriors and a highly optimized decoder. This approach outperformed familiar OOV search implementations in terms of speed and demonstrated comparable or better search quality The system was among the top three systems in the evaluation",
    "checked": true,
    "id": "3e9273372998ace0b454fc6d589bae2171480eaf",
    "semantic_title": "the stc keyword search system for openkws 2016 evaluation",
    "citation_count": 13,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sun17_interspeech.html": {
    "title": "Compressed Time Delay Neural Network for Small-Footprint Keyword Spotting",
    "volume": "main",
    "abstract": "In this paper we investigate a time delay neural network (TDNN) for a keyword spotting task that requires low CPU, memory and latency. The TDNN is trained with transfer learning and multi-task learning. Temporal subsampling enabled by the time delay architecture reduces computational complexity. We propose to apply singular value decomposition (SVD) to further reduce TDNN complexity. This allows us to first train a larger full-rank TDNN model which is not limited by CPU/memory constraints. The larger TDNN usually achieves better performance. Afterwards, its size can be compressed by SVD to meet the budget requirements. Hidden Markov models (HMM) are used in conjunction with the networks to perform keyword detection and performance is measured in terms of area under the curve (AUC) for detection error tradeoff (DET) curves. Our experimental results on a large in-house far-field corpus show that the full-rank TDNN achieves a 19.7% DET AUC reduction compared to a similar-size deep neural network (DNN) baseline. If we train a larger size full-rank TDNN first and then reduce it via SVD to the comparable size of the DNN, we obtain a 37.6% reduction in DET AUC compared to the DNN baseline",
    "checked": true,
    "id": "36a8a3fc95052d3a494ef3c77d898ed2da84a5a2",
    "semantic_title": "compressed time delay neural network for small-footprint keyword spotting",
    "citation_count": 108,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/suzuki17b_interspeech.html": {
    "title": "Symbol Sequence Search from Telephone Conversation",
    "volume": "main",
    "abstract": "We propose a method for searching for symbol sequences in conversations. Symbol sequences can include phone numbers, credit card numbers, and any kind of ticket (identification) numbers and are often communicated in call center conversations. Automatic extraction of these from speech is a key to many automatic speech recognition (ASR) applications such as question answering and summarization. Compared with spoken term detection (STD), symbol sequence searches have two additional problems. First, the entire symbol sequence is typically not observed continuously but in sub sequences, where customers or agents speak these sequences in fragments, while the recipient repeats them to ensure they have the correct sequence. Second, we have to distinguish between different symbol sequences, for example, phone numbers versus ticket numbers or customer identification numbers. To deal with these problems, we propose to apply STD to symbol-sequence fragments and subsequently use confidence scoring to obtain the entire symbol sequence. For the confidence scoring, We propose a long short-term memory (LSTM) based approach that inputs word before and after fragments. We also propose to detect repetitions of fragments and use it for confidence scoring. Our proposed method achieves a 0.87 F-measure, in an eight-digit customer identification number search task, when operating at 20.3% WER",
    "checked": true,
    "id": "09b630aa13ecabfd61e2dfc12864ba49e35a3004",
    "semantic_title": "symbol sequence search from telephone conversation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gundogdu17_interspeech.html": {
    "title": "Similarity Learning Based Query Modeling for Keyword Search",
    "volume": "main",
    "abstract": "In this paper, we propose a novel approach for query modeling using neural networks for posteriorgram based keyword search (KWS). We aim to help the conventional large vocabulary continuous speech recognition (LVCSR) based KWS systems, especially on out-of-vocabulary (OOV) terms by converting the task into a template matching problem, just like the query-by-example retrieval tasks. For this, we use a dynamic time warping (DTW) based similarity search on the speaker independent posteriorgram space. In order to model the text queries as posteriorgrams, we propose a non-symmetric Siamese neural network structure which both learns a distance measure to be used in DTW and the frame representations for this specific measure. We compare this new technique with similar DTW based systems using other distance measures and query modeling techniques. We also apply system fusion of the proposed system with the LVCSR based baseline KWS system. We show that, the proposed system works significantly better than other similar systems. Furthermore, when combined with the LVSCR based baseline, the proposed system provides up to 37.9% improvement on OOV terms and 9.8% improvement on all terms",
    "checked": true,
    "id": "b6df29f0e8b486c182d642a9372c210b57e0dbae",
    "semantic_title": "similarity learning based query modeling for keyword search",
    "citation_count": 11,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/samui17_interspeech.html": {
    "title": "Deep Recurrent Neural Network Based Monaural Speech Separation Using Recurrent Temporal Restricted Boltzmann Machines",
    "volume": "main",
    "abstract": "This paper presents a single-channel speech separation method implemented with a deep recurrent neural network (DRNN) using recurrent temporal restricted Boltzmann machines (RTRBM). Although deep neural network (DNN) based speech separation (denoising task) methods perform quite well compared to the conventional statistical model based speech enhancement techniques, in DNN-based methods, the temporal correlations across speech frames are often ignored, resulting in loss of spectral detail in the reconstructed output speech. In order to alleviate this issue, one RTRBM is employed for modelling the acoustic features of input (mixture) signal and two RTRBMs are trained for the two training targets (source signals). Each RTRBM attempts to model the abstractions present in the training data at each time step as well as the temporal dependencies in the training data. The entire network (consisting of three RTRBMs and one recurrent neural network) can be fine-tuned by the joint optimization of the DRNN with an extra masking layer which enforces a reconstruction constraint. The proposed method has been evaluated on the IEEE corpus and TIMIT dataset for speech denoising task. Experimental results have established that the proposed approach outperforms NMF and conventional DNN and DRNN-based speech enhancement methods",
    "checked": true,
    "id": "9048c2d06afa3d30320e5504c8e863a10949101f",
    "semantic_title": "deep recurrent neural network based monaural speech separation using recurrent temporal restricted boltzmann machines",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/huang17g_interspeech.html": {
    "title": "Improved Codebook-Based Speech Enhancement Based on MBE Model",
    "volume": "main",
    "abstract": "This paper provides an improved codebook-based speech enhancement method using multi-band excitation (MBE) model. It aims to remove the noise between the harmonics, which may exist in codebook-based enhanced speech. In general, the proposed system is based on analysis-with-synthesis (AwS) framework. During the analysis stage, acoustic features are extracted including pitch, harmonic magnitude and voicing from noisy speech. These parameters are obtained on the basis of the spectral magnitudes obtained by codebook-based method. During the synthesis stage, different synthesis strategies for voiced and unvoiced speech are employed. Besides, this paper introduces speech presence probability to modify the codebook-based Wiener filter so that more accurate acoustic parameters can be obtained. The proposed system can eliminate noise not only between the harmonics, but also in the silent segments, especially in low SNR noise environment. Experiments show that, the performance of the proposed method is better than traditional codebook-based method for different types of noise",
    "checked": true,
    "id": "e4667be1dbaa0ff3d163484e5a5583b7954949c7",
    "semantic_title": "improved codebook-based speech enhancement based on mbe model",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17o_interspeech.html": {
    "title": "Improving Mask Learning Based Speech Enhancement System with Restoration Layers and Residual Connection",
    "volume": "main",
    "abstract": "For single-channel speech enhancement, mask learning based approach through neural network has been shown to outperform the feature mapping approach, and to be effective as a pre-processor for automatic speech recognition. However, its assumption that the mixture and clean reference must have the correspondent scale doesn't hold in data collected from real world, and thus leads to significant performance degradation on parallel recorded data. In this paper, we first extend the mask learning based speech enhancement by integrating two types of restoration layer to address the scale mismatch problem. We further propose a novel residual learning based speech enhancement model via adding different shortcut connections to a feature mapping network. We show such a structure can benefit from both the mask learning and the feature mapping. We evaluate the proposed speech enhancement models on CHiME 3 data. Without retraining the acoustic model, the best bi-direction LSTM with residue connections yields 24.90% relative WER reduction on real data and 34.57% WER on simulated data",
    "checked": true,
    "id": "5bba9adb3cc3952fd01834321631e3f2d2ef9fee",
    "semantic_title": "improving mask learning based speech enhancement system with restoration layers and residual connection",
    "citation_count": 23,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/yan17_interspeech.html": {
    "title": "Exploring Low-Dimensional Structures of Modulation Spectra for Robust Speech Recognition",
    "volume": "main",
    "abstract": "Developments of noise robustness techniques are vital to the success of automatic speech recognition (ASR) systems in face of varying sources of environmental interference. Recent studies have shown that exploring low-dimensional structures of speech features can yield good robustness. Along this vein, research on low-rank representation (LRR), which considers the intrinsic structures of speech features lying on some low dimensional subspaces, has gained considerable interest from the ASR community. When speech features are contaminated with various types of environmental noise, its corresponding modulation spectra can be regarded as superpositions of unstructured sparse noise over the inherent linguistic information. As such, we in this paper endeavor to explore the low dimensional structures of modulation spectra, in the hope to obtain more noise-robust speech features. The main contribution is that we propose a novel use of the LRR-based method to discover the subspace structures of modulation spectra, thereby alleviating the negative effects of noise interference. Furthermore, we also extensively compare our approach with several well-practiced feature-based normalization methods. All experiments were conducted and verified on the Aurora-4 database and task. The empirical results show that the proposed LRR-based method can provide significant word error reductions for a typical DNN-HMM hybrid ASR system",
    "checked": true,
    "id": "fcf735a67409b32eb2fedf0a25d80a59c3eb7613",
    "semantic_title": "exploring low-dimensional structures of modulation spectra for robust speech recognition",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/pascual17_interspeech.html": {
    "title": "SEGAN: Speech Enhancement Generative Adversarial Network",
    "volume": "main",
    "abstract": "Current speech enhancement techniques operate on the spectral domain and/or exploit some higher-level feature. The majority of them tackle a limited number of noise conditions and rely on first-order statistics. To circumvent these issues, deep networks are being increasingly used, thanks to their ability to learn complex functions from large example sets. In this work, we propose the use of generative adversarial networks for speech enhancement. In contrast to current techniques, we operate at the waveform level, training the model end-to-end, and incorporate 28 speakers and 40 different noise conditions into the same model, such that model parameters are shared across them. We evaluate the proposed model using an independent, unseen test set with two speakers and 20 alternative noise conditions. The enhanced samples confirm the viability of the proposed model, and both objective and subjective evaluations confirm the effectiveness of it. With that, we open the exploration of generative architectures for speech enhancement, which may progressively incorporate further speech-centric design choices to improve their performance",
    "checked": true,
    "id": "f8d43ff00585c53f65eb04e15477113c2d2b758b",
    "semantic_title": "segan: speech enhancement generative adversarial network",
    "citation_count": 1007,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/maiti17_interspeech.html": {
    "title": "Concatenative Resynthesis Using Twin Networks",
    "volume": "main",
    "abstract": "Traditional noise reduction systems modify a noisy signal to make it more like the original clean signal. For speech, these methods suffer from two main problems: under-suppression of noise and over-suppression of target speech. Instead, synthesizing clean speech based on the noisy signal could produce outputs that are both noise-free and high quality. Our previous work introduced such a system using concatenative synthesis, but it required processing the clean speech at run time, which was slow and not scalable. In order to make such a system scalable, we propose here learning a similarity metric using two separate networks, one network processing the clean segments offline and another processing the noisy segments at run time. This system incorporates a ranking loss to optimize for the retrieval of appropriate clean speech segments. This model is compared against our original on the CHiME2-GRID corpus, measuring ranking performance and subjective listening tests of resyntheses",
    "checked": true,
    "id": "cf34bda14a2f6148f330213e0ec0dbfe3062c959",
    "semantic_title": "concatenative resynthesis using twin networks",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/stafylakis17_interspeech.html": {
    "title": "Combining Residual Networks with LSTMs for Lipreading",
    "volume": "main",
    "abstract": "We propose an end-to-end deep learning architecture for word-level visual speech recognition. The system is a combination of spatiotemporal convolutional, residual and bidirectional Long Short-Term Memory networks. We train and evaluate it on the Lipreading In-The-Wild benchmark, a challenging database of 500-size target-words consisting of 1.28sec video excerpts from BBC TV broadcasts. The proposed network attains word accuracy equal to 83.0%, yielding 6.8% absolute improvement over the current state-of-the-art, without using information about word boundaries during training or testing",
    "checked": true,
    "id": "4afdb836301c0233bb8cf0d8a33212ac0c113381",
    "semantic_title": "combining residual networks with lstms for lipreading",
    "citation_count": 256,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/thangthai17_interspeech.html": {
    "title": "Improving Computer Lipreading via DNN Sequence Discriminative Training Techniques",
    "volume": "main",
    "abstract": "Although there have been some promising results in computer lipreading, there has been a paucity of data on which to train automatic systems. However the recent emergence of the TCD-TIMIT corpus, with around 6000 words, 59 speakers and seven hours of recorded audio-visual speech, allows the deployment of more recent techniques in audio-speech such as Deep Neural Networks (DNNs) and sequence discriminative training In this paper we combine the DNN with a Hidden Markov Model (HMM) to the, so called, hybrid DNN-HMM configuration which we train using a variety of sequence discriminative training methods. This is then followed with a weighted finite state transducer. The conclusion is that the DNN offers very substantial improvement over a conventional classifier which uses a Gaussian Mixture Model (GMM) to model the densities even when optimised with Speaker Adaptive Training. Sequence adaptive training offers further improvements depending on the precise variety employed but those improvements are of the order of 10% improvement in word accuracy. Putting these two results together implies that lipreading is moving from something of rather esoteric interest to becoming a practical reality in the foreseeable future",
    "checked": true,
    "id": "139f53c9371dc76bebd708b862f08ef5a1a8f3b5",
    "semantic_title": "improving computer lipreading via dnn sequence discriminative training techniques",
    "citation_count": 19,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wand17_interspeech.html": {
    "title": "Improving Speaker-Independent Lipreading with Domain-Adversarial Training",
    "volume": "main",
    "abstract": "We present a Lipreading system, i.e. a speech recognition system using only visual features, which uses domain-adversarial training for speaker independence. Domain-adversarial training is integrated into the optimization of a lipreader based on a stack of feedforward and LSTM (Long Short-Term Memory) recurrent neural networks, yielding an end-to-end trainable system which only requires a very small number of frames of untranscribed target data to substantially improve the recognition accuracy on the target speaker. On pairs of different source and target speakers, we achieve a relative accuracy improvement of around 40% with only 15 to 20 seconds of untranscribed target speech data. On multi-speaker training setups, the accuracy improvements are smaller but still substantial",
    "checked": true,
    "id": "bd0d116ba34a29cbd50c636445a93ce78c059f99",
    "semantic_title": "improving speaker-independent lipreading with domain-adversarial training",
    "citation_count": 37,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/abdelaziz17_interspeech.html": {
    "title": "Turbo Decoders for Audio-Visual Continuous Speech Recognition",
    "volume": "main",
    "abstract": "Visual speech, i.e., video recordings of speakers' mouths, plays an important role in improving the robustness properties of automatic speech recognition (ASR) against noise. Optimal fusion of audio and video modalities is still one of the major challenges that attracts significant interest in the realm of audio-visual ASR. Recently, turbo decoders (TDs) have been successful in addressing the audio-visual fusion problem. The idea of the TD framework is to iteratively exchange some kind of soft information between the audio and video decoders until convergence. The forward-backward algorithm (FBA) is mostly applied to the decoding graphs to estimate this soft information. Applying the FBA to the complex graphs that are usually used in large vocabulary tasks may be computationally expensive. In this paper, I propose to apply the forward-backward algorithm to a lattice of most likely state sequences instead of using the entire decoding graph. Using lattices allows for TD to be easily applied to large vocabulary tasks. The proposed approach is evaluated using the newly released TCD-TIMIT corpus, where a standard recipe for large vocabulary ASR is employed. The modified TD performs significantly better than the feature and decision fusion models in all clean and noisy test conditions",
    "checked": true,
    "id": "8dbb7b6fbb35fd566a163498061075f37d234674",
    "semantic_title": "turbo decoders for audio-visual continuous speech recognition",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/csapo17_interspeech.html": {
    "title": "DNN-Based Ultrasound-to-Speech Conversion for a Silent Speech Interface",
    "volume": "main",
    "abstract": "In this paper we present our initial results in articulatory-to-acoustic conversion based on tongue movement recordings using Deep Neural Networks (DNNs). Despite the fact that deep learning has revolutionized several fields, so far only a few researchers have applied DNNs for this task. Here, we compare various possible feature representation approaches combined with DNN-based regression. As the input, we recorded synchronized 2D ultrasound images and speech signals. The task of the DNN was to estimate Mel-Generalized Cepstrum-based Line Spectral Pair (MGC-LSP) coefficients, which then served as input to a standard pulse-noise vocoder for speech synthesis. As the raw ultrasound images have a relatively high resolution, we experimented with various feature selection and transformation approaches to reduce the size of the feature vectors. The synthetic speech signals resulting from the various DNN configurations were evaluated both using objective measures and a subjective listening test. We found that the representation that used several neighboring image frames in combination with a feature selection method was preferred both by the subjects taking part in the listening experiments, and in terms of the Normalized Mean Squared Error. Our results may be useful for creating Silent Speech Interface applications in the future",
    "checked": true,
    "id": "5af869961ff8e3243d7b3408f0576c8011ed75b5",
    "semantic_title": "dnn-based ultrasound-to-speech conversion for a silent speech interface",
    "citation_count": 55,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kamper17_interspeech.html": {
    "title": "Visually Grounded Learning of Keyword Prediction from Untranscribed Speech",
    "volume": "main",
    "abstract": "During language acquisition, infants have the benefit of visual cues to ground spoken language. Robots similarly have access to audio and visual sensors. Recent work has shown that images and spoken captions can be mapped into a meaningful common space, allowing images to be retrieved using speech and vice versa. In this setting of images paired with untranscribed spoken captions, we consider whether computer vision systems can be used to obtain textual labels for the speech. Concretely, we use an image-to-words multi-label visual classifier to tag images with soft textual labels, and then train a neural network to map from the speech to these soft targets. We show that the resulting speech system is able to predict which words occur in an utterance — acting as a spoken bag-of-words classifier — without seeing any parallel speech and text. We find that the model often confuses semantically related words, e.g. \"man\" and \"person\", making it even more effective as a semantic keyword spotter",
    "checked": true,
    "id": "93f46618df0a6cd81d9e42361082e7a80fc9adf7",
    "semantic_title": "visually grounded learning of keyword prediction from untranscribed speech",
    "citation_count": 61,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chien17d_interspeech.html": {
    "title": "Deep Neural Factorization for Speech Recognition",
    "volume": "main",
    "abstract": "Conventional speech recognition system is constructed by unfolding the spectral-temporal input matrices into one-way vectors and using these vectors to estimate the affine parameters of neural network according to the vector-based error back-propagation algorithm. System performance is constrained because the contextual correlations in frequency and time horizons are disregarded and the spectral and temporal factors are excluded. This paper proposes a spectral-temporal factorized neural network (STFNN) to tackle this weakness. The spectral-temporal structure is preserved and factorized in hidden layers through two ways of factor matrices which are trained by using the factorized error backpropagation. Affine transformation in standard neural network is generalized to the spectro-temporal factorization in STFNN. The structural features or patterns are extracted and forwarded towards the softmax outputs. A deep neural factorization is built by cascading a number of factorization layers with fully-connected layers for speech recognition. An orthogonal constraint is imposed in factor matrices for redundancy reduction. Experimental results show the merit of integrating the factorized features in deep feedforward and recurrent neural networks for speech recognition",
    "checked": true,
    "id": "9aba8c2eed05294f8c23079e03e30b046571c328",
    "semantic_title": "deep neural factorization for speech recognition",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/vesely17_interspeech.html": {
    "title": "Semi-Supervised DNN Training with Word Selection for ASR",
    "volume": "main",
    "abstract": "Not all the questions related to the semi-supervised training of hybrid ASR system with DNN acoustic model were already deeply investigated. In this paper, we focus on the question of the granularity of confidences (per-sentence, per-word, per-frame), the question of how the data should be used (data-selection by masks, or in mini-batch SGD with confidences as weights). Then, we propose to re-tune the system with the manually transcribed data, both with the ‘frame CE' training and ‘sMBR' training Our preferred semi-supervised recipe which is both simple and efficient is following: we select words according to the word accuracy we obtain on the development set. Such recipe, which does not rely on a grid-search of the training hyper-parameter, generalized well for: Babel Vietnamese (transcribed 11h, untranscribed 74h), Babel Bengali (transcribed 11h, untranscribed 58h) and our custom Switchboard setup (transcribed 14h, untranscribed 95h). We obtained the absolute WER improvements 2.5% for Vietnamese, 2.3% for Bengali and 3.2% for Switchboard",
    "checked": true,
    "id": "28a2bd5befecf2d09eaa86a5631800a0f1367139",
    "semantic_title": "semi-supervised dnn training with word selection for asr",
    "citation_count": 21,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hou17b_interspeech.html": {
    "title": "Gaussian Prediction Based Attention for Online End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "Recently end-to-end speech recognition has obtained much attention. One of the popular models to achieve end-to-end speech recognition is attention based encoder-decoder model, which usually generating output sequences iteratively by attending the whole representations of the input sequences. However, predicting outputs until receiving the whole input sequence is not practical for online or low time latency speech recognition. In this paper, we present a simple but effective attention mechanism which can make the encoder-decoder model generate outputs without attending the entire input sequence and can apply to online speech recognition. At each prediction step, the attention is assumed to be a time-moving gaussian window with variable size and can be predicted by using previous input and output information instead of the content based computation on the whole input sequence. To further improve the online performance of the model, we employ deep convolutional neural networks as encoder. Experiments show that the gaussian prediction based attention works well and under the help of deep convolutional neural networks the online model achieves 19.5% phoneme error rate in TIMIT ASR task",
    "checked": true,
    "id": "6e8b1bd63e0e33fe633d00742560de1a4ea8e30f",
    "semantic_title": "gaussian prediction based attention for online end-to-end speech recognition",
    "citation_count": 30,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/fukuda17_interspeech.html": {
    "title": "Efficient Knowledge Distillation from an Ensemble of Teachers",
    "volume": "main",
    "abstract": "This paper describes the effectiveness of knowledge distillation using teacher student training for building accurate and compact neural networks. We show that with knowledge distillation, information from multiple acoustic models like very deep VGG networks and Long Short-Term Memory (LSTM) models can be used to train standard convolutional neural network (CNN) acoustic models for a variety of systems requiring a quick turnaround. We examine two strategies to leverage multiple teacher labels for training student models. In the first technique, the weights of the student model are updated by switching teacher labels at the minibatch level. In the second method, student models are trained on multiple streams of information from various teacher distributions via data augmentation. We show that standard CNN acoustic models can achieve comparable recognition accuracy with much smaller number of model parameters compared to teacher VGG and LSTM acoustic models. Additionally we also investigate the effectiveness of using broadband teacher labels as privileged knowledge for training better narrowband acoustic models within this framework. We show the benefit of this simple technique by training narrowband student models with broadband teacher soft labels on the Aurora 4 task",
    "checked": true,
    "id": "86dc692fc0b6ee97077ae4132517cb8538802bcc",
    "semantic_title": "efficient knowledge distillation from an ensemble of teachers",
    "citation_count": 178,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/prabhavalkar17b_interspeech.html": {
    "title": "An Analysis of \"Attention\" in Sequence-to-Sequence Models",
    "volume": "main",
    "abstract": "In this paper, we conduct a detailed investigation of attention-based models for automatic speech recognition (ASR). First, we explore different types of attention, including \"online\" and \"full-sequence\" attention. Second, we explore different subword units to see how much of the end-to-end ASR process can reasonably be captured by an attention model. In experimental evaluations, we find that although attention is typically focused over a small region of the acoustics during each step of next label prediction, \"full-sequence\" attention outperforms \"online\" attention, although this gap can be significantly reduced by increasing the length of the segments over which attention is computed. Furthermore, we find that context-independent phonemes are a reasonable sub-word unit for attention models. When used in the second-pass to rescore N-best hypotheses, these models provide over a 10% relative improvement in word error rate",
    "checked": true,
    "id": "d30445cba38b3e8cbc553909df2d905f19e23d7b",
    "semantic_title": "an analysis of \"attention\" in sequence-to-sequence models",
    "citation_count": 43,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/soltau17_interspeech.html": {
    "title": "Neural Speech Recognizer: Acoustic-to-Word LSTM Model for Large Vocabulary Speech Recognition",
    "volume": "main",
    "abstract": "We present results that show it is possible to build a competitive, greatly simplified, large vocabulary continuous speech recognition system with whole words as acoustic units. We model the output vocabulary of about 100,000 words directly using deep bi-directional LSTM RNNs with CTC loss. The model is trained on 125,000 hours of semi-supervised acoustic training data, which enables us to alleviate the data sparsity problem for word models. We show that the CTC word models work very well as an end-to-end all-neural speech recognition model without the use of traditional context-dependent sub-word phone units that require a pronunciation lexicon, and without any language model removing the need to decode. We demonstrate that the CTC word models perform better than a strong, more complex, state-of-the-art baseline with sub-word units",
    "checked": true,
    "id": "b1cb867270f87f96397cb5f0d76cbb58cdf2c2f2",
    "semantic_title": "neural speech recognizer: acoustic-to-word lstm model for large vocabulary speech recognition",
    "citation_count": 302,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/guo17c_interspeech.html": {
    "title": "CNN-Based Joint Mapping of Short and Long Utterance i-Vectors for Speaker Verification Using Short Utterances",
    "volume": "main",
    "abstract": "Text-independent speaker recognition using short utterances is a highly challenging task due to the large variation and content mismatch between short utterances. I-vector and probabilistic linear discriminant analysis (PLDA) based systems have become the standard in speaker verification applications, but they are less effective with short utterances. To address this issue, we propose a novel method, which trains a convolutional neural network (CNN) model to map the i-vectors extracted from short utterances to the corresponding long-utterance i-vectors. In order to simultaneously learn the representation of the original short-utterance i-vectors and fit the target long-version i-vectors, we jointly train a supervised-regression model with an autoencoder using CNNs. The trained CNN model is then used to generate the mapped version of short-utterance i-vectors in the evaluation stage. We compare our proposed CNN-based joint mapping method with a GMM-based joint modeling method under matched and mismatched PLDA training conditions. Experimental results using the NIST SRE 2008 dataset show that the proposed technique achieves up to 30% relative improvement under duration mismatched PLDA-training conditions and outperforms the GMM-based method. The improved systems also perform better compared with the matched-length PLDA training condition using short utterances",
    "checked": true,
    "id": "51cdac34ba20d1ce2b8a4b84bf80236165bb0f96",
    "semantic_title": "cnn-based joint mapping of short and long utterance i-vectors for speaker verification using short utterances",
    "citation_count": 12,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ranjan17b_interspeech.html": {
    "title": "Curriculum Learning Based Probabilistic Linear Discriminant Analysis for Noise Robust Speaker Recognition",
    "volume": "main",
    "abstract": "This study introduces a novel Curriculum Learning based Probabilistic Linear Discriminant Analysis (CL-PLDA) algorithm for improving speaker recognition in noisy conditions. CL-PLDA operates by initializing the training EM algorithm with cleaner data ( easy examples), and successively adds noisier data ( difficult examples) as the training progresses. This curriculum learning based approach guides the parameters of CL-PLDA to better local minima compared to regular PLDA. We test CL-PLDA on speaker verification task of the severely noisy and degraded DARPA RATS data, and show it to significantly outperform regular PLDA across test-sets of varying duration",
    "checked": true,
    "id": "c47d15731d48199ee46d046ef4b1f9b50c4d8a2f",
    "semantic_title": "curriculum learning based probabilistic linear discriminant analysis for noise robust speaker recognition",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mahto17_interspeech.html": {
    "title": "i-Vector Transformation Using a Novel Discriminative Denoising Autoencoder for Noise-Robust Speaker Recognition",
    "volume": "main",
    "abstract": "This paper proposes i-vector transformations using neural networks for achieving noise-robust speaker recognition. A novel discriminative denoising autoencoder (DDAE) is employed on i-vectors to remove additive noise effects. The DDAE is trained to denoise and classify noisy i-vectors simultaneously, making it possible to add discriminability to the denoised i-vectors. Speaker recognition experiments on the NIST SRE 2012 task shows 32% better error performance as compared to a baseline system. Also, our proposed method outperforms such conventional methods as multi-condition training and a basic denoising autoencoder",
    "checked": true,
    "id": "7cd93876bde524a5ac6f69c6fc39b35e79a86395",
    "semantic_title": "i-vector transformation using a novel discriminative denoising autoencoder for noise-robust speaker recognition",
    "citation_count": 14,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wang17l_interspeech.html": {
    "title": "Unsupervised Discriminative Training of PLDA for Domain Adaptation in Speaker Verification",
    "volume": "main",
    "abstract": "This paper presents, for the first time, unsupervised discriminative training of probabilistic linear discriminant analysis (unsupervised DT-PLDA). While discriminative training avoids the problem of generative training based on probabilistic model assumptions that often do not agree with actual data, it has been difficult to apply it to unsupervised scenarios because it can fit data with almost any labels. This paper focuses on unsupervised training of DT-PLDA in the application of domain adaptation in i-vector based speaker verification systems, using unlabeled in-domain data. The proposed method makes it possible to conduct discriminative training, i.e., estimation of model parameters and unknown labels, by employing data statistics as a regularization term in addition to the original objective function in DT-PLDA. An experiment on a NIST Speaker Recognition Evaluation task shows that the proposed method outperforms a conventional method using speaker clustering and performs almost as well as supervised DT-PLDA",
    "checked": true,
    "id": "53159572d2253979c07dc34815960f76bf25fa60",
    "semantic_title": "unsupervised discriminative training of plda for domain adaptation in speaker verification",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/alam17b_interspeech.html": {
    "title": "Speaker Verification Under Adverse Conditions Using i-Vector Adaptation and Neural Networks",
    "volume": "main",
    "abstract": "The main challenges introduced in the 2016 NIST speaker recognition evaluation (SRE16) are domain mismatch between training and evaluation data, duration variability in test recordings and unlabeled in-domain training data. This paper outlines the systems developed at CRIM for SRE16. To tackle the domain mismatch problem, we apply minimum divergence training to adapt a conventional i-vector extractor to the task domain. Specifically, we take an out-of-domain trained i-vector extractor as an initialization and perform few iterations of minimum divergence training on the unlabeled data provided. Next, we non-linearly transform the adapted i-vectors by learning a speaker classifier neural network. Speaker features extracted from this network have been shown to be more robust than i-vectors under domain mismatch conditions with a reduction in equal error rates of 2–3% absolute. Finally, we propose a new Beta-Bernoulli backend that models the features supplied by the speaker classifier network. Our best single system is the speaker classifier network - Beta-Bernoulli backend combination. Overall system performance was very satisfactory for the fixed condition task. With our submitted fused system we achieve an equal error rate of 9.89%",
    "checked": true,
    "id": "3e85a2d3b15f15b2a1e1c922f2620572c5fcf1a7",
    "semantic_title": "speaker verification under adverse conditions using i-vector adaptation and neural networks",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/castan17_interspeech.html": {
    "title": "Improving Robustness of Speaker Recognition to New Conditions Using Unlabeled Data",
    "volume": "main",
    "abstract": "Unsupervised techniques for the adaptation of speaker recognition are important due to the problem of condition mismatch that is prevalent when applying speaker recognition technology to new conditions and the general scarcity of labeled ‘in-domain' data. In the recent NIST 2016 Speaker Recognition Evaluation (SRE), symmetric score normalization (S-norm) and calibration using unlabeled in-domain data were shown to be beneficial. Because calibration requires speaker labels for training, speaker-clustering techniques were used to generate pseudo-speakers for learning calibration parameters in those cases where only unlabeled in-domain data was available. These methods performed well in the SRE16. It is unclear, however, whether those techniques generalize well to other data sources. In this work, we benchmark these approaches on several distinctly different databases, after we describe our SRI-CON-UAM team system submission for the NIST 2016 SRE. Our analysis shows that while the benefit of S-norm is also observed across other datasets, applying speaker-clustered calibration provides considerably greater benefit to the system in the context of new acoustic conditions",
    "checked": true,
    "id": "3a8e1f43391b5808a325aa07893c7693fcb907cf",
    "semantic_title": "improving robustness of speaker recognition to new conditions using unlabeled data",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/abidi17_interspeech.html": {
    "title": "CALYOU: A Comparable Spoken Algerian Corpus Harvested from YouTube",
    "volume": "main",
    "abstract": "This paper addresses the issue of comparability of comments extracted from Youtube. The comments concern spoken Algerian that could be either local Arabic, Modern Standard Arabic or French. This diversity of expression gives rise to a huge number of problems concerning the data processing. In this article, several methods of alignment will be proposed and tested. The method which permits to best align is Word2Vec-based approach that will be used iteratively. This recurrent call of Word2Vec allows us improve significantly the results of comparability. In fact, a dictionary-based approach leads to a Recall of 4, while our approach allows one to get a Recall of 33 at rank 1. Thanks to this approach, we built from Youtube CALYOU, a Comparable Corpus of the spoken Algerian",
    "checked": true,
    "id": "aedb7f574fd123b36a5c50d1aae4c8af07a71f29",
    "semantic_title": "calyou: a comparable spoken algerian corpus harvested from youtube",
    "citation_count": 21,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/narwekar17_interspeech.html": {
    "title": "PRAV: A Phonetically Rich Audio Visual Corpus",
    "volume": "main",
    "abstract": "This paper describes the acquisition of PRAV, a phonetically rich audio-visual Corpus. The PRAV Corpus contains audio as well as visual recordings of 2368 sentences from the TIMIT corpus each spoken by four subjects, making it the largest audio-visual corpus in the literature in terms of the number of sentences per subject. Visual features, comprising the coordinates of points along the contour of the subjects lips, have been extracted for the entire PRAV Corpus using the Active Appearance Models (AAM) algorithm and have been made available along with the audio and video recordings. The subjects being Indian makes PRAV an ideal resource for audio-visual speech study with non-native English speakers. Moreover, this paper describes how the large number of sentences per subject makes the PRAV Corpus a significant dataset by highlighting its utility in exploring a number of potential research problems including visual speech synthesis and perception studies",
    "checked": true,
    "id": "4346f73ecbaf688cccc7d694d92a9a2c512c6786",
    "semantic_title": "prav: a phonetically rich audio visual corpus",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/abdelaziz17b_interspeech.html": {
    "title": "NTCD-TIMIT: A New Database and Baseline for Noise-Robust Audio-Visual Speech Recognition",
    "volume": "main",
    "abstract": "Although audio-visual speech is well known to improve the robustness properties of automatic speech recognition (ASR) systems against noise, the realm of audio-visual ASR (AV-ASR) has not gathered the research momentum it deserves. This is mainly due to the lack of audio-visual corpora and the need to combine two fields of knowledge: ASR and computer vision. This paper describes the NTCD-TIMIT database and baseline that can overcome these two barriers and attract more research interest to AV-ASR. The NTCD-TIMIT corpus has been created by adding six noise types at a range of signal-to-noise ratios to the speech material of the recently published TCD-TIMIT corpus. NTCD-TIMIT comprises visual features that have been extracted from the TCD-TIMIT video recordings using the visual front-end presented in this paper. The database contains also Kaldi scripts for training and decoding audio-only, video-only, and audio-visual ASR models. The baseline experiments and results obtained using these scripts are detailed in this paper",
    "checked": true,
    "id": "0c6131f3efeb4b9a1164d5b5bad95af5d61a5440",
    "semantic_title": "ntcd-timit: a new database and baseline for noise-robust audio-visual speech recognition",
    "citation_count": 25,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/howcroft17_interspeech.html": {
    "title": "The Extended SPaRKy Restaurant Corpus: Designing a Corpus with Variable Information Density",
    "volume": "main",
    "abstract": "Natural language generation (NLG) systems rely on corpora for both hand-crafted approaches in a traditional NLG architecture and for statistical end-to-end (learned) generation systems. Limitations in existing resources, however, make it difficult to develop systems which can vary the linguistic properties of an utterance as needed. For example, when users' attention is split between a linguistic and a secondary task such as driving, a generation system may need to reduce the information density of an utterance to compensate for the reduction in user attention We introduce a new corpus in the restaurant recommendation and comparison domain, collected in a paraphrasing paradigm, where subjects wrote texts targeting either a general audience or an elderly family member. This design resulted in a corpus of more than 5000 texts which exhibit a variety of lexical and syntactic choices and differ with respect to average word & sentence length and surprisal. The corpus includes two levels of meaning representation: flat ‘semantic stacks' for propositional content and Rhetorical Structure Theory (RST) relations between these propositions",
    "checked": true,
    "id": "ae5e78337ace70f57997b65852250305de7c0196",
    "semantic_title": "the extended sparky restaurant corpus: designing a corpus with variable information density",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mansikkaniemi17_interspeech.html": {
    "title": "Automatic Construction of the Finnish Parliament Speech Corpus",
    "volume": "main",
    "abstract": "Automatic speech recognition (ASR) systems require large amounts of transcribed speech data, for training state-of-the-art deep neural network (DNN) acoustic models. Transcribed speech is a scarce and expensive resource, and ASR systems are prone to underperform in domains where there is not a lot of training data available. In this work, we open up a vast and previously unused resource of transcribed speech for Finnish, by retrieving and aligning all the recordings and meeting transcripts from the web portal of the Parliament of Finland. Short speech-text segment pairs are retrieved from the audio and text material, by using the Levenshtein algorithm to align the first-pass ASR hypotheses with the corresponding meeting transcripts. DNN acoustic models are trained on the automatically constructed corpus, and performance is compared to other models trained on a commercially available speech corpus. Model performance is evaluated on Finnish parliament speech, by dividing the testing set into seen and unseen speakers. Performance is also evaluated on broadcast speech to test the general applicability of the parliament speech corpus. We also study the use of meeting transcripts in language model adaptation, to achieve additional gains in speech recognition accuracy of Finnish parliament speech",
    "checked": true,
    "id": "e4e10c155822d39d71e3fecde0f4c95a298f0560",
    "semantic_title": "automatic construction of the finnish parliament speech corpus",
    "citation_count": 35,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/abdo17_interspeech.html": {
    "title": "Building Audio-Visual Phonetically Annotated Arabic Corpus for Expressive Text to Speech",
    "volume": "main",
    "abstract": "The present research aims to build an MSA audio-visual corpus. The corpus is annotated both phonetically and visually and dedicated to emotional speech processing studies. The building of the corpus consists of 5 main stages: speaker selection, sentences selection, recording, annotation and evaluation. 500 sentences were critically selected based on their phonemic distribution. The speaker was instructed to read the same 500 sentences with 6 emotions (Happiness – Sadness – Fear – Anger – Inquiry – Neutral). A sample of 50 sentences was selected for annotation. The corpus evaluation modules were: audio, visual and audio-visual subjective evaluation The corpus evaluation process showed that happy, anger and inquiry emotions were better recognized visually (94%, 96% and 96%) than audibly (63.6%, 74% and 74%) and the audio visual evaluation scores (96%, 89.6% and 80.8%). Sadness and fear emotion on the other hand were better recognized audibly (76.8% and 97.6%) than visually (58% and 78.8 %) and the audio visual evaluation scores were (65.6% and 90%)",
    "checked": true,
    "id": "1b29d4e3102694208f3bb1741c350ab80f7ec860",
    "semantic_title": "building audio-visual phonetically annotated arabic corpus for expressive text to speech",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hughes17_interspeech.html": {
    "title": "What is the Relevant Population? Considerations for the Computation of Likelihood Ratios in Forensic Voice Comparison",
    "volume": "main",
    "abstract": "In forensic voice comparison, it is essential to consider not only the similarity between samples, but also the typicality of the evidence in the relevant population. This is explicit within the likelihood ratio (LR) framework. A significant issue, however, is the definition of the relevant population. This paper explores the complexity of population selection for voice evidence. We evaluate the effects of population specificity in terms of regional background on LR output using combinations of the F1, F2, and F3 trajectories of the diphthong /aɪ/. LRs were computed using development and reference data which were regionally matched (Standard Southern British English) and mixed (general British English) relative to the test data. These conditions reflect the paradox that without knowing who the offender is, it is not possible to know the population of which he is a member. Results show that the more specific population produced stronger evidence and better system validity than the more general definition. However, as region-specific voice features (lower formants) were removed, the difference in the output from the matched and mixed systems was reduced. This shows that the effects of population selection are dependent on the sociolinguistic constraints on the feature analysed",
    "checked": true,
    "id": "6c4f77cb9649831b2a12b1947554a28ef80eee55",
    "semantic_title": "what is the relevant population? considerations for the computation of likelihood ratios in forensic voice comparison",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/delvaux17_interspeech.html": {
    "title": "Voice Disguise vs. Impersonation: Acoustic and Perceptual Measurements of Vocal Flexibility in Non Experts",
    "volume": "main",
    "abstract": "The aim of this study is to assess the potential for deliberately changing one's voice as a means to conceal or falsify identity, comparing acoustic and perceptual measurements of carefully controlled speech productions Twenty-two non expert speakers read a phonetically-balanced text 5 times in various conditions including natural speech, free vocal disguise (2 disguises per speaker), impersonation of a common target for all speakers, impersonation of one specific target per speaker. Long-term average spectra (LTAS) were computed for each reading and multiple pairwise comparisons were performed using the SDDD dissimilarity index The acoustic analysis showed that all speakers were able to deliberately change their voice beyond self-typical natural variation, whether in attempting to simply disguise their identity or to impersonate a specific target. Although the magnitude of the acoustic changes was comparable in disguise vs. impersonation, overall it was limited in that it did not achieved between-speaker variation levels. Perceptual judgements performed on the same material revealed that naive listeners were better at discriminating between impersonators and targets than at simply detecting voice disguise",
    "checked": true,
    "id": "0278ae1ea297c2b352e2d5c809bf4f9b3d00c575",
    "semantic_title": "voice disguise vs. impersonation: acoustic and perceptual measurements of vocal flexibility in non experts",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wu17g_interspeech.html": {
    "title": "Schwa Realization in French: Using Automatic Speech Processing to Study Phonological and Socio-Linguistic Factors in Large Corpora",
    "volume": "main",
    "abstract": "The study investigates different factors influencing schwa realization in French: phonological factors, speech style, gender, and socio-professional status. Three large corpora, two of public journalistic speech (ESTER and ETAPE) and one of casual speech (NCCFr) are used. The absence/presence of schwa is automatically decided via forced alignment, which has a successful performance rate of 95%. Only polysyllabic words including a potential schwa in the word-initial syllable are studied in order to control for variability in word structure and position. The effect of the left context, grouped into classes of a word final vowel or final consonant or a pause, is studied. Words preceded by a vowel (V#) tend to favor schwa deletion. Interestingly, words preceded by a consonant or a pause have similar behaviors: speakers tend to maintain schwa in both contexts. As can be expected, the more casual the speech, the more frequently schwa is dropped. Males tend to delete more schwas than females, and journalists are more likely to delete schwa than politicians. These results suggest that beyond phonology, other factors such as gender, style and socio-professional status influence the realization of schwa",
    "checked": true,
    "id": "c0a380aa2edb0dd1ef3fd7263f8d46ab83fe157f",
    "semantic_title": "schwa realization in french: using automatic speech processing to study phonological and socio-linguistic factors in large corpora",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/duran17_interspeech.html": {
    "title": "The Social Life of Setswana Ejectives",
    "volume": "main",
    "abstract": "This paper presents a first phonetic analysis of voiced, devoiced and ejectivized stop sounds in Setswana taken from two different speech databases. It is observed that rules governing the voicing/devoicing processes depend on sociophonetic and ethnolinguistic factors. Speakers, especially women, from the rural North West area of South Africa tend to preserve the phonologically stronger devoiced (or even ejectivized) forms, both in single standing plosives as well as in the post-nasal context (NC̥). On the other hand, in the more industrialized area of Gauteng, voiced forms of plosives prevail. The empirically observed data is modelled with KaMoso, a computational multi-agent simulation framework. So far, this framework focused on open social structures ( whole world networks) that facilitate language modernization through exchange between different phonetic forms. The updated model has been enriched with social/phonetic simulation scenarios in which speech agents interact between each other in a so-called parochial setting, reflecting smaller, closed communities. Both configurations correspond to the sociopolitical changes that have been taking place in South Africa over the last decades, showing the differences in speech between women and men from rural and industrialized areas of the country",
    "checked": true,
    "id": "1028278a7b3ac2c7de8f68bf17e6af6edded8162",
    "semantic_title": "the social life of setswana ejectives",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kohtz17_interspeech.html": {
    "title": "How Long is Too Long? How Pause Features After Requests Affect the Perceived Willingness of Affirmative Answers",
    "volume": "main",
    "abstract": "A perception experiment involving 28 German listeners is presented. It investigates — for sequences of request, pause, and affirmative answer — the effect of pause duration on the answerer's perceived willingness to comply with the request. Replicating earlier results on American English, perceived willingness was found to decrease with increasing pause duration, particularly above a \"tolerance threshold\" of 600 ms. Refining and qualifying this replicated result, the perception experiment showed additional effects of speaking-rate context and pause quality (silence vs. breathing vs. café noise) on perceived willingness judgments. The overall results picture is discussed with respect to the origin of the \"tolerance threshold\", the status of breathing in speech, and the function of pauses in communication",
    "checked": true,
    "id": "0734c4ced9f11c75dab5ed8240ece2b25713667d",
    "semantic_title": "how long is too long? how pause features after requests affect the perceived willingness of affirmative answers",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gessinger17_interspeech.html": {
    "title": "Shadowing Synthesized Speech — Segmental Analysis of Phonetic Convergence",
    "volume": "main",
    "abstract": "To shed light on the question whether humans converge phonetically to synthesized speech, a shadowing experiment was conducted using three different types of stimuli — natural speaker, diphone synthesis, and HMM synthesis. Three segment-level phonetic features of German that are well-known to vary across native speakers were examined. The first feature triggered convergence in roughly one third of the cases for all stimulus types. The second feature showed generally a small amount of convergence, which may be due to the nature of the feature itself. Still the effect was strongest for the natural stimuli, followed by the HMM stimuli and weakest for the diphone stimuli. The effect of the third feature was clearly observable for the natural stimuli and less pronounced in the synthetic stimuli. This is presumably a result of the partly insufficient perceptibility of this target feature in the synthetic stimuli and demonstrates the necessity of gaining fine-grained control over the synthesis output, should it be intended to implement capabilities of phonetic convergence on the segmental level in spoken dialogue systems",
    "checked": true,
    "id": "2f19a28d2332f1ee2ce453ba098fdfcc941b54db",
    "semantic_title": "shadowing synthesized speech - segmental analysis of phonetic convergence",
    "citation_count": 19,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ghaffarzadegan17_interspeech.html": {
    "title": "Occupancy Detection in Commercial and Residential Environments Using Audio Signal",
    "volume": "main",
    "abstract": "Occupancy detection, including presence detection and head count, as one of the fast growing areas plays an important role in providing safety, comfort and reducing energy consumption both in residential and commercial setups. The focus of this study is proposing affordable strategies to increase occupancy detection performance in realistic scenarios using only audio signal collected from the environment. We use approximately 100-hour of audio data in residential and commercial environments to analyze and evaluate our setup. In this study, we take advantage of developments in feature selection methods to choose the most relevant audio features for the task. Attribute and error vs. human activity analysis are also performed to gain a better understanding of the environmental sounds and possible solutions to enhance the performance. Experimental results confirm the effectiveness of audio sensor for occupancy detection using a cost effective system with presence detection accuracy of 96% and 99%, and the head count accuracy of 70% and 95% for the residential and commercial setups, respectively",
    "checked": true,
    "id": "75025838f40610bb8d5a3b01f3eb3563f3bcb8f7",
    "semantic_title": "occupancy detection in commercial and residential environments using audio signal",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tran17b_interspeech.html": {
    "title": "Data Augmentation, Missing Feature Mask and Kernel Classification for Through-the-Wall Acoustic Surveillance",
    "volume": "main",
    "abstract": "This paper deals with sound event classification from poor quality signals in the context of \"through-the-wall\" (TTW) surveillance. The task is extremely challenging due to the high level of distortion and attenuation caused by complex sound propagation and modulation effect from signal acquisition. Another problem, facing in TTW surveillance, is the lack of comprehensive training data as the recording is much more complicated than conventional approaches using audio microphones. To address that challenge, we employ a recurrent neural network, particularly the Long Short-Term Memory (LSTM) encoder, to transform conventional clean and noisy audio signals into TTW signals to augment additional training data. Furthermore, a novel missing feature mask kernel classification is developed to optimize the classification accuracy of TTW sound event classification. Particularly, Wasserstein distance is calculated from reliable intersection regions between pair-wise sound image representations and embedded into a probabilistic distance Support Vector Machine (SVM) kernel to optimize the TTW data separation. The proposed missing feature mask kernel allows effective training with inhomogeneously distorted data and the experimental results show promising results on TTW audio recordings, outperforming several state-of-art methods",
    "checked": true,
    "id": "57663209a76c2a2767ea3c16b04e5e03888e584d",
    "semantic_title": "data augmentation, missing feature mask and kernel classification for through-the-wall acoustic surveillance",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chang17_interspeech.html": {
    "title": "Endpoint Detection Using Grid Long Short-Term Memory Networks for Streaming Speech Recognition",
    "volume": "main",
    "abstract": "The task of endpointing is to determine when the user has finished speaking. This is important for interactive speech applications such as voice search and Google Home. In this paper, we propose a GLDNN-based (grid long short-term memory deep neural network) endpointer model and show that it provides significant improvements over a state-of-the-art CLDNN (convolutional, long short-term memory, deep neural network) model. Specifically, we replace the convolution layer in the CLDNN with a grid LSTM layer that models both spectral and temporal variations through recurrent connections. Results show that the GLDNN achieves 32% relative improvement in false alarm rate at a fixed false reject rate of 2%, and reduces median latency by 11%. We also include detailed experiments investigating why grid LSTMs offer better performance than convolution layers. Analysis reveals that the recurrent connection along the frequency axis is an important factor that greatly contributes to the performance of grid LSTMs, especially in the presence of background noise. Finally, we also show that multichannel input further increases robustness to background speech. Overall, we achieve 16% (100 ms) endpointer latency improvement relative to our previous best model on a Voice Search Task",
    "checked": true,
    "id": "28e094b2d73dda2089dac8153f6275802a8a8e81",
    "semantic_title": "endpoint detection using grid long short-term memory networks for streaming speech recognition",
    "citation_count": 28,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/baby17_interspeech.html": {
    "title": "Deep Learning Techniques in Tandem with Signal Processing Cues for Phonetic Segmentation for Text to Speech Synthesis in Indian Languages",
    "volume": "main",
    "abstract": "Automatic detection of phoneme boundaries is an important sub-task in building speech processing applications, especially text-to-speech synthesis (TTS) systems. The main drawback of the Gaussian mixture model - hidden Markov model (GMM-HMM) based forced-alignment is that the phoneme boundaries are not explicitly modeled. In an earlier work, we had proposed the use of signal processing cues in tandem with GMM-HMM based forced alignment for boundary correction for building Indian language TTS systems. In this paper, we capitalise on the ability of robust acoustic modeling techniques such as deep neural networks (DNN) and convolutional deep neural networks (CNN) for acoustic modeling. The GMM-HMM based forced alignment is replaced by DNN-HMM/CNN-HMM based forced alignment. Signal processing cues are used to correct the segment boundaries obtained using DNN-HMM/CNN-HMM segmentation. TTS systems built using these boundaries show a relative improvement in synthesis quality",
    "checked": true,
    "id": "bca765831defa3a4fad9914f28e8f408d3fc54bf",
    "semantic_title": "deep learning techniques in tandem with signal processing cues for phonetic segmentation for text to speech synthesis in indian languages",
    "citation_count": 15,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wang17m_interspeech.html": {
    "title": "Gate Activation Signal Analysis for Gated Recurrent Neural Networks and its Correlation with Phoneme Boundaries",
    "volume": "main",
    "abstract": "In this paper we analyze the gate activation signals inside the gated recurrent neural networks, and find the temporal structure of such signals is highly correlated with the phoneme boundaries. This correlation is further verified by a set of experiments for phoneme segmentation, in which better results compared to standard approaches were obtained",
    "checked": true,
    "id": "2b687cdfa014244435b1700795d3b10c428f8476",
    "semantic_title": "gate activation signal analysis for gated recurrent neural networks and its correlation with phoneme boundaries",
    "citation_count": 39,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/yin17_interspeech.html": {
    "title": "Speaker Change Detection in Broadcast TV Using Bidirectional Long Short-Term Memory Networks",
    "volume": "main",
    "abstract": "Speaker change detection is an important step in a speaker diarization system. It aims at finding speaker change points in the audio stream. In this paper, it is treated as a sequence labeling task and addressed by Bidirectional long short term memory networks (Bi-LSTM). The system is trained and evaluated on the Broadcast TV subset from ETAPE database. The result shows that the proposed model brings good improvement over conventional methods based on BIC and Gaussian Divergence. For instance, in comparison to Gaussian divergence, it produces speech turns that are 19.5% longer on average, with the same level of purity",
    "checked": true,
    "id": "edffb62b32ffcc2b5cc846e26375cb300fac9ecc",
    "semantic_title": "speaker change detection in broadcast tv using bidirectional long short-term memory networks",
    "citation_count": 62,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/do17c_interspeech.html": {
    "title": "Improved Automatic Speech Recognition Using Subband Temporal Envelope Features and Time-Delay Neural Network Denoising Autoencoder",
    "volume": "main",
    "abstract": "This paper investigates the use of perceptually-motivated subband temporal envelope (STE) features and time-delay neural network (TDNN) denoising autoencoder (DAE) to improve deep neural network (DNN)-based automatic speech recognition (ASR). STEs are estimated by full-wave rectification and low-pass filtering of band-passed speech using a Gammatone filter-bank. TDNNs are used either as DAE or acoustic models. ASR experiments are performed on Aurora-4 corpus. STE features provide 2.2% and 3.7% relative word error rate (WER) reduction compared to conventional log-mel filter-bank (FBANK) features when used in ASR systems using DNN and TDNN as acoustic models, respectively. Features enhanced by TDNN DAE are better recognized with ASR system using DNN acoustic models than using TDNN acoustic models. Improved ASR performance is obtained when features enhanced by TDNN DAE are used in ASR system using DNN acoustic models. In this scenario, using STE features provides 9.8% relative WER reduction compared to when using FBANK features",
    "checked": true,
    "id": "b5e1bc8f99a1007437974729fb12a4595539dea9",
    "semantic_title": "improved automatic speech recognition using subband temporal envelope features and time-delay neural network denoising autoencoder",
    "citation_count": 14,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/fujimoto17_interspeech.html": {
    "title": "Factored Deep Convolutional Neural Networks for Noise Robust Speech Recognition",
    "volume": "main",
    "abstract": "In this paper, we present a framework of a factored deep convolutional neural network (CNN) learning for noise robust automatic speech recognition (ASR). Deep CNN architecture, which has attracted great attention in various research areas, has also been successfully applied to ASR. However, to ensure noise robustness, since merely introducing deep CNN architecture into the acoustic modeling of ASR is insufficient, we introduce factored network architecture into deep CNN-based acoustic modeling. The proposed factored deep CNN framework factors out feature enhancement, delta parameter learning, and hidden Markov model state classification into three specific network blocks. By assigning specific roles to each block, the noise robustness of deep CNN-based acoustic models can be improved. With various comparative evaluations, we reveal that the proposed method successfully improves ASR accuracies in noise environments",
    "checked": true,
    "id": "33ca0bfc2b72fcef91b952299befe6859d081c53",
    "semantic_title": "factored deep convolutional neural networks for noise robust speech recognition",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/papadopoulos17b_interspeech.html": {
    "title": "Global SNR Estimation of Speech Signals for Unknown Noise Conditions Using Noise Adapted Non-Linear Regression",
    "volume": "main",
    "abstract": "The performance of speech technologies deteriorates in the presence of noise. Additionally, we need these technologies to be able to operate across a variety of noise levels and conditions. SNR estimation can guide the design and operation of such technologies or can be used as a pre-processing tool in database creation (e.g. identify/discard noisy signals). We propose a new method to estimate the global SNR of a speech signal when prior information about the noise that corrupts the signal, and speech boundaries within the signal, are not available. To achieve this goal, we train a neural network that performs non-linear regression to estimate the SNR. We use energy ratios as features, as well as ivectors to provide information about the noise that corrupts the signal. We compare our method against others in the literature, using the Mean Absolute Error (MAE) metric, and show that our method outperforms them consistently",
    "checked": true,
    "id": "3567f2148b9e6450d08aa4bcc622d61fb78ee5a3",
    "semantic_title": "global snr estimation of speech signals for unknown noise conditions using noise adapted non-linear regression",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ge17_interspeech.html": {
    "title": "Joint Training of Multi-Channel-Condition Dereverberation and Acoustic Modeling of Microphone Array Speech for Robust Distant Speech Recognition",
    "volume": "main",
    "abstract": "We propose a novel data utilization strategy, called multi-channel-condition learning, leveraging upon complementary information captured in microphone array speech to jointly train dereverberation and acoustic deep neural network (DNN) models for robust distant speech recognition. Experimental results, with a single automatic speech recognition (ASR) system, on the REVERB2014 simulated evaluation data show that, on 1-channel testing, the baseline joint training scheme attains a word error rate (WER) of 7.47%, reduced from 8.72% for separate training. The proposed multi-channel-condition learning scheme has been experimented on different channel data combinations and usage showing many interesting implications. Finally, training on all 8-channel data and with DNN-based language model rescoring, a state-of-the-art WER of 4.05% is achieved. We anticipate an even lower WER when combining more top ASR systems",
    "checked": true,
    "id": "2413f067d2512bab995b0653101b0279aadc901d",
    "semantic_title": "joint training of multi-channel-condition dereverberation and acoustic modeling of microphone array speech for robust distant speech recognition",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tran17c_interspeech.html": {
    "title": "Uncertainty Decoding with Adaptive Sampling for Noise Robust DNN-Based Acoustic Modeling",
    "volume": "main",
    "abstract": "Although deep neural network (DNN) based acoustic models have obtained remarkable results, the automatic speech recognition (ASR) performance still remains low in noise and reverberant conditions. To address this issue, a speech enhancement front-end is often used before recognition to reduce noise. However, the front-end cannot fully suppress noise and often introduces artifacts that are limiting the ASR performance improvement. Uncertainty decoding has been proposed to better interconnect the speech enhancement front-end and ASR back-end and mitigate the mismatch caused by residual noise and artifacts. By considering features as distributions instead of point estimates, the uncertainty decoding approach modifies the conventional decoding rules to account for the uncertainty emanating from the speech enhancement. Although the concept of uncertainty decoding has been investigated for DNN acoustic models recently, finding efficient ways to incorporate distribution of the enhanced features within a DNN acoustic model still requires further investigations. In this paper, we propose to parameterize the distribution of the enhanced feature and estimate the parameters by backpropagation using an unsupervised adaptation scheme. We demonstrate the effectiveness of the proposed approach on real audio data of the CHiME3 dataset",
    "checked": true,
    "id": "6e851fb1d623582c648304281ce8c16222bc15db",
    "semantic_title": "uncertainty decoding with adaptive sampling for noise robust dnn-based acoustic modeling",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhang17l_interspeech.html": {
    "title": "Attention-Based LSTM with Multi-Task Learning for Distant Speech Recognition",
    "volume": "main",
    "abstract": "Distant speech recognition is a highly challenging task due to background noise, reverberation, and speech overlap. Recently, there has been an increasing focus on attention mechanism. In this paper, we explore the attention mechanism embedded within the long short-term memory (LSTM) based acoustic model for large vocabulary distant speech recognition, trained using speech recorded from a single distant microphone (SDM) and multiple distant microphones (MDM). Furthermore, multi-task learning architecture is incorporated to improve robustness in which the network is trained to perform both a primary senone classification task and a secondary feature enhancement task. Experiments were conducted on the AMI meeting corpus. On average our model achieved 3.3% and 5.0% relative improvements in word error rate (WER) over the LSTM baseline model in the SDM and MDM cases, respectively. In addition, the model provided between a 2–4% absolute WER reduction compared to a conventional pipeline of independent processing stage on the MDM task",
    "checked": true,
    "id": "6da3d977f95af949158710bf19ab1504d46d9057",
    "semantic_title": "attention-based lstm with multi-task learning for distant speech recognition",
    "citation_count": 22,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/huang17h_interspeech.html": {
    "title": "To Improve the Robustness of LSTM-RNN Acoustic Models Using Higher-Order Feedback from Multiple Histories",
    "volume": "main",
    "abstract": "This paper investigates a novel multiple-history long short-term memory (MH-LSTM) RNN acoustic model to mitigate the robustness problem of noisy outputs in the form of mis-labeled data and/or mis-alignments. Conceptually, after an RNN is unfolded in time, the hidden units in each layer are re-arranged into ordered sub-layers with a master sub-layer on top and a set of auxiliary sub-layers below it. Only the master sub-layer generates outputs for the next layer whereas the auxiliary sub-layers run in parallel with the master sub-layer but with increasing time lags. Each sub-layer also receives higher-order feedback from a fixed number of sub-layers below it. As a result, each sub-layer maintains a different history of the input speech, and the ensemble of all the different histories lends itself to the model's robustness. The higher-order connections not only provide shorter feedback paths for error signals to propagate to the farther preceding hidden states to better model the long-term memory, but also more feedback paths to each model parameter and smooth its update during training. Phoneme recognition results on both real TIMIT data as well as synthetic TIMIT data with noisy labels or alignments show that the new model outperforms the conventional LSTM RNN model",
    "checked": true,
    "id": "cdd7df32dbff386e780a10ac4961801d11d4e761",
    "semantic_title": "to improve the robustness of lstm-rnn acoustic models using higher-order feedback from multiple histories",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kim17h_interspeech.html": {
    "title": "End-to-End Speech Recognition with Auditory Attention for Multi-Microphone Distance Speech Recognition",
    "volume": "main",
    "abstract": "End-to-End speech recognition is a recently proposed approach that directly transcribes input speech to text using a single model. End-to-End speech recognition methods including Connectionist Temporal Classification and Attention-based Encoder Decoder Networks have been shown to obtain state-of-the-art performance on a number of tasks and significantly simplify the modeling, training and decoding procedures for speech recognition. In this paper, we extend our prior work on End-to-End speech recognition focusing on the effectiveness of these models in far-field environments. Specifically, we propose introducing Auditory Attention to integrate input from multiple microphones directly within an End-to-End speech recognition model, leveraging the attention mechanism to dynamically tune the model's attention to the most reliable input sources. We evaluate our proposed model on the CHiME-4 task, and show substantial improvement compared to a model optimized for a single microphone input",
    "checked": true,
    "id": "86a2193fd844aea190a95a2e9f7ca8750a692597",
    "semantic_title": "end-to-end speech recognition with auditory attention for multi-microphone distance speech recognition",
    "citation_count": 17,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/menon17_interspeech.html": {
    "title": "Robust Speech Recognition Based on Binaural Auditory Processing",
    "volume": "main",
    "abstract": "This paper discusses a combination of techniques for improving speech recognition accuracy in the presence of reverberation and spatially-separated interfering sound sources. Interaural Time Delay (ITD), observed as a consequence of the difference in arrival times of a sound to the two ears, is an important feature used by the human auditory system to reliably localize and separate sound sources. In addition, the \"precedence effect\" helps the auditory system differentiate between the direct sound and its subsequent reflections in reverberant environments. This paper uses a cross-correlation-based measure across the two channels of a binaural signal to isolate the target source by rejecting portions of the signal corresponding to larger ITDs. To overcome the effects of reverberation, the steady-state components of speech are suppressed, effectively boosting the onsets, so as to retain the direct sound and suppress the reflections. Experimental results show a significant improvement in recognition accuracy using both these techniques. Cross-correlation-based processing and steady-state suppression are carried out separately, and the order in which these techniques are applied produces differences in the resulting recognition accuracy",
    "checked": true,
    "id": "99b7f9b4277cbaef35a4d9fafd03a5fcf01dd627",
    "semantic_title": "robust speech recognition based on binaural auditory processing",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/caroselli17_interspeech.html": {
    "title": "Adaptive Multichannel Dereverberation for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Reverberation is known to degrade the performance of automatic speech recognition (ASR) systems dramatically in far-field conditions. Adopting the weighted prediction error (WPE) approach, we formulate an online dereverberation algorithm for a multi-microphone array. The key contributions of this paper are: (a) we demonstrate that dereverberation using WPE improves performance even when the acoustic models are trained using multi-style training (MTR) with noisy, reverberated speech; (b) we show that the gains from WPE are preserved even in large and diverse real-world data sets; (c) we propose an adaptive version for online multichannel ASR tasks which gives similar gains as the non-causal version; and (d) while the algorithm can just be applied for evaluation, we show that also including dereverberation during training gives increased performance gains. We also report how different parameter settings of the dereverberation algorithm impacts the ASR performance",
    "checked": true,
    "id": "d07b7131bc7690e5da61e0bc806def6ffd713968",
    "semantic_title": "adaptive multichannel dereverberation for automatic speech recognition",
    "citation_count": 26,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zihlmann17_interspeech.html": {
    "title": "The Effects of Real and Placebo Alcohol on Deaffrication",
    "volume": "main",
    "abstract": "The more alcohol a person has consumed, the more mispronunciations occur. This study investigates how deaffrication surfaces in Bernese Swiss German when speakers are moderately intoxicated (0.05–0.08% Vol.), whether these effects can be hidden, and whether a placebo effect interacting with mispronunciation occurs. Five participants reading a text were recorded as follows. In stage I, they read the text before and after drinking placebo alcohol, and finally again after being told to enunciate very clearly. 3–7 days later, the same experiment was repeated with real alcohol. The recordings were then analysed with Praat. Despite interspeaker variation, the following generalisations can be made. The most deaffrication occurs in the C_C context both when speakers are sober and inebriated; affricates in _#, V_C, and V_V position encounter more deaffrication in the alcohol stage; and /͡tʃ/ and ͡kx are deaffricated more when the speaker is intoxicated, with /͡tʃ/ being the most susceptible to mispronunciation. Moreover, when alcohol is consumed, more deaffrication occurs, which cannot consciously be controlled. Furthermore, a statistically significant difference between the pre- and the post-placebo-drinking experiment could be found, which implies that a placebo effect takes place. Nevertheless, the effects of real alcohol are considerably stronger",
    "checked": true,
    "id": "679021a795ea7b029e44f1d0f76e662f7b33fd47",
    "semantic_title": "the effects of real and placebo alcohol on deaffrication",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mcauliffe17b_interspeech.html": {
    "title": "Polyglot and Speech Corpus Tools: A System for Representing, Integrating, and Querying Speech Corpora",
    "volume": "main",
    "abstract": "Speech datasets from many languages, styles, and sources exist in the world, representing significant potential for scientific studies of speech — particularly given structural similarities among all speech datasets. However, studies using multiple speech corpora remain difficult in practice, due to corpus size, complexity, and differing formats. We introduce open-source software for unified corpus analysis: integrating speech corpora and querying across them. Corpora are stored in a custom ‘polyglot persistence' scheme that combines three sub-databases mirroring different data types: a Neo4j graph database to represent temporal annotation graph structure, and SQL and InfluxDB databases to represent meta- and acoustic data. This scheme abstracts away from the idiosyncratic formats of different speech corpora, while mirroring the structure of different data types improves speed and scalability. A Python API and a GUI both allow for: enriching the database with positional, hierarchical, temporal, and signal measures (e.g. utterance boundaries, f0) that are useful for linguistic analysis; querying the database using a simple query language; and exporting query results to standard formats for further analysis. We describe the software, summarize two case studies using it to examine effects on pitch and duration across languages, and outline planned future development",
    "checked": true,
    "id": "becc2a1a45a01f81c5cbf2353d364e1a43c95896",
    "semantic_title": "polyglot and speech corpus tools: a system for representing, integrating, and querying speech corpora",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hughes17b_interspeech.html": {
    "title": "Mapping Across Feature Spaces in Forensic Voice Comparison: The Contribution of Auditory-Based Voice Quality to (Semi-)Automatic System Testing",
    "volume": "main",
    "abstract": "In forensic voice comparison, there is increasing focus on the integration of automatic and phonetic methods to improve the validity and reliability of voice evidence to the courts. In line with this, we present a comparison of long-term measures of the speech signal to assess the extent to which they capture complementary speaker-specific information. Likelihood ratio-based testing was conducted using MFCCs and (linear and Mel-weighted) long-term formant distributions (LTFDs). Fusing automatic and semi-automatic systems yielded limited improvement in performance over the baseline MFCC system, indicating that these measures capture essentially the same speaker-specific information. The output from the best performing system was used to evaluate the contribution of auditory-based analysis of supralaryngeal (filter) and laryngeal (source) voice quality in system testing. Results suggest that the problematic speakers for the (semi-)automatic system are, to some extent, predictable from their supralaryngeal voice quality profiles, with the least distinctive speakers producing the weakest evidence and most misclassifications. However, the misclassified pairs were still easily differentiated via auditory analysis. Laryngeal voice quality may thus be useful in resolving problematic pairs for (semi-)automatic systems, potentially improving their overall performance",
    "checked": true,
    "id": "87166a50c7accc2bc2f519e7dd366c93d7ba6a7b",
    "semantic_title": "mapping across feature spaces in forensic voice comparison: the contribution of auditory-based voice quality to (semi-)automatic system testing",
    "citation_count": 14,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/arantes17_interspeech.html": {
    "title": "Effect of Language, Speaking Style and Speaker on Long-Term F0 Estimation",
    "volume": "main",
    "abstract": "In this study, we compared three long-term fundamental frequency estimates — mean, median and base value — with respect to how fast they approach a stable value, as a function of language, speaking style and speaker. The base value concept was developed in search for an f value which should be invariant under prosodic variation. It has since also been tested in forensic phonetics as a possible speaker-specific f value. Data used in this study — recorded speech by male and female speakers in seven languages and three speaking styles, spontaneous, phrase reading and word list reading — had been recorded for a previous project. Average stabilisation times for the mean, median and base value are 9.76, 9.67 and 8.01 s. Base values stabilise significantly faster. Languages differ in both average and variability of the stabilisation times. Values range from 7.14 to 11.41 (mean), 7.5 to 11.33 (median) and 6.74 to 9.34 (base value). Spontaneous speech yields the most variable stabilisation times for the three estimators in Italian and Swedish, for the median in French and Portuguese and base value in German. Speakers within each language do not differ significantly in terms of stabilisation time variability for the three estimators",
    "checked": true,
    "id": "8fb4c50b4e4d7a86701f9831f62a93a466a4d6de",
    "semantic_title": "effect of language, speaking style and speaker on long-term f0 estimation",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/volin17_interspeech.html": {
    "title": "Stability of Prosodic Characteristics Across Age and Gender Groups",
    "volume": "main",
    "abstract": "The indexical function of speech prosody signals the membership of a speaker in a social group. The factors of age and gender are relatively easy to establish but their reflection in speech characteristics can be less straightforward as they interact with other social aspects. Therefore, diverse speaker communities should be investigated with the aim of their subsequent comparison. Our study provides data for the population of adult speakers of Czech — a West Slavic language of Central Europe. The sample consists of six age groups (20 to 80 years of age) with balanced representation of gender. The search for age and gender related attributes covered both global acoustic descriptors and linguistically informed prosodic feature extraction. Apart from commonly used measures and methods we also exploited Legendre polynomials, k-means clustering and a newly designed Cumulative Slope Index (CSI). The results specify general deceleration of articulation rate with age and lowering of F0 in aging Czech women, and reveal an increase in CSI of both F0 tracks and intensity curves with age. Furthermore, various melodic shapes were found to be distributed unequally across the age groups",
    "checked": true,
    "id": "d1ca1df09de15798ee850dfe37f8157541e2aa25",
    "semantic_title": "stability of prosodic characteristics across age and gender groups",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/plantehebert17_interspeech.html": {
    "title": "Electrophysiological Correlates of Familiar Voice Recognition",
    "volume": "main",
    "abstract": "Our previous work using voice lineups has established that listeners can recognize with near-perfect accuracy the voice of familiar individuals. In a forensic perspective, however, there are limitations to the application of voice lineups in that some witnesses may not wish to recognize the familiar voice of a parent or close friend or else provide unreliable responses. Considering this problem, the present study aimed to isolate the electrophysiological markers of voice familiarity. We recorded the evoked response potentials (ERPs) of 11 participants as they listened to a set of similar voices in varying utterances (standards of voice line ups were used in selecting voices). Within the presented set, only one voice was familiar to the listener (the voice of a parent, close friend, etc.). The ERPs showed a marked difference for heard familiar voices compared to an unfamiliar set. These are the first findings of a neural marker of voice recognition based on voices that are actually familiar to a listener and which take into account utterances rather than isolated vowels. The present results thus indicate that protocols of near-perfect voice recognition can be devised without using behavioral responses",
    "checked": true,
    "id": "e56da3a00ecf9c2ac02694a6ef14f1e930ba73ef",
    "semantic_title": "electrophysiological correlates of familiar voice recognition",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/cooperleavitt17_interspeech.html": {
    "title": "Developing an Embosi (Bantu C25) Speech Variant Dictionary to Model Vowel Elision and Morpheme Deletion",
    "volume": "main",
    "abstract": "This paper investigates vowel elision and morpheme deletion in Embosi (Bantu C25), an under-resourced language spoken in the Republic of Congo. We propose that the observed morpheme deletion is morphological, and that vowel elision is phonological. The study focuses on vowel elision that occurs across word boundaries between the contact of long/short vowels (i.e. CV[long] # V[short].CV), and between the contact of short/short vowels (CV[short] # V[short].CV). Several different categories of morphemes are explored: (i) prepositions ( ya, mo), (ii) class-noun nominal prefixes ( ba, etc.), (iii) singular subject pronouns ( ngá, nɔ, wa). For example, the preposition, ya, regularly deletes allowing for vowel elision if vowel contact occurs between the head of the noun phrase and the previous word. Phonetically motivated speech variants are proposed in the lexicon used for forced alignment (segmentation) enabling these phenomena to be quantified in the corpus so as to develop a dictionary containing relevant phonetic variants",
    "checked": true,
    "id": "162592ec07e5bca6ce631686b57a23deed9073da",
    "semantic_title": "developing an embosi (bantu c25) speech variant dictionary to model vowel elision and morpheme deletion",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/murphy17_interspeech.html": {
    "title": "Rd as a Control Parameter to Explore Affective Correlates of the Tense-Lax Continuum",
    "volume": "main",
    "abstract": "This study uses the R glottal waveshape parameter to simulate the phonatory tense-lax continuum and to explore its affective correlates in terms of activation and valence. Based on a natural utterance which was inverse filtered and source-parameterised, a range of synthesized stimuli varying along the tense-lax continuum were generated using R as a control parameter. Two additional stimuli were included, which were versions of the most lax stimuli with additional creak (lax-creaky voice). In a listening test, participants chose an emotion from a set of affective labels and indicated its perceived strength. They also indicated the naturalness of the stimulus and their confidence in their judgment. Results showed that stimuli at the tense end of the range were most frequently associated with angry, at the lax end of the range the association was with sad, and in the intermediate range, the association was with content. Results also indicate, as was found in our earlier work, that a particular stimulus can be associated with more than one affect. Overall these results show that R can be used as a single control parameter to generate variation along the tense-lax continuum of phonation",
    "checked": true,
    "id": "29ebc9c60d862a0d1881fe545e41df6270a12487",
    "semantic_title": "rd as a control parameter to explore affective correlates of the tense-lax continuum",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/barbosa17_interspeech.html": {
    "title": "Cross-Linguistic Distinctions Between Professional and Non-Professional Speaking Styles",
    "volume": "main",
    "abstract": "This work investigates acoustic and perceptual differences in four language varieties by using a corpus of professional and non-professional speaking styles. The professional stimuli are composed of excerpts of broadcast news and political discourses from six subjects in each case. The non-professional stimuli are made up of recordings of 10 subjects who read a long story and narrated it subsequently. All this material was obtained in four language varieties: Brazilian and European Portuguese, standard French and German. The corpus is balanced for gender. Eight melodic and intensity parameters were automatically obtained from excerpts of 10 to 20 seconds. We showed that 6 out of 8 parameters partially distinguish professional from non-professional style in the four language varieties. Classification and discrimination tests carried out with 12 Brazilian listeners using delexicalised speech showed that these subjects are able to distinguish professional style from non-professional style with about 2/3 of hits irrespective of language. In comparison, an automatic classification using an LDA model performed better in classifying non-professional (96%) against professional styles, but not in classifying professional (42%) against non-professional styles",
    "checked": true,
    "id": "f2672fa0240a9e14c06979ef036c605aa1b26fad",
    "semantic_title": "cross-linguistic distinctions between professional and non-professional speaking styles",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gendrot17_interspeech.html": {
    "title": "Perception and Production of Word-Final /ʁ/ in French",
    "volume": "main",
    "abstract": "Variability of (French) /ʁ/ is a frequently studied phenomenon showing that /ʁ/ can have multiple realizations. In French, all these studies were undertaken using small read corpora and we have reason to believe that these corpora don't allow to look at the full picture. Indeed factors such as local word frequency, as well as speech rate can have almost as much influence as phonemic context in the realization of /ʁ/ According to Ohala's Aerodynamic Voicing principle, /ʁ/ would tend to be either an unvoiced fricative or a voiced approximant. We chose to analyze word final /ʁ/s as they tend to embrace the largest spectrum of variation. The study realized here is two-fold: a perception study in a specific phonemic context, between /a/ and /l/, where /ʁ/ is realized as an approximant, so as to better understand the parameters and their thresholds necessary for /ʁ/ identification, and provide a measure of rhoticity In a second step, keeping the rhoticity measurement in mind, we analyzed the realizations of word final /ʁ/s in two continuous speech corpora and modelled the realization of /ʁ/ using predictors such as diphone and digram frequency, phonemic context and speech rate",
    "checked": true,
    "id": "dcfcb6907875319414d541f22b8a4f42a9cd701c",
    "semantic_title": "perception and production of word-final /ʁ/ in french",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/narendra17_interspeech.html": {
    "title": "Glottal Source Estimation from Coded Telephone Speech Using a Deep Neural Network",
    "volume": "main",
    "abstract": "In speech analysis, the information about the glottal source is obtained from speech by using glottal inverse filtering (GIF). The accuracy of state-of-the-art GIF methods is sufficiently high when the input speech signal is of high-quality (i.e., with little noise or reverberation). However, in realistic conditions, particularly when GIF is computed from coded telephone speech, the accuracy of GIF methods deteriorates severely. To robustly estimate the glottal source under coded condition, a deep neural network (DNN)-based method is proposed. The proposed method utilizes a DNN to map the speech features extracted from the coded speech to the glottal flow waveform estimated from the corresponding clean speech. To generate the coded telephone speech, adaptive multi-rate (AMR) codec is utilized which is a widely used speech compression method. The proposed glottal source estimation method is compared with two existing GIF methods, closed phase covariance analysis (CP) and iterative adaptive inverse filtering (IAIF). The results indicate that the proposed DNN-based method is capable of estimating glottal flow waveforms from coded telephone speech with a considerably better accuracy in comparison to CP and IAIF",
    "checked": true,
    "id": "b6fa85987584cb387cb99d27e3048e5bfd706847",
    "semantic_title": "glottal source estimation from coded telephone speech using a deep neural network",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/christodoulides17_interspeech.html": {
    "title": "Automatic Labelling of Prosodic Prominence, Phrasing and Disfluencies in French Speech by Simulating the Perception of Naïve and Expert Listeners",
    "volume": "main",
    "abstract": "We explore the use of machine learning techniques (notably SVM classifiers and Conditional Random Fields) to automate the prosodic labelling of French speech, based on modelling and simulating the perception of prosodic events by naïve and expert listeners. The models are based on previous work on the perception of syllabic prominence and hesitation-related disfluencies, and on an experiment on the real-time perception of prosodic boundaries. Expert and non-expert listeners annotated samples from three multi-genre corpora (CPROM, CPROM-PFC, LOCAS-F). Automatic prosodic annotation is approached as a sequence labelling problem, drawing on multiple information sources (acoustic features, lexical and shallow syntactic features) in accordance with the experimental findings showing that listeners integrate all such information in their perception of prosodic segmentation and events. We test combinations of features and machine learning methods, and we compare the automatic labelling with expert annotation. The result of this study is a tool that automatically annotates prosodic events by simulating the perception of expert and naïve listeners",
    "checked": true,
    "id": "421efb44a471baa24f7f9cc36023319cfc39b6e3",
    "semantic_title": "automatic labelling of prosodic prominence, phrasing and disfluencies in french speech by simulating the perception of naïve and expert listeners",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/levit17_interspeech.html": {
    "title": "Don't Count on ASR to Transcribe for You: Breaking Bias with Two Crowds",
    "volume": "main",
    "abstract": "A crowdsourcing approach for collecting high-quality speech transcriptions is presented. The approach addresses typical weakness of traditional semi-supervised transcription strategies that show ASR hypotheses to transcribers to help them cope with unclear or ambiguous audio and speed up transcriptions. We explain how the traditional methods introduce bias into transcriptions that make it difficult to objectively measure system improvements against existing baselines, and suggest a two-stage crowdsourcing alternative that, first, iteratively collects transcription hypotheses and, then, asks a different crowd to pick the best of them. We show that this alternative not only outperforms the traditional method in a side-by-side comparison, but it also leads to ASR improvements due to superior quality of acoustic and language models trained on the transcribed data",
    "checked": true,
    "id": "89e467fb93247e3e94d2dcb025e437e4bc19cd27",
    "semantic_title": "don't count on asr to transcribe for you: breaking bias with two crowds",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/airaksinen17_interspeech.html": {
    "title": "Effects of Training Data Variety in Generating Glottal Pulses from Acoustic Features with DNNs",
    "volume": "main",
    "abstract": "Glottal volume velocity waveform, the acoustical excitation of voiced speech, cannot be acquired through direct measurements in normal production of continuous speech. Glottal inverse filtering (GIF), however, can be used to estimate the glottal flow from recorded speech signals. Unfortunately, the usefulness of GIF algorithms is limited since they are sensitive to noise and call for high-quality recordings. Recently, efforts have been taken to expand the use of GIF by training deep neural networks (DNNs) to learn a statistical mapping between frame-level acoustic features and glottal pulses estimated by GIF. This framework has been successfully utilized in statistical speech synthesis in the form of the GlottDNN vocoder which uses a DNN to generate glottal pulses to be used as the synthesizer's excitation waveform. In this study, we investigate how the DNN-based generation of glottal pulses is affected by training data variety. The evaluation is done using both objective measures as well as subjective listening tests of synthetic speech. The results suggest that the performance of the glottal pulse generation with DNNs is affected particularly by how well the training corpus suits GIF: processing low-pitched male speech and sustained phonations shows better performance than processing high-pitched female voices or continuous speech",
    "checked": true,
    "id": "ae7f91be29f171239b1c515ea1c0748cd6a438d3",
    "semantic_title": "effects of training data variety in generating glottal pulses from acoustic features with dnns",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hantke17b_interspeech.html": {
    "title": "Towards Intelligent Crowdsourcing for Audio Data Annotation: Integrating Active Learning in the Real World",
    "volume": "main",
    "abstract": "In this contribution, we combine the advantages of traditional crowdsourcing with contemporary machine learning algorithms with the aim of ultimately obtaining reliable training data for audio processing in a faster, cheaper and therefore more efficient manner than has been previously possible. We propose a novel crowdsourcing approach, which brings a simulated active learning annotation scenario into a real world environment creating an intelligent and gamified crowdsourcing platform for manual audio annotation. Our platform combines two active learning query strategies with an internally calculated trustability score to efficiently reduce manual labelling efforts. This reduction is achieved in a twofold manner: first our system automatically decides if an instance requires annotation; second, it dynamically decides, depending on the quality of previously gathered annotations, on exactly how many annotations are needed to reliably label an instance. Results presented indicate that our approach drastically reduces the annotation load and is considerably more efficient than conventional methods",
    "checked": true,
    "id": "31b1eaa70aae71f850ff621d9e3112b86999d12a",
    "semantic_title": "towards intelligent crowdsourcing for audio data annotation: integrating active learning in the real world",
    "citation_count": 24,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/henter17_interspeech.html": {
    "title": "Principles for Learning Controllable TTS from Annotated and Latent Variation",
    "volume": "main",
    "abstract": "For building flexible and appealing high-quality speech synthesisers, it is desirable to be able to accommodate and reproduce fine variations in vocal expression present in natural speech. Synthesisers can enable control over such output properties by adding adjustable control parameters in parallel to their text input. If not annotated in training data, the values of these control inputs can be optimised jointly with the model parameters. We describe how this established method can be seen as approximate maximum likelihood and MAP inference in a latent variable model. This puts previous ideas of (learned) synthesiser inputs such as sentence-level control vectors on a more solid theoretical footing. We furthermore extend the method by restricting the latent variables to orthogonal subspaces via a sparse prior. This enables us to learn dimensions of variation present also within classes in coarsely annotated speech. As an example, we train an LSTM-based TTS system to learn nuances in emotional expression from a speech database annotated with seven different acted emotions. Listening tests show that our proposal successfully can synthesise speech with discernible differences in expression within each emotion, without compromising the recognisability of synthesised emotions compared to an identical system without learned nuances",
    "checked": true,
    "id": "694cc0c91301e47cac695382331d557c86c117c5",
    "semantic_title": "principles for learning controllable tts from annotated and latent variation",
    "citation_count": 18,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/takamichi17_interspeech.html": {
    "title": "Sampling-Based Speech Parameter Generation Using Moment-Matching Networks",
    "volume": "main",
    "abstract": "This paper presents sampling-based speech parameter generation using moment-matching networks for Deep Neural Network (DNN)-based speech synthesis. Although people never produce exactly the same speech even if we try to express the same linguistic and para-linguistic information, typical statistical speech synthesis produces completely the same speech, i.e., there is no inter-utterance variation in synthetic speech. To give synthetic speech natural inter-utterance variation, this paper builds DNN acoustic models that make it possible to randomly sample speech parameters. The DNNs are trained so that they make the moments of generated speech parameters close to those of natural speech parameters. Since the variation of speech parameters is compressed into a low-dimensional simple prior noise vector, our algorithm has lower computation cost than direct sampling of speech parameters. As the first step towards generating synthetic speech that has natural inter-utterance variation, this paper investigates whether or not the proposed sampling-based generation deteriorates synthetic speech quality. In evaluation, we compare speech quality of conventional maximum likelihood-based generation and proposed sampling-based generation. The result demonstrates the proposed generation causes no degradation in speech quality",
    "checked": true,
    "id": "a7d8dca8380f771d1617d619c8c877df8f90d849",
    "semantic_title": "sampling-based speech parameter generation using moment-matching networks",
    "citation_count": 13,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/pollet17_interspeech.html": {
    "title": "Unit Selection with Hierarchical Cascaded Long Short Term Memory Bidirectional Recurrent Neural Nets",
    "volume": "main",
    "abstract": "Bidirectional recurrent neural nets have demonstrated state-of-the-art performance for parametric speech synthesis. In this paper, we introduce a top-down application of recurrent neural net models to unit-selection synthesis. A hierarchical cascaded network graph predicts context phone duration, speech unit encoding and frame-level logF0 information that serves as targets for the search of units. The new approach is compared with an existing state-of-art hybrid system that uses Hidden Markov Models as basis for the statistical unit search",
    "checked": true,
    "id": "b2a7687709dc217ddba60045137d452f62ac7c85",
    "semantic_title": "unit selection with hierarchical cascaded long short term memory bidirectional recurrent neural nets",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/cooper17_interspeech.html": {
    "title": "Utterance Selection for Optimizing Intelligibility of TTS Voices Trained on ASR Data",
    "volume": "main",
    "abstract": "This paper describes experiments in training HMM-based text-to-speech (TTS) voices on data collected for Automatic Speech Recognition (ASR) training. We compare a number of filtering techniques designed to identify the best utterances from a noisy, multi-speaker corpus for training voices, to exclude speech containing noise and to include speech close in nature to more traditionally-collected TTS corpora. We also evaluate the use of automatic speech recognizers for intelligibility assessment in comparison with crowdsourcing methods. While the goal of this work is to develop natural-sounding and intelligible TTS voices in Low Resource Languages (LRLs) rapidly and easily, without the expense of recording data specifically for this purpose, we focus on English initially to identify the best filtering techniques and evaluation methods. We find that, when a large amount of data is available, selecting from the corpus based on criteria such as standard deviation of f0, fast speaking rate, and hypo-articulation produces the most intelligible voices",
    "checked": true,
    "id": "937f4bef19666bd886674d8c6e6652ff835698e1",
    "semantic_title": "utterance selection for optimizing intelligibility of tts voices trained on asr data",
    "citation_count": 17,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/rosenberg17_interspeech.html": {
    "title": "Bias and Statistical Significance in Evaluating Speech Synthesis with Mean Opinion Scores",
    "volume": "main",
    "abstract": "Listening tests and Mean Opinion Scores (MOS) are the most commonly used techniques for the evaluation of speech synthesis quality and naturalness. These are invaluable in the assessment of subjective qualities of machine generated stimuli. However, there are a number of challenges in understanding the MOS scores that come out of listening tests Primarily, we advocate for the use of non-parametric statistical tests in the calculation of statistical significance when comparing listening test results Additionally, based on the results of 46 legacy listening tests, we measure the impact of two sources of bias. Bias introduced by individual participants and synthesized text can a dramatic impact on observed MOS scores. For example, we find that on average the mean difference between the highest and lowest scoring rater is over 2 MOS points (on a 5 point scale). From this observation, we caution against using any statistical test without adjusting for this bias, and provide specific non-parametric recommendations",
    "checked": true,
    "id": "b2b1d01336323f3794f54de26567335aa0bcac46",
    "semantic_title": "bias and statistical significance in evaluating speech synthesis with mean opinion scores",
    "citation_count": 31,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/adiga17b_interspeech.html": {
    "title": "Phase Modeling Using Integrated Linear Prediction Residual for Statistical Parametric Speech Synthesis",
    "volume": "main",
    "abstract": "The conventional statistical parametric speech synthesis (SPSS) focus on characteristics of the magnitude spectrum of speech for speech synthesis by ignoring phase characteristics of speech. In this work, the role of phase information to improve the naturalness of synthetic speech is explored. The phase characteristics of excitation signal are estimated from the integrated linear prediction residual (ILPR) using an all-pass (AP) filter. The coefficients of the AP filter are estimated by minimizing an entropy based objective function from the cosine phase of the analytical signal obtained from ILPR signal. The AP filter coefficients (APCs) derived from the AP filter are used as features for modeling phase in SPSS. During synthesis time, to generate the excitation signal, frame wise generated APCs are used to add the group delay to the impulse excitation. The proposed method is compared with the group delay based phase excitation used in the STRAIGHT method. The experimental results show that proposed phased modeling having a better perceptual synthesis quality when compared with the STRAIGHT method",
    "checked": true,
    "id": "71857d11f82afc25f1d6faea0a3454f09e89c71b",
    "semantic_title": "phase modeling using integrated linear prediction residual for statistical parametric speech synthesis",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gonzalez17_interspeech.html": {
    "title": "Evaluation of a Silent Speech Interface Based on Magnetic Sensing and Deep Learning for a Phonetically Rich Vocabulary",
    "volume": "main",
    "abstract": "To help people who have lost their voice following total laryngectomy, we present a speech restoration system that produces audible speech from articulator movement. The speech articulators are monitored by sensing changes in magnetic field caused by movements of small magnets attached to the lips and tongue. Then, articulator movement is mapped to a sequence of speech parameter vectors using a transformation learned from simultaneous recordings of speech and articulatory data. In this work, this transformation is performed using a type of recurrent neural network (RNN) with fixed latency, which is suitable for real-time processing. The system is evaluated on a phonetically-rich database with simultaneous recordings of speech and articulatory data made by non-impaired subjects. Experimental results show that our RNN-based mapping obtains more accurate speech reconstructions (evaluated using objective quality metrics and a listening test) than articulatory-to-acoustic mappings using Gaussian mixture models (GMMs) or deep neural networks (DNNs). Moreover, our fixed-latency RNN architecture provides comparable performance to an utterance-level batch mapping using bidirectional RNNs (BiRNNs)",
    "checked": true,
    "id": "8f1369e9665b9c98d5a7f27734362d37c3d2d1db",
    "semantic_title": "Evaluation of a Silent Speech Interface Based on Magnetic Sensing and Deep Learning for a Phonetically Rich Vocabulary",
    "citation_count": 26,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/greenwood17_interspeech.html": {
    "title": "Predicting Head Pose from Speech with a Conditional Variational Autoencoder",
    "volume": "main",
    "abstract": "Natural movement plays a significant role in realistic speech animation. Numerous studies have demonstrated the contribution visual cues make to the degree we, as human observers, find an animation acceptable Rigid head motion is one visual mode that universally co-occurs with speech, and so it is a reasonable strategy to seek a transformation from the speech mode to predict the head pose. Several previous authors have shown that prediction is possible, but experiments are typically confined to rigidly produced dialogue. Natural, expressive, emotive and prosodic speech exhibit motion patterns that are far more difficult to predict with considerable variation in expected head pose Recently, Long Short Term Memory (LSTM) networks have become an important tool for modelling speech and natural language tasks. We employ Deep Bi-Directional LSTMs (BLSTM) capable of learning long-term structure in language, to model the relationship that speech has with rigid head motion. We then extend our model by conditioning with prior motion. Finally, we introduce a generative head motion model, conditioned on audio features using a Conditional Variational Autoencoder (CVAE). Each approach mitigates the problems of the one to many mapping that a speech to head pose model must accommodate",
    "checked": true,
    "id": "de1acb674eb6611929aebd1e3b3e54e39e4c08af",
    "semantic_title": "predicting head pose from speech with a conditional variational autoencoder",
    "citation_count": 33,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wester17_interspeech.html": {
    "title": "Real-Time Reactive Speech Synthesis: Incorporating Interruptions",
    "volume": "main",
    "abstract": "The ability to be interrupted and react in a realistic manner is a key requirement for interactive speech interfaces. While previous systems have long implemented techniques such as ‘barge in' where speech output can be halted at word or phrase boundaries, less work has explored how to mimic human speech output responses to real-time events like interruptions which require a reaction from the system. Unlike previous work which has focused on incremental production, here we explore a novel re-planning approach. The proposed system is versatile and offers a large range of possible ways to react. A focus group was used to evaluate the approach, where participants interacted with a system reading out a text. The system would react to audio interruptions, either with no reactions, passive reactions, or active negative reactions (i.e. getting increasingly irritated). Participants preferred a reactive system",
    "checked": true,
    "id": "05bbfbdf7a04fd69dc35b7ac168d283dde8dc188",
    "semantic_title": "real-time reactive speech synthesis: incorporating interruptions",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/blaauw17_interspeech.html": {
    "title": "A Neural Parametric Singing Synthesizer",
    "volume": "main",
    "abstract": "We present a new model for singing synthesis based on a modified version of the WaveNet architecture. Instead of modeling raw waveform, we model features produced by a parametric vocoder that separates the influence of pitch and timbre. This allows conveniently modifying pitch to match any target melody, facilitates training on more modest dataset sizes, and significantly reduces training and generation times. Our model makes frame-wise predictions using mixture density outputs rather than categorical outputs in order to reduce the required parameter count. As we found overfitting to be an issue with the relatively small datasets used in our experiments, we propose a method to regularize the model and make the autoregressive generation process more robust to prediction errors. Using a simple multi-stream architecture, harmonic, aperiodic and voiced/unvoiced components can all be predicted in a coherent manner. We compare our method to existing parametric statistical and state-of-the-art concatenative methods using quantitative metrics and a listening test. While naive implementations of the autoregressive generation algorithm tend to be inefficient, using a smart algorithm we can greatly speed up the process and obtain a system that's competitive in both speed and quality",
    "checked": true,
    "id": "a479ba5e4e1d2356a19dcaf236c9e3408b73fe18",
    "semantic_title": "a neural parametric singing synthesizer",
    "citation_count": 124,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wang17n_interspeech.html": {
    "title": "Tacotron: Towards End-to-End Speech Synthesis",
    "volume": "main",
    "abstract": "A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices. In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. Given <text, audio> pairs, the model can be trained completely from scratch with random initialization. We present several key techniques to make the sequence-to-sequence framework perform well for this challenging task. Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness. In addition, since Tacotron generates speech at the frame level, it's substantially faster than sample-level autoregressive methods",
    "checked": true,
    "id": "a072c2a400f62f720b68dc54a662fb1ae115bf06",
    "semantic_title": "tacotron: towards end-to-end speech synthesis",
    "citation_count": 1433,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/capes17_interspeech.html": {
    "title": "Siri On-Device Deep Learning-Guided Unit Selection Text-to-Speech System",
    "volume": "main",
    "abstract": "This paper describes Apple's hybrid unit selection speech synthesis system, which provides the voices for Siri with the requirement of naturalness, personality and expressivity. It has been deployed into hundreds of millions of desktop and mobile devices (e.g. iPhone, iPad, Mac, etc.) via iOS and macOS in multiple languages. The system is following the classical unit selection framework with the advantage of using deep learning techniques to boost the performance. In particular, deep and recurrent mixture density networks are used to predict the target and concatenation reference distributions for respective costs during unit selection. In this paper, we present an overview of the run-time TTS engine and the voice building process. We also describe various techniques that enable on-device capability such as preselection optimization, caching for low latency, and unit pruning for low footprint, as well as techniques that improve the naturalness and expressivity of the voice such as the use of long units",
    "checked": true,
    "id": "702eaa99bcb366d08d7f450ed7e354f9f6920b23",
    "semantic_title": "siri on-device deep learning-guided unit selection text-to-speech system",
    "citation_count": 57,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/esch17_interspeech.html": {
    "title": "An Expanded Taxonomy of Semiotic Classes for Text Normalization",
    "volume": "main",
    "abstract": "We describe an expanded taxonomy of semiotic classes for text normalization, building upon the work in [1]. We add a large number of categories of non-standard words (NSWs) that we believe a robust real-world text normalization system will have to be able to process. Our new categories are based upon empirical findings encountered while building text normalization systems across many languages, for both speech recognition and speech synthesis purposes. We believe our new taxonomy is useful both for ensuring high coverage when writing manual grammars, as well as for eliciting training data to build machine learning-based text normalization systems",
    "checked": true,
    "id": "8c715376812a784a888294c68d3e9ab588647f4d",
    "semantic_title": "an expanded taxonomy of semiotic classes for text normalization",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/nakashika17b_interspeech.html": {
    "title": "Complex-Valued Restricted Boltzmann Machine for Direct Learning of Frequency Spectra",
    "volume": "main",
    "abstract": "In this paper, we propose a new energy-based probabilistic model where a restricted Boltzmann machine (RBM) is extended to deal with complex-valued visible units. The RBM that automatically learns the relationships between visible units and hidden units (but without connections in the visible or the hidden units) has been widely used as a feature extractor, a generator, a classifier, pre-training of deep neural networks, etc. However, all the conventional RBMs have assumed the visible units to be either binary-valued or real-valued, and therefore complex-valued data cannot be fed to the RBM In various applications, however, complex-valued data is frequently used such examples include complex spectra of speech, fMRI images, wireless signals, and acoustic intensity. For the direct learning of such the complex-valued data, we define the new model called \"complex-valued RBM (CRBM)\" where the conditional probability of the complex-valued visible units given the hidden units forms a complex-Gaussian distribution. Another important characteristic of the CRBM is to have connections between real and imaginary parts of each of the visible units unlike the conventional real-valued RBM. Our experiments demonstrated that the proposed CRBM can directly encode complex spectra of speech signals without decoupling imaginary number or phase from the complex-value data",
    "checked": true,
    "id": "8c2cbb2f37a99dfdfabadb8a8a95ab23fb062cd6",
    "semantic_title": "complex-valued restricted boltzmann machine for direct learning of frequency spectra",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zioko17_interspeech.html": {
    "title": "Soundtracing for Realtime Speech Adjustment to Environmental Conditions in 3D Simulations",
    "volume": "main",
    "abstract": "We present a 3D realtime audio engine which utilizes frustum tracing to create realistic audio auralization, modifying speech in architectural walkthroughs. All audio effects are computed based on both the geometrical (e.g. walls, furniture) and acoustical scene properties (e.g. materials, air attenuation). The sound changes dynamically as we change the point of perception and sound sources. The engine can be configured to use as little as 10 percent of available processing power. Our demonstration will be based on listening radio samples in rooms with similar shape, but different acoustical properties. The described system is a component of a virtual reality trainer for firefighters using Oculus Rift. It allows to conduct dialogues with victims and to locate them based on sound cues",
    "checked": true,
    "id": "bf4903c0520ed9592c87fe553f925ca4c858153c",
    "semantic_title": "soundtracing for realtime speech adjustment to environmental conditions in 3d simulations",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/arai17b_interspeech.html": {
    "title": "Vocal-Tract Model with Static Articulators: Lips, Teeth, Tongue, and More",
    "volume": "main",
    "abstract": "Our physical models of the human vocal tract successfully demonstrate theories such as the source-filter theory of speech production, mechanisms such as the relationship between vocal-tract configuration and vowel quality, and phenomena such as formant frequency estimation. Earlier models took one of two directions: either simplification, showing only a few target themes, or diversification, simulating human articulation more broadly. In this study, we have designed a static, hybrid model. Each model of this type produces one vowel. However, the model also simulates the human articulators more broadly, including the lips, teeth, and tongue. The sagittal block is enclosed with transparent plates so that the inside of the vocal tract is visible from the outside. We also colored the articulators to make them more easily identified. In testing, we confirmed that the vocal-tract models can produce the target vowel. These models have great potential, with applications not only in acoustics and phonetics education, but also pronunciation training in language learning and speech therapy in the clinical setting",
    "checked": true,
    "id": "2cd6671162a0a52c24cb9571da02cfa9f3e45495",
    "semantic_title": "vocal-tract model with static articulators: lips, teeth, tongue, and more",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/masudakatsuse17_interspeech.html": {
    "title": "Remote Articulation Test System Based on WebRTC",
    "volume": "main",
    "abstract": "A remote articulation test system with multimedia communication has been developed in order that outside speech-language-hearing therapists (STs) can exam pronunciations of the students in special education classes in regular elementary schools and give advice to their teachers. The proposed system has video and voice communication and image transmission functions based on WebRTC. Using image transmission, the ST presents picture cards for the word test to the student and asks what is depicted. Using video / voice communication, the ST confirms the student's voice and articulation movement. Compared to our previous system in which written words were presented, the proposed system enables a more formal and accurate articulation test",
    "checked": true,
    "id": "b8fed993345d050b2bd976495e0efd7d14dffab4",
    "semantic_title": "remote articulation test system based on webrtc",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bunnell17_interspeech.html": {
    "title": "The ModelTalker Project: A Web-Based Voice Banking Pipeline for ALS/MND Patients",
    "volume": "main",
    "abstract": "The Nemours ModelTalker supports voice banking for users diagnosed with ALS/MND and related neurodegenerative diseases. Users record up to 1600 sentences from which a synthetic voice is constructed. For the past two years we have focused on extending and refining a web-based recording tool to support this process. In this demonstration, we illustrate the features of the web-based pipeline that guides patients through the process of setting up to record at home, recording a standard speech inventory, adding custom recordings, and screening alternative versions of their voice and alternative synthesis parameter settings. Finally, we summarize results from 352 individuals with a wide range of speaking ability, who have recently used this voice banking pipeline",
    "checked": true,
    "id": "b621921e575ec67a5fc9ef0ed4ddbfbaeca94f9c",
    "semantic_title": "the modeltalker project: a web-based voice banking pipeline for als/mnd patients",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2017/heeringa17_interspeech.html": {
    "title": "Visible Vowels: A Tool for the Visualization of Vowel Variation",
    "volume": "main",
    "abstract": "This paper presents Visible Vowels, a web app that visualizes variation in f0, formants and duration. It combines user friendliness with maximum functionality and flexibility, using a live plot view",
    "checked": true,
    "id": "771ec29870df460c8c53907cf0e9d0664af0ef04",
    "semantic_title": "visible vowels: a tool for the visualization of vowel variation",
    "citation_count": 9,
    "authors": []
  }
}