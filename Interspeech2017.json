{
  "https://www.isca-speech.org/archive/interspeech_2017/li17_interspeech.html": {
    "title": "ISCA Medal for Scientific Achievement",
    "volume": "main",
    "abstract": "The ISCA Medal for Scientific Achievement 2017 will be awarded to Professor Fumitada Itakura by the President of ISCA during the opening ceremony",
    "checked": true,
    "id": "f9caa3560e50318f82b850d9c254cbe89fd52d93",
    "semantic_title": "isca medal for scientific achievement",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kinnunen17_interspeech.html": {
    "title": "The ASVspoof 2017 Challenge: Assessing the Limits of Replay Spoofing Attack Detection",
    "volume": "main",
    "abstract": "The ASVspoof initiative was created to promote the development of countermeasures which aim to protect automatic speaker verification (ASV) from spoofing attacks. The first community-led, common evaluation held in 2015 focused on countermeasures for speech synthesis and voice conversion spoofing attacks. Arguably, however, it is replay attacks which pose the greatest threat. Such attacks involve the replay of recordings collected from enrolled speakers in order to provoke false alarms and can be mounted with greater ease using everyday consumer devices. ASVspoof 2017, the second in the series, hence focused on the development of replay attack countermeasures. This paper describes the database, protocols and initial findings. The evaluation entailed highly heterogeneous acoustic recording and replay conditions which increased the equal error rate (EER) of a baseline ASV system from 1.76% to 31.46%. Submissions were received from 49 research teams, 20 of which improved upon a baseline replay spoofing detector EER of 24.77%, in terms of replay/non-replay discrimination. While largely successful, the evaluation indicates that the quest for countermeasures which are resilient in the face of variable replay attacks remains very much alive",
    "checked": true,
    "id": "66e1dff1be7ad92104ef33a031b3392517a19b58",
    "semantic_title": "the asvspoof 2017 challenge: assessing the limits of replay spoofing attack detection",
    "citation_count": 406
  },
  "https://www.isca-speech.org/archive/interspeech_2017/font17_interspeech.html": {
    "title": "Experimental Analysis of Features for Replay Attack Detection — Results on the ASVspoof 2017 Challenge",
    "volume": "main",
    "abstract": "This paper presents an experimental comparison of different features for the detection of replay spoofing attacks in Automatic Speaker Verification systems. We evaluate the proposed countermeasures using two recently introduced databases, including the dataset provided for the ASVspoof 2017 challenge. This challenge provides researchers with a common framework for the evaluation of replay attack detection systems, with a particular focus on the generalization to new, unknown conditions (for instance, replay devices different from those used during system training). Our cross-database experiments show that, although achieving this level of generalization is indeed a challenging task, it is possible to train classifiers that exhibit stable and consistent results across different experiments. The proposed approach for the ASVspoof 2017 challenge consists in the score-level fusion of several base classifiers using logistic regression. These base classifiers are 2-class Gaussian Mixture Models (GMMs) representing genuine and spoofed speech respectively. Our best system achieves an Equal Error Rate of 10.52% on the challenge evaluation set. As a result of this set of experiments, we provide some general conclusions regarding feature extraction for replay attack detection and identify which features show the most promising results",
    "checked": false,
    "id": "2e61d0f2a57365a703ad647d017709c0d1193d2d",
    "semantic_title": "experimental analysis of features for replay attack detection - results on the asvspoof 2017 challenge",
    "citation_count": 110
  },
  "https://www.isca-speech.org/archive/interspeech_2017/patil17_interspeech.html": {
    "title": "Novel Variable Length Teager Energy Separation Based Instantaneous Frequency Features for Replay Detection",
    "volume": "main",
    "abstract": "Replay attacks presents a great risk for Automatic Speaker Verification (ASV) system. In this paper, we propose a novel replay detector based on Variable length Teager Energy Operator-Energy Separation Algorithm-Instantaneous Frequency Cosine Coefficients (VESA-IFCC) for the ASV spoof 2017 challenge. The key idea here is to exploit the contribution of IF in each subband energy via ESA to capture possible changes in spectral envelope (due to transmission and channel characteristics of replay device) of replayed speech. The IF is computed from narrowband components of speech signal, and DCT is applied in IF to get proposed feature set. We compare the performance of the proposed VESA-IFCC feature set with the features developed for detecting synthetic and voice converted speech. This includes the CQCC, CFCCIF and prosody-based features. On the development set, the proposed VESA-IFCC features when fused at score-level with a variant of CFCCIF and prosody-based features gave the least EER of 0.12%. On the evaluation set, this combination gave an EER of 18.33%. However, post-evaluation results of challenge indicate that VESA-IFCC features alone gave the relatively least EER of 14.06% (i.e., relatively 16.11% less compared to baseline CQCC) and hence, is a very useful countermeasure to detect replay attacks",
    "checked": true,
    "id": "aae57915d9b43f9fdb1286f1b8286b37704d70ea",
    "semantic_title": "novel variable length teager energy separation based instantaneous frequency features for replay detection",
    "citation_count": 86
  },
  "https://www.isca-speech.org/archive/interspeech_2017/cai17_interspeech.html": {
    "title": "Countermeasures for Automatic Speaker Verification Replay Spoofing Attack : On Data Augmentation, Feature Representation, Classification and Fusion",
    "volume": "main",
    "abstract": "The ongoing ASVspoof 2017 challenge aims to detect replay attacks for text dependent speaker verification. In this paper, we propose multiple replay spoofing countermeasure systems, with some of them boosting the CQCC-GMM baseline system after score level fusion. We investigate different steps in the system building pipeline, including data augmentation, feature representation, classification and fusion. First, in order to augment training data and simulate the unseen replay conditions, we converted the raw genuine training data into replay spoofing data with parametric sound reverberator and phase shifter. Second, we employed the original spectrogram rather than CQCC as input to explore the end-to-end feature representation learning methods. The spectrogram is randomly cropped into fixed size segments, and then fed into a deep residual network (ResNet). Third, upon the CQCC features, we replaced the subsequent GMM classifier with deep neural networks including fully-connected deep neural network (FDNN) and Bidirectional Long Short Term Memory neural network (BLSTM). Experiments showed that data augmentation strategy can significantly improve the system performance. The final fused system achieves to 16.39% EER on the test set of ASVspoof 2017 for the common task",
    "checked": true,
    "id": "3da78ce05b00b16aae951563bb9ec30831d3cb65",
    "semantic_title": "countermeasures for automatic speaker verification replay spoofing attack : on data augmentation, feature representation, classification and fusion",
    "citation_count": 63
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jelil17_interspeech.html": {
    "title": "Spoof Detection Using Source, Instantaneous Frequency and Cepstral Features",
    "volume": "main",
    "abstract": "This work describes the techniques used for spoofed speech detection for the ASVspoof 2017 challenge. The main focus of this work is on exploiting the differences in the speech-specific nature of genuine speech signals and spoofed speech signals generated by replay attacks. This is achieved using glottal closure instants, epoch strength, and the peak to side lobe ratio of the Hilbert envelope of linear prediction residual. Apart from these source features, the instantaneous frequency cosine coefficient feature, and two cepstral features namely, constant Q cepstral coefficients and mel frequency cepstral coefficients are used. A combination of all these features is performed to obtain a high degree of accuracy for spoof detection. Initially, efficacy of these features are tested on the development set of the ASVspoof 2017 database with Gaussian mixture model based systems. The systems are then fused at score level which acts as the final combined system for the challenge. The combined system is able to outperform the individual systems by a significant margin. Finally, the experiments are repeated on the evaluation set of the database and the combined system results in an equal error rate of 13.95%",
    "checked": true,
    "id": "c298f364ec1cc2e81aba3462d3a90751bc6b94ba",
    "semantic_title": "spoof detection using source, instantaneous frequency and cepstral features",
    "citation_count": 86
  },
  "https://www.isca-speech.org/archive/interspeech_2017/witkowski17_interspeech.html": {
    "title": "Audio Replay Attack Detection Using High-Frequency Features",
    "volume": "main",
    "abstract": "This paper presents our contribution to the ASVspoof 2017 Challenge. It addresses a replay spoofing attack against a speaker recognition system by detecting that the analysed signal has passed through multiple analogue-to-digital (AD) conversions. Specifically, we show that most of the cues that enable to detect the replay attacks can be found in the high-frequency band of the replayed recordings. The described anti-spoofing countermeasures are based on (1) modelling the subband spectrum and (2) using the proposed features derived from the linear prediction (LP) analysis. The results of the investigated methods show a significant improvement in comparison to the baseline system of the ASVspoof 2017 Challenge. A relative equal error rate (EER) reduction by 70% was achieved for the development set and a reduction by 30% was obtained for the evaluation set",
    "checked": true,
    "id": "7e41a8535e4fa2f9a88b0c59c602c2343eaf085d",
    "semantic_title": "audio replay attack detection using high-frequency features",
    "citation_count": 132
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wang17_interspeech.html": {
    "title": "Feature Selection Based on CQCCs for Automatic Speaker Verification Spoofing",
    "volume": "main",
    "abstract": "The ASVspoof 2017 challenge aims to assess spoofing and countermeasures attack detection accuracy for automatic speaker verification. It has been proven that constant Q cepstral coefficients (CQCCs) processes speech in different frequencies with variable resolution and performs much better than traditional features. When coupled with a Gaussian mixture model (GMM), it is an excellently effective spoofing countermeasure. The baseline CQCC+GMM system considers short-term impacts while ignoring the whole influence of channel. In the meanwhile, dimension of the feature is relatively higher than the traditional feature and usually with a higher variance. This paper explores different features for ASVspoof 2017 challenge. The mean and variance of the CQCC features of an utterance is used as the representation of the whole utterance. Feature selection method is introduced to avoid high variance and overfitting for spoofing detection. Experimental results on ASVspoof 2017 dataset show that feature selection followed by Support Vector Machine (SVM) gets an improvement compared to the baseline. It is also shown that pitch feature contributes to the performance improvement, and it obtains a relative improvement of 37.39% over the baseline CQCC+GMM system",
    "checked": true,
    "id": "2b37e26aa92c4433bc4ea54c8d41426e3c6242a5",
    "semantic_title": "feature selection based on cqccs for automatic speaker verification spoofing",
    "citation_count": 31
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ylmaz17_interspeech.html": {
    "title": "Longitudinal Speaker Clustering and Verification Corpus with Code-Switching Frisian-Dutch Speech",
    "volume": "main",
    "abstract": "In this paper, we present a new longitudinal and bilingual broadcast database designed for speaker clustering and text-independent verification research. The broadcast data is extracted from the archives of Omrop Fryslân which is the regional broadcaster in the province of Fryslân, located in the north of the Netherlands. Two speaker verification tasks are provided in a standard enrollment-test setting with language consistent trials. The first task contains target trials from all speakers available appearing in at least two different programs, while the second task contains target trials from a subgroup of speakers appearing in programs recorded in multiple years. The second task is designed to investigate the effects of ageing on the accuracy of speaker verification systems. This database also contains unlabeled spoken segments from different radio programs for speaker clustering research. We provide the output of an existing speaker diarization system for baseline verification experiments. Finally, we present the baseline speaker verification results using the Kaldi GMM- and DNN-UBM speaker verification system. This database will be an extension to the recently presented open source Frisian data collection and it is publicly available for research purposes",
    "checked": true,
    "id": "efd7f517a1f1f61b39e57a8582fd692ae8c97d6b",
    "semantic_title": "longitudinal speaker clustering and verification corpus with code-switching frisian-dutch speech",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ylmaz17b_interspeech.html": {
    "title": "Exploiting Untranscribed Broadcast Data for Improved Code-Switching Detection",
    "volume": "main",
    "abstract": "We have recently presented an automatic speech recognition (ASR) system operating on Frisian-Dutch code-switched speech. This type of speech requires careful handling of unexpected language switches that may occur in a single utterance. In this paper, we extend this work by using some raw broadcast data to improve multilingually trained deep neural networks (DNN) that have been trained on 11.5 hours of manually annotated bilingual speech. For this purpose, we apply the initial ASR to the untranscribed broadcast data and automatically create transcriptions based on the recognizer output using different language models for rescoring. Then, we train new acoustic models on the combined data, i.e., the manually and automatically transcribed bilingual broadcast data, and investigate the automatic transcription quality based on the recognition accuracies on a separate set of development and test data. Finally, we report code-switching detection performance elaborating on the correlation between the ASR and the code-switching detection performance",
    "checked": true,
    "id": "0f93bff2437383e29bd1c2f14de1d5377d6d35f3",
    "semantic_title": "exploiting untranscribed broadcast data for improved code-switching detection",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ramanarayanan17_interspeech.html": {
    "title": "Jee haan, I'd like both, por favor: Elicitation of a Code-Switched Corpus of Hindi–English and Spanish–English Human–Machine Dialog",
    "volume": "main",
    "abstract": "We present a database of code-switched conversational human–machine dialog in English–Hindi and English–Spanish. We leveraged HALEF, an open-source standards-compliant cloud-based dialog system to capture audio and video of bilingual crowd workers as they interacted with the system. We designed conversational items with intra-sentential code-switched machine prompts, and examine its efficacy in eliciting code-switched speech in a total of over 700 dialogs. We analyze various characteristics of the code-switched corpus and discuss some considerations that should be taken into account while collecting and processing such data. Such a database can be leveraged for a wide range of potential applications, including automated processing, recognition and understanding of code-switched speech and language learning applications for new language learners",
    "checked": false,
    "id": "bce75eb27fada3cb4e3777da0f9d4e35631b51a7",
    "semantic_title": "jee haan, i'd like both, por favor: elicitation of a code-switched corpus of hindi-english and spanish-english human-machine dialog",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2017/rallabandi17_interspeech.html": {
    "title": "On Building Mixed Lingual Speech Synthesis Systems",
    "volume": "main",
    "abstract": "Codemixing — phenomenon where lexical items from one language are embedded in the utterance of another — is relatively frequent in multilingual communities. However, TTS systems today are not fully capable of effectively handling such mixed content despite achieving high quality in the monolingual case. In this paper, we investigate various mechanisms for building mixed lingual systems which are built using a mixture of monolingual corpora and are capable of synthesizing such content. First, we explore the possibility of manipulating the phoneme representation: using target word to source phone mapping with the aim of emulating the native speaker intuition. We then present experiments at the acoustic stage investigating training techniques at both spectral and prosodic levels. Subjective evaluation shows that our systems are capable of generating high quality synthesis in codemixed scenarios",
    "checked": true,
    "id": "02a20ed2182475b40a4e7744aa6555607adffa62",
    "semantic_title": "on building mixed lingual speech synthesis systems",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chandu17_interspeech.html": {
    "title": "Speech Synthesis for Mixed-Language Navigation Instructions",
    "volume": "main",
    "abstract": "Text-to-Speech (TTS) systems that can read navigation instructions are one of the most widely used speech interfaces today. Text in the navigation domain may contain named entities such as location names that are not in the language that the TTS database is recorded in. Moreover, named entities can be compound words where individual lexical items belong to different languages. These named entities may be transliterated into the script that the TTS system is trained on. This may result in incorrect pronunciation rules being used for such words. We describe experiments to extend our previous work in generating code-mixed speech to synthesize navigation instructions, with a mixed-lingual TTS system. We conduct subjective listening tests with two sets of users, one being students who are native speakers of an Indian language and very proficient in English, and the other being drivers with low English literacy, but familiarity with location names. We find that in both sets of users, there is a significant preference for our proposed system over a baseline system that synthesizes instructions in English",
    "checked": true,
    "id": "99f07e194197a55fd017657d4cd1a8d9c349de05",
    "semantic_title": "speech synthesis for mixed-language navigation instructions",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2017/amazouz17_interspeech.html": {
    "title": "Addressing Code-Switching in French/Algerian Arabic Speech",
    "volume": "main",
    "abstract": "This study focuses on code-switching (CS) in French/Algerian Arabic bilingual communities and investigates how speech technologies, such as automatic data partitioning, language identification and automatic speech recognition (ASR) can serve to analyze and classify this type of bilingual speech. A preliminary study carried out using a corpus of Maghrebian broadcast data revealed a relatively high presence of CS Algerian Arabic as compared to the neighboring countries Morocco and Tunisia. Therefore this study focuses on code switching produced by bilingual Algerian speakers who can be considered native speakers of both Algerian Arabic and French. A specific corpus of four hours of speech from 8 bilingual French Algerian speakers was collected. This corpus contains read speech and conversational speech in both languages and includes stretches of code-switching. We provide a linguistic description of the code-switching stretches in terms of intra-sentential and inter-sentential switches, the speech duration in each language. We report on some initial studies to locate French, Arabic and the code-switched stretches, using ASR system word posteriors for this pair of languages",
    "checked": true,
    "id": "c935bcc748c4e9b87420deaf23074535592950d0",
    "semantic_title": "addressing code-switching in french/algerian arabic speech",
    "citation_count": 43
  },
  "https://www.isca-speech.org/archive/interspeech_2017/guzman17_interspeech.html": {
    "title": "Metrics for Modeling Code-Switching Across Corpora",
    "volume": "main",
    "abstract": "In developing technologies for code-switched speech, it would be desirable to be able to predict how much language mixing might be expected in the signal and the regularity with which it might occur. In this work, we offer various metrics that allow for the classification and visualization of multilingual corpora according to the ratio of languages represented, the probability of switching between them, and the time-course of switching. Applying these metrics to corpora of different languages and genres, we find that they display distinct probabilities and periodicities of switching, information useful for speech processing of mixed-language data",
    "checked": true,
    "id": "25a5cf5c7dc2269cf67d98b2fb46317a4d16b581",
    "semantic_title": "metrics for modeling code-switching across corpora",
    "citation_count": 50
  },
  "https://www.isca-speech.org/archive/interspeech_2017/westhuizen17_interspeech.html": {
    "title": "Synthesising isiZulu-English Code-Switch Bigrams Using Word Embeddings",
    "volume": "main",
    "abstract": "Code-switching is prevalent among South African speakers, and presents a challenge to automatic speech recognition systems. It is predominantly a spoken phenomenon, and generally does not occur in textual form. Therefore a particularly serious challenge is the extreme lack of training material for language modelling. We investigate the use of word embeddings to synthesise isiZulu-to-English code-switch bigrams with which to augment such sparse language model training data. A variety of word embeddings are trained on a monolingual English web text corpus, and subsequently queried to synthesise code-switch bigrams. Our evaluation is performed on language models trained on a new, although small, English-isiZulu code-switch corpus compiled from South African soap operas. This data is characterised by fast, spontaneously spoken speech containing frequent code-switching. We show that the augmentation of the training data with code-switched bigrams synthesised in this way leads to a reduction in perplexity",
    "checked": true,
    "id": "7b263493401762ede1f3a1f1e134e3c0e8584ed8",
    "semantic_title": "synthesising isizulu-english code-switch bigrams using word embeddings",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2017/soto17_interspeech.html": {
    "title": "Crowdsourcing Universal Part-of-Speech Tags for Code-Switching",
    "volume": "main",
    "abstract": "Code-switching is the phenomenon by which bilingual speakers switch between multiple languages during communication. The importance of developing language technologies for code-switching data is immense, given the large populations that routinely code-switch. High-quality linguistic annotations are extremely valuable for any NLP task, and performance is often limited by the amount of high-quality labeled data. However, little such data exists for code-switching. In this paper, we describe crowd-sourcing universal part-of-speech tags for the Miami Bangor Corpus of Spanish-English code-switched speech. We split the annotation task into three subtasks: one in which a subset of tokens are labeled automatically, one in which questions are specifically designed to disambiguate a subset of high frequency words, and a more general cascaded approach for the remaining data in which questions are displayed to the worker following a decision tree structure. Each subtask is extended and adapted for a multilingual setting and the universal tagset. The quality of the annotation process is measured using hidden check questions annotated with gold labels. The overall agreement between gold standard labels and the majority vote is between 0.95 and 0.96 for just three labels and the average recall across part-of-speech tags is between 0.87 and 0.99, depending on the task",
    "checked": true,
    "id": "6a4e10bcbb86e5ea55f3dfbfd269dccba62b5db9",
    "semantic_title": "crowdsourcing universal part-of-speech tags for code-switching",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lavrentyeva17_interspeech.html": {
    "title": "Audio Replay Attack Detection with Deep Learning Frameworks",
    "volume": "main",
    "abstract": "Nowadays spoofing detection is one of the priority research areas in the field of automatic speaker verification. The success of Automatic Speaker Verification Spoofing and Countermeasures (ASVspoof) Challenge 2015 confirmed the impressive perspective in detection of unforeseen spoofing trials based on speech synthesis and voice conversion techniques. However, there is a small number of researches addressed to replay spoofing attacks which are more likely to be used by non-professional impersonators. This paper describes the Speech Technology Center (STC) anti-spoofing system submitted for ASVspoof 2017 which is focused on replay attacks detection. Here we investigate the efficiency of a deep learning approach for solution of the mentioned-above task. Experimental results obtained on the Challenge corpora demonstrate that the selected approach outperforms current state-of-the-art baseline systems in terms of spoofing detection quality. Our primary system produced an EER of 6.73% on the evaluation part of the corpora which is 72% relative improvement over the ASVspoof 2017 baseline system",
    "checked": true,
    "id": "a2b4c396dc1064fb90bb5455525733733c761a7f",
    "semantic_title": "audio replay attack detection with deep learning frameworks",
    "citation_count": 232
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ji17_interspeech.html": {
    "title": "Ensemble Learning for Countermeasure of Audio Replay Spoofing Attack in ASVspoof2017",
    "volume": "main",
    "abstract": "To enhance the security and reliability of automatic speaker verification (ASV) systems, ASVspoof 2017 challenge focuses on the detection problem of known and unknown audio replay attacks. We proposed an ensemble learning classifier for CNCB team's submitted system scores, which across uses a variety of acoustic features and classifiers. An effective post-processing method is studied to improve the performance of Constant Q cepstral coefficients (CQCC) and to form a base feature set with some other classical acoustic features. We also proposed using an ensemble classifier set, which includes multiple Gaussian Mixture Model (GMM) based classifiers and two novel GMM mean supervector-Gradient Boosting Decision Tree (GSV-GBDT) and GSV-Random Forest (GSV-RF) classifiers. Experimental results have shown that the proposed ensemble learning system can provide substantially better performance than baseline. On common training condition of the challenge, Equal Error Rate (EER) of primary system on development set is 1.5%, compared to baseline 10.4%. EER of primary system (S02 in ASVspoof 2017 board) on evaluation data set are 12.3% (with only train dataset) and 10.8% (with train+dev dataset), which are also much better than baseline 30.6% and 24.8%, given by ASVSpoof 2017 organizer, with 59.7% and 56.4% relative performance improvement",
    "checked": true,
    "id": "8a862946febf154effe65d4ec94d8e6bd39f690d",
    "semantic_title": "ensemble learning for countermeasure of audio replay spoofing attack in asvspoof2017",
    "citation_count": 45
  },
  "https://www.isca-speech.org/archive/interspeech_2017/li17b_interspeech.html": {
    "title": "A Study on Replay Attack and Anti-Spoofing for Automatic Speaker Verification",
    "volume": "main",
    "abstract": "For practical automatic speaker verification (ASV) systems, replay attack poses a true risk. By replaying a pre-recorded speech signal of the genuine speaker, ASV systems tend to be easily fooled. An effective replay detection method is therefore highly desirable. In this study, we investigate a major difficulty in replay detection: the over-fitting problem caused by variability factors in speech signal. An F-ratio probing tool is proposed and three variability factors are investigated using this tool: speaker identity, speech content and playback & recording device. The analysis shows that device is the most influential factor that contributes the highest over-fitting risk. A frequency warping approach is studied to alleviate the over-fitting problem, as verified on the ASV-spoof 2017 database",
    "checked": true,
    "id": "466196986827959c10dfddce40202ad3e8341968",
    "semantic_title": "a study on replay attack and anti-spoofing for automatic speaker verification",
    "citation_count": 50
  },
  "https://www.isca-speech.org/archive/interspeech_2017/nagarsheth17_interspeech.html": {
    "title": "Replay Attack Detection Using DNN for Channel Discrimination",
    "volume": "main",
    "abstract": "Voice is projected to be the next input interface for portable devices. The increased use of audio interfaces can be mainly attributed to the success of speech and speaker recognition technologies. With these advances comes the risk of criminal threats where attackers are reportedly trying to access sensitive information using diverse voice spoofing techniques. Among them, replay attacks pose a real challenge to voice biometrics. This paper addresses the problem by proposing a deep learning architecture in tandem with low-level cepstral features. We investigate the use of a deep neural network (DNN) to discriminate between the different channel conditions available in the ASVSpoof 2017 dataset, namely recording, playback and session conditions. The high-level feature vectors derived from this network are used to discriminate between genuine and spoofed audio. Two kinds of low-level features are utilized: state-of-the-art constant-Q cepstral coefficients (CQCC), and our proposed high-frequency cepstral coefficients (HFCC) that derive from the high-frequency spectrum of the audio. The fusion of both features proved to be effective in generalizing well across diverse replay attacks seen in the evaluation of the ASVSpoof 2017 challenge, with an equal error rate of 11.5%, that is 53% better than the baseline Gaussian Mixture Model (GMM) applied on CQCC",
    "checked": true,
    "id": "9089817cb2a7eaf1d5835034fa5ba3f76f375cd4",
    "semantic_title": "replay attack detection using dnn for channel discrimination",
    "citation_count": 100
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17_interspeech.html": {
    "title": "ResNet and Model Fusion for Automatic Spoofing Detection",
    "volume": "main",
    "abstract": "Speaker verification systems have achieved great progress in recent years. Unfortunately, they are still highly prone to different kinds of spoofing attacks such as speech synthesis, voice conversion, and fake audio recordings etc. Inspired by the success of ResNet in image recognition, we investigated the effectiveness of using ResNet for automatic spoofing detection. Experimental results on the ASVspoof2017 data set show that ResNet performs the best among all the single-model systems. Model fusion is a good way to further improve the system performance. Nevertheless, we found that if the same feature is used for different fused models, the resulting system can hardly be improved. By using different features and models, our best fused model further reduced the Equal Error Rate (EER) by 18% relatively, compared with the best single-model system",
    "checked": true,
    "id": "6a7b88c8dc37850f8ffe48dcf7d839c6f0d47873",
    "semantic_title": "resnet and model fusion for automatic spoofing detection",
    "citation_count": 117
  },
  "https://www.isca-speech.org/archive/interspeech_2017/alluri17_interspeech.html": {
    "title": "SFF Anti-Spoofer: IIIT-H Submission for Automatic Speaker Verification Spoofing and Countermeasures Challenge 2017",
    "volume": "main",
    "abstract": "The ASVspoof 2017 challenge is about the detection of replayed speech from human speech. The proposed system makes use of the fact that when the speech signals are replayed, they pass through multiple channels as opposed to original recordings. This channel information is typically embedded in low signal to noise ratio regions. A speech signal processing method with high spectro-temporal resolution is required to extract robust features from such regions. The single frequency filtering (SFF) is one such technique, which we propose to use for replay attack detection. While SFF based feature representation was used at front-end, Gaussian mixture model and bi-directional long short-term memory models are investigated at the backend as classifiers. The experimental results on ASVspoof 2017 dataset reveal that, SFF based representation is very effective in detecting replay attacks. The score level fusion of back end classifiers further improved the performance of the system which indicates that both classifiers capture complimentary information",
    "checked": true,
    "id": "249e6da34b4218a3960e65f92c4c554ede62a412",
    "semantic_title": "sff anti-spoofer: iiit-h submission for automatic speaker verification spoofing and countermeasures challenge 2017",
    "citation_count": 56
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hartmann17_interspeech.html": {
    "title": "Improved Single System Conversational Telephone Speech Recognition with VGG Bottleneck Features",
    "volume": "main",
    "abstract": "On small datasets, discriminatively trained bottleneck features from deep networks commonly outperform more traditional spectral or cepstral features. While these features are typically trained with small, fully-connected networks, recent studies have used more sophisticated networks with great success. We use the recent deep CNN (VGG) network for bottleneck feature extraction — previously used only for low-resource tasks — and apply it to the Switchboard English conversational telephone speech task. Unlike features derived from traditional MLP networks, the VGG features outperform cepstral features even when used with BLSTM acoustic models trained on large amounts of data. We achieve the best BBN single system performance when combining the VGG features with a BLSTM acoustic model. When decoding with an n-gram language model, which are used for deployable systems, we have a realistic production system with a WER of 7.4%. This result is competitive with the current state-of-the-art in the literature. While our focus is on realistic single system performance, we further reduce the WER to 6.1% through system combination and using expensive neural network language model rescoring",
    "checked": true,
    "id": "850730adb72d900c9b484829c42f07c20e9af06d",
    "semantic_title": "improved single system conversational telephone speech recognition with vgg bottleneck features",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wong17_interspeech.html": {
    "title": "Student-Teacher Training with Diverse Decision Tree Ensembles",
    "volume": "main",
    "abstract": "Student-teacher training allows a large teacher model or ensemble of teachers to be compressed into a single student model, for the purpose of efficient decoding. However, current approaches in automatic speech recognition assume that the state clusters, often defined by Phonetic Decision Trees (PDT), are the same across all models. This limits the diversity that can be captured within the ensemble, and also the flexibility when selecting the complexity of the student model output. This paper examines an extension to student-teacher training that allows for the possibility of having different PDTs between teachers, and also for the student to have a different PDT from the teacher. The proposal is to train the student to emulate the logical context dependent state posteriors of the teacher, instead of the frame posteriors. This leads to a method of mapping frame posteriors from one PDT to another. This approach is evaluated on three speech recognition tasks: the Tok Pisin and Javanese low resource conversational telephone speech tasks from the IARPA Babel programme, and the HUB4 English broadcast news task",
    "checked": true,
    "id": "15a1e3afa2e91477f21c3ddcb1fc58a58b6d772f",
    "semantic_title": "student-teacher training with diverse decision tree ensembles",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2017/cui17_interspeech.html": {
    "title": "Embedding-Based Speaker Adaptive Training of Deep Neural Networks",
    "volume": "main",
    "abstract": "An embedding-based speaker adaptive training (SAT) approach is proposed and investigated in this paper for deep neural network acoustic modeling. In this approach, speaker embedding vectors, which are a constant given a particular speaker, are mapped through a control network to layer-dependent element-wise affine transformations to canonicalize the internal feature representations at the output of hidden layers of a main network. The control network for generating the speaker-dependent mappings are jointly estimated with the main network for the overall speaker adaptive acoustic modeling. Experiments on large vocabulary continuous speech recognition (LVCSR) tasks show that the proposed SAT scheme can yield superior performance over the widely-used speaker-aware training using i-vectors with speaker-adapted input features",
    "checked": true,
    "id": "0a0820133e29e8039e9eff70aaeb4ecf76cdb25c",
    "semantic_title": "embedding-based speaker adaptive training of deep neural networks",
    "citation_count": 37
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ma17_interspeech.html": {
    "title": "Improving Deliverable Speech-to-Text Systems with Multilingual Knowledge Transfer",
    "volume": "main",
    "abstract": "This paper reports our recent progress on using multilingual data for improving speech-to-text (STT) systems that can be easily delivered. We continued the work BBN conducted on the use of multilingual data for improving Babel evaluation systems, but focused on training time-delay neural network (TDNN) based chain models. As done for the Babel evaluations, we used multilingual data in two ways: first, to train multilingual deep neural networks (DNN) for extracting bottle-neck (BN) features, and second, for initializing training on target languages Our results show that TDNN chain models trained on multilingual DNN bottleneck features yield significant gains over their counterparts trained on MFCC plus i-vector features. By initializing from models trained on multilingual data, TDNN chain models can achieve great improvements over random initializations of the network weights on target languages. Two other important findings are: 1) initialization with multilingual TDNN chain models produces larger gains on target languages that have less training data; 2) inclusion of target languages in multilingual training for either BN feature extraction or initialization have limited impact on performance measured on the target languages. Our results also reveal that for TDNN chain models, the combination of multilingual BN features and multilingual initialization achieves the best performance on all target languages",
    "checked": true,
    "id": "661da1a08bba29ec88c454c8913dbd828cf69f4c",
    "semantic_title": "improving deliverable speech-to-text systems with multilingual knowledge transfer",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2017/saon17_interspeech.html": {
    "title": "English Conversational Telephone Speech Recognition by Humans and Machines",
    "volume": "main",
    "abstract": "Word error rates on the Switchboard conversational corpus that just a few years ago were 14% have dropped to 8.0%, then 6.6% and most recently 5.8%, and are now believed to be within striking range of human performance. This then raises two issues: what is human performance, and how far down can we still drive speech recognition error rates? In trying to assess human performance, we performed an independent set of measurements on the Switchboard and CallHome subsets of the Hub5 2000 evaluation and found that human accuracy may be considerably better than what was earlier reported, giving the community a significantly harder goal to achieve. We also report on our own efforts in this area, presenting a set of acoustic and language modeling techniques that lowered the WER of our system to 5.5%/10.3% on these subsets, which is a new performance milestone (albeit not at what we measure to be human performance). On the acoustic side, we use a score fusion of one LSTM with multiple feature inputs, a second LSTM trained with speaker-adversarial multi-task learning and a third convolutional residual net (ResNet). On the language modeling side, we use word and character LSTMs and convolutional WaveNet-style language models",
    "checked": true,
    "id": "c9bd15c7838c1d3cdd5f5113a2efd9440f86b3da",
    "semantic_title": "english conversational telephone speech recognition by humans and machines",
    "citation_count": 346
  },
  "https://www.isca-speech.org/archive/interspeech_2017/stolcke17_interspeech.html": {
    "title": "Comparing Human and Machine Errors in Conversational Speech Transcription",
    "volume": "main",
    "abstract": "Recent work in automatic recognition of conversational telephone speech (CTS) has achieved accuracy levels comparable to human transcribers, although there is some debate how to precisely quantify human performance on this task, using the NIST 2000 CTS evaluation set. This raises the question what systematic differences, if any, may be found differentiating human from machine transcription errors. In this paper we approach this question by comparing the output of our most accurate CTS recognition system to that of a standard speech transcription vendor pipeline. We find that the most frequent substitution, deletion and insertion error types of both outputs show a high degree of overlap. The only notable exception is that the automatic recognizer tends to confuse filled pauses (\"uh\") and backchannel acknowledgments (\"uhhuh\"). Human tend not to make this error, presumably due to the distinctive and opposing pragmatic functions attached to these words. Furthermore, we quantify the correlation between human and machine errors at the speaker level, and investigate the effect of speaker overlap between training and test data. Finally, we report on an informal \"Turing test\" asking humans to discriminate between automatic and human transcription error cases",
    "checked": true,
    "id": "002b9e38bf1f82a2b876082cb866c67e8ace2ba1",
    "semantic_title": "comparing human and machine errors in conversational speech transcription",
    "citation_count": 51
  },
  "https://www.isca-speech.org/archive/interspeech_2017/petukhova17_interspeech.html": {
    "title": "Multimodal Markers of Persuasive Speech: Designing a Virtual Debate Coach",
    "volume": "main",
    "abstract": "The study presented in this paper is carried out to support debate performance assessment in the context of debate skills training. The perception of good performance as a debater is influenced by how believable and convincing the debater's argumentation is. We identified a number of features that are useful for explaining perceived properties of persuasive speech and for defining rules and strategies to produce and assess debate performance. We collected and analysed multimodal and multisensory data of the trainees debate behaviour, and contrasted it with those of skilled professional debaters. Observational, correlation and machine learning studies were performed to identify multimodal markers of persuasive speech and link them to experts' assessments. A combination of multimodal in- and out-of-domain debate data, and various non-verbal, prosodic, lexical, linguistic and structural features has been computed based on our analysis and assessed used to , and several classification procedures has been applied achieving an accuracy of 0.79 on spoken debate data",
    "checked": true,
    "id": "814f31f7dbaf5f6341407cc9223d72ba4ff6c5ae",
    "semantic_title": "multimodal markers of persuasive speech: designing a virtual debate coach",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bone17_interspeech.html": {
    "title": "Acoustic-Prosodic and Physiological Response to Stressful Interactions in Children with Autism Spectrum Disorder",
    "volume": "main",
    "abstract": "Social anxiety is a prevalent condition affecting individuals to varying degrees. Research on autism spectrum disorder (ASD), a group of neurodevelopmental disorders marked by impairments in social communication, has found that social anxiety occurs more frequently in this population. Our study aims to further understand the multimodal manifestation of social stress for adolescents with ASD versus neurotypically developing (TD) peers. We investigate this through objective measures of speech behavior and physiology (mean heart rate) acquired during three tasks: a low-stress conversation, a medium-stress interview, and a high-stress presentation. Measurable differences are found to exist for speech behavior and heart rate in relation to task-induced stress. Additionally, we find the acoustic measures are particularly effective for distinguishing between diagnostic groups. Individuals with ASD produced higher prosodic variability, agreeing with previous reports. Moreover, the most informative features captured an individual's vocal changes between low and high social-stress, suggesting an interaction between vocal production and social stressors in ASD",
    "checked": true,
    "id": "4cec5b4afd01f59558942db3fe13e697f85a2c67",
    "semantic_title": "acoustic-prosodic and physiological response to stressful interactions in children with autism spectrum disorder",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2017/burmania17_interspeech.html": {
    "title": "A Stepwise Analysis of Aggregated Crowdsourced Labels Describing Multimodal Emotional Behaviors",
    "volume": "main",
    "abstract": "Affect recognition is a difficult problem that most often relies on human annotated data to train automated systems. As humans perceive emotion differently based on personality, cognitive state and past experiences, it is important to collect rankings from multiple individuals to assess the emotional content in corpora, which are later aggregated with rules such as majority vote. With the increased use of crowdsourcing services for perceptual evaluations, collecting large amount of data is now feasible. It becomes important to question the amount of data needed to create well-trained classifiers. How different are the aggregated labels collected from five raters compared to the ones obtained from twenty evaluators? Is it worthwhile to spend resources to increase the number of evaluators beyond those used in conventional/laboratory studies? This study evaluates the consensus labels obtained by incrementally adding new evaluators during perceptual evaluations. Using majority vote over categorical emotional labels, we compare the changes in the aggregated labels starting with one rater, and finishing with 20 raters. The large number of evaluators in a subset of the MSP-IMPROV database and the ability to filter annotators by quality allows us to better understand label aggregation as a function of the number of annotators",
    "checked": true,
    "id": "34a45c92139a9ffd0c76f8eee2201485232bdb20",
    "semantic_title": "a stepwise analysis of aggregated crowdsourced labels describing multimodal emotional behaviors",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2017/fotedar17_interspeech.html": {
    "title": "An Information Theoretic Analysis of the Temporal Synchrony Between Head Gestures and Prosodic Patterns in Spontaneous Speech",
    "volume": "main",
    "abstract": "We analyze the temporal co-ordination between head gestures and prosodic patterns in spontaneous speech in a data-driven manner. For this study, we consider head motion and speech data from 24 subjects while they tell a fixed set of five stories. The head motion, captured using a motion capture system, is converted to Euler angles and translations in X, Y and Z-directions to represent head gestures. Pitch and short-time energy in voiced segments are used to represent the prosodic patterns. To capture the statistical relationship between head gestures and prosodic patterns, mutual information (MI) is computed at various delays between the two using data from 24 subjects in six native languages. The estimated MI, averaged across all subjects, is found to be maximum when the head gestures lag the prosodic patterns by 30msec. This is found to be true when subjects tell stories in English as well as in their native language. We observe a similar pattern in the root mean squared error of predicting head gestures from prosodic patterns using Gaussian mixture model. These results indicate that there could be an asynchrony between head gestures and prosody during spontaneous speech where head gestures follow the corresponding prosodic patterns",
    "checked": true,
    "id": "7292a91864be55f734a7088be70594db075e8d00",
    "semantic_title": "an information theoretic analysis of the temporal synchrony between head gestures and prosodic patterns in spontaneous speech",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2017/huang17_interspeech.html": {
    "title": "Multimodal Prediction of Affective Dimensions via Fusing Multiple Regression Techniques",
    "volume": "main",
    "abstract": "This paper presents a multimodal approach to predict affective dimensions, that makes full use of features from audio, video, Electrodermal Activity (EDA) and Electrocardiogram (ECG) using three regression techniques such as support vector regression (SVR), partial least squares regression (PLS), and a deep bidirectional long short-term memory recurrent neural network (DBLSTM-RNN) regression. Each of the three regression techniques performs multimodal affective dimension prediction followed by a fusion of different models on features of four modalities using a support vector regression. A support vector regression is also applied for a final fusion of the three regression systems. Experiments show that our proposed approach obtains promising results on the AVEC 2015 benchmark dataset for prediction of multimodal affective dimensions. For the development set, the concordance correlation coefficient (CCC) achieves results of 0.856 for arousal and 0.720 for valence, which increases 3.88% and 4.66% of the top-performer of AVEC 2015 in arousal and valence, respectively",
    "checked": true,
    "id": "dad58628834572e085c815b2601171187602e3b4",
    "semantic_title": "multimodal prediction of affective dimensions via fusing multiple regression techniques",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2017/dohen17_interspeech.html": {
    "title": "Co-Production of Speech and Pointing Gestures in Clear and Perturbed Interactive Tasks: Multimodal Designation Strategies",
    "volume": "main",
    "abstract": "Designation consists in attracting an interlocutor's attention on a specific object and/or location. It is most often achieved using both speech (e.g., demonstratives) and gestures (e.g., manual pointing). This study aims at analyzing how speech and pointing gestures are co-produced in a semi-directed interactive task involving designation. 20 native speakers of French were involved in a cooperative task in which they provided instructions to a partner for her to reproduce a model she could not see on a grid both of them saw. They had to use only sentences of the form ‘The [target word] goes there.'. They did this in two conditions: silence and noise. Their speech and articulatory/hand movements (motion capture) were recorded. The analyses show that the participants' speech features were modified in noise (Lombard effect). They also spoke slower and made more pauses and errors. Their pointing gestures lasted longer and started later showing an adaptation of gesture production to speech. The condition did not influence speech/gesture coordination. The apex (part of the gesture that shows) mainly occurred at the same time as the target word and not as the demonstrative showing that speakers group speech and gesture carrying complementary rather than redundant information",
    "checked": true,
    "id": "d80b346673bb9c4db02e8aca4935b218941af9a3",
    "semantic_title": "co-production of speech and pointing gestures in clear and perturbed interactive tasks: multimodal designation strategies",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2017/guzewich17_interspeech.html": {
    "title": "Improving Speaker Verification for Reverberant Conditions with Deep Neural Network Dereverberation Processing",
    "volume": "main",
    "abstract": "We present an improved method for training Deep Neural Networks for dereverberation and show that it can improve performance for the speech processing tasks of speaker verification and speech enhancement. We replicate recently proposed methods for dereverberation using Deep Neural Networks and present our improved method, highlighting important aspects that influence performance. We then experimentally evaluate the capabilities and limitations of the method with respect to speech quality and speaker verification to show that ours achieves better performance than other proposed methods",
    "checked": true,
    "id": "5fad2d973e1a1ec3903d38b999fbda27342560ef",
    "semantic_title": "improving speaker verification for reverberant conditions with deep neural network dereverberation processing",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bulling17_interspeech.html": {
    "title": "Stepsize Control for Acoustic Feedback Cancellation Based on the Detection of Reverberant Signal Periods and the Estimated System Distance",
    "volume": "main",
    "abstract": "A new approach for acoustic feedback cancellation is presented. The challenge in acoustic feedback cancellation is a strong correlation between the local speech and the loudspeaker signal. Due to this correlation, the convergence rate of adaptive algorithms is limited. Therefore, a novel stepsize control of the adaptive filter is presented. The stepsize control exploits reverberant signal periods to update the adaptive filter. As soon as local speech stops, the reverberation energy of the system decays exponentially. This means that during reverberation there is only excitation of the filter but no local speech. Thus, signals are not correlated and the filter can converge without correlation problems. Consequently, the stepsize control accelerates the adaption process during reverberation and slows it down at the beginning of speech activity. It is shown, that with a particular gain control, the reverb-based stepsize control can be interpreted as the theoretical optimum stepsize. However, for this purpose a precise estimation of the system distance is required. One estimation method is presented. The proposed estimator has a rescue mechanism to detect enclosure dislocations. Both, simulations and real world testing show that the acoustic feedback canceler is capable of improving stability and convergence rate, even at high system gains",
    "checked": true,
    "id": "36a39db98d31c8aabbd6852345edd63898a55c2e",
    "semantic_title": "stepsize control for acoustic feedback cancellation based on the detection of reverberant signal periods and the estimated system distance",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2017/franzen17_interspeech.html": {
    "title": "A Delay-Flexible Stereo Acoustic Echo Cancellation for DFT-Based In-Car Communication (ICC) Systems",
    "volume": "main",
    "abstract": "In-car communication (ICC) systems supporting speech communication in noise by reproducing amplified speech from the car cabin in the car cabin ask for low-delay acoustic echo cancellation (AEC). In this paper we propose a delay-flexible DFT-based stereo AEC capable of cancelling also the echoes stemming from the audio player or FM radio. For the price of a somewhat higher complexity we are able to reduce the 32 ms delay of the baseline down to 4 ms, loosing only 1 dB in ERLE while even preserving system distance properties",
    "checked": true,
    "id": "bc0f522aa0e225a2bc83ba46c4fd7570c9b51cd6",
    "semantic_title": "a delay-flexible stereo acoustic echo cancellation for dft-based in-car communication (icc) systems",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wang17b_interspeech.html": {
    "title": "Speech Enhancement Based on Harmonic Estimation Combined with MMSE to Improve Speech Intelligibility for Cochlear Implant Recipients",
    "volume": "main",
    "abstract": "In this paper, a speech enhancement algorithm is proposed to improve the speech intelligibility for cochlear implant recipients. Our method is based on combination of harmonic estimation and traditional statistical method. Traditional statistical based speech enhancement method is effective only for stationary noise suppression, but not non-stationary noise. To address more complex noise scenarios, we explore the harmonic structure of target speech to obtain a more accurate noise estimation. The estimated noise is then employed in the MMSE framework to obtain the gain function for recovering the target speech. Listening test experiments show a substantial speech intelligibility improvement for cochlear implant recipients in noisy environments",
    "checked": true,
    "id": "1e118317c8b26448b13559e0dd69010cab2aa954",
    "semantic_title": "speech enhancement based on harmonic estimation combined with mmse to improve speech intelligibility for cochlear implant recipients",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ayllon17_interspeech.html": {
    "title": "Improving Speech Intelligibility in Binaural Hearing Aids by Estimating a Time-Frequency Mask with a Weighted Least Squares Classifier",
    "volume": "main",
    "abstract": "An efficient algorithm for speech enhancement in binaural hearing aids is proposed. The algorithm is based on the estimation of a time-frequency mask using supervised machine learning. The standard least-squares linear classifier is reformulated to optimize a metric related to speech/noise separation. The method is energy-efficient in two ways: the computational complexity is limited and the wireless data transmission optimized. The ability of the algorithm to enhance speech contaminated with different types of noise and low SNR has been evaluated. Objective measures of speech intelligibility and speech quality demonstrate that the algorithm increments both the hearing comfort and speech understanding of the user. These results are supported by subjective listening tests",
    "checked": true,
    "id": "616054b72d505b539a3b9ce2a17e485123505fa2",
    "semantic_title": "improving speech intelligibility in binaural hearing aids by estimating a time-frequency mask with a weighted least squares classifier",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wu17_interspeech.html": {
    "title": "Simulations of High-Frequency Vocoder on Mandarin Speech Recognition for Acoustic Hearing Preserved Cochlear Implant",
    "volume": "main",
    "abstract": "Vocoder simulations are generally adopted to simulate the electrical hearing induced by the cochlear implant (CI). Our research group is developing a new four-electrode CI microsystem which induces high-frequency electrical hearing while preserving low-frequency acoustic hearing. To simulate the functionality of this CI, a previously developed hearing-impaired (HI) hearing model is combined with a 4-channel vocoder in this paper to respectively mimic the perceived acoustic hearing and electrical hearing. Psychoacoustic experiments are conducted on Mandarin speech recognition for determining parameters of electrodes for this CI. Simulation results show that initial consonants of Mandarin are more difficult to recognize than final vowels of Mandarin via acoustic hearing of HI patients. After electrical hearing being induced through logarithmic-frequency distributed electrodes, speech intelligibility of HI patients is boosted for all Mandarin phonemes, especially for initial consonants. Similar results are consistently observed in clean and noisy test conditions",
    "checked": true,
    "id": "6dfe77ccbea7e0eef743e5d6a4e8af58a2b2ac37",
    "semantic_title": "simulations of high-frequency vocoder on mandarin speech recognition for acoustic hearing preserved cochlear implant",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hermes17_interspeech.html": {
    "title": "Phonetic Correlates of Pharyngeal and Pharyngealized Consonants in Saudi, Lebanese, and Jordanian Arabic: An rt-MRI Study",
    "volume": "main",
    "abstract": "The phonemic inventory of Arabic includes sounds that involve a pharyngeal constriction. Sounds referred to as ‘pharyngeal' (/ʕ/ and /ħ/) are reported to have a primary constriction in the pharynx, while sounds referred to as ‘pharyngealized' (/s /, /t /, /d /, and /ð / or /z /) are reported to have a secondary constriction in the pharynx. Some studies propose grouping both types of sounds together, citing phonetic and phonological evidence. Phonetically, pharyngeal consonants are argued to have a primary constriction below the pharynx, and are thus posited to be pharyngealized laryngeals. Under this view, the pharyngeal constriction is secondary, not primary. Phonologically, it has been established that pharyngealized sounds trigger pharyngealization spread, and proposals for grouping pharyngeal and pharyngealized consonants together cite similar, but not identical, spread patterns triggered by pharyngeals. In this study, Real-time Magnetic Resonance Imaging is employed to investigate the phonetic correlates of the pharyngeal constriction in both pharyngeal and pharyngealized sounds in Saudi, Lebanese, and Jordanian Arabic as exemplified by one speaker from each dialect. Our findings demonstrate a difference in the location of constriction among both types of sounds. These distinctions in place possibly account for the differences in the spread patterns triggered by each type of sound",
    "checked": true,
    "id": "f9f4c903762e1102ad98f44444d6f5d9923da1fc",
    "semantic_title": "phonetic correlates of pharyngeal and pharyngealized consonants in saudi, lebanese, and jordanian arabic: an rt-mri study",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2017/elie17_interspeech.html": {
    "title": "Glottal Opening and Strategies of Production of Fricatives",
    "volume": "main",
    "abstract": "This work investigates the influence of the gradual opening of the glottis along its length during the production of fricatives in intervocalic contexts. Acoustic simulations reveal the existence of a transient zone in the articulatory space where the frication noise level is very sensitive to small perturbations of the glottal opening. This corresponds to the configurations where both frication noise and voiced contributions are present in the speech signal. To avoid this unstability, speakers may adopt different strategies to ensure the voiced/voiceless contrast of fricatives. This is evidenced by experimental data of simultaneous glottal opening measurements, performed with ePGG, and audio recordings of vowel-fricative-vowel pseudowords. Voiceless fricatives are usually longer, in order to maximize the number of voiceless time frames over voiced frames due to the crossing of the transient regime. For voiced fricatives, the speaker may avoid the unstable regime by keeping low frication noise level, and thus by favoring the voicing characteristic, or by doing very short crossings into the unstable regime. It is also shown that when speakers are asked to sustain voiced fricatives longer than in natural speech, they adopt the strategy of keeping low frication noise level to avoid the unstable regime",
    "checked": true,
    "id": "9bbb42a98fdd09c51c264b293e20590e684e4527",
    "semantic_title": "glottal opening and strategies of production of fricatives",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2017/frej17_interspeech.html": {
    "title": "Acoustics and Articulation of Medial versus Final Coronal Stop Gemination Contrasts in Moroccan Arabic",
    "volume": "main",
    "abstract": "This paper presents results of a simultaneous acoustic and articulatory investigation of word-medial and word-final geminate/singleton coronal stop contrasts in Moroccan Arabic (MA). The acoustic analysis revealed that, only for the word-medial contrast, the two MA speakers adopted comparable strategies in contrasting geminates with singletons, mainly by significantly lengthening closure duration in geminates, relative to singletons. In word-final position, two speaker-specific contrasting patterns emerged. While one speaker also lengthened the closure duration for final geminates, the other speaker instead lengthened only the release duration for final geminates, relative to singletons. Consonant closure and preceding vowel were significantly longer for the geminate only in medial position, not in final position. These temporal differences were even more clearly delineated in the articulatory signal, captured via ultrasound, to which we applied the novel approach of using TRACTUS [Temporally Resolved Articulatory Configuration Tracking of UltraSound: 15] to index temporal properties of closure gestures for these geminate/singleton contrasts",
    "checked": true,
    "id": "55e2565e1b3f996c114078e593b9cd8768e032d2",
    "semantic_title": "acoustics and articulation of medial versus final coronal stop gemination contrasts in moroccan arabic",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2017/turco17_interspeech.html": {
    "title": "How are Four-Level Length Distinctions Produced? Evidence from Moroccan Arabic",
    "volume": "main",
    "abstract": "We investigate the durational properties of Moroccan Arabic identical consonant sequences contrasting singleton (S) and geminate (G) dental fricatives, in six combinations of four-level length contrasts across word boundaries (#) (one timing slot for #S, two for #G and S#S, three for S#G and G#S, and four for G#G). The aim is to determine the nature of the mapping between discrete phonological timing units and phonetic durations. Acoustic results show that the largest and most systematic jump in duration is displayed between the singleton fricative on the one hand and the other sequences on the other hand. Looking at these sequences, S#S is shown to have the same duration as #G. When a geminate is within the sequence, a temporal reorganization is observed: G#S is not significantly longer than S#S and #G; and G#G is only slightly longer than S#G. Instead of a four-way hierarchy, our data point towards a possible upper limit of three-way length contrasts for consonants: S < G=S#S=G#S < S#G=G#G. The interplay of a number of factors resulting in this mismatch between phonological length and phonetic duration are discussed, and a working hypothesis is provided for why duration contrasts are rarely ternary, and almost never quaternary",
    "checked": true,
    "id": "c7a6ebd5e51b1cf995fec0eed7c3e2bebd04e1ad",
    "semantic_title": "how are four-level length distinctions produced? evidence from moroccan arabic",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jones17_interspeech.html": {
    "title": "Vowels in the Barunga Variety of North Australian Kriol",
    "volume": "main",
    "abstract": "North Australian Kriol is an English based creole spoken widely by Indigenous people in northern Australia in areas where the traditional languages are endangered or no longer spoken. This paper offers the first acoustic description of the vowel phonology of Roper Kriol, within a variety spoken at Barunga Community, east of the town of Katherine in the Northern Territory Drawing on a new corpus for Barunga Kriol, the paper presents analyses of the short and long monophthongs, as well as the diphthongs in the spontaneous speech of young adults. The results show the durations and spectral characteristics of the vowels, including major patterns of allophony (i.e. coarticulation and context effects). This updates the phonology over the previous description from the 1970s, showing that there is an additional front low vowel phoneme in the speech of young people today, as well as a vowel length contrast. Interestingly there are points of similarity with the vowel acoustics for traditional Aboriginal languages of the region, for example in a relatively compact vowel space and in the modest trajectories of diphthongs",
    "checked": true,
    "id": "5e38240b3a9425798125345ddec3c6b6a5033744",
    "semantic_title": "vowels in the barunga variety of north australian kriol",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2017/dutta17_interspeech.html": {
    "title": "Nature of Contrast and Coarticulation: Evidence from Mizo Tones and Assamese Vowel Harmony",
    "volume": "main",
    "abstract": "Tonal coarticulation is universally found to be greater in extent in the carryover direction compared to the anticipatory direction ([1], [2], [3], [4], [5]) leading to assimilatory processes. In general, carryover coarticulation has been understood to be due to intertio-mechanical forces, and, anticipatory effects are seen to be a consequence of parallel activation of articulatory plans ([6]). In this paper, we report on results from a set of Artificial Neural Networks (ANN) trained to predict adjacent tones in disyllabic sequences. Our results confirm the universal pattern of greater carryover effects in Mizo leading to tonal assimilation. In addition, we report on results from single-layered ANN models and Support Vector Machines (SVM) that predict the identity of V from V (anticipatory) consistently better than V from V (carryover) in Assamese non-harmonic #…V CV …# sequences. The directionality in the performance of the V and V models, help us conclude that the directionality effect of coarticulation in Assamese non-harmonic sequences is greater in the anticipatory direction, which is the same direction as in the harmonic sequences. We argue that coarticulatory propensity exhibits a great deal of sensitivity to the nature of contrast in a language",
    "checked": true,
    "id": "09a76f55dfd08a5515410c1ef5782ece9795f28b",
    "semantic_title": "nature of contrast and coarticulation: evidence from mizo tones and assamese vowel harmony",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2017/cabral17_interspeech.html": {
    "title": "The Influence of Synthetic Voice on the Evaluation of a Virtual Character",
    "volume": "main",
    "abstract": "Graphical realism and the naturalness of the voice used are important aspects to consider when designing a virtual agent or character. In this work, we evaluate how synthetic speech impacts people's perceptions of a rendered virtual character. Using a controlled experiment, we focus on the role that speech, in particular voice expressiveness in the form of personality, has on the assessment of voice level and character level perceptions. We found that people rated a real human voice as more expressive, understandable and likeable than the expressive synthetic voice we developed. Contrary to our expectations, we found that the voices did not have a significant impact on the character level judgments; people in the voice conditions did not significantly vary on their ratings of appeal, credibility, human-likeness and voice matching the character. The implications this has for character design and how this compares with previous work are discussed",
    "checked": true,
    "id": "ad48f676dc43d5e65035ed0d0d79f54243746e68",
    "semantic_title": "the influence of synthetic voice on the evaluation of a virtual character",
    "citation_count": 25
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gully17_interspeech.html": {
    "title": "Articulatory Text-to-Speech Synthesis Using the Digital Waveguide Mesh Driven by a Deep Neural Network",
    "volume": "main",
    "abstract": "Following recent advances in direct modeling of the speech waveform using a deep neural network, we propose a novel method that directly estimates a physical model of the vocal tract from the speech waveform, rather than magnetic resonance imaging data. This provides a clear relationship between the model and the size and shape of the vocal tract, offering considerable flexibility in terms of speech characteristics such as age and gender. Initial tests indicate that despite a highly simplified physical model, intelligible synthesized speech is obtained. This illustrates the potential of the combined technique for the control of physical models in general, and hence the generation of more natural-sounding synthetic speech",
    "checked": true,
    "id": "a09a29e6fc522420114314c024933d3dfe7345d3",
    "semantic_title": "articulatory text-to-speech synthesis using the digital waveguide mesh driven by a deep neural network",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2017/maguer17_interspeech.html": {
    "title": "An HMM/DNN Comparison for Synchronized Text-to-Speech and Tongue Motion Synthesis",
    "volume": "main",
    "abstract": "We present an end-to-end text-to-speech (TTS) synthesis system that generates audio and synchronized tongue motion directly from text. This is achieved by adapting a statistical shape space model of the tongue surface to an articulatory speech corpus and training a speech synthesis system directly on the tongue model parameter weights. We focus our analysis on the application of two standard methodologies, based on Hidden Markov Models (HMMs) and Deep Neural Networks (DNNs), respectively, to train both acoustic models and the tongue model parameter weights. We evaluate both methodologies at every step by comparing the predicted articulatory movements against the reference data. The results show that even with less than 2h of data, DNNs already outperform HMMs",
    "checked": true,
    "id": "a464a5a31c35a4cab1fc1eeed31177cdcf260b8b",
    "semantic_title": "an hmm/dnn comparison for synchronized text-to-speech and tongue motion synthesis",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2017/alexander17_interspeech.html": {
    "title": "VCV Synthesis Using Task Dynamics to Animate a Factor-Based Articulatory Model",
    "volume": "main",
    "abstract": "This paper presents an initial architecture for articulatory synthesis which combines a dynamical system for the control of vocal tract shaping with a novel MATLAB implementation of an articulatory synthesizer. The dynamical system controls a speaker-specific vocal tract model derived by factor analysis of mid-sagittal real-time MRI data and provides input to the articulatory synthesizer, which simulates the propagation of sound waves in the vocal tract. First, parameters of the dynamical system are estimated from real-time MRI data of human speech production. Second, vocal-tract dynamics is simulated for vowel-consonant-vowel utterances using a sequence of two dynamical systems: the first one starts from a vowel vocal-tract configuration and achieves a vocal-tract closure; the second one starts from the closure and achieves the target configuration of the second vowel. Third, vocal-tract dynamics is converted to area function dynamics and is input to the synthesizer to generate the acoustic signal. Synthesized vowel-consonant-vowel examples demonstrate the feasibility of the method",
    "checked": true,
    "id": "1d8e1de2cbd6b7c1e508f01d1a6a8f4bd6738556",
    "semantic_title": "vcv synthesis using task dynamics to animate a factor-based articulatory model",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mendelson17_interspeech.html": {
    "title": "Beyond the Listening Test: An Interactive Approach to TTS Evaluation",
    "volume": "main",
    "abstract": "Traditionally, subjective text-to-speech (TTS) evaluation is performed through audio-only listening tests, where participants evaluate unrelated, context-free utterances. The ecological validity of these tests is questionable, as they do not represent real-world end-use scenarios. In this paper, we examine a novel approach to TTS evaluation in an imagined end-use, via a complex interaction with an avatar. 6 different voice conditions were tested: Natural speech, Unit Selection and Parametric Synthesis, in neutral and expressive realizations. Results were compared to a traditional audio-only evaluation baseline. Participants in both studies rated the voices for naturalness and expressivity. The baseline study showed canonical results for naturalness: Natural speech scored highest, followed by Unit Selection, then Parametric synthesis. Expressivity was clearly distinguishable in all conditions. In the avatar interaction study, participants rated naturalness in the same order as the baseline, though with smaller effect size; expressivity was not distinguishable. Further, no significant correlations were found between cognitive or affective responses and any voice conditions. This highlights 2 primary challenges in designing more valid TTS evaluations: in real-world use-cases involving interaction, listeners generally interact with a single voice, making comparative analysis unfeasible, and in complex interactions, the context and content may confound perception of voice quality",
    "checked": true,
    "id": "e25869e27af783fc2943cd34586edd5aa476a4fe",
    "semantic_title": "beyond the listening test: an interactive approach to tts evaluation",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2017/cao17_interspeech.html": {
    "title": "Integrating Articulatory Information in Deep Learning-Based Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "Articulatory information has been shown to be effective in improving the performance of hidden Markov model (HMM)-based text-to-speech (TTS) synthesis. Recently, deep learning-based TTS has outperformed HMM-based approaches. However, articulatory information has rarely been integrated in deep learning-based TTS. This paper investigated the effectiveness of integrating articulatory movement data to deep learning-based TTS. The integration of articulatory information was achieved in two ways: (1) direct integration, where articulatory and acoustic features were the output of a deep neural network (DNN), and (2) direct integration plus forward-mapping, where the output articulatory features were mapped to acoustic features by an additional DNN; These forward-mapped acoustic features were then combined with the output acoustic features to produce the final acoustic features. Articulatory (tongue and lip) and acoustic data collected from male and female speakers were used in the experiment. Both objective measures and subjective judgment by human listeners showed the approaches integrated articulatory information outperformed the baseline approach (without using articulatory information) in terms of naturalness and speaker voice identity (voice similarity)",
    "checked": true,
    "id": "bd3c26cb973503743829c4276b4eb6d3582eb2e7",
    "semantic_title": "integrating articulatory information in deep learning-based text-to-speech synthesis",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ma17b_interspeech.html": {
    "title": "Approaches for Neural-Network Language Model Adaptation",
    "volume": "main",
    "abstract": "Language Models (LMs) for Automatic Speech Recognition (ASR) are typically trained on large text corpora from news articles, books and web documents. These types of corpora, however, are unlikely to match the test distribution of ASR systems, which expect spoken utterances. Therefore, the LM is typically adapted to a smaller held-out in-domain dataset that is drawn from the test distribution. We propose three LM adaptation approaches for Deep NN and Long Short-Term Memory (LSTM): (1) Adapting the softmax layer in the Neural Network (NN); (2) Adding a non-linear adaptation layer before the softmax layer that is trained only in the adaptation phase; (3) Training the extra non-linear adaptation layer in pre-training and adaptation phases. Aiming to improve upon a hierarchical Maximum Entropy (MaxEnt) second-pass LM baseline, which factors the model into word-cluster and word models, we build an NN LM that predicts only word clusters. Adapting the LSTM LM by training the adaptation layer in both training and adaptation phases (Approach 3), we reduce the cluster perplexity by 30% on a held-out dataset compared to an unadapted LSTM LM. Initial experiments using a state-of-the-art ASR system show a 2.3% relative reduction in WER on top of an adapted MaxEnt LM",
    "checked": true,
    "id": "8fb0151c211b6394fe6fda64abff62bbb0061fef",
    "semantic_title": "approaches for neural-network language model adaptation",
    "citation_count": 33
  },
  "https://www.isca-speech.org/archive/interspeech_2017/oualil17_interspeech.html": {
    "title": "A Batch Noise Contrastive Estimation Approach for Training Large Vocabulary Language Models",
    "volume": "main",
    "abstract": "Training large vocabulary Neural Network Language Models (NNLMs) is a difficult task due to the explicit requirement of the output layer normalization, which typically involves the evaluation of the full softmax function over the complete vocabulary. This paper proposes a Batch Noise Contrastive Estimation (B-NCE) approach to alleviate this problem. This is achieved by reducing the vocabulary, at each time step, to the target words in the batch and then replacing the softmax by the noise contrastive estimation approach, where these words play the role of targets and noise samples at the same time. In doing so, the proposed approach can be fully formulated and implemented using optimal dense matrix operations. Applying B-NCE to train different NNLMs on the Large Text Compression Benchmark (LTCB) and the One Billion Word Benchmark (OBWB) shows a significant reduction of the training time with no noticeable degradation of the models performance. This paper also presents a new baseline comparative study of different standard NNLMs on the large OBWB on a single Titan-X GPU",
    "checked": true,
    "id": "e2dc0acd574bfc5f956788bb517a714080c0b111",
    "semantic_title": "a batch noise contrastive estimation approach for training large vocabulary language models",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17b_interspeech.html": {
    "title": "Investigating Bidirectional Recurrent Neural Network Language Models for Speech Recognition",
    "volume": "main",
    "abstract": "Recurrent neural network language models (RNNLMs) are powerful language modeling techniques. Significant performance improvements have been reported in a range of tasks including speech recognition compared to n-gram language models. Conventional n-gram and neural network language models are trained to predict the probability of the next word given its preceding context history. In contrast, bidirectional recurrent neural network based language models consider the context from future words as well. This complicates the inference process, but has theoretical benefits for tasks such as speech recognition as additional context information can be used. However to date, very limited or no gains in speech recognition performance have been reported with this form of model. This paper examines the issues of training bidirectional recurrent neural network language models (bi-RNNLMs) for speech recognition. A bi-RNNLM probability smoothing technique is proposed, that addresses the very sharp posteriors that are often observed in these models. The performance of the bi-RNNLMs is evaluated on three speech recognition tasks: broadcast news; meeting transcription (AMI); and low-resource systems (Babel data). On all tasks gains are observed by applying the smoothing technique to the bi-RNNLM. In addition consistent performance gains can be obtained by combining bi-RNNLMs with n-gram and uni-directional RNNLMs",
    "checked": true,
    "id": "7fe37b79f80e8937ecba653b57ebc989a56b29f9",
    "semantic_title": "investigating bidirectional recurrent neural network language models for speech recognition",
    "citation_count": 41
  },
  "https://www.isca-speech.org/archive/interspeech_2017/huang17b_interspeech.html": {
    "title": "Fast Neural Network Language Model Lookups at N-Gram Speeds",
    "volume": "main",
    "abstract": "Feed forward Neural Network Language Models (NNLM) have shown consistent gains over backoff word n-gram models in a variety of tasks. However, backoff n-gram models still remain dominant in applications with real time decoding requirements as word probabilities can be computed orders of magnitude faster than the NNLM. In this paper, we present a combination of techniques that allows us to speed up the probability computation from a neural net language model to make it comparable to the word n-gram model without any approximations. We present results on state of the art systems for Broadcast news transcription and conversational speech which demonstrate the speed improvements in real time factor and probability computation while retaining the WER gains from NNLM",
    "checked": true,
    "id": "3dac8b53c0a43df06129986eed236d179f1d16a8",
    "semantic_title": "fast neural network language model lookups at n-gram speeds",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kurata17_interspeech.html": {
    "title": "Empirical Exploration of Novel Architectures and Objectives for Language Models",
    "volume": "main",
    "abstract": "While recurrent neural network language models based on Long Short Term Memory (LSTM) have shown good gains in many automatic speech recognition tasks, Convolutional Neural Network (CNN) language models are relatively new and have not been studied in-depth. In this paper we present an empirical comparison of LSTM and CNN language models on English broadcast news and various conversational telephone speech transcription tasks. We also present a new type of CNN language model that leverages dilated causal convolution to efficiently exploit long range history. We propose a novel criterion for training language models that combines word and class prediction in a multi-task learning framework. We apply this criterion to train word and character based LSTM language models and CNN language models and show that it improves performance. Our results also show that CNN and LSTM language models are complementary and can be combined to obtain further gains",
    "checked": true,
    "id": "b0bd00f3a30bc5cbcb1a277b8ecd0bb9b4919f89",
    "semantic_title": "empirical exploration of novel architectures and objectives for language models",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2017/benes17_interspeech.html": {
    "title": "Residual Memory Networks in Language Modeling: Improving the Reputation of Feed-Forward Networks",
    "volume": "main",
    "abstract": "We introduce the Residual Memory Network (RMN) architecture to language modeling. RMN is an architecture of feed-forward neural networks that incorporates residual connections and time-delay connections that allow us to naturally incorporate information from a substantial time context. As this is the first time RMNs are applied for language modeling, we thoroughly investigate their behaviour on the well studied Penn Treebank corpus. We change the model slightly for the needs of language modeling, reducing both its time and memory consumption. Our results show that RMN is a suitable choice for small-sized neural language models: With test perplexity 112.7 and as few as 2.3M parameters, they out-perform both a much larger vanilla RNN (PPL 124, 8M parameters) and a similarly sized LSTM (PPL 115, 2.08M parameters), while being only by less than 3 perplexity points worse than twice as big LSTM",
    "checked": true,
    "id": "2ee7ee38745e9fcf89860dfb3d41c2155521e3a3",
    "semantic_title": "residual memory networks in language modeling: improving the reputation of feed-forward networks",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2017/poorjam17_interspeech.html": {
    "title": "Dominant Distortion Classification for Pre-Processing of Vowels in Remote Biomedical Voice Analysis",
    "volume": "main",
    "abstract": "Advances in speech signal analysis facilitate the development of techniques for remote biomedical voice assessment. However, the performance of these techniques is affected by noise and distortion in signals. In this paper, we focus on the vowel /a/ as the most widely-used voice signal for pathological voice assessments and investigate the impact of four major types of distortion that are commonly present during recording or transmission in voice analysis, namely: background noise, reverberation, clipping and compression, on Mel-frequency cepstral coefficients (MFCCs) — the most widely-used features in biomedical voice analysis. Then, we propose a new distortion classification approach to detect the most dominant distortion in such voice signals. The proposed method involves MFCCs as frame-level features and a support vector machine as classifier to detect the presence and type of distortion in frames of a given voice signal. Experimental results obtained from the healthy and Parkinson's voices show the effectiveness of the proposed approach in distortion detection and classification",
    "checked": true,
    "id": "6003f076ac577e2cd9e22d674b90f984fa7763fe",
    "semantic_title": "dominant distortion classification for pre-processing of vowels in remote biomedical voice analysis",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2017/le17_interspeech.html": {
    "title": "Automatic Paraphasia Detection from Aphasic Speech: A Preliminary Study",
    "volume": "main",
    "abstract": "Aphasia is an acquired language disorder resulting from brain damage that can cause significant communication difficulties. Aphasic speech is often characterized by errors known as paraphasias, the analysis of which can be used to determine an appropriate course of treatment and to track an individual's recovery progress. Being able to detect paraphasias automatically has many potential clinical benefits; however, this problem has not previously been investigated in the literature. In this paper, we perform the first study on detecting phonemic and neologistic paraphasias from scripted speech samples in AphasiaBank. We propose a speech recognition system with task-specific language models to transcribe aphasic speech automatically. We investigate features based on speech duration, Goodness of Pronunciation, phone edit distance, and Dynamic Time Warping on phoneme posteriorgrams. Our results demonstrate the feasibility of automatic paraphasia detection and outline the path toward enabling this system in real-world clinical applications",
    "checked": true,
    "id": "a61f6c2cb8618fcf193f3de695f0a59cc719df9e",
    "semantic_title": "automatic paraphasia detection from aphasic speech: a preliminary study",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2017/garcia17_interspeech.html": {
    "title": "Evaluation of the Neurological State of People with Parkinson's Disease Using i-Vectors",
    "volume": "main",
    "abstract": "The i-vector approach is used to model the speech of PD patients with the aim of assessing their condition. Features related to the articulation, phonation, and prosody dimensions of speech were used to train different i-vector extractors. Each i-vector extractor is trained using utterances from both PD patients and healthy controls. The i-vectors of the healthy control (HC) speakers are averaged to form a single i-vector that represents the HC group, i.e., the reference i-vector. A similar process is done to create a reference of the group with PD patients. Then the i-vectors of test speakers are compared to these reference i-vectors using the cosine distance. Three analyses are performed using this distance: classification between PD patients and HC, prediction of the neurological state of PD patients according to the MDS-UPDRS-III scale, and prediction of a modified version of the Frenchay Dysarthria Assessment. The Spearman's correlation between this cosine distance and the MDS-UPDRS-III scale was 0.63. These results show the suitability of this approach to monitor the neurological state of people with Parkinson's Disease",
    "checked": true,
    "id": "edad4b36bf583c4949dd2f1272b143309f300bcd",
    "semantic_title": "evaluation of the neurological state of people with parkinson's disease using i-vectors",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chien17_interspeech.html": {
    "title": "Objective Severity Assessment from Disordered Voice Using Estimated Glottal Airflow",
    "volume": "main",
    "abstract": "In clinical practice, the severity of disordered voice is typically rated by a professional with auditory-perceptual judgment. The present study aims to automate this assessment procedure, in an attempt to make the assessment objective and less labor-intensive. In the automated analysis, glottal airflow is estimated from the analyzed voice signal with an inverse filtering algorithm. Automatic assessment is realized by a regressor that predicts from temporal and spectral features of the glottal airflow. A regressor trained on overtone amplitudes and harmonic richness factors extracted from a set of continuous-speech utterances was applied to a set of sustained-vowel utterances, giving severity predictions (on a scale of ratings from 0 to 100) with an average error magnitude of 14",
    "checked": true,
    "id": "eae51baf6bd12f879314b459e5d8ed502fb8e7c3",
    "semantic_title": "objective severity assessment from disordered voice using estimated glottal airflow",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2017/pokorny17_interspeech.html": {
    "title": "Earlier Identification of Children with Autism Spectrum Disorder: An Automatic Vocalisation-Based Approach",
    "volume": "main",
    "abstract": "Autism spectrum disorder (ASD) is a neurodevelopmental disorder usually diagnosed in or beyond toddlerhood. ASD is defined by repetitive and restricted behaviours, and deficits in social communication. The early speech-language development of individuals with ASD has been characterised as delayed. However, little is known about ASD-related characteristics of pre-linguistic vocalisations at the feature level. In this study, we examined pre-linguistic vocalisations of 10-month-old individuals later diagnosed with ASD and a matched control group of typically developing individuals (N = 20). We segmented 684 vocalisations from parent-child interaction recordings. All vocalisations were annotated and signal-analytically decomposed. We analysed ASD-related vocalisation specificities on the basis of a standardised set (eGeMAPS) of 88 acoustic features selected for clinical speech analysis applications. 54 features showed evidence for a differentiation between vocalisations of individuals later diagnosed with ASD and controls. In addition, we evaluated the feasibility of automated, vocalisation-based identification of individuals later diagnosed with ASD. We compared linear kernel support vector machines and a 1-layer bidirectional long short-term memory neural network. Both classification approaches achieved an accuracy of 75% for subject-wise identification in a subject-independent 3-fold cross-validation scheme. Our promising results may be an important contribution en-route to facilitate earlier identification of ASD",
    "checked": true,
    "id": "65616754cbd08935c0f17c2f34258670b61a5fc9",
    "semantic_title": "earlier identification of children with autism spectrum disorder: an automatic vocalisation-based approach",
    "citation_count": 40
  },
  "https://www.isca-speech.org/archive/interspeech_2017/vasquezcorrea17_interspeech.html": {
    "title": "Convolutional Neural Network to Model Articulation Impairments in Patients with Parkinson's Disease",
    "volume": "main",
    "abstract": "Speech impairments are one of the earliest manifestations in patients with Parkinson's disease. Particularly, articulation deficits related to the capability of the speaker to start/stop the vibration of the vocal folds have been observed in the patients. Those difficulties can be assessed by modeling the transitions between voiced and unvoiced segments from speech. A robust strategy to model the articulatory deficits related to the starting or stopping vibration of the vocal folds is proposed in this study. The transitions between voiced and unvoiced segments are modeled by a convolutional neural network that extracts suitable information from two time-frequency representations: the short time Fourier transform and the continuous wavelet transform. The proposed approach improves the results previously reported in the literature. Accuracies of up to 89% are obtained for the classification of Parkinson's patients vs. healthy speakers. This study is a step towards the robust modeling of the speech impairments in patients with neuro-degenerative disorders",
    "checked": true,
    "id": "1b316a46a9d0cd0cf8edd44367a888fd71429ce9",
    "semantic_title": "convolutional neural network to model articulation impairments in patients with parkinson's disease",
    "citation_count": 47
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bai17_interspeech.html": {
    "title": "Phone Classification Using a Non-Linear Manifold with Broad Phone Class Dependent DNNs",
    "volume": "main",
    "abstract": "Most state-of-the-art automatic speech recognition (ASR) systems use a single deep neural network (DNN) to map the acoustic space to the decision space. However, different phonetic classes employ different production mechanisms and are best described by different types of features. Hence it may be advantageous to replace this single DNN with several phone class dependent DNNs. The appropriate mathematical formalism for this is a manifold. This paper assesses the use of a non-linear manifold structure with multiple DNNs for phone classification. The system has two levels. The first comprises a set of broad phone class (BPC) dependent DNN-based mappings and the second level is a fusion network. Various ways of designing and training the networks in both levels are assessed, including varying the size of hidden layers, the use of the bottleneck or softmax outputs as input to the fusion network, and the use of different broad class definitions. Phone classification experiments are performed on TIMIT. The results show that using the BPC-dependent DNNs provides small but significant improvements in phone classification accuracy relative to a single global DNN. The paper concludes with visualisations of the structures learned by the local and global DNNs and discussion of their interpretations",
    "checked": true,
    "id": "2537e647d4ca5017451805e31596c96b6de535cb",
    "semantic_title": "phone classification using a non-linear manifold with broad phone class dependent dnns",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17c_interspeech.html": {
    "title": "An Investigation of Crowd Speech for Room Occupancy Estimation",
    "volume": "main",
    "abstract": "Room occupancy estimation technology has been shown to reduce building energy cost significantly. However speech-based occupancy estimation has not been well explored. In this paper, we investigate energy mode and babble speaker count methods for estimating both small and large crowds in a party-mode room setting. We also examine how distance between speakers and microphone affects their estimation accuracies. Then we propose a novel entropy-based method, which is invariant to different speakers and their different positions in a room. Evaluations on synthetic crowd speech generated using the TIMIT corpus show that acoustic volume features are less affected by distance, and our proposed method outperforms existing methods across a range of different conditions",
    "checked": true,
    "id": "daf4097464bf07d4e53ad21a7a3a5cb3adba7a29",
    "semantic_title": "an investigation of crowd speech for room occupancy estimation",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2017/vijayan17_interspeech.html": {
    "title": "Time-Frequency Coherence for Periodic-Aperiodic Decomposition of Speech Signals",
    "volume": "main",
    "abstract": "Decomposing speech signals into periodic and aperiodic components is an important task, finding applications in speech synthesis, coding, denoising, etc. In this paper, we construct a time-frequency coherence function to analyze spectro-temporal signatures of speech signals for distinguishing between deterministic and stochastic components of speech. The narrowband speech spectrogram is segmented into patches, which are represented as 2-D cosine carriers modulated in amplitude and frequency. Separation of carrier and amplitude/frequency modulations is achieved by 2-D demodulation using Riesz transform, which is the 2-D extension of Hilbert transform. The demodulated AM component reflects contributions of the vocal tract to spectrogram. The frequency modulated carrier (FM-carrier) signal exhibits properties of the excitation. The time-frequency coherence is defined with respect to FM-carrier and a coherence map is constructed, in which highly coherent regions represent nearly periodic and deterministic components of speech, whereas the incoherent regions correspond to unstructured components. The coherence map shows a clear distinction between deterministic and stochastic components in speech characterized by jitter, shimmer, lip radiation, type of excitation, etc. Binary masks prepared from the time-frequency coherence function are used for periodic-aperiodic decomposition of speech. Experimental results are presented to validate the efficiency of the proposed method",
    "checked": true,
    "id": "93b97386a78e815f4612aa8378784dd9c442e59d",
    "semantic_title": "time-frequency coherence for periodic-aperiodic decomposition of speech signals",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2017/meireles17_interspeech.html": {
    "title": "Musical Speech: A New Methodology for Transcribing Speech Prosody",
    "volume": "main",
    "abstract": "Musical Speech is a new methodology for transcribing speech prosody using musical notation. The methodology presented in this paper is an updated version of our work [12]. Our work is situated in a historical context with a brief survey of the literature of speech melodies, in which we highlight the pioneering works of John Steele, Leoš Janávcek, Engelbert Humperdinck, and Arnold Schoenberg, followed by a linguistic view of musical notation in the analysis of speech. Finally, we present the current state-of-the-art of our innovative methodology that uses a quarter-tone scale for transcribing speech, and shows some initial results of the application of this methodology to prosodic transcription",
    "checked": true,
    "id": "3ef82fe149a24e1406e354c8b0b4ad35579853a2",
    "semantic_title": "musical speech: a new methodology for transcribing speech prosody",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2017/nataraj17_interspeech.html": {
    "title": "Estimation of Place of Articulation of Fricatives from Spectral Characteristics for Speech Training",
    "volume": "main",
    "abstract": "A visual feedback of the place of articulation is considered to be useful for speech training aids for hearing-impaired children and for learners of second languages in helping them in improving pronunciation. For such applications, the relation between place of articulation of fricatives and their spectral characteristics is investigated using English fricatives available in the XRMB database, which provides simultaneously acquired speech signal and articulogram. Place of articulation is estimated from the articulogram as the position of maximum constriction in the oral cavity, using an automated graphical technique. The magnitude spectrum is smoothed by critical band based median and mean filters for improving the consistency of the spectral parameters. Out of several spectral parameters investigated, spectral moments and spectral slope appear to be related to the place of articulation of the fricative segment of the utterances as measured from articulogram. The data are used to train and test a Gaussian mixture model to estimate the place of articulation with spectral parameters as the inputs. The estimated values showed a good match with those obtained from the articulograms",
    "checked": true,
    "id": "a4ff3e50abe2dc21fc1fe30acbfc5ab2fc744938",
    "semantic_title": "estimation of place of articulation of fricatives from spectral characteristics for speech training",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2017/backstrom17_interspeech.html": {
    "title": "Estimation of the Probability Distribution of Spectral Fine Structure in the Speech Source",
    "volume": "main",
    "abstract": "The efficiency of many speech processing methods rely on accurate modeling of the distribution of the signal spectrum and a majority of prior works suggest that the spectral components follow the Laplace distribution. To improve the probability distribution models based on our knowledge of speech source modeling, we argue that the model should in fact be a multiplicative mixture model, including terms for voiced and unvoiced utterances. While prior works have applied Gaussian mixture models, we demonstrate that a mixture of generalized Gaussian models more accurately follows the observations. The proposed estimation method is based on measuring the ratio of L -norms between spectral bands. Such ratios follow the Beta-distribution when the input signal is generalized Gaussian, whereby the estimated parameters can be used to determine the underlying parameters of the mixture of generalized Gaussian distributions",
    "checked": true,
    "id": "c3a964d36176261d54457b62e46c01d69733f0d1",
    "semantic_title": "estimation of the probability distribution of spectral fine structure in the speech source",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ghosh17_interspeech.html": {
    "title": "End-to-End Acoustic Feedback in Language Learning for Correcting Devoiced French Final-Fricatives",
    "volume": "main",
    "abstract": "This work aims at providing an end-to-end acoustic feedback framework to help learners of French to pronounce voiced fricatives. A classifier ensemble detects voiced/unvoiced utterances, then a correction method is proposed to improve the perception and production of voiced fricatives in a word-final position. Realizations of voiced fricatives contained in French sentences uttered by French and German speakers were analyzed to find out the deviations between the acoustic cues realized by the two groups of speakers. The correction method consists in substituting the erroneous devoiced fricative by TD-PSOLA concatenative synthesis that uses exemplars of voiced fricatives chosen from a French speaker corpus. To achieve a seamless concatenation the energy of the replacement fricative was adjusted with respect to the energy levels of the learner's and French speaker's preceding vowels. Finally, a perception experiment with the corrected stimuli has been carried out with French native speakers to check the appropriateness of the fricative revoicing. The results showed that the proposed revoicing strategy proved to be very efficient and can be used as an acoustic feedback",
    "checked": true,
    "id": "b2cf454158654121cce523b2620286aef81b47e1",
    "semantic_title": "end-to-end acoustic feedback in language learning for correcting devoiced french final-fricatives",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jacewicz17_interspeech.html": {
    "title": "Dialect Perception by Older Children",
    "volume": "main",
    "abstract": "The acquisition of regional dialect variation is an inherent part of the language learning process that takes place in the specific environments in which the child participates. This study examined dialect perception by 9–12-year-olds who grew up in two very diverse dialect regions in the United States, Western North Carolina (NC) and Southeastern Wisconsin (WI). In a dialect identification task, each group of children responded to 120 talkers from the same dialects representing three generations, ranging in age from old adults to children. There was a robust discrepancy in the children's dialect identification performance: WI children were able to identify talker dialect quite well (although still not as well as the adults) whereas NC children were at chance level. WI children were also more sensitive to cross-generational changes in both dialects as a function of diachronic sound change. It is concluded that both groups of children demonstrated their sociolinguistic awareness in very different ways, corresponding to relatively stable (WI) and changing (NC) socio-cultural environments in their respective speech communities",
    "checked": true,
    "id": "3399778ddace86ff24e28c9996188a90e7daf7c8",
    "semantic_title": "dialect perception by older children",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2017/yoneyama17_interspeech.html": {
    "title": "Perception of Non-Contrastive Variations in American English by Japanese Learners: Flaps are Less Favored Than Stops",
    "volume": "main",
    "abstract": "Alveolar flaps are non-contrastive allophonic variants of alveolar stops in American English. A lexical decision experiment was conducted with Japanese learners of English (JE) to investigate whether second-language (L2) learners are sensitive to such allophonic variations when recognizing words in L2. The stimuli consisted of 36 isolated bisyllabic English words containing word-medial /t/, half of which were flap-favored words, e.g. city, and the other half were [t]-favored words, e.g. faster. All stimuli were recorded with two surface forms: /t/ as a flap, e.g. city with a flap, or as [t], e.g. city with [t]. The stimuli were counterbalanced so that participants only heard one of the two surface forms of each word. The accuracy data indicated that flap-favored words pronounced with a flap, e.g. city with a flap, were recognized significantly less accurately than flap-favored words with [t], e.g. city with [t], and [t]-favored words with [t], e.g. faster with [t]. These results suggest that JE learners prefer canonical forms over frequent forms produced with context-dependent allophonic variations. These results are inconsistent with previous studies that found native speakers' preference for frequent forms, and highlight differences in the effect of allophonic variations on the perception of native-language and L2 speech",
    "checked": true,
    "id": "c102db8f188e0c90f09e9544a237411f06910a23",
    "semantic_title": "perception of non-contrastive variations in american english by japanese learners: flaps are less favored than stops",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2017/maastricht17_interspeech.html": {
    "title": "L1 Perceptions of L2 Prosody: The Interplay Between Intonation, Rhythm, and Speech Rate and Their Contribution to Accentedness and Comprehensibility",
    "volume": "main",
    "abstract": "This study investigates the cumulative effect of (non-)native intonation, rhythm, and speech rate in utterances produced by Spanish learners of Dutch on Dutch native listeners' perceptions. In order to assess the relative contribution of these language-specific properties to perceived accentedness and comprehensibility, speech produced by Spanish learners of Dutch was manipulated using transplantation and resynthesis techniques. Thus, eight manipulation conditions reflecting all possible combinations of L1 and L2 intonation, rhythm, and speech rate were created, resulting in 320 utterances that were rated by 50 Dutch natives on their degree of foreign accent and ease of comprehensibility Our analyses show that all manipulations result in lower accentedness and higher comprehensibility ratings. Moreover, both measures are not affected in the same way by different combinations of prosodic features: For accentedness, Dutch listeners appear most influenced by intonation, and intonation combined with speech rate. This holds for comprehensibility ratings as well, but here the combination of all three properties, including rhythm, also significantly affects ratings by native speakers. Thus, our study reaffirms the importance of differentiating between different aspects of perception and provides insight into those features that are most likely to affect how native speakers perceive second language learners",
    "checked": true,
    "id": "a928e082af0d19e51b51ea9b5688d0d4b87477c9",
    "semantic_title": "l1 perceptions of l2 prosody: the interplay between intonation, rhythm, and speech rate and their contribution to accentedness and comprehensibility",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2017/takiguchi17_interspeech.html": {
    "title": "Effects of Pitch Fall and L1 on Vowel Length Identification in L2 Japanese",
    "volume": "main",
    "abstract": "This study investigated whether and how the role of pitch fall in the first language (L1) interacts with its use as a cue for Japanese phonological vowel length in the second language (L2). Native listeners of Japanese (NJ) and L2 learners of Japanese with L1 backgrounds in Mandarin Chinese (NC), Seoul Korean (NK), American English (NE), and French (NFr) participated in a perception experiment. The results showed that the proportion of \"long\" responses increased as a function of vowel duration for all groups, giving s-shaped curves. Meanwhile, the presence or absence of a pitch fall within a syllable affected only NJ and NC's perception. Their category boundary occurred at a shorter duration for vowels with a pitch fall than without a pitch fall. Among the four groups of L2 learners, only NC use pitch fall to distinguish words in the L1. Thus, it is possible to think that the role of pitch fall as an L1 cue relates to its use as a cue for L2 length identification. L2 learners tend to attend to an important phonetic feature as a cue for perceiving an L1 category differentiating L1 words even in the L2 as implied by the Feature Hypothesis",
    "checked": true,
    "id": "170d7b909363e5c47c7255b13e940ce6a618fcdb",
    "semantic_title": "effects of pitch fall and l1 on vowel length identification in l2 japanese",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhang17_interspeech.html": {
    "title": "A Preliminary Study of Prosodic Disambiguation by Chinese EFL Learners",
    "volume": "main",
    "abstract": "This study investigated whether Chinese learners of English as a foreign language (EFL learners hereafter) could use prosodic cues to resolve syntactically ambiguous sentences in English. 8 sentences with 3 types of syntactic ambiguity were adopted. They were far/near PP attachment, left/right word attachment and wide/narrow scope. In the production experiment, 15 Chinese college students who passed the annual national examination CET (College English Test) Band 4 and 5 native English speakers from America were recruited. They were asked to read the 8 target sentences after hearing the contexts spoken by a Native American speaker, which clarified the intended meaning of the ambiguous sentences. The preliminary results showed that, as the native speakers did, Chinese EFL learners employed different durational patterns to express the alternative meanings of the ambiguous sentences by altering prosodic phrasing. That is, the duration of the pre-boundary items were lengthened and pause were inserted at the boundary. But the perception experiment showed that the utterances produced by Chinese EFL learners couldn't be effectively perceived by the native speakers due to their different use of pre-boundary lengthening and pause. The conclusion is that Chinese EFL learners find prosodic disambiguation difficult",
    "checked": true,
    "id": "e30be62b4da615745ef3c9c2cccf772267901c96",
    "semantic_title": "a preliminary study of prosodic disambiguation by chinese efl learners",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kim17_interspeech.html": {
    "title": "Generation of Large-Scale Simulated Utterances in Virtual Rooms to Train Deep-Neural Networks for Far-Field Speech Recognition in Google Home",
    "volume": "main",
    "abstract": "We describe the structure and application of an acoustic room simulator to generate large-scale simulated data for training deep neural networks for far-field speech recognition. The system simulates millions of different room dimensions, a wide distribution of reverberation time and signal-to-noise ratios, and a range of microphone and sound source locations. We start with a relatively clean training set as the source and artificially create simulated data by randomly sampling a noise configuration for every new training example. As a result, the acoustic model is trained using examples that are virtually never repeated. We evaluate performance of this approach based on room simulation using a factored complex Fast Fourier Transform (CFFT) acoustic model introduced in our earlier work, which uses CFFT layers and LSTM AMs for joint multichannel processing and acoustic modeling. Results show that the simulator-driven approach is quite effective in obtaining large improvements not only in simulated test conditions, but also in real / rerecorded conditions. This room simulation system has been employed in training acoustic models including the ones for the recently released Google Home",
    "checked": true,
    "id": "49636d64a097f708ac131eb24c46719dfcd6d6b2",
    "semantic_title": "generation of large-scale simulated utterances in virtual rooms to train deep-neural networks for far-field speech recognition in google home",
    "citation_count": 217
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kinoshita17_interspeech.html": {
    "title": "Neural Network-Based Spectrum Estimation for Online WPE Dereverberation",
    "volume": "main",
    "abstract": "In this paper, we propose a novel speech dereverberation framework that utilizes deep neural network (DNN)-based spectrum estimation to construct linear inverse filters. The proposed dereverberation framework is based on the state-of-the-art inverse filter estimation algorithm called weighted prediction error (WPE) algorithm, which is known to effectively reduce reverberation and greatly boost the ASR performance in various conditions. In WPE, the accuracy of the inverse filter estimation, and thus the dereverberation performance, is largely dependent on the estimation of the power spectral density (PSD) of the target signal. Therefore, the conventional WPE iteratively performs the inverse filter estimation, actual dereverberation and the PSD estimation to gradually improve the PSD estimate. However, while such iterative procedure works well when sufficiently long acoustically-stationary observed signals are available, WPE's performance degrades when the duration of observed/accessible data is short, which typically is the case for real-time applications using online block-batch processing with small batches. To solve this problem, we incorporate the DNN-based spectrum estimator into the framework of WPE, because a DNN can estimate the PSD robustly even from very short observed data. We experimentally show that the proposed framework outperforms the conventional WPE, and improves the ASR performance in real noisy reverberant environments in both single-channel and multichannel cases",
    "checked": true,
    "id": "2b5fe0dceaf84e0ad0d2f06ce4a9865f065a6f63",
    "semantic_title": "neural network-based spectrum estimation for online wpe dereverberation",
    "citation_count": 81
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ichikawa17_interspeech.html": {
    "title": "Factorial Modeling for Effective Suppression of Directional Noise",
    "volume": "main",
    "abstract": "The assumed scenario is transcription of a face-to-face conversation, such as in the financial industry when an agent and a customer talk over a desk with microphones placed between the speakers and then it is transcribed. From the automatic speech recognition (ASR) perspective, one of the speakers is the target speaker, and the other speaker is a directional noise source. When the number of microphones is small, we often accept microphone intervals that are larger than the spatial aliasing limit because the performance of the beamformer is better. Unfortunately, such a configuration results in significant leakage of directional noise in certain frequency bands because the spatial aliasing makes the beamformer and post-filter inaccurate there. Thus, we introduce a factorial model to compensate only the degraded bands with information from the reliable bands in a probabilistic framework integrating our proposed metrics and speech model. In our experiments, the proposed method reduced the errors from 29.8% to 24.9%",
    "checked": true,
    "id": "58084137d1a8227433f9f113fb57cefd1262f839",
    "semantic_title": "factorial modeling for effective suppression of directional noise",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tu17_interspeech.html": {
    "title": "On Design of Robust Deep Models for CHiME-4 Multi-Channel Speech Recognition with Multiple Configurations of Array Microphones",
    "volume": "main",
    "abstract": "We design a novel deep learning framework for multi-channel speech recognition in two aspects. First, for the front-end, an iterative mask estimation (IME) approach based on deep learning is presented to improve the beamforming approach based on the conventional complex Gaussian mixture model (CGMM). Second, for the back-end, deep convolutional neural networks (DCNNs), with augmentation of both noisy and beamformed training data, are adopted for acoustic modeling while the forward and backward long short-term memory recurrent neural networks (LSTM-RNNs) are used for language modeling. The proposed framework can be quite effective to multi-channel speech recognition with random combinations of fixed microphones. Testing on the CHiME-4 Challenge speech recognition task with a single set of acoustic and language models, our approach achieves the best performance of all three tracks (1-channel, 2-channel, and 6-channel) among submitted systems",
    "checked": true,
    "id": "f8e7eed4f75a5ea89a9d7659fb0b784cf8f5ffd8",
    "semantic_title": "on design of robust deep models for chime-4 multi-channel speech recognition with multiple configurations of array microphones",
    "citation_count": 24
  },
  "https://www.isca-speech.org/archive/interspeech_2017/li17c_interspeech.html": {
    "title": "Acoustic Modeling for Google Home",
    "volume": "main",
    "abstract": "This paper describes the technical and system building advances made to the Google Home multichannel speech recognition system, which was launched in November 2016. Technical advances include an adaptive dereverberation frontend, the use of neural network models that do multichannel processing jointly with acoustic modeling, and Grid-LSTMs to model frequency variations. On the system level, improvements include adapting the model using Google Home specific data. We present results on a variety of multichannel sets. The combination of technical and system advances result in a reduction of WER of 8–28% relative compared to the current production system",
    "checked": true,
    "id": "13e30c5dccae82477ee5d38e4d9c96b504a13d29",
    "semantic_title": "acoustic modeling for google home",
    "citation_count": 147
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mirsamadi17_interspeech.html": {
    "title": "On Multi-Domain Training and Adaptation of End-to-End RNN Acoustic Models for Distant Speech Recognition",
    "volume": "main",
    "abstract": "Recognition of distant (far-field) speech is a challenge for ASR due to mismatch in recording conditions resulting from room reverberation and environment noise. Given the remarkable learning capacity of deep neural networks, there is increasing interest to address this problem by using a large corpus of reverberant far-field speech to train robust models. In this study, we explore how an end-to-end RNN acoustic model trained on speech from different rooms and acoustic conditions (different domains) achieves robustness to environmental variations. It is shown that the first hidden layer acts as a domain separator, projecting the data from different domains into different subspaces. The subsequent layers then use this encoded domain knowledge to map these features to final representations that are invariant to domain change. This mechanism is closely related to noise-aware or room-aware approaches which append manually-extracted domain signatures to the input features. Additionally, we demonstrate how this understanding of the learning procedure provides useful guidance for model adaptation to new acoustic conditions. We present results based on AMI corpus to demonstrate the propagation of domain information in a deep RNN, and perform recognition experiments which indicate the role of encoded domain knowledge on training and adaptation of RNN acoustic models",
    "checked": true,
    "id": "691e9714cca248f95f967d39328b23fcdaf8c040",
    "semantic_title": "on multi-domain training and adaptation of end-to-end rnn acoustic models for distant speech recognition",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2017/morise17_interspeech.html": {
    "title": "Low-Dimensional Representation of Spectral Envelope Without Deterioration for Full-Band Speech Analysis/Synthesis System",
    "volume": "main",
    "abstract": "A speech coding for a full-band speech analysis/synthesis system is described. In this work, full-band speech is defined as speech with a sampling frequency above 40 kHz, whose Nyquist frequency covers the audible frequency range. In prior works, speech coding has generally focused on the narrow-band speech with a sampling frequency below 16 kHz. On the other hand, statistical parametric speech synthesis currently uses the full-band speech, and low-dimensional representation of speech parameters is being used. The purpose of this study is to achieve speech coding without deterioration for full-band speech. We focus on a high-quality speech analysis/synthesis system and mel-cepstral analysis using frequency warping. In the frequency warping function, we directly use three auditory scales. We carried out a subjective evaluation using the WORLD vocoder and found that the optimum number of dimensions was around 50. The kind of frequency warping did not significantly affect the sound quality in the dimensions",
    "checked": true,
    "id": "ce1e8d04fdff25e91399e2fbb8bc8159dd1ea58a",
    "semantic_title": "low-dimensional representation of spectral envelope without deterioration for full-band speech analysis/synthesis system",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2017/loweimi17_interspeech.html": {
    "title": "Robust Source-Filter Separation of Speech Signal in the Phase Domain",
    "volume": "main",
    "abstract": "In earlier work we proposed a framework for speech source-filter separation that employs phase-based signal processing. This paper presents a further theoretical investigation of the model and optimisations that make the filter and source representations less sensitive to the effects of noise and better matched to downstream processing. To this end, first, in computing the Hilbert transform, the log function is replaced by the generalised logarithmic function. This introduces a tuning parameter that adjusts both the dynamic range and distribution of the phase-based representation. Second, when computing the group delay, a more robust estimate for the derivative is formed by applying a regression filter instead of using sample differences. The effectiveness of these modifications is evaluated in clean and noisy conditions by considering the accuracy of the fundamental frequency extracted from the estimated source, and the performance of speech recognition features extracted from the estimated filter. In particular, the proposed filter-based front-end reduces Aurora-2 WERs by 6.3% (average 0–20 dB) compared with previously reported results. Furthermore, when tested in a LVCSR task (Aurora-4) the new features resulted in 5.8% absolute WER reduction compared to MFCCs without performance loss in the clean/matched condition",
    "checked": false,
    "id": "d73e9b415841f4eeb43a66a031027a30b5312312",
    "semantic_title": "source-filter separation of speech signal in the phase domain",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2017/stone17_interspeech.html": {
    "title": "A Time-Warping Pitch Tracking Algorithm Considering Fast f0 Changes",
    "volume": "main",
    "abstract": "Accurately tracking the fundamental frequency (f ) or pitch in speech data is of great interest in numerous contexts. All currently available pitch tracking algorithms perform a short-term analysis of a speech signal to extract the f under the assumption that the pitch does not change within a single analysis frame, a simplification that introduces errors when the f changes rather quickly over time. This paper proposes a new algorithm that warps the time axis of an analysis frame to counteract intra-frame f changes and thus to improve the total tracking results. The algorithm was evaluated on a set of 4718 sentences from 20 speakers (10 male, 10 female) and with added white and babble noise. It was comparative in performance to the state-of-the-art algorithms RAPT and PRAAT to Pitch (ac) under clean conditions and outperformed both of them under noisy conditions",
    "checked": true,
    "id": "35cabd729c7aad08d2a591e767cbe2555642af76",
    "semantic_title": "a time-warping pitch tracking algorithm considering fast f0 changes",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kawahara17_interspeech.html": {
    "title": "A Modulation Property of Time-Frequency Derivatives of Filtered Phase and its Application to Aperiodicity and fo Estimation",
    "volume": "main",
    "abstract": "We introduce a simple and linear SNR (strictly speaking, periodic to random power ratio) estimator (0 dB to 80 dB without additional calibration/linearization) for providing reliable descriptions of aperiodicity in speech corpus. The main idea of this method is to estimate the background random noise level without directly extracting the background noise. The proposed method is applicable to a wide variety of time windowing functions with very low sidelobe levels. The estimate combines the frequency derivative and the time-frequency derivative of the mapping from filter center frequency to the output instantaneous frequency. This procedure can replace the periodicity detection and aperiodicity estimation subsystems of recently introduced open source vocoder, YANG vocoder. Source code of MATLAB implementation of this method will also be open sourced",
    "checked": true,
    "id": "dfad99fe6d9bbba16d089a0c8779894b3d749446",
    "semantic_title": "a modulation property of time-frequency derivatives of filtered phase and its application to aperiodicity and fo estimation",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kumar17_interspeech.html": {
    "title": "Non-Local Estimation of Speech Signal for Vowel Onset Point Detection in Varied Environments",
    "volume": "main",
    "abstract": "Vowel onset point (VOP) is an important information extensively employed in speech analysis and synthesis. Detecting the VOPs in a given speech sequence, independent of the text contexts and recording environments, is a challenging area of research. Performance of existing VOP detection methods have not yet been extensively studied in varied environmental conditions. In this paper, we have exploited the non-local means estimation to detect those regions in the speech sequence which are of high signal-to-noise ratio and exhibit periodicity. Mostly, those regions happen to be the vowel regions. This helps in overcoming the ill-effects of environmental degradations. Next, for each short-time frame of estimated speech sequence, we cumulatively sum the magnitude of the corresponding Fourier transform spectrum. The cumulative sum is then used as the feature to detect the VOPs. The experiments conducted on TIMIT database show that the proposed approach provides better results in terms of detection and spurious rate when compared to a few existing methods under clean and noisy test conditions",
    "checked": true,
    "id": "7de52c37d55db5532b31a336cea3149315d48024",
    "semantic_title": "non-local estimation of speech signal for vowel onset point detection in varied environments",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2017/alradhi17_interspeech.html": {
    "title": "Time-Domain Envelope Modulating the Noise Component of Excitation in a Continuous Residual-Based Vocoder for Statistical Parametric Speech Synthesis",
    "volume": "main",
    "abstract": "In this paper, we present an extension of a novel continuous residual-based vocoder for statistical parametric speech synthesis. Previous work has shown the advantages of adding envelope modulated noise to the voiced excitation, but this has not been investigated yet in the context of continuous vocoders, i.e. of which all parameters are continuous. The noise component is often not accurately modeled in modern vocoders (e.g. STRAIGHT). For more natural sounding speech synthesis, four time-domain envelopes (Amplitude, Hilbert, Triangular and True) are investigated and enhanced, and then applied to the noise component of the excitation in our continuous vocoder. The performance evaluation is based on the study of time envelopes. In an objective experiment, we investigated the Phase Distortion Deviation of vocoded samples. A MUSHRA type subjective listening test was also conducted comparing natural and vocoded speech samples. Both experiments have shown that the proposed framework using Hilbert and True envelopes provides high-quality vocoding while outperforming the two other envelopes",
    "checked": true,
    "id": "aac5326c98894ef028f2ec2bb8f6383f83c91795",
    "semantic_title": "time-domain envelope modulating the noise component of excitation in a continuous residual-based vocoder for statistical parametric speech synthesis",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wu17b_interspeech.html": {
    "title": "Wavelet Speech Enhancement Based on Robust Principal Component Analysis",
    "volume": "main",
    "abstract": "Most state-of-the-art speech enhancement (SE) techniques prefer to enhance utterances in the frequency domain rather than in the time domain. However, the overlap-add (OLA) operation in the short-time Fourier transform (STFT) for speech signal processing possibly distorts the signal and limits the performance of the SE techniques. In this study, a novel SE method that integrates the discrete wavelet packet transform (DWPT) and a novel subspace-based method, robust principal component analysis (RPCA), is proposed to enhance noise-corrupted signals directly in the time domain. We evaluate the proposed SE method on the Mandarin hearing in noise test (MHINT) sentences. The experimental results show that the new method reduces the signal distortions dramatically, thereby improving speech quality and intelligibility significantly. In addition, the newly proposed method outperforms the STFT-RPCA-based speech enhancement system",
    "checked": true,
    "id": "2f90d9735de7025cbc19815aacaa87fa5a75817e",
    "semantic_title": "wavelet speech enhancement based on robust principal component analysis",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sharma17_interspeech.html": {
    "title": "Vowel Onset Point Detection Using Sonority Information",
    "volume": "main",
    "abstract": "Vowel onset point (VOP) refers to the starting event of a vowel, that may be reflected in different aspects of the speech signal. The major issue in VOP detection using existing methods is the confusion among the vowels and other categories of sounds preceding them. This work explores the usefulness of sonority information to reduce this confusion and improve VOP detection. Vowels are the most sonorant sounds followed by semivowels, nasals, voiced fricatives, voiced stops. The sonority feature is derived from the vocal-tract system, excitation source and suprasegmental aspects. As this feature has the capability to discriminate among different sonorant sound units, it reduces the confusion among onset of vowels with that of other sonorant sounds. This results in improved detection and resolution of VOP detection for continuous speech. The performance of proposed sonority information based VOP detection is found to be 92.4%, compared to 85.2% by the existing method. Also the resolution of localizing VOP within 10 ms is significantly enhanced and a performance of 73.0% is achieved as opposed to 60.2% by the existing method",
    "checked": true,
    "id": "38274f465e50f0e74e38bbc2aad51af587096590",
    "semantic_title": "vowel onset point detection using sonority information",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2017/laine17_interspeech.html": {
    "title": "Analytic Filter Bank for Speech Analysis, Feature Extraction and Perceptual Studies",
    "volume": "main",
    "abstract": "Speech signal consists of events in time and frequency, and therefore its analysis with high-resolution time-frequency tools is often of importance. Analytic filter bank provides a simple, fast, and flexible method to construct time-frequency representations of signals. Its parameters can be easily adapted to different situations from uniform to any auditory frequency scale, or even to a focused resolution. Since the Hilbert magnitude values of the channels are obtained at every sample, it provides a practical tool for a high-resolution time-frequency analysis The present study describes the basic theory of analytic filters and tests their main properties. Applications of analytic filter bank to different speech analysis tasks including pitch period estimation and pitch synchronous analysis of formant frequencies and bandwidths are demonstrated. In addition, a new feature vector called group delay vector is introduced. It is shown that this representation provides comparable, or even better results, than those obtained by spectral magnitude feature vectors in the analysis and classification of vowels. The implications of this observation are discussed also from the speech perception point of view",
    "checked": true,
    "id": "800f5f51e48c9ecce30239b264884dd99715c401",
    "semantic_title": "analytic filter bank for speech analysis, feature extraction and perceptual studies",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kroos17_interspeech.html": {
    "title": "Learning the Mapping Function from Voltage Amplitudes to Sensor Positions in 3D-EMA Using Deep Neural Networks",
    "volume": "main",
    "abstract": "The first generation of three-dimensional Electromagnetic Articulography devices (Carstens AG500) suffered from occasional critical tracking failures. Although now superseded by new devices, the AG500 is still in use in many speech labs and many valuable data sets exist. In this study we investigate whether deep neural networks (DNNs) can learn the mapping function from raw voltage amplitudes to sensor positions based on a comprehensive movement data set. This is compared to arriving sample by sample at individual position values via direct optimisation as used in previous methods. We found that with appropriate hyperparameter settings a DNN was able to approximate the mapping function with good accuracy, leading to a smaller error than the previous methods, but that the DNN-based approach was not able to solve the tracking problem completely",
    "checked": true,
    "id": "63629b14ca0c4af978ef9b4df2fa32f90e2291ec",
    "semantic_title": "learning the mapping function from voltage amplitudes to sensor positions in 3d-ema using deep neural networks",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/dai17_interspeech.html": {
    "title": "Multilingual i-Vector Based Statistical Modeling for Music Genre Classification",
    "volume": "main",
    "abstract": "For music signal processing, compared with the strategy which models each short-time frame independently, when the long-time features are considered, the time-series characteristics of the music signal can be better presented. As a typical kind of long-time modeling strategy, the identification vector (i-vector) uses statistical modeling to model the audio signal in the segment level. It can better capture the important elements of the music signal, and these important elements may benefit to the classification of music signal. In this paper, the i-vector based statistical feature for music genre classification is explored. In addition to learn enough important elements for music signal, a new multilingual i-vector feature is proposed based on the multilingual model. The experimental results show that the multilingual i-vector based models can achieve better classification performances than conventional short-time modeling based methods",
    "checked": true,
    "id": "8c5c6522778d906045da87fc14da89515056ae28",
    "semantic_title": "multilingual i-vector based statistical modeling for music genre classification",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2017/khonglah17_interspeech.html": {
    "title": "Indoor/Outdoor Audio Classification Using Foreground Speech Segmentation",
    "volume": "main",
    "abstract": "The task of indoor/ outdoor audio classification using foreground speech segmentation is attempted in this work. Foreground speech segmentation is the use of features to segment between foreground speech and background interfering sources like noise. Initially, the foreground and background segments are obtained from foreground speech segmentation by using the normalized autocorrelation peak strength (NAPS) of the zero frequency filtered signal (ZFFS) as a feature. The background segments are then considered for determining whether a particular segment is an indoor or outdoor audio sample. The mel frequency cepstral coefficients are obtained from the background segments of both the indoor and outdoor audio samples and are used to train the Support Vector Machine (SVM) classifier. The use of foreground speech segmentation gives a promising performance for the indoor/ outdoor audio classification task",
    "checked": true,
    "id": "46393584d0aefd22fcc669439ddd5a6e1049a922",
    "semantic_title": "indoor/outdoor audio classification using foreground speech segmentation",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2017/guo17_interspeech.html": {
    "title": "Attention Based CLDNNs for Short-Duration Acoustic Scene Classification",
    "volume": "main",
    "abstract": "Recently, neural networks with deep architecture have been widely applied to acoustic scene classification. Both Convolutional Neural Networks (CNNs) and Long Short-Term Memory Networks (LSTMs) have shown improvements over fully connected Deep Neural Networks (DNNs). Motivated by the fact that CNNs, LSTMs and DNNs are complimentary in their modeling capability, we apply the CLDNNs (Convolutional, Long Short-Term Memory, Deep Neural Networks) framework to short-duration acoustic scene classification in a unified architecture. The CLDNNs take advantage of frequency modeling with CNNs, temporal modeling with LSTM, and discriminative training with DNNs. Based on the CLDNN architecture, several novel attention-based mechanisms are proposed and applied on the LSTM layer to predict the importance of each time step. We evaluate the proposed method on the truncated version of the 2016 TUT acoustic scenes dataset which consists of recordings from 15 different scenes. By using CLDNNs with bidirectional LSTM, we achieve higher performance compared to the conventional neural network architectures. Moreover, by combining the attention-weighted output with LSTM final time step output, significant improvement can be further achieved",
    "checked": true,
    "id": "9c80aa1dec0087fb754d0c3351f3d6803dbb5685",
    "semantic_title": "attention based cldnns for short-duration acoustic scene classification",
    "citation_count": 44
  },
  "https://www.isca-speech.org/archive/interspeech_2017/xia17_interspeech.html": {
    "title": "Frame-Wise Dynamic Threshold Based Polyphonic Acoustic Event Detection",
    "volume": "main",
    "abstract": "Acoustic event detection, the determination of the acoustic event type and the localisation of the event, has been widely applied in many real-world applications. Many works adopt multi-label classification techniques to perform the polyphonic acoustic event detection with a global threshold to detect the active acoustic events. However, the global threshold has to be set manually and is highly dependent on the database being tested. To deal with this, we replaced the fixed threshold method with a frame-wise dynamic threshold approach in this paper. Two novel approaches, namely contour and regressor based dynamic threshold approaches are proposed in this work. Experimental results on the popular TUT Acoustic Scenes 2016 database of polyphonic events demonstrated the superior performance of the proposed approaches",
    "checked": true,
    "id": "5db7c47f98bc91ba41f14c2d2e9cf25785df8359",
    "semantic_title": "frame-wise dynamic threshold based polyphonic acoustic event detection",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jang17_interspeech.html": {
    "title": "Enhanced Feature Extraction for Speech Detection in Media Audio",
    "volume": "main",
    "abstract": "Speech detection is an important first step for audio analysis on media contents, whose goal is to discriminate the presence of speech from non-speech. It remains a challenge owing to various sound sources included in media audio. In this work, we present a novel audio feature extraction method to reflect the acoustic characteristic of the media audio in the time-frequency domain. Since the degree of combination of harmonic and percussive components varies depending on the type of sound source, the audio features which further distinguish between speech and non-speech can be obtained by decomposing the signal into both components. For the evaluation, we use over 20 hours of drama which manually annotated for speech detection as well as 4 full-length movies with annotations released for a research community, whose total length is over 8 hours. Experimental results with deep neural network show superior performance of the proposed in media audio condition",
    "checked": true,
    "id": "8f51d1738c4719e0c5cdd71c5f4cd84984bb657a",
    "semantic_title": "enhanced feature extraction for speech detection in media audio",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sonowal17_interspeech.html": {
    "title": "Audio Classification Using Class-Specific Learned Descriptors",
    "volume": "main",
    "abstract": "This paper presents a classification scheme for audio signals using high-level feature descriptors. The descriptor is designed to capture the relevance of each acoustic feature group (or feature set like mel-frequency cepstral coefficients, perceptual features etc.) in recognizing an audio class. For this, a bank of RVM classifiers are modeled for each ‘audio class'–‘feature group' pair. The response of an input signal to this bank of RVM classifiers forms the entries of the descriptor. Each entry of the descriptor thus measures the proximity of the input signal to an audio class based on a single feature group. This form of signal representation offers two-fold advantages. First, it helps to determine the effectiveness of each feature group in classifying a specific audio class. Second, the descriptor offers higher discriminability than the low-level feature groups and a simple SVM classifier trained on the descriptor produces better performance than several state-of-the-art methods",
    "checked": true,
    "id": "97d804c4e8341534232709cc633bae37e145e52f",
    "semantic_title": "audio classification using class-specific learned descriptors",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ebbers17_interspeech.html": {
    "title": "Hidden Markov Model Variational Autoencoder for Acoustic Unit Discovery",
    "volume": "main",
    "abstract": "Variational Autoencoders (VAEs) have been shown to provide efficient neural-network-based approximate Bayesian inference for observation models for which exact inference is intractable. Its extension, the so-called Structured VAE (SVAE) allows inference in the presence of both discrete and continuous latent variables. Inspired by this extension, we developed a VAE with Hidden Markov Models (HMMs) as latent models. We applied the resulting HMM-VAE to the task of acoustic unit discovery in a zero resource scenario. Starting from an initial model based on variational inference in an HMM with Gaussian Mixture Model (GMM) emission probabilities, the accuracy of the acoustic unit discovery could be significantly improved by the HMM-VAE. In doing so we were able to demonstrate for an unsupervised learning task what is well-known in the supervised learning case: Neural networks provide superior modeling power compared to GMMs",
    "checked": true,
    "id": "e8591d52f6054cc13c1f3603ce086bb83c7d4499",
    "semantic_title": "hidden markov model variational autoencoder for acoustic unit discovery",
    "citation_count": 42
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zohrer17_interspeech.html": {
    "title": "Virtual Adversarial Training and Data Augmentation for Acoustic Event Detection with Gated Recurrent Neural Networks",
    "volume": "main",
    "abstract": "In this paper, we use gated recurrent neural networks (GRNNs) for efficiently detecting environmental events of the IEEE Detection and Classification of Acoustic Scenes and Events challenge (DCASE2016). For this acoustic event detection task data is limited. Therefore, we propose data augmentation such as on-the-fly shuffling and virtual adversarial training for regularization of the GRNNs. Both improve the performance using GRNNs. We obtain a segment-based error rate of 0.59 and an F-score of 58.6%",
    "checked": true,
    "id": "c11170c830de3ea834d6d533f515240d6f060c53",
    "semantic_title": "virtual adversarial training and data augmentation for acoustic event detection with gated recurrent neural networks",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mcauliffe17_interspeech.html": {
    "title": "Montreal Forced Aligner: Trainable Text-Speech Alignment Using Kaldi",
    "volume": "main",
    "abstract": "We present the Montreal Forced Aligner (MFA), a new open-source system for speech-text alignment. MFA is an update to the Prosodylab-Aligner, and maintains its key functionality of trainability on new data, as well as incorporating improved architecture (triphone acoustic models and speaker adaptation), and other features. MFA uses Kaldi instead of HTK, allowing MFA to be distributed as a stand-alone package, and to exploit parallel processing for computationally-intensive training and scaling to larger datasets. We evaluate MFA's performance on aligning word and phone boundaries in English conversational and laboratory speech, relative to human-annotated boundaries, focusing on the effects of aligner architecture and training on the data to be aligned. MFA performs well relative to two existing open-source aligners with simpler architecture (Prosodylab-Aligner and FAVE), and both its improved architecture and training on data to be aligned generally result in more accurate boundaries",
    "checked": true,
    "id": "9e8b06c60722fee06d7f01d4eeaf3ae81e0247d7",
    "semantic_title": "montreal forced aligner: trainable text-speech alignment using kaldi",
    "citation_count": 661
  },
  "https://www.isca-speech.org/archive/interspeech_2017/meenakshi17_interspeech.html": {
    "title": "A Robust Voiced/Unvoiced Phoneme Classification from Whispered Speech Using the ‘Color' of Whispered Phonemes and Deep Neural Network",
    "volume": "main",
    "abstract": "In this work, we propose a robust method to perform frame-level classification of voiced (V) and unvoiced (UV) phonemes from whispered speech, a challenging task due to its voiceless and noise-like nature. We hypothesize that a whispered speech spectrum can be represented as a linear combination of a set of colored noise spectra. A five-dimensional (5D) feature is computed by employing non-negative matrix factorization with a fixed basis dictionary, constructed using spectra of five colored noises. Deep Neural Network (DNN) is used as the classifier. We consider two baseline features-1) Mel Frequency Cepstral Coefficients (MFCC), 2) features computed from a data driven dictionary. Experiments reveal that the features from the colored noise dictionary perform better (on average) than that using the data driven dictionary, with a relative improvement in the average V/UV accuracy of 10.30%, within, and 10.41%, across, data from seven subjects. We also find that the MFCCs and 5D features carry complementary information regarding the nature of voicing decisions in whispered speech. Hence, across all subjects, we obtain a balanced frame-level V/UV classification performance, when MFCC and 5D features are combined, compared to a skewed performance when they are considered separately",
    "checked": false,
    "id": "9367699d3722a28986488933bc929006f05f14a6",
    "semantic_title": "a robust voiced/unvoiced phoneme classification from whispered speech using the 'color' of whispered phonemes and deep neural network",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2017/williams17_interspeech.html": {
    "title": "Rescoring-Aware Beam Search for Reduced Search Errors in Contextual Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Using context in automatic speech recognition allows the recognition system to dynamically task-adapt and bring gains to a broad variety of use-cases. An important mechanism of context-inclusion is on-the-fly rescoring of hypotheses with contextual language model content available only in real-time In systems where rescoring occurs on the lattice during its construction as part of beam search decoding, hypotheses eligible for rescoring may be missed due to pruning. This can happen for many reasons: the language model and rescoring model may assign significantly different scores, there may be a lot of noise in the utterance, or word prefixes with a high out-degree may necessitate aggressive pruning to keep the search tractable. This results in misrecognitions when contextually-relevant hypotheses are pruned before rescoring, even if a contextual rescoring model favors those hypotheses by a large margin We present a technique to adapt the beam search algorithm to preserve hypotheses when they may benefit from rescoring. We show that this technique significantly reduces the number of search pruning errors on rescorable hypotheses, without a significant increase in the search space size. This technique makes it feasible to use one base language model, but still achieve high-accuracy speech recognition results in all contexts",
    "checked": true,
    "id": "31fc9159ecb3a43c107a45e8fe32ab9f1c39cebb",
    "semantic_title": "rescoring-aware beam search for reduced search errors in contextual automatic speech recognition",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zenkel17_interspeech.html": {
    "title": "Comparison of Decoding Strategies for CTC Acoustic Models",
    "volume": "main",
    "abstract": "Connectionist Temporal Classification has recently attracted a lot of interest as it offers an elegant approach to building acoustic models (AMs) for speech recognition. The CTC loss function maps an input sequence of observable feature vectors to an output sequence of symbols. Output symbols are conditionally independent of each other under CTC loss, so a language model (LM) can be incorporated conveniently during decoding, retaining the traditional separation of acoustic and linguistic components in ASR For fixed vocabularies, Weighted Finite State Transducers provide a strong baseline for efficient integration of CTC AMs with n-gram LMs. Character-based neural LMs provide a straight forward solution for open vocabulary speech recognition and all-neural models, and can be decoded with beam search. Finally, sequence-to-sequence models can be used to translate a sequence of individual sounds into a word string We compare the performance of these three approaches, and analyze their error patterns, which provides insightful guidance for future research and development in this important area",
    "checked": true,
    "id": "7b8022a139564225ffd34d8edf8ee92af104ae20",
    "semantic_title": "comparison of decoding strategies for ctc acoustic models",
    "citation_count": 40
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hadian17_interspeech.html": {
    "title": "Phone Duration Modeling for LVCSR Using Neural Networks",
    "volume": "main",
    "abstract": "We describe our work on incorporating probabilities of phone durations, learned by a neural net, into an ASR system. Phone durations are incorporated via lattice rescoring. The input features are derived from the phone identities of a context window of phones, plus the durations of preceding phones within that window. Unlike some previous work, our network outputs the probability of different durations (in frames) directly, up to a fixed limit. We evaluate this method on several large vocabulary tasks, and while we consistently see improvements inWord Error Rates, the improvements are smaller when the lattices are generated with neural net based acoustic models",
    "checked": true,
    "id": "e5b181fe8c7711fb4bbe13990655a86309aba873",
    "semantic_title": "phone duration modeling for lvcsr using neural networks",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chorowski17_interspeech.html": {
    "title": "Towards Better Decoding and Language Model Integration in Sequence to Sequence Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/li17d_interspeech.html": {
    "title": "Empirical Evaluation of Parallel Training Algorithms on Acoustic Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/xiang17_interspeech.html": {
    "title": "Binary Deep Neural Networks for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chandrashekaran17_interspeech.html": {
    "title": "Hierarchical Constrained Bayesian Optimization for Feature, Acoustic Model and Decoder Parameter Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/toyama17_interspeech.html": {
    "title": "Use of Global and Acoustic Features Associated with Contextual Factors to Adapt Language Models for Spontaneous Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/pahuja17_interspeech.html": {
    "title": "Joint Learning of Correlated Sequence Labeling Tasks Using Bidirectional Recurrent Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/shen17_interspeech.html": {
    "title": "Estimation of Gap Between Current Language Models and Human Performance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/moro17_interspeech.html": {
    "title": "A Phonological Phrase Sequence Modelling Approach for Resource Efficient and Robust Real-Time Punctuation Recovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wang17c_interspeech.html": {
    "title": "Factors Affecting the Intelligibility of Low-Pass Filtered Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wang17d_interspeech.html": {
    "title": "Phonetic Restoration of Temporally Reversed Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ishida17_interspeech.html": {
    "title": "Simultaneous Articulatory and Acoustic Distortion in L1 and L2 Listening: Locally Time-Reversed \"Fast\" Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/burchfield17_interspeech.html": {
    "title": "Lexically Guided Perceptual Learning in Mandarin Chinese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/davis17_interspeech.html": {
    "title": "The Effect of Spectral Profile on the Intelligibility of Emotional Speech in Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/maslowski17_interspeech.html": {
    "title": "Whether Long-Term Tracking of Speech Rate Affects Perception Depends on Who is Talking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/peres17_interspeech.html": {
    "title": "Emotional Thin-Slicing: A Proposal for a Short- and Long-Term Division of Emotional Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/guevararukoz17_interspeech.html": {
    "title": "Predicting Epenthetic Vowel Quality from Acoustics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/matsui17_interspeech.html": {
    "title": "The Effect of Spectral Tilt on Size Discrimination of Voiced Speech Sounds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lorenzotrueba17_interspeech.html": {
    "title": "Misperceptions of the Emotional Content of Natural and Vocoded Speech in a Car",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/niebuhr17_interspeech.html": {
    "title": "The Relative Cueing Power of F0 and Duration in German Prominence Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/marques17_interspeech.html": {
    "title": "Perception and Acoustics of Vowel Nasality in Brazilian Portuguese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kim17b_interspeech.html": {
    "title": "Sociophonetic Realizations Guide Subsequent Lexical Access",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/silva17_interspeech.html": {
    "title": "Critical Articulators Identification from RT-MRI of the Vocal Tract",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/somandepalli17_interspeech.html": {
    "title": "Semantic Edge Detection for Tracking Vocal Tract Air-Tissue Boundaries in Real-Time Magnetic Resonance Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/asadiabadi17_interspeech.html": {
    "title": "Vocal Tract Airway Tissue Boundary Tracking for rtMRI Using Shape and Appearance Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ananthapadmanabha17_interspeech.html": {
    "title": "An Objective Critical Distance Measure Based on the Relative Level of Spectral Valley",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sorensen17_interspeech.html": {
    "title": "Database of Volumetric and Real-Time Vocal Tract MRI for Speech Science",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/cao17b_interspeech.html": {
    "title": "The Influence on Realization and Perception of Lexical Tones from Affricate's Aspiration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/franken17_interspeech.html": {
    "title": "Audiovisual Recalibration of Vowel Categories",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/peters17_interspeech.html": {
    "title": "The Effect of Gesture on Persuasive Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lai17_interspeech.html": {
    "title": "Auditory-Visual Integration of Talker Gender in Cantonese Tone Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ito17_interspeech.html": {
    "title": "Event-Related Potentials Associated with Somatosensory Effect in Audio-Visual Speech Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/renner17_interspeech.html": {
    "title": "When a Dog is a Cat and How it Changes Your Pupil Size: Pupil Dilation in Response to Information Mismatch",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kyaw17_interspeech.html": {
    "title": "Cross-Modal Analysis Between Phonation Differences and Texture Images Based on Sentiment Correlations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mehta17_interspeech.html": {
    "title": "Wireless Neck-Surface Accelerometer and Microphone on Flex Circuit with Application to Noise-Robust Monitoring of Lombard Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bandini17_interspeech.html": {
    "title": "Video-Based Tracking of Jaw Movements During Speech: Preliminary Results and Future Directions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sb17_interspeech.html": {
    "title": "Accurate Synchronization of Speech and EGG Signal Using Phase Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/romren17_interspeech.html": {
    "title": "The Acquisition of Focal Lengthening in Stockholm Swedish",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhou17_interspeech.html": {
    "title": "Multilingual Recurrent Neural Networks with Residual Learning for Low-Resource Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/siohan17_interspeech.html": {
    "title": "CTC Training of Multi-Phone Acoustic Models for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tong17_interspeech.html": {
    "title": "An Investigation of Deep Neural Networks for Multilingual Speech Recognition Training and Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/karafiat17_interspeech.html": {
    "title": "2016 BUT Babel System: Multilingual BLSTM Acoustic Model with i-Vector Based Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/matassoni17_interspeech.html": {
    "title": "Optimizing DNN Adaptation for Recognition of Enhanced Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kim17c_interspeech.html": {
    "title": "Deep Least Squares Regression for Speaker Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/do17_interspeech.html": {
    "title": "Multi-Task Learning Using Mismatched Transcription for Under-Resourced Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/joy17_interspeech.html": {
    "title": "Generalized Distillation Framework for Speaker Normalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/samarakoon17_interspeech.html": {
    "title": "Learning Factorized Transforms for Unsupervised Adaptation of LSTM-RNN Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/fainberg17_interspeech.html": {
    "title": "Factorised Representations for Neural Network Adaptation to Diverse Acoustic Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sproat17_interspeech.html": {
    "title": "An RNN Model of Text Normalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/rendel17_interspeech.html": {
    "title": "Weakly-Supervised Phrase Assignment from Text in a Speech-Synthesis System Using Noisy Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ijima17_interspeech.html": {
    "title": "Prosody Aware Word-Level Encoder Based on BLSTM-RNNs for DNN-Based Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ni17_interspeech.html": {
    "title": "Global Syllable Vectors for Building TTS Front-End with Deep Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/fukuoka17_interspeech.html": {
    "title": "Prosody Control of Utterance Sequence for Information Delivering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/huang17c_interspeech.html": {
    "title": "Multi-Task Learning for Prosodic Structure Generation Using BLSTM RNN with Structured Output Layer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zheng17_interspeech.html": {
    "title": "Investigating Efficient Feature Representation Methods and Training Objective for BLSTM-Based Phone Duration Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17d_interspeech.html": {
    "title": "Discrete Duration Model for Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17e_interspeech.html": {
    "title": "Comparison of Modeling Target in LSTM-RNN Duration Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ribeiro17_interspeech.html": {
    "title": "Learning Word Vector Representations Based on Acoustic Counts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/szekely17_interspeech.html": {
    "title": "Synthesising Uncertainty: The Interplay of Vocal Effort and Hesitation Disfluencies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/oktem17_interspeech.html": {
    "title": "Prosograph: A Tool for Prosody Visualisation of Large Speech Corpora",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/vetchinnikova17_interspeech.html": {
    "title": "ChunkitApp: Investigating the Relevant Units of Online Speech Processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jochim17_interspeech.html": {
    "title": "Extending the EMU Speech Database Management System: Cloud Hosting, Team Collaboration, Automatic Revision Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/warlaumont17_interspeech.html": {
    "title": "HomeBank: A Repository for Long-Form Real-World Audio Recordings of Children",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bell17_interspeech.html": {
    "title": "A System for Real Time Collaborative Transcription Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bhat17_interspeech.html": {
    "title": "MoPAReST — Mobile Phone Assisted Remote Speech Therapy Platform",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jaumardhakoun17_interspeech.html": {
    "title": "An Apparatus to Investigate Western Opera Singing Skill Learning Using Performance and Result Biofeedback, and Measuring its Neural Correlates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/draxler17_interspeech.html": {
    "title": "PercyConfigurator — Perception Experiments as a Service",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/salimbajevs17_interspeech.html": {
    "title": "System for Speech Transcription and Post-Editing in Microsoft Word",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/park17_interspeech.html": {
    "title": "Emojive! Collecting Emotion Data from Speech and Facial Expression Using Mobile Game App",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lennes17_interspeech.html": {
    "title": "Mylly — The Mill: A New Platform for Processing Speech and Text Corpora Easily and Efficiently",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/suzuki17_interspeech.html": {
    "title": "Visual Learning 2: Pronunciation App Using Ultrasound, Video, and MRI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/allen17_interspeech.html": {
    "title": "Dialogue as Collaborative Problem Solving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/stasak17_interspeech.html": {
    "title": "Elicitation Design for Acoustic Depression Classification: An Investigation of Articulation Effort, Linguistic Complexity, and Word Affect",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/novoa17_interspeech.html": {
    "title": "Robustness Over Time-Varying Channels in DNN-HMM ASR Based Human-Robot Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/turker17_interspeech.html": {
    "title": "Analysis of Engagement and User Experience with a Laughter Responsive Social Robot",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/baird17_interspeech.html": {
    "title": "Automatic Classification of Autistic Child Vocalisations: A Novel Database and Results",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/oertel17_interspeech.html": {
    "title": "Crowd-Sourced Design of Artificial Attentive Listeners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lancia17_interspeech.html": {
    "title": "Studying the Link Between Inter-Speaker Coordination and Speech Imitation Through Human-Machine Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/delalez17_interspeech.html": {
    "title": "Adjusting the Frame: Biphasic Performative Control of Speech Rhythm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/saryazdi17_interspeech.html": {
    "title": "Attentional Factors in Listeners' Uptake of Gesture Cues During Speech Processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ishi17_interspeech.html": {
    "title": "Motion Analysis in Vocalized Surprise Expressions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ruede17_interspeech.html": {
    "title": "Enhancing Backchannel Prediction Using Word Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/raveh17_interspeech.html": {
    "title": "A Computational Model for Phonetically Responsive Spoken Dialogue Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ebhotemhen17_interspeech.html": {
    "title": "Incremental Dialogue Act Recognition: Token- vs Chunk-Based Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/niebuhr17b_interspeech.html": {
    "title": "Clear Speech — Mere Speech? How Segmental and Prosodic Speech Reduction Shape the Impression That Speakers Create on Listeners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kouklia17_interspeech.html": {
    "title": "Relationships Between Speech Timing and Perceived Hostility in a French Corpus of Political Debates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gallardo17_interspeech.html": {
    "title": "Towards Speaker Characterization: Identifying and Predicting Dimensions of Person Attribution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ishi17b_interspeech.html": {
    "title": "Prosodic Analysis of Attention-Drawing Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/simpson17_interspeech.html": {
    "title": "Perceptual and Acoustic CorreLates of Gender in the Prepubertal Voice",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/schweitzer17_interspeech.html": {
    "title": "To See or not to See: Interlocutor Visibility and Likeability Influence Convergence in Intonation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/weirich17_interspeech.html": {
    "title": "Acoustic Correlates of Parental Role and Gender Identity in the Speech of Expecting Parents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/soleraurena17_interspeech.html": {
    "title": "A Semi-Supervised Learning Approach for Acoustic-Prosodic Personality Perception in Under-Resourced Domains",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tatman17_interspeech.html": {
    "title": "Effects of Talker Dialect, Gender & Race on Accuracy of Bing Speech and YouTube Automatic Captions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/prabhavalkar17_interspeech.html": {
    "title": "A Comparison of Sequence-to-Sequence Models for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zeyer17_interspeech.html": {
    "title": "CTC in the Context of Generalized Full-Sum HMM Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hori17_interspeech.html": {
    "title": "Advances in Joint CTC-Attention Based End-to-End Speech Recognition with a Deep CNN Encoder and RNN-LM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lu17_interspeech.html": {
    "title": "Multitask Learning with CTC and Segmental CRF for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/audhkhasi17_interspeech.html": {
    "title": "Direct Acoustics-to-Word Models for English Conversational Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/li17e_interspeech.html": {
    "title": "Reducing the Computational Complexity of Two-Dimensional LSTMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lucero17_interspeech.html": {
    "title": "Functional Principal Component Analysis of Vocal Tract Area Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sivaraman17_interspeech.html": {
    "title": "Analysis of Acoustic-to-Articulatory Speech Inversion Across Different Accents and Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/arai17_interspeech.html": {
    "title": "Integrated Mechanical Model of [r]-[l] and [b]-[m]-[w] Producing Consonant Cluster [br]",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/badino17_interspeech.html": {
    "title": "A Speaker Adaptive DNN Training Approach for Speaker-Independent Acoustic Inversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/uchida17_interspeech.html": {
    "title": "Acoustic-to-Articulatory Mapping Based on Mixture of Probabilistic Canonical Correlation Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sorensen17b_interspeech.html": {
    "title": "Test-Retest Repeatability of Articulatory Strategies Using Real-Time Magnetic Resonance Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/snyder17_interspeech.html": {
    "title": "Deep Neural Network Embeddings for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/villalba17_interspeech.html": {
    "title": "Tied Variational Autoencoder Backends for i-Vector Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ranjan17_interspeech.html": {
    "title": "Improved Gender Independent Speaker Recognition Using Convolutional Neural Network Based Bottleneck Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/shon17_interspeech.html": {
    "title": "Autoencoder Based Domain Adaptation for Speaker Recognition Under Insufficient Channel Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/khosravani17_interspeech.html": {
    "title": "Nonparametrically Trained Probabilistic Linear Discriminant Analysis for i-Vector Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jorrin17_interspeech.html": {
    "title": "DNN Bottleneck Features for Speaker Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/aare17_interspeech.html": {
    "title": "Creak as a Feature of Lexical Stress in Estonian",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/yanushevskaya17_interspeech.html": {
    "title": "Cross-Speaker Variation in Voice Source Correlates of Focus and Deaccentuation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kalita17_interspeech.html": {
    "title": "Acoustic Characterization of Word-Final Glottal Stops in Mizo and Assam Sora",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mokhtari17_interspeech.html": {
    "title": "Iterative Optimal Preemphasis for Improved Glottal-Flow Estimation by Iterative Adaptive Inverse Filtering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sheena17_interspeech.html": {
    "title": "Automatic Measurement of Pre-Aspiration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/nara17_interspeech.html": {
    "title": "Acoustic and Electroglottographic Study of Breathy and Modal Vowels as Produced by Heritage and Native Gujarati Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wang17e_interspeech.html": {
    "title": "An RNN-Based Quantized F0 Model with Multi-Tier Feedback Links for Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/klimkov17_interspeech.html": {
    "title": "Phrase Break Prediction for Long-Form Reading TTS: Exploiting Text Structure Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tanaka17_interspeech.html": {
    "title": "Physically Constrained Statistical F0 Prediction for Electrolaryngeal Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hojo17_interspeech.html": {
    "title": "DNN-SPACE: DNN-HMM-Based Generative Model of Voice F0 Contours for Statistical Phrase/Accent Command Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/malisz17_interspeech.html": {
    "title": "Controlling Prominence Realisation in Parametric DNN-Based Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/betz17_interspeech.html": {
    "title": "Increasing Recall of Lengthening Detection via Semi-Automatic Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/satt17_interspeech.html": {
    "title": "Efficient Emotion Recognition from Speech Using Deep Learning on Spectrograms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhang17b_interspeech.html": {
    "title": "Interaction and Transition Model for Speech Emotion Recognition in Dialogue",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gideon17_interspeech.html": {
    "title": "Progressive Neural Networks for Transfer Learning in Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/parthasarathy17_interspeech.html": {
    "title": "Jointly Predicting Arousal, Valence and Dominance with Multi-Task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/le17b_interspeech.html": {
    "title": "Discretized Continuous Speech Emotion Recognition with Multi-Task Deep Recurrent Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kim17d_interspeech.html": {
    "title": "Towards Speech Emotion Recognition \"in the Wild\" Using Aggregated Corpora and Deep Multi-Task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tamamori17_interspeech.html": {
    "title": "Speaker-Dependent WaveNet Vocoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gu17_interspeech.html": {
    "title": "Waveform Modeling Using Stacked Dilated Convolutional Neural Networks for Speech Bandwidth Extension",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/takaki17_interspeech.html": {
    "title": "Direct Modeling of Frequency Spectra and Waveform Generation Based on Phase Recovery for DNN-Based Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ronanki17_interspeech.html": {
    "title": "A Hierarchical Encoder-Decoder Model for Statistical Parametric Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kobayashi17_interspeech.html": {
    "title": "Statistical Voice Conversion with WaveNet-Based Waveform Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wan17_interspeech.html": {
    "title": "Google's Next-Generation Real-Time Unit-Selection Synthesizer Using Sequence-to-Sequence LSTM-Based Autoencoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kain17_interspeech.html": {
    "title": "A Comparison of Sentence-Level Speech Intelligibility Metrics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/irino17_interspeech.html": {
    "title": "An Auditory Model of Speaker Size Perception for Voiced Speech Sounds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bosch17_interspeech.html": {
    "title": "The Recognition of Compounds: A Computational Account",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jahromi17_interspeech.html": {
    "title": "Humans do not Maximize the Probability of Correct Decision When Recognizing DANTALE Words in Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/huber17_interspeech.html": {
    "title": "Single-Ended Prediction of Listening Effort Based on Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/neufeld17_interspeech.html": {
    "title": "Modeling Categorical Perception with the Receptive Fields of Auditory Neurons",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wang17f_interspeech.html": {
    "title": "A Maximum Likelihood Approach to Deep Neural Network Based Nonlinear Spectral Mapping for Single-Channel Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/higuchi17_interspeech.html": {
    "title": "Deep Clustering-Based Beamforming for Separation with Unknown Number of Sources",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/pirhosseinloo17_interspeech.html": {
    "title": "Time-Frequency Masking for Blind Source Separation with Preserved Spatial Cues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chien17b_interspeech.html": {
    "title": "Variational Recurrent Neural Networks for Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/andrei17_interspeech.html": {
    "title": "Detecting Overlapped Speech on Short Timeframes Using Deep Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/li17f_interspeech.html": {
    "title": "Ideal Ratio Mask Estimation Using Deep Neural Networks for Monaural Speech Segregation in Noisy Reverberant Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/quiroz17_interspeech.html": {
    "title": "The Vocative Chant and Beyond: German Calling Melodies Under Routine and Urgent Contexts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/simko17_interspeech.html": {
    "title": "Comparing Languages Using Hierarchical Prosodic Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ip17_interspeech.html": {
    "title": "Intonation Facilitates Prediction of Focus Even in the Presence of Lexical Tones",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zahner17_interspeech.html": {
    "title": "Mind the Peak: When Museum is Temporarily Understood as Musical in Australian English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/rognoni17_interspeech.html": {
    "title": "Pashto Intonation Patterns",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/maekawa17_interspeech.html": {
    "title": "A New Model of Final Lowering in Spontaneous Monologue",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ma17c_interspeech.html": {
    "title": "Speech Emotion Recognition with Emotion-Pair Based Framework Considering Emotion Distribution Information in Dimensional Emotion Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sahu17_interspeech.html": {
    "title": "Adversarial Auto-Encoders for Speech Based Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/dang17_interspeech.html": {
    "title": "An Investigation of Emotion Prediction Uncertainty Using Gaussian Mixture Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/khorram17_interspeech.html": {
    "title": "Capturing Long-Term Temporal Dependencies with Convolutional Networks for Continuous Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chasaide17_interspeech.html": {
    "title": "Voice-to-Affect Mapping: Inferences on Language Voice Baseline Settings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/neumann17_interspeech.html": {
    "title": "Attentive Convolutional Neural Network Based Speech Emotion Recognition: A Study on the Impact of Input Features, Signal Length, and Acted Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/miyoshi17_interspeech.html": {
    "title": "Voice Conversion Using Sequence-to-Sequence Learning of Context Posterior Probabilities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hsu17_interspeech.html": {
    "title": "Learning Latent Representations for Speech Generation and Transformation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hashimoto17_interspeech.html": {
    "title": "Parallel-Data-Free Many-to-Many Voice Conversion Based on DNN Integrated with Eigenspace Using a Non-Parallel Speech Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kaneko17_interspeech.html": {
    "title": "Sequence-to-Sequence Voice Conversion with Similarity Metric Learned Using Generative Adversarial Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ardaillon17_interspeech.html": {
    "title": "A Mouth Opening Effect Based on Pole Modification for Expressive Singing Voice Transformation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mohammadi17_interspeech.html": {
    "title": "Siamese Autoencoders for Speech Style Extraction and Switching Applied to Voice Identification and Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sak17_interspeech.html": {
    "title": "Recurrent Neural Aligner: An Encoder-Decoder Neural Network Model for Sequence to Sequence Mapping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/pundak17_interspeech.html": {
    "title": "Highway-LSTM and Recurrent Highway Networks for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ravanelli17_interspeech.html": {
    "title": "Improving Speech Recognition by Revising Gated Recurrent Units",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chien17c_interspeech.html": {
    "title": "Stochastic Recurrent Neural Network for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ratajczak17_interspeech.html": {
    "title": "Frame and Segment Level Recurrent Neural Networks for Phone Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/han17_interspeech.html": {
    "title": "Deep Learning-Based Telephony Speech Recognition in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lee17_interspeech.html": {
    "title": "The I4U Mega Fusion and Collaboration for NIST Speaker Recognition Evaluation 2016",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/torrescarrasquillo17_interspeech.html": {
    "title": "The MIT-LL, JHU and LRDE NIST 2016 Speaker Recognition Evaluation System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/colibro17_interspeech.html": {
    "title": "Nuance - Politecnico di Torino's 2016 NIST Speaker Recognition Evaluation System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhang17c_interspeech.html": {
    "title": "UTD-CRSS Systems for 2016 NIST Speaker Recognition Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/plchot17_interspeech.html": {
    "title": "Analysis and Description of ABC Submission to NIST SRE 2016",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sadjadi17_interspeech.html": {
    "title": "The 2016 NIST Speaker Recognition Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kawahara17b_interspeech.html": {
    "title": "A New Cosine Series Antialiasing Function and its Application to Aliasing-Free Glottal Source Models for Speech and Singing Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lopez17_interspeech.html": {
    "title": "Speaking Style Conversion from Normal to Lombard Speech Using a Glottal Vocoder and Bayesian GMMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/juvela17_interspeech.html": {
    "title": "Reducing Mismatch in Training of DNN-Based Glottal Excitation Models in a Statistical Parametric Text-to-Speech System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sorin17_interspeech.html": {
    "title": "Semi Parametric Concatenative TTS with Instant Voice Modification Capabilities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/manriquez17_interspeech.html": {
    "title": "Modeling Laryngeal Muscle Activation Noise for Low-Order Physiological Based Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/espic17_interspeech.html": {
    "title": "Direct Modelling of Magnitude and Phase Spectra for Statistical Parametric Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kember17_interspeech.html": {
    "title": "Similar Prosodic Structure Perceived Differently in German and English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hou17_interspeech.html": {
    "title": "Disambiguate or not? — The Role of Prosody in Unambiguous and Potentially Ambiguous Anaphora Production in Strictly Mandarin Parallel Structures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/athanasopoulou17_interspeech.html": {
    "title": "Acoustic Properties of Canonical and Non-Canonical Stress in French, Turkish, Armenian and Brazilian Portuguese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/plug17_interspeech.html": {
    "title": "Phonological Complexity, Segment Rate and Speech Tempo Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/yang17_interspeech.html": {
    "title": "On the Duration of Mandarin Tones",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ewald17_interspeech.html": {
    "title": "The Formant Dynamics of Long Close Vowels in Three Varieties of Swedish",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/qian17_interspeech.html": {
    "title": "Bidirectional LSTM-RNN for Improving Automated Assessment of Non-Native Children's Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/yue17_interspeech.html": {
    "title": "Automatic Scoring of Shadowing Speech Based on DNN Posteriors and Their DTW",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lee17b_interspeech.html": {
    "title": "Off-Topic Spoken Response Detection Using Siamese Convolutional Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/arora17_interspeech.html": {
    "title": "Phonological Feature Based Mispronunciation Detection and Diagnosis Using Multi-Task DNNs and Active Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/proenca17_interspeech.html": {
    "title": "Detection of Mispronunciations and Disfluencies in Children Reading Aloud",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/escuderomancebo17_interspeech.html": {
    "title": "Automatic Assessment of Non-Native Prosody by Measuring Distances on Prosodic Label Sequences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ward17_interspeech.html": {
    "title": "Inferring Stance from Prosody",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/levow17_interspeech.html": {
    "title": "Exploring Dynamic Measures of Stance in Spoken Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/barriere17_interspeech.html": {
    "title": "Opinion Dynamics Modeling for Movie Review Transcripts Classification with Hidden Conditional Random Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/luo17_interspeech.html": {
    "title": "Transfer Learning Between Concepts for Human Behavior Modeling: An Application to Sincerity and Deception Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/schroder17_interspeech.html": {
    "title": "The Sound of Deception — What Makes a Speaker Credible?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mendels17_interspeech.html": {
    "title": "Hybrid Acoustic-Lexical Deep Learning Approach for Deception Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/swart17_interspeech.html": {
    "title": "A Generative Model for Score Normalization in Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/dey17_interspeech.html": {
    "title": "Content Normalization for Text-Dependent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhang17d_interspeech.html": {
    "title": "End-to-End Text-Independent Speaker Verification with Triplet Loss on Short Utterances",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/yu17_interspeech.html": {
    "title": "Adversarial Network Bottleneck Features for Noise Robust Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wang17g_interspeech.html": {
    "title": "What Does the Speaker Embedding Encode?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ma17d_interspeech.html": {
    "title": "Incorporating Local Acoustic Variability Information into Short Duration Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhong17_interspeech.html": {
    "title": "DNN i-Vector Speaker Verification with Short, Text-Constrained Test Utterances",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/vestman17_interspeech.html": {
    "title": "Time-Varying Autoregressions for Speaker Verification in Reverberant Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bhattacharya17_interspeech.html": {
    "title": "Deep Speaker Embeddings for Short-Duration Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/park17b_interspeech.html": {
    "title": "Using Voice Quality Features to Improve Short-Utterance, Text-Independent Speaker Verification Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lee17c_interspeech.html": {
    "title": "Gain Compensation for Fast i-Vector Extraction Over Short Duration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/heo17_interspeech.html": {
    "title": "Joint Training of Expanded End-to-End DNN for Text-Dependent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17f_interspeech.html": {
    "title": "Speaker Verification via Estimating Total Variability Space Using Probabilistic Partial Least Squares",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/li17g_interspeech.html": {
    "title": "Deep Speaker Feature Learning for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bousquet17_interspeech.html": {
    "title": "Duration Mismatch Compensation Using Four-Covariance Model and Deep Neural Network for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mccree17_interspeech.html": {
    "title": "Extended Variability Modeling and Unsupervised Adaptation for PLDA Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/borgstrom17_interspeech.html": {
    "title": "Improving the Effectiveness of Speaker Verification Domain Adaptation with Inadequate In-Domain Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tan17_interspeech.html": {
    "title": "i-Vector DNN Scoring and Calibration for Noise Robust Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/matejka17_interspeech.html": {
    "title": "Analysis of Score Normalization in Multilingual Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/silnova17_interspeech.html": {
    "title": "Alternative Approaches to Neural Network Based Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/travadi17_interspeech.html": {
    "title": "A Distribution Free Formulation of the Total Variability Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/rahman17_interspeech.html": {
    "title": "Domain Mismatch Modeling of Out-Domain i-Vectors for PLDA Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/cheng17_interspeech.html": {
    "title": "An Exploration of Dropout with LSTMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kim17e_interspeech.html": {
    "title": "Residual LSTM: Design of a Deep Recurrent Architecture for Distant Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tran17_interspeech.html": {
    "title": "Unfolded Deep Recurrent Convolutional Neural Network with Jump Ahead Connections for Acoustic Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/karita17_interspeech.html": {
    "title": "Forward-Backward Convolutional LSTM for Acoustic Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ark17_interspeech.html": {
    "title": "Convolutional Recurrent Neural Networks for Small-Footprint Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wu17c_interspeech.html": {
    "title": "Deep Activation Mixture Model for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/heck17_interspeech.html": {
    "title": "Ensembles of Multi-Scale VGG Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/grosz17_interspeech.html": {
    "title": "Training Context-Dependent DNN Acoustic Models Using Probabilistic Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/grosz17b_interspeech.html": {
    "title": "A Comparative Evaluation of GMM-Free State Tying Methods for ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wang17h_interspeech.html": {
    "title": "Backstitch: Counteracting Finite-Sample Bias via Negative Steps",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/takeda17_interspeech.html": {
    "title": "Node Pruning Based on Entropy of Weights and Node Activity for Small-Footprint Acoustic Model Based on Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/variani17_interspeech.html": {
    "title": "End-to-End Training of Acoustic Models for Large Vocabulary Continuous Speech Recognition with TensorFlow",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sim17_interspeech.html": {
    "title": "An Efficient Phone N-Gram Forward-Backward Computation Using Dense Matrix Multiplication",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tuske17_interspeech.html": {
    "title": "Parallel Neural Network Features for Improved Tandem Acoustic Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tang17_interspeech.html": {
    "title": "Acoustic Feature Learning via Deep Variational Canonical Correlation Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/masumura17_interspeech.html": {
    "title": "Online End-of-Turn Detection from Speech Based on Stacked Time-Asynchronous Sequential Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wodarczak17_interspeech.html": {
    "title": "Improving Prediction of Speech Activity Using Multi-Participant Respiratory State",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/heeman17_interspeech.html": {
    "title": "Turn-Taking Offsets and Dialogue Context",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/maier17_interspeech.html": {
    "title": "Towards Deep End-of-Turn Prediction for Situated Spoken Dialogue Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ishimoto17_interspeech.html": {
    "title": "End-of-Utterance Prediction by Prosodic Features and Phrase-Dependency Structure in Spontaneous Japanese Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/liu17_interspeech.html": {
    "title": "Turn-Taking Estimation Model Based on Joint Embedding of Lexical and Prosodic Contents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/inaguma17_interspeech.html": {
    "title": "Social Signal Detection in Spontaneous Dialogue Using Bidirectional LSTM-CTC",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/rahimi17_interspeech.html": {
    "title": "Entrainment in Multi-Party Spoken Dialogues at Multiple Linguistic Levels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/reverdy17_interspeech.html": {
    "title": "Measuring Synchrony in Task-Based Dialogues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/crook17_interspeech.html": {
    "title": "Sequence to Sequence Modeling for User Simulation in Dialog Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ramanarayanan17b_interspeech.html": {
    "title": "Human and Automated Scoring of Fluency, Pronunciation and Intonation During Human–Machine Spoken Dialog Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ando17_interspeech.html": {
    "title": "Hierarchical LSTMs with Joint Learning for Estimating Customer Satisfaction from Contact Center Calls",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ultes17_interspeech.html": {
    "title": "Domain-Independent User Satisfaction Reward Estimation for Dialogue Policy Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/nakamura17_interspeech.html": {
    "title": "Analysis of the Relationship Between Prosodic Features of Fillers and its Forms or Occurrence Positions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/fatima17_interspeech.html": {
    "title": "Cross-Subject Continuous Emotion Recognition Using Speech and Body Motion in Dyadic Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/elsner17_interspeech.html": {
    "title": "An Automatically Aligned Corpus of Child-Directed Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bohn17_interspeech.html": {
    "title": "A Comparison of Danish Listeners' Processing Cost in Judging the Truth Value of Norwegian, Swedish, and English Sentences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kleber17_interspeech.html": {
    "title": "On the Role of Temporal Variability in the Acquisition of the German Vowel Length Contrast",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/reidy17_interspeech.html": {
    "title": "A Data-Driven Approach for Perceptually Validated Acoustic Features for Children's Sibilant Fricative Productions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/xiao17_interspeech.html": {
    "title": "Proficiency Assessment of ESL Learner's Sentence Prosody with TTS Synthesized Voice as Reference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17g_interspeech.html": {
    "title": "Mechanisms of Tone Sandhi Rule Application by Non-Native Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wiener17_interspeech.html": {
    "title": "Changes in Early L2 Cue-Weighting of Non-Native Speech: Evidence from Learners of Mandarin Chinese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17h_interspeech.html": {
    "title": "Directing Attention During Perceptual Training: A Preliminary Study of Phonetic Learning in Southern Min by Mandarin Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/luo17b_interspeech.html": {
    "title": "Prosody Analysis of L2 English for Naturalness Evaluation Through Speech Modification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/grigonyte17_interspeech.html": {
    "title": "Measuring Encoding Efficiency in Swedish and English Language Learner Speech Production",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hanulikova17_interspeech.html": {
    "title": "Lexical Adaptation to a Novel Accent in German: A Comparison Between German, Swedish, and Finnish Listeners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/fernandez17_interspeech.html": {
    "title": "Qualitative Differences in L3 Learners' Neurophysiological Response to L1 versus L2 Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sjons17_interspeech.html": {
    "title": "Articulation Rate in Swedish Child-Directed Speech Increases as a Function of the Age of the Child Even When Surprisal is Controlled for",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhang17e_interspeech.html": {
    "title": "The Relationship Between the Perception and Production of Non-Native Tones",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/marklund17_interspeech.html": {
    "title": "MMN Responses in Adults After Exposure to Bimodal and Unimodal Frequency Distributions of Rotated Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/berisha17_interspeech.html": {
    "title": "Float Like a Butterfly Sting Like a Bee: Changes in Speech Preceded Parkinsonism Diagnosis for Muhammad Ali",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/castellana17_interspeech.html": {
    "title": "Cepstral and Entropy Analyses in Vowels Excerpted from Continuous Speech of Dysphonic and Control Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bandini17b_interspeech.html": {
    "title": "Classification of Bulbar ALS from Kinematic Features of the Jaw and Lips: Towards Computer-Mediated Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/adiga17_interspeech.html": {
    "title": "Zero Frequency Filter Based Analysis of Voice Disorders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/k17_interspeech.html": {
    "title": "Hypernasality Severity Analysis in Cleft Lip and Palate Speech Using Vowel Space Area",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/laaridh17_interspeech.html": {
    "title": "Automatic Prediction of Speech Evaluation Metrics for Dysarthric Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/klumpp17_interspeech.html": {
    "title": "Apkinson — A Mobile Monitoring Solution for Parkinson's Disease",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hlavnicka17_interspeech.html": {
    "title": "Dysprosody Differentiate Between Parkinson's Disease, Progressive Supranuclear Palsy, and Multiple System Atrophy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tu17b_interspeech.html": {
    "title": "Interpretable Objective Assessment of Dysarthric Speech Based on Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/vachhani17_interspeech.html": {
    "title": "Deep Autoencoder Based Speech Features for Improved Dysarthric Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lilley17_interspeech.html": {
    "title": "Prediction of Speech Delay from Acoustic Measurements",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/li17h_interspeech.html": {
    "title": "The Frequency Range of \"The Ling Six Sounds\" in Standard Chinese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gu17b_interspeech.html": {
    "title": "Production of Sustained Vowels and Categorical Perception of Tones in Mandarin Among Cochlear-Implanted Children",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kumar17b_interspeech.html": {
    "title": "Audio Content Based Geotagging in Multimedia",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/huang17d_interspeech.html": {
    "title": "Time Delay Histogram Based Speech Source Separation Using a Planar Array",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/pradhan17_interspeech.html": {
    "title": "Excitation Source Features for Improving the Detection of Vowel Onset and Offset Points in a Speech Sequence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gao17_interspeech.html": {
    "title": "A Contrast Function and Algorithm for Blind Separation of Audio Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/xu17_interspeech.html": {
    "title": "Weighted Spatial Covariance Matrix Estimation for MUSIC Based TDOA Estimation of Speech Source",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/guo17b_interspeech.html": {
    "title": "Speaker Direction-of-Arrival Estimation Based on Frequency-Independent Beampattern",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wang17i_interspeech.html": {
    "title": "A Mask Estimation Method Integrating Data Field Model for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/shannon17_interspeech.html": {
    "title": "Improved End-of-Query Detection for Streaming Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/he17_interspeech.html": {
    "title": "Using Approximated Auditory Roughness as a Pre-Filtering Feature for Human Screaming and Affective Speech AED",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zegers17_interspeech.html": {
    "title": "Improving Source Separation via Multi-Speaker Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/yang17b_interspeech.html": {
    "title": "Multiple Sound Source Counting and Localization Based on Spatial Principal Eigenvector",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/karthik17_interspeech.html": {
    "title": "Subband Selection for Binaural Speech Source Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17i_interspeech.html": {
    "title": "Unmixing Convolutive Mixtures by Exploiting Amplitude Co-Modulation: Methods and Evaluation on Mandarin Speech Recordings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tao17_interspeech.html": {
    "title": "Bimodal Recurrent Neural Network for Audiovisual Voice Activity Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/maas17_interspeech.html": {
    "title": "Domain-Specific Utterance End-Point Detection for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kothapally17_interspeech.html": {
    "title": "Speech Detection and Enhancement Using Single Microphone for Distant Speech Applications in Reverberant Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wu17d_interspeech.html": {
    "title": "A Post-Filtering Approach Based on Locally Linear Embedding Difference Compensation for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhang17f_interspeech.html": {
    "title": "Multi-Target Ensemble Learning for Monaural Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ogawa17_interspeech.html": {
    "title": "Improved Example-Based Speech Enhancement by Using Deep Neural Network Acoustic Model for Noise Robust Example Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gelderblom17_interspeech.html": {
    "title": "Subjective Intelligibility of Deep Neural Network-Based Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/koutsogiannaki17_interspeech.html": {
    "title": "Real-Time Modulation Enhancement of Temporal Envelopes for Increasing Speech Intelligibility",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hirsch17_interspeech.html": {
    "title": "On the Influence of Modifying Magnitude and Phase Spectrum to Enhance Noisy Speech Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/rehr17_interspeech.html": {
    "title": "MixMax Approximation as a Super-Gaussian Log-Spectral Amplitude Estimator for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/marxer17_interspeech.html": {
    "title": "Binary Mask Estimation Strategies for Constrained Imputation-Based Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/park17c_interspeech.html": {
    "title": "A Fully Convolutional Neural Network for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/li17i_interspeech.html": {
    "title": "Speech Enhancement Using Non-Negative Spectrogram Models with Mel-Generalized Cepstral Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/websdale17_interspeech.html": {
    "title": "A Comparison of Perceptually Motivated Loss Functions for Binary Mask Estimation in Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/michelsanti17_interspeech.html": {
    "title": "Conditional Generative Adversarial Networks for Speech Enhancement and Noise-Robust Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/qian17b_interspeech.html": {
    "title": "Speech Enhancement Using Bayesian Wavenet",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhang17g_interspeech.html": {
    "title": "Binaural Reverberant Speech Separation Based on Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zorila17_interspeech.html": {
    "title": "On the Quality and Intelligibility of Noisy Speech Processed for Near-End Listening Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/meermeier17_interspeech.html": {
    "title": "Applications of the BBN Sage Speech Processing Platform",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/cernak17_interspeech.html": {
    "title": "Bob Speaks Kaldi",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lenarczyk17_interspeech.html": {
    "title": "Real Time Pitch Shifting with Formant Structure Preservation Using the Phase Vocoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chennupati17_interspeech.html": {
    "title": "A Signal Processing Approach for Speaker Separation Using SFF Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/stemmer17_interspeech.html": {
    "title": "Speech Recognition and Understanding on Hardware-Accelerated DSP",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tsuji17_interspeech.html": {
    "title": "MetaLab: A Repository for Meta-Analyses on Language Development, and More",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/daniel17_interspeech.html": {
    "title": "Evolving Recurrent Neural Networks That Process and Classify Raw Audio in a Streaming Fashion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/milosevic17_interspeech.html": {
    "title": "Combining Gaussian Mixture Models and Segmental Feature Models for Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hagerer17_interspeech.html": {
    "title": "Did you laugh enough today?\" — Deep Neural Networks for Mobile and Wearable Laughter Trackers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jeon17_interspeech.html": {
    "title": "Low-Frequency Ultrasonic Communication for Speech Broadcasting in Public Transportation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wood17_interspeech.html": {
    "title": "Real-Time Speech Enhancement with GCC-NMF: Demonstration on the Raspberry Pi and NVIDIA Jetson",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/rouhe17_interspeech.html": {
    "title": "Reading Validation for Pronunciation Evaluation in the Digitala Project",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/pelachaud17_interspeech.html": {
    "title": "Conversing with Social Agents That Smile and Laugh",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/papadopoulos17_interspeech.html": {
    "title": "Team ELISA System for DARPA LORELEI Speech Evaluation 2016",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mihajlik17_interspeech.html": {
    "title": "First Results in Developing a Medieval Latin Language Charter Dictation System for the East-Central Europe Region",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/watson17_interspeech.html": {
    "title": "The Motivation and Development of MPAi, a Māori Pronunciation Aid",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/feng17_interspeech.html": {
    "title": "On the Linguistic Relevance of Speech Units Learned by Unsupervised Acoustic Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/das17_interspeech.html": {
    "title": "Deep Auto-Encoder Based Multi-Task Learning Using Probabilistic Transcriptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gutkin17_interspeech.html": {
    "title": "Areal and Phylogenetic Features for Multilingual Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hall17_interspeech.html": {
    "title": "SLPAnnotator: Tools for Implementing Sign Language Phonetic Annotation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/schwarz17_interspeech.html": {
    "title": "The LENA System Applied to Swedish: Reliability of the Adult Word Count Estimate",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/casillas17_interspeech.html": {
    "title": "What do Babies Hear? Analyses of Child- and Adult-Directed Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/casillas17b_interspeech.html": {
    "title": "A New Workflow for Semi-Automatized Annotations: Tests with Long-Form Naturalistic Recordings of Childrens Language Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bergmann17_interspeech.html": {
    "title": "Top-Down versus Bottom-Up Theories of Phonological Acquisition: A Big Data Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tsuji17b_interspeech.html": {
    "title": "Which Acoustic and Phonological Factors Shape Infants' Vowel Discrimination? Exploiting Natural Variation in InPhonDB",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chasaide17b_interspeech.html": {
    "title": "The ABAIR Initiative: Bringing Spoken Irish into the Digital Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/saeb17_interspeech.html": {
    "title": "Very Low Resource Radio Browsing for Agile Developmental and Humanitarian Monitoring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/malandrakis17_interspeech.html": {
    "title": "Extracting Situation Frames from Non-English Speech: Evaluation Framework and Pilot Results",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kocharov17_interspeech.html": {
    "title": "Eliciting Meaningful Units from Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bhati17_interspeech.html": {
    "title": "Unsupervised Speech Signal to Symbol Transformation for Zero Resource Speech Applications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gauthier17_interspeech.html": {
    "title": "Machine Assisted Analysis of Vowel Length Contrasts in Wolof",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/glarner17_interspeech.html": {
    "title": "Leveraging Text Data for Word Segmentation for Underresourced Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhuang17_interspeech.html": {
    "title": "Improving DNN Bluetooth Narrowband Acoustic Models by Cross-Bandwidth and Cross-Lingual Initialization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/abraham17_interspeech.html": {
    "title": "Joint Estimation of Articulatory Features and Acoustic Models for Low-Resource Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/abraham17b_interspeech.html": {
    "title": "Transfer Learning and Distillation Techniques to Improve the Acoustic Modeling of Low Resource Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/helgadottir17_interspeech.html": {
    "title": "Building an ASR Corpus Using Althingi's Parliamentary Speeches",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/alumae17_interspeech.html": {
    "title": "Implementation of a Radiology Speech Recognition System for Estonian Using Open Source Software",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gunason17_interspeech.html": {
    "title": "Building ASR Corpora Using Eyra",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/niekerk17_interspeech.html": {
    "title": "Rapid Development of TTS Corpora for Four South African Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gutkin17b_interspeech.html": {
    "title": "Uniform Multilingual Multi-Speaker Acoustic Model for Statistical Parametric Speech Synthesis of Low-Resourced Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mendelson17b_interspeech.html": {
    "title": "Nativization of Foreign Names in TTS for Automatic Reading of World News in Swahili",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tong17b_interspeech.html": {
    "title": "Multi-Task Learning for Mispronunciation Detection on Singapore Children's Mandarin Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/larsen17_interspeech.html": {
    "title": "Relating Unsupervised Word Segmentation to Reported Vocabulary Acquisition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wiren17_interspeech.html": {
    "title": "Modelling the Informativeness of Non-Verbal Cues in Parent-Child Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/marklund17b_interspeech.html": {
    "title": "Computational Simulations of Temporal Vocalization Behavior in Adult-Child Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/strombergsson17_interspeech.html": {
    "title": "Approximating Phonotactic Input in Children's Linguistic Environments from Orthographic Transcripts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chaabouni17_interspeech.html": {
    "title": "Learning Weakly Supervised Multimodal Phoneme Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/obuchi17_interspeech.html": {
    "title": "Personalized Quantification of Voice Attractiveness in Multidimensional Merit Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bosker17_interspeech.html": {
    "title": "The Role of Temporal Amplitude Modulations in the Political Arena: Hillary Clinton vs. Donald Trump",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gallardo17b_interspeech.html": {
    "title": "Perceptual Ratings of Voice Likability Collected Through In-Lab Listening Tests vs. Mobile-Based Crowdsourcing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/trouvain17_interspeech.html": {
    "title": "Attractiveness of French Voices for German Listeners — Results from Native and Non-Native Read Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/schweitzer17b_interspeech.html": {
    "title": "Social Attractiveness in Dialogs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/novaktot17_interspeech.html": {
    "title": "A Gender Bias in the Acoustic-Melodic Features of Charismatic Speech?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/michalsky17_interspeech.html": {
    "title": "Pitch Convergence as an Effect of Perceived Attractiveness and Likability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jiao17_interspeech.html": {
    "title": "Does Posh English Sound Attractive?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/baumann17_interspeech.html": {
    "title": "Large-Scale Speaker Ranking from Crowdsourced Pairwise Listener Ratings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/signorello17_interspeech.html": {
    "title": "Aerodynamic Features of French Fricatives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/serrurier17_interspeech.html": {
    "title": "Inter-Speaker Variability: Speaker Normalisation and Quantitative Estimation of Articulatory Invariants in Speech Production for French",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/patil17b_interspeech.html": {
    "title": "Comparison of Basic Beatboxing Articulations Between Expert and Novice Artists Using Real-Time Magnetic Resonance Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tang17b_interspeech.html": {
    "title": "Speaker-Specific Biomechanical Model-Based Investigation of a Simple Speech Task Based on Tagged-MRI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/blaylock17_interspeech.html": {
    "title": "Sounds of the Human Vocal Tract",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/uezu17_interspeech.html": {
    "title": "A Simulation Study on the Effect of Glottal Boundary Conditions on Vocal Tract Formants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gangamohan17_interspeech.html": {
    "title": "A Robust and Alternative Approach to Zero Frequency Filtering Method for Epoch Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hua17_interspeech.html": {
    "title": "Improving YANGsaf F0 Estimator with Adaptive Kalman Filter",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/dhiman17_interspeech.html": {
    "title": "A Spectro-Temporal Demodulation Technique for Pitch Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/miwa17_interspeech.html": {
    "title": "Robust Method for Estimating F0 of Complex Tone Based on Pitch Perception of Amplitude Modulated Signal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/graf17_interspeech.html": {
    "title": "Low-Complexity Pitch Estimation Based on Phase Differences Between Low-Resolution Spectra",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/morise17b_interspeech.html": {
    "title": "Harvest: A High-Performance Fundamental Frequency Estimator from Speech Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/stehwien17_interspeech.html": {
    "title": "Prosodic Event Recognition Using Convolutional Neural Networks with Context Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/galvez17_interspeech.html": {
    "title": "Prosodic Facilitation and Interference While Judging on the Veracity of Synthesized Statements",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zellers17_interspeech.html": {
    "title": "An Investigation of Pitch Matching Across Adjacent Turns in a Corpus of Spontaneous German",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mukherjee17_interspeech.html": {
    "title": "The Relationship Between F0 Synchrony and Speech Convergence in Dyadic Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/luque17_interspeech.html": {
    "title": "The Role of Linguistic and Prosodic Cues on the Prediction of Self-Reported Satisfaction in Contact Centre Phone Calls",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/brusco17_interspeech.html": {
    "title": "Cross-Linguistic Study of the Production of Turn-Taking Cues in American English and Argentine Spanish",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/egorow17_interspeech.html": {
    "title": "Emotional Features for Speech Overlaps Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17j_interspeech.html": {
    "title": "Computing Multimodal Dyadic Behaviors During Spontaneous Diagnosis Interviews Toward Automatic Categorization of Autism Spectrum Disorder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lin17_interspeech.html": {
    "title": "Deriving Dyad-Level Interaction Representation Using Interlocutors Structural and Expressive Multimodal Behavior Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/brueckner17_interspeech.html": {
    "title": "Spotting Social Signals in Conversational Speech over IP: A Deep Learning Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gosztolya17_interspeech.html": {
    "title": "Optimized Time Series Filters for Detecting Laughter and Filler Events",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/haider17_interspeech.html": {
    "title": "Visual, Laughter, Applause and Spoken Expression Features for Predicting Engagement Within TED Talks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/li17j_interspeech.html": {
    "title": "Large-Scale Domain Adaptation via Teacher-Student Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ahmad17_interspeech.html": {
    "title": "Improving Children's Speech Recognition Through Explicit Pitch Scaling Based on Iterative Spectrogram Inversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/xie17_interspeech.html": {
    "title": "RNN-LDA Clustering for Feature Based DNN Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/arsikere17_interspeech.html": {
    "title": "Robust Online i-Vectors for Unsupervised Adaptation of DNN Acoustic Models: A Study in the Context of Digital Voice Assistants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/srinivasamurthy17_interspeech.html": {
    "title": "Semi-Supervised Learning with Semantic Knowledge Extraction for Improved Speech Recognition in Air Traffic Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kim17f_interspeech.html": {
    "title": "Dynamic Layer Normalization for Adaptive Neural Acoustic Modeling in Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bosker17b_interspeech.html": {
    "title": "An Entrained Rhythm's Frequency, Not Phase, Influences Temporal Sampling of Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wang17j_interspeech.html": {
    "title": "Context Regularity Indexed by Auditory N1 and P2 Event-Related Potentials",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/verma17_interspeech.html": {
    "title": "Discovering Language in Marmoset Vocalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/watanabe17_interspeech.html": {
    "title": "Subject-Independent Classification of Japanese Spoken Sentences by Multiple Frequency Bands Phase Pattern of EEG Response During Speech Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/rietmolen17_interspeech.html": {
    "title": "The Phonological Status of the French Initial Accent and its Role in Semantic Processing: An Event-Related Potentials Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhao17_interspeech.html": {
    "title": "A Neuro-Experimental Evidence for the Motor Theory of Speech Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/agrawal17_interspeech.html": {
    "title": "Speech Representation Learning Using Unsupervised Data-Driven Modulation Filtering for Robust ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mimura17_interspeech.html": {
    "title": "Combined Multi-Channel NMF-Based Robust Beamforming for Noisy Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/yu17b_interspeech.html": {
    "title": "Recognizing Multi-Talker Speech with Permutation Invariant Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tachioka17_interspeech.html": {
    "title": "Coupled Initialization of Multi-Channel Non-Negative Matrix Factorization Based on Spatial and Spectral Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/loweimi17b_interspeech.html": {
    "title": "Channel Compensation in the Generalised Vector Taylor Series Approach to Robust ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/king17_interspeech.html": {
    "title": "Robust Speech Recognition via Anchor Word Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bapna17_interspeech.html": {
    "title": "Towards Zero-Shot Frame Semantic Parsing for Domain Scaling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/georgiadou17_interspeech.html": {
    "title": "ClockWork-RNN Based Architectures for Slot Filling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jannet17_interspeech.html": {
    "title": "Investigating the Effect of ASR Tuning on Named Entity Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/dinarelli17_interspeech.html": {
    "title": "Label-Dependency Coding in Simple Recurrent Networks for Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/meng17_interspeech.html": {
    "title": "Minimum Semantic Error Cost Training of Deep Long Short-Term Memory Networks for Topic Spotting on Conversational Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/liu17b_interspeech.html": {
    "title": "Topic Identification for Speech Without ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/liu17c_interspeech.html": {
    "title": "An End-to-End Trainable Neural Network Model with Belief Tracking for Task-Oriented Dialog",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/cuayahuitl17_interspeech.html": {
    "title": "Deep Reinforcement Learning of Dialogue Policies with Less Weight Updates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bayer17_interspeech.html": {
    "title": "Towards End-to-End Spoken Dialogue Systems with Turn Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/akhtiamov17_interspeech.html": {
    "title": "Speech and Text Analysis for Multimodal Addressee Detection in Human-Human-Computer Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ramanarayanan17c_interspeech.html": {
    "title": "Rushing to Judgement: How do Laypeople Rate Caller Engagement in Thin-Slice Videos of Human–Machine Dialog?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kraljevski17_interspeech.html": {
    "title": "Hyperarticulation of Corrections in Multilingual Dialogue Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/milde17_interspeech.html": {
    "title": "Multitask Sequence-to-Sequence Models for Grapheme-to-Phoneme Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhang17h_interspeech.html": {
    "title": "Acoustic Data-Driven Lexicon Learning Based on a Greedy Pronunciation Selection Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/shinozaki17_interspeech.html": {
    "title": "Semi-Supervised Learning of a Pronunciation Dictionary from Disjoint Phonemic Transcripts and Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/smit17_interspeech.html": {
    "title": "Improved Subword Modeling for WFST-Based Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bruguier17_interspeech.html": {
    "title": "Pronunciation Learning with RNN-Transducers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/naaman17_interspeech.html": {
    "title": "Learning Similarity Functions for Pronunciation Variations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gelly17_interspeech.html": {
    "title": "Spoken Language Identification Using LSTM-Based Angular Proximity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jin17_interspeech.html": {
    "title": "End-to-End Language Identification Using High-Order Utterance Representation with Bilinear Pooling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhang17i_interspeech.html": {
    "title": "Dialect Recognition Based on Unsupervised Bottleneck Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/irtza17_interspeech.html": {
    "title": "Investigating Scalability in Hierarchical Language Identification System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/qian17c_interspeech.html": {
    "title": "Improving Sub-Phone Modeling for Better Native Language Identification with Non-Native English Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/khurana17_interspeech.html": {
    "title": "QMDIS: QCRI-MIT Advanced Dialect Identification System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/alluri17b_interspeech.html": {
    "title": "Detection of Replay Attacks Using Single Frequency Filtering Cepstral Coefficients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sailor17_interspeech.html": {
    "title": "Unsupervised Representation Learning Using Convolutional Restricted Boltzmann Machine for Spoof Speech Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/suthokumar17_interspeech.html": {
    "title": "Independent Modelling of High and Low Energy Speech Frames for Spoofing Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sarkar17_interspeech.html": {
    "title": "Improving Speaker Verification Performance in Presence of Spoofing Attacks Using Out-of-Domain Spoofed Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/nagrani17_interspeech.html": {
    "title": "VoxCeleb: A Large-Scale Speaker Identification Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jones17b_interspeech.html": {
    "title": "Call My Net Corpus: A Multilingual Corpus for Evaluation of Speaker Recognition Technology",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/weiss17_interspeech.html": {
    "title": "Sequence-to-Sequence Models Can Directly Translate Foreign Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kano17_interspeech.html": {
    "title": "Structured-Based Curriculum Learning for End-to-End English-Japanese Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ruiz17_interspeech.html": {
    "title": "Assessing the Tolerance of Neural Machine Translation Systems Against Speech Recognition Errors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/do17b_interspeech.html": {
    "title": "Toward Expressive Speech Translation: A Unified Sequence-to-Sequence LSTMs Approach for Translating Words and Emphasis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/cho17_interspeech.html": {
    "title": "NMT-Based Segmentation and Punctuation Insertion for Real-Time Spoken Language Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/drude17_interspeech.html": {
    "title": "Tight Integration of Spatial and Spectral Features for BSS with Deep Clustering Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zmolikova17_interspeech.html": {
    "title": "Speaker-Aware Neural Network Based Beamformer for Speaker Extraction in Speech Mixtures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/pfeifenberger17_interspeech.html": {
    "title": "Eigenvector-Based Speech Mask Estimation Using Logistic Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wood17b_interspeech.html": {
    "title": "Real-Time Speech Enhancement with GCC-NMF",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ji17b_interspeech.html": {
    "title": "Coherence-Based Dual-Channel Noise Reduction Algorithm in a Complex Noisy Environment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhang17j_interspeech.html": {
    "title": "Glottal Model Based Speech Beamforming for ad-hoc Microphone Arrays",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/liu17d_interspeech.html": {
    "title": "Acoustic Assessment of Disordered Voice with Continuous Speech Based on Utterance-Level ASR Posterior Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ylmaz17c_interspeech.html": {
    "title": "Multi-Stage DNN Training for Automatic Recognition of Dysarthric Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/smith17_interspeech.html": {
    "title": "Improving Child Speech Disorder Assessment by Incorporating Out-of-Domain Adult Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/joy17b_interspeech.html": {
    "title": "On Improving Acoustic Models for TORGO Dysarthric Speech Database",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/simantiraki17_interspeech.html": {
    "title": "Glottal Source Features for Automatic Speech-Based Depression Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sadeghian17_interspeech.html": {
    "title": "Speech Processing Approach for Diagnosing Dementia in an Early Stage",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/biadsy17_interspeech.html": {
    "title": "Effectively Building Tera Scale MaxEnt Language Models Incorporating Non-Linguistic Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/deena17_interspeech.html": {
    "title": "Semi-Supervised Adaptation of RNNLMs by Fine-Tuning with Domain-Specific Auxiliary Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/singh17_interspeech.html": {
    "title": "Approximated and Domain-Adapted LSTM Language Models for First-Pass Decoding in Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chelba17_interspeech.html": {
    "title": "Sparse Non-Negative Matrix Language Modeling: Maximum Entropy Flexibility on the Cheap",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kumar17c_interspeech.html": {
    "title": "Multi-Scale Context Adaptation for Improving Child Automatic Speech Recognition in Child-Adult Spoken Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhu17_interspeech.html": {
    "title": "Using Knowledge Graph and Search Query Click Logs in Statistical Language Model for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/dimitriadis17_interspeech.html": {
    "title": "Developing On-Line Speaker Diarization System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/seshadri17_interspeech.html": {
    "title": "Comparison of Non-Parametric Bayesian Mixture Models for Syllable Clustering and Zero-Resource Speech Processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/proenca17b_interspeech.html": {
    "title": "Automatic Evaluation of Children Reading Aloud on Sentences and Pseudowords",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/yoon17_interspeech.html": {
    "title": "Off-Topic Spoken Response Detection with Word Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/li17k_interspeech.html": {
    "title": "Improving Mispronunciation Detection for Non-Native Learners with Multisource Information and LSTM-Based Deep Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tsujimura17_interspeech.html": {
    "title": "Automatic Explanation Spot Estimation Method Targeted at Text and Figures in Lecture Slides",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kim17g_interspeech.html": {
    "title": "Multiview Representation Learning via Deep CCA for Silent Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/knill17_interspeech.html": {
    "title": "Use of Graphemic Lexicons for Spoken Language Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/yi17_interspeech.html": {
    "title": "Distilling Knowledge from an Ensemble of Models for Punctuation Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/pusateri17_interspeech.html": {
    "title": "A Mostly Data-Driven Approach to Inverse Text Normalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17k_interspeech.html": {
    "title": "Mismatched Crowdsourcing from Multiple Annotator Languages for Recognizing Zero-Resourced Languages: A Nullspace Clustering Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gale17_interspeech.html": {
    "title": "Experiments in Character-Level Neural Network Models for Punctuation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kaushik17_interspeech.html": {
    "title": "Multi-Channel Apollo Mission Speech Transcripts Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mclaren17_interspeech.html": {
    "title": "Calibration Approaches for Language Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/fernando17_interspeech.html": {
    "title": "Bidirectional Modelling for Short Duration Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/shen17b_interspeech.html": {
    "title": "Conditional Generative Adversarial Nets Classifier for Spoken Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/miguel17_interspeech.html": {
    "title": "Tied Hidden Factors in Neural Networks for End-to-End Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/yun17_interspeech.html": {
    "title": "Speaker Clustering by Iteratively Finding Discriminative Feature Space and Cluster Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/vinals17_interspeech.html": {
    "title": "Domain Adaptation of PLDA Models in Broadcast Diarization by Means of Unsupervised Speaker Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/india17_interspeech.html": {
    "title": "LSTM Neural Network-Based Speaker Segmentation Using Acoustic and Language Modelling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gresse17_interspeech.html": {
    "title": "Acoustic Pairing of Original and Dubbed Voices in the Context of Video Game Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ajili17_interspeech.html": {
    "title": "Homogeneity Measure Impact on Target and Non-Target Trials in Forensic Voice Comparison",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/solewicz17_interspeech.html": {
    "title": "Null-Hypothesis LLR: A Proposal for Forensic Automatic Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/liu17e_interspeech.html": {
    "title": "The Opensesame NIST 2016 Speaker Recognition Evaluation System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kumar17d_interspeech.html": {
    "title": "IITG-Indigo System for NIST 2016 SRE Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/misra17_interspeech.html": {
    "title": "Locally Weighted Linear Discriminant Analysis for Robust Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/shon17b_interspeech.html": {
    "title": "Recursive Whitening Transformation for Speaker Recognition on Language Mismatched Condition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/settle17_interspeech.html": {
    "title": "Query-by-Example Search with Discriminative Neural Acoustic Word Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kaneko17b_interspeech.html": {
    "title": "Constructing Acoustic Distances Between Subwords and States Obtained from a Deep Neural Network for Spoken Term Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/khokhlov17_interspeech.html": {
    "title": "Fast and Accurate OOV Decoder on High-Level Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17l_interspeech.html": {
    "title": "Exploring the Use of Significant Words Language Modeling for Spoken Document Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tasaki17_interspeech.html": {
    "title": "Incorporating Acoustic Features for Spontaneous Speech Driven Content Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lu17b_interspeech.html": {
    "title": "Order-Preserving Abstractive Summarization for Spoken Content Based on Connectionist Temporal Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tsuchiya17_interspeech.html": {
    "title": "Automatic Alignment Between Classroom Lecture Utterances and Slide Components",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lopezotero17_interspeech.html": {
    "title": "Compensating Gender Variability in Query-by-Example Search on Speech Using Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kumar17e_interspeech.html": {
    "title": "Zero-Shot Learning Across Heterogeneous Overlapping Domains",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tsunoo17_interspeech.html": {
    "title": "Hierarchical Recurrent Neural Network for Story Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bouchekif17_interspeech.html": {
    "title": "Evaluating Automatic Topic Segmentation as a Segment Retrieval Task",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bang17_interspeech.html": {
    "title": "Improving Speech Recognizers by Refining Broadcast Data with Inaccurate Subtitle Timestamps",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/svec17_interspeech.html": {
    "title": "A Relevance Score Estimation for Spoken Term Detection Based on RNN-Generated Pronunciation Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gallardo17c_interspeech.html": {
    "title": "Predicting Automatic Speech Recognition Performance Over Communication Channels from Instrumental Speech Quality and Intelligibility Scores",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/botinhao17_interspeech.html": {
    "title": "Speech Intelligibility in Cars: The Effect of Speaking Style, Noise and Listener Age",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/yamamoto17_interspeech.html": {
    "title": "Predicting Speech Intelligibility Using a Gammachirp Envelope Distortion Index Based on the Signal-to-Distortion Ratio",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17m_interspeech.html": {
    "title": "Intelligibilities of Mandarin Chinese Sentences with Spectral \"Holes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ward17b_interspeech.html": {
    "title": "The Effect of Situation-Specific Non-Speech Acoustic Cues on the Intelligibility of Speech in Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/andersen17_interspeech.html": {
    "title": "On the Use of Band Importance Weighting in the Short-Time Objective Intelligibility Measure",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/spille17_interspeech.html": {
    "title": "Listening in the Dips: Comparing Relevant Features for Speech Recognition in Humans and Machines",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sugai17_interspeech.html": {
    "title": "Mental Representation of Japanese Mora; Focusing on its Intrinsic Duration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ying17_interspeech.html": {
    "title": "Temporal Dynamics of Lateral Channel Formation in /l/: 3D EMA Data from Australian English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/klingler17_interspeech.html": {
    "title": "Vowel and Consonant Sequences in three Bavarian Dialects of Austria",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/issa17_interspeech.html": {
    "title": "Acoustic Cues to the Singleton-Geminate Contrast: The Case of Libyan Arabic Sonorants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/brandt17_interspeech.html": {
    "title": "Mel-Cepstral Distortion of German Vowels in Different Information Density Contexts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/boril17_interspeech.html": {
    "title": "Effect of Formant and F0 Discontinuity on Perceived Vowel Duration: Impacts for Concatenative Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tabain17_interspeech.html": {
    "title": "An Ultrasound Study of Alveolar and Retroflex Consonants in Arrernte: Stressed and Unstressed Syllables",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gobl17_interspeech.html": {
    "title": "Reshaping the Transformed LF Model: Generating the Glottal Source from the Waveshape Parameter Rd",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/benus17_interspeech.html": {
    "title": "Kinematic Signatures of Prosody in Lombard Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jochim17b_interspeech.html": {
    "title": "What do Finnish and Central Bavarian Have in Common? Towards an Acoustically Based Quantity Typology",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/nellore17_interspeech.html": {
    "title": "Locating Burst Onsets Using SFF Envelope and Phase Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ding17_interspeech.html": {
    "title": "A Preliminary Phonetic Investigation of Alphabetic Words in Mandarin Chinese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/schatz17_interspeech.html": {
    "title": "A Quantitative Measure of the Impact of Coarticulation on Phone Discriminability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lin17b_interspeech.html": {
    "title": "Sinusoidal Partials Tracking for Singing Analysis Using the Heuristic of the Minimal Frequency and Magnitude Difference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/phan17_interspeech.html": {
    "title": "Audio Scene Classification with Deep Recurrent Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sandsten17_interspeech.html": {
    "title": "Automatic Time-Frequency Analysis of Echolocation Signals Using the Matched Gaussian Multitaper Spectrogram",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/matousek17_interspeech.html": {
    "title": "Classification-Based Detection of Glottal Closure Instants from Speech Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/qi17_interspeech.html": {
    "title": "A Domain Knowledge-Assisted Nonlinear Model for Head-Related Transfer Functions Based on Bottleneck Deep Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jesus17_interspeech.html": {
    "title": "Laryngeal Articulation During Trumpet Performance: An Exploratory Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/guan17_interspeech.html": {
    "title": "Matrix of Polynomials Model Based Polynomial Dictionary Learning Method for Acoustic Impulse Response Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hyder17_interspeech.html": {
    "title": "Acoustic Scene Classification Using a CNN-SuperVector System Trained with Auditory and Spectrogram Image Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/feng17b_interspeech.html": {
    "title": "An Environmental Feature Representation for Robust Speech Recognition and for Environment Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/xu17b_interspeech.html": {
    "title": "Attention and Localization Based on a Deep Convolutional Recurrent Model for Weakly Supervised Audio Tagging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/pan17_interspeech.html": {
    "title": "An Audio Based Piano Performance Evaluation Method Using Deep Neural Network Based Acoustic Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chowdhury17_interspeech.html": {
    "title": "Music Tempo Estimation Using Sub-Band Synchrony",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wang17k_interspeech.html": {
    "title": "A Transfer Learning Based Feature Extractor for Polyphonic Sound Event Detection Using Connectionist Temporal Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mostafa17_interspeech.html": {
    "title": "A Note Based Query By Humming System Using Convolutional Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sailor17b_interspeech.html": {
    "title": "Unsupervised Filterbank Learning Using Convolutional Restricted Boltzmann Machine for Environmental Sound Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/soni17_interspeech.html": {
    "title": "Novel Shifted Real Spectrum for Exact Signal Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/weiner17_interspeech.html": {
    "title": "Manual and Automatic Transcriptions in Dementia Detection from Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gupta17_interspeech.html": {
    "title": "An Affect Prediction Approach Through Depression Severity Parameter Incorporation in Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gillespie17_interspeech.html": {
    "title": "Cross-Database Models for the Classification of Dysarthria Presence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/novotny17_interspeech.html": {
    "title": "Acoustic Evaluation of Nasality in Cerebellar Syndromes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hantke17_interspeech.html": {
    "title": "Emotional Speech of Mentally and Physically Disabled Individuals: Introducing the EmotAsS Database and First Findings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/agurto17_interspeech.html": {
    "title": "Phonological Markers of Oxytocin and MDMA Ingestion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mirheidari17_interspeech.html": {
    "title": "An Avatar-Based System for Identifying Individuals Likely to Develop Dementia",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhang17k_interspeech.html": {
    "title": "Cross-Domain Classification of Drowsiness in Speech: The Case of Alcohol Intoxication and Sleep Deprivation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lopezotero17b_interspeech.html": {
    "title": "Depression Detection Using Automatic Transcriptions of De-Identified Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wankerl17_interspeech.html": {
    "title": "An N-Gram Based Approach to the Automatic Diagnosis of Alzheimer's Disease from Spoken Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mundnich17_interspeech.html": {
    "title": "Exploiting Intra-Annotator Rating Consistency Through Copeland's Method for Estimation of Ground Truth Labels in Couples' Therapy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/pettorino17_interspeech.html": {
    "title": "Rhythmic Characteristics of Parkinsonian Speech: A Study on Mandarin and Polish",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tu17c_interspeech.html": {
    "title": "Trisyllabic Tone 3 Sandhi Patterns in Mandarin Produced by Cantonese Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sahkai17_interspeech.html": {
    "title": "Intonation of Contrastive Topic in Estonian",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hao17_interspeech.html": {
    "title": "Reanalyze Fundamental Frequency Peak Delay in Mandarin",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/michelas17_interspeech.html": {
    "title": "How Does the Absence of Shared Knowledge Between Interlocutors Affect the Production of French Prosodic Forms?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wagner17_interspeech.html": {
    "title": "Three Dimensions of Sentence Prosody and Their (Non-)Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kleinhans17_interspeech.html": {
    "title": "Using Prosody to Classify Discourse Relations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/godoy17_interspeech.html": {
    "title": "Canonical Correlation Analysis and Prediction of Perceived Rhythmic Prominences and Pitch Tones in Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kakouros17_interspeech.html": {
    "title": "Evaluation of Spectral Tilt Measures for Sentence Prominence Under Different Noise Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kuang17_interspeech.html": {
    "title": "Creaky Voice as a Function of Tonal Categories and Prosodic Boundaries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/skarnitzl17_interspeech.html": {
    "title": "The Acoustics of Word Stress in Czech as a Function of Speaking Style",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wagner17b_interspeech.html": {
    "title": "What You See is What You Get Prosodically Less — Visibility Shapes Prosodic Prominence Production in Spontaneous Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hsu17b_interspeech.html": {
    "title": "Focus Acoustics in Mandarin Nominals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lundmark17_interspeech.html": {
    "title": "Exploring Multidimensionality: Acoustic and Articulatory Correlates of Swedish Word Accents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/puga17_interspeech.html": {
    "title": "The Perception of English Intonation Patterns by German L2 Speakers of English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/paradacabaleiro17_interspeech.html": {
    "title": "The Perception of Emotions in Noisified Nonsense Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gibson17_interspeech.html": {
    "title": "Attention Networks for Modeling Behaviors in Addiction Counseling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wortwein17_interspeech.html": {
    "title": "Computational Analysis of Acoustic Descriptors in Psychotic Patients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wu17e_interspeech.html": {
    "title": "Modeling Perceivers Neural-Responses Using Lobe-Dependent Convolutional Neural Network to Improve Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/vlasenko17_interspeech.html": {
    "title": "Implementing Gender-Dependent Vowel-Level Analysis for Boosting Speech-Based Depression Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/siddique17_interspeech.html": {
    "title": "Bilingual Word Embeddings for Cross-Lingual Personality Recognition Using Convolutional Neural Nets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/arimoto17_interspeech.html": {
    "title": "Emotion Category Mapping to Emotional Space by Cross-Corpus Emotion Labeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/fayet17_interspeech.html": {
    "title": "Big Five vs. Prosodic Features as Cues to Detect Abnormality in SSPNET-Personality Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/akira17_interspeech.html": {
    "title": "Speech Rate Comparison When Talking to a System and Talking to a Human: A Study from a Speech-to-Speech, Machine Translation Mediated Map Task",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tseng17_interspeech.html": {
    "title": "Approaching Human Performance in Behavior Estimation in Couples Therapy Using Deep Sentence Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/nasir17_interspeech.html": {
    "title": "Complexity in Speech and its Relation to Emotional Bond in Therapist-Patient Interactions During Suicide Risk Assessment Interviews",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/huang17e_interspeech.html": {
    "title": "An Investigation of Emotion Dynamics and Kalman Filtering for Speech-Based Emotion Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sadamitsu17_interspeech.html": {
    "title": "Zero-Shot Learning for Natural Language Understanding Using Domain-Independent Sequential Structure and Question Types",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sawada17_interspeech.html": {
    "title": "Parallel Hierarchical Attention Networks with Shared Memory Reader for Multi-Stream Conversational Document Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/morchid17_interspeech.html": {
    "title": "Internal Memory Gate for Recurrent Neural Networks with Application to Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/korpusik17_interspeech.html": {
    "title": "Character-Based Embedding Models and Reranking Strategies for Understanding Natural Language Meal Descriptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/parcollet17_interspeech.html": {
    "title": "Quaternion Denoising Encoder-Decoder for Theme Identification of Telephone Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/simonnet17_interspeech.html": {
    "title": "ASR Error Management for Improving Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ma17e_interspeech.html": {
    "title": "Jointly Trained Sequential Labeling and Classification by Sparse Attention Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/nayak17_interspeech.html": {
    "title": "To Plan or not to Plan? Discourse Planning in Slot-Value Informed Sequence to Sequence Models for Language Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/riou17_interspeech.html": {
    "title": "Online Adaptation of an Attention-Based Neural Network for Natural Language Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/martinezhinarejos17_interspeech.html": {
    "title": "Spanish Sign Language Recognition with Different Topology Hidden Markov Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/morales17_interspeech.html": {
    "title": "OpenMM: An Open-Source Multimodal Feature Extraction Tool",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/huang17f_interspeech.html": {
    "title": "Speaker Dependency Analysis, Audiovisual Fusion Cues and a Multimodal BLSTM for Conversational Engagement Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hsu17c_interspeech.html": {
    "title": "Voice Conversion from Unaligned Corpora Using Variational Autoencoding Wasserstein Generative Adversarial Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/nakashika17_interspeech.html": {
    "title": "CAB: An Energy-Based Speaker Clustering Model for Rapid Adaptation in Non-Parallel Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/aihara17_interspeech.html": {
    "title": "Phoneme-Discriminative Features for Dysarthric Speech Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wu17f_interspeech.html": {
    "title": "Denoising Recurrent Neural Network for Deep Bidirectional LSTM Based Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tanaka17b_interspeech.html": {
    "title": "Speaker Dependent Approach for Enhancing a Glossectomy Patient's Speech via GMM-Based Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kaneko17c_interspeech.html": {
    "title": "Generative Adversarial Network-Based Postfilter for STFT Spectrograms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bollepalli17_interspeech.html": {
    "title": "Generative Adversarial Network-Based Glottal Waveform Model for Statistical Parametric Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/luo17c_interspeech.html": {
    "title": "Emotional Voice Conversion with Adaptive Scales F0 Based on Wavelet Transform Using Limited Amount of Emotional Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/doddipatla17_interspeech.html": {
    "title": "Speaker Adaptation in DNN-Based Speech Synthesis Using d-Vectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/li17l_interspeech.html": {
    "title": "Spectro-Temporal Modelling with Time-Frequency LSTM and Structured Output Layer for Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ramos17_interspeech.html": {
    "title": "Segment Level Voice Conversion with Recurrent Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/moore17_interspeech.html": {
    "title": "Creating a Voice for MiRo, the World's First Commercial Biomimetic Robot",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/dominguez17_interspeech.html": {
    "title": "A Thematicity-Based Prosody Enrichment Tool for CTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gruber17_interspeech.html": {
    "title": "WebSubDub — Experimental System for Creating High-Quality Alternative Audio Track for TV Broadcasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/juzova17_interspeech.html": {
    "title": "Voice Conservation and TTS System for People Facing Total Laryngectomy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ghone17_interspeech.html": {
    "title": "TBT (Toolkit to Build TTS): A High Performance Framework to Build Multiple Language HTS Voice",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/karhila17_interspeech.html": {
    "title": "SIAK — A Game for Foreign Language Pronunciation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/larsson17_interspeech.html": {
    "title": "Integrating the Talkamatic Dialogue Manager with Alexa",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ahmed17_interspeech.html": {
    "title": "A Robust Medical Speech-to-Speech/Speech-to-Sign Phraselator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/duckhorn17_interspeech.html": {
    "title": "Towards an Autarkic Embedded Cognitive User Interface",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/winata17_interspeech.html": {
    "title": "Nora the Empathetic Psychologist",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/alam17_interspeech.html": {
    "title": "Modifying Amazon's Alexa ASR Grammar and Lexicon — A Case Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lindblom17_interspeech.html": {
    "title": "Re-Inventing Speech — The Biological Way",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/schuller17_interspeech.html": {
    "title": "The INTERSPEECH 2017 Computational Paralinguistics Challenge: Addressee, Cold & Snoring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/krajewski17_interspeech.html": {
    "title": "Description of the Upper Respiratory Tract Infection Corpus (URTIC)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/janott17_interspeech.html": {
    "title": "Description of the Munich-Passau Snore Sound Corpus (MPSSC)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bergelson17_interspeech.html": {
    "title": "Description of the Homebank Child/Adult Addressee Corpus (HB-CHAAC)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/huckvale17_interspeech.html": {
    "title": "It Sounds Like You Have a Cold! Testing Voice Features for the Interspeech 2017 Computational Paralinguistics Cold Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/cai17b_interspeech.html": {
    "title": "End-to-End Deep Learning Framework for Speech Paralinguistics Detection Based on Perception Aware Spectrum",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wagner17c_interspeech.html": {
    "title": "Infected Phonemes: How a Cold Impairs Speech on a Phonetic Level",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/suresh17_interspeech.html": {
    "title": "Phoneme State Posteriorgram Features for Speech Based Automatic Classification of Speakers in Cold and Healthy Condition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/nwe17_interspeech.html": {
    "title": "An Integrated Solution for Snoring Sound Classification Using Bhattacharyya Distance Based GMM Supervectors with SVM, Feature Selection with Random Forest and Spectrogram with CNN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kitamura17_interspeech.html": {
    "title": "Acoustic Analysis of Detailed Three-Dimensional Shape of the Human Nasal Cavity and Paranasal Sinuses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/arnela17_interspeech.html": {
    "title": "A Semi-Polar Grid Strategy for the Three-Dimensional Finite Element Simulation of Vowel-Vowel Sequences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/vasudevan17_interspeech.html": {
    "title": "A Fast Robust 1D Flow Model for a Self-Oscillating Coupled 2D FEM Vocal Fold Simulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/murtola17_interspeech.html": {
    "title": "Waveform Patterns in Pitch Glides Near a Vocal Tract Resonance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/degirmenci17_interspeech.html": {
    "title": "A Unified Numerical Simulation of Vowel Production That Comprises Phonation and the Emitted Sound",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/dabbaghchian17_interspeech.html": {
    "title": "Synthesis of VV Utterances from Muscle Activation to Sound with a 3D Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mv17_interspeech.html": {
    "title": "A Dual Source-Filter Model of Snore Audio for Snorer Group Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/freitag17_interspeech.html": {
    "title": "An ‘End-to-Evolution' Hybrid Approach for Snore Sound Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/amiriparian17_interspeech.html": {
    "title": "Snore Sound Classification Using Image-Based Deep Spectrum Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tavarez17_interspeech.html": {
    "title": "Exploring Fusion Methods and Feature Space for the Classification of Paralinguistic Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gosztolya17b_interspeech.html": {
    "title": "DNN-Based Feature Extraction and Classifier Combination for Child-Directed Speech, Cold and Snoring Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kaya17_interspeech.html": {
    "title": "Introducing Weighted Kernel Classifiers for Handling Imbalanced Paralinguistic Corpora: Snoring, Addressee and Cold",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/steidl17_interspeech.html": {
    "title": "The INTERSPEECH 2017 Computational Paralinguistics Challenge: A Summary of Results",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/schuller17b_interspeech.html": {
    "title": "Discussion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/toshniwal17_interspeech.html": {
    "title": "Multitask Learning with Low-Level Auxiliary Tasks for Encoder-Decoder Based Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/shannon17b_interspeech.html": {
    "title": "Optimizing Expected Word Error Rate via Sampling for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sainath17_interspeech.html": {
    "title": "Annealed f-Smoothing as a Mechanism to Speed up Neural Network Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/meng17b_interspeech.html": {
    "title": "Non-Uniform MCE Training of Deep Long Short-Term Memory Recurrent Neural Networks for Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/dighe17_interspeech.html": {
    "title": "Exploiting Eigenposteriors for Semi-Supervised Training of DNN Acoustic Models with Sequence Discrimination",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/yang17c_interspeech.html": {
    "title": "Discriminative Autoencoders for Acoustic Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zajic17_interspeech.html": {
    "title": "Speaker Diarization Using Convolutional Neural Network for Statistics Accumulation Refinement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jati17_interspeech.html": {
    "title": "Speaker2Vec: Unsupervised Learning and Adaptation of a Speaker Manifold Using Deep Neural Networks with an Evaluation on Speaker Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lan17_interspeech.html": {
    "title": "A Triplet Ranking-Based Neural Network for Speaker Diarization and Linking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/cohen17_interspeech.html": {
    "title": "Estimating Speaker Clustering Quality Using Logistic Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wisniewksi17_interspeech.html": {
    "title": "Combining Speaker Turn Embedding and Incremental Structure Prediction for Low-Latency Speaker Diarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bredin17_interspeech.html": {
    "title": "pyannote.metrics: A Toolkit for Reproducible Evaluation, Diagnostic, and Error Analysis of Speaker Diarization Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17n_interspeech.html": {
    "title": "A Rescoring Approach for Keyword Search Using Lattice Context Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/trmal17_interspeech.html": {
    "title": "The Kaldi OpenKWS System: Improving Low Resource Keyword Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/khokhlov17b_interspeech.html": {
    "title": "The STC Keyword Search System for OpenKWS 2016 Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sun17_interspeech.html": {
    "title": "Compressed Time Delay Neural Network for Small-Footprint Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/suzuki17b_interspeech.html": {
    "title": "Symbol Sequence Search from Telephone Conversation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gundogdu17_interspeech.html": {
    "title": "Similarity Learning Based Query Modeling for Keyword Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/samui17_interspeech.html": {
    "title": "Deep Recurrent Neural Network Based Monaural Speech Separation Using Recurrent Temporal Restricted Boltzmann Machines",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/huang17g_interspeech.html": {
    "title": "Improved Codebook-Based Speech Enhancement Based on MBE Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17o_interspeech.html": {
    "title": "Improving Mask Learning Based Speech Enhancement System with Restoration Layers and Residual Connection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/yan17_interspeech.html": {
    "title": "Exploring Low-Dimensional Structures of Modulation Spectra for Robust Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/pascual17_interspeech.html": {
    "title": "SEGAN: Speech Enhancement Generative Adversarial Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/maiti17_interspeech.html": {
    "title": "Concatenative Resynthesis Using Twin Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/stafylakis17_interspeech.html": {
    "title": "Combining Residual Networks with LSTMs for Lipreading",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/thangthai17_interspeech.html": {
    "title": "Improving Computer Lipreading via DNN Sequence Discriminative Training Techniques",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wand17_interspeech.html": {
    "title": "Improving Speaker-Independent Lipreading with Domain-Adversarial Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/abdelaziz17_interspeech.html": {
    "title": "Turbo Decoders for Audio-Visual Continuous Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/csapo17_interspeech.html": {
    "title": "DNN-Based Ultrasound-to-Speech Conversion for a Silent Speech Interface",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kamper17_interspeech.html": {
    "title": "Visually Grounded Learning of Keyword Prediction from Untranscribed Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chien17d_interspeech.html": {
    "title": "Deep Neural Factorization for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/vesely17_interspeech.html": {
    "title": "Semi-Supervised DNN Training with Word Selection for ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hou17b_interspeech.html": {
    "title": "Gaussian Prediction Based Attention for Online End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/fukuda17_interspeech.html": {
    "title": "Efficient Knowledge Distillation from an Ensemble of Teachers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/prabhavalkar17b_interspeech.html": {
    "title": "An Analysis of \"Attention\" in Sequence-to-Sequence Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/soltau17_interspeech.html": {
    "title": "Neural Speech Recognizer: Acoustic-to-Word LSTM Model for Large Vocabulary Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/guo17c_interspeech.html": {
    "title": "CNN-Based Joint Mapping of Short and Long Utterance i-Vectors for Speaker Verification Using Short Utterances",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ranjan17b_interspeech.html": {
    "title": "Curriculum Learning Based Probabilistic Linear Discriminant Analysis for Noise Robust Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mahto17_interspeech.html": {
    "title": "i-Vector Transformation Using a Novel Discriminative Denoising Autoencoder for Noise-Robust Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wang17l_interspeech.html": {
    "title": "Unsupervised Discriminative Training of PLDA for Domain Adaptation in Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/alam17b_interspeech.html": {
    "title": "Speaker Verification Under Adverse Conditions Using i-Vector Adaptation and Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/castan17_interspeech.html": {
    "title": "Improving Robustness of Speaker Recognition to New Conditions Using Unlabeled Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/abidi17_interspeech.html": {
    "title": "CALYOU: A Comparable Spoken Algerian Corpus Harvested from YouTube",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/narwekar17_interspeech.html": {
    "title": "PRAV: A Phonetically Rich Audio Visual Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/abdelaziz17b_interspeech.html": {
    "title": "NTCD-TIMIT: A New Database and Baseline for Noise-Robust Audio-Visual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/howcroft17_interspeech.html": {
    "title": "The Extended SPaRKy Restaurant Corpus: Designing a Corpus with Variable Information Density",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mansikkaniemi17_interspeech.html": {
    "title": "Automatic Construction of the Finnish Parliament Speech Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/abdo17_interspeech.html": {
    "title": "Building Audio-Visual Phonetically Annotated Arabic Corpus for Expressive Text to Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hughes17_interspeech.html": {
    "title": "What is the Relevant Population? Considerations for the Computation of Likelihood Ratios in Forensic Voice Comparison",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/delvaux17_interspeech.html": {
    "title": "Voice Disguise vs. Impersonation: Acoustic and Perceptual Measurements of Vocal Flexibility in Non Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wu17g_interspeech.html": {
    "title": "Schwa Realization in French: Using Automatic Speech Processing to Study Phonological and Socio-Linguistic Factors in Large Corpora",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/duran17_interspeech.html": {
    "title": "The Social Life of Setswana Ejectives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kohtz17_interspeech.html": {
    "title": "How Long is Too Long? How Pause Features After Requests Affect the Perceived Willingness of Affirmative Answers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gessinger17_interspeech.html": {
    "title": "Shadowing Synthesized Speech — Segmental Analysis of Phonetic Convergence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ghaffarzadegan17_interspeech.html": {
    "title": "Occupancy Detection in Commercial and Residential Environments Using Audio Signal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tran17b_interspeech.html": {
    "title": "Data Augmentation, Missing Feature Mask and Kernel Classification for Through-the-Wall Acoustic Surveillance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chang17_interspeech.html": {
    "title": "Endpoint Detection Using Grid Long Short-Term Memory Networks for Streaming Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/baby17_interspeech.html": {
    "title": "Deep Learning Techniques in Tandem with Signal Processing Cues for Phonetic Segmentation for Text to Speech Synthesis in Indian Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wang17m_interspeech.html": {
    "title": "Gate Activation Signal Analysis for Gated Recurrent Neural Networks and its Correlation with Phoneme Boundaries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/yin17_interspeech.html": {
    "title": "Speaker Change Detection in Broadcast TV Using Bidirectional Long Short-Term Memory Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/do17c_interspeech.html": {
    "title": "Improved Automatic Speech Recognition Using Subband Temporal Envelope Features and Time-Delay Neural Network Denoising Autoencoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/fujimoto17_interspeech.html": {
    "title": "Factored Deep Convolutional Neural Networks for Noise Robust Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/papadopoulos17b_interspeech.html": {
    "title": "Global SNR Estimation of Speech Signals for Unknown Noise Conditions Using Noise Adapted Non-Linear Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ge17_interspeech.html": {
    "title": "Joint Training of Multi-Channel-Condition Dereverberation and Acoustic Modeling of Microphone Array Speech for Robust Distant Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tran17c_interspeech.html": {
    "title": "Uncertainty Decoding with Adaptive Sampling for Noise Robust DNN-Based Acoustic Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhang17l_interspeech.html": {
    "title": "Attention-Based LSTM with Multi-Task Learning for Distant Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/huang17h_interspeech.html": {
    "title": "To Improve the Robustness of LSTM-RNN Acoustic Models Using Higher-Order Feedback from Multiple Histories",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kim17h_interspeech.html": {
    "title": "End-to-End Speech Recognition with Auditory Attention for Multi-Microphone Distance Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/menon17_interspeech.html": {
    "title": "Robust Speech Recognition Based on Binaural Auditory Processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/caroselli17_interspeech.html": {
    "title": "Adaptive Multichannel Dereverberation for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zihlmann17_interspeech.html": {
    "title": "The Effects of Real and Placebo Alcohol on Deaffrication",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mcauliffe17b_interspeech.html": {
    "title": "Polyglot and Speech Corpus Tools: A System for Representing, Integrating, and Querying Speech Corpora",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hughes17b_interspeech.html": {
    "title": "Mapping Across Feature Spaces in Forensic Voice Comparison: The Contribution of Auditory-Based Voice Quality to (Semi-)Automatic System Testing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/arantes17_interspeech.html": {
    "title": "Effect of Language, Speaking Style and Speaker on Long-Term F0 Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/volin17_interspeech.html": {
    "title": "Stability of Prosodic Characteristics Across Age and Gender Groups",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/plantehebert17_interspeech.html": {
    "title": "Electrophysiological Correlates of Familiar Voice Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/cooperleavitt17_interspeech.html": {
    "title": "Developing an Embosi (Bantu C25) Speech Variant Dictionary to Model Vowel Elision and Morpheme Deletion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/murphy17_interspeech.html": {
    "title": "Rd as a Control Parameter to Explore Affective Correlates of the Tense-Lax Continuum",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/barbosa17_interspeech.html": {
    "title": "Cross-Linguistic Distinctions Between Professional and Non-Professional Speaking Styles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gendrot17_interspeech.html": {
    "title": "Perception and Production of Word-Final /ʁ/ in French",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/narendra17_interspeech.html": {
    "title": "Glottal Source Estimation from Coded Telephone Speech Using a Deep Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/christodoulides17_interspeech.html": {
    "title": "Automatic Labelling of Prosodic Prominence, Phrasing and Disfluencies in French Speech by Simulating the Perception of Naïve and Expert Listeners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/levit17_interspeech.html": {
    "title": "Don't Count on ASR to Transcribe for You: Breaking Bias with Two Crowds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/airaksinen17_interspeech.html": {
    "title": "Effects of Training Data Variety in Generating Glottal Pulses from Acoustic Features with DNNs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hantke17b_interspeech.html": {
    "title": "Towards Intelligent Crowdsourcing for Audio Data Annotation: Integrating Active Learning in the Real World",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/henter17_interspeech.html": {
    "title": "Principles for Learning Controllable TTS from Annotated and Latent Variation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/takamichi17_interspeech.html": {
    "title": "Sampling-Based Speech Parameter Generation Using Moment-Matching Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/pollet17_interspeech.html": {
    "title": "Unit Selection with Hierarchical Cascaded Long Short Term Memory Bidirectional Recurrent Neural Nets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/cooper17_interspeech.html": {
    "title": "Utterance Selection for Optimizing Intelligibility of TTS Voices Trained on ASR Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/rosenberg17_interspeech.html": {
    "title": "Bias and Statistical Significance in Evaluating Speech Synthesis with Mean Opinion Scores",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/adiga17b_interspeech.html": {
    "title": "Phase Modeling Using Integrated Linear Prediction Residual for Statistical Parametric Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gonzalez17_interspeech.html": {
    "title": "Evaluation of a Silent Speech Interface Based on Magnetic Sensing and Deep Learning for a Phonetically Rich Vocabulary",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/greenwood17_interspeech.html": {
    "title": "Predicting Head Pose from Speech with a Conditional Variational Autoencoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wester17_interspeech.html": {
    "title": "Real-Time Reactive Speech Synthesis: Incorporating Interruptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/blaauw17_interspeech.html": {
    "title": "A Neural Parametric Singing Synthesizer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wang17n_interspeech.html": {
    "title": "Tacotron: Towards End-to-End Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/capes17_interspeech.html": {
    "title": "Siri On-Device Deep Learning-Guided Unit Selection Text-to-Speech System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/esch17_interspeech.html": {
    "title": "An Expanded Taxonomy of Semiotic Classes for Text Normalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/nakashika17b_interspeech.html": {
    "title": "Complex-Valued Restricted Boltzmann Machine for Direct Learning of Frequency Spectra",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zioko17_interspeech.html": {
    "title": "Soundtracing for Realtime Speech Adjustment to Environmental Conditions in 3D Simulations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/arai17b_interspeech.html": {
    "title": "Vocal-Tract Model with Static Articulators: Lips, Teeth, Tongue, and More",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/masudakatsuse17_interspeech.html": {
    "title": "Remote Articulation Test System Based on WebRTC",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bunnell17_interspeech.html": {
    "title": "The ModelTalker Project: A Web-Based Voice Banking Pipeline for ALS/MND Patients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/heeringa17_interspeech.html": {
    "title": "Visible Vowels: A Tool for the Visualization of Vowel Variation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  }
}