{
  "https://www.isca-speech.org/archive/interspeech_2017/li17_interspeech.html": {
    "title": "ISCA Medal for Scientific Achievement",
    "volume": "main",
    "abstract": "The ISCA Medal for Scientific Achievement 2017 will be awarded to Professor Fumitada Itakura by the President of ISCA during the opening ceremony",
    "checked": true,
    "id": "f9caa3560e50318f82b850d9c254cbe89fd52d93",
    "semantic_title": "isca medal for scientific achievement",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kinnunen17_interspeech.html": {
    "title": "The ASVspoof 2017 Challenge: Assessing the Limits of Replay Spoofing Attack Detection",
    "volume": "main",
    "abstract": "The ASVspoof initiative was created to promote the development of countermeasures which aim to protect automatic speaker verification (ASV) from spoofing attacks. The first community-led, common evaluation held in 2015 focused on countermeasures for speech synthesis and voice conversion spoofing attacks. Arguably, however, it is replay attacks which pose the greatest threat. Such attacks involve the replay of recordings collected from enrolled speakers in order to provoke false alarms and can be mounted with greater ease using everyday consumer devices. ASVspoof 2017, the second in the series, hence focused on the development of replay attack countermeasures. This paper describes the database, protocols and initial findings. The evaluation entailed highly heterogeneous acoustic recording and replay conditions which increased the equal error rate (EER) of a baseline ASV system from 1.76% to 31.46%. Submissions were received from 49 research teams, 20 of which improved upon a baseline replay spoofing detector EER of 24.77%, in terms of replay/non-replay discrimination. While largely successful, the evaluation indicates that the quest for countermeasures which are resilient in the face of variable replay attacks remains very much alive",
    "checked": true,
    "id": "66e1dff1be7ad92104ef33a031b3392517a19b58",
    "semantic_title": "the asvspoof 2017 challenge: assessing the limits of replay spoofing attack detection",
    "citation_count": 406
  },
  "https://www.isca-speech.org/archive/interspeech_2017/font17_interspeech.html": {
    "title": "Experimental Analysis of Features for Replay Attack Detection — Results on the ASVspoof 2017 Challenge",
    "volume": "main",
    "abstract": "This paper presents an experimental comparison of different features for the detection of replay spoofing attacks in Automatic Speaker Verification systems. We evaluate the proposed countermeasures using two recently introduced databases, including the dataset provided for the ASVspoof 2017 challenge. This challenge provides researchers with a common framework for the evaluation of replay attack detection systems, with a particular focus on the generalization to new, unknown conditions (for instance, replay devices different from those used during system training). Our cross-database experiments show that, although achieving this level of generalization is indeed a challenging task, it is possible to train classifiers that exhibit stable and consistent results across different experiments. The proposed approach for the ASVspoof 2017 challenge consists in the score-level fusion of several base classifiers using logistic regression. These base classifiers are 2-class Gaussian Mixture Models (GMMs) representing genuine and spoofed speech respectively. Our best system achieves an Equal Error Rate of 10.52% on the challenge evaluation set. As a result of this set of experiments, we provide some general conclusions regarding feature extraction for replay attack detection and identify which features show the most promising results",
    "checked": false,
    "id": "2e61d0f2a57365a703ad647d017709c0d1193d2d",
    "semantic_title": "experimental analysis of features for replay attack detection - results on the asvspoof 2017 challenge",
    "citation_count": 110
  },
  "https://www.isca-speech.org/archive/interspeech_2017/patil17_interspeech.html": {
    "title": "Novel Variable Length Teager Energy Separation Based Instantaneous Frequency Features for Replay Detection",
    "volume": "main",
    "abstract": "Replay attacks presents a great risk for Automatic Speaker Verification (ASV) system. In this paper, we propose a novel replay detector based on Variable length Teager Energy Operator-Energy Separation Algorithm-Instantaneous Frequency Cosine Coefficients (VESA-IFCC) for the ASV spoof 2017 challenge. The key idea here is to exploit the contribution of IF in each subband energy via ESA to capture possible changes in spectral envelope (due to transmission and channel characteristics of replay device) of replayed speech. The IF is computed from narrowband components of speech signal, and DCT is applied in IF to get proposed feature set. We compare the performance of the proposed VESA-IFCC feature set with the features developed for detecting synthetic and voice converted speech. This includes the CQCC, CFCCIF and prosody-based features. On the development set, the proposed VESA-IFCC features when fused at score-level with a variant of CFCCIF and prosody-based features gave the least EER of 0.12%. On the evaluation set, this combination gave an EER of 18.33%. However, post-evaluation results of challenge indicate that VESA-IFCC features alone gave the relatively least EER of 14.06% (i.e., relatively 16.11% less compared to baseline CQCC) and hence, is a very useful countermeasure to detect replay attacks",
    "checked": true,
    "id": "aae57915d9b43f9fdb1286f1b8286b37704d70ea",
    "semantic_title": "novel variable length teager energy separation based instantaneous frequency features for replay detection",
    "citation_count": 86
  },
  "https://www.isca-speech.org/archive/interspeech_2017/cai17_interspeech.html": {
    "title": "Countermeasures for Automatic Speaker Verification Replay Spoofing Attack : On Data Augmentation, Feature Representation, Classification and Fusion",
    "volume": "main",
    "abstract": "The ongoing ASVspoof 2017 challenge aims to detect replay attacks for text dependent speaker verification. In this paper, we propose multiple replay spoofing countermeasure systems, with some of them boosting the CQCC-GMM baseline system after score level fusion. We investigate different steps in the system building pipeline, including data augmentation, feature representation, classification and fusion. First, in order to augment training data and simulate the unseen replay conditions, we converted the raw genuine training data into replay spoofing data with parametric sound reverberator and phase shifter. Second, we employed the original spectrogram rather than CQCC as input to explore the end-to-end feature representation learning methods. The spectrogram is randomly cropped into fixed size segments, and then fed into a deep residual network (ResNet). Third, upon the CQCC features, we replaced the subsequent GMM classifier with deep neural networks including fully-connected deep neural network (FDNN) and Bidirectional Long Short Term Memory neural network (BLSTM). Experiments showed that data augmentation strategy can significantly improve the system performance. The final fused system achieves to 16.39% EER on the test set of ASVspoof 2017 for the common task",
    "checked": true,
    "id": "3da78ce05b00b16aae951563bb9ec30831d3cb65",
    "semantic_title": "countermeasures for automatic speaker verification replay spoofing attack : on data augmentation, feature representation, classification and fusion",
    "citation_count": 63
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jelil17_interspeech.html": {
    "title": "Spoof Detection Using Source, Instantaneous Frequency and Cepstral Features",
    "volume": "main",
    "abstract": "This work describes the techniques used for spoofed speech detection for the ASVspoof 2017 challenge. The main focus of this work is on exploiting the differences in the speech-specific nature of genuine speech signals and spoofed speech signals generated by replay attacks. This is achieved using glottal closure instants, epoch strength, and the peak to side lobe ratio of the Hilbert envelope of linear prediction residual. Apart from these source features, the instantaneous frequency cosine coefficient feature, and two cepstral features namely, constant Q cepstral coefficients and mel frequency cepstral coefficients are used. A combination of all these features is performed to obtain a high degree of accuracy for spoof detection. Initially, efficacy of these features are tested on the development set of the ASVspoof 2017 database with Gaussian mixture model based systems. The systems are then fused at score level which acts as the final combined system for the challenge. The combined system is able to outperform the individual systems by a significant margin. Finally, the experiments are repeated on the evaluation set of the database and the combined system results in an equal error rate of 13.95%",
    "checked": true,
    "id": "c298f364ec1cc2e81aba3462d3a90751bc6b94ba",
    "semantic_title": "spoof detection using source, instantaneous frequency and cepstral features",
    "citation_count": 86
  },
  "https://www.isca-speech.org/archive/interspeech_2017/witkowski17_interspeech.html": {
    "title": "Audio Replay Attack Detection Using High-Frequency Features",
    "volume": "main",
    "abstract": "This paper presents our contribution to the ASVspoof 2017 Challenge. It addresses a replay spoofing attack against a speaker recognition system by detecting that the analysed signal has passed through multiple analogue-to-digital (AD) conversions. Specifically, we show that most of the cues that enable to detect the replay attacks can be found in the high-frequency band of the replayed recordings. The described anti-spoofing countermeasures are based on (1) modelling the subband spectrum and (2) using the proposed features derived from the linear prediction (LP) analysis. The results of the investigated methods show a significant improvement in comparison to the baseline system of the ASVspoof 2017 Challenge. A relative equal error rate (EER) reduction by 70% was achieved for the development set and a reduction by 30% was obtained for the evaluation set",
    "checked": true,
    "id": "7e41a8535e4fa2f9a88b0c59c602c2343eaf085d",
    "semantic_title": "audio replay attack detection using high-frequency features",
    "citation_count": 132
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wang17_interspeech.html": {
    "title": "Feature Selection Based on CQCCs for Automatic Speaker Verification Spoofing",
    "volume": "main",
    "abstract": "The ASVspoof 2017 challenge aims to assess spoofing and countermeasures attack detection accuracy for automatic speaker verification. It has been proven that constant Q cepstral coefficients (CQCCs) processes speech in different frequencies with variable resolution and performs much better than traditional features. When coupled with a Gaussian mixture model (GMM), it is an excellently effective spoofing countermeasure. The baseline CQCC+GMM system considers short-term impacts while ignoring the whole influence of channel. In the meanwhile, dimension of the feature is relatively higher than the traditional feature and usually with a higher variance. This paper explores different features for ASVspoof 2017 challenge. The mean and variance of the CQCC features of an utterance is used as the representation of the whole utterance. Feature selection method is introduced to avoid high variance and overfitting for spoofing detection. Experimental results on ASVspoof 2017 dataset show that feature selection followed by Support Vector Machine (SVM) gets an improvement compared to the baseline. It is also shown that pitch feature contributes to the performance improvement, and it obtains a relative improvement of 37.39% over the baseline CQCC+GMM system",
    "checked": true,
    "id": "2b37e26aa92c4433bc4ea54c8d41426e3c6242a5",
    "semantic_title": "feature selection based on cqccs for automatic speaker verification spoofing",
    "citation_count": 31
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ylmaz17_interspeech.html": {
    "title": "Longitudinal Speaker Clustering and Verification Corpus with Code-Switching Frisian-Dutch Speech",
    "volume": "main",
    "abstract": "In this paper, we present a new longitudinal and bilingual broadcast database designed for speaker clustering and text-independent verification research. The broadcast data is extracted from the archives of Omrop Fryslân which is the regional broadcaster in the province of Fryslân, located in the north of the Netherlands. Two speaker verification tasks are provided in a standard enrollment-test setting with language consistent trials. The first task contains target trials from all speakers available appearing in at least two different programs, while the second task contains target trials from a subgroup of speakers appearing in programs recorded in multiple years. The second task is designed to investigate the effects of ageing on the accuracy of speaker verification systems. This database also contains unlabeled spoken segments from different radio programs for speaker clustering research. We provide the output of an existing speaker diarization system for baseline verification experiments. Finally, we present the baseline speaker verification results using the Kaldi GMM- and DNN-UBM speaker verification system. This database will be an extension to the recently presented open source Frisian data collection and it is publicly available for research purposes",
    "checked": true,
    "id": "efd7f517a1f1f61b39e57a8582fd692ae8c97d6b",
    "semantic_title": "longitudinal speaker clustering and verification corpus with code-switching frisian-dutch speech",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ylmaz17b_interspeech.html": {
    "title": "Exploiting Untranscribed Broadcast Data for Improved Code-Switching Detection",
    "volume": "main",
    "abstract": "We have recently presented an automatic speech recognition (ASR) system operating on Frisian-Dutch code-switched speech. This type of speech requires careful handling of unexpected language switches that may occur in a single utterance. In this paper, we extend this work by using some raw broadcast data to improve multilingually trained deep neural networks (DNN) that have been trained on 11.5 hours of manually annotated bilingual speech. For this purpose, we apply the initial ASR to the untranscribed broadcast data and automatically create transcriptions based on the recognizer output using different language models for rescoring. Then, we train new acoustic models on the combined data, i.e., the manually and automatically transcribed bilingual broadcast data, and investigate the automatic transcription quality based on the recognition accuracies on a separate set of development and test data. Finally, we report code-switching detection performance elaborating on the correlation between the ASR and the code-switching detection performance",
    "checked": true,
    "id": "0f93bff2437383e29bd1c2f14de1d5377d6d35f3",
    "semantic_title": "exploiting untranscribed broadcast data for improved code-switching detection",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ramanarayanan17_interspeech.html": {
    "title": "Jee haan, I'd like both, por favor: Elicitation of a Code-Switched Corpus of Hindi–English and Spanish–English Human–Machine Dialog",
    "volume": "main",
    "abstract": "We present a database of code-switched conversational human–machine dialog in English–Hindi and English–Spanish. We leveraged HALEF, an open-source standards-compliant cloud-based dialog system to capture audio and video of bilingual crowd workers as they interacted with the system. We designed conversational items with intra-sentential code-switched machine prompts, and examine its efficacy in eliciting code-switched speech in a total of over 700 dialogs. We analyze various characteristics of the code-switched corpus and discuss some considerations that should be taken into account while collecting and processing such data. Such a database can be leveraged for a wide range of potential applications, including automated processing, recognition and understanding of code-switched speech and language learning applications for new language learners",
    "checked": false,
    "id": "bce75eb27fada3cb4e3777da0f9d4e35631b51a7",
    "semantic_title": "jee haan, i'd like both, por favor: elicitation of a code-switched corpus of hindi-english and spanish-english human-machine dialog",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2017/rallabandi17_interspeech.html": {
    "title": "On Building Mixed Lingual Speech Synthesis Systems",
    "volume": "main",
    "abstract": "Codemixing — phenomenon where lexical items from one language are embedded in the utterance of another — is relatively frequent in multilingual communities. However, TTS systems today are not fully capable of effectively handling such mixed content despite achieving high quality in the monolingual case. In this paper, we investigate various mechanisms for building mixed lingual systems which are built using a mixture of monolingual corpora and are capable of synthesizing such content. First, we explore the possibility of manipulating the phoneme representation: using target word to source phone mapping with the aim of emulating the native speaker intuition. We then present experiments at the acoustic stage investigating training techniques at both spectral and prosodic levels. Subjective evaluation shows that our systems are capable of generating high quality synthesis in codemixed scenarios",
    "checked": true,
    "id": "02a20ed2182475b40a4e7744aa6555607adffa62",
    "semantic_title": "on building mixed lingual speech synthesis systems",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chandu17_interspeech.html": {
    "title": "Speech Synthesis for Mixed-Language Navigation Instructions",
    "volume": "main",
    "abstract": "Text-to-Speech (TTS) systems that can read navigation instructions are one of the most widely used speech interfaces today. Text in the navigation domain may contain named entities such as location names that are not in the language that the TTS database is recorded in. Moreover, named entities can be compound words where individual lexical items belong to different languages. These named entities may be transliterated into the script that the TTS system is trained on. This may result in incorrect pronunciation rules being used for such words. We describe experiments to extend our previous work in generating code-mixed speech to synthesize navigation instructions, with a mixed-lingual TTS system. We conduct subjective listening tests with two sets of users, one being students who are native speakers of an Indian language and very proficient in English, and the other being drivers with low English literacy, but familiarity with location names. We find that in both sets of users, there is a significant preference for our proposed system over a baseline system that synthesizes instructions in English",
    "checked": true,
    "id": "99f07e194197a55fd017657d4cd1a8d9c349de05",
    "semantic_title": "speech synthesis for mixed-language navigation instructions",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2017/amazouz17_interspeech.html": {
    "title": "Addressing Code-Switching in French/Algerian Arabic Speech",
    "volume": "main",
    "abstract": "This study focuses on code-switching (CS) in French/Algerian Arabic bilingual communities and investigates how speech technologies, such as automatic data partitioning, language identification and automatic speech recognition (ASR) can serve to analyze and classify this type of bilingual speech. A preliminary study carried out using a corpus of Maghrebian broadcast data revealed a relatively high presence of CS Algerian Arabic as compared to the neighboring countries Morocco and Tunisia. Therefore this study focuses on code switching produced by bilingual Algerian speakers who can be considered native speakers of both Algerian Arabic and French. A specific corpus of four hours of speech from 8 bilingual French Algerian speakers was collected. This corpus contains read speech and conversational speech in both languages and includes stretches of code-switching. We provide a linguistic description of the code-switching stretches in terms of intra-sentential and inter-sentential switches, the speech duration in each language. We report on some initial studies to locate French, Arabic and the code-switched stretches, using ASR system word posteriors for this pair of languages",
    "checked": true,
    "id": "c935bcc748c4e9b87420deaf23074535592950d0",
    "semantic_title": "addressing code-switching in french/algerian arabic speech",
    "citation_count": 43
  },
  "https://www.isca-speech.org/archive/interspeech_2017/guzman17_interspeech.html": {
    "title": "Metrics for Modeling Code-Switching Across Corpora",
    "volume": "main",
    "abstract": "In developing technologies for code-switched speech, it would be desirable to be able to predict how much language mixing might be expected in the signal and the regularity with which it might occur. In this work, we offer various metrics that allow for the classification and visualization of multilingual corpora according to the ratio of languages represented, the probability of switching between them, and the time-course of switching. Applying these metrics to corpora of different languages and genres, we find that they display distinct probabilities and periodicities of switching, information useful for speech processing of mixed-language data",
    "checked": true,
    "id": "25a5cf5c7dc2269cf67d98b2fb46317a4d16b581",
    "semantic_title": "metrics for modeling code-switching across corpora",
    "citation_count": 50
  },
  "https://www.isca-speech.org/archive/interspeech_2017/westhuizen17_interspeech.html": {
    "title": "Synthesising isiZulu-English Code-Switch Bigrams Using Word Embeddings",
    "volume": "main",
    "abstract": "Code-switching is prevalent among South African speakers, and presents a challenge to automatic speech recognition systems. It is predominantly a spoken phenomenon, and generally does not occur in textual form. Therefore a particularly serious challenge is the extreme lack of training material for language modelling. We investigate the use of word embeddings to synthesise isiZulu-to-English code-switch bigrams with which to augment such sparse language model training data. A variety of word embeddings are trained on a monolingual English web text corpus, and subsequently queried to synthesise code-switch bigrams. Our evaluation is performed on language models trained on a new, although small, English-isiZulu code-switch corpus compiled from South African soap operas. This data is characterised by fast, spontaneously spoken speech containing frequent code-switching. We show that the augmentation of the training data with code-switched bigrams synthesised in this way leads to a reduction in perplexity",
    "checked": true,
    "id": "7b263493401762ede1f3a1f1e134e3c0e8584ed8",
    "semantic_title": "synthesising isizulu-english code-switch bigrams using word embeddings",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2017/soto17_interspeech.html": {
    "title": "Crowdsourcing Universal Part-of-Speech Tags for Code-Switching",
    "volume": "main",
    "abstract": "Code-switching is the phenomenon by which bilingual speakers switch between multiple languages during communication. The importance of developing language technologies for code-switching data is immense, given the large populations that routinely code-switch. High-quality linguistic annotations are extremely valuable for any NLP task, and performance is often limited by the amount of high-quality labeled data. However, little such data exists for code-switching. In this paper, we describe crowd-sourcing universal part-of-speech tags for the Miami Bangor Corpus of Spanish-English code-switched speech. We split the annotation task into three subtasks: one in which a subset of tokens are labeled automatically, one in which questions are specifically designed to disambiguate a subset of high frequency words, and a more general cascaded approach for the remaining data in which questions are displayed to the worker following a decision tree structure. Each subtask is extended and adapted for a multilingual setting and the universal tagset. The quality of the annotation process is measured using hidden check questions annotated with gold labels. The overall agreement between gold standard labels and the majority vote is between 0.95 and 0.96 for just three labels and the average recall across part-of-speech tags is between 0.87 and 0.99, depending on the task",
    "checked": true,
    "id": "6a4e10bcbb86e5ea55f3dfbfd269dccba62b5db9",
    "semantic_title": "crowdsourcing universal part-of-speech tags for code-switching",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lavrentyeva17_interspeech.html": {
    "title": "Audio Replay Attack Detection with Deep Learning Frameworks",
    "volume": "main",
    "abstract": "Nowadays spoofing detection is one of the priority research areas in the field of automatic speaker verification. The success of Automatic Speaker Verification Spoofing and Countermeasures (ASVspoof) Challenge 2015 confirmed the impressive perspective in detection of unforeseen spoofing trials based on speech synthesis and voice conversion techniques. However, there is a small number of researches addressed to replay spoofing attacks which are more likely to be used by non-professional impersonators. This paper describes the Speech Technology Center (STC) anti-spoofing system submitted for ASVspoof 2017 which is focused on replay attacks detection. Here we investigate the efficiency of a deep learning approach for solution of the mentioned-above task. Experimental results obtained on the Challenge corpora demonstrate that the selected approach outperforms current state-of-the-art baseline systems in terms of spoofing detection quality. Our primary system produced an EER of 6.73% on the evaluation part of the corpora which is 72% relative improvement over the ASVspoof 2017 baseline system",
    "checked": true,
    "id": "a2b4c396dc1064fb90bb5455525733733c761a7f",
    "semantic_title": "audio replay attack detection with deep learning frameworks",
    "citation_count": 232
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ji17_interspeech.html": {
    "title": "Ensemble Learning for Countermeasure of Audio Replay Spoofing Attack in ASVspoof2017",
    "volume": "main",
    "abstract": "To enhance the security and reliability of automatic speaker verification (ASV) systems, ASVspoof 2017 challenge focuses on the detection problem of known and unknown audio replay attacks. We proposed an ensemble learning classifier for CNCB team's submitted system scores, which across uses a variety of acoustic features and classifiers. An effective post-processing method is studied to improve the performance of Constant Q cepstral coefficients (CQCC) and to form a base feature set with some other classical acoustic features. We also proposed using an ensemble classifier set, which includes multiple Gaussian Mixture Model (GMM) based classifiers and two novel GMM mean supervector-Gradient Boosting Decision Tree (GSV-GBDT) and GSV-Random Forest (GSV-RF) classifiers. Experimental results have shown that the proposed ensemble learning system can provide substantially better performance than baseline. On common training condition of the challenge, Equal Error Rate (EER) of primary system on development set is 1.5%, compared to baseline 10.4%. EER of primary system (S02 in ASVspoof 2017 board) on evaluation data set are 12.3% (with only train dataset) and 10.8% (with train+dev dataset), which are also much better than baseline 30.6% and 24.8%, given by ASVSpoof 2017 organizer, with 59.7% and 56.4% relative performance improvement",
    "checked": true,
    "id": "8a862946febf154effe65d4ec94d8e6bd39f690d",
    "semantic_title": "ensemble learning for countermeasure of audio replay spoofing attack in asvspoof2017",
    "citation_count": 45
  },
  "https://www.isca-speech.org/archive/interspeech_2017/li17b_interspeech.html": {
    "title": "A Study on Replay Attack and Anti-Spoofing for Automatic Speaker Verification",
    "volume": "main",
    "abstract": "For practical automatic speaker verification (ASV) systems, replay attack poses a true risk. By replaying a pre-recorded speech signal of the genuine speaker, ASV systems tend to be easily fooled. An effective replay detection method is therefore highly desirable. In this study, we investigate a major difficulty in replay detection: the over-fitting problem caused by variability factors in speech signal. An F-ratio probing tool is proposed and three variability factors are investigated using this tool: speaker identity, speech content and playback & recording device. The analysis shows that device is the most influential factor that contributes the highest over-fitting risk. A frequency warping approach is studied to alleviate the over-fitting problem, as verified on the ASV-spoof 2017 database",
    "checked": true,
    "id": "466196986827959c10dfddce40202ad3e8341968",
    "semantic_title": "a study on replay attack and anti-spoofing for automatic speaker verification",
    "citation_count": 50
  },
  "https://www.isca-speech.org/archive/interspeech_2017/nagarsheth17_interspeech.html": {
    "title": "Replay Attack Detection Using DNN for Channel Discrimination",
    "volume": "main",
    "abstract": "Voice is projected to be the next input interface for portable devices. The increased use of audio interfaces can be mainly attributed to the success of speech and speaker recognition technologies. With these advances comes the risk of criminal threats where attackers are reportedly trying to access sensitive information using diverse voice spoofing techniques. Among them, replay attacks pose a real challenge to voice biometrics. This paper addresses the problem by proposing a deep learning architecture in tandem with low-level cepstral features. We investigate the use of a deep neural network (DNN) to discriminate between the different channel conditions available in the ASVSpoof 2017 dataset, namely recording, playback and session conditions. The high-level feature vectors derived from this network are used to discriminate between genuine and spoofed audio. Two kinds of low-level features are utilized: state-of-the-art constant-Q cepstral coefficients (CQCC), and our proposed high-frequency cepstral coefficients (HFCC) that derive from the high-frequency spectrum of the audio. The fusion of both features proved to be effective in generalizing well across diverse replay attacks seen in the evaluation of the ASVSpoof 2017 challenge, with an equal error rate of 11.5%, that is 53% better than the baseline Gaussian Mixture Model (GMM) applied on CQCC",
    "checked": true,
    "id": "9089817cb2a7eaf1d5835034fa5ba3f76f375cd4",
    "semantic_title": "replay attack detection using dnn for channel discrimination",
    "citation_count": 100
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17_interspeech.html": {
    "title": "ResNet and Model Fusion for Automatic Spoofing Detection",
    "volume": "main",
    "abstract": "Speaker verification systems have achieved great progress in recent years. Unfortunately, they are still highly prone to different kinds of spoofing attacks such as speech synthesis, voice conversion, and fake audio recordings etc. Inspired by the success of ResNet in image recognition, we investigated the effectiveness of using ResNet for automatic spoofing detection. Experimental results on the ASVspoof2017 data set show that ResNet performs the best among all the single-model systems. Model fusion is a good way to further improve the system performance. Nevertheless, we found that if the same feature is used for different fused models, the resulting system can hardly be improved. By using different features and models, our best fused model further reduced the Equal Error Rate (EER) by 18% relatively, compared with the best single-model system",
    "checked": true,
    "id": "6a7b88c8dc37850f8ffe48dcf7d839c6f0d47873",
    "semantic_title": "resnet and model fusion for automatic spoofing detection",
    "citation_count": 117
  },
  "https://www.isca-speech.org/archive/interspeech_2017/alluri17_interspeech.html": {
    "title": "SFF Anti-Spoofer: IIIT-H Submission for Automatic Speaker Verification Spoofing and Countermeasures Challenge 2017",
    "volume": "main",
    "abstract": "The ASVspoof 2017 challenge is about the detection of replayed speech from human speech. The proposed system makes use of the fact that when the speech signals are replayed, they pass through multiple channels as opposed to original recordings. This channel information is typically embedded in low signal to noise ratio regions. A speech signal processing method with high spectro-temporal resolution is required to extract robust features from such regions. The single frequency filtering (SFF) is one such technique, which we propose to use for replay attack detection. While SFF based feature representation was used at front-end, Gaussian mixture model and bi-directional long short-term memory models are investigated at the backend as classifiers. The experimental results on ASVspoof 2017 dataset reveal that, SFF based representation is very effective in detecting replay attacks. The score level fusion of back end classifiers further improved the performance of the system which indicates that both classifiers capture complimentary information",
    "checked": true,
    "id": "249e6da34b4218a3960e65f92c4c554ede62a412",
    "semantic_title": "sff anti-spoofer: iiit-h submission for automatic speaker verification spoofing and countermeasures challenge 2017",
    "citation_count": 56
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hartmann17_interspeech.html": {
    "title": "Improved Single System Conversational Telephone Speech Recognition with VGG Bottleneck Features",
    "volume": "main",
    "abstract": "On small datasets, discriminatively trained bottleneck features from deep networks commonly outperform more traditional spectral or cepstral features. While these features are typically trained with small, fully-connected networks, recent studies have used more sophisticated networks with great success. We use the recent deep CNN (VGG) network for bottleneck feature extraction — previously used only for low-resource tasks — and apply it to the Switchboard English conversational telephone speech task. Unlike features derived from traditional MLP networks, the VGG features outperform cepstral features even when used with BLSTM acoustic models trained on large amounts of data. We achieve the best BBN single system performance when combining the VGG features with a BLSTM acoustic model. When decoding with an n-gram language model, which are used for deployable systems, we have a realistic production system with a WER of 7.4%. This result is competitive with the current state-of-the-art in the literature. While our focus is on realistic single system performance, we further reduce the WER to 6.1% through system combination and using expensive neural network language model rescoring",
    "checked": true,
    "id": "850730adb72d900c9b484829c42f07c20e9af06d",
    "semantic_title": "improved single system conversational telephone speech recognition with vgg bottleneck features",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wong17_interspeech.html": {
    "title": "Student-Teacher Training with Diverse Decision Tree Ensembles",
    "volume": "main",
    "abstract": "Student-teacher training allows a large teacher model or ensemble of teachers to be compressed into a single student model, for the purpose of efficient decoding. However, current approaches in automatic speech recognition assume that the state clusters, often defined by Phonetic Decision Trees (PDT), are the same across all models. This limits the diversity that can be captured within the ensemble, and also the flexibility when selecting the complexity of the student model output. This paper examines an extension to student-teacher training that allows for the possibility of having different PDTs between teachers, and also for the student to have a different PDT from the teacher. The proposal is to train the student to emulate the logical context dependent state posteriors of the teacher, instead of the frame posteriors. This leads to a method of mapping frame posteriors from one PDT to another. This approach is evaluated on three speech recognition tasks: the Tok Pisin and Javanese low resource conversational telephone speech tasks from the IARPA Babel programme, and the HUB4 English broadcast news task",
    "checked": true,
    "id": "15a1e3afa2e91477f21c3ddcb1fc58a58b6d772f",
    "semantic_title": "student-teacher training with diverse decision tree ensembles",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2017/cui17_interspeech.html": {
    "title": "Embedding-Based Speaker Adaptive Training of Deep Neural Networks",
    "volume": "main",
    "abstract": "An embedding-based speaker adaptive training (SAT) approach is proposed and investigated in this paper for deep neural network acoustic modeling. In this approach, speaker embedding vectors, which are a constant given a particular speaker, are mapped through a control network to layer-dependent element-wise affine transformations to canonicalize the internal feature representations at the output of hidden layers of a main network. The control network for generating the speaker-dependent mappings are jointly estimated with the main network for the overall speaker adaptive acoustic modeling. Experiments on large vocabulary continuous speech recognition (LVCSR) tasks show that the proposed SAT scheme can yield superior performance over the widely-used speaker-aware training using i-vectors with speaker-adapted input features",
    "checked": true,
    "id": "0a0820133e29e8039e9eff70aaeb4ecf76cdb25c",
    "semantic_title": "embedding-based speaker adaptive training of deep neural networks",
    "citation_count": 37
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ma17_interspeech.html": {
    "title": "Improving Deliverable Speech-to-Text Systems with Multilingual Knowledge Transfer",
    "volume": "main",
    "abstract": "This paper reports our recent progress on using multilingual data for improving speech-to-text (STT) systems that can be easily delivered. We continued the work BBN conducted on the use of multilingual data for improving Babel evaluation systems, but focused on training time-delay neural network (TDNN) based chain models. As done for the Babel evaluations, we used multilingual data in two ways: first, to train multilingual deep neural networks (DNN) for extracting bottle-neck (BN) features, and second, for initializing training on target languages Our results show that TDNN chain models trained on multilingual DNN bottleneck features yield significant gains over their counterparts trained on MFCC plus i-vector features. By initializing from models trained on multilingual data, TDNN chain models can achieve great improvements over random initializations of the network weights on target languages. Two other important findings are: 1) initialization with multilingual TDNN chain models produces larger gains on target languages that have less training data; 2) inclusion of target languages in multilingual training for either BN feature extraction or initialization have limited impact on performance measured on the target languages. Our results also reveal that for TDNN chain models, the combination of multilingual BN features and multilingual initialization achieves the best performance on all target languages",
    "checked": true,
    "id": "661da1a08bba29ec88c454c8913dbd828cf69f4c",
    "semantic_title": "improving deliverable speech-to-text systems with multilingual knowledge transfer",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2017/saon17_interspeech.html": {
    "title": "English Conversational Telephone Speech Recognition by Humans and Machines",
    "volume": "main",
    "abstract": "Word error rates on the Switchboard conversational corpus that just a few years ago were 14% have dropped to 8.0%, then 6.6% and most recently 5.8%, and are now believed to be within striking range of human performance. This then raises two issues: what is human performance, and how far down can we still drive speech recognition error rates? In trying to assess human performance, we performed an independent set of measurements on the Switchboard and CallHome subsets of the Hub5 2000 evaluation and found that human accuracy may be considerably better than what was earlier reported, giving the community a significantly harder goal to achieve. We also report on our own efforts in this area, presenting a set of acoustic and language modeling techniques that lowered the WER of our system to 5.5%/10.3% on these subsets, which is a new performance milestone (albeit not at what we measure to be human performance). On the acoustic side, we use a score fusion of one LSTM with multiple feature inputs, a second LSTM trained with speaker-adversarial multi-task learning and a third convolutional residual net (ResNet). On the language modeling side, we use word and character LSTMs and convolutional WaveNet-style language models",
    "checked": true,
    "id": "c9bd15c7838c1d3cdd5f5113a2efd9440f86b3da",
    "semantic_title": "english conversational telephone speech recognition by humans and machines",
    "citation_count": 346
  },
  "https://www.isca-speech.org/archive/interspeech_2017/stolcke17_interspeech.html": {
    "title": "Comparing Human and Machine Errors in Conversational Speech Transcription",
    "volume": "main",
    "abstract": "Recent work in automatic recognition of conversational telephone speech (CTS) has achieved accuracy levels comparable to human transcribers, although there is some debate how to precisely quantify human performance on this task, using the NIST 2000 CTS evaluation set. This raises the question what systematic differences, if any, may be found differentiating human from machine transcription errors. In this paper we approach this question by comparing the output of our most accurate CTS recognition system to that of a standard speech transcription vendor pipeline. We find that the most frequent substitution, deletion and insertion error types of both outputs show a high degree of overlap. The only notable exception is that the automatic recognizer tends to confuse filled pauses (\"uh\") and backchannel acknowledgments (\"uhhuh\"). Human tend not to make this error, presumably due to the distinctive and opposing pragmatic functions attached to these words. Furthermore, we quantify the correlation between human and machine errors at the speaker level, and investigate the effect of speaker overlap between training and test data. Finally, we report on an informal \"Turing test\" asking humans to discriminate between automatic and human transcription error cases",
    "checked": true,
    "id": "002b9e38bf1f82a2b876082cb866c67e8ace2ba1",
    "semantic_title": "comparing human and machine errors in conversational speech transcription",
    "citation_count": 51
  },
  "https://www.isca-speech.org/archive/interspeech_2017/petukhova17_interspeech.html": {
    "title": "Multimodal Markers of Persuasive Speech: Designing a Virtual Debate Coach",
    "volume": "main",
    "abstract": "The study presented in this paper is carried out to support debate performance assessment in the context of debate skills training. The perception of good performance as a debater is influenced by how believable and convincing the debater's argumentation is. We identified a number of features that are useful for explaining perceived properties of persuasive speech and for defining rules and strategies to produce and assess debate performance. We collected and analysed multimodal and multisensory data of the trainees debate behaviour, and contrasted it with those of skilled professional debaters. Observational, correlation and machine learning studies were performed to identify multimodal markers of persuasive speech and link them to experts' assessments. A combination of multimodal in- and out-of-domain debate data, and various non-verbal, prosodic, lexical, linguistic and structural features has been computed based on our analysis and assessed used to , and several classification procedures has been applied achieving an accuracy of 0.79 on spoken debate data",
    "checked": true,
    "id": "814f31f7dbaf5f6341407cc9223d72ba4ff6c5ae",
    "semantic_title": "multimodal markers of persuasive speech: designing a virtual debate coach",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bone17_interspeech.html": {
    "title": "Acoustic-Prosodic and Physiological Response to Stressful Interactions in Children with Autism Spectrum Disorder",
    "volume": "main",
    "abstract": "Social anxiety is a prevalent condition affecting individuals to varying degrees. Research on autism spectrum disorder (ASD), a group of neurodevelopmental disorders marked by impairments in social communication, has found that social anxiety occurs more frequently in this population. Our study aims to further understand the multimodal manifestation of social stress for adolescents with ASD versus neurotypically developing (TD) peers. We investigate this through objective measures of speech behavior and physiology (mean heart rate) acquired during three tasks: a low-stress conversation, a medium-stress interview, and a high-stress presentation. Measurable differences are found to exist for speech behavior and heart rate in relation to task-induced stress. Additionally, we find the acoustic measures are particularly effective for distinguishing between diagnostic groups. Individuals with ASD produced higher prosodic variability, agreeing with previous reports. Moreover, the most informative features captured an individual's vocal changes between low and high social-stress, suggesting an interaction between vocal production and social stressors in ASD",
    "checked": true,
    "id": "4cec5b4afd01f59558942db3fe13e697f85a2c67",
    "semantic_title": "acoustic-prosodic and physiological response to stressful interactions in children with autism spectrum disorder",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2017/burmania17_interspeech.html": {
    "title": "A Stepwise Analysis of Aggregated Crowdsourced Labels Describing Multimodal Emotional Behaviors",
    "volume": "main",
    "abstract": "Affect recognition is a difficult problem that most often relies on human annotated data to train automated systems. As humans perceive emotion differently based on personality, cognitive state and past experiences, it is important to collect rankings from multiple individuals to assess the emotional content in corpora, which are later aggregated with rules such as majority vote. With the increased use of crowdsourcing services for perceptual evaluations, collecting large amount of data is now feasible. It becomes important to question the amount of data needed to create well-trained classifiers. How different are the aggregated labels collected from five raters compared to the ones obtained from twenty evaluators? Is it worthwhile to spend resources to increase the number of evaluators beyond those used in conventional/laboratory studies? This study evaluates the consensus labels obtained by incrementally adding new evaluators during perceptual evaluations. Using majority vote over categorical emotional labels, we compare the changes in the aggregated labels starting with one rater, and finishing with 20 raters. The large number of evaluators in a subset of the MSP-IMPROV database and the ability to filter annotators by quality allows us to better understand label aggregation as a function of the number of annotators",
    "checked": true,
    "id": "34a45c92139a9ffd0c76f8eee2201485232bdb20",
    "semantic_title": "a stepwise analysis of aggregated crowdsourced labels describing multimodal emotional behaviors",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2017/fotedar17_interspeech.html": {
    "title": "An Information Theoretic Analysis of the Temporal Synchrony Between Head Gestures and Prosodic Patterns in Spontaneous Speech",
    "volume": "main",
    "abstract": "We analyze the temporal co-ordination between head gestures and prosodic patterns in spontaneous speech in a data-driven manner. For this study, we consider head motion and speech data from 24 subjects while they tell a fixed set of five stories. The head motion, captured using a motion capture system, is converted to Euler angles and translations in X, Y and Z-directions to represent head gestures. Pitch and short-time energy in voiced segments are used to represent the prosodic patterns. To capture the statistical relationship between head gestures and prosodic patterns, mutual information (MI) is computed at various delays between the two using data from 24 subjects in six native languages. The estimated MI, averaged across all subjects, is found to be maximum when the head gestures lag the prosodic patterns by 30msec. This is found to be true when subjects tell stories in English as well as in their native language. We observe a similar pattern in the root mean squared error of predicting head gestures from prosodic patterns using Gaussian mixture model. These results indicate that there could be an asynchrony between head gestures and prosody during spontaneous speech where head gestures follow the corresponding prosodic patterns",
    "checked": true,
    "id": "7292a91864be55f734a7088be70594db075e8d00",
    "semantic_title": "an information theoretic analysis of the temporal synchrony between head gestures and prosodic patterns in spontaneous speech",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2017/huang17_interspeech.html": {
    "title": "Multimodal Prediction of Affective Dimensions via Fusing Multiple Regression Techniques",
    "volume": "main",
    "abstract": "This paper presents a multimodal approach to predict affective dimensions, that makes full use of features from audio, video, Electrodermal Activity (EDA) and Electrocardiogram (ECG) using three regression techniques such as support vector regression (SVR), partial least squares regression (PLS), and a deep bidirectional long short-term memory recurrent neural network (DBLSTM-RNN) regression. Each of the three regression techniques performs multimodal affective dimension prediction followed by a fusion of different models on features of four modalities using a support vector regression. A support vector regression is also applied for a final fusion of the three regression systems. Experiments show that our proposed approach obtains promising results on the AVEC 2015 benchmark dataset for prediction of multimodal affective dimensions. For the development set, the concordance correlation coefficient (CCC) achieves results of 0.856 for arousal and 0.720 for valence, which increases 3.88% and 4.66% of the top-performer of AVEC 2015 in arousal and valence, respectively",
    "checked": true,
    "id": "dad58628834572e085c815b2601171187602e3b4",
    "semantic_title": "multimodal prediction of affective dimensions via fusing multiple regression techniques",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2017/dohen17_interspeech.html": {
    "title": "Co-Production of Speech and Pointing Gestures in Clear and Perturbed Interactive Tasks: Multimodal Designation Strategies",
    "volume": "main",
    "abstract": "Designation consists in attracting an interlocutor's attention on a specific object and/or location. It is most often achieved using both speech (e.g., demonstratives) and gestures (e.g., manual pointing). This study aims at analyzing how speech and pointing gestures are co-produced in a semi-directed interactive task involving designation. 20 native speakers of French were involved in a cooperative task in which they provided instructions to a partner for her to reproduce a model she could not see on a grid both of them saw. They had to use only sentences of the form ‘The [target word] goes there.'. They did this in two conditions: silence and noise. Their speech and articulatory/hand movements (motion capture) were recorded. The analyses show that the participants' speech features were modified in noise (Lombard effect). They also spoke slower and made more pauses and errors. Their pointing gestures lasted longer and started later showing an adaptation of gesture production to speech. The condition did not influence speech/gesture coordination. The apex (part of the gesture that shows) mainly occurred at the same time as the target word and not as the demonstrative showing that speakers group speech and gesture carrying complementary rather than redundant information",
    "checked": true,
    "id": "d80b346673bb9c4db02e8aca4935b218941af9a3",
    "semantic_title": "co-production of speech and pointing gestures in clear and perturbed interactive tasks: multimodal designation strategies",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2017/guzewich17_interspeech.html": {
    "title": "Improving Speaker Verification for Reverberant Conditions with Deep Neural Network Dereverberation Processing",
    "volume": "main",
    "abstract": "We present an improved method for training Deep Neural Networks for dereverberation and show that it can improve performance for the speech processing tasks of speaker verification and speech enhancement. We replicate recently proposed methods for dereverberation using Deep Neural Networks and present our improved method, highlighting important aspects that influence performance. We then experimentally evaluate the capabilities and limitations of the method with respect to speech quality and speaker verification to show that ours achieves better performance than other proposed methods",
    "checked": true,
    "id": "5fad2d973e1a1ec3903d38b999fbda27342560ef",
    "semantic_title": "improving speaker verification for reverberant conditions with deep neural network dereverberation processing",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bulling17_interspeech.html": {
    "title": "Stepsize Control for Acoustic Feedback Cancellation Based on the Detection of Reverberant Signal Periods and the Estimated System Distance",
    "volume": "main",
    "abstract": "A new approach for acoustic feedback cancellation is presented. The challenge in acoustic feedback cancellation is a strong correlation between the local speech and the loudspeaker signal. Due to this correlation, the convergence rate of adaptive algorithms is limited. Therefore, a novel stepsize control of the adaptive filter is presented. The stepsize control exploits reverberant signal periods to update the adaptive filter. As soon as local speech stops, the reverberation energy of the system decays exponentially. This means that during reverberation there is only excitation of the filter but no local speech. Thus, signals are not correlated and the filter can converge without correlation problems. Consequently, the stepsize control accelerates the adaption process during reverberation and slows it down at the beginning of speech activity. It is shown, that with a particular gain control, the reverb-based stepsize control can be interpreted as the theoretical optimum stepsize. However, for this purpose a precise estimation of the system distance is required. One estimation method is presented. The proposed estimator has a rescue mechanism to detect enclosure dislocations. Both, simulations and real world testing show that the acoustic feedback canceler is capable of improving stability and convergence rate, even at high system gains",
    "checked": true,
    "id": "36a39db98d31c8aabbd6852345edd63898a55c2e",
    "semantic_title": "stepsize control for acoustic feedback cancellation based on the detection of reverberant signal periods and the estimated system distance",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2017/franzen17_interspeech.html": {
    "title": "A Delay-Flexible Stereo Acoustic Echo Cancellation for DFT-Based In-Car Communication (ICC) Systems",
    "volume": "main",
    "abstract": "In-car communication (ICC) systems supporting speech communication in noise by reproducing amplified speech from the car cabin in the car cabin ask for low-delay acoustic echo cancellation (AEC). In this paper we propose a delay-flexible DFT-based stereo AEC capable of cancelling also the echoes stemming from the audio player or FM radio. For the price of a somewhat higher complexity we are able to reduce the 32 ms delay of the baseline down to 4 ms, loosing only 1 dB in ERLE while even preserving system distance properties",
    "checked": true,
    "id": "bc0f522aa0e225a2bc83ba46c4fd7570c9b51cd6",
    "semantic_title": "a delay-flexible stereo acoustic echo cancellation for dft-based in-car communication (icc) systems",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wang17b_interspeech.html": {
    "title": "Speech Enhancement Based on Harmonic Estimation Combined with MMSE to Improve Speech Intelligibility for Cochlear Implant Recipients",
    "volume": "main",
    "abstract": "In this paper, a speech enhancement algorithm is proposed to improve the speech intelligibility for cochlear implant recipients. Our method is based on combination of harmonic estimation and traditional statistical method. Traditional statistical based speech enhancement method is effective only for stationary noise suppression, but not non-stationary noise. To address more complex noise scenarios, we explore the harmonic structure of target speech to obtain a more accurate noise estimation. The estimated noise is then employed in the MMSE framework to obtain the gain function for recovering the target speech. Listening test experiments show a substantial speech intelligibility improvement for cochlear implant recipients in noisy environments",
    "checked": true,
    "id": "1e118317c8b26448b13559e0dd69010cab2aa954",
    "semantic_title": "speech enhancement based on harmonic estimation combined with mmse to improve speech intelligibility for cochlear implant recipients",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ayllon17_interspeech.html": {
    "title": "Improving Speech Intelligibility in Binaural Hearing Aids by Estimating a Time-Frequency Mask with a Weighted Least Squares Classifier",
    "volume": "main",
    "abstract": "An efficient algorithm for speech enhancement in binaural hearing aids is proposed. The algorithm is based on the estimation of a time-frequency mask using supervised machine learning. The standard least-squares linear classifier is reformulated to optimize a metric related to speech/noise separation. The method is energy-efficient in two ways: the computational complexity is limited and the wireless data transmission optimized. The ability of the algorithm to enhance speech contaminated with different types of noise and low SNR has been evaluated. Objective measures of speech intelligibility and speech quality demonstrate that the algorithm increments both the hearing comfort and speech understanding of the user. These results are supported by subjective listening tests",
    "checked": true,
    "id": "616054b72d505b539a3b9ce2a17e485123505fa2",
    "semantic_title": "improving speech intelligibility in binaural hearing aids by estimating a time-frequency mask with a weighted least squares classifier",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wu17_interspeech.html": {
    "title": "Simulations of High-Frequency Vocoder on Mandarin Speech Recognition for Acoustic Hearing Preserved Cochlear Implant",
    "volume": "main",
    "abstract": "Vocoder simulations are generally adopted to simulate the electrical hearing induced by the cochlear implant (CI). Our research group is developing a new four-electrode CI microsystem which induces high-frequency electrical hearing while preserving low-frequency acoustic hearing. To simulate the functionality of this CI, a previously developed hearing-impaired (HI) hearing model is combined with a 4-channel vocoder in this paper to respectively mimic the perceived acoustic hearing and electrical hearing. Psychoacoustic experiments are conducted on Mandarin speech recognition for determining parameters of electrodes for this CI. Simulation results show that initial consonants of Mandarin are more difficult to recognize than final vowels of Mandarin via acoustic hearing of HI patients. After electrical hearing being induced through logarithmic-frequency distributed electrodes, speech intelligibility of HI patients is boosted for all Mandarin phonemes, especially for initial consonants. Similar results are consistently observed in clean and noisy test conditions",
    "checked": true,
    "id": "6dfe77ccbea7e0eef743e5d6a4e8af58a2b2ac37",
    "semantic_title": "simulations of high-frequency vocoder on mandarin speech recognition for acoustic hearing preserved cochlear implant",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hermes17_interspeech.html": {
    "title": "Phonetic Correlates of Pharyngeal and Pharyngealized Consonants in Saudi, Lebanese, and Jordanian Arabic: An rt-MRI Study",
    "volume": "main",
    "abstract": "The phonemic inventory of Arabic includes sounds that involve a pharyngeal constriction. Sounds referred to as ‘pharyngeal' (/ʕ/ and /ħ/) are reported to have a primary constriction in the pharynx, while sounds referred to as ‘pharyngealized' (/s /, /t /, /d /, and /ð / or /z /) are reported to have a secondary constriction in the pharynx. Some studies propose grouping both types of sounds together, citing phonetic and phonological evidence. Phonetically, pharyngeal consonants are argued to have a primary constriction below the pharynx, and are thus posited to be pharyngealized laryngeals. Under this view, the pharyngeal constriction is secondary, not primary. Phonologically, it has been established that pharyngealized sounds trigger pharyngealization spread, and proposals for grouping pharyngeal and pharyngealized consonants together cite similar, but not identical, spread patterns triggered by pharyngeals. In this study, Real-time Magnetic Resonance Imaging is employed to investigate the phonetic correlates of the pharyngeal constriction in both pharyngeal and pharyngealized sounds in Saudi, Lebanese, and Jordanian Arabic as exemplified by one speaker from each dialect. Our findings demonstrate a difference in the location of constriction among both types of sounds. These distinctions in place possibly account for the differences in the spread patterns triggered by each type of sound",
    "checked": true,
    "id": "f9f4c903762e1102ad98f44444d6f5d9923da1fc",
    "semantic_title": "phonetic correlates of pharyngeal and pharyngealized consonants in saudi, lebanese, and jordanian arabic: an rt-mri study",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2017/elie17_interspeech.html": {
    "title": "Glottal Opening and Strategies of Production of Fricatives",
    "volume": "main",
    "abstract": "This work investigates the influence of the gradual opening of the glottis along its length during the production of fricatives in intervocalic contexts. Acoustic simulations reveal the existence of a transient zone in the articulatory space where the frication noise level is very sensitive to small perturbations of the glottal opening. This corresponds to the configurations where both frication noise and voiced contributions are present in the speech signal. To avoid this unstability, speakers may adopt different strategies to ensure the voiced/voiceless contrast of fricatives. This is evidenced by experimental data of simultaneous glottal opening measurements, performed with ePGG, and audio recordings of vowel-fricative-vowel pseudowords. Voiceless fricatives are usually longer, in order to maximize the number of voiceless time frames over voiced frames due to the crossing of the transient regime. For voiced fricatives, the speaker may avoid the unstable regime by keeping low frication noise level, and thus by favoring the voicing characteristic, or by doing very short crossings into the unstable regime. It is also shown that when speakers are asked to sustain voiced fricatives longer than in natural speech, they adopt the strategy of keeping low frication noise level to avoid the unstable regime",
    "checked": true,
    "id": "9bbb42a98fdd09c51c264b293e20590e684e4527",
    "semantic_title": "glottal opening and strategies of production of fricatives",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2017/frej17_interspeech.html": {
    "title": "Acoustics and Articulation of Medial versus Final Coronal Stop Gemination Contrasts in Moroccan Arabic",
    "volume": "main",
    "abstract": "This paper presents results of a simultaneous acoustic and articulatory investigation of word-medial and word-final geminate/singleton coronal stop contrasts in Moroccan Arabic (MA). The acoustic analysis revealed that, only for the word-medial contrast, the two MA speakers adopted comparable strategies in contrasting geminates with singletons, mainly by significantly lengthening closure duration in geminates, relative to singletons. In word-final position, two speaker-specific contrasting patterns emerged. While one speaker also lengthened the closure duration for final geminates, the other speaker instead lengthened only the release duration for final geminates, relative to singletons. Consonant closure and preceding vowel were significantly longer for the geminate only in medial position, not in final position. These temporal differences were even more clearly delineated in the articulatory signal, captured via ultrasound, to which we applied the novel approach of using TRACTUS [Temporally Resolved Articulatory Configuration Tracking of UltraSound: 15] to index temporal properties of closure gestures for these geminate/singleton contrasts",
    "checked": true,
    "id": "55e2565e1b3f996c114078e593b9cd8768e032d2",
    "semantic_title": "acoustics and articulation of medial versus final coronal stop gemination contrasts in moroccan arabic",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2017/turco17_interspeech.html": {
    "title": "How are Four-Level Length Distinctions Produced? Evidence from Moroccan Arabic",
    "volume": "main",
    "abstract": "We investigate the durational properties of Moroccan Arabic identical consonant sequences contrasting singleton (S) and geminate (G) dental fricatives, in six combinations of four-level length contrasts across word boundaries (#) (one timing slot for #S, two for #G and S#S, three for S#G and G#S, and four for G#G). The aim is to determine the nature of the mapping between discrete phonological timing units and phonetic durations. Acoustic results show that the largest and most systematic jump in duration is displayed between the singleton fricative on the one hand and the other sequences on the other hand. Looking at these sequences, S#S is shown to have the same duration as #G. When a geminate is within the sequence, a temporal reorganization is observed: G#S is not significantly longer than S#S and #G; and G#G is only slightly longer than S#G. Instead of a four-way hierarchy, our data point towards a possible upper limit of three-way length contrasts for consonants: S < G=S#S=G#S < S#G=G#G. The interplay of a number of factors resulting in this mismatch between phonological length and phonetic duration are discussed, and a working hypothesis is provided for why duration contrasts are rarely ternary, and almost never quaternary",
    "checked": true,
    "id": "c7a6ebd5e51b1cf995fec0eed7c3e2bebd04e1ad",
    "semantic_title": "how are four-level length distinctions produced? evidence from moroccan arabic",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jones17_interspeech.html": {
    "title": "Vowels in the Barunga Variety of North Australian Kriol",
    "volume": "main",
    "abstract": "North Australian Kriol is an English based creole spoken widely by Indigenous people in northern Australia in areas where the traditional languages are endangered or no longer spoken. This paper offers the first acoustic description of the vowel phonology of Roper Kriol, within a variety spoken at Barunga Community, east of the town of Katherine in the Northern Territory Drawing on a new corpus for Barunga Kriol, the paper presents analyses of the short and long monophthongs, as well as the diphthongs in the spontaneous speech of young adults. The results show the durations and spectral characteristics of the vowels, including major patterns of allophony (i.e. coarticulation and context effects). This updates the phonology over the previous description from the 1970s, showing that there is an additional front low vowel phoneme in the speech of young people today, as well as a vowel length contrast. Interestingly there are points of similarity with the vowel acoustics for traditional Aboriginal languages of the region, for example in a relatively compact vowel space and in the modest trajectories of diphthongs",
    "checked": true,
    "id": "5e38240b3a9425798125345ddec3c6b6a5033744",
    "semantic_title": "vowels in the barunga variety of north australian kriol",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2017/dutta17_interspeech.html": {
    "title": "Nature of Contrast and Coarticulation: Evidence from Mizo Tones and Assamese Vowel Harmony",
    "volume": "main",
    "abstract": "Tonal coarticulation is universally found to be greater in extent in the carryover direction compared to the anticipatory direction ([1], [2], [3], [4], [5]) leading to assimilatory processes. In general, carryover coarticulation has been understood to be due to intertio-mechanical forces, and, anticipatory effects are seen to be a consequence of parallel activation of articulatory plans ([6]). In this paper, we report on results from a set of Artificial Neural Networks (ANN) trained to predict adjacent tones in disyllabic sequences. Our results confirm the universal pattern of greater carryover effects in Mizo leading to tonal assimilation. In addition, we report on results from single-layered ANN models and Support Vector Machines (SVM) that predict the identity of V from V (anticipatory) consistently better than V from V (carryover) in Assamese non-harmonic #…V CV …# sequences. The directionality in the performance of the V and V models, help us conclude that the directionality effect of coarticulation in Assamese non-harmonic sequences is greater in the anticipatory direction, which is the same direction as in the harmonic sequences. We argue that coarticulatory propensity exhibits a great deal of sensitivity to the nature of contrast in a language",
    "checked": true,
    "id": "09a76f55dfd08a5515410c1ef5782ece9795f28b",
    "semantic_title": "nature of contrast and coarticulation: evidence from mizo tones and assamese vowel harmony",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2017/cabral17_interspeech.html": {
    "title": "The Influence of Synthetic Voice on the Evaluation of a Virtual Character",
    "volume": "main",
    "abstract": "Graphical realism and the naturalness of the voice used are important aspects to consider when designing a virtual agent or character. In this work, we evaluate how synthetic speech impacts people's perceptions of a rendered virtual character. Using a controlled experiment, we focus on the role that speech, in particular voice expressiveness in the form of personality, has on the assessment of voice level and character level perceptions. We found that people rated a real human voice as more expressive, understandable and likeable than the expressive synthetic voice we developed. Contrary to our expectations, we found that the voices did not have a significant impact on the character level judgments; people in the voice conditions did not significantly vary on their ratings of appeal, credibility, human-likeness and voice matching the character. The implications this has for character design and how this compares with previous work are discussed",
    "checked": true,
    "id": "ad48f676dc43d5e65035ed0d0d79f54243746e68",
    "semantic_title": "the influence of synthetic voice on the evaluation of a virtual character",
    "citation_count": 25
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gully17_interspeech.html": {
    "title": "Articulatory Text-to-Speech Synthesis Using the Digital Waveguide Mesh Driven by a Deep Neural Network",
    "volume": "main",
    "abstract": "Following recent advances in direct modeling of the speech waveform using a deep neural network, we propose a novel method that directly estimates a physical model of the vocal tract from the speech waveform, rather than magnetic resonance imaging data. This provides a clear relationship between the model and the size and shape of the vocal tract, offering considerable flexibility in terms of speech characteristics such as age and gender. Initial tests indicate that despite a highly simplified physical model, intelligible synthesized speech is obtained. This illustrates the potential of the combined technique for the control of physical models in general, and hence the generation of more natural-sounding synthetic speech",
    "checked": true,
    "id": "a09a29e6fc522420114314c024933d3dfe7345d3",
    "semantic_title": "articulatory text-to-speech synthesis using the digital waveguide mesh driven by a deep neural network",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2017/maguer17_interspeech.html": {
    "title": "An HMM/DNN Comparison for Synchronized Text-to-Speech and Tongue Motion Synthesis",
    "volume": "main",
    "abstract": "We present an end-to-end text-to-speech (TTS) synthesis system that generates audio and synchronized tongue motion directly from text. This is achieved by adapting a statistical shape space model of the tongue surface to an articulatory speech corpus and training a speech synthesis system directly on the tongue model parameter weights. We focus our analysis on the application of two standard methodologies, based on Hidden Markov Models (HMMs) and Deep Neural Networks (DNNs), respectively, to train both acoustic models and the tongue model parameter weights. We evaluate both methodologies at every step by comparing the predicted articulatory movements against the reference data. The results show that even with less than 2h of data, DNNs already outperform HMMs",
    "checked": true,
    "id": "a464a5a31c35a4cab1fc1eeed31177cdcf260b8b",
    "semantic_title": "an hmm/dnn comparison for synchronized text-to-speech and tongue motion synthesis",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2017/alexander17_interspeech.html": {
    "title": "VCV Synthesis Using Task Dynamics to Animate a Factor-Based Articulatory Model",
    "volume": "main",
    "abstract": "This paper presents an initial architecture for articulatory synthesis which combines a dynamical system for the control of vocal tract shaping with a novel MATLAB implementation of an articulatory synthesizer. The dynamical system controls a speaker-specific vocal tract model derived by factor analysis of mid-sagittal real-time MRI data and provides input to the articulatory synthesizer, which simulates the propagation of sound waves in the vocal tract. First, parameters of the dynamical system are estimated from real-time MRI data of human speech production. Second, vocal-tract dynamics is simulated for vowel-consonant-vowel utterances using a sequence of two dynamical systems: the first one starts from a vowel vocal-tract configuration and achieves a vocal-tract closure; the second one starts from the closure and achieves the target configuration of the second vowel. Third, vocal-tract dynamics is converted to area function dynamics and is input to the synthesizer to generate the acoustic signal. Synthesized vowel-consonant-vowel examples demonstrate the feasibility of the method",
    "checked": true,
    "id": "1d8e1de2cbd6b7c1e508f01d1a6a8f4bd6738556",
    "semantic_title": "vcv synthesis using task dynamics to animate a factor-based articulatory model",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mendelson17_interspeech.html": {
    "title": "Beyond the Listening Test: An Interactive Approach to TTS Evaluation",
    "volume": "main",
    "abstract": "Traditionally, subjective text-to-speech (TTS) evaluation is performed through audio-only listening tests, where participants evaluate unrelated, context-free utterances. The ecological validity of these tests is questionable, as they do not represent real-world end-use scenarios. In this paper, we examine a novel approach to TTS evaluation in an imagined end-use, via a complex interaction with an avatar. 6 different voice conditions were tested: Natural speech, Unit Selection and Parametric Synthesis, in neutral and expressive realizations. Results were compared to a traditional audio-only evaluation baseline. Participants in both studies rated the voices for naturalness and expressivity. The baseline study showed canonical results for naturalness: Natural speech scored highest, followed by Unit Selection, then Parametric synthesis. Expressivity was clearly distinguishable in all conditions. In the avatar interaction study, participants rated naturalness in the same order as the baseline, though with smaller effect size; expressivity was not distinguishable. Further, no significant correlations were found between cognitive or affective responses and any voice conditions. This highlights 2 primary challenges in designing more valid TTS evaluations: in real-world use-cases involving interaction, listeners generally interact with a single voice, making comparative analysis unfeasible, and in complex interactions, the context and content may confound perception of voice quality",
    "checked": true,
    "id": "e25869e27af783fc2943cd34586edd5aa476a4fe",
    "semantic_title": "beyond the listening test: an interactive approach to tts evaluation",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2017/cao17_interspeech.html": {
    "title": "Integrating Articulatory Information in Deep Learning-Based Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "Articulatory information has been shown to be effective in improving the performance of hidden Markov model (HMM)-based text-to-speech (TTS) synthesis. Recently, deep learning-based TTS has outperformed HMM-based approaches. However, articulatory information has rarely been integrated in deep learning-based TTS. This paper investigated the effectiveness of integrating articulatory movement data to deep learning-based TTS. The integration of articulatory information was achieved in two ways: (1) direct integration, where articulatory and acoustic features were the output of a deep neural network (DNN), and (2) direct integration plus forward-mapping, where the output articulatory features were mapped to acoustic features by an additional DNN; These forward-mapped acoustic features were then combined with the output acoustic features to produce the final acoustic features. Articulatory (tongue and lip) and acoustic data collected from male and female speakers were used in the experiment. Both objective measures and subjective judgment by human listeners showed the approaches integrated articulatory information outperformed the baseline approach (without using articulatory information) in terms of naturalness and speaker voice identity (voice similarity)",
    "checked": true,
    "id": "bd3c26cb973503743829c4276b4eb6d3582eb2e7",
    "semantic_title": "integrating articulatory information in deep learning-based text-to-speech synthesis",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ma17b_interspeech.html": {
    "title": "Approaches for Neural-Network Language Model Adaptation",
    "volume": "main",
    "abstract": "Language Models (LMs) for Automatic Speech Recognition (ASR) are typically trained on large text corpora from news articles, books and web documents. These types of corpora, however, are unlikely to match the test distribution of ASR systems, which expect spoken utterances. Therefore, the LM is typically adapted to a smaller held-out in-domain dataset that is drawn from the test distribution. We propose three LM adaptation approaches for Deep NN and Long Short-Term Memory (LSTM): (1) Adapting the softmax layer in the Neural Network (NN); (2) Adding a non-linear adaptation layer before the softmax layer that is trained only in the adaptation phase; (3) Training the extra non-linear adaptation layer in pre-training and adaptation phases. Aiming to improve upon a hierarchical Maximum Entropy (MaxEnt) second-pass LM baseline, which factors the model into word-cluster and word models, we build an NN LM that predicts only word clusters. Adapting the LSTM LM by training the adaptation layer in both training and adaptation phases (Approach 3), we reduce the cluster perplexity by 30% on a held-out dataset compared to an unadapted LSTM LM. Initial experiments using a state-of-the-art ASR system show a 2.3% relative reduction in WER on top of an adapted MaxEnt LM",
    "checked": true,
    "id": "8fb0151c211b6394fe6fda64abff62bbb0061fef",
    "semantic_title": "approaches for neural-network language model adaptation",
    "citation_count": 33
  },
  "https://www.isca-speech.org/archive/interspeech_2017/oualil17_interspeech.html": {
    "title": "A Batch Noise Contrastive Estimation Approach for Training Large Vocabulary Language Models",
    "volume": "main",
    "abstract": "Training large vocabulary Neural Network Language Models (NNLMs) is a difficult task due to the explicit requirement of the output layer normalization, which typically involves the evaluation of the full softmax function over the complete vocabulary. This paper proposes a Batch Noise Contrastive Estimation (B-NCE) approach to alleviate this problem. This is achieved by reducing the vocabulary, at each time step, to the target words in the batch and then replacing the softmax by the noise contrastive estimation approach, where these words play the role of targets and noise samples at the same time. In doing so, the proposed approach can be fully formulated and implemented using optimal dense matrix operations. Applying B-NCE to train different NNLMs on the Large Text Compression Benchmark (LTCB) and the One Billion Word Benchmark (OBWB) shows a significant reduction of the training time with no noticeable degradation of the models performance. This paper also presents a new baseline comparative study of different standard NNLMs on the large OBWB on a single Titan-X GPU",
    "checked": true,
    "id": "e2dc0acd574bfc5f956788bb517a714080c0b111",
    "semantic_title": "a batch noise contrastive estimation approach for training large vocabulary language models",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17b_interspeech.html": {
    "title": "Investigating Bidirectional Recurrent Neural Network Language Models for Speech Recognition",
    "volume": "main",
    "abstract": "Recurrent neural network language models (RNNLMs) are powerful language modeling techniques. Significant performance improvements have been reported in a range of tasks including speech recognition compared to n-gram language models. Conventional n-gram and neural network language models are trained to predict the probability of the next word given its preceding context history. In contrast, bidirectional recurrent neural network based language models consider the context from future words as well. This complicates the inference process, but has theoretical benefits for tasks such as speech recognition as additional context information can be used. However to date, very limited or no gains in speech recognition performance have been reported with this form of model. This paper examines the issues of training bidirectional recurrent neural network language models (bi-RNNLMs) for speech recognition. A bi-RNNLM probability smoothing technique is proposed, that addresses the very sharp posteriors that are often observed in these models. The performance of the bi-RNNLMs is evaluated on three speech recognition tasks: broadcast news; meeting transcription (AMI); and low-resource systems (Babel data). On all tasks gains are observed by applying the smoothing technique to the bi-RNNLM. In addition consistent performance gains can be obtained by combining bi-RNNLMs with n-gram and uni-directional RNNLMs",
    "checked": true,
    "id": "7fe37b79f80e8937ecba653b57ebc989a56b29f9",
    "semantic_title": "investigating bidirectional recurrent neural network language models for speech recognition",
    "citation_count": 41
  },
  "https://www.isca-speech.org/archive/interspeech_2017/huang17b_interspeech.html": {
    "title": "Fast Neural Network Language Model Lookups at N-Gram Speeds",
    "volume": "main",
    "abstract": "Feed forward Neural Network Language Models (NNLM) have shown consistent gains over backoff word n-gram models in a variety of tasks. However, backoff n-gram models still remain dominant in applications with real time decoding requirements as word probabilities can be computed orders of magnitude faster than the NNLM. In this paper, we present a combination of techniques that allows us to speed up the probability computation from a neural net language model to make it comparable to the word n-gram model without any approximations. We present results on state of the art systems for Broadcast news transcription and conversational speech which demonstrate the speed improvements in real time factor and probability computation while retaining the WER gains from NNLM",
    "checked": true,
    "id": "3dac8b53c0a43df06129986eed236d179f1d16a8",
    "semantic_title": "fast neural network language model lookups at n-gram speeds",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kurata17_interspeech.html": {
    "title": "Empirical Exploration of Novel Architectures and Objectives for Language Models",
    "volume": "main",
    "abstract": "While recurrent neural network language models based on Long Short Term Memory (LSTM) have shown good gains in many automatic speech recognition tasks, Convolutional Neural Network (CNN) language models are relatively new and have not been studied in-depth. In this paper we present an empirical comparison of LSTM and CNN language models on English broadcast news and various conversational telephone speech transcription tasks. We also present a new type of CNN language model that leverages dilated causal convolution to efficiently exploit long range history. We propose a novel criterion for training language models that combines word and class prediction in a multi-task learning framework. We apply this criterion to train word and character based LSTM language models and CNN language models and show that it improves performance. Our results also show that CNN and LSTM language models are complementary and can be combined to obtain further gains",
    "checked": true,
    "id": "b0bd00f3a30bc5cbcb1a277b8ecd0bb9b4919f89",
    "semantic_title": "empirical exploration of novel architectures and objectives for language models",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2017/benes17_interspeech.html": {
    "title": "Residual Memory Networks in Language Modeling: Improving the Reputation of Feed-Forward Networks",
    "volume": "main",
    "abstract": "We introduce the Residual Memory Network (RMN) architecture to language modeling. RMN is an architecture of feed-forward neural networks that incorporates residual connections and time-delay connections that allow us to naturally incorporate information from a substantial time context. As this is the first time RMNs are applied for language modeling, we thoroughly investigate their behaviour on the well studied Penn Treebank corpus. We change the model slightly for the needs of language modeling, reducing both its time and memory consumption. Our results show that RMN is a suitable choice for small-sized neural language models: With test perplexity 112.7 and as few as 2.3M parameters, they out-perform both a much larger vanilla RNN (PPL 124, 8M parameters) and a similarly sized LSTM (PPL 115, 2.08M parameters), while being only by less than 3 perplexity points worse than twice as big LSTM",
    "checked": true,
    "id": "2ee7ee38745e9fcf89860dfb3d41c2155521e3a3",
    "semantic_title": "residual memory networks in language modeling: improving the reputation of feed-forward networks",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2017/poorjam17_interspeech.html": {
    "title": "Dominant Distortion Classification for Pre-Processing of Vowels in Remote Biomedical Voice Analysis",
    "volume": "main",
    "abstract": "Advances in speech signal analysis facilitate the development of techniques for remote biomedical voice assessment. However, the performance of these techniques is affected by noise and distortion in signals. In this paper, we focus on the vowel /a/ as the most widely-used voice signal for pathological voice assessments and investigate the impact of four major types of distortion that are commonly present during recording or transmission in voice analysis, namely: background noise, reverberation, clipping and compression, on Mel-frequency cepstral coefficients (MFCCs) — the most widely-used features in biomedical voice analysis. Then, we propose a new distortion classification approach to detect the most dominant distortion in such voice signals. The proposed method involves MFCCs as frame-level features and a support vector machine as classifier to detect the presence and type of distortion in frames of a given voice signal. Experimental results obtained from the healthy and Parkinson's voices show the effectiveness of the proposed approach in distortion detection and classification",
    "checked": true,
    "id": "6003f076ac577e2cd9e22d674b90f984fa7763fe",
    "semantic_title": "dominant distortion classification for pre-processing of vowels in remote biomedical voice analysis",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2017/le17_interspeech.html": {
    "title": "Automatic Paraphasia Detection from Aphasic Speech: A Preliminary Study",
    "volume": "main",
    "abstract": "Aphasia is an acquired language disorder resulting from brain damage that can cause significant communication difficulties. Aphasic speech is often characterized by errors known as paraphasias, the analysis of which can be used to determine an appropriate course of treatment and to track an individual's recovery progress. Being able to detect paraphasias automatically has many potential clinical benefits; however, this problem has not previously been investigated in the literature. In this paper, we perform the first study on detecting phonemic and neologistic paraphasias from scripted speech samples in AphasiaBank. We propose a speech recognition system with task-specific language models to transcribe aphasic speech automatically. We investigate features based on speech duration, Goodness of Pronunciation, phone edit distance, and Dynamic Time Warping on phoneme posteriorgrams. Our results demonstrate the feasibility of automatic paraphasia detection and outline the path toward enabling this system in real-world clinical applications",
    "checked": true,
    "id": "a61f6c2cb8618fcf193f3de695f0a59cc719df9e",
    "semantic_title": "automatic paraphasia detection from aphasic speech: a preliminary study",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2017/garcia17_interspeech.html": {
    "title": "Evaluation of the Neurological State of People with Parkinson's Disease Using i-Vectors",
    "volume": "main",
    "abstract": "The i-vector approach is used to model the speech of PD patients with the aim of assessing their condition. Features related to the articulation, phonation, and prosody dimensions of speech were used to train different i-vector extractors. Each i-vector extractor is trained using utterances from both PD patients and healthy controls. The i-vectors of the healthy control (HC) speakers are averaged to form a single i-vector that represents the HC group, i.e., the reference i-vector. A similar process is done to create a reference of the group with PD patients. Then the i-vectors of test speakers are compared to these reference i-vectors using the cosine distance. Three analyses are performed using this distance: classification between PD patients and HC, prediction of the neurological state of PD patients according to the MDS-UPDRS-III scale, and prediction of a modified version of the Frenchay Dysarthria Assessment. The Spearman's correlation between this cosine distance and the MDS-UPDRS-III scale was 0.63. These results show the suitability of this approach to monitor the neurological state of people with Parkinson's Disease",
    "checked": true,
    "id": "edad4b36bf583c4949dd2f1272b143309f300bcd",
    "semantic_title": "evaluation of the neurological state of people with parkinson's disease using i-vectors",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chien17_interspeech.html": {
    "title": "Objective Severity Assessment from Disordered Voice Using Estimated Glottal Airflow",
    "volume": "main",
    "abstract": "In clinical practice, the severity of disordered voice is typically rated by a professional with auditory-perceptual judgment. The present study aims to automate this assessment procedure, in an attempt to make the assessment objective and less labor-intensive. In the automated analysis, glottal airflow is estimated from the analyzed voice signal with an inverse filtering algorithm. Automatic assessment is realized by a regressor that predicts from temporal and spectral features of the glottal airflow. A regressor trained on overtone amplitudes and harmonic richness factors extracted from a set of continuous-speech utterances was applied to a set of sustained-vowel utterances, giving severity predictions (on a scale of ratings from 0 to 100) with an average error magnitude of 14",
    "checked": true,
    "id": "eae51baf6bd12f879314b459e5d8ed502fb8e7c3",
    "semantic_title": "objective severity assessment from disordered voice using estimated glottal airflow",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2017/pokorny17_interspeech.html": {
    "title": "Earlier Identification of Children with Autism Spectrum Disorder: An Automatic Vocalisation-Based Approach",
    "volume": "main",
    "abstract": "Autism spectrum disorder (ASD) is a neurodevelopmental disorder usually diagnosed in or beyond toddlerhood. ASD is defined by repetitive and restricted behaviours, and deficits in social communication. The early speech-language development of individuals with ASD has been characterised as delayed. However, little is known about ASD-related characteristics of pre-linguistic vocalisations at the feature level. In this study, we examined pre-linguistic vocalisations of 10-month-old individuals later diagnosed with ASD and a matched control group of typically developing individuals (N = 20). We segmented 684 vocalisations from parent-child interaction recordings. All vocalisations were annotated and signal-analytically decomposed. We analysed ASD-related vocalisation specificities on the basis of a standardised set (eGeMAPS) of 88 acoustic features selected for clinical speech analysis applications. 54 features showed evidence for a differentiation between vocalisations of individuals later diagnosed with ASD and controls. In addition, we evaluated the feasibility of automated, vocalisation-based identification of individuals later diagnosed with ASD. We compared linear kernel support vector machines and a 1-layer bidirectional long short-term memory neural network. Both classification approaches achieved an accuracy of 75% for subject-wise identification in a subject-independent 3-fold cross-validation scheme. Our promising results may be an important contribution en-route to facilitate earlier identification of ASD",
    "checked": true,
    "id": "65616754cbd08935c0f17c2f34258670b61a5fc9",
    "semantic_title": "earlier identification of children with autism spectrum disorder: an automatic vocalisation-based approach",
    "citation_count": 40
  },
  "https://www.isca-speech.org/archive/interspeech_2017/vasquezcorrea17_interspeech.html": {
    "title": "Convolutional Neural Network to Model Articulation Impairments in Patients with Parkinson's Disease",
    "volume": "main",
    "abstract": "Speech impairments are one of the earliest manifestations in patients with Parkinson's disease. Particularly, articulation deficits related to the capability of the speaker to start/stop the vibration of the vocal folds have been observed in the patients. Those difficulties can be assessed by modeling the transitions between voiced and unvoiced segments from speech. A robust strategy to model the articulatory deficits related to the starting or stopping vibration of the vocal folds is proposed in this study. The transitions between voiced and unvoiced segments are modeled by a convolutional neural network that extracts suitable information from two time-frequency representations: the short time Fourier transform and the continuous wavelet transform. The proposed approach improves the results previously reported in the literature. Accuracies of up to 89% are obtained for the classification of Parkinson's patients vs. healthy speakers. This study is a step towards the robust modeling of the speech impairments in patients with neuro-degenerative disorders",
    "checked": true,
    "id": "1b316a46a9d0cd0cf8edd44367a888fd71429ce9",
    "semantic_title": "convolutional neural network to model articulation impairments in patients with parkinson's disease",
    "citation_count": 47
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bai17_interspeech.html": {
    "title": "Phone Classification Using a Non-Linear Manifold with Broad Phone Class Dependent DNNs",
    "volume": "main",
    "abstract": "Most state-of-the-art automatic speech recognition (ASR) systems use a single deep neural network (DNN) to map the acoustic space to the decision space. However, different phonetic classes employ different production mechanisms and are best described by different types of features. Hence it may be advantageous to replace this single DNN with several phone class dependent DNNs. The appropriate mathematical formalism for this is a manifold. This paper assesses the use of a non-linear manifold structure with multiple DNNs for phone classification. The system has two levels. The first comprises a set of broad phone class (BPC) dependent DNN-based mappings and the second level is a fusion network. Various ways of designing and training the networks in both levels are assessed, including varying the size of hidden layers, the use of the bottleneck or softmax outputs as input to the fusion network, and the use of different broad class definitions. Phone classification experiments are performed on TIMIT. The results show that using the BPC-dependent DNNs provides small but significant improvements in phone classification accuracy relative to a single global DNN. The paper concludes with visualisations of the structures learned by the local and global DNNs and discussion of their interpretations",
    "checked": true,
    "id": "2537e647d4ca5017451805e31596c96b6de535cb",
    "semantic_title": "phone classification using a non-linear manifold with broad phone class dependent dnns",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17c_interspeech.html": {
    "title": "An Investigation of Crowd Speech for Room Occupancy Estimation",
    "volume": "main",
    "abstract": "Room occupancy estimation technology has been shown to reduce building energy cost significantly. However speech-based occupancy estimation has not been well explored. In this paper, we investigate energy mode and babble speaker count methods for estimating both small and large crowds in a party-mode room setting. We also examine how distance between speakers and microphone affects their estimation accuracies. Then we propose a novel entropy-based method, which is invariant to different speakers and their different positions in a room. Evaluations on synthetic crowd speech generated using the TIMIT corpus show that acoustic volume features are less affected by distance, and our proposed method outperforms existing methods across a range of different conditions",
    "checked": true,
    "id": "daf4097464bf07d4e53ad21a7a3a5cb3adba7a29",
    "semantic_title": "an investigation of crowd speech for room occupancy estimation",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2017/vijayan17_interspeech.html": {
    "title": "Time-Frequency Coherence for Periodic-Aperiodic Decomposition of Speech Signals",
    "volume": "main",
    "abstract": "Decomposing speech signals into periodic and aperiodic components is an important task, finding applications in speech synthesis, coding, denoising, etc. In this paper, we construct a time-frequency coherence function to analyze spectro-temporal signatures of speech signals for distinguishing between deterministic and stochastic components of speech. The narrowband speech spectrogram is segmented into patches, which are represented as 2-D cosine carriers modulated in amplitude and frequency. Separation of carrier and amplitude/frequency modulations is achieved by 2-D demodulation using Riesz transform, which is the 2-D extension of Hilbert transform. The demodulated AM component reflects contributions of the vocal tract to spectrogram. The frequency modulated carrier (FM-carrier) signal exhibits properties of the excitation. The time-frequency coherence is defined with respect to FM-carrier and a coherence map is constructed, in which highly coherent regions represent nearly periodic and deterministic components of speech, whereas the incoherent regions correspond to unstructured components. The coherence map shows a clear distinction between deterministic and stochastic components in speech characterized by jitter, shimmer, lip radiation, type of excitation, etc. Binary masks prepared from the time-frequency coherence function are used for periodic-aperiodic decomposition of speech. Experimental results are presented to validate the efficiency of the proposed method",
    "checked": true,
    "id": "93b97386a78e815f4612aa8378784dd9c442e59d",
    "semantic_title": "time-frequency coherence for periodic-aperiodic decomposition of speech signals",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2017/meireles17_interspeech.html": {
    "title": "Musical Speech: A New Methodology for Transcribing Speech Prosody",
    "volume": "main",
    "abstract": "Musical Speech is a new methodology for transcribing speech prosody using musical notation. The methodology presented in this paper is an updated version of our work [12]. Our work is situated in a historical context with a brief survey of the literature of speech melodies, in which we highlight the pioneering works of John Steele, Leoš Janávcek, Engelbert Humperdinck, and Arnold Schoenberg, followed by a linguistic view of musical notation in the analysis of speech. Finally, we present the current state-of-the-art of our innovative methodology that uses a quarter-tone scale for transcribing speech, and shows some initial results of the application of this methodology to prosodic transcription",
    "checked": true,
    "id": "3ef82fe149a24e1406e354c8b0b4ad35579853a2",
    "semantic_title": "musical speech: a new methodology for transcribing speech prosody",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2017/nataraj17_interspeech.html": {
    "title": "Estimation of Place of Articulation of Fricatives from Spectral Characteristics for Speech Training",
    "volume": "main",
    "abstract": "A visual feedback of the place of articulation is considered to be useful for speech training aids for hearing-impaired children and for learners of second languages in helping them in improving pronunciation. For such applications, the relation between place of articulation of fricatives and their spectral characteristics is investigated using English fricatives available in the XRMB database, which provides simultaneously acquired speech signal and articulogram. Place of articulation is estimated from the articulogram as the position of maximum constriction in the oral cavity, using an automated graphical technique. The magnitude spectrum is smoothed by critical band based median and mean filters for improving the consistency of the spectral parameters. Out of several spectral parameters investigated, spectral moments and spectral slope appear to be related to the place of articulation of the fricative segment of the utterances as measured from articulogram. The data are used to train and test a Gaussian mixture model to estimate the place of articulation with spectral parameters as the inputs. The estimated values showed a good match with those obtained from the articulograms",
    "checked": true,
    "id": "a4ff3e50abe2dc21fc1fe30acbfc5ab2fc744938",
    "semantic_title": "estimation of place of articulation of fricatives from spectral characteristics for speech training",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2017/backstrom17_interspeech.html": {
    "title": "Estimation of the Probability Distribution of Spectral Fine Structure in the Speech Source",
    "volume": "main",
    "abstract": "The efficiency of many speech processing methods rely on accurate modeling of the distribution of the signal spectrum and a majority of prior works suggest that the spectral components follow the Laplace distribution. To improve the probability distribution models based on our knowledge of speech source modeling, we argue that the model should in fact be a multiplicative mixture model, including terms for voiced and unvoiced utterances. While prior works have applied Gaussian mixture models, we demonstrate that a mixture of generalized Gaussian models more accurately follows the observations. The proposed estimation method is based on measuring the ratio of L -norms between spectral bands. Such ratios follow the Beta-distribution when the input signal is generalized Gaussian, whereby the estimated parameters can be used to determine the underlying parameters of the mixture of generalized Gaussian distributions",
    "checked": true,
    "id": "c3a964d36176261d54457b62e46c01d69733f0d1",
    "semantic_title": "estimation of the probability distribution of spectral fine structure in the speech source",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ghosh17_interspeech.html": {
    "title": "End-to-End Acoustic Feedback in Language Learning for Correcting Devoiced French Final-Fricatives",
    "volume": "main",
    "abstract": "This work aims at providing an end-to-end acoustic feedback framework to help learners of French to pronounce voiced fricatives. A classifier ensemble detects voiced/unvoiced utterances, then a correction method is proposed to improve the perception and production of voiced fricatives in a word-final position. Realizations of voiced fricatives contained in French sentences uttered by French and German speakers were analyzed to find out the deviations between the acoustic cues realized by the two groups of speakers. The correction method consists in substituting the erroneous devoiced fricative by TD-PSOLA concatenative synthesis that uses exemplars of voiced fricatives chosen from a French speaker corpus. To achieve a seamless concatenation the energy of the replacement fricative was adjusted with respect to the energy levels of the learner's and French speaker's preceding vowels. Finally, a perception experiment with the corrected stimuli has been carried out with French native speakers to check the appropriateness of the fricative revoicing. The results showed that the proposed revoicing strategy proved to be very efficient and can be used as an acoustic feedback",
    "checked": true,
    "id": "b2cf454158654121cce523b2620286aef81b47e1",
    "semantic_title": "end-to-end acoustic feedback in language learning for correcting devoiced french final-fricatives",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jacewicz17_interspeech.html": {
    "title": "Dialect Perception by Older Children",
    "volume": "main",
    "abstract": "The acquisition of regional dialect variation is an inherent part of the language learning process that takes place in the specific environments in which the child participates. This study examined dialect perception by 9–12-year-olds who grew up in two very diverse dialect regions in the United States, Western North Carolina (NC) and Southeastern Wisconsin (WI). In a dialect identification task, each group of children responded to 120 talkers from the same dialects representing three generations, ranging in age from old adults to children. There was a robust discrepancy in the children's dialect identification performance: WI children were able to identify talker dialect quite well (although still not as well as the adults) whereas NC children were at chance level. WI children were also more sensitive to cross-generational changes in both dialects as a function of diachronic sound change. It is concluded that both groups of children demonstrated their sociolinguistic awareness in very different ways, corresponding to relatively stable (WI) and changing (NC) socio-cultural environments in their respective speech communities",
    "checked": true,
    "id": "3399778ddace86ff24e28c9996188a90e7daf7c8",
    "semantic_title": "dialect perception by older children",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2017/yoneyama17_interspeech.html": {
    "title": "Perception of Non-Contrastive Variations in American English by Japanese Learners: Flaps are Less Favored Than Stops",
    "volume": "main",
    "abstract": "Alveolar flaps are non-contrastive allophonic variants of alveolar stops in American English. A lexical decision experiment was conducted with Japanese learners of English (JE) to investigate whether second-language (L2) learners are sensitive to such allophonic variations when recognizing words in L2. The stimuli consisted of 36 isolated bisyllabic English words containing word-medial /t/, half of which were flap-favored words, e.g. city, and the other half were [t]-favored words, e.g. faster. All stimuli were recorded with two surface forms: /t/ as a flap, e.g. city with a flap, or as [t], e.g. city with [t]. The stimuli were counterbalanced so that participants only heard one of the two surface forms of each word. The accuracy data indicated that flap-favored words pronounced with a flap, e.g. city with a flap, were recognized significantly less accurately than flap-favored words with [t], e.g. city with [t], and [t]-favored words with [t], e.g. faster with [t]. These results suggest that JE learners prefer canonical forms over frequent forms produced with context-dependent allophonic variations. These results are inconsistent with previous studies that found native speakers' preference for frequent forms, and highlight differences in the effect of allophonic variations on the perception of native-language and L2 speech",
    "checked": true,
    "id": "c102db8f188e0c90f09e9544a237411f06910a23",
    "semantic_title": "perception of non-contrastive variations in american english by japanese learners: flaps are less favored than stops",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2017/maastricht17_interspeech.html": {
    "title": "L1 Perceptions of L2 Prosody: The Interplay Between Intonation, Rhythm, and Speech Rate and Their Contribution to Accentedness and Comprehensibility",
    "volume": "main",
    "abstract": "This study investigates the cumulative effect of (non-)native intonation, rhythm, and speech rate in utterances produced by Spanish learners of Dutch on Dutch native listeners' perceptions. In order to assess the relative contribution of these language-specific properties to perceived accentedness and comprehensibility, speech produced by Spanish learners of Dutch was manipulated using transplantation and resynthesis techniques. Thus, eight manipulation conditions reflecting all possible combinations of L1 and L2 intonation, rhythm, and speech rate were created, resulting in 320 utterances that were rated by 50 Dutch natives on their degree of foreign accent and ease of comprehensibility Our analyses show that all manipulations result in lower accentedness and higher comprehensibility ratings. Moreover, both measures are not affected in the same way by different combinations of prosodic features: For accentedness, Dutch listeners appear most influenced by intonation, and intonation combined with speech rate. This holds for comprehensibility ratings as well, but here the combination of all three properties, including rhythm, also significantly affects ratings by native speakers. Thus, our study reaffirms the importance of differentiating between different aspects of perception and provides insight into those features that are most likely to affect how native speakers perceive second language learners",
    "checked": true,
    "id": "a928e082af0d19e51b51ea9b5688d0d4b87477c9",
    "semantic_title": "l1 perceptions of l2 prosody: the interplay between intonation, rhythm, and speech rate and their contribution to accentedness and comprehensibility",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2017/takiguchi17_interspeech.html": {
    "title": "Effects of Pitch Fall and L1 on Vowel Length Identification in L2 Japanese",
    "volume": "main",
    "abstract": "This study investigated whether and how the role of pitch fall in the first language (L1) interacts with its use as a cue for Japanese phonological vowel length in the second language (L2). Native listeners of Japanese (NJ) and L2 learners of Japanese with L1 backgrounds in Mandarin Chinese (NC), Seoul Korean (NK), American English (NE), and French (NFr) participated in a perception experiment. The results showed that the proportion of \"long\" responses increased as a function of vowel duration for all groups, giving s-shaped curves. Meanwhile, the presence or absence of a pitch fall within a syllable affected only NJ and NC's perception. Their category boundary occurred at a shorter duration for vowels with a pitch fall than without a pitch fall. Among the four groups of L2 learners, only NC use pitch fall to distinguish words in the L1. Thus, it is possible to think that the role of pitch fall as an L1 cue relates to its use as a cue for L2 length identification. L2 learners tend to attend to an important phonetic feature as a cue for perceiving an L1 category differentiating L1 words even in the L2 as implied by the Feature Hypothesis",
    "checked": true,
    "id": "170d7b909363e5c47c7255b13e940ce6a618fcdb",
    "semantic_title": "effects of pitch fall and l1 on vowel length identification in l2 japanese",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhang17_interspeech.html": {
    "title": "A Preliminary Study of Prosodic Disambiguation by Chinese EFL Learners",
    "volume": "main",
    "abstract": "This study investigated whether Chinese learners of English as a foreign language (EFL learners hereafter) could use prosodic cues to resolve syntactically ambiguous sentences in English. 8 sentences with 3 types of syntactic ambiguity were adopted. They were far/near PP attachment, left/right word attachment and wide/narrow scope. In the production experiment, 15 Chinese college students who passed the annual national examination CET (College English Test) Band 4 and 5 native English speakers from America were recruited. They were asked to read the 8 target sentences after hearing the contexts spoken by a Native American speaker, which clarified the intended meaning of the ambiguous sentences. The preliminary results showed that, as the native speakers did, Chinese EFL learners employed different durational patterns to express the alternative meanings of the ambiguous sentences by altering prosodic phrasing. That is, the duration of the pre-boundary items were lengthened and pause were inserted at the boundary. But the perception experiment showed that the utterances produced by Chinese EFL learners couldn't be effectively perceived by the native speakers due to their different use of pre-boundary lengthening and pause. The conclusion is that Chinese EFL learners find prosodic disambiguation difficult",
    "checked": true,
    "id": "e30be62b4da615745ef3c9c2cccf772267901c96",
    "semantic_title": "a preliminary study of prosodic disambiguation by chinese efl learners",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kim17_interspeech.html": {
    "title": "Generation of Large-Scale Simulated Utterances in Virtual Rooms to Train Deep-Neural Networks for Far-Field Speech Recognition in Google Home",
    "volume": "main",
    "abstract": "We describe the structure and application of an acoustic room simulator to generate large-scale simulated data for training deep neural networks for far-field speech recognition. The system simulates millions of different room dimensions, a wide distribution of reverberation time and signal-to-noise ratios, and a range of microphone and sound source locations. We start with a relatively clean training set as the source and artificially create simulated data by randomly sampling a noise configuration for every new training example. As a result, the acoustic model is trained using examples that are virtually never repeated. We evaluate performance of this approach based on room simulation using a factored complex Fast Fourier Transform (CFFT) acoustic model introduced in our earlier work, which uses CFFT layers and LSTM AMs for joint multichannel processing and acoustic modeling. Results show that the simulator-driven approach is quite effective in obtaining large improvements not only in simulated test conditions, but also in real / rerecorded conditions. This room simulation system has been employed in training acoustic models including the ones for the recently released Google Home",
    "checked": true,
    "id": "49636d64a097f708ac131eb24c46719dfcd6d6b2",
    "semantic_title": "generation of large-scale simulated utterances in virtual rooms to train deep-neural networks for far-field speech recognition in google home",
    "citation_count": 217
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kinoshita17_interspeech.html": {
    "title": "Neural Network-Based Spectrum Estimation for Online WPE Dereverberation",
    "volume": "main",
    "abstract": "In this paper, we propose a novel speech dereverberation framework that utilizes deep neural network (DNN)-based spectrum estimation to construct linear inverse filters. The proposed dereverberation framework is based on the state-of-the-art inverse filter estimation algorithm called weighted prediction error (WPE) algorithm, which is known to effectively reduce reverberation and greatly boost the ASR performance in various conditions. In WPE, the accuracy of the inverse filter estimation, and thus the dereverberation performance, is largely dependent on the estimation of the power spectral density (PSD) of the target signal. Therefore, the conventional WPE iteratively performs the inverse filter estimation, actual dereverberation and the PSD estimation to gradually improve the PSD estimate. However, while such iterative procedure works well when sufficiently long acoustically-stationary observed signals are available, WPE's performance degrades when the duration of observed/accessible data is short, which typically is the case for real-time applications using online block-batch processing with small batches. To solve this problem, we incorporate the DNN-based spectrum estimator into the framework of WPE, because a DNN can estimate the PSD robustly even from very short observed data. We experimentally show that the proposed framework outperforms the conventional WPE, and improves the ASR performance in real noisy reverberant environments in both single-channel and multichannel cases",
    "checked": true,
    "id": "2b5fe0dceaf84e0ad0d2f06ce4a9865f065a6f63",
    "semantic_title": "neural network-based spectrum estimation for online wpe dereverberation",
    "citation_count": 81
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ichikawa17_interspeech.html": {
    "title": "Factorial Modeling for Effective Suppression of Directional Noise",
    "volume": "main",
    "abstract": "The assumed scenario is transcription of a face-to-face conversation, such as in the financial industry when an agent and a customer talk over a desk with microphones placed between the speakers and then it is transcribed. From the automatic speech recognition (ASR) perspective, one of the speakers is the target speaker, and the other speaker is a directional noise source. When the number of microphones is small, we often accept microphone intervals that are larger than the spatial aliasing limit because the performance of the beamformer is better. Unfortunately, such a configuration results in significant leakage of directional noise in certain frequency bands because the spatial aliasing makes the beamformer and post-filter inaccurate there. Thus, we introduce a factorial model to compensate only the degraded bands with information from the reliable bands in a probabilistic framework integrating our proposed metrics and speech model. In our experiments, the proposed method reduced the errors from 29.8% to 24.9%",
    "checked": true,
    "id": "58084137d1a8227433f9f113fb57cefd1262f839",
    "semantic_title": "factorial modeling for effective suppression of directional noise",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tu17_interspeech.html": {
    "title": "On Design of Robust Deep Models for CHiME-4 Multi-Channel Speech Recognition with Multiple Configurations of Array Microphones",
    "volume": "main",
    "abstract": "We design a novel deep learning framework for multi-channel speech recognition in two aspects. First, for the front-end, an iterative mask estimation (IME) approach based on deep learning is presented to improve the beamforming approach based on the conventional complex Gaussian mixture model (CGMM). Second, for the back-end, deep convolutional neural networks (DCNNs), with augmentation of both noisy and beamformed training data, are adopted for acoustic modeling while the forward and backward long short-term memory recurrent neural networks (LSTM-RNNs) are used for language modeling. The proposed framework can be quite effective to multi-channel speech recognition with random combinations of fixed microphones. Testing on the CHiME-4 Challenge speech recognition task with a single set of acoustic and language models, our approach achieves the best performance of all three tracks (1-channel, 2-channel, and 6-channel) among submitted systems",
    "checked": true,
    "id": "f8e7eed4f75a5ea89a9d7659fb0b784cf8f5ffd8",
    "semantic_title": "on design of robust deep models for chime-4 multi-channel speech recognition with multiple configurations of array microphones",
    "citation_count": 24
  },
  "https://www.isca-speech.org/archive/interspeech_2017/li17c_interspeech.html": {
    "title": "Acoustic Modeling for Google Home",
    "volume": "main",
    "abstract": "This paper describes the technical and system building advances made to the Google Home multichannel speech recognition system, which was launched in November 2016. Technical advances include an adaptive dereverberation frontend, the use of neural network models that do multichannel processing jointly with acoustic modeling, and Grid-LSTMs to model frequency variations. On the system level, improvements include adapting the model using Google Home specific data. We present results on a variety of multichannel sets. The combination of technical and system advances result in a reduction of WER of 8–28% relative compared to the current production system",
    "checked": true,
    "id": "13e30c5dccae82477ee5d38e4d9c96b504a13d29",
    "semantic_title": "acoustic modeling for google home",
    "citation_count": 147
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mirsamadi17_interspeech.html": {
    "title": "On Multi-Domain Training and Adaptation of End-to-End RNN Acoustic Models for Distant Speech Recognition",
    "volume": "main",
    "abstract": "Recognition of distant (far-field) speech is a challenge for ASR due to mismatch in recording conditions resulting from room reverberation and environment noise. Given the remarkable learning capacity of deep neural networks, there is increasing interest to address this problem by using a large corpus of reverberant far-field speech to train robust models. In this study, we explore how an end-to-end RNN acoustic model trained on speech from different rooms and acoustic conditions (different domains) achieves robustness to environmental variations. It is shown that the first hidden layer acts as a domain separator, projecting the data from different domains into different subspaces. The subsequent layers then use this encoded domain knowledge to map these features to final representations that are invariant to domain change. This mechanism is closely related to noise-aware or room-aware approaches which append manually-extracted domain signatures to the input features. Additionally, we demonstrate how this understanding of the learning procedure provides useful guidance for model adaptation to new acoustic conditions. We present results based on AMI corpus to demonstrate the propagation of domain information in a deep RNN, and perform recognition experiments which indicate the role of encoded domain knowledge on training and adaptation of RNN acoustic models",
    "checked": true,
    "id": "691e9714cca248f95f967d39328b23fcdaf8c040",
    "semantic_title": "on multi-domain training and adaptation of end-to-end rnn acoustic models for distant speech recognition",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2017/morise17_interspeech.html": {
    "title": "Low-Dimensional Representation of Spectral Envelope Without Deterioration for Full-Band Speech Analysis/Synthesis System",
    "volume": "main",
    "abstract": "A speech coding for a full-band speech analysis/synthesis system is described. In this work, full-band speech is defined as speech with a sampling frequency above 40 kHz, whose Nyquist frequency covers the audible frequency range. In prior works, speech coding has generally focused on the narrow-band speech with a sampling frequency below 16 kHz. On the other hand, statistical parametric speech synthesis currently uses the full-band speech, and low-dimensional representation of speech parameters is being used. The purpose of this study is to achieve speech coding without deterioration for full-band speech. We focus on a high-quality speech analysis/synthesis system and mel-cepstral analysis using frequency warping. In the frequency warping function, we directly use three auditory scales. We carried out a subjective evaluation using the WORLD vocoder and found that the optimum number of dimensions was around 50. The kind of frequency warping did not significantly affect the sound quality in the dimensions",
    "checked": true,
    "id": "ce1e8d04fdff25e91399e2fbb8bc8159dd1ea58a",
    "semantic_title": "low-dimensional representation of spectral envelope without deterioration for full-band speech analysis/synthesis system",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2017/loweimi17_interspeech.html": {
    "title": "Robust Source-Filter Separation of Speech Signal in the Phase Domain",
    "volume": "main",
    "abstract": "In earlier work we proposed a framework for speech source-filter separation that employs phase-based signal processing. This paper presents a further theoretical investigation of the model and optimisations that make the filter and source representations less sensitive to the effects of noise and better matched to downstream processing. To this end, first, in computing the Hilbert transform, the log function is replaced by the generalised logarithmic function. This introduces a tuning parameter that adjusts both the dynamic range and distribution of the phase-based representation. Second, when computing the group delay, a more robust estimate for the derivative is formed by applying a regression filter instead of using sample differences. The effectiveness of these modifications is evaluated in clean and noisy conditions by considering the accuracy of the fundamental frequency extracted from the estimated source, and the performance of speech recognition features extracted from the estimated filter. In particular, the proposed filter-based front-end reduces Aurora-2 WERs by 6.3% (average 0–20 dB) compared with previously reported results. Furthermore, when tested in a LVCSR task (Aurora-4) the new features resulted in 5.8% absolute WER reduction compared to MFCCs without performance loss in the clean/matched condition",
    "checked": false,
    "id": "d73e9b415841f4eeb43a66a031027a30b5312312",
    "semantic_title": "source-filter separation of speech signal in the phase domain",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2017/stone17_interspeech.html": {
    "title": "A Time-Warping Pitch Tracking Algorithm Considering Fast f0 Changes",
    "volume": "main",
    "abstract": "Accurately tracking the fundamental frequency (f ) or pitch in speech data is of great interest in numerous contexts. All currently available pitch tracking algorithms perform a short-term analysis of a speech signal to extract the f under the assumption that the pitch does not change within a single analysis frame, a simplification that introduces errors when the f changes rather quickly over time. This paper proposes a new algorithm that warps the time axis of an analysis frame to counteract intra-frame f changes and thus to improve the total tracking results. The algorithm was evaluated on a set of 4718 sentences from 20 speakers (10 male, 10 female) and with added white and babble noise. It was comparative in performance to the state-of-the-art algorithms RAPT and PRAAT to Pitch (ac) under clean conditions and outperformed both of them under noisy conditions",
    "checked": true,
    "id": "35cabd729c7aad08d2a591e767cbe2555642af76",
    "semantic_title": "a time-warping pitch tracking algorithm considering fast f0 changes",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kawahara17_interspeech.html": {
    "title": "A Modulation Property of Time-Frequency Derivatives of Filtered Phase and its Application to Aperiodicity and fo Estimation",
    "volume": "main",
    "abstract": "We introduce a simple and linear SNR (strictly speaking, periodic to random power ratio) estimator (0 dB to 80 dB without additional calibration/linearization) for providing reliable descriptions of aperiodicity in speech corpus. The main idea of this method is to estimate the background random noise level without directly extracting the background noise. The proposed method is applicable to a wide variety of time windowing functions with very low sidelobe levels. The estimate combines the frequency derivative and the time-frequency derivative of the mapping from filter center frequency to the output instantaneous frequency. This procedure can replace the periodicity detection and aperiodicity estimation subsystems of recently introduced open source vocoder, YANG vocoder. Source code of MATLAB implementation of this method will also be open sourced",
    "checked": true,
    "id": "dfad99fe6d9bbba16d089a0c8779894b3d749446",
    "semantic_title": "a modulation property of time-frequency derivatives of filtered phase and its application to aperiodicity and fo estimation",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kumar17_interspeech.html": {
    "title": "Non-Local Estimation of Speech Signal for Vowel Onset Point Detection in Varied Environments",
    "volume": "main",
    "abstract": "Vowel onset point (VOP) is an important information extensively employed in speech analysis and synthesis. Detecting the VOPs in a given speech sequence, independent of the text contexts and recording environments, is a challenging area of research. Performance of existing VOP detection methods have not yet been extensively studied in varied environmental conditions. In this paper, we have exploited the non-local means estimation to detect those regions in the speech sequence which are of high signal-to-noise ratio and exhibit periodicity. Mostly, those regions happen to be the vowel regions. This helps in overcoming the ill-effects of environmental degradations. Next, for each short-time frame of estimated speech sequence, we cumulatively sum the magnitude of the corresponding Fourier transform spectrum. The cumulative sum is then used as the feature to detect the VOPs. The experiments conducted on TIMIT database show that the proposed approach provides better results in terms of detection and spurious rate when compared to a few existing methods under clean and noisy test conditions",
    "checked": true,
    "id": "7de52c37d55db5532b31a336cea3149315d48024",
    "semantic_title": "non-local estimation of speech signal for vowel onset point detection in varied environments",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2017/alradhi17_interspeech.html": {
    "title": "Time-Domain Envelope Modulating the Noise Component of Excitation in a Continuous Residual-Based Vocoder for Statistical Parametric Speech Synthesis",
    "volume": "main",
    "abstract": "In this paper, we present an extension of a novel continuous residual-based vocoder for statistical parametric speech synthesis. Previous work has shown the advantages of adding envelope modulated noise to the voiced excitation, but this has not been investigated yet in the context of continuous vocoders, i.e. of which all parameters are continuous. The noise component is often not accurately modeled in modern vocoders (e.g. STRAIGHT). For more natural sounding speech synthesis, four time-domain envelopes (Amplitude, Hilbert, Triangular and True) are investigated and enhanced, and then applied to the noise component of the excitation in our continuous vocoder. The performance evaluation is based on the study of time envelopes. In an objective experiment, we investigated the Phase Distortion Deviation of vocoded samples. A MUSHRA type subjective listening test was also conducted comparing natural and vocoded speech samples. Both experiments have shown that the proposed framework using Hilbert and True envelopes provides high-quality vocoding while outperforming the two other envelopes",
    "checked": true,
    "id": "aac5326c98894ef028f2ec2bb8f6383f83c91795",
    "semantic_title": "time-domain envelope modulating the noise component of excitation in a continuous residual-based vocoder for statistical parametric speech synthesis",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wu17b_interspeech.html": {
    "title": "Wavelet Speech Enhancement Based on Robust Principal Component Analysis",
    "volume": "main",
    "abstract": "Most state-of-the-art speech enhancement (SE) techniques prefer to enhance utterances in the frequency domain rather than in the time domain. However, the overlap-add (OLA) operation in the short-time Fourier transform (STFT) for speech signal processing possibly distorts the signal and limits the performance of the SE techniques. In this study, a novel SE method that integrates the discrete wavelet packet transform (DWPT) and a novel subspace-based method, robust principal component analysis (RPCA), is proposed to enhance noise-corrupted signals directly in the time domain. We evaluate the proposed SE method on the Mandarin hearing in noise test (MHINT) sentences. The experimental results show that the new method reduces the signal distortions dramatically, thereby improving speech quality and intelligibility significantly. In addition, the newly proposed method outperforms the STFT-RPCA-based speech enhancement system",
    "checked": true,
    "id": "2f90d9735de7025cbc19815aacaa87fa5a75817e",
    "semantic_title": "wavelet speech enhancement based on robust principal component analysis",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sharma17_interspeech.html": {
    "title": "Vowel Onset Point Detection Using Sonority Information",
    "volume": "main",
    "abstract": "Vowel onset point (VOP) refers to the starting event of a vowel, that may be reflected in different aspects of the speech signal. The major issue in VOP detection using existing methods is the confusion among the vowels and other categories of sounds preceding them. This work explores the usefulness of sonority information to reduce this confusion and improve VOP detection. Vowels are the most sonorant sounds followed by semivowels, nasals, voiced fricatives, voiced stops. The sonority feature is derived from the vocal-tract system, excitation source and suprasegmental aspects. As this feature has the capability to discriminate among different sonorant sound units, it reduces the confusion among onset of vowels with that of other sonorant sounds. This results in improved detection and resolution of VOP detection for continuous speech. The performance of proposed sonority information based VOP detection is found to be 92.4%, compared to 85.2% by the existing method. Also the resolution of localizing VOP within 10 ms is significantly enhanced and a performance of 73.0% is achieved as opposed to 60.2% by the existing method",
    "checked": true,
    "id": "38274f465e50f0e74e38bbc2aad51af587096590",
    "semantic_title": "vowel onset point detection using sonority information",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2017/laine17_interspeech.html": {
    "title": "Analytic Filter Bank for Speech Analysis, Feature Extraction and Perceptual Studies",
    "volume": "main",
    "abstract": "Speech signal consists of events in time and frequency, and therefore its analysis with high-resolution time-frequency tools is often of importance. Analytic filter bank provides a simple, fast, and flexible method to construct time-frequency representations of signals. Its parameters can be easily adapted to different situations from uniform to any auditory frequency scale, or even to a focused resolution. Since the Hilbert magnitude values of the channels are obtained at every sample, it provides a practical tool for a high-resolution time-frequency analysis The present study describes the basic theory of analytic filters and tests their main properties. Applications of analytic filter bank to different speech analysis tasks including pitch period estimation and pitch synchronous analysis of formant frequencies and bandwidths are demonstrated. In addition, a new feature vector called group delay vector is introduced. It is shown that this representation provides comparable, or even better results, than those obtained by spectral magnitude feature vectors in the analysis and classification of vowels. The implications of this observation are discussed also from the speech perception point of view",
    "checked": true,
    "id": "800f5f51e48c9ecce30239b264884dd99715c401",
    "semantic_title": "analytic filter bank for speech analysis, feature extraction and perceptual studies",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kroos17_interspeech.html": {
    "title": "Learning the Mapping Function from Voltage Amplitudes to Sensor Positions in 3D-EMA Using Deep Neural Networks",
    "volume": "main",
    "abstract": "The first generation of three-dimensional Electromagnetic Articulography devices (Carstens AG500) suffered from occasional critical tracking failures. Although now superseded by new devices, the AG500 is still in use in many speech labs and many valuable data sets exist. In this study we investigate whether deep neural networks (DNNs) can learn the mapping function from raw voltage amplitudes to sensor positions based on a comprehensive movement data set. This is compared to arriving sample by sample at individual position values via direct optimisation as used in previous methods. We found that with appropriate hyperparameter settings a DNN was able to approximate the mapping function with good accuracy, leading to a smaller error than the previous methods, but that the DNN-based approach was not able to solve the tracking problem completely",
    "checked": true,
    "id": "63629b14ca0c4af978ef9b4df2fa32f90e2291ec",
    "semantic_title": "learning the mapping function from voltage amplitudes to sensor positions in 3d-ema using deep neural networks",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/dai17_interspeech.html": {
    "title": "Multilingual i-Vector Based Statistical Modeling for Music Genre Classification",
    "volume": "main",
    "abstract": "For music signal processing, compared with the strategy which models each short-time frame independently, when the long-time features are considered, the time-series characteristics of the music signal can be better presented. As a typical kind of long-time modeling strategy, the identification vector (i-vector) uses statistical modeling to model the audio signal in the segment level. It can better capture the important elements of the music signal, and these important elements may benefit to the classification of music signal. In this paper, the i-vector based statistical feature for music genre classification is explored. In addition to learn enough important elements for music signal, a new multilingual i-vector feature is proposed based on the multilingual model. The experimental results show that the multilingual i-vector based models can achieve better classification performances than conventional short-time modeling based methods",
    "checked": true,
    "id": "8c5c6522778d906045da87fc14da89515056ae28",
    "semantic_title": "multilingual i-vector based statistical modeling for music genre classification",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2017/khonglah17_interspeech.html": {
    "title": "Indoor/Outdoor Audio Classification Using Foreground Speech Segmentation",
    "volume": "main",
    "abstract": "The task of indoor/ outdoor audio classification using foreground speech segmentation is attempted in this work. Foreground speech segmentation is the use of features to segment between foreground speech and background interfering sources like noise. Initially, the foreground and background segments are obtained from foreground speech segmentation by using the normalized autocorrelation peak strength (NAPS) of the zero frequency filtered signal (ZFFS) as a feature. The background segments are then considered for determining whether a particular segment is an indoor or outdoor audio sample. The mel frequency cepstral coefficients are obtained from the background segments of both the indoor and outdoor audio samples and are used to train the Support Vector Machine (SVM) classifier. The use of foreground speech segmentation gives a promising performance for the indoor/ outdoor audio classification task",
    "checked": true,
    "id": "46393584d0aefd22fcc669439ddd5a6e1049a922",
    "semantic_title": "indoor/outdoor audio classification using foreground speech segmentation",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2017/guo17_interspeech.html": {
    "title": "Attention Based CLDNNs for Short-Duration Acoustic Scene Classification",
    "volume": "main",
    "abstract": "Recently, neural networks with deep architecture have been widely applied to acoustic scene classification. Both Convolutional Neural Networks (CNNs) and Long Short-Term Memory Networks (LSTMs) have shown improvements over fully connected Deep Neural Networks (DNNs). Motivated by the fact that CNNs, LSTMs and DNNs are complimentary in their modeling capability, we apply the CLDNNs (Convolutional, Long Short-Term Memory, Deep Neural Networks) framework to short-duration acoustic scene classification in a unified architecture. The CLDNNs take advantage of frequency modeling with CNNs, temporal modeling with LSTM, and discriminative training with DNNs. Based on the CLDNN architecture, several novel attention-based mechanisms are proposed and applied on the LSTM layer to predict the importance of each time step. We evaluate the proposed method on the truncated version of the 2016 TUT acoustic scenes dataset which consists of recordings from 15 different scenes. By using CLDNNs with bidirectional LSTM, we achieve higher performance compared to the conventional neural network architectures. Moreover, by combining the attention-weighted output with LSTM final time step output, significant improvement can be further achieved",
    "checked": true,
    "id": "9c80aa1dec0087fb754d0c3351f3d6803dbb5685",
    "semantic_title": "attention based cldnns for short-duration acoustic scene classification",
    "citation_count": 44
  },
  "https://www.isca-speech.org/archive/interspeech_2017/xia17_interspeech.html": {
    "title": "Frame-Wise Dynamic Threshold Based Polyphonic Acoustic Event Detection",
    "volume": "main",
    "abstract": "Acoustic event detection, the determination of the acoustic event type and the localisation of the event, has been widely applied in many real-world applications. Many works adopt multi-label classification techniques to perform the polyphonic acoustic event detection with a global threshold to detect the active acoustic events. However, the global threshold has to be set manually and is highly dependent on the database being tested. To deal with this, we replaced the fixed threshold method with a frame-wise dynamic threshold approach in this paper. Two novel approaches, namely contour and regressor based dynamic threshold approaches are proposed in this work. Experimental results on the popular TUT Acoustic Scenes 2016 database of polyphonic events demonstrated the superior performance of the proposed approaches",
    "checked": true,
    "id": "5db7c47f98bc91ba41f14c2d2e9cf25785df8359",
    "semantic_title": "frame-wise dynamic threshold based polyphonic acoustic event detection",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jang17_interspeech.html": {
    "title": "Enhanced Feature Extraction for Speech Detection in Media Audio",
    "volume": "main",
    "abstract": "Speech detection is an important first step for audio analysis on media contents, whose goal is to discriminate the presence of speech from non-speech. It remains a challenge owing to various sound sources included in media audio. In this work, we present a novel audio feature extraction method to reflect the acoustic characteristic of the media audio in the time-frequency domain. Since the degree of combination of harmonic and percussive components varies depending on the type of sound source, the audio features which further distinguish between speech and non-speech can be obtained by decomposing the signal into both components. For the evaluation, we use over 20 hours of drama which manually annotated for speech detection as well as 4 full-length movies with annotations released for a research community, whose total length is over 8 hours. Experimental results with deep neural network show superior performance of the proposed in media audio condition",
    "checked": true,
    "id": "8f51d1738c4719e0c5cdd71c5f4cd84984bb657a",
    "semantic_title": "enhanced feature extraction for speech detection in media audio",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sonowal17_interspeech.html": {
    "title": "Audio Classification Using Class-Specific Learned Descriptors",
    "volume": "main",
    "abstract": "This paper presents a classification scheme for audio signals using high-level feature descriptors. The descriptor is designed to capture the relevance of each acoustic feature group (or feature set like mel-frequency cepstral coefficients, perceptual features etc.) in recognizing an audio class. For this, a bank of RVM classifiers are modeled for each ‘audio class'–‘feature group' pair. The response of an input signal to this bank of RVM classifiers forms the entries of the descriptor. Each entry of the descriptor thus measures the proximity of the input signal to an audio class based on a single feature group. This form of signal representation offers two-fold advantages. First, it helps to determine the effectiveness of each feature group in classifying a specific audio class. Second, the descriptor offers higher discriminability than the low-level feature groups and a simple SVM classifier trained on the descriptor produces better performance than several state-of-the-art methods",
    "checked": true,
    "id": "97d804c4e8341534232709cc633bae37e145e52f",
    "semantic_title": "audio classification using class-specific learned descriptors",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ebbers17_interspeech.html": {
    "title": "Hidden Markov Model Variational Autoencoder for Acoustic Unit Discovery",
    "volume": "main",
    "abstract": "Variational Autoencoders (VAEs) have been shown to provide efficient neural-network-based approximate Bayesian inference for observation models for which exact inference is intractable. Its extension, the so-called Structured VAE (SVAE) allows inference in the presence of both discrete and continuous latent variables. Inspired by this extension, we developed a VAE with Hidden Markov Models (HMMs) as latent models. We applied the resulting HMM-VAE to the task of acoustic unit discovery in a zero resource scenario. Starting from an initial model based on variational inference in an HMM with Gaussian Mixture Model (GMM) emission probabilities, the accuracy of the acoustic unit discovery could be significantly improved by the HMM-VAE. In doing so we were able to demonstrate for an unsupervised learning task what is well-known in the supervised learning case: Neural networks provide superior modeling power compared to GMMs",
    "checked": true,
    "id": "e8591d52f6054cc13c1f3603ce086bb83c7d4499",
    "semantic_title": "hidden markov model variational autoencoder for acoustic unit discovery",
    "citation_count": 42
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zohrer17_interspeech.html": {
    "title": "Virtual Adversarial Training and Data Augmentation for Acoustic Event Detection with Gated Recurrent Neural Networks",
    "volume": "main",
    "abstract": "In this paper, we use gated recurrent neural networks (GRNNs) for efficiently detecting environmental events of the IEEE Detection and Classification of Acoustic Scenes and Events challenge (DCASE2016). For this acoustic event detection task data is limited. Therefore, we propose data augmentation such as on-the-fly shuffling and virtual adversarial training for regularization of the GRNNs. Both improve the performance using GRNNs. We obtain a segment-based error rate of 0.59 and an F-score of 58.6%",
    "checked": true,
    "id": "c11170c830de3ea834d6d533f515240d6f060c53",
    "semantic_title": "virtual adversarial training and data augmentation for acoustic event detection with gated recurrent neural networks",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mcauliffe17_interspeech.html": {
    "title": "Montreal Forced Aligner: Trainable Text-Speech Alignment Using Kaldi",
    "volume": "main",
    "abstract": "We present the Montreal Forced Aligner (MFA), a new open-source system for speech-text alignment. MFA is an update to the Prosodylab-Aligner, and maintains its key functionality of trainability on new data, as well as incorporating improved architecture (triphone acoustic models and speaker adaptation), and other features. MFA uses Kaldi instead of HTK, allowing MFA to be distributed as a stand-alone package, and to exploit parallel processing for computationally-intensive training and scaling to larger datasets. We evaluate MFA's performance on aligning word and phone boundaries in English conversational and laboratory speech, relative to human-annotated boundaries, focusing on the effects of aligner architecture and training on the data to be aligned. MFA performs well relative to two existing open-source aligners with simpler architecture (Prosodylab-Aligner and FAVE), and both its improved architecture and training on data to be aligned generally result in more accurate boundaries",
    "checked": true,
    "id": "9e8b06c60722fee06d7f01d4eeaf3ae81e0247d7",
    "semantic_title": "montreal forced aligner: trainable text-speech alignment using kaldi",
    "citation_count": 661
  },
  "https://www.isca-speech.org/archive/interspeech_2017/meenakshi17_interspeech.html": {
    "title": "A Robust Voiced/Unvoiced Phoneme Classification from Whispered Speech Using the ‘Color' of Whispered Phonemes and Deep Neural Network",
    "volume": "main",
    "abstract": "In this work, we propose a robust method to perform frame-level classification of voiced (V) and unvoiced (UV) phonemes from whispered speech, a challenging task due to its voiceless and noise-like nature. We hypothesize that a whispered speech spectrum can be represented as a linear combination of a set of colored noise spectra. A five-dimensional (5D) feature is computed by employing non-negative matrix factorization with a fixed basis dictionary, constructed using spectra of five colored noises. Deep Neural Network (DNN) is used as the classifier. We consider two baseline features-1) Mel Frequency Cepstral Coefficients (MFCC), 2) features computed from a data driven dictionary. Experiments reveal that the features from the colored noise dictionary perform better (on average) than that using the data driven dictionary, with a relative improvement in the average V/UV accuracy of 10.30%, within, and 10.41%, across, data from seven subjects. We also find that the MFCCs and 5D features carry complementary information regarding the nature of voicing decisions in whispered speech. Hence, across all subjects, we obtain a balanced frame-level V/UV classification performance, when MFCC and 5D features are combined, compared to a skewed performance when they are considered separately",
    "checked": false,
    "id": "9367699d3722a28986488933bc929006f05f14a6",
    "semantic_title": "a robust voiced/unvoiced phoneme classification from whispered speech using the 'color' of whispered phonemes and deep neural network",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2017/williams17_interspeech.html": {
    "title": "Rescoring-Aware Beam Search for Reduced Search Errors in Contextual Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Using context in automatic speech recognition allows the recognition system to dynamically task-adapt and bring gains to a broad variety of use-cases. An important mechanism of context-inclusion is on-the-fly rescoring of hypotheses with contextual language model content available only in real-time In systems where rescoring occurs on the lattice during its construction as part of beam search decoding, hypotheses eligible for rescoring may be missed due to pruning. This can happen for many reasons: the language model and rescoring model may assign significantly different scores, there may be a lot of noise in the utterance, or word prefixes with a high out-degree may necessitate aggressive pruning to keep the search tractable. This results in misrecognitions when contextually-relevant hypotheses are pruned before rescoring, even if a contextual rescoring model favors those hypotheses by a large margin We present a technique to adapt the beam search algorithm to preserve hypotheses when they may benefit from rescoring. We show that this technique significantly reduces the number of search pruning errors on rescorable hypotheses, without a significant increase in the search space size. This technique makes it feasible to use one base language model, but still achieve high-accuracy speech recognition results in all contexts",
    "checked": true,
    "id": "31fc9159ecb3a43c107a45e8fe32ab9f1c39cebb",
    "semantic_title": "rescoring-aware beam search for reduced search errors in contextual automatic speech recognition",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zenkel17_interspeech.html": {
    "title": "Comparison of Decoding Strategies for CTC Acoustic Models",
    "volume": "main",
    "abstract": "Connectionist Temporal Classification has recently attracted a lot of interest as it offers an elegant approach to building acoustic models (AMs) for speech recognition. The CTC loss function maps an input sequence of observable feature vectors to an output sequence of symbols. Output symbols are conditionally independent of each other under CTC loss, so a language model (LM) can be incorporated conveniently during decoding, retaining the traditional separation of acoustic and linguistic components in ASR For fixed vocabularies, Weighted Finite State Transducers provide a strong baseline for efficient integration of CTC AMs with n-gram LMs. Character-based neural LMs provide a straight forward solution for open vocabulary speech recognition and all-neural models, and can be decoded with beam search. Finally, sequence-to-sequence models can be used to translate a sequence of individual sounds into a word string We compare the performance of these three approaches, and analyze their error patterns, which provides insightful guidance for future research and development in this important area",
    "checked": true,
    "id": "7b8022a139564225ffd34d8edf8ee92af104ae20",
    "semantic_title": "comparison of decoding strategies for ctc acoustic models",
    "citation_count": 40
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hadian17_interspeech.html": {
    "title": "Phone Duration Modeling for LVCSR Using Neural Networks",
    "volume": "main",
    "abstract": "We describe our work on incorporating probabilities of phone durations, learned by a neural net, into an ASR system. Phone durations are incorporated via lattice rescoring. The input features are derived from the phone identities of a context window of phones, plus the durations of preceding phones within that window. Unlike some previous work, our network outputs the probability of different durations (in frames) directly, up to a fixed limit. We evaluate this method on several large vocabulary tasks, and while we consistently see improvements inWord Error Rates, the improvements are smaller when the lattices are generated with neural net based acoustic models",
    "checked": true,
    "id": "e5b181fe8c7711fb4bbe13990655a86309aba873",
    "semantic_title": "phone duration modeling for lvcsr using neural networks",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chorowski17_interspeech.html": {
    "title": "Towards Better Decoding and Language Model Integration in Sequence to Sequence Models",
    "volume": "main",
    "abstract": "The recently proposed Sequence-to-Sequence (seq2seq) framework advocates replacing complex data processing pipelines, such as an entire automatic speech recognition system, with a single neural network trained in an end-to-end fashion. In this contribution, we analyse an attention-based seq2seq speech recognition system that directly transcribes recordings into characters. We observe two shortcomings: overconfidence in its predictions and a tendency to produce incomplete transcriptions when language models are used. We propose practical solutions to both problems achieving competitive speaker independent word error rates on the Wall Street Journal dataset: without separate language models we reach 10.6% WER, while together with a trigram language model, we reach 6.7% WER, a state-of-the-art result for HMM-free methods",
    "checked": true,
    "id": "7dbb2d983ab95da04e5d47c87ddd2cd9a8f20786",
    "semantic_title": "towards better decoding and language model integration in sequence to sequence models",
    "citation_count": 328
  },
  "https://www.isca-speech.org/archive/interspeech_2017/li17d_interspeech.html": {
    "title": "Empirical Evaluation of Parallel Training Algorithms on Acoustic Modeling",
    "volume": "main",
    "abstract": "Deep learning models (DLMs) are state-of-the-art techniques in speech recognition. However, training good DLMs can be time consuming especially for production-size models and corpora. Although several parallel training algorithms have been proposed to improve training efficiency, there is no clear guidance on which one to choose for the task in hand due to lack of systematic and fair comparison among them. In this paper we aim at filling this gap by comparing four popular parallel training algorithms in speech recognition, namely asynchronous stochastic gradient descent (ASGD), blockwise model-update filtering (BMUF), bulk synchronous parallel (BSP) and elastic averaging stochastic gradient descent (EASGD), on 1000-hour LibriSpeech corpora using feed-forward deep neural networks (DNNs) and convolutional, long short-term memory, DNNs (CLDNNs). Based on our experiments, we recommend using BMUF as the top choice to train acoustic models since it is most stable, scales well with number of GPUs, can achieve reproducible results, and in many cases even outperforms single-GPU SGD. ASGD can be used as a substitute in some cases",
    "checked": true,
    "id": "9907faa68764e8c57aec05c1e3f6779cf84c7235",
    "semantic_title": "empirical evaluation of parallel training algorithms on acoustic modeling",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2017/xiang17_interspeech.html": {
    "title": "Binary Deep Neural Networks for Speech Recognition",
    "volume": "main",
    "abstract": "Deep neural networks (DNNs) are widely used in most current automatic speech recognition (ASR) systems. To guarantee good recognition performance, DNNs usually require significant computational resources, which limits their application to low-power devices. Thus, it is appealing to reduce the computational cost while keeping the accuracy. In this work, in light of the success in image recognition, binary DNNs are utilized in speech recognition, which can achieve competitive performance and substantial speed up. To our knowledge, this is the first time that binary DNNs have been used in speech recognition. For binary DNNs, network weights and activations are constrained to be binary values, which enables faster matrix multiplication based on bit operations. By exploiting the hardware population count instructions, the proposed binary matrix multiplication can achieve 5~7 times speed up compared with highly optimized floating-point matrix multiplication. This results in much faster DNN inference since matrix multiplication is the most computationally expensive operation. Experiments on both TIMIT phone recognition and a 50-hour Switchboard speech recognition show that, binary DNNs can run about 4 times faster than standard DNNs during inference, with roughly 10.0% relative accuracy reduction",
    "checked": true,
    "id": "8f1f128889f9236e83d4380a82ce7e96a89d85c3",
    "semantic_title": "binary deep neural networks for speech recognition",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chandrashekaran17_interspeech.html": {
    "title": "Hierarchical Constrained Bayesian Optimization for Feature, Acoustic Model and Decoder Parameter Optimization",
    "volume": "main",
    "abstract": "We describe the implementation of a hierarchical constrained Bayesian Optimization algorithm and it's application to joint optimization of features, acoustic model structure and decoding parameters for deep neural network (DNN)-based large vocabulary continuous speech recognition (LVCSR) systems. Within our hierarchical optimization method we perform constrained Bayesian optimization jointly of feature hyper-parameters and acoustic model structure in the first-level, and then perform an iteration of constrained Bayesian optimization for the decoder hyper-parameters in the second. We show the the proposed hierarchical optimization method can generate a model with higher performance than a manually optimized system on a server platform. Furthermore, we demonstrate that the proposed framework can be used to automatically build real-time speech recognition systems for graphics processing unit (GPU)-enabled embedded platforms that retain similar accuracy to a server platform, while running with constrained computing resources",
    "checked": true,
    "id": "d74e5b93e243ae59cf36baf0fe5680edf0ae3f68",
    "semantic_title": "hierarchical constrained bayesian optimization for feature, acoustic model and decoder parameter optimization",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/toyama17_interspeech.html": {
    "title": "Use of Global and Acoustic Features Associated with Contextual Factors to Adapt Language Models for Spontaneous Speech Recognition",
    "volume": "main",
    "abstract": "In this study, we propose a new method of adapting language models for speech recognition using para-linguistic and extra-linguistic features in speech. When we talk with others, we often change the way of lexical choice and speaking style according to various contextual factors. This fact indicates that the performance of automatic speech recognition can be improved by taking the contextual factors into account, which can be estimated from speech acoustics. In this study, we attempt to find global and acoustic features that are associated with those contextual factors, then integrate those features into Recurrent Neural Network (RNN) language models for speech recognition. In experiments, using Japanese spontaneous speech corpora, we examine how i-vector and openSMILE are associated with contextual factors. Then, we use those features in the reranking process of RNN-based language models. Results show that perplexity is reduced by 16% relative and word error rate is reduced by 2.1% relative for highly emotional speech",
    "checked": true,
    "id": "abd4ec54d6feb7c96266667304968bf8b5d01cc3",
    "semantic_title": "use of global and acoustic features associated with contextual factors to adapt language models for spontaneous speech recognition",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2017/pahuja17_interspeech.html": {
    "title": "Joint Learning of Correlated Sequence Labeling Tasks Using Bidirectional Recurrent Neural Networks",
    "volume": "main",
    "abstract": "The stream of words produced by Automatic Speech Recognition (ASR) systems is typically devoid of punctuations and formatting. Most natural language processing applications expect segmented and well-formatted texts as input, which is not available in ASR output. This paper proposes a novel technique of jointly modeling multiple correlated tasks such as punctuation and capitalization using bidirectional recurrent neural networks, which leads to improved performance for each of these tasks. This method could be extended for joint modeling of any other correlated sequence labeling tasks",
    "checked": true,
    "id": "dcae7738f1dbb5148544fabdd8464946a9a51b17",
    "semantic_title": "joint learning of correlated sequence labeling tasks using bidirectional recurrent neural networks",
    "citation_count": 29
  },
  "https://www.isca-speech.org/archive/interspeech_2017/shen17_interspeech.html": {
    "title": "Estimation of Gap Between Current Language Models and Human Performance",
    "volume": "main",
    "abstract": "Language models (LMs) have gained dramatic improvement in the past years due to the wide application of neural networks. This raises the question of how far we are away from the perfect language model and how much more research is needed in language modelling. As for perplexity giving a value for human perplexity (as an upper bound of what is reasonably expected from an LM) is difficult. Word error rate (WER) has the disadvantage that it also measures the quality of other components of a speech recognizer like the acoustic model and the feature extraction. We therefore suggest evaluating LMs in a generative setting (which has been done before on selected hand-picked examples) and running a human evaluation on the generated sentences. The results imply that LMs need about 10 to 20 more years of research before human performance is reached. Moreover, we show that the human judgement scores on the generated sentences and perplexity are closely correlated. This leads to an estimated perplexity of 12 for an LM that would be able to pass the human judgement test in the setting we suggested",
    "checked": true,
    "id": "7fe4e308de5b2e5b509be8636c169e7928c242d9",
    "semantic_title": "estimation of gap between current language models and human performance",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2017/moro17_interspeech.html": {
    "title": "A Phonological Phrase Sequence Modelling Approach for Resource Efficient and Robust Real-Time Punctuation Recovery",
    "volume": "main",
    "abstract": "For the automatic punctuation of Automatic Speech Recognition (ASR) output, both prosodic and text based features are used, often in combination. Pure prosody based approaches usually have low computation needs, introduce little latency (delay) and they are also more robust to ASR errors. Text based approaches usually yield better performance, they are however resource demanding (both regarding their training and computational needs), often introduce high time latency and are more sensitive to ASR errors. The present paper proposes a lightweight prosody based punctuation approach following a new paradigm: we argue in favour of an all-inclusive modelling of speech prosody instead of just relying on distinct acoustic markers: first, the entire phonological phrase structure is reconstructed, then its close correlation with punctuations is exploited in a sequence modelling approach with recurrent neural networks. With this tiny and easy to implement model we reach performance in Hungarian punctuation comparable to large, text based models for other languages by keeping resource requirements minimal and suitable for real-time operation with low latency",
    "checked": true,
    "id": "fd9c8e54db7fd691c490bca061181fdfbc82ba44",
    "semantic_title": "a phonological phrase sequence modelling approach for resource efficient and robust real-time punctuation recovery",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wang17c_interspeech.html": {
    "title": "Factors Affecting the Intelligibility of Low-Pass Filtered Speech",
    "volume": "main",
    "abstract": "Frequency compression is an effective alternative to conventional hearing aids amplification for patients with severe-to-profound middle- and high-frequency hearing loss and with some low-frequency residual hearing. In order to develop novel frequency compression strategy, it is important to first understand the mechanism for recognizing low-pass filtered speech, which simulates high-frequency hearing loss. The present work investigated three factors affecting the intelligibility of low-pass filtered speech, i.e., vowels, temporal fine-structure, and fundamental frequency (F0) contour. Mandarin sentences were processed to generate three types (i.e., vowel-only, fine-structure-only, and F0-contour-flattened) of low-pass filtered stimuli. Listening experiments with normal-hearing listeners showed that among the three factors assessed, the vowel-only low-pass filtered speech was the most intelligible, which was followed by the fine-structure-based low-pass filtered speech. Flattening F0-contour significantly deteriorated the intelligibility of low-pass filtered speech",
    "checked": true,
    "id": "644ae1984221281054d387e84d8def4bde9f360f",
    "semantic_title": "factors affecting the intelligibility of low-pass filtered speech",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wang17d_interspeech.html": {
    "title": "Phonetic Restoration of Temporally Reversed Speech",
    "volume": "main",
    "abstract": "Early study showed that temporally reversed speech may still be very intelligible. The present work further assessed the role of acoustic cues accounting for the intelligibility of temporally reversed speech. Mandarin sentences were edited to be temporally reversed. Experiment 1 preserved the original consonant segments, and experiment 2 only preserved the temporally reversed fine-structure waveform. Experimental results with normal-hearing listeners showed that for Mandarin speech, listeners could still perfectly understand the temporally reversed speech with a reversion duration up to 50 ms. Preserving original consonant segments did not significantly improve the intelligibility of the temporally reversed speech, suggesting that the reversion processing applied to vowels largely affected the intelligibility of temporally reversed speech. When the local short-time envelope waveform was removed, listeners could still understand stimuli with primarily temporally reversed fine-structure waveform, suggesting the perceptual role of temporally reversed fine-structure to the intelligibility of temporally reversed speech",
    "checked": true,
    "id": "183cdbf28b760e85cf545688770f9b539fbed37e",
    "semantic_title": "phonetic restoration of temporally reversed speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ishida17_interspeech.html": {
    "title": "Simultaneous Articulatory and Acoustic Distortion in L1 and L2 Listening: Locally Time-Reversed \"Fast\" Speech",
    "volume": "main",
    "abstract": "The current study explores how native and non-native speakers cope with simultaneous articulatory and acoustic distortion in speech perception. The articulatory distortion was generated by asking a speaker to articulate target speech as fast as possible (fast speech). The acoustic distortion was created by dividing speech signals into small segments with equal time duration (e.g., 50 ms) from the onset of speech, and flipping every segment on a temporal axis, and putting them back together (locally time-reversed speech). This study explored how \"locally time-reversed fast speech\" was intelligible as compared to \"locally time-reversed normal speech\" measured in Ishida, Samuel, and Arai (2016). Participants were native English speakers and native Japanese speakers who spoke English as a second language. They listened to English words and pseudowords that contained a lot of stop consonants. These items were spoken fast and locally time-reversed at every 10, 20, 30, 40, 50, or 60 ms. In general, \"locally time-reversed fast speech\" became gradually unintelligible as the length of reversed segments increased. Native speakers generally understood locally time-reversed fast spoken words well but not pseudowords, while non-native speakers hardly understood both words and pseudowords. Language proficiency strongly supported the perceptual restoration of locally time-reversed fast speech",
    "checked": true,
    "id": "ba3cc5c345884802fc18d16f7b584b414d63c61b",
    "semantic_title": "simultaneous articulatory and acoustic distortion in l1 and l2 listening: locally time-reversed \"fast\" speech",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2017/burchfield17_interspeech.html": {
    "title": "Lexically Guided Perceptual Learning in Mandarin Chinese",
    "volume": "main",
    "abstract": "Lexically guided perceptual learning refers to the use of lexical knowledge to retune speech categories and thereby adapt to a novel talker's pronunciation. This adaptation has been extensively documented, but primarily for segmental-based learning in English and Dutch. In languages with lexical tone, such as Mandarin Chinese, tonal categories can also be retuned in this way, but segmental category retuning had not been studied. We report two experiments in which Mandarin Chinese listeners were exposed to an ambiguous mixture of [f] and [s] in lexical contexts favoring an interpretation as either [f] or [s]. Listeners were subsequently more likely to identify sounds along a continuum between [f] and [s], and to interpret minimal word pairs, in a manner consistent with this exposure. Thus lexically guided perceptual learning of segmental categories had indeed taken place, consistent with suggestions that such learning may be a universally available adaptation process",
    "checked": true,
    "id": "e0ae92eaab02b6a1454e1785a33dd87aefcf3ddf",
    "semantic_title": "lexically guided perceptual learning in mandarin chinese",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2017/davis17_interspeech.html": {
    "title": "The Effect of Spectral Profile on the Intelligibility of Emotional Speech in Noise",
    "volume": "main",
    "abstract": "The current study investigated why the intelligibility of expressive speech in noise varies as a function of the emotion expressed (e.g., happiness being more intelligible than sadness), even though the signal-to-noise ratio is the same. We tested the straightforward proposal that the expression of some emotions affect speech intelligibility by shifting spectral energy above the energy profile of the noise masker. This was done by determining how the spectral profile of speech is affected by different emotional expressions using three different expressive speech databases. We then examined if these changes were correlated with scores produced by an objective intelligibility metric. We found a relatively consistent shift in spectral energy for different emotions across the databases and a high correlation between the extent of these changes and the objective intelligibility scores. Moreover, the pattern of intelligibility scores is consistent with human perception studies (although there was considerable individual variation). We suggest that the intelligibility of emotion speech in noise is simply related to its audibility as conditioned by the effect that the expression of emotion has on its spectral profile",
    "checked": true,
    "id": "a8d7a2d2d3af19313fb5da98bfe7637c88970b49",
    "semantic_title": "the effect of spectral profile on the intelligibility of emotional speech in noise",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2017/maslowski17_interspeech.html": {
    "title": "Whether Long-Term Tracking of Speech Rate Affects Perception Depends on Who is Talking",
    "volume": "main",
    "abstract": "Speech rate is known to modulate perception of temporally ambiguous speech sounds. For instance, a vowel may be perceived as short when the immediate speech context is slow, but as long when the context is fast. Yet, effects of long-term tracking of speech rate are largely unexplored. Two experiments tested whether long-term tracking of rate influences perception of the temporal Dutch vowel contrast /ɑ/-/a:/. In Experiment 1, one low-rate group listened to ‘neutral' rate speech from talker A and to slow speech from talker B. Another high-rate group was exposed to the same neutral speech from A, but to fast speech from B. Between-group comparison of the ‘neutral' trials revealed that the low-rate group reported a higher proportion of /a:/ in A's ‘neutral' speech, indicating that A sounded faster when B was slow. Experiment 2 tested whether one's own speech rate also contributes to effects of long-term tracking of rate. Here, talker B's speech was replaced by playback of participants' own fast or slow speech. No evidence was found that one's own voice affected perception of talker A in larger speech contexts. These results carry implications for our understanding of the mechanisms involved in rate-dependent speech perception and of dialogue",
    "checked": true,
    "id": "4d5e5ee02b3bb51e208f028b06e27f009fd000f3",
    "semantic_title": "whether long-term tracking of speech rate affects perception depends on who is talking",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2017/peres17_interspeech.html": {
    "title": "Emotional Thin-Slicing: A Proposal for a Short- and Long-Term Division of Emotional Speech",
    "volume": "main",
    "abstract": "Human listeners are adept at successfully recovering linguistically- and socially-relevant information from very brief utterances. Studies using the ‘thin-slicing' approach show that accurate judgments of the speaker's emotional state can be made from minimal quantities of speech. The present experiment tested the performance of listeners exposed to thin-sliced samples of spoken Brazilian Portuguese selected to exemplify four emotions ( anger, fear, sadness, happiness). Rather than attaching verbal labels to the audio samples, participants were asked to pair the excerpts with averaged facial images illustrating the four emotion categories. Half of the listeners were native speakers of Brazilian Portuguese, while the others were native English speakers who knew no Portuguese. Both groups of participants were found to be accurate and consistent in assigning the audio samples to the expected emotion category, but some emotions were more reliably identified than others. Fear was misidentified most frequently. We conclude that the phonetic cues to speakers' emotional states are sufficiently salient and differentiated that listeners need only a few syllables upon which to base judgments, and that as a species we owe our perceptual sensitivity in this area to the survival value of being able to make rapid decisions concerning the psychological states of others",
    "checked": true,
    "id": "2b2119fc6b6bafd23f933914c4167530283c7f39",
    "semantic_title": "emotional thin-slicing: a proposal for a short- and long-term division of emotional speech",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2017/guevararukoz17_interspeech.html": {
    "title": "Predicting Epenthetic Vowel Quality from Acoustics",
    "volume": "main",
    "abstract": "Past research has shown that sound sequences not permitted in our native language may be distorted by our perceptual system. A well-documented example is vowel epenthesis, a phenomenon by which listeners hallucinate non-existent vowels within illegal consonantal sequences. As reported in previous work, this occurs for instance in Japanese (JP) and Brazilian Portuguese (BP), languages for which the ‘default' epenthetic vowels are /u/ and /i/, respectively. In a perceptual experiment, we corroborate the finding that the quality of this illusory vowel is language-dependent, but also that this default choice can be overridden by coarticulatory information present on the consonant cluster. In a second step, we analyse recordings of JP and BP speakers producing ‘epenthesized' versions of stimuli from the perceptual task. Results reveal that the default vowel corresponds to the vowel with the most reduced acoustic characteristics and whose formants are acoustically closest to formant transitions present in consonantal clusters. Lastly, we model behavioural responses from the perceptual experiment with an exemplar model using dynamic time warping (DTW)-based similarity measures on MFCCs",
    "checked": true,
    "id": "88dff104f0035574e0e9abee51e8d7124055e371",
    "semantic_title": "predicting epenthetic vowel quality from acoustics",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2017/matsui17_interspeech.html": {
    "title": "The Effect of Spectral Tilt on Size Discrimination of Voiced Speech Sounds",
    "volume": "main",
    "abstract": "A number of studies, with either voiced or unvoiced speech, have demonstrated that a speaker's geometric mean formant frequency (MFF) has a large effect on the perception of the speaker's size, as would be expected. One study with unvoiced speech showed that lifting the slope of the speech spectrum by 6 dB/octave also led to a reduction in the perceived size of the speaker. This paper reports an analogous experiment to determine whether lifting the slope of the speech spectrum by 6 dB/octave affects the perception of speaker size with voiced speech (words). The results showed that voiced speech with high-frequency enhancement was perceived to arise from smaller speakers. On average, the point of subjective equality in MFF discrimination was reduced by about 5%. However, there were large individual differences; some listeners were effectively insensitive to spectral enhancement of 6 dB/octave; others showed a consistent effect of the same enhancement. The results suggest that models of speaker size perception will need to include a listener specific parameter for the effect of spectral slope",
    "checked": true,
    "id": "564032b3709dc9539a64f5370e125ae7300950ef",
    "semantic_title": "the effect of spectral tilt on size discrimination of voiced speech sounds",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lorenzotrueba17_interspeech.html": {
    "title": "Misperceptions of the Emotional Content of Natural and Vocoded Speech in a Car",
    "volume": "main",
    "abstract": "This paper analyzes a) how often listeners interpret the emotional content of an utterance incorrectly when listening to vocoded or natural speech in adverse conditions; b) which noise conditions cause the most misperceptions; and c) which group of listeners misinterpret emotions the most. The long-term goal is to construct new emotional speech synthesizers that adapt to the environment and to the listener. We performed a large-scale listening test where over 400 listeners between the ages of 21 and 72 assessed natural and vocoded acted emotional speech stimuli. The stimuli had been artificially degraded using a room impulse response recorded in a car and various in-car noise types recorded in a real car. Experimental results show that the recognition rates for emotions and perceived emotional strength degrade as signal-to-noise ratio decreases. Interestingly, misperceptions seem to be more pronounced for negative and low-arousal emotions such as calmness or anger, while positive emotions such as happiness appear to be more robust to noise. An ANOVA analysis of listener meta-data further revealed that gender and age also influenced results, with elderly male listeners most likely to incorrectly identify emotions",
    "checked": true,
    "id": "64a5b2233094db9b082a3ff0eb16d8b217d6e7ed",
    "semantic_title": "misperceptions of the emotional content of natural and vocoded speech in a car",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2017/niebuhr17_interspeech.html": {
    "title": "The Relative Cueing Power of F0 and Duration in German Prominence Perception",
    "volume": "main",
    "abstract": "Previous studies showed for German and other (West) Germanic language, including English, that perceived syllable prominence is primarily controlled by changes in duration and F0, with the latter cue being more powerful than the former. Our study is an initial approach to develop this prominence hierarchy further by putting numbers on the interplay of duration and F0. German listeners indirectly judged through lexical identification the relative prominence levels of two neighboring syllables. Results show that an increase in F0 of between 0.49 and 0.76 st is required to outweigh the prominence effect of a 30% increase in duration of a neighboring syllable. These numbers are fairly stable across a large range of absolute F0 and duration levels and hence useful in speech technology",
    "checked": true,
    "id": "a7da20824d041ad054a52e880fb7edbdd16615ea",
    "semantic_title": "the relative cueing power of f0 and duration in german prominence perception",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2017/marques17_interspeech.html": {
    "title": "Perception and Acoustics of Vowel Nasality in Brazilian Portuguese",
    "volume": "main",
    "abstract": "This study explores the relationship between identification, degree of nasality and vowel quality in oral, nasal and nasalized vowels in Brazilian Portuguese. Despite common belief that the language possesses contrastive nasal vowels, literature examination shows that nasal vowels may be followed by a nasal resonance, while nasalized vowels must be followed by a nasal consonant. It is argued that the nasal resonance may be the remains of a consonant that nasalizes the vowel, making nasal vowels simply coarticulatorily nasalized (e.g. [1]). If so, vowel nasality should not be more informative for the perception of a word containing a nasal vowel than for a word containing a nasalized vowel, as nasality is attributed to coarticulation. To test this hypothesis, randomized stimuli containing the first syllable of words with oral, nasal and nasalized vowels were presented to BP listeners who had to identify the stimuli original word. Preliminary results demonstrate that accuracy decreased for nasal and nasalized stimuli. A comparison between patterns of response to measured degrees of vowel acoustic nasality and formant values demonstrate that vowel quality differences may play a more relevant role in word identification than type of nasality in a vowel",
    "checked": true,
    "id": "68d0c63ebf60b539abab52a8e977d9b8da786b82",
    "semantic_title": "perception and acoustics of vowel nasality in brazilian portuguese",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kim17b_interspeech.html": {
    "title": "Sociophonetic Realizations Guide Subsequent Lexical Access",
    "volume": "main",
    "abstract": "Previous studies on spoken word recognition suggest that lexical access is facilitated when social information attributed to the voice is congruent with the social characteristics associated with the word. This paper builds on this work, presenting results from a lexical decision task in which target words associated with different age groups were preceded by sociophonetic primes. No age-related phonetic cues were provided within the target words; instead, the non-related prime words contained a sociophonetic variable involved in ongoing change. We found that age-associated words are recognized faster when preceded by an age-congruent phonetic variant in the prime word. The results demonstrate that lexical access is influenced by sociophonetic variation, a result which we argue arises from experience-based probabilities of covariation between sounds and words",
    "checked": true,
    "id": "458b5b6330fce26eeee7bd57d5330f736c0909e4",
    "semantic_title": "sociophonetic realizations guide subsequent lexical access",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2017/silva17_interspeech.html": {
    "title": "Critical Articulators Identification from RT-MRI of the Vocal Tract",
    "volume": "main",
    "abstract": "Several technologies, such as electromagnetic midsagittal articulography (EMA) or real-time magnetic resonance (RT-MRI), enable studying the static and dynamic aspects of speech production. The resulting knowledge can, in turn, inform the improvement of speech production models, e.g., for articulatory speech synthesis, by enabling the identification of which articulators and gestures are involved in producing specific sounds The amount of data available from these technologies, and the need for a systematic quantitative assessment, advise tackling these matters through data-driven approaches, preferably unsupervised, since annotated data is scarce. In this context, a method for statistical identification of critical articulators has been proposed, in the literature, and successfully applied to EMA data. However, the many differences regarding the data available from other technologies, such as RT-MRI, and language-specific aspects create a challenging setting for its direct and wider applicability In this article, we address the steps needed to extend the applicability of the proposed statistical analyses, initially applied to EMA, to an existing RT-MRI corpus and test it for a different language, European Portuguese. The obtained results, for three speakers, and considering 33 phonemes, provide phonologically meaningful critical articulator outcomes and show evidence of the applicability of the method to RT-MRI",
    "checked": true,
    "id": "cf9524ff4f764d2b277e9da7b7a338bc877e183d",
    "semantic_title": "critical articulators identification from rt-mri of the vocal tract",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2017/somandepalli17_interspeech.html": {
    "title": "Semantic Edge Detection for Tracking Vocal Tract Air-Tissue Boundaries in Real-Time Magnetic Resonance Images",
    "volume": "main",
    "abstract": "Recent developments in real-time magnetic resonance imaging (rtMRI) have enabled the study of vocal tract dynamics during production of running speech at high frame rates (e.g., 83 frames per second). Such large amounts of acquired data require scalable automated methods to identify different articulators (e.g., tongue, velum) for further analysis. In this paper, we propose a convolutional neural network with an encoder-decoder architecture to jointly detect the relevant air-tissue boundaries as well as to label them, which we refer to as ‘semantic edge detection'. We pose this as a pixel labeling problem, with the outline contour of each articulator of interest as positive class and the remaining tissue and airway as negative classes. We introduce a loss function modified with additional penalty for misclassification at air-tissue boundaries to account for class imbalance and improve edge localization. We then use a greedy search algorithm to draw contours from the probability maps of the positive classes predicted by the network. The articulator contours obtained by our method are comparable to the true labels generated by iteratively fitting a manually created subject-specific template. Our results generalize well across subjects and different vocal tract postures, demonstrating a significant improvement over the structured regression baseline",
    "checked": true,
    "id": "7c816a5babe0fa571450cedc0c1dc2265f82e0ba",
    "semantic_title": "semantic edge detection for tracking vocal tract air-tissue boundaries in real-time magnetic resonance images",
    "citation_count": 27
  },
  "https://www.isca-speech.org/archive/interspeech_2017/asadiabadi17_interspeech.html": {
    "title": "Vocal Tract Airway Tissue Boundary Tracking for rtMRI Using Shape and Appearance Priors",
    "volume": "main",
    "abstract": "Knowledge about the dynamic shape of the vocal tract is the basis of many speech production applications such as, articulatory analysis, modeling and synthesis. Vocal tract airway tissue boundary segmentation in the mid-sagittal plane is necessary as an initial step for extraction of the cross-sectional area function. This segmentation problem is however challenging due to poor resolution of real-time speech MRI, grainy noise and the rapidly varying vocal tract shape. We present a novel approach to vocal tract airway tissue boundary tracking by training a statistical shape and appearance model for human vocal tract. We manually segment a set of vocal tract profiles and utilize a statistical approach to train a shape and appearance model for the tract. An active contour approach is employed to segment the airway tissue boundaries of the vocal tract while restricting the curve movement to the trained shape and appearance model. Then the contours in subsequent frames are tracked using dense motion estimation methods. Experimental evaluations over the mean square error metric indicate significant improvements compared to the state-of-the-art",
    "checked": true,
    "id": "8d2b02ea41f3a1d497bcfa2c0dad3b75e3eed317",
    "semantic_title": "vocal tract airway tissue boundary tracking for rtmri using shape and appearance priors",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ananthapadmanabha17_interspeech.html": {
    "title": "An Objective Critical Distance Measure Based on the Relative Level of Spectral Valley",
    "volume": "main",
    "abstract": "Spectral integration is a subjective phenomenon in which a vowel with two formants, spaced below a critical distance, is perceived to be of the same phonetic quality as that of a vowel with a single formant. It is tedious to conduct perceptual tests to determine the critical distance for various experimental conditions. To alleviate this difficulty, we propose an objective critical distance (OCD) that can be determined from the spectral envelope of a speech signal. OCD is defined as that spacing between the adjacent formants when the level of the spectral valley between them reaches the mean spectral value. The measured OCD lies in the same range of 3 to 3.5 Bark as the subjective critical distance for similar experimental conditions giving credibility to the definition. However, it is noted that OCD for front vowels is significantly different from that for the back vowels",
    "checked": true,
    "id": "c9a2596fe2aaf5cd10e1befffaa7d34bbf6263e1",
    "semantic_title": "an objective critical distance measure based on the relative level of spectral valley",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sorensen17_interspeech.html": {
    "title": "Database of Volumetric and Real-Time Vocal Tract MRI for Speech Science",
    "volume": "main",
    "abstract": "We present the USC Speech and Vocal Tract Morphology MRI Database, a 17-speaker magnetic resonance imaging database for speech research. The database consists of real-time magnetic resonance images (rtMRI) of dynamic vocal tract shaping, denoised audio recorded simultaneously with rtMRI, and 3D volumetric MRI of vocal tract shapes during sustained speech sounds. We acquired 2D real-time MRI of vocal tract shaping during consonant-vowel-consonant sequences, vowel-consonant-vowel sequences, read passages, and spontaneous speech. We acquired 3D volumetric MRI of the full set of vowels and continuant consonants of American English. Each 3D volumetric MRI was acquired in one 7-second scan in which the participant sustained the sound. This is the first database to combine rtMRI of dynamic vocal tract shaping and 3D volumetric MRI of the entire vocal tract. The database provides a unique resource with which to examine the relationship between vocal tract morphology and vocal tract function. The USC Speech and Vocal Tract Morphology MRI Database is provided free for research use at http://sail.usc.edu/span/morphdb",
    "checked": true,
    "id": "435217e38709ca2238ed0330fc01d70fdd59fc14",
    "semantic_title": "database of volumetric and real-time vocal tract mri for speech science",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2017/cao17b_interspeech.html": {
    "title": "The Influence on Realization and Perception of Lexical Tones from Affricate's Aspiration",
    "volume": "main",
    "abstract": "Consonants in /CV/ syllables usually have potential influence on onset fundamental frequency (i.e., onset f0) of succeeding vowels. Previous studies showed such effect with respect to the aspiration of stops with evidence from Mandarin, a tonal language. While few studies investigated the effect on onset f0 from the aspiration of affricates. The differences between stops and affricates in aspiration leave space for further investigations. We examined the effect of affricate's aspiration on the realization of onset f0 of following vowels in the form of isolated syllables and continuous speech by reference to a minimal pair of syllables which differ only in aspiration. Besides, we conducted tone identification tests using two sets of tone continua based on the same minimal pair of syllables. Experimental results showed that the aspirated syllables increased the onset f0 of following vowels compared with unaspirated counterparts in both kinds of contexts. While the magnitude of differences varied with tones. And the perception results showed that aspirated syllables tended to be perceived as tones that have relative lower onset f0, which in turn supported the production result. The present study may have applications for speech identification and speech synthesis",
    "checked": true,
    "id": "28888dd79a2462d6429539e206ded49abd6b8ba8",
    "semantic_title": "the influence on realization and perception of lexical tones from affricate's aspiration",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2017/franken17_interspeech.html": {
    "title": "Audiovisual Recalibration of Vowel Categories",
    "volume": "main",
    "abstract": "One of the most daunting tasks of a listener is to map a continuous auditory stream onto known speech sound categories and lexical items. A major issue with this mapping problem is the variability in the acoustic realizations of sound categories, both within and across speakers. Past research has suggested listeners may use visual information (e.g., lip-reading) to calibrate these speech categories to the current speaker. Previous studies have focused on audiovisual recalibration of consonant categories. The present study explores whether vowel categorization, which is known to show less sharply defined category boundaries, also benefit from visual cues Participants were exposed to videos of a speaker pronouncing one out of two vowels, paired with audio that was ambiguous between the two vowels. After exposure, it was found that participants had recalibrated their vowel categories. In addition, individual variability in audiovisual recalibration is discussed. It is suggested that listeners' category sharpness may be related to the weight they assign to visual information in audiovisual speech perception. Specifically, listeners with less sharp categories assign more weight to visual information during audiovisual speech recognition",
    "checked": true,
    "id": "514fa68fb45771418b63ad021ca1b886bb607f47",
    "semantic_title": "audiovisual recalibration of vowel categories",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2017/peters17_interspeech.html": {
    "title": "The Effect of Gesture on Persuasive Speech",
    "volume": "main",
    "abstract": "Speech perception is multimodal, with not only speech, but also gesture presumably playing a role in how a message is perceived. However, there have not been many studies on the effect that hand gestures may have on speech perception in general, and on persuasive speech in particular. Moreover, we do not yet know whether an effect of gestures may be larger when addressees are not involved in the topic of the discourse, and are therefore more focused on peripheral cues, rather than the content of the message. In the current study participants were shown a speech with or without gestures. Some participants were involved in the topic of the speech, others were not. We studied five measures of persuasiveness. Results showed that for all but one measure, viewing the video with accompanying gestures made the speech more persuasive. In addition, there were several interactions, showing that the performance of the speaker and the factual accuracy of the speech scored high especially for those participants who not only saw gestures but were also not involved in the topic of the speech",
    "checked": true,
    "id": "d5f035bd94cc137960c922a0ebef42ec86ceffb4",
    "semantic_title": "the effect of gesture on persuasive speech",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lai17_interspeech.html": {
    "title": "Auditory-Visual Integration of Talker Gender in Cantonese Tone Perception",
    "volume": "main",
    "abstract": "This study investigated the auditory-visual integration of talker gender in the perception of tone variances. Two experiments were conducted to evaluate how listeners use the information of talker gender to adjust their expectation towards speakers' pitch range and uncover intended tonal targets in Cantonese tone perception. Results from an audio-only tone identification task showed that tone categorization along the same pitch continuum shifted under different conditions of voice gender. Listeners generally heard a tone of lower pitch when the word was produced by a female voice, while they heard a tone of higher pitch when the word was produced at the same pitch level by a male voice. Results from an audio-visual tone identification task showed that tone categorization along the same pitch continuum shifted under different conditions of face gender, despite the fact that the photos of different genders were disguised for the same set of stimuli in identical voices with identical pitch heights. These findings show that gender normalization plays a role in uncovering linguistic pitch targets, and lend support to a hypothesis according to which listeners make use of socially constructed stereotypes to facilitate their basic phonological categorization in speech perception and processing",
    "checked": true,
    "id": "63b50026b773732607a9e243925ff796909ab0b9",
    "semantic_title": "auditory-visual integration of talker gender in cantonese tone perception",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ito17_interspeech.html": {
    "title": "Event-Related Potentials Associated with Somatosensory Effect in Audio-Visual Speech Perception",
    "volume": "main",
    "abstract": "Speech perception often involves multisensory processing. Although previous studies have demonstrated visual [1, 2] and somatosensory interactions [3, 4] with auditory processing, it is not clear whether somatosensory information can contribute to the processing of audio-visual speech perception. This study explored the neural consequence of somatosensory interactions in audio-visual speech processing. We assessed whether somatosensory orofacial stimulation influenced event-related potentials (ERPs) in response to an audio-visual speech illusion (the McGurk Effect [1]). 64 scalp sites of ERPs were recorded in response to audio-visual speech stimulation and somatosensory stimulation. In the audio-visual condition, an auditory stimulus /ba/ was synchronized with the video of congruent facial motion (the production of /ba/) or incongruent facial motion (the production of the /da/: McGurk condition). These two audio-visual stimulations were randomly presented with and without somatosensory stimulation associated with facial skin deformation. We found ERPs differences associated with the McGurk effect in the presence of the somatosensory conditions. ERPs for the McGurk effect reliably diverge around 280 ms after auditory onset. The results demonstrate a change of cortical potential of audio-visual processing due to somatosensory inputs and suggest that somatosensory information encoding facial motion also influences speech processing",
    "checked": true,
    "id": "d4b6120e938858660d82e08233746d842ddbac49",
    "semantic_title": "event-related potentials associated with somatosensory effect in audio-visual speech perception",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2017/renner17_interspeech.html": {
    "title": "When a Dog is a Cat and How it Changes Your Pupil Size: Pupil Dilation in Response to Information Mismatch",
    "volume": "main",
    "abstract": "In the present study, we investigate pupil dilation as a measure of lexical retrieval. We captured pupil size changes in reaction to a match or a mismatch between a picture and an auditorily presented word in 120 trials presented to ten native speakers of Swedish. In each trial a picture was displayed for six seconds, and 2.5 seconds into the trial the word was played through loudspeakers. The picture and the word were matching in half of the trials, and all stimuli were common high-frequency monosyllabic Swedish words. The difference in pupil diameter trajectories across the two conditions was analyzed with Functional Data Analysis. In line with the expectations, the results indicate greater dilation in the mismatch condition starting from around 800 ms after the stimulus onset. Given that similar processes were observed in brain imaging studies, pupil dilation measurements seem to provide an appropriate tool to reveal lexical retrieval. The results suggest that pupillometry could be a viable alternative to existing methods in the field of speech and language processing, for instance across different ages and clinical groups",
    "checked": true,
    "id": "2eaf4130dccfd375cea7d1037f2215eceb0bb99b",
    "semantic_title": "when a dog is a cat and how it changes your pupil size: pupil dilation in response to information mismatch",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kyaw17_interspeech.html": {
    "title": "Cross-Modal Analysis Between Phonation Differences and Texture Images Based on Sentiment Correlations",
    "volume": "main",
    "abstract": "Motivated by the success of speech characteristics representation by color attributes, we analyzed the cross-modal sentiment correlations between voice source characteristics and textural image characteristics. For the analysis, we employed vowel sounds with representative three phonation differences (modal, creaky and breathy) and 36 texture images with 36 semantic attributes (e.g., banded, cracked and scaly) annotated one semantic attribute for each texture. By asking 40 subjects to select the most fitted textures from 36 figures with different textures after listening 30 speech samples with different phonations, we measured the correlations between acoustic parameters showing voice source variations and the parameters of selected textural image differences showing coarseness, contrast, directionality, busyness, complexity and strength. From the texture classifications, voice characteristics can be roughly characterized by textural differences: modal — gauzy, banded and smeared, creaky — porous, crystalline, cracked and scaly, breathy — smeared, freckled and stained. We have also found significant correlations between voice source acoustic parameters and textural parameters. These correlations suggest the possibility of cross-modal mapping between voice source characteristics and textural parameters, which enables visualization of speech information with source variations reflecting human sentiment perception",
    "checked": true,
    "id": "3abc020a625a5599bd5bb13043f7f22d52b0a487",
    "semantic_title": "cross-modal analysis between phonation differences and texture images based on sentiment correlations",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mehta17_interspeech.html": {
    "title": "Wireless Neck-Surface Accelerometer and Microphone on Flex Circuit with Application to Noise-Robust Monitoring of Lombard Speech",
    "volume": "main",
    "abstract": "Ambulatory monitoring of real-world voice characteristics and behavior has the potential to provide important assessment of voice and speech disorders and psychological and emotional state. In this paper, we report on the novel development of a lightweight, wireless voice monitor that synchronously records dual-channel data from an acoustic microphone and a neck-surface accelerometer embedded on a flex circuit. In this paper, Lombard speech effects were investigated in pilot data from four adult speakers with normal vocal function who read a phonetically balanced paragraph in the presence of different ambient acoustic noise levels. Whereas the signal-to-noise ratio (SNR) of the microphone signal decreased in the presence of increasing ambient noise level, the SNR of the accelerometer sensor remained high. Lombard speech properties were thus robustly computed from the accelerometer signal and observed in all four speakers who exhibited increases in average estimates of sound pressure level (+2.3 dB), fundamental frequency (+21.4 Hz), and cepstral peak prominence (+1.3 dB) from quiet to loud ambient conditions. Future work calls for ambulatory data collection in naturalistic environments, where the microphone acts as a sound level meter and the accelerometer functions as a noise-robust voicing sensor to assess voice disorders, neurological conditions, and cognitive load",
    "checked": true,
    "id": "1afc1f765d62b85cb8a82d0086ec1b95f2c5748a",
    "semantic_title": "wireless neck-surface accelerometer and microphone on flex circuit with application to noise-robust monitoring of lombard speech",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bandini17_interspeech.html": {
    "title": "Video-Based Tracking of Jaw Movements During Speech: Preliminary Results and Future Directions",
    "volume": "main",
    "abstract": "Facial (e.g., lips and jaw) movements can provide important information for the assessment, diagnosis and treatment of motor speech disorders. However, due to the high costs of the instrumentation used to record speech movements, such information is typically limited to research studies. With the recent development of depth sensors and efficient algorithms for facial tracking, clinical applications of this technology may be possible. Although lip tracking methods have been validated in the past, jaw tracking remains a challenge. In this study, we assessed the accuracy of tracking jaw movements with a video-based system composed of a face tracker and a depth sensor, specifically developed for short range applications (Intel RealSense SR300). The assessment was performed on healthy subjects during speech and non-speech tasks. Preliminary results showed that jaw movements can be tracked with reasonable accuracy (RMSE≈2mm), with better performance for slow movements. Further tests are needed in order to improve the performance of these systems and develop accurate methodologies that can reveal subtle changes in jaw movements for the assessment and treatment of motor speech disorders",
    "checked": true,
    "id": "f7428622667a1cc5b86fe83254136ad9db65da09",
    "semantic_title": "video-based tracking of jaw movements during speech: preliminary results and future directions",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sb17_interspeech.html": {
    "title": "Accurate Synchronization of Speech and EGG Signal Using Phase Information",
    "volume": "main",
    "abstract": "Synchronization of speech and corresponding Electroglottographic (EGG) signal is very helpful for speech processing research and development. During simultaneous recording of speech and EGG signals, the speech signal will be delayed by the duration corresponding to the speech wave propagation from the glottis to the microphone relative to the EGG signal. Even in same session of recording, the delay between the speech and the EGG signals is varying due to the natural movement of speaker's head and movement of microphone in case MIC is held by hand. To study and model the information within glottal cycles, precise synchronization of speech and EGG signals is of utmost necessity. In this work, we propose a method for synchronization of speech and EGG signals based on the glottal activity information present in the signals. The performance of the proposed method is demonstrated by estimation of delay between the two signals (speech signals and corresponding EGG signals) and synchronizing these signals by compensating the estimated delay. The CMU-Arctic database consist of simultaneous recording of the speech and the EGG signals is used for the evaluation of the proposed method",
    "checked": true,
    "id": "99f53c1117ead2974c5ff7729a77a6755809e133",
    "semantic_title": "accurate synchronization of speech and egg signal using phase information",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2017/romren17_interspeech.html": {
    "title": "The Acquisition of Focal Lengthening in Stockholm Swedish",
    "volume": "main",
    "abstract": "In order to be efficient communicators, children need to adapt their utterances to the common ground shared between themselves and their conversational partners. One way of doing this is by prosodically highlighting focal information. In this paper we look at one specific prosodic manipulation, namely word duration, asking whether Swedish-speaking children lengthen words to mark focus, as compared to adult controls. To the best of our knowledge, this is the first study on the relationship between focus and word duration in Swedish-speaking children",
    "checked": true,
    "id": "901106c767818b7405cf2f5dc0abf1c2f19504c8",
    "semantic_title": "the acquisition of focal lengthening in stockholm swedish",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhou17_interspeech.html": {
    "title": "Multilingual Recurrent Neural Networks with Residual Learning for Low-Resource Speech Recognition",
    "volume": "main",
    "abstract": "The shared-hidden-layer multilingual deep neural network (SHL-MDNN), in which the hidden layers of feed-forward deep neural network (DNN) are shared across multiple languages while the softmax layers are language dependent, has been shown to be effective on acoustic modeling of multilingual low-resource speech recognition. In this paper, we propose that the shared-hidden-layer with Long Short-Term Memory (LSTM) recurrent neural networks can achieve further performance improvement considering LSTM has outperformed DNN as the acoustic model of automatic speech recognition (ASR). Moreover, we reveal that shared-hidden-layer multilingual LSTM (SHL-MLSTM) with residual learning can yield additional moderate but consistent gain from multilingual tasks given the fact that residual learning can alleviate the degradation problem of deep LSTMs. Experimental results demonstrate that SHL-MLSTM can relatively reduce word error rate (WER) by 2.1–6.8% over SHL-MDNN trained using six languages and 2.6–7.3% over monolingual LSTM trained using the language specific data on CALLHOME datasets. Additional WER reduction, about relatively 2% over SHL-MLSTM, can be obtained through residual learning on CALLHOME datasets, which demonstrates residual learning is useful for SHL-MLSTM on multilingual low-resource ASR",
    "checked": true,
    "id": "da650416734a210981b4cdcda63d9dc0f920219a",
    "semantic_title": "multilingual recurrent neural networks with residual learning for low-resource speech recognition",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2017/siohan17_interspeech.html": {
    "title": "CTC Training of Multi-Phone Acoustic Models for Speech Recognition",
    "volume": "main",
    "abstract": "Phone-sized acoustic units such as triphones cannot properly capture the long-term co-articulation effects that occur in spontaneous speech. For that reason, it is interesting to construct acoustic units covering a longer time-span such as syllables or words. Unfortunately, the frequency distribution of those units is such that a few high frequency units account for most of the tokens, while many units rarely occur. As a result, those units suffer from data sparsity and can be difficult to train. In this paper we propose a scalable data-driven approach to construct a set of salient units made of sequences of phones called M-phones. We illustrate that since the decomposition of a word sequence into a sequence of M-phones is ambiguous, those units are well suited to be used with a connectionist temporal classification (CTC) approach which does not rely on an explicit frame-level segmentation of the word sequence into a sequence of acoustic units. Experiments are presented on a Voice Search task using 12,500 hours of training data",
    "checked": true,
    "id": "15067b905682139b5c4d0f642bb019249923a56b",
    "semantic_title": "ctc training of multi-phone acoustic models for speech recognition",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tong17_interspeech.html": {
    "title": "An Investigation of Deep Neural Networks for Multilingual Speech Recognition Training and Adaptation",
    "volume": "main",
    "abstract": "Different training and adaptation techniques for multilingual Automatic Speech Recognition (ASR) are explored in the context of hybrid systems, exploiting Deep Neural Networks (DNN) and Hidden Markov Models (HMM). In multilingual DNN training, the hidden layers (possibly extracting bottleneck features) are usually shared across languages, and the output layer can either model multiple sets of language-specific senones or one single universal IPA-based multilingual senone set. Both architectures are investigated, exploiting and comparing different language adaptive training (LAT) techniques originating from successful DNN-based speaker-adaptation. More specifically, speaker adaptive training methods such as Cluster Adaptive Training (CAT) and Learning Hidden Unit Contribution (LHUC) are considered. In addition, a language adaptive output architecture for IPA-based universal DNN is also studied and tested Experiments show that LAT improves the performance and adaptation on the top layer further improves the accuracy. By combining state-level minimum Bayes risk (sMBR) sequence training with LAT, we show that a language adaptively trained IPA-based universal DNN outperforms a monolingually sequence trained model",
    "checked": true,
    "id": "190b71beb0b27fdcf596812b2d935fa3e10a1092",
    "semantic_title": "an investigation of deep neural networks for multilingual speech recognition training and adaptation",
    "citation_count": 39
  },
  "https://www.isca-speech.org/archive/interspeech_2017/karafiat17_interspeech.html": {
    "title": "2016 BUT Babel System: Multilingual BLSTM Acoustic Model with i-Vector Based Adaptation",
    "volume": "main",
    "abstract": "The paper provides an analysis of BUT automatic speech recognition systems (ASR) built for the 2016 IARPA Babel evaluation. The IARPA Babel program concentrates on building ASR system for many low resource languages, where only a limited amount of transcribed speech is available for each language. In such scenario, we found essential to train the ASR systems in a multilingual fashion. In this work, we report superior results obtained with pre-trained multilingual BLSTM acoustic models, where we used multi-task training with separate classification layer for each language. The results reported on three Babel Year 4 languages show over 3% absolute WER reductions obtained from such multilingual pre-training. Experiments with different input features show that the multilingual BLSTM performs the best with simple log-Mel-filter-bank outputs, which makes our previously successful multilingual stack bottleneck features with CMLLR adaptation obsolete. Finally, we experiment with different configurations of i-vector based speaker adaptation in the mono- and multi-lingual BLSTM architectures. This results in additional WER reductions over 1% absolute",
    "checked": true,
    "id": "ea174d36c2d3ebc42664455064c676b5df866235",
    "semantic_title": "2016 but babel system: multilingual blstm acoustic model with i-vector based adaptation",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2017/matassoni17_interspeech.html": {
    "title": "Optimizing DNN Adaptation for Recognition of Enhanced Speech",
    "volume": "main",
    "abstract": "Speech enhancement directly using deep neural network (DNN) is of major interest due to the capability of DNN to tangibly reduce the impact of noisy conditions in speech recognition tasks. Similarly, DNN based acoustic model adaptation to new environmental conditions is another challenging topic. In this paper we present an analysis of acoustic model adaptation in presence of a disjoint speech enhancement component, identifying an optimal setting for improving the speech recognition performance. Adaptation is derived from a consolidated technique that introduces in the training process a regularization term to prevent overfitting. We propose to optimize the adaptation of the clean acoustic models towards the enhanced speech by tuning the regularization term based on the degree of enhancement. Experiments on a popular noisy dataset (e.g., AURORA-4) demonstrate the validity of the proposed approach",
    "checked": true,
    "id": "3c873111fbae4f11f6f2c0d817540dbb7b2d7409",
    "semantic_title": "optimizing dnn adaptation for recognition of enhanced speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kim17c_interspeech.html": {
    "title": "Deep Least Squares Regression for Speaker Adaptation",
    "volume": "main",
    "abstract": "Recently, speaker adaptation methods in deep neural networks (DNNs) have been widely studied for automatic speech recognition. However, almost all adaptation methods for DNNs have to consider various heuristic conditions such as mini-batch sizes, learning rate scheduling, stopping criteria, and initialization conditions because of the inherent property of a stochastic gradient descent (SGD)-based training process. Unfortunately, those heuristic conditions are hard to be properly tuned. To alleviate those difficulties, in this paper, we propose a least squares regression-based speaker adaptation method in a DNN framework utilizing posterior mean of each class. Also, we show how the proposed method can provide a unique solution which is quite easy and fast to calculate without SGD. The proposed method was evaluated in the TED-LIUM corpus. Experimental results showed that the proposed method achieved up to a 4.6% relative improvement against a speaker independent DNN. In addition, we report further performance improvement of the proposed method with speaker-adapted features",
    "checked": true,
    "id": "1dc883dbe0d5545fd3513f45fbc730cca2f9bbe8",
    "semantic_title": "deep least squares regression for speaker adaptation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/do17_interspeech.html": {
    "title": "Multi-Task Learning Using Mismatched Transcription for Under-Resourced Speech Recognition",
    "volume": "main",
    "abstract": "It is challenging to obtain large amounts of native (matched) labels for audio in under-resourced languages. This could be due to a lack of literate speakers of the language or a lack of universally acknowledged orthography. One solution is to increase the amount of labeled data by using mismatched transcription, which employs transcribers who do not speak the language (in place of native speakers), to transcribe what they hear as nonsense speech in their own language (e.g., Mandarin). This paper presents a multi-task learning framework where the DNN acoustic model is simultaneously trained using both a limited amount of native (matched) transcription and a larger set of mismatched transcription. We find that by using a multi-task learning framework, we achieve improvements over monolingual baselines and previously proposed mismatched transcription adaptation techniques. In addition, we show that using alignments provided by a GMM adapted by mismatched transcription further improves acoustic modeling performance. Our experiments on Georgian data from the IARPA Babel program show the effectiveness of the proposed method",
    "checked": true,
    "id": "0fdf5679df010da76a4510006e40d8742942952e",
    "semantic_title": "multi-task learning using mismatched transcription for under-resourced speech recognition",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2017/joy17_interspeech.html": {
    "title": "Generalized Distillation Framework for Speaker Normalization",
    "volume": "main",
    "abstract": "Generalized distillation framework has been shown to be effective in speech enhancement in the past. We extend this idea to speaker normalization without any explicit adaptation data in this paper. In the generalized distillation framework, we assume the presence of some \"privileged\" information to guide the training process in addition to the training data. In the proposed approach, the privileged information is obtained from a \"teacher\" model, trained on speaker-normalized FMLLR features. The \"student\" model is trained on un-normalized filterbank features and uses teacher's supervision for cross-entropy training. The proposed distillation method does not need first pass decode information during testing and imposes no constraints on the duration of the test data for computing speaker-specific transforms unlike in FMLLR or i-vector. Experiments done on Switchboard and AMI corpus show that the generalized distillation framework shows improvement over un-normalized features with or without i-vectors",
    "checked": true,
    "id": "2904a5c43940a2d1da84d4c4a387cb17de987ffb",
    "semantic_title": "generalized distillation framework for speaker normalization",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2017/samarakoon17_interspeech.html": {
    "title": "Learning Factorized Transforms for Unsupervised Adaptation of LSTM-RNN Acoustic Models",
    "volume": "main",
    "abstract": "Factorized Hidden Layer (FHL) adaptation has been proposed for speaker adaptation of deep neural network (DNN) based acoustic models. In FHL adaptation, a speaker-dependent (SD) transformation matrix and an SD bias are included in addition to the standard affine transformation. The SD transformation is a linear combination of rank-1 matrices whereas the SD bias is a linear combination of vectors. Recently, the Long Short-Term Memory (LSTM) Recurrent Neural Networks (RNNs) have shown to outperform DNN acoustic models in many Automatic Speech Recognition (ASR) tasks. In this work, we investigate the effectiveness of SD transformations for LSTM-RNN acoustic models. Experimental results show that when combined with scaling of LSTM cell states' outputs, SD transformations achieve 2.3% and 2.1% absolute improvements over the baseline LSTM systems for the AMI IHM and AMI SDM tasks respectively",
    "checked": true,
    "id": "9329958df2f1216af0235a0bca6c0e34b048b37a",
    "semantic_title": "learning factorized transforms for unsupervised adaptation of lstm-rnn acoustic models",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2017/fainberg17_interspeech.html": {
    "title": "Factorised Representations for Neural Network Adaptation to Diverse Acoustic Environments",
    "volume": "main",
    "abstract": "Adapting acoustic models jointly to both speaker and environment has been shown to be effective. In many realistic scenarios, however, either the speaker or environment at test time might be unknown, or there may be insufficient data to learn a joint transform. Generating independent speaker and environment transforms improves the match of an acoustic model to unseen combinations. Using i-vectors, we demonstrate that it is possible to factorise speaker or environment information using multi-condition training with neural networks. Specifically, we extract bottleneck features from networks trained to classify either speakers or environments. We perform experiments on the Wall Street Journal corpus combined with environment noise from the Diverse Environments Multichannel Acoustic Noise Database. Using the factorised i-vectors we show improvements in word error rates on perturbed versions of the eval92 and dev93 test sets, both when one factor is missing and when the factors are seen but not in the desired combination",
    "checked": true,
    "id": "77051beda83f2dd1828b2f1cac28a259b698ad14",
    "semantic_title": "factorised representations for neural network adaptation to diverse acoustic environments",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sproat17_interspeech.html": {
    "title": "An RNN Model of Text Normalization",
    "volume": "main",
    "abstract": "We present a recurrent neural net (RNN) model of text normalization — defined as the mapping of written text to its spoken form, and a description of the open-source dataset that we used in our experiments. We show that while the RNN model achieves very high overall accuracies, there remain errors that would be unacceptable in a speech application like TTS. We then show that a simple FST-based filter can help mitigate those errors. Even with that mitigation challenges remain, and we end the paper outlining some possible solutions. In releasing our data we are thereby inviting others to help solve this problem",
    "checked": true,
    "id": "499225b952b07877f3767415a0094406a4b9ffb8",
    "semantic_title": "an rnn model of text normalization",
    "citation_count": 42
  },
  "https://www.isca-speech.org/archive/interspeech_2017/rendel17_interspeech.html": {
    "title": "Weakly-Supervised Phrase Assignment from Text in a Speech-Synthesis System Using Noisy Labels",
    "volume": "main",
    "abstract": "The proper segmentation of an input text string into meaningful intonational phrase units is a fundamental task in the text-processing component of a text-to-speech (TTS) system that generates intelligible and natural synthesis. In this work we look at the creation of a symbolic, phrase-assignment model within the front end (FE) of a North American English TTS system when high-quality labels for supervised learning are unavailable and/or potentially mismatched to the target corpus and domain. We explore a labeling scheme that merges heuristics derived from (i) automatic high-quality phonetic alignments, (ii) linguistic rules, and (iii) a legacy acoustic phrase-labeling system to arrive at a ground truth that can be used to train a bidirectional recurrent neural network model. We evaluate the performance of this model in terms of objective metrics describing categorical phrase assignment within the FE proper, as well as on the effect that these intermediate labels carry onto the TTS back end for the task of continuous prosody prediction (i.e., intonation and duration contours, and pausing). For this second task, we rely on subjective listening tests and demonstrate that the proposed system significantly outperforms a linguistic rules-based baseline for two different synthetic voices",
    "checked": true,
    "id": "f7ffa465ab9980126d74a16c628a2235189861b7",
    "semantic_title": "weakly-supervised phrase assignment from text in a speech-synthesis system using noisy labels",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ijima17_interspeech.html": {
    "title": "Prosody Aware Word-Level Encoder Based on BLSTM-RNNs for DNN-Based Speech Synthesis",
    "volume": "main",
    "abstract": "Recent studies have shown the effectiveness of the use of word vectors in DNN-based speech synthesis. However, these word vectors trained from a large amount of text generally carry not prosodic information, which is important information for speech synthesis, but semantic information. Therefore, if word vectors that take prosodic information into account can be obtained, it would be expected to improve the quality of synthesized speech. In this paper, to obtain word-level vectors that take prosodic information into account, we propose a novel prosody aware word-level encoder. A novel point of the proposed technique is to train a word-level encoder by using a large speech corpus constructed for automatic speech recognition. A word-level encoder that estimates the F0 contour for each word from the input word sequence is trained. The outputs of the bottleneck layer in the trained encoder are used as the word-level vector. By training the relationship between words and their prosodic information by using large speech corpus, the outputs of the bottleneck layer would be expected to contain prosodic information. The results of objective and subjective experiments indicate the proposed technique can synthesize speech with improved naturalness",
    "checked": true,
    "id": "b0e758bd38511b13e2712ad3a013633af7fb682c",
    "semantic_title": "prosody aware word-level encoder based on blstm-rnns for dnn-based speech synthesis",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ni17_interspeech.html": {
    "title": "Global Syllable Vectors for Building TTS Front-End with Deep Learning",
    "volume": "main",
    "abstract": "Recent vector space representations of words have succeeded in capturing syntactic and semantic regularities. In the context of text-to-speech (TTS) synthesis, a front-end is a key component for extracting multi-level linguistic features from text, where syllable acts as a link between low- and high-level features. This paper describes the use of global syllable vectors as features to build a front-end, particularly evaluated in Chinese. The global syllable vectors directly capture global statistics of syllable-syllable co-occurrences in a large-scale text corpus. They are learned by a global log-bilinear regression model in an unsupervised manner, whilst the front-end is built using deep bidirectional recurrent neural networks in a supervised fashion. Experiments are conducted on large-scale Chinese speech and treebank text corpora, evaluating grapheme to phoneme (G2P) conversion, word segmentation, part of speech (POS) tagging, phrasal chunking, and pause break prediction. Results show that the proposed method is efficient for building a compact and robust front-end with high performance. The global syllable vectors can be acquired relatively cheaply from plain text resources, therefore, they are vital to develop multilingual speech synthesis, especially for under-resourced language modeling",
    "checked": true,
    "id": "4173177f47461dafca636777c60bd1e12aab6e62",
    "semantic_title": "global syllable vectors for building tts front-end with deep learning",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2017/fukuoka17_interspeech.html": {
    "title": "Prosody Control of Utterance Sequence for Information Delivering",
    "volume": "main",
    "abstract": "We propose a conversational speech synthesis system in which the prosodic features of each utterance are controlled throughout the entire input text. We have developed a \"news-telling system,\" which delivered news articles through spoken language. The speech synthesis system for the news-telling should be able to highlight utterances containing noteworthy information in the article with a particular way of speaking so as to impress them on the users. To achieve this, we introduced role and position features of the individual utterances in the article into the control parameters for prosody generation throughout the text. We defined three categories for the role feature: a nucleus (which is assigned to the utterance including the noteworthy information), a front satellite (which precedes the nucleus) and a rear satellite (which follows the nucleus). We investigated how the prosodic features differed depending on the role and position features through an analysis of news-telling speech data uttered by a voice actress. We designed the speech synthesis system on the basis of a deep neural network having the role and position features added to its input layer. Objective and subjective evaluation results showed that introducing those features was effective in the speech synthesis for the information delivering",
    "checked": true,
    "id": "0cb0cb06d8303499ad080a0f49c4e96c0f857093",
    "semantic_title": "prosody control of utterance sequence for information delivering",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2017/huang17c_interspeech.html": {
    "title": "Multi-Task Learning for Prosodic Structure Generation Using BLSTM RNN with Structured Output Layer",
    "volume": "main",
    "abstract": "Prosodic structure generation from text plays an important role in Chinese text-to-speech (TTS) synthesis, which greatly influences the naturalness and intelligibility of the synthesized speech. This paper proposes a multi-task learning method for prosodic structure generation using bidirectional long short-term memory (BLSTM) recurrent neural network (RNN) and structured output layer (SOL). Unlike traditional methods where prerequisites such as lexicon word or even syntactic tree are usually required as the input, the proposed method predicts prosodic boundary labels directly from Chinese characters. BLSTM RNN is used to capture the bidirectional contextual dependencies of prosodic boundary labels. SOL further models correlations between prosodic structures, lexicon words as well as part-of-speech (POS), where the prediction of prosodic boundary labels are conditioned upon word tokenization and POS tagging results. Experimental results demonstrate the effectiveness of the proposed method",
    "checked": true,
    "id": "1b8372912fd6bde2f0d213bc31bee12f0a40a417",
    "semantic_title": "multi-task learning for prosodic structure generation using blstm rnn with structured output layer",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zheng17_interspeech.html": {
    "title": "Investigating Efficient Feature Representation Methods and Training Objective for BLSTM-Based Phone Duration Prediction",
    "volume": "main",
    "abstract": "Accurate modeling and prediction of speech-sound durations are important in generating natural synthetic speech. This paper focuses on both feature and training objective aspects to improve the performance of the phone duration model for speech synthesis system. In feature aspect, we combine the feature representation from gradient boosting decision tree (GBDT) and phoneme identity embedding model (which is realized by the jointly training of phoneme embedded vector (PEV) and word embedded vector (WEV)) for BLSTM to predict the phone duration. The PEV is used to replace the one-hot phoneme identity, and GBDT is utilized to transform the traditional contextual features. In the training objective aspect, a new training objective function which taking into account of the correlation and consistency between the predicted utterance and the natural utterance is proposed. Perceptual tests indicate the proposed methods could improve the naturalness of the synthetic speech, which benefits from the proposed feature representation methods could capture more precise contextual features, and the proposed training objective function could tackle the over-averaged problem for the generated phone durations",
    "checked": true,
    "id": "036c8943a91ee5749dd4a4d8e6e44b3fb54420d5",
    "semantic_title": "investigating efficient feature representation methods and training objective for blstm-based phone duration prediction",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17d_interspeech.html": {
    "title": "Discrete Duration Model for Speech Synthesis",
    "volume": "main",
    "abstract": "The acoustic model and the duration model are the two major components in statistical parametric speech synthesis (SPSS) systems. The neural network based acoustic model makes it possible to model phoneme duration at phone-level instead of state-level in conventional hidden Markov model (HMM) based SPSS systems. Since the duration of phonemes is countable value, the distribution of the phone-level duration is discrete given the linguistic features, which means the Gaussian hypothesis is no longer necessary. This paper provides an investigation on the performance of LSTM-RNN duration model that directly models the probability of the countable duration values given linguistic features using cross entropy as criteria. The multi-task learning is also experimented at the same time, with a comparison to the standard LSTM-RNN duration model in objective and subjective measures. The result shows that directly modeling the discrete distribution has its benefit and multi-task model achieves better performance in phone-level duration modeling",
    "checked": true,
    "id": "4d5a73b160ed297050b0b388ae88012350cdd9ba",
    "semantic_title": "discrete duration model for speech synthesis",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17e_interspeech.html": {
    "title": "Comparison of Modeling Target in LSTM-RNN Duration Model",
    "volume": "main",
    "abstract": "Speech duration is an important component in statistical parameter speech synthesis(SPSS). In LSTM-RNN based SPSS system, the speech duration affects the quality of synthesized speech in two aspects, the prosody of speech and the position features in acoustic model. This paper investigated the effects of duration in LSTM-RNN based SPSS system. The performance of the acoustic models with position features at different levels are compared. Also, duration models with different network architectures are presented. A method to utilize the priori knowledge that the sum of state duration of a phoneme should be equal to the phone duration is proposed and proved to have better performance in both state duration and phone duration modeling. The result shows that acoustic model with state-level position features has better performance in acoustic modeling (especially in voice/unvoice classification), which means state-level duration model still has its advantage and the duration models with the priori knowledge can result in better speech quality",
    "checked": true,
    "id": "6a58a50cef15d59925fb888390a4226d5d32c3d5",
    "semantic_title": "comparison of modeling target in lstm-rnn duration model",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ribeiro17_interspeech.html": {
    "title": "Learning Word Vector Representations Based on Acoustic Counts",
    "volume": "main",
    "abstract": "This paper presents a simple count-based approach to learning word vector representations by leveraging statistics of co-occurrences between text and speech. This type of representation requires two discrete sequences of units defined across modalities. Two possible methods for the discretization of an acoustic signal are presented, which are then applied to fundamental frequency and energy contours of a transcribed corpus of speech, yielding a sequence of textual objects (e.g. words, syllables) aligned with a sequence of discrete acoustic events. Constructing a matrix recording the co-occurrence of textual objects with acoustic events and reducing its dimensionality with matrix decomposition results in a set of context-independent representations of word types. These are applied to the task of acoustic modelling for speech synthesis; objective and subjective results indicate that these representations are useful for the generation of acoustic parameters in a text-to-speech (TTS) system. In general, we observe that the more discretization approaches, acoustic signals, and levels of linguistic analysis are incorporated into a TTS system via these count-based representations, the better that TTS system performs",
    "checked": true,
    "id": "c0d3d9f3c1f0e019663ab2c5c450355cd7f3152c",
    "semantic_title": "learning word vector representations based on acoustic counts",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2017/szekely17_interspeech.html": {
    "title": "Synthesising Uncertainty: The Interplay of Vocal Effort and Hesitation Disfluencies",
    "volume": "main",
    "abstract": "As synthetic voices become more flexible, and conversational systems gain more potential to adapt to the environmental and social situation, the question needs to be examined, how different modifications to the synthetic speech interact with each other and how their specific combinations influence perception. This work investigates how the vocal effort of the synthetic speech together with added disfluencies affect listeners' perception of the degree of uncertainty in an utterance. We introduce a DNN voice built entirely from spontaneous conversational speech data and capable of producing a continuum of vocal efforts, prolongations and filled pauses with a corpus-based method. Results of a listener evaluation indicate that decreased vocal effort, filled pauses and prolongation of function words increase the degree of perceived uncertainty of conversational utterances expressing the speaker's beliefs. We demonstrate that the effect of these three cues are not merely additive, but that interaction effects, in particular between the two types of disfluencies and between vocal effort and prolongations need to be considered when aiming to communicate a specific level of uncertainty. The implications of these findings are relevant for adaptive and incremental conversational systems using expressive speech synthesis and aspiring to communicate the attitude of uncertainty",
    "checked": true,
    "id": "6398aee06c840d0332fb90853bcaa3b80ab9f660",
    "semantic_title": "synthesising uncertainty: the interplay of vocal effort and hesitation disfluencies",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2017/oktem17_interspeech.html": {
    "title": "Prosograph: A Tool for Prosody Visualisation of Large Speech Corpora",
    "volume": "main",
    "abstract": "This paper presents an open-source tool that has been developed to visualize a speech corpus with its transcript and prosodic features aligned at word level. In particular, the tool is aimed at providing a simple and clear way to visualize prosodic patterns along large segments of speech corpora, and can be applied in any research that involves prosody analysis",
    "checked": true,
    "id": "e33fc523c7677de80f610fd9920dcaab5ee393d4",
    "semantic_title": "prosograph: a tool for prosody visualisation of large speech corpora",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2017/vetchinnikova17_interspeech.html": {
    "title": "ChunkitApp: Investigating the Relevant Units of Online Speech Processing",
    "volume": "main",
    "abstract": "This paper presents a web-based application for tablets ‘ChunkitApp' developed to investigate chunking in online speech processing. The design of the app is based on recent theoretical developments in linguistics and cognitive science, and in particular on the suggestions of Linear Unit Grammar [1]. The data collected using the app provides evidence for the reality of online chunking in language processing and the validity of the construct. In addition to experimental uses, the app has potential applications in language education and speech recognition",
    "checked": true,
    "id": "4fb5c8bbfdacb088e44912fe6cf56493794d79b2",
    "semantic_title": "chunkitapp: investigating the relevant units of online speech processing",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jochim17_interspeech.html": {
    "title": "Extending the EMU Speech Database Management System: Cloud Hosting, Team Collaboration, Automatic Revision Control",
    "volume": "main",
    "abstract": "In this paper, we introduce a new component of the EMU Speech Database Management System [1, 2] to improve the team workflow of handling production data (both acoustic and physiological) in phonetics and the speech sciences. It is named emuDB Manager, and it facilitates the coordination of team efforts, possibly distributed over several nations, by introducing automatic revision control (based on Git), cloud hosting (in private clouds provided by the researchers themselves or a third party), by keeping track of which parts of the database have already been edited (and by whom), and by centrally collecting and making searchable the notes made during the edit process",
    "checked": true,
    "id": "9debe14abe9acb069f9998dbd73b44f7d965efa8",
    "semantic_title": "extending the emu speech database management system: cloud hosting, team collaboration, automatic revision control",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2017/warlaumont17_interspeech.html": {
    "title": "HomeBank: A Repository for Long-Form Real-World Audio Recordings of Children",
    "volume": "main",
    "abstract": "HomeBank is a new component of the TalkBank system, focused on long-form (i.e., multi-hour, typically daylong) real-world recordings of children's language experiences, and it is linked to a GitHub repository in which tools for analyzing those recordings can be shared. HomeBank constitutes not only a rich resource for researchers interested in early language acquisition specifically, but also for those seeking to study spontaneous speech, media exposure, and audio environments more generally. This Show and Tell describes the procedures for accessing and contributing HomeBank data and code. It also overviews the current contents of the repositories, and provides some examples of audio recordings, available transcriptions, and currently available analysis tools",
    "checked": true,
    "id": "dff69c0bf85eda41acd1a0e3f98e00a02914aba0",
    "semantic_title": "homebank: a repository for long-form real-world audio recordings of children",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bell17_interspeech.html": {
    "title": "A System for Real Time Collaborative Transcription Correction",
    "volume": "main",
    "abstract": "We present a system to enable efficient, collaborative human correction of ASR transcripts, designed to operate in real-time situations, for example, when post-editing live captions generated for news broadcasts. In the system, confusion networks derived from ASR lattices are used to highlight low-confident words and present alternatives to the user for quick correction. The system uses a client-server architecture, whereby information about each manual edit is posted to the server. Such information can be used to dynamically update the one-best ASR output for all utterances currently in the editing pipeline. We propose to make updates in three different ways; by finding a new one-best path through an existing ASR lattice consistent with the correction received; by identifying further instances of out-of-vocabulary terms entered by the user; and by adapting the language model on the fly. Updates are received asynchronously by the client",
    "checked": true,
    "id": "2b5322873c60424d7f8442951bff2471d3248e00",
    "semantic_title": "a system for real time collaborative transcription correction",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bhat17_interspeech.html": {
    "title": "MoPAReST — Mobile Phone Assisted Remote Speech Therapy Platform",
    "volume": "main",
    "abstract": "Through this paper, we present the Mobile Phone Assisted Remote Speech Therapy Platform for individuals with speech disabilities to avail the benefits of therapy remotely with minimal face-to-face sessions with the Speech Language Pathologist (SLP). The objective is to address the skewed ratio of SLP to patients as well increase the efficacy of the therapy by keeping the patient engaged more frequently albeit asynchronously and remotely. The platform comprises (1) A web-interface to be used by the SLP to monitor the progress of their patients at a time convenient to them and (2) A mobile application along with speech processing algorithms to provide instant feedback to the patient. We envision this platform to cut down the therapy time, especially for rural Indian patients. Evaluation of this platform is being done for five patients with mis-articulation in Marathi language",
    "checked": false,
    "id": "af5e8db3a5daa43fafe6ede33d9b7578b04e09b1",
    "semantic_title": "moparest - mobile phone assisted remote speech therapy platform",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jaumardhakoun17_interspeech.html": {
    "title": "An Apparatus to Investigate Western Opera Singing Skill Learning Using Performance and Result Biofeedback, and Measuring its Neural Correlates",
    "volume": "main",
    "abstract": "We present our preliminary developments on a biofeedback interface for Western operatic style training, combining performance and result biofeedback. Electromyographic performance feedbacks, as well as formant-tuning result feedbacks are displayed visually, using continuously scrolling displays, or discrete post-trial evaluations. Our final aim is to investigate electroencephalographic (EEG) measurements in order to identify neural correlates of feedback-based skill learning",
    "checked": true,
    "id": "502dd2534e6233fd0165e57fac527b3585a99818",
    "semantic_title": "an apparatus to investigate western opera singing skill learning using performance and result biofeedback, and measuring its neural correlates",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2017/draxler17_interspeech.html": {
    "title": "PercyConfigurator — Perception Experiments as a Service",
    "volume": "main",
    "abstract": "PercyConfigurator is an experiment editor that eliminates the need for programming; the experiment definition and content are simply dropped onto the PercyConfigurator web page for interactive editing and testing. When the editing is done, the experiment definition and content are uploaded to the server. The server returns a link to the experiment which is then distributed to potential participants The Bavarian Archive for Speech Signals (BAS) hosts PercyConfigurator as a free service to the academic community",
    "checked": false,
    "id": "fe61af3a82cc996b78e0f7bce4b02ce9c12985a0",
    "semantic_title": "percyconfigurator - perception experiments as a service",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/salimbajevs17_interspeech.html": {
    "title": "System for Speech Transcription and Post-Editing in Microsoft Word",
    "volume": "main",
    "abstract": "In this demonstration paper, we introduce a transcription service that can be used for transcription of different meetings, sessions etc. The service performs speaker diarization, automatic speech recognition, punctuation restoration and produces human-readable transcripts as special Microsoft Word documents that have audio and word alignments embedded. Thereby, a widely-used word processor is transformed into a transcription post-editing tool. Currently, Latvian and Lithuanian languages are supported, but other languages can be easily added",
    "checked": true,
    "id": "d0942abd9a3f1d27811d58b0330b8c2eea91fd86",
    "semantic_title": "system for speech transcription and post-editing in microsoft word",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2017/park17_interspeech.html": {
    "title": "Emojive! Collecting Emotion Data from Speech and Facial Expression Using Mobile Game App",
    "volume": "main",
    "abstract": "We developed Emojive!, a mobile game app to make emotion recognition from audio and image interactive and fun, motivating the users to play with the app. The game is to act out a specific emotion, among six emotion labels (happy, sad, anger, anxiety, loneliness, criticism), given by the system. Double player mode lets two people to compete their acting skills. The more users play the game, the more emotion-labelled data will be acquired. We are using deep Convolutional Neural Network (CNN) models to recognize emotion from audio and facial image in real-time with a mobile front-end client including intuitive user interface and simple data visualization",
    "checked": true,
    "id": "fea0f385a0023c7096fcd7c9e2109d9e3358b03a",
    "semantic_title": "emojive! collecting emotion data from speech and facial expression using mobile game app",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lennes17_interspeech.html": {
    "title": "Mylly — The Mill: A New Platform for Processing Speech and Text Corpora Easily and Efficiently",
    "volume": "main",
    "abstract": "Speech and language researchers need to manage and analyze increasing quantities of material. Various tools are available for various stages of the work, but they often require the researcher to use different interfaces and to convert the output from each tool into suitable input for the next one The Language Bank of Finland (Kielipankki) is developing an on-line platform called Mylly for processing speech and language data in a graphical user interface that integrates different tools into a single workflow. Mylly provides tools and computational resources for processing material and for the inspecting the results. The tools plugged into Mylly include a parser, morphological analyzers, generic finite-state technology, and a speech recognizer. Users can upload data and download any intermediate results in the tool chain. Mylly runs on CSC's Taito cluster and is an instance of the Chipster platform. Access rights to Mylly are given for academic use The Language Bank of Finland is a collection of corpora, tools and other services maintained by FIN-CLARIN, a consortium of Finnish universities and research organizations coordinated by the University of Helsinki. The technological infrastructure for the Language Bank of Finland is provided by CSC – IT Center for Science",
    "checked": false,
    "id": "b42c184106e4be7938df15c2f21afdb384917b28",
    "semantic_title": "mylly - the mill: a new platform for processing speech and text corpora easily and efficiently",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/suzuki17_interspeech.html": {
    "title": "Visual Learning 2: Pronunciation App Using Ultrasound, Video, and MRI",
    "volume": "main",
    "abstract": "We demonstrate Visual Learning 2, an English pronunciation app for second-language (L2) learners and phonetics students. This iOS app links together audio, front and side video, MRI and ultrasound movies of a native speaker reading a phonetically balanced text. Users can watch and shadow front and side video overlaid with an ultrasound tongue movie. They are able to play the video at three speeds and start the video from any word by tapping on it, with a choice of display in either English or IPA. Users can record their own audio/video and play it back in sync with the model for comparison",
    "checked": true,
    "id": "ea7de7b9ae7d06832913cdf671849969b9184a64",
    "semantic_title": "visual learning 2: pronunciation app using ultrasound, video, and mri",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2017/allen17_interspeech.html": {
    "title": "Dialogue as Collaborative Problem Solving",
    "volume": "main",
    "abstract": "I will describe the current status of a long-term effort at developing dialogue systems that go beyond simple task execution models to systems that involve collaborative problem solving. Such systems involve open-ended discussion and the tasks cannot be accomplished without extensive interaction (e.g., 10 turns or more). The key idea is that dialogue itself arises from an agent's ability for collaborative problem solving (CPS). In such dialogues, agents may introduce, modify and negotiate goals; propose and discuss the merits possible paths to solutions; explicitly discuss progress as the two agents work towards the goals; and evaluate how well a goal was accomplished. To complicate matters, user utterances in such settings are much more complex than seen in simple task execution dialogues and requires full semantic parsing. A key question we have been exploring in the past few years is how much of dialogue can be accounted for by domain-independent mechanisms. I will discuss these issues and draw examples from a dialogue system we have built that, except for the specialized domain reasoning required in each case, uses the same architecture to perform three different tasks: collaborative blocks world planning, when the system and user build structures and may have differing goals; biocuration, in which a biologist and the system interact in order to build executable causal models of biological pathways; and collaborative composition, where the user and system collaborate to compose simple pieces of music",
    "checked": true,
    "id": "c7c65d31949aacbc83921e280e5f36e6c808a808",
    "semantic_title": "dialogue as collaborative problem solving",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2017/stasak17_interspeech.html": {
    "title": "Elicitation Design for Acoustic Depression Classification: An Investigation of Articulation Effort, Linguistic Complexity, and Word Affect",
    "volume": "main",
    "abstract": "Assessment of neurological and psychiatric disorders like depression are unusual from a speech processing perspective, in that speakers can be prompted or instructed in what they should say (e.g. as part of a clinical assessment). Despite prior speech-based depression studies that have used a variety of speech elicitation methods, there has been little evaluation of the best elicitation mode. One approach to understand this better is to analyze an existing database from the perspective of articulation effort, word affect, and linguistic complexity measures as proxies for depression sub-symptoms (e.g. psychomotor retardation, negative stimulus suppression, cognitive impairment). Here a novel measure for quantifying articulation effort is introduced, and when applied experimentally to the DAIC corpus shows promise for identifying speech data that are more discriminative of depression. Interestingly, experiment results demonstrate that by selecting speech with higher articulation effort, linguistic complexity, or word-based arousal/valence, improvements in acoustic speech-based feature depression classification performance can be achieved, serving as a guide for future elicitation design",
    "checked": true,
    "id": "46daf35c87f68efafb6352926a99677104319d39",
    "semantic_title": "elicitation design for acoustic depression classification: an investigation of articulation effort, linguistic complexity, and word affect",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2017/novoa17_interspeech.html": {
    "title": "Robustness Over Time-Varying Channels in DNN-HMM ASR Based Human-Robot Interaction",
    "volume": "main",
    "abstract": "This paper addresses the problem of time-varying channels in speech-recognition-based human-robot interaction using Locally-Normalized Filter-Bank features (LNFB), and training strategies that compensate for microphone response and room acoustics. Testing utterances were generated by re-recording the Aurora-4 testing database using a PR2 mobile robot, equipped with a Kinect audio interface while performing head rotations and movements toward and away from a fixed source. Three training conditions were evaluated called Clean, 1-IR and 33-IR. With Clean training, the DNN-HMM system was trained using the Aurora-4 clean training database. With 1-IR training, the same training data were convolved with an impulse response estimated at one meter from the source with no rotation of the robot head. With 33-IR training, the Aurora-4 training data were convolved with impulse responses estimated at one, two and three meters from the source and 11 angular positions of the robot head. The 33-IR training method produced reductions in WER greater than 50% when compared with Clean training using both LNFB and conventional Mel filterbank features. Nevertheless, LNFB features provided a WER 23% lower than MelFB using 33-IR training. The use of 33-IR training and LNFB features reduced WER by 64% compared to Clean training and MelFB features",
    "checked": true,
    "id": "2beda4bb9b72b15fb23453c25ebaf68ceab4e846",
    "semantic_title": "robustness over time-varying channels in dnn-hmm asr based human-robot interaction",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2017/turker17_interspeech.html": {
    "title": "Analysis of Engagement and User Experience with a Laughter Responsive Social Robot",
    "volume": "main",
    "abstract": "We explore the effect of laughter perception and response in terms of engagement in human-robot interaction. We designed two distinct experiments in which the robot has two modes: laughter responsive and laughter non-responsive. In responsive mode, the robot detects laughter using a multimodal real-time laughter detection module and invokes laughter as a backchannel to users accordingly. In non-responsive mode, robot has no utilization of detection, thus provides no feedback. In the experimental design, we use a straightforward question-answer based interaction scenario using a back-projected robot head. We evaluate the interactions with objective and subjective measurements of engagement and user experience",
    "checked": true,
    "id": "e97124ba9d246dc0e3b7487c849ed338a2ddcb96",
    "semantic_title": "analysis of engagement and user experience with a laughter responsive social robot",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2017/baird17_interspeech.html": {
    "title": "Automatic Classification of Autistic Child Vocalisations: A Novel Database and Results",
    "volume": "main",
    "abstract": "Humanoid robots have in recent years shown great promise for supporting the educational needs of children on the autism spectrum. To further improve the efficacy of such interactions, user-adaptation strategies based on the individual needs of a child are required. In this regard, the proposed study assesses the suitability of a range of speech-based classification approaches for automatic detection of autism severity according to the commonly used Social Responsiveness Scale second edition (SRS-2). Autism is characterised by socialisation limitations including child language and communication ability. When compared to neurotypical children of the same age these can be a strong indication of severity. This study introduces a novel dataset of 803 utterances recorded from 14 autistic children aged between 4–10 years, during Wizard-of-Oz interactions with a humanoid robot. Our results demonstrate the suitability of support vector machines (SVMs) which use acoustic feature sets from multiple Interspeech COMPARE challenges. We also evaluate deep spectrum features, extracted via an image classification convolutional neural network (CNN) from the spectrogram of autistic speech instances. At best, by using SVMs on the acoustic feature sets, we achieved a UAR of 73.7% for the proposed 3-class task",
    "checked": true,
    "id": "8102aaeb504d51b8133a5acb18bc9dc0de24fd5b",
    "semantic_title": "automatic classification of autistic child vocalisations: a novel database and results",
    "citation_count": 33
  },
  "https://www.isca-speech.org/archive/interspeech_2017/oertel17_interspeech.html": {
    "title": "Crowd-Sourced Design of Artificial Attentive Listeners",
    "volume": "main",
    "abstract": "Feedback generation is an important component of human-human communication. Humans can choose to signal support, understanding, agreement or also scepticism by means of feedback tokens. Many studies have focused on the timing of feedback behaviours. In the current study, however, we keep the timing constant and instead focus on the lexical form and prosody of feedback tokens as well as their sequential patterns For this we crowdsourced participant's feedback behaviour in identical interactional contexts in order to model a virtual agent that is able to provide feedback as an attentive/supportive as well as attentive/sceptical listener. The resulting models were realised in a robot which was evaluated by third-party observers",
    "checked": true,
    "id": "e9b15b5b74f2d3591c20e0a699ed08ed84a01e15",
    "semantic_title": "crowd-sourced design of artificial attentive listeners",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lancia17_interspeech.html": {
    "title": "Studying the Link Between Inter-Speaker Coordination and Speech Imitation Through Human-Machine Interactions",
    "volume": "main",
    "abstract": "According to accounts of inter-speaker coordination based on internal predictive models, speakers tend to imitate each other each time they need to coordinate their behavior. According to accounts based on the notion of dynamical coupling, imitation should be observed only if it helps stabilizing the specific coordinative pattern produced by the interlocutors or if it is a direct consequence of inter-speaker coordination. To compare these accounts, we implemented an artificial agent designed to repeat a speech utterance while coordinating its behavior with that of a human speaker performing the same task. We asked 10 Italian speakers to repeat the utterance /topkop/ simultaneously with the agent during short time intervals. In some interactions, the agent was parameterized to cooperate with the speakers (by producing its syllables simultaneously with those of the human) while in others it was parameterized to compete with them (by producing its syllables in-between those of the human). A positive correlation between the stability of inter-speaker coordination and the degree of f0 imitation was observed only in cooperative interactions. However, in line with accounts based on prediction, speakers imitate the f0 of the agent regardless of whether this is parameterized to cooperate or to compete with them",
    "checked": true,
    "id": "127ca0b3b865019f7283826262676b1c5e5cd6fc",
    "semantic_title": "studying the link between inter-speaker coordination and speech imitation through human-machine interactions",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/delalez17_interspeech.html": {
    "title": "Adjusting the Frame: Biphasic Performative Control of Speech Rhythm",
    "volume": "main",
    "abstract": "Performative time and pitch scaling is a new research paradigm for prosodic analysis by synthesis. In this paper, a system for real-time recorded speech time and pitch scaling by the means of hands or feet gestures is designed and evaluated. Pitch is controlled with the preferred hand, using a stylus on a graphic tablet. Time is controlled using rhythmic frames, or constriction gestures, defined by pairs of control points. The \"Arsis\" corresponds to the constriction (weak beat of the syllable) and the \"Thesis\" corresponds to the vocalic nucleus (strong beat of the syllable). This biphasic control of rhythmic units is performed by the non-preferred hand using a button. Pitch and time scales are modified according to these gestural controls with the help of a real-time pitch synchronous overlap-add technique (RT-PSOLA). Rhythm and pitch control accuracy are assessed in a prosodic imitation experiment: the task is to reproduce intonation and rhythm of various sentences. The results show that inter-vocalic durations differ on average of only 20 ms. The system appears as a new and effective tool for performative speech and singing synthesis. Consequences and applications in speech prosody research are discussed",
    "checked": true,
    "id": "5686b901dc53803657687f0df335cdc1878f7c45",
    "semantic_title": "adjusting the frame: biphasic performative control of speech rhythm",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2017/saryazdi17_interspeech.html": {
    "title": "Attentional Factors in Listeners' Uptake of Gesture Cues During Speech Processing",
    "volume": "main",
    "abstract": "In conversation, speakers spontaneously produce manual gestures that can facilitate listeners' comprehension of speech. However, various factors may affect listeners' ability to use gesture cues. Here we examine a situation where a speaker is referring to physical objects in the contextual here-and-now. In this situation, objects for potential reference will compete with gestures for visual attention. In two experiments, a speaker provided instructions to pick up objects in the visual environment (\" Pick up the candy\"). On some trials, the speaker produced a \"pick up\" gesture that reflected the size/shape of the target object. Gaze position was recorded to evaluate how listeners allocated attention to scene elements. Experiment 1 showed that, although iconic gestures (when present) were rarely fixated directly, peripheral uptake of these cues speeded listeners' visual identification of intended referents as the instruction unfolded. However, the benefit was mild and occurred primarily for small/hard-to-identify objects. In Experiment 2, background noise was added to reveal whether challenging auditory environments lead listeners to allocate additional visual attention to gesture cues in a compensatory manner. Interestingly, background noise actually reduced listeners' use of gesture cues. Together the findings highlight how situational factors govern the use of visual cues during multimodal communication",
    "checked": true,
    "id": "3c4b29a7cecb7b7085f811708dcab527cedd422a",
    "semantic_title": "attentional factors in listeners' uptake of gesture cues during speech processing",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ishi17_interspeech.html": {
    "title": "Motion Analysis in Vocalized Surprise Expressions",
    "volume": "main",
    "abstract": "The background of our research is the generation of natural human-like motions during speech in android robots that have a highly human-like appearance. Mismatches in speech and motion are sources of unnaturalness, especially when emotion expressions are involved. Surprise expressions often occur in dialogue interactions, and they are often accompanied by verbal interjectional utterances. In this study, we analyze facial, head and body motions during several types of vocalized surprise expressions appearing in human-human dialogue interactions. The analysis results indicate an inter-dependence between motion types and different types of surprise expression (such as emotional, social or quoted) as well as different degrees of surprise expression. The synchronization between motion and surprise utterances is also analyzed",
    "checked": true,
    "id": "549e5ddb8053f43a829dfe0ed53b009dec174d53",
    "semantic_title": "motion analysis in vocalized surprise expressions",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ruede17_interspeech.html": {
    "title": "Enhancing Backchannel Prediction Using Word Embeddings",
    "volume": "main",
    "abstract": "Backchannel responses like \"uh-huh\", \"yeah\", \"right\" are used by the listener in a social dialog as a way to provide feedback to the speaker. In the context of human-computer interaction, these responses can be used by an artificial agent to build rapport in conversations with users. In the past, multiple approaches have been proposed to detect backchannel cues and to predict the most natural timing to place those backchannel utterances. Most of these are based on manually optimized fixed rules, which may fail to generalize. Many systems rely on the location and duration of pauses and pitch slopes of specific lengths. In the past, we proposed an approach by training artificial neural networks on acoustic features such as pitch and power and also attempted to add word embeddings via word2vec. In this work, we refined this approach by evaluating different methods to add timed word embeddings via word2vec. Comparing the performance using various feature combinations, we could show that adding linguistic features improves the performance over a prediction system that only uses acoustic features",
    "checked": true,
    "id": "6627a1e0f4ddeb593dcc0cfc6cbc95a402b746c6",
    "semantic_title": "enhancing backchannel prediction using word embeddings",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2017/raveh17_interspeech.html": {
    "title": "A Computational Model for Phonetically Responsive Spoken Dialogue Systems",
    "volume": "main",
    "abstract": "This paper introduces a model for segment-level phonetic responsiveness. It is based on behavior observed in human-human interaction, and is designed to be integrated into spoken dialogue systems to capture potential phonetic variation and simulate convergence capabilities. Each step in the process is responsible for an aspect of the interaction, including monitoring the input speech and appropriately analyzing it. Various parameters can be tuned to configure the speech handling and adjust the response style. Evaluation was performed by simulating simple end-to-end dialogue scenarios, including analyzing the synthesized output of the model. The results show promising ground for further extensions",
    "checked": true,
    "id": "ce5569e53e775ce85f3ccf35add29a3f8d59b6ca",
    "semantic_title": "a computational model for phonetically responsive spoken dialogue systems",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ebhotemhen17_interspeech.html": {
    "title": "Incremental Dialogue Act Recognition: Token- vs Chunk-Based Classification",
    "volume": "main",
    "abstract": "This paper presents a machine learning based approach to incremental dialogue act classification with a focus on the recognition of communicative functions associated with dialogue segments in a multidimensional space, as defined in the ISO 24617-2 dialogue act annotation standard. The main goal is to establish the nature of an increment whose processing will result in a reliable overall system performance. We explore scenarios where increments are tokens or syntactically, semantically or prosodically motivated chunks. Combing local classification with meta-classifiers at a late fusion decision level we obtained state-of-the-art classification performance. Experiments were carried out on manually corrected transcriptions and on potentially erroneous ASR output. Chunk-based classification yields better results on the manual transcriptions, whereas token-based classification shows a more robust performance on the ASR output. It is also demonstrated that layered hierarchical and cascade training procedures result in better classification performance than the single-layered approach based on a joint classification predicting complex class labels",
    "checked": true,
    "id": "27f69b94a3f3d4a70d4bd2d448a6e55d826e9387",
    "semantic_title": "incremental dialogue act recognition: token- vs chunk-based classification",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2017/niebuhr17b_interspeech.html": {
    "title": "Clear Speech — Mere Speech? How Segmental and Prosodic Speech Reduction Shape the Impression That Speakers Create on Listeners",
    "volume": "main",
    "abstract": "Research on speech reduction is primarily concerned with analyzing, modeling, explaining, and, ultimately, predicting phonetic variation. That is, the focus is on the speech signal itself. The present paper adds a little side note to this fundamental line of research by addressing the question whether variation in the degree of reduction also has a systematic effect on the attributes we ascribe to the speaker who produces the speech signal. A perception experiment was carried out for German in which 46 listeners judged whether or not speakers showing 3 different combinations of segmental and prosodic reduction levels (unreduced, moderately reduced, strongly reduced) are appropriately described by 13 physical, social, and cognitive attributes. The experiment shows that clear speech is not mere speech, and less clear speech is not just reduced either. Rather, results revealed a complex interplay of reduction levels and perceived speaker attributes in which moderate reduction can make a better impression on listeners than no reduction. In addition to its relevance in reduction models and theories, this interplay is instructive for various fields of speech application from social robotics to charisma coaching",
    "checked": false,
    "id": "a11af0e71aa252dabf583c99cdb0f803f2a7fabb",
    "semantic_title": "clear speech - mere speech? how segmental and prosodic speech reduction shape the impression that speakers create on listeners",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kouklia17_interspeech.html": {
    "title": "Relationships Between Speech Timing and Perceived Hostility in a French Corpus of Political Debates",
    "volume": "main",
    "abstract": "This study investigates the relationship between perceived hostility and speech timing features within extracts from Montreuil's City Council sessions in 2013, marked by a tense political context at this time. A dataset of 118 speech extracts from the mayor (Dominique Voynet) and four of her political opponents during the City Council has been analyzed through the combination of perception tests and speech timing phenomena, estimated from classical timing-related measurements and custom metrics. We also develop a methodological framework for the phonetic analysis of nonscripted speech: a double perceptive evaluation of the original dataset (22 participants) allowed us to measure the difference of hostility perceived (dHost) between the original audio extracts and their read transcriptions, and the five speakers produced the same utterances in a controlled reading task to make the direct comparison with original extracts possible. Correlations between dHost and speech timing features differences between each original utterance and its control counterpart show that perceived hostility is mainly influenced by local deviations to the expected accentuation pattern in French combined with the insertion of silent pauses. Moreover, a finer-grained analysis of rhythmic features reveals different strategies amongst speakers, especially regarding the realization of interpausal speech rate variation and final syllables lengthening",
    "checked": true,
    "id": "c5a538ea5bc6ca643ce9ccabfbb3506ad8208ec7",
    "semantic_title": "relationships between speech timing and perceived hostility in a french corpus of political debates",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gallardo17_interspeech.html": {
    "title": "Towards Speaker Characterization: Identifying and Predicting Dimensions of Person Attribution",
    "volume": "main",
    "abstract": "A great number of investigations on person characterization rely on the assessment of the Big-Five personality traits, a prevalent and widely accepted model with strong psychological foundation. However, in the context on characterizing unfamiliar individuals from their voices only, it may be hard for assessors to determine the Big-Five traits based on their first impression. In this study, a 28-item semantic differential rating scale has been completed by a total of 33 listeners who were presented with 15 male voice stimuli. A factor analysis on their responses enabled us to identify five perceptual factors of person attribution: (social and physical) attractiveness, confidence, apathy, serenity, and incompetence. A discussion on the relations of these dimensions of speaker attribution to the Big-Five factors is provided and speech features relevant to the automatic prediction of our dimensions are analyzed, together with SVM regression performance. Although more data are needed to validate our findings, we believe that our approach can lead to establish a space of person attributions with dimensions that can easily be detected from utterances in zero-acquaintance scenarios",
    "checked": true,
    "id": "d5f0aa9727a8224075eb0ffca5f8fe0f87331043",
    "semantic_title": "towards speaker characterization: identifying and predicting dimensions of person attribution",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ishi17b_interspeech.html": {
    "title": "Prosodic Analysis of Attention-Drawing Speech",
    "volume": "main",
    "abstract": "The term \"attention drawing\" refers to the action of sellers who call out to get the attention of people passing by in front of their stores or shops to invite them inside to buy or sample products. Since the speaking styles exhibited in such attention-drawing speech are clearly different from conversational speech, in this study, we focused on prosodic analyses of attention-drawing speech and collected the speech data of multiple people with previous attention-drawing experience by simulating several situations. We then investigated the effects of several factors, including background noise, interaction phases, and shop categories on the prosodic features of attention-drawing utterances. Analysis results indicate that compared to dialogue interaction utterances, attention-drawing utterances usually have higher power, higher mean F0s, smaller F0 ranges, and do not drop at the end of sentences, regardless of the presence or absence of background noise. Analysis of sentence-final syllable intonation indicates the presence of lengthened flat or rising tones in attention-drawing utterances",
    "checked": true,
    "id": "e041470ced09fc59ff5b40adaa152bfc9a629e6d",
    "semantic_title": "prosodic analysis of attention-drawing speech",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2017/simpson17_interspeech.html": {
    "title": "Perceptual and Acoustic CorreLates of Gender in the Prepubertal Voice",
    "volume": "main",
    "abstract": "This study investigates the perceptual and acoustic correlates of gender in the prepubertal voice. 23 German-speaking primary school pupils (13 female, 10 male) aged 8–9 years were recorded producing 10 sentences each. Two sentences from each speaker were presented in random order to a group of listeners who were asked to assign a gender to each stimulus. Single utterances from each of the three male and three female speakers whose gender was identified most reliably were played in a second experiment to two further groups of listeners who judged each stimulus against seven perceptual attribute pairs. Acoustic analysis of those parameters corresponding most directly to the perceptual attributes revealed a number of highly significant correlations, indicating some aspects of the voice and speech (f0, harmonics-to-noise ratio, tempo) that children use to construct and adults use to identify gender in the prepubertal voice",
    "checked": true,
    "id": "6e55a68c6499594414fa1385f36a85ae8a7e2886",
    "semantic_title": "perceptual and acoustic correlates of gender in the prepubertal voice",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2017/schweitzer17_interspeech.html": {
    "title": "To See or not to See: Interlocutor Visibility and Likeability Influence Convergence in Intonation",
    "volume": "main",
    "abstract": "In this paper we look at convergence and divergence in intonation in the context of social qualities. Specifically we examine pitch accent realisations in the GECO corpus of German conversations. Pitch accents are represented as 6-dimensional vectors where each dimension corresponds to a characteristic of the accent's shape. Convergence/divergence is then measured by calculating the distance between pitch accent realisations of conversational partners. A decrease of distance values over time indicates convergence, an increase divergence. The corpus comprises dialogue sessions in two modalities: partners either saw each other during the conversation or not. Linear mixed model analyses show convergence as well as divergence effects in the realisations of H*L accents. This convergence/divergence is strongly related to the modality and to how much speakers like their partners: generally, seeing the partner comes with divergence, whereas when the dialogue partners cannot see each other, there is convergence. The effect varies, however, depending on the extent to which a speaker likes their partner. Less liking entails a greater change in the realisations over time — stronger divergence when partners could see each other, and stronger convergence when they could not",
    "checked": true,
    "id": "c69a3010fabf8e8ebac0cef24670a790af5790cc",
    "semantic_title": "to see or not to see: interlocutor visibility and likeability influence convergence in intonation",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2017/weirich17_interspeech.html": {
    "title": "Acoustic Correlates of Parental Role and Gender Identity in the Speech of Expecting Parents",
    "volume": "main",
    "abstract": "Differences between male and female speakers have been explained in terms of biological inevitabilities but also in terms of behavioral and socially motivated factors. The aim of this study is to investigate the latter by examining gender-specific variability within the same gender The speech of 29 German men and women — all of them expecting their first child but varying in the time they plan to stay at home during their child's first year (parental role) — is analyzed. Acoustic analyses comprise the vowel space size and the realization of the inter-sibilant contrast While the data is part of a larger longitudinal project investigating adult- and infant-directed speech during the infant's first year of life, this study concentrates on the recordings made before the birth of the child. Inter-speaker variability is investigated in relation to 1) the chosen parental role and 2) self-ascribed ratings on positive feminine attributes (gender identity) Results show that both factors (planned duration of parental leave and the femininity ratings) contribute to the variability found between, but also within the same gender. In particular, the vowel space size was found to be positively correlated with self-ascribed femininity ratings in male speakers",
    "checked": true,
    "id": "f706b059d82357f9a92be366471e942bac6db260",
    "semantic_title": "acoustic correlates of parental role and gender identity in the speech of expecting parents",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2017/soleraurena17_interspeech.html": {
    "title": "A Semi-Supervised Learning Approach for Acoustic-Prosodic Personality Perception in Under-Resourced Domains",
    "volume": "main",
    "abstract": "Automatic personality analysis has gained attention in the last years as a fundamental dimension in human-to-human and human-to-machine interaction. However, it still suffers from limited number and size of speech corpora for specific domains, such as the assessment of children's personality. This paper investigates a semi-supervised training approach to tackle this scenario. We devise an experimental setup with age and language mismatch and two training sets: a small labeled training set from the Interspeech 2012 Personality Sub-challenge, containing French adult speech labeled with personality OCEAN traits, and a large unlabeled training set of Portuguese children's speech. As test set, a corpus of Portuguese children's speech labeled with OCEAN traits is used. Based on this setting, we investigate a weak supervision approach that iteratively refines an initial model trained with the labeled data-set using the unlabeled data-set. We also investigate knowledge-based features, which leverage expert knowledge in acoustic-prosodic cues and thus need no extra data. Results show that, despite the large mismatch imposed by language and age differences, it is possible to attain improvements with these techniques, pointing both to the benefits of using a weak supervision and expert-based acoustic-prosodic features across age and language",
    "checked": true,
    "id": "9542bcf34857660b988b358712637ea7e2f04887",
    "semantic_title": "a semi-supervised learning approach for acoustic-prosodic personality perception in under-resourced domains",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tatman17_interspeech.html": {
    "title": "Effects of Talker Dialect, Gender & Race on Accuracy of Bing Speech and YouTube Automatic Captions",
    "volume": "main",
    "abstract": "This project compares the accuracy of two automatic speech recognition (ASR) systems — Bing Speech and YouTube's automatic captions — across gender, race and four dialects of American English. The dialects included were chosen for their acoustic dissimilarity. Bing Speech had differences in word error rate (WER) between dialects and ethnicities, but they were not statistically reliable. YouTube's automatic captions, however, did have statistically different WERs between dialects and races. The lowest average error rates were for General American and white talkers, respectively. Neither system had a reliably different WER between genders, which had been previously reported for YouTube's automatic captions [1]. However, the higher error rate non-white talkers is worrying, as it may reduce the utility of these systems for talkers of color",
    "checked": true,
    "id": "1080dc00733e010fdd6a9b999506a0d4d864519d",
    "semantic_title": "effects of talker dialect, gender & race on accuracy of bing speech and youtube automatic captions",
    "citation_count": 76
  },
  "https://www.isca-speech.org/archive/interspeech_2017/prabhavalkar17_interspeech.html": {
    "title": "A Comparison of Sequence-to-Sequence Models for Speech Recognition",
    "volume": "main",
    "abstract": "In this work, we conduct a detailed evaluation of various all-neural, end-to-end trained, sequence-to-sequence models applied to the task of speech recognition. Notably, each of these systems directly predicts graphemes in the written domain, without using an external pronunciation lexicon, or a separate language model. We examine several sequence-to-sequence models including connectionist temporal classification (CTC), the recurrent neural network (RNN) transducer, an attention-based model, and a model which augments the RNN transducer with an attention mechanism We find that the sequence-to-sequence models are competitive with traditional state-of-the-art approaches on dictation test sets, although the baseline, which uses a separate pronunciation and language model, outperforms these models on voice-search test sets",
    "checked": true,
    "id": "6cc68e8adf34b580f3f37d1bd267ee701974edde",
    "semantic_title": "a comparison of sequence-to-sequence models for speech recognition",
    "citation_count": 262
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zeyer17_interspeech.html": {
    "title": "CTC in the Context of Generalized Full-Sum HMM Training",
    "volume": "main",
    "abstract": "We formulate a generalized hybrid HMM-NN training procedure using the full-sum over the hidden state-sequence and identify CTC as a special case of it. We present an analysis of the alignment behavior of such a training procedure and explain the strong localization of label output behavior of full-sum training (also referred to as peaky or spiky behavior). We show how to avoid that behavior by using a state prior. We discuss the temporal decoupling between output label position/time-frame, and the corresponding evidence in the input observations when this is trained with BLSTM models. We also show a way how to overcome this by jointly training a FFNN. We implemented the Baum-Welch alignment algorithm in CUDA to be able to do fast soft realignments on GPU. We have published this code along with some of our experiments as part of RETURNN, RWTH's extensible training framework for universal recurrent neural networks. We finish with experimental validation of our study on WSJ and Switchboard",
    "checked": true,
    "id": "2ec4f0035b840b4d90a114b298aaa1f4551a93d1",
    "semantic_title": "ctc in the context of generalized full-sum hmm training",
    "citation_count": 34
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hori17_interspeech.html": {
    "title": "Advances in Joint CTC-Attention Based End-to-End Speech Recognition with a Deep CNN Encoder and RNN-LM",
    "volume": "main",
    "abstract": "We present a state-of-the-art end-to-end Automatic Speech Recognition (ASR) model. We learn to listen and write characters with a joint Connectionist Temporal Classification (CTC) and attention-based encoder-decoder network. The encoder is a deep Convolutional Neural Network (CNN) based on the VGG network. The CTC network sits on top of the encoder and is jointly trained with the attention-based decoder. During the beam search process, we combine the CTC predictions, the attention-based decoder predictions and a separately trained LSTM language model. We achieve a 5–10% error reduction compared to prior systems on spontaneous Japanese and Chinese speech, and our end-to-end model beats out traditional hybrid ASR systems",
    "checked": true,
    "id": "c102ac8c779ee0a53dc8e4ee20b4088ac2c7e186",
    "semantic_title": "advances in joint ctc-attention based end-to-end speech recognition with a deep cnn encoder and rnn-lm",
    "citation_count": 266
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lu17_interspeech.html": {
    "title": "Multitask Learning with CTC and Segmental CRF for Speech Recognition",
    "volume": "main",
    "abstract": "Segmental conditional random fields (SCRFs) and connectionist temporal classification (CTC) are two sequence labeling methods used for end-to-end training of speech recognition models. Both models define a transcription probability by marginalizing decisions about latent segmentation alternatives to derive a sequence probability: the former uses a globally normalized joint model of segment labels and durations, and the latter classifies each frame as either an output symbol or a \"continuation\" of the previous label. In this paper, we train a recognition model by optimizing an interpolation between the SCRF and CTC losses, where the same recurrent neural network (RNN) encoder is used for feature extraction for both outputs. We find that this multitask objective improves recognition accuracy when decoding with either the SCRF or CTC models. Additionally, we show that CTC can also be used to pretrain the RNN encoder, which improves the convergence rate when learning the joint model",
    "checked": true,
    "id": "3d2988f54c7600adb5489de2dfb2c0f8f1316b88",
    "semantic_title": "multitask learning with ctc and segmental crf for speech recognition",
    "citation_count": 24
  },
  "https://www.isca-speech.org/archive/interspeech_2017/audhkhasi17_interspeech.html": {
    "title": "Direct Acoustics-to-Word Models for English Conversational Speech Recognition",
    "volume": "main",
    "abstract": "Recent work on end-to-end automatic speech recognition (ASR) has shown that the connectionist temporal classification (CTC) loss can be used to convert acoustics to phone or character sequences. Such systems are used with a dictionary and separately-trained Language Model (LM) to produce word sequences. However, they are not truly end-to-end in the sense of mapping acoustics directly to words without an intermediate phone representation. In this paper, we present the first results employing direct acoustics-to-word CTC models on two well-known public benchmark tasks: Switchboard and CallHome. These models do not require an LM or even a decoder at run-time and hence recognize speech with minimal complexity. However, due to the large number of word output units, CTC word models require orders of magnitude more data to train reliably compared to traditional systems. We present some techniques to mitigate this issue. Our CTC word model achieves a word error rate of 13.0%/18.8% on the Hub5-2000 Switchboard/CallHome test sets without any LM or decoder compared with 9.6%/16.0% for phone-based CTC with a 4-gram LM. We also present rescoring results on CTC word model lattices to quantify the performance benefits of a LM, and contrast the performance of word and phone CTC models",
    "checked": false,
    "id": "cf0e9724e51b420bc51a1d0625410c86d36641db",
    "semantic_title": "building competitive direct acoustics-to-word models for english conversational speech recognition",
    "citation_count": 185
  },
  "https://www.isca-speech.org/archive/interspeech_2017/li17e_interspeech.html": {
    "title": "Reducing the Computational Complexity of Two-Dimensional LSTMs",
    "volume": "main",
    "abstract": "Long Short-Term Memory Recurrent Neural Networks (LSTMs) are good at modeling temporal variations in speech recognition tasks, and have become an integral component of many state-of-the-art ASR systems. More recently, LSTMs have been extended to model variations in the speech signal in two dimensions, namely time and frequency [1, 2]. However, one of the problems with two-dimensional LSTMs, such as Grid-LSTMs, is that the processing in both time and frequency occurs sequentially, thus increasing computational complexity. In this work, we look at minimizing the dependence of the Grid-LSTM with respect to previous time and frequency points in the sequence, thus reducing computational complexity. Specifically, we compare reducing computation using a bidirectional Grid-LSTM (biGrid-LSTM) with non-overlapping frequency sub-band processing, a PyraMiD-LSTM [3] and a frequency-block Grid-LSTM (fbGrid-LSTM) for parallel time-frequency processing. We find that the fbGrid-LSTM can reduce computation costs by a factor of four with no loss in accuracy, on a 12,500 hour Voice Search task",
    "checked": true,
    "id": "c8d1eeb3a278e09804476049fd86d45d3f3c82af",
    "semantic_title": "reducing the computational complexity of two-dimensional lstms",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lucero17_interspeech.html": {
    "title": "Functional Principal Component Analysis of Vocal Tract Area Functions",
    "volume": "main",
    "abstract": "This paper shows the application of a functional version of principal component analysis to build a parametrization of vocal tract area functions for vowel production. Sets of measured area values for ten vowels are expressed as smooth functional data and next decomposed into a mean area function and a basis of orthogonal eigenfunctions. Interpretations of the first four eigenfunctions are provided in terms of tongue movements and vocal tract length variations. Also, an alternative set of eigenfunctions with closer association to specific regions of the vocal tract is obtained via a varimax rotation. The general intention of the paper is to show the benefits of a functional approach to analyze vocal tract shapes and motivate further applications",
    "checked": true,
    "id": "de4f8358b43236c755fa5cc2e3ecebd262da1348",
    "semantic_title": "functional principal component analysis of vocal tract area functions",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sivaraman17_interspeech.html": {
    "title": "Analysis of Acoustic-to-Articulatory Speech Inversion Across Different Accents and Languages",
    "volume": "main",
    "abstract": "The focus of this paper is estimating articulatory movements of the tongue and lips from acoustic speech data. While there are several potential applications of such a method in speech therapy and pronunciation training, performance of such acoustic-to-articulatory inversion systems is not very high due to limited availability of simultaneous acoustic and articulatory data, substantial speaker variability, and variable methods of data collection. This paper therefore evaluates the impact of speaker, language and accent variability on the performance of an acoustic-to-articulatory speech inversion system. The articulatory dataset used in this study consists of 21 Dutch speakers reading Dutch and English words and sentences, and 22 UK English speakers reading English words and sentences. We trained several acoustic-to-articulatory speech inversion systems both based on deep and shallow neural network architectures in order to estimate electromagnetic articulography (EMA) sensor positions, as well as vocal tract variables (TVs). Our results show that with appropriate feature and target normalization, a speaker-independent speech inversion system trained on data from one language is able to estimate sensor positions (or TVs) for the same language correlating at about r = 0.53 with the actual sensor positions (or TVs). Cross-language results show a reduced performance of r = 0.47",
    "checked": true,
    "id": "f8db55708164b21f1a79d84a682996a4194ee56e",
    "semantic_title": "analysis of acoustic-to-articulatory speech inversion across different accents and languages",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2017/arai17_interspeech.html": {
    "title": "Integrated Mechanical Model of [r]-[l] and [b]-[m]-[w] Producing Consonant Cluster [br]",
    "volume": "main",
    "abstract": "We have developed two types of mechanical models of the human vocal tract. The first model was designed for the retroflex approximant [r] and the alveolar lateral approximant [l]. It consisted of the main vocal tract and a flapping tongue, where the front half of the tongue can be rotated against the palate. When the tongue is short and rotated approximately 90 degrees, the retroflex approximant [r] is produced. The second model was designed for [b], [m], and [w]. Besides the main vocal tract, this model contains a movable lower lip for lip closure and a nasal cavity with a controllable velopharyngeal port. In the present study, we joined these two mechanical models to form a new model containing the main vocal tract, the flapping tongue, the movable lower lip, and the nasal cavity with the controllable velopharyngeal port. This integrated model now makes it possible to produce consonant sequences. Therefore, we examined the sequence [br], in particular, adjusting the timing of the lip and lingual gestures to produce the best sound. Because the gestures are visually observable from the outside of this model, the timing of the gestures were examined with the use of a high-speed video camera",
    "checked": true,
    "id": "da8165d055aff164679683c2dee089e9a2d8ee8f",
    "semantic_title": "integrated mechanical model of [r]-[l] and [b]-[m]-[w] producing consonant cluster [br]",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2017/badino17_interspeech.html": {
    "title": "A Speaker Adaptive DNN Training Approach for Speaker-Independent Acoustic Inversion",
    "volume": "main",
    "abstract": "We address the speaker-independent acoustic inversion (AI) problem, also referred to as acoustic-to-articulatory mapping. The scarce availability of multi-speaker articulatory data makes it difficult to learn a mapping which generalizes from a limited number of training speakers and reliably reconstructs the articulatory movements of unseen speakers. In this paper, we propose a Multi-task Learning (MTL)-based approach that explicitly separates the modeling of each training speaker AI peculiarities from the modeling of AI characteristics that are shared by all speakers. Our approach stems from the well known Regularized MTL approach and extends it to feed-forward deep neural networks (DNNs). Given multiple training speakers, we learn for each an acoustic-to-articulatory mapping represented by a DNN. Then, through an iterative procedure, we search for a canonical speaker-independent DNN that is \"similar\" to all speaker-dependent DNNs. The degree of similarity is controlled by a regularization parameter. We report experiments on the University of Wisconsin X-ray Microbeam Database under different training/testing experimental settings. The results obtained indicate that our MTL-trained canonical DNN largely outperforms a standardly trained (i.e., single task learning-based) speaker independent DNN",
    "checked": true,
    "id": "47449a33b7b0eac650349b905b5a107780d7ba46",
    "semantic_title": "a speaker adaptive dnn training approach for speaker-independent acoustic inversion",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2017/uchida17_interspeech.html": {
    "title": "Acoustic-to-Articulatory Mapping Based on Mixture of Probabilistic Canonical Correlation Analysis",
    "volume": "main",
    "abstract": "In this paper, we propose a novel acoustic-to-articulatory mapping model based on mixture of probabilistic canonical correlation analysis (mPCCA). In PCCA, it is assumed that two different kinds of data are observed as results from different linear transforms of a common latent variable. It is expected that this variable represents a common factor which is inherent in the different domains, such as acoustic and articulatory feature spaces. mPCCA is an expansion of PCCA and it can model a much more complex structure. In mPCCA, covariance matrices of a joint probabilistic distribution of acoustic-articulatory data are structuralized reasonably by using transformation coefficients of the linear transforms. Even if the number of components in mPCCA increases, the structuralized covariance matrices can be expected to avoid over-fitting. Training and mapping processes of the mPCCA-based mapping model are reasonably derived by using the EM algorithm. Experiments using MOCHA-TIMIT show that the proposed mapping method has achieved better mapping performance than the conventional GMM-based mapping",
    "checked": true,
    "id": "fdc90d55b26324a0c9cc02c0e81e68be87e15ad9",
    "semantic_title": "acoustic-to-articulatory mapping based on mixture of probabilistic canonical correlation analysis",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sorensen17b_interspeech.html": {
    "title": "Test-Retest Repeatability of Articulatory Strategies Using Real-Time Magnetic Resonance Imaging",
    "volume": "main",
    "abstract": "Real-time magnetic resonance imaging (rtMRI) provides information about the dynamic shaping of the vocal tract during speech production. This paper introduces and evaluates a method for quantifying articulatory strategies using rtMRI. The method decomposes the formation and release of a constriction in the vocal tract into the contributions of individual articulators such as the jaw, tongue, lips, and velum. The method uses an anatomically guided factor analysis and dynamical principles from the framework of Task Dynamics. We evaluated the method within a test-retest repeatability framework. We imaged healthy volunteers (n = 8, 4 females, 4 males) in two scans on the same day and quantified inter-study agreement with the intraclass correlation coefficient and mean within-subject standard deviation. The evaluation established a limit on effect size and intra-group differences in articulatory strategy which can be studied using the method",
    "checked": true,
    "id": "af69e971da2fba4f85f9ae2c8ab2d5d48991b076",
    "semantic_title": "test-retest repeatability of articulatory strategies using real-time magnetic resonance imaging",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/snyder17_interspeech.html": {
    "title": "Deep Neural Network Embeddings for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "This paper investigates replacing i-vectors for text-independent speaker verification with embeddings extracted from a feed-forward deep neural network. Long-term speaker characteristics are captured in the network by a temporal pooling layer that aggregates over the input speech. This enables the network to be trained to discriminate between speakers from variable-length speech segments. After training, utterances are mapped directly to fixed-dimensional speaker embeddings and pairs of embeddings are scored using a PLDA-based backend. We compare performance with a traditional i-vector baseline on NIST SRE 2010 and 2016. We find that the embeddings outperform i-vectors for short speech segments and are competitive on long duration test conditions. Moreover, the two representations are complementary, and their fusion improves on the baseline at all operating points. Similar systems have recently shown promising results when trained on very large proprietary datasets, but to the best of our knowledge, these are the best results reported for speaker-discriminative neural networks when trained and tested on publicly available corpora",
    "checked": true,
    "id": "369728d7576683a25de8890e4bc02fae6132fccb",
    "semantic_title": "deep neural network embeddings for text-independent speaker verification",
    "citation_count": 657
  },
  "https://www.isca-speech.org/archive/interspeech_2017/villalba17_interspeech.html": {
    "title": "Tied Variational Autoencoder Backends for i-Vector Speaker Recognition",
    "volume": "main",
    "abstract": "Probabilistic linear discriminant analysis (PLDA) is the de facto standard for backends in i-vector speaker recognition. If we try to extend the PLDA paradigm using non-linear models, e.g., deep neural networks, the posterior distributions of the latent variables and the marginal likelihood become intractable. In this paper, we propose to approach this problem using stochastic gradient variational Bayes. We generalize the PLDA model to let i-vectors depend non-linearly on the latent factors. We approximate the evidence lower bound (ELBO) by Monte Carlo sampling using the reparametrization trick. This enables us to optimize of the ELBO using backpropagation to jointly estimate the parameters that define the model and the approximate posteriors of the latent factors. We also present a reformulation of the likelihood ratio, which we call Q-scoring. Q-scoring makes possible to efficiently score the speaker verification trials for this model. Experimental results on NIST SRE10 suggest that more data might be required to exploit the potential of this method",
    "checked": true,
    "id": "4488cba0d06ae06b4b7b99cbb3639731c9eefe32",
    "semantic_title": "tied variational autoencoder backends for i-vector speaker recognition",
    "citation_count": 28
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ranjan17_interspeech.html": {
    "title": "Improved Gender Independent Speaker Recognition Using Convolutional Neural Network Based Bottleneck Features",
    "volume": "main",
    "abstract": "This paper proposes a novel framework to improve performance of gender independent i-Vector PLDA based speaker recognition using convolutional neural network (CNN). Convolutional layers of a CNN offer robustness to variations in input features including those due to gender. A CNN is trained for ASR with a linear bottleneck layer. Bottleneck features extracted using the CNN are then used to train a gender-independent UBM to obtain frame posteriors for training an i-Vector extractor matrix. To preserve speaker specific information, a hybrid approach to training the i-Vector extractor matrix using MFCC features with corresponding frame posteriors derived from bottleneck features is proposed. On the NIST SRE10 C5 condition pooled trials, our approach reduces the EER and minDCF 2010 by +14.62% and +14.42% respectively compared to a standard mfcc based gender-independent speaker recognition system",
    "checked": true,
    "id": "2a1882ea2c3c63bc0cfd7ec98ccabdba9beec452",
    "semantic_title": "improved gender independent speaker recognition using convolutional neural network based bottleneck features",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2017/shon17_interspeech.html": {
    "title": "Autoencoder Based Domain Adaptation for Speaker Recognition Under Insufficient Channel Information",
    "volume": "main",
    "abstract": "In real-life conditions, mismatch between development and test domain degrades speaker recognition performance. To solve the issue, many researchers explored domain adaptation approaches using matched in-domain dataset. However, adaptation would be not effective if the dataset is insufficient to estimate channel variability of the domain. In this paper, we explore the problem of performance degradation under such a situation of insufficient channel information. In order to exploit limited in-domain dataset effectively, we propose an unsupervised domain adaptation approach using Autoencoder based Domain Adaptation (AEDA). The proposed approach combines an autoencoder with a denoising autoencoder to adapt resource-rich development dataset to test domain. The proposed technique is evaluated on the Domain Adaptation Challenge 13 experimental protocols that is widely used in speaker recognition for domain mismatched condition. The results show significant improvements over baselines and results from other prior studies",
    "checked": true,
    "id": "6634ed2960aeb5f5c2495262d4dd1136b3ec35bf",
    "semantic_title": "autoencoder based domain adaptation for speaker recognition under insufficient channel information",
    "citation_count": 30
  },
  "https://www.isca-speech.org/archive/interspeech_2017/khosravani17_interspeech.html": {
    "title": "Nonparametrically Trained Probabilistic Linear Discriminant Analysis for i-Vector Speaker Verification",
    "volume": "main",
    "abstract": "In this paper we propose to estimate the parameters of the probabilistic linear discriminant analysis (PLDA) in text-independent i-vector speaker verification framework using a nonparametric form rather than maximum likelihood estimation (MLE) obtained by an EM algorithm. In this approach the between-speaker covariance matrix that represents global information about the speaker variability is replaced with a local estimation computed on a nearest neighbor basis for each target speaker. The nonparametric between- and within-speaker scatter matrices can better exploit the discriminant information in training data and is more adapted to sample distribution especially when it does not satisfy Gaussian assumption as in i-vectors without length-normalization. We evaluated this approach on the recent NIST 2016 speaker recognition evaluation (SRE) as well as NIST 2010 core condition and found significant performance improvement compared with a generatively trained PLDA model",
    "checked": true,
    "id": "8e909cec10684ebcf0c61a2b249ed321cd19f5c2",
    "semantic_title": "nonparametrically trained probabilistic linear discriminant analysis for i-vector speaker verification",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jorrin17_interspeech.html": {
    "title": "DNN Bottleneck Features for Speaker Clustering",
    "volume": "main",
    "abstract": "In this work, we explore deep neural network bottleneck features (BNF) in the context of speaker clustering. A straightforward manner to deal with speaker clustering is to reuse the bottleneck features extracted from speaker recognition. However, the selection of a bottleneck architecture or nonlinearity impacts the performance of both systems. In this work, we analyze the bottleneck features obtained for speaker recognition and test them in a speaker clustering scenario. We observe that there are deep neural network topologies that work better for both cases, even when their classification criteria (senone classification) is loosely met. We present results that outperform a traditional MFCC system by 21% for speaker recognition and between 20% and 37% in clustering using the same topology",
    "checked": true,
    "id": "1e1002c9d53f8566b50e27538ee764b18cf8f86a",
    "semantic_title": "dnn bottleneck features for speaker clustering",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2017/aare17_interspeech.html": {
    "title": "Creak as a Feature of Lexical Stress in Estonian",
    "volume": "main",
    "abstract": "In addition to typological, turn-taking or sociolinguistic factors, presence of creaky voice in spontaneous interaction is also influenced by the syntactic and phonological properties of speech. For example, creaky voice is reportedly more frequent in function words than content words, has been observed to accompany unstressed syllables and ends of phrases, and is associated with relaxation and reduced speech In Estonian, creaky voice is frequently used by all speakers. In this paper, we observe the use of creaky voice in spontaneous Estonian in connection to syllabic properties of words, lexical stress, word class, lengthening, and timing in phrases The results indicate that creak occurs less in syllables with primary stress than in unstressed syllables. However, syllables with secondary stress are most frequently creaky. In content words, the primary stressed syllables creak less frequently and unstressed syllables more frequently compared to function words. The stress-related pattern is similar in both function and content words, but more contrastive in content words. The probability of creakiness increases considerably with non-final lengthening within words, and for all syllables towards the end of the intonational phrase",
    "checked": true,
    "id": "4fd64f72132c91c237982ce36ed5e9414259bc1d",
    "semantic_title": "creak as a feature of lexical stress in estonian",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2017/yanushevskaya17_interspeech.html": {
    "title": "Cross-Speaker Variation in Voice Source Correlates of Focus and Deaccentuation",
    "volume": "main",
    "abstract": "This paper describes cross-speaker variation in the voice source correlates of focal accentuation and deaccentuation. A set of utterances with varied narrow focus placement as well as broad focus and deaccented renditions were produced by six speakers of English. These were manually inverse filtered and parameterized on a pulse-by-pulse basis using the LF source model. Z-normalized F0, EE, OQ and RD parameters (selected through correlation and factor analysis) were used to generate speaker specific baseline voice profiles and to explore cross-speaker variation in focal and non-focal (post- and prefocal) syllables. As expected, source parameter values were found to differ in the focal and postfocal portions of the utterance. For four of the six speakers the measures revealed a trend of tenser phonation on the focal syllable (an increase in EE and F0 and typically, a decrease in OQ and RD) as well as increased laxness in the postfocal part of the utterance. For two of the speakers, however, the measurements showed a different trend. These speakers had very high F0 and often high EE on the focal accent. In these cases, RD and OQ values tended to be raised rather than lowered. The possible reasons for these differences are discussed",
    "checked": true,
    "id": "6075220ef193248202d02a8802f973e7cc4f1e6d",
    "semantic_title": "cross-speaker variation in voice source correlates of focus and deaccentuation",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kalita17_interspeech.html": {
    "title": "Acoustic Characterization of Word-Final Glottal Stops in Mizo and Assam Sora",
    "volume": "main",
    "abstract": "The present work proposed an approach to characterize the word-final glottal stops in Mizo and Assam Sora language. Generally, glottal stops have more strong glottal and ventricular constriction at the coda position than at the onset. However, the primary source characteristics of glottal stops are irregular glottal cycles, abrupt glottal closing, and reduced open cycle. These changes will not only affect the vocal quality parameters but may also significantly affect the vocal tract characteristics due to changes in the subglottal coupling behavior. This motivates to analyze the dynamic vocal tract characteristics in terms of source behavior, apart from the excitation source features computed from the Linear Prediction (LP) residual for the acoustic characterization of the word-final glottal stops. The dominant resonance frequency (DRF) of the vocal tract using Hilbert Envelope of Numerator Group Delay (HNGD) are extracted at every sample instants as a cue to study this deviation. The gradual increase in the DRF and significantly lower duration for which subglottal coupling is occurring is observed for the glottal stop region for both the languages",
    "checked": true,
    "id": "a2007f7af4ee7b7f0320ccade66c5a2b40e553bf",
    "semantic_title": "acoustic characterization of word-final glottal stops in mizo and assam sora",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mokhtari17_interspeech.html": {
    "title": "Iterative Optimal Preemphasis for Improved Glottal-Flow Estimation by Iterative Adaptive Inverse Filtering",
    "volume": "main",
    "abstract": "Iterative adaptive inverse filtering (IAIF) [1] remains among the state-of-the-art algorithms for estimating glottal flow from the recorded speech signal. Here, we re-examine IAIF in light of its foundational, classical model of voiced (non-nasalized) speech, wherein the overall spectral tilt is caused only by lip-radiation and glottal effects, while the vocal-tract transfer function contains formant peaks but is otherwise not tilted. In contrast, IAIF initially models and cancels the formants after only a first-order preemphasis of the speech signal, which is generally not enough to completely remove spectral tilt Iterative optimal preemphasis (IOP) is therefore proposed to replace IAIF's initial step. IOP is a rapidly converging algorithm that models a signal (then inverse-filters it) with one real pole (zero) at a time, until spectral tilt is flattened. IOP-IAIF is evaluated on sustained /a/ in a range of voice qualities from weak-breathy to shouted-tense. Compared with standard IAIF, IOP-IAIF yields: (i) an acceptable glottal flow even for a weak breathy voice that the standard algorithm failed to handle; (ii) generally smoother glottal flows that nevertheless retain pulse shape and closed phase; and (iii) enhanced separation of voice qualities in both normalized amplitude quotient (NAQ) and glottal harmonic spectra",
    "checked": true,
    "id": "96acd5d8c7ccaaba570d8badacd2e357b61be71d",
    "semantic_title": "iterative optimal preemphasis for improved glottal-flow estimation by iterative adaptive inverse filtering",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sheena17_interspeech.html": {
    "title": "Automatic Measurement of Pre-Aspiration",
    "volume": "main",
    "abstract": "Pre-aspiration is defined as the period of glottal friction occurring in sequences of vocalic/consonantal sonorants and phonetically voiceless obstruents. We propose two machine learning methods for automatic measurement of pre-aspiration duration: a feedforward neural network, which works at the frame level; and a structured prediction model, which relies on manually designed feature functions, and works at the segment level. The input for both algorithms is a speech signal of an arbitrary length containing a single obstruent, and the output is a pair of times which constitutes the pre-aspiration boundaries. We train both models on a set of manually annotated examples. Results suggest that the structured model is superior to the frame-based model as it yields higher accuracy in predicting the boundaries and generalizes to new speakers and new languages. Finally, we demonstrate the applicability of our structured prediction algorithm by replicating linguistic analysis of pre-aspiration in Aberystwyth English with high correlation",
    "checked": true,
    "id": "c870b6e0d968fbf63f33ff366bf17f40f0945e7d",
    "semantic_title": "automatic measurement of pre-aspiration",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2017/nara17_interspeech.html": {
    "title": "Acoustic and Electroglottographic Study of Breathy and Modal Vowels as Produced by Heritage and Native Gujarati Speakers",
    "volume": "main",
    "abstract": "While all languages of the world use modal phonation, many also rely on other phonation types such as breathy or creaky voice. For example, Gujarati, an Indo-Aryan language, makes a distinction between breathy and modal phonation among consonants and vowels: /b aɾ/ ‘burden', /baɾ/ ‘twelve', and /ba̤ɾ/ ‘outside' [1, 2]. This study, which is a replication and an extension of Khan [3], aims to determine the acoustic and articulatory parameters that distinguish breathy and modal vowels. The participants of this study are heritage and native Gujarati speakers The materials consisted of 40 target words with the modal and breathy pairs of the three vowel qualities: /a/ vs /a̤/, /e/ vs /e̤/, and /o/ vs /o̤/. The participants uttered the words in the context of a sentence. Acoustic measurements such as H1-H2, H1-A1, harmonic-to-noise ratio and articulatory measurements such as contact quotient were calculated throughout the vowel duration The results of the Smoothing Spline ANOVA analyses indicated that measures such as H1-A1, harmonic to noise ratio, and contact quotient distinguished modal and breathy vowels for native speakers. Heritage speakers also had a contrast between breathy and modal vowels, however the contrast is not as robust as that of native speakers",
    "checked": true,
    "id": "02d679ffc224da71ea37122645440a799a63f628",
    "semantic_title": "acoustic and electroglottographic study of breathy and modal vowels as produced by heritage and native gujarati speakers",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wang17e_interspeech.html": {
    "title": "An RNN-Based Quantized F0 Model with Multi-Tier Feedback Links for Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "A recurrent-neural-network-based F0 model for text-to-speech (TTS) synthesis that generates F0 contours given textual features is proposed. In contrast to related F0 models, the proposed one is designed to learn the temporal correlation of F0 contours at multiple levels. The frame-level correlation is covered by feeding back the F0 output of the previous frame as the additional input of the current frame; meanwhile, the correlation over long-time spans is similarly modeled but by using F0 features aggregated over the phoneme and syllable. Another difference is that the output of the proposed model is not the interpolated continuous-valued F0 contour but rather a sequence of discrete symbols, including quantized F0 levels and a symbol for the unvoiced condition. By using the discrete F0 symbols, the proposed model avoids the influence of artificially interpolated F0 curves. Experiments demonstrated that the proposed F0 model, which was trained using a dropout strategy, generated smooth F0 contours with relatively better perceived quality than those from baseline RNN models",
    "checked": true,
    "id": "9f1d5d90262ec9cebba2055f234e3a197d7f6f78",
    "semantic_title": "an rnn-based quantized f0 model with multi-tier feedback links for text-to-speech synthesis",
    "citation_count": 35
  },
  "https://www.isca-speech.org/archive/interspeech_2017/klimkov17_interspeech.html": {
    "title": "Phrase Break Prediction for Long-Form Reading TTS: Exploiting Text Structure Information",
    "volume": "main",
    "abstract": "Phrasing structure is one of the most important factors in increasing the naturalness of text-to-speech (TTS) systems, in particular for long-form reading. Most existing TTS systems are optimized for isolated short sentences, and completely discard the larger context or structure of the text This paper presents how we have built phrasing models based on data extracted from audiobooks. We investigate how various types of textual features can improve phrase break prediction: part-of-speech (POS), guess POS (GPOS), dependency tree features and word embeddings. These features are fed into a bidirectional LSTM or a CART baseline. The resulting systems are compared using both objective and subjective evaluations. Using BiLSTM and word embeddings proves to be beneficial",
    "checked": true,
    "id": "3a51eb66b9785af6f53cfd19188eeb5ecf9544fd",
    "semantic_title": "phrase break prediction for long-form reading tts: exploiting text structure information",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tanaka17_interspeech.html": {
    "title": "Physically Constrained Statistical F0 Prediction for Electrolaryngeal Speech Enhancement",
    "volume": "main",
    "abstract": "Electrolaryngeal (EL) speech produced by a laryngectomee using an electrolarynx to mechanically generate artificial excitation sounds severely suffers from unnatural fundamental frequency (F ) patterns caused by monotonic excitation sounds. To address this issue, we have previously proposed EL speech enhancement systems using statistical F pattern prediction methods based on a Gaussian Mixture Model (GMM), making it possible to predict the underlying F pattern of EL speech from its spectral feature sequence. Our previous work revealed that the naturalness of the predicted F pattern can be improved by incorporating a physically based generative model of F patterns into the GMM-based statistical F prediction system within a Product-of-Expert framework. However, one drawback of this method is that it requires an iterative procedure to obtain a predicted F pattern, making it difficult to realize a real-time system. In this paper, we propose yet another approach to physically based statistical F pattern prediction by using a HMM-GMM framework. This approach is noteworthy in that it allows to generate an F pattern that is both statistically likely and physically natural without iterative procedures. Experimental results demonstrated that the proposed method was capable of generating F patterns more similar to those in normal speech than the conventional GMM-based method",
    "checked": true,
    "id": "5dc88f622202ee2c7732e672bb0f546ade2c8f2c",
    "semantic_title": "physically constrained statistical f0 prediction for electrolaryngeal speech enhancement",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hojo17_interspeech.html": {
    "title": "DNN-SPACE: DNN-HMM-Based Generative Model of Voice F0 Contours for Statistical Phrase/Accent Command Estimation",
    "volume": "main",
    "abstract": "This paper proposes a method to extract prosodic features from a speech signal by leveraging auxiliary linguistic information. A prosodic feature extractor called the statistical phrase/accent command estimation (SPACE) has recently been proposed. This extractor is based on a statistical model formulated as a stochastic counterpart of the Fujisaki model, a well-founded mathematical model representing the control mechanism of vocal fold vibration. The key idea of this approach is that a phrase/accent command pair sequence is modeled as an output sequence of a path-restricted hidden Markov model (HMM) so that estimating the state transition amounts to estimating the phrase/accent commands. Since the phrase and accent commands are related to linguistic information, we may expect to improve the command estimation accuracy by using them as auxiliary information for the inference. To model the relationship between the phrase/accent commands and linguistic information, we construct a deep neural network (DNN) that maps the linguistic feature vectors to the state posterior probabilities of the HMM. Thus, given a pitch contour and linguistic information, we can estimate phrase/accent commands via state decoding. We call this method \"DNN-SPACE.\" Experimental results revealed that using linguistic information was effective in improving the command estimation accuracy",
    "checked": true,
    "id": "6b3f95382dd3acfd18461018700ea4cdb9512dea",
    "semantic_title": "dnn-space: dnn-hmm-based generative model of voice f0 contours for statistical phrase/accent command estimation",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2017/malisz17_interspeech.html": {
    "title": "Controlling Prominence Realisation in Parametric DNN-Based Speech Synthesis",
    "volume": "main",
    "abstract": "This work aims to improve text-to-speech synthesis for Wikipedia by advancing and implementing models of prosodic prominence. We propose a new system architecture with explicit prominence modeling and test the first component of the architecture. We automatically extract a phonetic feature related to prominence from the speech signal in the ARCTIC corpus. We then modify the label files and train an experimental TTS system based on the feature using Merlin, a statistical-parametric DNN-based engine. Test sentences with contrastive prominence on the word-level are synthesised and separate listening tests a) evaluating the level of prominence control in generated speech, and b) naturalness, are conducted. Our results show that the prominence feature-enhanced system successfully places prominence on the appropriate words and increases perceived naturalness relative to the baseline",
    "checked": true,
    "id": "783f6a1d6eb0e48a7abaaf6c64f7c59662e1acd1",
    "semantic_title": "controlling prominence realisation in parametric dnn-based speech synthesis",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2017/betz17_interspeech.html": {
    "title": "Increasing Recall of Lengthening Detection via Semi-Automatic Classification",
    "volume": "main",
    "abstract": "Lengthening is the ideal hesitation strategy for synthetic speech and dialogue systems: it is unobtrusive and hard to notice, because it occurs frequently in everyday speech before phrase boundaries, in accentuation, and in hesitation. Despite its elusiveness, it allows valuable extra time for computing or information highlighting in incremental spoken dialogue systems. The elusiveness of the matter, however, poses a challenge for extracting lengthening instances from corpus data: we suspect a recall problem, as human annotators might not be able to consistently label lengthening instances. We address this issue by filtering corpus data for instances of lengthening, using a simple classification method, based on a threshold for normalized phone duration. The output is then manually labeled for disfluency. This is compared to an existing, fully manual disfluency annotation, showing that recall is significantly higher with semi-automatic pre-classification. This shows that it is inevitable to use semi-automatic pre-selection to gather enough candidate data points for manual annotation and subsequent lengthening analyses. Also, it is desirable to further increase the performance of the automatic classification. We evaluate in detail human versus semi-automatic annotation and train another classifier on the resulting dataset to check the integrity of the disfluent – non-disfluent distinction",
    "checked": true,
    "id": "d393408fed9d2e43baa4bb1bd2a0331c6144e4b0",
    "semantic_title": "increasing recall of lengthening detection via semi-automatic classification",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2017/satt17_interspeech.html": {
    "title": "Efficient Emotion Recognition from Speech Using Deep Learning on Spectrograms",
    "volume": "main",
    "abstract": "We present a new implementation of emotion recognition from the para-lingual information in the speech, based on a deep neural network, applied directly to spectrograms. This new method achieves higher recognition accuracy compared to previously published results, while also limiting the latency. It processes the speech input in smaller segments — up to 3 seconds, and splits a longer input into non-overlapping parts to reduce the prediction latency The deep network comprises common neural network tools — convolutional and recurrent networks — which are shown to effectively learn the information that represents emotions directly from spectrograms. Convolution-only lower-complexity deep network achieves a prediction accuracy of 66% over four emotions (tested on IEMOCAP — a common evaluation corpus), while a combined convolution-LSTM higher-complexity model achieves 68% The use of spectrograms in the role of speech-representing features enables effective handling of background non-speech signals such as music (excl. singing) and crowd noise, even at noise levels comparable with the speech signal levels. Using harmonic modeling to remove non-speech components from the spectrogram, we demonstrate significant improvement of the emotion recognition accuracy in the presence of unknown background non-speech signals",
    "checked": true,
    "id": "de47fc09bc8dcd032c8b3450a0b2a816c376e07e",
    "semantic_title": "efficient emotion recognition from speech using deep learning on spectrograms",
    "citation_count": 264
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhang17b_interspeech.html": {
    "title": "Interaction and Transition Model for Speech Emotion Recognition in Dialogue",
    "volume": "main",
    "abstract": "In this paper we propose a novel emotion recognition method modeling interaction and transition in dialogue. Conventional emotion recognition utilizes intra-features such as MFCCs or F0s within individual utterance. However, human perceive emotions not only through individual utterances but also by contextual information. The proposed method takes in account the contextual effect of utterance in dialogue, which the conventional method fails to. Proposed method introduces Emotion Interaction and Transition (EIT) models which is constructed by end-to-end LSTMs. The inputs of EIT model are the previous emotions of both target and opponent speaker, estimated by state-of-the-art utterance emotion recognition model. The experimental results show that the proposed method improves overall accuracy and average precision by a relative error reduction of 18.8% and 22.6% respectively",
    "checked": true,
    "id": "46e8d8da8fa7a90b35f354efd429c0aead610e79",
    "semantic_title": "interaction and transition model for speech emotion recognition in dialogue",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gideon17_interspeech.html": {
    "title": "Progressive Neural Networks for Transfer Learning in Emotion Recognition",
    "volume": "main",
    "abstract": "Many paralinguistic tasks are closely related and thus representations learned in one domain can be leveraged for another. In this paper, we investigate how knowledge can be transferred between three paralinguistic tasks: speaker, emotion, and gender recognition. Further, we extend this problem to cross-dataset tasks, asking how knowledge captured in one emotion dataset can be transferred to another. We focus on progressive neural networks and compare these networks to the conventional deep learning method of pre-training and fine-tuning. Progressive neural networks provide a way to transfer knowledge and avoid the forgetting effect present when pre-training neural networks on different tasks. Our experiments demonstrate that: (1) emotion recognition can benefit from using representations originally learned for different paralinguistic tasks and (2) transfer learning can effectively leverage additional datasets to improve the performance of emotion recognition systems",
    "checked": true,
    "id": "9c753bb807a3abb3563e68c898bc0f2cd473d84c",
    "semantic_title": "progressive neural networks for transfer learning in emotion recognition",
    "citation_count": 110
  },
  "https://www.isca-speech.org/archive/interspeech_2017/parthasarathy17_interspeech.html": {
    "title": "Jointly Predicting Arousal, Valence and Dominance with Multi-Task Learning",
    "volume": "main",
    "abstract": "An appealing representation of emotions is the use of emotional attributes such as arousal (passive versus active), valence (negative versus positive) and dominance (weak versus strong). While previous studies have considered these dimensions as orthogonal descriptors to represent emotions, there are strong theoretical and practical evidences showing the interrelation between these emotional attributes. This observation suggests that predicting emotional attributes with a unified framework should outperform machine learning algorithms that separately predict each attribute. This study presents methods to jointly learn emotional attributes by exploiting their interdependencies. The framework relies on multi-task learning (MTL) implemented with deep neural networks (DNN) with shared hidden layers. The framework provides a principled approach to learn shared feature representations that maximize the performance of regression models. The results of within-corpus and cross-corpora evaluation show the benefits of MTL over single task learning (STL). MTL achieves gains on concordance correlation coefficient (CCC) as high as 4.7% for within-corpus evaluations, and 14.0% for cross-corpora evaluations. The visualization of the activations of the last hidden layers illustrates that MTL creates better feature representation. The best structure has shared layers followed by attribute-dependent layers, capturing better the relation between attributes",
    "checked": true,
    "id": "9e616407b71813d7426793ae88cf01b2ebecbd0b",
    "semantic_title": "jointly predicting arousal, valence and dominance with multi-task learning",
    "citation_count": 105
  },
  "https://www.isca-speech.org/archive/interspeech_2017/le17b_interspeech.html": {
    "title": "Discretized Continuous Speech Emotion Recognition with Multi-Task Deep Recurrent Neural Network",
    "volume": "main",
    "abstract": "Estimating continuous emotional states from speech as a function of time has traditionally been framed as a regression problem. In this paper, we present a novel approach that moves the problem into the classification domain by discretizing the training labels at different resolutions. We employ a multi-task deep bidirectional long-short term memory (BLSTM) recurrent neural network (RNN) trained with cost-sensitive Cross Entropy loss to model these labels jointly. We introduce an emotion decoding algorithm that incorporates long- and short-term temporal properties of the signal to produce more robust time series estimates. We show that our proposed approach achieves competitive audio-only performance on the RECOLA dataset, relative to previously published works as well as other strong regression baselines. This work provides a link between regression and classification, and contributes an alternative approach for continuous emotion recognition",
    "checked": true,
    "id": "7720940403cfb35b7226103ab6e1df1a2bf71df1",
    "semantic_title": "discretized continuous speech emotion recognition with multi-task deep recurrent neural network",
    "citation_count": 46
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kim17d_interspeech.html": {
    "title": "Towards Speech Emotion Recognition \"in the Wild\" Using Aggregated Corpora and Deep Multi-Task Learning",
    "volume": "main",
    "abstract": "One of the challenges in Speech Emotion Recognition (SER) \"in the wild\" is the large mismatch between training and test data (e.g. speakers and tasks). In order to improve the generalisation capabilities of the emotion models, we propose to use Multi-Task Learning (MTL) and use gender and naturalness as auxiliary tasks in deep neural networks. This method was evaluated in within-corpus and various cross-corpus classification experiments that simulate conditions \"in the wild\". In comparison to Single-Task Learning (STL) based state of the art methods, we found that our MTL method proposed improved performance significantly. Particularly, models using both gender and naturalness achieved more gains than those using either gender or naturalness separately. This benefit was also found in the high-level representations of the feature space, obtained from our method proposed, where discriminative emotional clusters could be observed",
    "checked": true,
    "id": "e0ba2e87f8209a075c4cda0554014159c476c1c8",
    "semantic_title": "towards speech emotion recognition \"in the wild\" using aggregated corpora and deep multi-task learning",
    "citation_count": 75
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tamamori17_interspeech.html": {
    "title": "Speaker-Dependent WaveNet Vocoder",
    "volume": "main",
    "abstract": "In this study, we propose a speaker-dependent WaveNet vocoder, a method of synthesizing speech waveforms with WaveNet, by utilizing acoustic features from existing vocoder as auxiliary features of WaveNet. It is expected that WaveNet can learn a sample-by-sample correspondence between speech waveform and acoustic features. The advantage of the proposed method is that it does not require (1) explicit modeling of excitation signals and (2) various assumptions, which are based on prior knowledge specific to speech. We conducted both subjective and objective evaluation experiments on CMU-ARCTIC database. From the results of the objective evaluation, it was demonstrated that the proposed method could generate high-quality speech with phase information recovered, which was lost by a mel-cepstrum vocoder. From the results of the subjective evaluation, it was demonstrated that the sound quality of the proposed method was significantly improved from mel-cepstrum vocoder, and the proposed method could capture source excitation information more accurately",
    "checked": true,
    "id": "487aa8076bf3c0edb4134759e1ddf09d64f21476",
    "semantic_title": "speaker-dependent wavenet vocoder",
    "citation_count": 255
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gu17_interspeech.html": {
    "title": "Waveform Modeling Using Stacked Dilated Convolutional Neural Networks for Speech Bandwidth Extension",
    "volume": "main",
    "abstract": "This paper presents a waveform modeling and generation method for speech bandwidth extension (BWE) using stacked dilated convolutional neural networks (CNNs) with causal or non-causal convolutional layers. Such dilated CNNs describe the predictive distribution for each wideband or high-frequency speech sample conditioned on the input narrowband speech samples. Distinguished from conventional frame-based BWE approaches, the proposed methods can model the speech waveforms directly and therefore avert the spectral conversion and phase estimation problems. Experimental results prove that the BWE methods proposed in this paper can achieve better performance than the state-of-the-art frame-based approach utilizing recurrent neural networks (RNNs) incorporating long short-term memory (LSTM) cells in subjective preference tests",
    "checked": true,
    "id": "773e95ec372046a3be8c0e98cf17f9f1bfcbea1a",
    "semantic_title": "waveform modeling using stacked dilated convolutional neural networks for speech bandwidth extension",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2017/takaki17_interspeech.html": {
    "title": "Direct Modeling of Frequency Spectra and Waveform Generation Based on Phase Recovery for DNN-Based Speech Synthesis",
    "volume": "main",
    "abstract": "In statistical parametric speech synthesis (SPSS) systems using the high-quality vocoder, acoustic features such as mel-cepstrum coefficients and F0 are predicted from linguistic features in order to utilize the vocoder to generate speech waveforms. However, the generated speech waveform generally suffers from quality deterioration such as buzziness caused by utilizing the vocoder. Although several attempts such as improving an excitation model have been investigated to alleviate the problem, it is difficult to completely avoid it if the SPSS system is based on the vocoder. To overcome this problem, there have recently been attempts to directly model waveform samples. Superior performance has been demonstrated, but computation time and latency are still issues. With the aim to construct another type of DNN-based speech synthesizer with neither the vocoder nor computational explosion, we investigated direct modeling of frequency spectra and waveform generation based on phase recovery. In this framework, STFT spectral amplitudes that include harmonic information derived from F0 are directly predicted through a DNN-based acoustic model and we use Griffin and Lim's approach to recover phase and generate waveforms. The experimental results showed that the proposed system synthesized speech without buzziness and outperformed one generated from a conventional system using the vocoder",
    "checked": true,
    "id": "2ea59b2bcab7badfd028e92345ed3a273d6ee0b9",
    "semantic_title": "direct modeling of frequency spectra and waveform generation based on phase recovery for dnn-based speech synthesis",
    "citation_count": 39
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ronanki17_interspeech.html": {
    "title": "A Hierarchical Encoder-Decoder Model for Statistical Parametric Speech Synthesis",
    "volume": "main",
    "abstract": "Current approaches to statistical parametric speech synthesis using Neural Networks generally require input at the same temporal resolution as the output, typically a frame every 5ms, or in some cases at waveform sampling rate. It is therefore necessary to fabricate highly-redundant frame-level (or sample-level) linguistic features at the input. This paper proposes the use of a hierarchical encoder-decoder model to perform the sequence-to-sequence regression in a way that takes the input linguistic features at their original timescales, and preserves the relationships between words, syllables and phones. The proposed model is designed to make more effective use of suprasegmental features than conventional architectures, as well as being computationally efficient. Experiments were conducted on prosodically-varied audiobook material because the use of supra-segmental features is thought to be particularly important in this case. Both objective measures and results from subjective listening tests, which asked listeners to focus on prosody, show that the proposed method performs significantly better than a conventional architecture that requires the linguistic input to be at the acoustic frame rate We provide code and a recipe to enable our system to be reproduced using the Merlin toolkit",
    "checked": true,
    "id": "7042f7f82813021827beda34fb775fd6de6161a1",
    "semantic_title": "a hierarchical encoder-decoder model for statistical parametric speech synthesis",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kobayashi17_interspeech.html": {
    "title": "Statistical Voice Conversion with WaveNet-Based Waveform Generation",
    "volume": "main",
    "abstract": "This paper presents a statistical voice conversion (VC) technique with theWaveNet-based waveform generation. VC based on a Gaussian mixture model (GMM) makes it possible to convert the speaker identity of a source speaker into that of a target speaker. However, in the conventional vocoding process, various factors such as F extraction errors, parameterization errors and over-smoothing effects of converted feature trajectory cause the modeling errors of the speech waveform, which usually bring about sound quality degradation of the converted voice. To address this issue, we apply a direct waveform generation technique based on a WaveNet vocoder to VC. In the proposed method, first, the acoustic features of the source speaker are converted into those of the target speaker based on the GMM. Then, the waveform samples of the converted voice are generated based on the WaveNet vocoder conditioned on the converted acoustic features. In this paper, to investigate the modeling accuracies of the converted speech waveform, we compare several types of the acoustic features for training and synthesizing based on the WaveNet vocoder. The experimental results confirmed that the proposed VC technique achieves higher conversion accuracy on speaker individuality with comparable sound quality compared to the conventional VC technique",
    "checked": true,
    "id": "ac7ad4a80e5b0aac54ede084f93d3c3252d1b988",
    "semantic_title": "statistical voice conversion with wavenet-based waveform generation",
    "citation_count": 90
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wan17_interspeech.html": {
    "title": "Google's Next-Generation Real-Time Unit-Selection Synthesizer Using Sequence-to-Sequence LSTM-Based Autoencoders",
    "volume": "main",
    "abstract": "A neural network model that significant improves unit-selection-based Text-To-Speech synthesis is presented. The model employs a sequence-to-sequence LSTM-based autoencoder that compresses the acoustic and linguistic features of each unit to a fixed-size vector referred to as an embedding. Unit-selection is facilitated by formulating the target cost as an L distance in the embedding space. In open-domain speech synthesis the method achieves a 0.2 improvement in the MOS, while for limited-domain it reaches the cap of 4.5 MOS. Furthermore, the new TTS system halves the gap between the previous unit-selection system and WaveNet in terms of quality while retaining low computational cost and latency",
    "checked": true,
    "id": "19c5f127671aaa6e418caf1f77ad20b7cefd13ad",
    "semantic_title": "google's next-generation real-time unit-selection synthesizer using sequence-to-sequence lstm-based autoencoders",
    "citation_count": 39
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kain17_interspeech.html": {
    "title": "A Comparison of Sentence-Level Speech Intelligibility Metrics",
    "volume": "main",
    "abstract": "We examine existing and novel automatically-derived acoustic metrics that are predictive of speech intelligibility. We hypothesize that the degree of variability in feature space is correlated with the extent of a speaker's phonemic inventory, their degree of articulatory displacements, and thus with their degree of perceived speech intelligibility. We begin by using fully-automatic F1/F2 formant frequency trajectories for both vowel space area calculation and as input to a proposed class-separability metric. We then switch to representing vowels by means of short-term spectral features, and measure vowel separability in that space. Finally, we consider the case where phonetic labeling is unavailable; here we calculate short-term spectral features for the entire speech utterance and then estimate their entropy based on the length of a minimum spanning tree. In an alternative approach, we propose to first segment the speech signal using a hidden Markov model, and then calculate spectral feature separability based on the automatically-derived classes. We apply all approaches to a database with healthy controls as well as speakers with mild dysarthria, and report the resulting coefficients of determination",
    "checked": true,
    "id": "8d4f24cb8ea51fcffe595f46b9e27116168df2c1",
    "semantic_title": "a comparison of sentence-level speech intelligibility metrics",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/irino17_interspeech.html": {
    "title": "An Auditory Model of Speaker Size Perception for Voiced Speech Sounds",
    "volume": "main",
    "abstract": "An auditory model was developed to explain the results of behavioral experiments on perception of speaker size with voiced speech sounds. It is based on the dynamic, compressive gammachirp (dcGC) filterbank and a weighting function (SSI weight) derived from a theory of size-shape segregation in the auditory system. Voiced words with and without high-frequency emphasis (+6 dB/octave) were produced using a speech vocoder (STRAIGHT). The SSI weighting function reduces the effect of glottal pulse excitation in voiced speech, which, in turn, makes it possible for the model to explain the individual subject variability in the data",
    "checked": true,
    "id": "1ad03b893a0ff3beceeed1bffd9bd8b040d78654",
    "semantic_title": "an auditory model of speaker size perception for voiced speech sounds",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bosch17_interspeech.html": {
    "title": "The Recognition of Compounds: A Computational Account",
    "volume": "main",
    "abstract": "This paper investigates the processes in comprehending spoken noun-noun compounds, using data from the BALDEY database. BALDEY contains lexicality judgments and reaction times (RTs) for Dutch stimuli for which also linguistic information is included. Two different approaches are combined. The first is based on regression by Dynamic Survival Analysis, which models decisions and RTs as a consequence of the fact that a cumulative density function exceeds some threshold. The parameters of that function are estimated from the observed RT data. The second approach is based on DIANA, a process-oriented computational model of human word comprehension, which simulates the comprehension process with the acoustic stimulus as input. DIANA gives the identity and the number of the word candidates that are activated at each 10 ms time step Both approaches show how the processes involved in comprehending compounds change during a stimulus. Survival Analysis shows that the impact of word duration varies during the course of a stimulus. The density of word and non-word hypotheses in DIANA shows a corresponding pattern with different regimes. We show how the approaches complement each other, and discuss additional ways in which data and process models can be combined",
    "checked": true,
    "id": "ccfc2fe007d19cfd70b45e67e9ba62f7b5bf927a",
    "semantic_title": "the recognition of compounds: a computational account",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jahromi17_interspeech.html": {
    "title": "Humans do not Maximize the Probability of Correct Decision When Recognizing DANTALE Words in Noise",
    "volume": "main",
    "abstract": "Inspired by the DANTALE II listening test paradigm, which is used for determining the intelligibility of noisy speech, we assess the hypothesis that humans maximize the probability of correct decision when recognizing words contaminated by additive Gaussian, speech-shaped noise. We first propose a statistical Gaussian communication and classification scenario, where word models are built from short term spectra of human speech, and optimal classifiers in the sense of maximum a posteriori estimation are derived. Then, we perform a listening test, where the participants are instructed to make their best guess of words contaminated with speech-shaped Gaussian noise. Comparing the human's performance to that of the optimal classifier reveals that at high SNR, humans perform comparable to the optimal classifier. However, at low SNR, the human performance is inferior to that of the optimal classifier. This shows that, at least in this specialized task, humans are generally not able to maximize the probability of correct decision, when recognizing words",
    "checked": true,
    "id": "fa2fec8495c269af2d29526b7c9da95496b49fdf",
    "semantic_title": "humans do not maximize the probability of correct decision when recognizing dantale words in noise",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/huber17_interspeech.html": {
    "title": "Single-Ended Prediction of Listening Effort Based on Automatic Speech Recognition",
    "volume": "main",
    "abstract": "A new, single-ended, i.e. reference-free measure for the prediction of perceived listening effort of noisy speech is presented. It is based on phoneme posterior probabilities (or posteriorgrams) obtained from a deep neural network of an automatic speech recognition system. Additive noisy or other distortions of speech tend to smear the posteriorgrams. The smearing is quantified by a performance measure, which is used as a predictor for the perceived listening effort required to understand the noisy speech. The proposed measure was evaluated using a database obtained from the subjective evaluation of noise reduction algorithms of commercial hearing aids. Listening effort ratings of processed noisy speech samples were gathered from 20 hearing-impaired subjects. Averaged subjective ratings were compared with corresponding predictions computed by the proposed new method, the ITU-T standard P.563 for single-ended speech quality assessment, the American National Standard ANIQUE+ for single-ended speech quality assessment, and a single-ended SNR estimator. The proposed method achieved a good correlation with mean subjective ratings and clearly outperformed the standard speech quality measures and the SNR estimator",
    "checked": true,
    "id": "05bfb1e2e98b8cc43aa0c6c4f91d30ed90708a05",
    "semantic_title": "single-ended prediction of listening effort based on automatic speech recognition",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2017/neufeld17_interspeech.html": {
    "title": "Modeling Categorical Perception with the Receptive Fields of Auditory Neurons",
    "volume": "main",
    "abstract": "This paper demonstrates that a low-level, linear description of the response properties of auditory neurons can exhibit some of the high-level properties of the categorical perception of human speech. In particular, it is shown that the non-linearities observed in the human perception of speech sounds which span a categorical boundaries can be understood as arising rather naturally from a low-level statistical description of phonemic contrasts in the time-frequency plane, understood here as the receptive field of auditory neurons. The TIMIT database was used to train a model auditory neuron which discriminates between /s/ and /sh/, and a computer simulation was conducted which demonstrates that the neuron responds categorically to a linear continuum of synthetic fricative sounds which span the /s/-/sh/ boundary. The response of the model provides a good fit to human labeling behavior, and in addition, is able to account for asymmetries in reaction time across the two categories",
    "checked": true,
    "id": "1b52e267392cc72b76a7bcf658acb88f38bf452c",
    "semantic_title": "modeling categorical perception with the receptive fields of auditory neurons",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wang17f_interspeech.html": {
    "title": "A Maximum Likelihood Approach to Deep Neural Network Based Nonlinear Spectral Mapping for Single-Channel Speech Separation",
    "volume": "main",
    "abstract": "In contrast to the conventional minimum mean squared error (MMSE) training criterion for nonlinear spectral mapping based on deep neural networks (DNNs), we propose a probabilistic learning framework to estimate the DNN parameters for single-channel speech separation. A statistical analysis of the prediction error vector at the DNN output reveals that it follows a unimodal density for each log power spectral component. By characterizing the prediction error vector as a multivariate Gaussian density with zero mean vector and an unknown covariance matrix, we present a maximum likelihood (ML) approach to DNN parameter learning. Our experiments on the Speech Separation Challenge (SSC) corpus show that the proposed learning approach can achieve a better generalization capability and a faster convergence than MMSE-based DNN learning. Furthermore, we demonstrate that the ML-trained DNN consistently outperforms MMSE-trained DNN in all the objective measures of speech quality and intelligibility in single-channel speech separation",
    "checked": true,
    "id": "6d7b2fbbc71a5824a28e8b64cb5144dda38efcec",
    "semantic_title": "a maximum likelihood approach to deep neural network based nonlinear spectral mapping for single-channel speech separation",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2017/higuchi17_interspeech.html": {
    "title": "Deep Clustering-Based Beamforming for Separation with Unknown Number of Sources",
    "volume": "main",
    "abstract": "This paper extends a deep clustering algorithm for use with time-frequency masking-based beamforming and perform separation with an unknown number of sources. Deep clustering is a recently proposed single-channel source separation algorithm, which projects inputs into the embedding space and performs clustering in the embedding domain. In deep clustering, bi-directional long short-term memory (BLSTM) recurrent neural networks are trained to make embedding vectors orthogonal for different speakers and concurrent for the same speaker. Then, by clustering the embedding vectors at test time, we can estimate time-frequency masks for separation. In this paper, we extend the deep clustering algorithm to a multiple microphone setup and incorporate deep clustering-based time-frequency mask estimation into masking-based beamforming, which has been shown to be more effective than masking for automatic speech recognition. Moreover, we perform source counting by computing the rank of the covariance matrix of the embedding vectors. With our proposed approach, we can perform masking-based beamforming in a multiple-speaker case without knowing the number of speakers. Experimental results show that our proposed deep clustering-based beamformer achieves comparable source separation performance to that obtained with a complex Gaussian mixture model-based beamformer, which requires the number of sources in advance for mask estimation",
    "checked": true,
    "id": "469d81aa88e01205c3b3449b2d4010477acd56b1",
    "semantic_title": "deep clustering-based beamforming for separation with unknown number of sources",
    "citation_count": 38
  },
  "https://www.isca-speech.org/archive/interspeech_2017/pirhosseinloo17_interspeech.html": {
    "title": "Time-Frequency Masking for Blind Source Separation with Preserved Spatial Cues",
    "volume": "main",
    "abstract": "In this paper, we address the problem of speech source separation by relying on time-frequency binary masks to segregate binaural mixtures. We describe an algorithm which can tackle reverberant mixtures and can extract the original sources while preserving their original spatial locations. The performance of the proposed algorithm is evaluated objectively and subjectively, by assessing the estimated interaural time differences versus their theoretical values and by testing for localization acuity in normal-hearing listeners for different spatial locations in a reverberant room. Experimental results indicate that the proposed algorithm is capable of preserving the spatial information of the recovered source signals while keeping the signal-to-distortion and signal-to-interference ratios high",
    "checked": true,
    "id": "867d77c8cac059df025744c8d36ae67ac0cad93d",
    "semantic_title": "time-frequency masking for blind source separation with preserved spatial cues",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chien17b_interspeech.html": {
    "title": "Variational Recurrent Neural Networks for Speech Separation",
    "volume": "main",
    "abstract": "We present a new stochastic learning machine for speech separation based on the variational recurrent neural network (VRNN). This VRNN is constructed from the perspectives of generative stochastic network and variational auto-encoder. The idea is to faithfully characterize the randomness of hidden state of a recurrent neural network through variational learning. The neural parameters under this latent variable model are estimated by maximizing the variational lower bound of log marginal likelihood. An inference network driven by the variational distribution is trained from a set of mixed signals and the associated source targets. A novel supervised VRNN is developed for speech separation. The proposed VRNN provides a stochastic point of view which accommodates the uncertainty in hidden states and facilitates the analysis of model construction. The masking function is further employed in network outputs for speech separation. The benefit of using VRNN is demonstrated by the experiments on monaural speech separation",
    "checked": true,
    "id": "80d45f972fb9f9c07947edce2c90ac492b2e9789",
    "semantic_title": "variational recurrent neural networks for speech separation",
    "citation_count": 35
  },
  "https://www.isca-speech.org/archive/interspeech_2017/andrei17_interspeech.html": {
    "title": "Detecting Overlapped Speech on Short Timeframes Using Deep Learning",
    "volume": "main",
    "abstract": "The intent of this work is to demonstrate how deep learning techniques can be successfully used to detect overlapped speech on independent short timeframes. A secondary objective is to provide an understanding on how the duration of the signal frame influences the accuracy of the method. We trained a deep neural network with heterogeneous layers and obtained close to 80% inference accuracy on frames going as low as 25 milliseconds. The proposed system provides higher detection quality than existing work and can predict overlapped speech with up to 3 simultaneous speakers. The method exposes low response latency and does not require a high amount of computing power",
    "checked": true,
    "id": "96c5b7e95a83f763d9078f435dac88b607cd5dab",
    "semantic_title": "detecting overlapped speech on short timeframes using deep learning",
    "citation_count": 25
  },
  "https://www.isca-speech.org/archive/interspeech_2017/li17f_interspeech.html": {
    "title": "Ideal Ratio Mask Estimation Using Deep Neural Networks for Monaural Speech Segregation in Noisy Reverberant Conditions",
    "volume": "main",
    "abstract": "Monaural speech segregation is an important problem in robust speech processing and has been formulated as a supervised learning problem. In supervised learning methods, the ideal binary mask (IBM) is usually used as the target because of its simplicity and large speech intelligibility gains. Recently, the ideal ratio mask (IRM) has been found to improve the speech quality over the IBM. However, the IRM was originally defined in anechoic conditions and did not consider the effect of reverberation. In this paper, the IRM is extended to reverberant conditions where the direct sound and early reflections of target speech are regarded as the desired signal. Deep neural networks (DNNs) is employed to estimate the extended IRM in the noisy reverberant conditions. The estimated IRM is then applied to the noisy reverberant mixture for speech segregation. Experimental results show that the estimated IRM provides substantial improvements in speech intelligibility and speech quality over the unprocessed mixture signals under various noisy and reverberant conditions",
    "checked": true,
    "id": "4cc54596ef0d37d66b10d9ba8c5a8e2e755369fe",
    "semantic_title": "ideal ratio mask estimation using deep neural networks for monaural speech segregation in noisy reverberant conditions",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2017/quiroz17_interspeech.html": {
    "title": "The Vocative Chant and Beyond: German Calling Melodies Under Routine and Urgent Contexts",
    "volume": "main",
    "abstract": "This paper investigates calling melodies produced by 21 Standard German native speakers on a discourse completion task across two contexts: (i) routine context — calling a child from afar to come in for dinner; (ii) urgent context — calling a child from afar for a chastising. The intent of this investigation is to bring attention to other calling melodies found in German beside the vocative chant and to give an insight to their acoustic profile Three major melodies were identified in the two contexts: vocative chant (100% of routine context productions), urgent call (100% of male urgent context productions, 52.2% female productions), and stern call (47.8% female urgent context productions). A subsequent quantitative analysis was carried out on these calls across these parameters: (i) tonal scaling at tonal landmarks; (ii) proportional alignment of selected tonal landmarks with respect to the stressed or last vowel; and (iii) amplitude (integral and RMS) and (iv) duration of the stressed vowel, stressed syllable, and word. The resulting data were analyzed using a linear mixed model approach The results point to significant differences in the contours produced in the aforementioned parameters. We also proposed a phonological description of the contours in the framework of Autosegmental-Metrical Phonology",
    "checked": true,
    "id": "ddd6c7503b0d1c4ee685284b3b10043a7061b70a",
    "semantic_title": "the vocative chant and beyond: german calling melodies under routine and urgent contexts",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2017/simko17_interspeech.html": {
    "title": "Comparing Languages Using Hierarchical Prosodic Analysis",
    "volume": "main",
    "abstract": "We present a novel, data-driven approach to assessing mutual similarities and differences among a group of languages, based on purely prosodic characteristics, namely f and energy envelope signals. These signals are decomposed using continuous wavelet transform; the components represent f and energy patterns on three levels of prosodic hierarchy roughly corresponding to syllables, words and phrases. Unigram language models with states derived from a combination of Δ-features obtained from these components are trained and compared using a mutual perplexity measure. In this pilot study we apply this approach to a small corpus of spoken material from seven languages (Estonian, Finnish, Hungarian, German, Swedish, Russian and Slovak) with a rich history of mutual language contacts. We present similarity trees (dendrograms) derived from the models using the hierarchically decomposed prosodic signals separately as well as combined, and compare them with patterns obtained from non-decomposed signals. We show that (1) plausible similarity patterns, reflecting language family relationships and the known contact history can be obtained even from a relatively small data set, and (2) the hierarchical decomposition approach using both f and energy provides the most comprehensive results",
    "checked": true,
    "id": "3b1d2e383d15f5d6c0ca8524c88449a98d725bf8",
    "semantic_title": "comparing languages using hierarchical prosodic analysis",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ip17_interspeech.html": {
    "title": "Intonation Facilitates Prediction of Focus Even in the Presence of Lexical Tones",
    "volume": "main",
    "abstract": "In English and Dutch, listeners entrain to prosodic contours to predict where focus will fall in an utterance. However, is this strategy universally available, even in languages with different phonological systems? In a phoneme detection experiment, we examined whether prosodic entrainment is also found in Mandarin Chinese, a tone language, where in principle the use of pitch for lexical identity may take precedence over the use of pitch cues to salience. Consistent with the results from Germanic languages, response times were facilitated when preceding intonation predicted accent on the target-bearing word. Acoustic analyses revealed greater F0 range in the preceding intonation of the predicted-accent sentences. These findings have implications for how universal and language-specific mechanisms interact in the processing of salience",
    "checked": true,
    "id": "4b279998d590949153043c9616a4abb46295c2e1",
    "semantic_title": "intonation facilitates prediction of focus even in the presence of lexical tones",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zahner17_interspeech.html": {
    "title": "Mind the Peak: When Museum is Temporarily Understood as Musical in Australian English",
    "volume": "main",
    "abstract": "Intonation languages signal pragmatic functions (e.g. information structure) by means of different pitch accent types. Acoustically, pitch accent types differ in the alignment of pitch peaks (and valleys) in regard to stressed syllables, which makes the position of pitch peaks an unreliable cue to lexical stress (even though pitch peaks and lexical stress often coincide in intonation languages). We here investigate the effect of pitch accent type on lexical activation in English. Results of a visual-world eye-tracking study show that Australian English listeners temporarily activate SWW-words ( musical) if presented with WSW-words ( museum) with early-peak accents (H+!H*), compared to medial-peak accents (L+H*). Thus, in addition to signalling pragmatic functions, the alignment of tonal targets immediately affects lexical activation in English",
    "checked": true,
    "id": "c70fa54ab94d38a2633a6833744a684ecf24e4a4",
    "semantic_title": "mind the peak: when museum is temporarily understood as musical in australian english",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2017/rognoni17_interspeech.html": {
    "title": "Pashto Intonation Patterns",
    "volume": "main",
    "abstract": "A hand-labelled Pashto speech data set containing spontaneous conversations is analysed in order to propose an intonational inventory of Pashto. Basic intonation patterns observed in the language are summarised. The relationship between pitch accent and part of speech (PoS), which was also annotated for each word in the data set, is briefly addressed The results are compared with the intonational literature on Persian, a better-described and closely-related language. The results show that Pashto intonation patterns are similar to Persian, as well as reflecting common intonation patterns such as falling tone for statements and WH-questions, and yes/no questions ending in a rising tone. The data also show that the most frequently used intonation pattern in Pashto is the so-called hat pattern. The distribution of pitch accent is quite free both in Persian and Pashto, but there is a stronger association of pitch accent with content than with function words, as is typical of stress-accent languages The phonetic realisation of focus appears to be conveyed with the same acoustic cues as in Persian, with a higher pitch excursion and longer duration of the stressed syllable of the word in focus. The data also suggest that post-focus compression (PFC) is present in Pashto",
    "checked": true,
    "id": "0f48be8067ed474b8f54c6d75fc8eaf621953372",
    "semantic_title": "pashto intonation patterns",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2017/maekawa17_interspeech.html": {
    "title": "A New Model of Final Lowering in Spontaneous Monologue",
    "volume": "main",
    "abstract": "F0 downtrend observed in spontaneous monologues in the Corpus of Spontaneous Japanese was analyzed with special attention to the modeling of final lowering. In addition to the previous finding that the domain of final lowering covers all tones in the final accentual phrase, it turned out that the last L tone in the penultimate accentual phrase played important role in the control of final lowering. It is this tone that first reached the bottom of the speaker's pitch range in the time course of utterance; it also turned out that the phonetic realization of this tone is the most stable of all tones in terms of the F0 variability. Regression model of F0 downtrends is generated by generalized linear mixed-effect modeling and evaluated by cross-validation. The mean prediction error of z-normalized F0 values in the best model was 0.25 standard deviation",
    "checked": true,
    "id": "d0f495e430b9311870601625fb7f8f0d2014e3c8",
    "semantic_title": "a new model of final lowering in spontaneous monologue",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ma17c_interspeech.html": {
    "title": "Speech Emotion Recognition with Emotion-Pair Based Framework Considering Emotion Distribution Information in Dimensional Emotion Space",
    "volume": "main",
    "abstract": "In this work, an emotion-pair based framework is proposed for speech emotion recognition, which constructs more discriminative feature subspaces for every two different emotions (emotion-pair) to generate more precise emotion bi-classification results. Furthermore, it is found that in the dimensional emotion space, the distances between some of the archetypal emotions are closer than the others. Motivated by this, a Naive Bayes classifier based decision fusion strategy is proposed, which aims at capturing such useful emotion distribution information in deciding the final emotion category for emotion recognition. We evaluated the classification framework on the USC IEMOCAP database. Experimental results demonstrate that the proposed method outperforms the hierarchical binary decision tree approach on both weighted accuracy (WA) and unweighted accuracy (UA). Moreover, our framework possesses the advantages that it can be fully automatically generated without empirical guidance and is easier to be parallelized",
    "checked": true,
    "id": "7539945b5b66760ff4b909b4dcc55ee9e261fd34",
    "semantic_title": "speech emotion recognition with emotion-pair based framework considering emotion distribution information in dimensional emotion space",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sahu17_interspeech.html": {
    "title": "Adversarial Auto-Encoders for Speech Based Emotion Recognition",
    "volume": "main",
    "abstract": "Recently, generative adversarial networks and adversarial auto-encoders have gained a lot of attention in machine learning community due to their exceptional performance in tasks such as digit classification and face recognition. They map the auto-encoder's bottleneck layer output (termed as code vectors) to different noise Probability Distribution Functions (PDFs), that can be further regularized to cluster based on class information. In addition, they also allow a generation of synthetic samples by sampling the code vectors from the mapped PDFs. Inspired by these properties, we investigate the application of adversarial auto-encoders to the domain of emotion recognition. Specifically, we conduct experiments on the following two aspects: (i) their ability to encode high dimensional feature vector representations for emotional utterances into a compressed space (with a minimal loss of emotion class discriminability in the compressed space), and (ii) their ability to regenerate synthetic samples in the original feature space, to be later used for purposes such as training emotion recognition classifiers. We demonstrate promise of adversarial auto-encoders with regards to these aspects on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) corpus and present our analysis",
    "checked": true,
    "id": "73a646f211a43de989349123787dea58fd1eb9ec",
    "semantic_title": "adversarial auto-encoders for speech based emotion recognition",
    "citation_count": 60
  },
  "https://www.isca-speech.org/archive/interspeech_2017/dang17_interspeech.html": {
    "title": "An Investigation of Emotion Prediction Uncertainty Using Gaussian Mixture Regression",
    "volume": "main",
    "abstract": "Existing continuous emotion prediction systems implicitly assume that prediction certainty does not vary with time. However, perception differences among raters and other possible sources of variability suggest that prediction certainty varies with time, which warrants deeper consideration. In this paper, the correlation between the inter-rater variability and the uncertainty of predicted emotion is firstly studied. A new paradigm that estimates the uncertainty in prediction is proposed based on the strong correlation uncovered in the RECOLA database. This is implemented by including the inter-rater variability as a representation of the uncertainty information in a probabilistic Gaussian Mixture Regression (GMR) model. In addition, we investigate the correlation between the uncertainty and the performance of a typical emotion prediction system utilizing average rating as the ground truth, by comparing the prediction performance in the lower and higher uncertainty regions. As expected, it is observed that the performance in lower uncertainty regions is better than that in higher uncertainty regions, providing a path for improving emotion prediction systems",
    "checked": true,
    "id": "abe75f450830997f4e395f0816c08911faa5597a",
    "semantic_title": "an investigation of emotion prediction uncertainty using gaussian mixture regression",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2017/khorram17_interspeech.html": {
    "title": "Capturing Long-Term Temporal Dependencies with Convolutional Networks for Continuous Emotion Recognition",
    "volume": "main",
    "abstract": "The goal of continuous emotion recognition is to assign an emotion value to every frame in a sequence of acoustic features. We show that incorporating long-term temporal dependencies is critical for continuous emotion recognition tasks. To this end, we first investigate architectures that use dilated convolutions. We show that even though such architectures outperform previously reported systems, the output signals produced from such architectures undergo erratic changes between consecutive time steps. This is inconsistent with the slow moving ground-truth emotion labels that are obtained from human annotators. To deal with this problem, we model a downsampled version of the input signal and then generate the output signal through upsampling. Not only does the resulting downsampling/upsampling network achieve good performance, it also generates smooth output trajectories. Our method yields the best known audio-only performance on the RECOLA dataset",
    "checked": true,
    "id": "4c3292c5e8a334c23b04f37a081023d80a1259c7",
    "semantic_title": "capturing long-term temporal dependencies with convolutional networks for continuous emotion recognition",
    "citation_count": 35
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chasaide17_interspeech.html": {
    "title": "Voice-to-Affect Mapping: Inferences on Language Voice Baseline Settings",
    "volume": "main",
    "abstract": "Modulations of the voice convey affect, and the precise mapping of voice-to-affect may vary for different languages. However, affect-related modulations occur relative to the baseline affect-neutral voice, which tends to differ from language to language. Little is known about the characteristic long-term voice settings for different languages, and how they influence the use of voice quality to signal affect. In this paper, data from a voice-to-affect perception test involving Russian, English, Spanish and Japanese subjects is re-examined to glean insights concerning likely baseline settings in these languages. The test used synthetic stimuli with different voice qualities (modelled on a male voice), with or without extreme f contours as might be associated with affect. Cross-language differences in affect ratings for modal and tense voice suggest that the baseline in Spanish and Japanese is inherently tenser than in Russian and English, and that as a corollary, tense voice serves as a more potent cue to high-activation affects in the latter languages. A relatively tenser baseline in Japanese and Spanish is further suggested by the fact that tense voice can be associated with intimate, a low activation state, just as readily as with the high-activation state interested",
    "checked": true,
    "id": "df4133d7bf11212bd995bbade6231f8788b7b6d9",
    "semantic_title": "voice-to-affect mapping: inferences on language voice baseline settings",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2017/neumann17_interspeech.html": {
    "title": "Attentive Convolutional Neural Network Based Speech Emotion Recognition: A Study on the Impact of Input Features, Signal Length, and Acted Speech",
    "volume": "main",
    "abstract": "Speech emotion recognition is an important and challenging task in the realm of human-computer interaction. Prior work proposed a variety of models and feature sets for training a system. In this work, we conduct extensive experiments using an attentive convolutional neural network with multi-view learning objective function. We compare system performance using different lengths of the input signal, different types of acoustic features and different types of emotion speech (improvised/scripted). Our experimental results on the Interactive Emotional Motion Capture (IEMOCAP) database reveal that the recognition performance strongly depends on the type of speech data independent of the choice of input features. Furthermore, we achieved state-of-the-art results on the improvised speech data of IEMOCAP",
    "checked": true,
    "id": "ca7fb00692339f9704b4a5201604889994825a42",
    "semantic_title": "attentive convolutional neural network based speech emotion recognition: a study on the impact of input features, signal length, and acted speech",
    "citation_count": 200
  },
  "https://www.isca-speech.org/archive/interspeech_2017/miyoshi17_interspeech.html": {
    "title": "Voice Conversion Using Sequence-to-Sequence Learning of Context Posterior Probabilities",
    "volume": "main",
    "abstract": "Voice conversion (VC) using sequence-to-sequence learning of context posterior probabilities is proposed. Conventional VC using shared context posterior probabilities predicts target speech parameters from the context posterior probabilities estimated from the source speech parameters. Although conventional VC can be built from non-parallel data, it is difficult to convert speaker individuality such as phonetic property and speaking rate contained in the posterior probabilities because the source posterior probabilities are directly used for predicting target speech parameters. In this work, we assume that the training data partly include parallel speech data and propose sequence-to-sequence learning between the source and target posterior probabilities. The conversion models perform non-linear and variable-length transformation from the source probability sequence to the target one. Further, we propose a joint training algorithm for the modules. In contrast to conventional VC, which separately trains the speech recognition that estimates posterior probabilities and the speech synthesis that predicts target speech parameters, our proposed method jointly trains these modules along with the proposed probability conversion modules. Experimental results demonstrate that our approach outperforms the conventional VC",
    "checked": true,
    "id": "641b1c0980c9d8c0d897261af608d18e6a9c7d3b",
    "semantic_title": "voice conversion using sequence-to-sequence learning of context posterior probabilities",
    "citation_count": 56
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hsu17_interspeech.html": {
    "title": "Learning Latent Representations for Speech Generation and Transformation",
    "volume": "main",
    "abstract": "An ability to model a generative process and learn a latent representation for speech in an unsupervised fashion will be crucial to process vast quantities of unlabelled speech data. Recently, deep probabilistic generative models such as Variational Autoencoders (VAEs) have achieved tremendous success in modeling natural images. In this paper, we apply a convolutional VAE to model the generative process of natural speech. We derive latent space arithmetic operations to disentangle learned latent representations. We demonstrate the capability of our model to modify the phonetic content or the speaker identity for speech segments using the derived operations, without the need for parallel supervisory data",
    "checked": true,
    "id": "4c20dff792bc447fab730e05f5e997694b67a11e",
    "semantic_title": "learning latent representations for speech generation and transformation",
    "citation_count": 138
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hashimoto17_interspeech.html": {
    "title": "Parallel-Data-Free Many-to-Many Voice Conversion Based on DNN Integrated with Eigenspace Using a Non-Parallel Speech Corpus",
    "volume": "main",
    "abstract": "This paper proposes a novel approach to parallel-data-free and many-to-many voice conversion (VC). As 1-to-1 conversion has less flexibility, researchers focus on many-to-many conversion, where speaker identity is often represented using speaker space bases. In this case, utterances of the same sentences have to be collected from many speakers. This study aims at overcoming this constraint to realize a parallel-data-free and many-to-many conversion. This is made possible by integrating deep neural networks (DNNs) with eigenspace using a non-parallel speech corpus. In our previous study, many-to-many conversion was implemented using DNN, whose training was assisted by EVGMM conversion. By realizing the function of EVGMM equivalently by constructing eigenspace with a non-parallel speech corpus, the desired conversion is made possible. A key technique here is to estimate covariance terms without given parallel data between source and target speakers. Experiments show that objective assessment scores are comparable to those of the baseline system trained with parallel data",
    "checked": true,
    "id": "2d57285227ac3f18b37c67210e6b594aecfa85ab",
    "semantic_title": "parallel-data-free many-to-many voice conversion based on dnn integrated with eigenspace using a non-parallel speech corpus",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kaneko17_interspeech.html": {
    "title": "Sequence-to-Sequence Voice Conversion with Similarity Metric Learned Using Generative Adversarial Networks",
    "volume": "main",
    "abstract": "We propose a training framework for sequence-to-sequence voice conversion (SVC). A well-known problem regarding a conventional VC framework is that acoustic-feature sequences generated from a converter tend to be over-smoothed, resulting in buzzy-sounding speech. This is because a particular form of similarity metric or distribution for parameter training of the acoustic model is assumed so that the generated feature sequence that averagely fits the training target example is considered optimal. This over-smoothing occurs as long as a manually constructed similarity metric is used. To overcome this limitation, our proposed SVC framework uses a similarity metric implicitly derived from a generative adversarial network, enabling the measurement of the distance in the high-level abstract space. This would enable the model to mitigate the over-smoothing problem caused in the low-level data space. Furthermore, we use convolutional neural networks to model the long-range context-dependencies. This also enables the similarity metric to have a shift-invariant property; thus, making the model robust against misalignment errors involved in the parallel data. We tested our framework on a non-native-to-native VC task. The experimental results revealed that the use of the proposed framework had a certain effect in improving naturalness, clarity, and speaker individuality",
    "checked": true,
    "id": "870459a595a3511462d56a4431949d9c33933979",
    "semantic_title": "sequence-to-sequence voice conversion with similarity metric learned using generative adversarial networks",
    "citation_count": 94
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ardaillon17_interspeech.html": {
    "title": "A Mouth Opening Effect Based on Pole Modification for Expressive Singing Voice Transformation",
    "volume": "main",
    "abstract": "Improving expressiveness in singing voice synthesis systems requires to perform realistic timbre transformations, e.g. for varying voice intensity. In order to sing louder, singers tend to open their mouth more widely, which changes the vocal tract's shape and resonances. This study shows, by means of signal analysis and simulations, that the main effect of mouth opening is an increase of the 1 formant's frequency (F ) and a decrease of its bandwidth (BW ). From these observations, we then propose a rule for producing a mouth opening effect, by modifying F and BW , and an approach to apply this effect on real voice sounds. This approach is based on pole modification, by changing the AR coefficients of an estimated all-pole model of the spectral envelope. Finally, listening tests have been conducted to evaluate the effectiveness of the proposed effect",
    "checked": true,
    "id": "35c50660911af169c37743747a997aa562e97a3f",
    "semantic_title": "a mouth opening effect based on pole modification for expressive singing voice transformation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mohammadi17_interspeech.html": {
    "title": "Siamese Autoencoders for Speech Style Extraction and Switching Applied to Voice Identification and Conversion",
    "volume": "main",
    "abstract": "We propose an architecture called siamese autoencoders for extracting and switching pre-determined styles of speech signals while retaining the content. We apply this architecture to a voice conversion task in which we define the content to be the linguistic message and the style to be the speaker's voice. We assume two or more data streams with the same content but unique styles. The architecture is composed of two or more separate but shared-weight autoencoders that are joined by loss functions at the hidden layers. A hidden vector is composed of style and content sub-vectors and the loss functions constrain the encodings to decompose style and content. We can select an intended target speaker either by supplying the associated style vector, or by extracting a new style vector from a new utterance, using a proposed style extraction algorithm. We focus on in-training speakers but perform some initial experiments for out-of-training speakers as well. We propose and study several types of loss functions. The experiment results show that the proposed many-to-many model is able to convert voices successfully; however, its performance does not surpass that of the state-of-the-art one-to-one model's",
    "checked": true,
    "id": "85c25573745df89e2934bd5fc1daf7335fef65a2",
    "semantic_title": "siamese autoencoders for speech style extraction and switching applied to voice identification and conversion",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sak17_interspeech.html": {
    "title": "Recurrent Neural Aligner: An Encoder-Decoder Neural Network Model for Sequence to Sequence Mapping",
    "volume": "main",
    "abstract": "We introduce an encoder-decoder recurrent neural network model called Recurrent Neural Aligner (RNA) that can be used for sequence to sequence mapping tasks. Like connectionist temporal classification (CTC) models, RNA defines a probability distribution over target label sequences including blank labels corresponding to each time step in input. The probability of a label sequence is calculated by marginalizing over all possible blank label positions. Unlike CTC, RNA does not make a conditional independence assumption for label predictions; it uses the predicted label at time t-1 as an additional input to the recurrent model when predicting the label at time t. We apply this model to end-to-end speech recognition. RNA is capable of streaming recognition since the decoder does not employ attention mechanism. The model is trained on transcribed acoustic data to predict graphemes and no external language and pronunciation models are used for decoding. We employ an approximate dynamic programming method to optimize negative log likelihood, and a sampling-based sequence discriminative training technique to fine-tune the model to minimize expected word error rate. We show that the model achieves competitive accuracy without using an external language model nor doing beam search decoding",
    "checked": true,
    "id": "7703a2c5468ecbee5b62c048339a03358ed5fe19",
    "semantic_title": "recurrent neural aligner: an encoder-decoder neural network model for sequence to sequence mapping",
    "citation_count": 115
  },
  "https://www.isca-speech.org/archive/interspeech_2017/pundak17_interspeech.html": {
    "title": "Highway-LSTM and Recurrent Highway Networks for Speech Recognition",
    "volume": "main",
    "abstract": "Recently, very deep networks, with as many as hundreds of layers, have shown great success in image classification tasks. One key component that has enabled such deep models is the use of \"skip connections\", including either residual or highway connections, to alleviate the vanishing and exploding gradient problems. While these connections have been explored for speech, they have mainly been explored for feed-forward networks. Since recurrent structures, such as LSTMs, have produced state-of-the-art results on many of our Voice Search tasks, the goal of this work is to thoroughly investigate different approaches to adding depth to recurrent structures. Specifically, we experiment with novel Highway-LSTM models with bottlenecks skip connections and show that a 10 layer model can outperform a state-of-the-art 5 layer LSTM model with the same number of parameters by 2% relative WER. In addition, we experiment with Recurrent Highway layers and find these to be on par with Highway-LSTM models, when given sufficient depth",
    "checked": true,
    "id": "f84a30bf0554ee3e1c06cad52fe66f072956849b",
    "semantic_title": "highway-lstm and recurrent highway networks for speech recognition",
    "citation_count": 27
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ravanelli17_interspeech.html": {
    "title": "Improving Speech Recognition by Revising Gated Recurrent Units",
    "volume": "main",
    "abstract": "Speech recognition is largely taking advantage of deep learning, showing that substantial benefits can be obtained by modern Recurrent Neural Networks (RNNs). The most popular RNNs are Long Short-Term Memory (LSTMs), which typically reach state-of-the-art performance in many tasks thanks to their ability to learn long-term dependencies and robustness to vanishing gradients. Nevertheless, LSTMs have a rather complex design with three multiplicative gates, that might impair their efficient implementation. An attempt to simplify LSTMs has recently led to Gated Recurrent Units (GRUs), which are based on just two multiplicative gates This paper builds on these efforts by further revising GRUs and proposing a simplified architecture potentially more suitable for speech recognition. The contribution of this work is two-fold. First, we suggest to remove the reset gate in the GRU design, resulting in a more efficient single-gate architecture. Second, we propose to replace tanh with ReLU activations in the state update equations. Results show that, in our implementation, the revised architecture reduces the per-epoch training time with more than 30% and consistently improves recognition performance across different tasks, input features, and noisy conditions when compared to a standard GRU",
    "checked": true,
    "id": "d9e75a0c9d0a6538e822e460f068ac3963fbecc1",
    "semantic_title": "improving speech recognition by revising gated recurrent units",
    "citation_count": 45
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chien17c_interspeech.html": {
    "title": "Stochastic Recurrent Neural Network for Speech Recognition",
    "volume": "main",
    "abstract": "This paper presents a new stochastic learning approach to construct a latent variable model for recurrent neural network (RNN) based speech recognition. A hybrid generative and discriminative stochastic network is implemented to build a deep classification model. In the implementation, we conduct stochastic modeling for hidden states of recurrent neural network based on the variational auto-encoder. The randomness of hidden neurons is represented by the Gaussian distribution with mean and variance parameters driven by neural weights and learned from variational inference. Importantly, the class labels of input speech frames are incorporated to regularize this deep model to sample the informative and discriminative features for reconstruction of classification outputs. We accordingly propose the stochastic RNN (SRNN) to reflect the probabilistic property in RNN classification system. A stochastic error backpropagation algorithm is implemented. The experiments on speech recognition using TIMIT and Aurora4 show the merit of the proposed SRNN",
    "checked": true,
    "id": "162efff03a6550a1e98bc81a0885007062f45738",
    "semantic_title": "stochastic recurrent neural network for speech recognition",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ratajczak17_interspeech.html": {
    "title": "Frame and Segment Level Recurrent Neural Networks for Phone Classification",
    "volume": "main",
    "abstract": "We introduce a simple and efficient frame and segment level RNN model (FS-RNN) for phone classification. It processes the input at frame level and segment level by bidirectional gated RNNs. This type of processing is important to exploit the (temporal) information more effectively compared to (i) models which solely process the input at frame level and (ii) models which process the input on segment level using features obtained by heuristic aggregation of frame level features. Furthermore, we incorporated the activations of the last hidden layer of the FS-RNN as an additional feature type in a neural higher-order CRF (NHO-CRF). In experiments, we demonstrated excellent performance on the TIMIT phone classification task, reporting a performance of 13.8% phone error rate for the FS-RNN model and 11.9% when combined with the NHO-CRF. In both cases we significantly exceeded the state-of-the-art performance",
    "checked": true,
    "id": "d652a7edda630654389eb23a0ed3fdeb4e79fca5",
    "semantic_title": "frame and segment level recurrent neural networks for phone classification",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2017/han17_interspeech.html": {
    "title": "Deep Learning-Based Telephony Speech Recognition in the Wild",
    "volume": "main",
    "abstract": "In this paper, we explore the effectiveness of a variety of Deep Learning-based acoustic models for conversational telephony speech, specifically TDNN, bLSTM and CNN-bLSTM models. We evaluated these models on both research testsets, such as Switchboard and CallHome, as well as recordings from a real-world call-center application. Our best single system, consisting of a single CNN-bLSTM acoustic model, obtained a WER of 5.7% on the Switchboard testset, and in combination with other models a WER of 5.3% was obtained. On the CallHome testset a WER of 10.1% was achieved with model combination. On the test data collected from real-world call-centers, even with model adaptation using application specific data, the WER was significantly higher at 15.0%. We performed an error analysis on the real-world data and highlight the areas where speech recognition still has challenges",
    "checked": true,
    "id": "d0eccd60d800308cd6e59810769b92b40961c09a",
    "semantic_title": "deep learning-based telephony speech recognition in the wild",
    "citation_count": 30
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lee17_interspeech.html": {
    "title": "The I4U Mega Fusion and Collaboration for NIST Speaker Recognition Evaluation 2016",
    "volume": "main",
    "abstract": "The 2016 speaker recognition evaluation (SRE'16) is the latest edition in the series of benchmarking events conducted by the National Institute of Standards and Technology (NIST). I4U is a joint entry to SRE'16 as the result from the collaboration and active exchange of information among researchers from sixteen Institutes and Universities across 4 continents. The joint submission and several of its 32 sub-systems were among top-performing systems. A lot of efforts have been devoted to two major challenges, namely, unlabeled training data and dataset shift from Switchboard-Mixer to the new Call My Net dataset. This paper summarizes the lessons learned, presents our shared view from the sixteen research groups on recent advances, major paradigm shift, and common tool chain used in speaker recognition as we have witnessed in SRE'16. More importantly, we look into the intriguing question of fusing a large ensemble of sub-systems and the potential benefit of large-scale collaboration",
    "checked": true,
    "id": "8bd21e0d011150e59a50fecb2e639f4338229643",
    "semantic_title": "the i4u mega fusion and collaboration for nist speaker recognition evaluation 2016",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2017/torrescarrasquillo17_interspeech.html": {
    "title": "The MIT-LL, JHU and LRDE NIST 2016 Speaker Recognition Evaluation System",
    "volume": "main",
    "abstract": "In this paper, the NIST 2016 SRE system that resulted from the collaboration between MIT Lincoln Laboratory and the team at Johns Hopkins University is presented. The submissions for the 2016 evaluation consisted of three fixed condition submissions and a single system open condition submission. The primary submission on the fixed (and core) condition resulted in an actual DCF of .618. Details of the submissions are discussed along with some discussion and observations of the 2016 evaluation campaign",
    "checked": true,
    "id": "d6c6f46725f538cf5960d3a4a21eea2e9605f3a8",
    "semantic_title": "the mit-ll, jhu and lrde nist 2016 speaker recognition evaluation system",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2017/colibro17_interspeech.html": {
    "title": "Nuance - Politecnico di Torino's 2016 NIST Speaker Recognition Evaluation System",
    "volume": "main",
    "abstract": "This paper describes the Nuance–Politecnico di Torino (NPT) speaker recognition system submitted to the NIST SRE16 evaluation campaign. Included are the results of post-evaluation tests, focusing on the analysis of the performance of generative and discriminative classifiers, and of score normalization. The submitted system combines the results of four GMM-IVector models, two DNN-IVector models and a GMM-SVM acoustic system. Each system exploits acoustic front-end parameters that differ by feature type and dimension. We analyze the main components of our submission, which contributed to obtaining 8.1% EER and 0.532 actual C in the challenging SRE16 Fixed condition",
    "checked": false,
    "id": "29837ec6994baac76b198ada0d50f1ec8cc1d1b3",
    "semantic_title": "loquendo - politecnico di torino's 2010 nist speaker recognition evaluation system",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhang17c_interspeech.html": {
    "title": "UTD-CRSS Systems for 2016 NIST Speaker Recognition Evaluation",
    "volume": "main",
    "abstract": "This study describes systems submitted by the Center for Robust Speech Systems (CRSS) from the University of Texas at Dallas (UTD) to the 2016 National Institute of Standards and Technology (NIST) Speaker Recognition Evaluation (SRE).We developed 4 UBM and DNN i-vector based speaker recognition systems with alternate data sets and feature representations. Given that the emphasis of the NIST SRE 2016 is on language mismatch between training and enrollment/test data, so-called domain mismatch, in our system development we focused on: (i) utilizing unlabeled in-domain data for centralizing i-vectors to alleviate the domain mismatch; (ii) selecting the proper data sets and optimizing configurations for training LDA/PLDA; (iii) introducing a newly proposed dimension reduction technique which incorporates unlabeled in-domain data before PLDA training; (iv) unsupervised speaker clustering of unlabeled data and using them alone or with previous SREs for PLDA training, and finally (v) score calibration using unlabeled data with \"pseudo\" speaker labels generated from speaker clustering. NIST evaluations show that our proposed methods were very successful for the given task",
    "checked": true,
    "id": "1108510e22449d47fd1235815b654c54025b1733",
    "semantic_title": "utd-crss systems for 2016 nist speaker recognition evaluation",
    "citation_count": 42
  },
  "https://www.isca-speech.org/archive/interspeech_2017/plchot17_interspeech.html": {
    "title": "Analysis and Description of ABC Submission to NIST SRE 2016",
    "volume": "main",
    "abstract": "We present a condensed description and analysis of the joint submission for NIST SRE 2016, by Agnitio, BUT and CRIM (ABC). We concentrate on challenges that arose during development and we analyze the results obtained on the evaluation data and on our development sets. We show that testing on mismatched, non-English and short duration data introduced in NIST SRE 2016 is a difficult problem for current state-of-the-art systems. Testing on this data brought back the issue of score normalization and it also revealed that the bottleneck features (BN), which are superior when used for telephone English, are lacking in performance against the standard acoustic features like Mel Frequency Cepstral Coefficients (MFCCs). We offer ABC's insights, findings and suggestions for building a robust system suitable for mismatched, non-English and relatively noisy data such as those in NIST SRE 2016",
    "checked": true,
    "id": "add8fa15a22edc86e2eb7edee350357f6adc350c",
    "semantic_title": "analysis and description of abc submission to nist sre 2016",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sadjadi17_interspeech.html": {
    "title": "The 2016 NIST Speaker Recognition Evaluation",
    "volume": "main",
    "abstract": "In 2016, the National Institute of Standards and Technology (NIST) conducted the most recent in an ongoing series of speaker recognition evaluations (SRE) to foster research in robust text-independent speaker recognition, as well as measure performance of current state-of-the-art systems. Compared to previous NIST SREs, SRE16 introduced several new aspects including: an entirely online evaluation platform, a fixed training data condition, more variability in test segment duration (uniformly distributed between 10s and 60s), the use of non-English (Cantonese, Cebuano, Mandarin and Tagalog) conversational telephone speech (CTS) collected outside North America, and providing labeled and unlabeled development (a.k.a. validation) sets for system hyperparameter tuning and adaptation. The introduction of the new non-English CTS data made SRE16 more challenging due to domain/channel and language mismatches as compared to previous SREs. A total of 66 research organizations from industry and academia registered for SRE16, out of which 43 teams submitted 121 valid system outputs that produced scores. This paper presents an overview of the evaluation and analysis of system performance over all primary evaluation conditions. Initial results indicate that effective use of the development data was essential for the top performing systems, and that domain/channel, language, and duration mismatch had an adverse impact on system performance",
    "checked": false,
    "id": "1108510e22449d47fd1235815b654c54025b1733",
    "semantic_title": "utd-crss systems for 2016 nist speaker recognition evaluation",
    "citation_count": 42
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kawahara17b_interspeech.html": {
    "title": "A New Cosine Series Antialiasing Function and its Application to Aliasing-Free Glottal Source Models for Speech and Singing Synthesis",
    "volume": "main",
    "abstract": "We formulated and implemented a procedure to generate aliasing-free excitation source signals. It uses a new antialiasing filter in the continuous time domain followed by an IIR digital filter for response equalization. We introduced a cosine-series-based general design procedure for the new antialiasing function. We applied this new procedure to implement the antialiased Fujisaki-Ljungqvist model. We also applied it to revise our previous implementation of the antialiased Fant-Liljencrants model. A combination of these signals and a lattice implementation of the time varying vocal tract model provides a reliable and flexible basis to test f extractors and source aperiodicity analysis methods. MATLAB implementations of these antialiased excitation source models are available as part of our open source tools for speech science",
    "checked": true,
    "id": "82ac08209ef31bd25029e3007ff10696b490a009",
    "semantic_title": "a new cosine series antialiasing function and its application to aliasing-free glottal source models for speech and singing synthesis",
    "citation_count": 27
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lopez17_interspeech.html": {
    "title": "Speaking Style Conversion from Normal to Lombard Speech Using a Glottal Vocoder and Bayesian GMMs",
    "volume": "main",
    "abstract": "Speaking style conversion is the technology of converting natural speech signals from one style to another. In this study, we focus on normal-to-Lombard conversion. This can be used, for example, to enhance the intelligibility of speech in noisy environments. We propose a parametric approach that uses a vocoder to extract speech features. These features are mapped using Bayesian GMMs from utterances spoken in normal style to the corresponding features of Lombard speech. Finally, the mapped features are converted to a Lombard speech waveform with the vocoder. Two vocoders were compared in the proposed normal-to-Lombard conversion: a recently developed glottal vocoder that decomposes speech into glottal flow excitation and vocal tract, and the widely used STRAIGHT vocoder. The conversion quality was evaluated in two subjective listening tests measuring subjective similarity and naturalness. The similarity test results show that the system is able to convert normal speech into Lombard speech for the two vocoders. However, the subjective naturalness of the converted Lombard speech was clearly better using the glottal vocoder in comparison to STRAIGHT",
    "checked": true,
    "id": "4cecfeb43a2a6a0b325542387fa4a655f2cda783",
    "semantic_title": "speaking style conversion from normal to lombard speech using a glottal vocoder and bayesian gmms",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2017/juvela17_interspeech.html": {
    "title": "Reducing Mismatch in Training of DNN-Based Glottal Excitation Models in a Statistical Parametric Text-to-Speech System",
    "volume": "main",
    "abstract": "Neural network-based models that generate glottal excitation waveforms from acoustic features have been found to give improved quality in statistical parametric speech synthesis. Until now, however, these models have been trained separately from the acoustic model. This creates mismatch between training and synthesis, as the synthesized acoustic features used for the excitation model input differ from the original inputs, with which the model was trained on. Furthermore, due to the errors in predicting the vocal tract filter, the original excitation waveforms do not provide perfect reconstruction of the speech waveform even if predicted without error. To address these issues and to make the excitation model more robust against errors in acoustic modeling, this paper proposes two modifications to the excitation model training scheme. First, the excitation model is trained in a connected manner, with inputs generated by the acoustic model. Second, the target glottal waveforms are re-estimated by performing glottal inverse filtering with the predicted vocal tract filters. The results show that both of these modifications improve performance measured in MSE and MFCC distortion, and slightly improve the subjective quality of the synthetic speech",
    "checked": true,
    "id": "1eb145cb66a84f5bc36ba06ba262e2b973b16a42",
    "semantic_title": "reducing mismatch in training of dnn-based glottal excitation models in a statistical parametric text-to-speech system",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sorin17_interspeech.html": {
    "title": "Semi Parametric Concatenative TTS with Instant Voice Modification Capabilities",
    "volume": "main",
    "abstract": "Recently, a glottal vocoder has been integrated in the IBM concatenative TTS system and certain configurable global voice transformations were defined in the vocoder parameter space. The vocoder analysis employs a novel robust glottal source parameter estimation strategy. The vocoder is applied to the voiced speech only, while unvoiced speech is kept unparameterized, thus contributing to the perceived naturalness of the synthesized speech The semi-parametric system enables independent modifications of the glottal source and vocal tract components on-the-fly by embedding the voice transformations in the synthesis process. The transformations effect ranges from slight voice altering to a complete change of the perceived speaker personality. Pitch modifications enhance these changes. At the same time, the voice transformations are simple enough to be easily controlled externally to the system. This allows the users either to fine tune the voice sound or to create instantly multiple distinct virtual voices. In both cases, the synthesis is based on a large and meticulously cleaned concatenative TTS voice with a broad phonetic coverage. In this paper we present the system and provide subjective evaluations of its voice modification capabilities The technology presented in this paper is implemented in IBM Watson TTS service",
    "checked": true,
    "id": "7b370a25a7b71501dff3fcfd80084b24262199d5",
    "semantic_title": "semi parametric concatenative tts with instant voice modification capabilities",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2017/manriquez17_interspeech.html": {
    "title": "Modeling Laryngeal Muscle Activation Noise for Low-Order Physiological Based Speech Synthesis",
    "volume": "main",
    "abstract": "Physiological-based synthesis using low order lumped-mass models of phonation have been shown to mimic and predict complex physical phenomena observed in normal and pathological speech production, and have received significant attention due to their ability to efficiently perform comprehensive parametric investigations that are cost prohibitive with more advanced computational tools. Even though these numerical models have been shown to be useful research and clinical tools, several physiological aspects of them remain to be explored. One of the key components that has been neglected is the natural fluctuation of the laryngeal muscle activity that affects the configuration of the model parameters. In this study, a physiologically-based laryngeal muscle activation model that accounts for random fluctuations is proposed. The method is expected to improve the ability to model muscle related pathologies, such as muscle tension dysphonia and Parkinson's disease. The mathematical framework and underlying assumptions are described, and the effects of the added random muscle activity is tested in a well-known body-cover model of the vocal folds with acoustic propagation and interaction. Initial simulations illustrate that the random fluctuations in the muscle activity impact the resulting kinematics to varying degrees depending on the laryngeal configuration",
    "checked": true,
    "id": "0071037f870a5a85028d4eec224e50eed6df878e",
    "semantic_title": "modeling laryngeal muscle activation noise for low-order physiological based speech synthesis",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/espic17_interspeech.html": {
    "title": "Direct Modelling of Magnitude and Phase Spectra for Statistical Parametric Speech Synthesis",
    "volume": "main",
    "abstract": "We propose a simple new representation for the FFT spectrum tailored to statistical parametric speech synthesis. It consists of four feature streams that describe magnitude, phase and fundamental frequency using real numbers. The proposed feature extraction method does not attempt to decompose the speech structure (e.g., into source+filter or harmonics+noise). By avoiding the simplifications inherent in decomposition, we can dramatically reduce the \"phasiness\" and \"buzziness\" typical of most vocoders. The method uses simple and computationally cheap operations and can operate at a lower frame rate than the 200 frames-per-second typical in many systems. It avoids heuristics and methods requiring approximate or iterative solutions, including phase unwrapping Two DNN-based acoustic models were built — from male and female speech data — using the Merlin toolkit. Subjective comparisons were made with a state-of-the-art baseline, using the STRAIGHT vocoder. In all variants tested, and for both male and female voices, the proposed method substantially outperformed the baseline. We provide source code to enable our complete system to be replicated",
    "checked": true,
    "id": "86912603ea04354a9c23bea1a7a652c3cac7c996",
    "semantic_title": "direct modelling of magnitude and phase spectra for statistical parametric speech synthesis",
    "citation_count": 28
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kember17_interspeech.html": {
    "title": "Similar Prosodic Structure Perceived Differently in German and English",
    "volume": "main",
    "abstract": "English and German have similar prosody, but their speakers realize some pitch falls (not rises) in subtly different ways. We here test for asymmetry in perception. An ABX discrimination task requiring F0 slope or duration judgements on isolated vowels revealed no cross-language difference in duration or F0 fall discrimination, but discrimination of rises (realized similarly in each language) was less accurate for English than for German listeners. This unexpected finding may reflect greater sensitivity to rising patterns by German listeners, or reduced sensitivity by English listeners as a result of extensive exposure to phrase-final rises (\"uptalk\") in their language",
    "checked": true,
    "id": "179d7f4b91153b524fb36736098f506c5ed729fa",
    "semantic_title": "similar prosodic structure perceived differently in german and english",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hou17_interspeech.html": {
    "title": "Disambiguate or not? — The Role of Prosody in Unambiguous and Potentially Ambiguous Anaphora Production in Strictly Mandarin Parallel Structures",
    "volume": "main",
    "abstract": "It has been observed that the interpretation of pronouns can depend on their accentuation patterns in parallel sentences as \"John hit Bill and then George hit him\", in which ‘him' refers to Bill when unaccented but shifts to John when accented. While accentuation is widely regarded as a means of disambiguation, some studies have noticed that it also extends to unambiguous anaphors [7–10]. From the perspective of production, however, no strong experimental confirmation was found for the ‘shift' function of accented pronouns, which is due to the fact that production research has mainly focused on corpora [5, 6]. Hence, the nature of the accent on anaphors still remains obscure. By manipulating referential shift and ambiguity, this study explores the role of prosody in anaphora production in strictly Mandarin parallel structures. The results reveal a significantly higher F and longer duration for anaphors in referentially shifted conditions, suggesting that anaphoric accentuation signals a referential change in strictly parallel structures in Mandarin. No evidence was found that ambiguity plays a role in anaphoric accentuation. This finding challenges the general view on accented pronouns and will deepen our understanding on semantics-prosody relationship",
    "checked": false,
    "id": "61fecd3ba9d4d776d997ef6b08e167642198dba5",
    "semantic_title": "disambiguate or not? - the role of prosody in unambiguous and potentially ambiguous anaphora production in strictly mandarin parallel structures",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/athanasopoulou17_interspeech.html": {
    "title": "Acoustic Properties of Canonical and Non-Canonical Stress in French, Turkish, Armenian and Brazilian Portuguese",
    "volume": "main",
    "abstract": "Languages are often categorized as having either predictable (fixed or quantity-sensitive) or non-predictable stress. Despite their name, fixed stress languages may have exceptions, so in fact, their stress does not always appear in the same position. Since predictability has been shown to affect certain speech phenomena, with additional or redundant acoustic cues being provided when the linguistic content is less predictable (e.g., Smooth Signal Redundancy Hypothesis), we investigate whether, and to what extent, the predictability of stress position affects the manifestation of stress in different languages. We examine the acoustic properties of stress in three languages classified as having fixed stress (Turkish, French, Armenian), with exceptions, and in one language with non-predictable-stress, Brazilian Portuguese. Specifically, we compare the manifestation of stress in the canonical stress (typically \"fixed\") position with its manifestation in the non-canonical (exceptional) position, where it would potentially be less predictable. We also compare these patterns with the manifestation of stress in Portuguese, in both the \"default\" penultimate and the less common final position. Our results show that stress is manifested quite similarly in canonical and non-canonical positions in the \"fixed\" stress languages and stress is most clearly produced when it is least predictable",
    "checked": true,
    "id": "e08a1c15b5d59e5fd72093ec094f582f80731bd5",
    "semantic_title": "acoustic properties of canonical and non-canonical stress in french, turkish, armenian and brazilian portuguese",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2017/plug17_interspeech.html": {
    "title": "Phonological Complexity, Segment Rate and Speech Tempo Perception",
    "volume": "main",
    "abstract": "Studies of speech tempo commonly use syllable or segment rate as a proxy measure for perceived tempo. In languages whose phonologies allow substantial syllable complexity these measures can produce figures on quite different scales; however, little is known about the correlation between syllable and segment rate measurements on the one hand and naïve listeners' tempo judgements on the other We follow up on the findings of one relevant study on German [1], which suggest that listeners attend to both syllable and segment rates in making tempo estimates, through a weighted average of the rates in which syllable rate carries more weight. We report on an experiment in which we manipulate phonological complexity in English utterance pairs that are constant in syllable rate. Listeners decide for each pair which utterance sounds faster. Our results suggest that differences in segment rate that do not correspond to differences in syllable rate have little impact on perceived speech tempo in English",
    "checked": true,
    "id": "ba73fc540baaf55498187c388fc8ac2adbfdfdea",
    "semantic_title": "phonological complexity, segment rate and speech tempo perception",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2017/yang17_interspeech.html": {
    "title": "On the Duration of Mandarin Tones",
    "volume": "main",
    "abstract": "The present study compared the duration of Mandarin tones in three types of speech contexts: isolated monosyllables, formal text-reading passages, and casual conversations. A total of 156 adult speakers was recruited. The speech materials included 44 monosyllables recorded from each of 121 participants, 18 passages read by 2 participants, and 20 conversations conducted by 33 participants. The duration pattern of the four lexical tones in the isolated monosyllables was consistent with the pattern described in previous literature. However, the duration of the four lexical tones became much shorter and tended to converge to that of the neutral tone (i.e., tone 0) in the text-reading and conversational speech. The maximum-likelihood estimator revealed that the durational cue contributed to tone recognition in the isolated monosyllables. With a single speaker, the average tone recognition based on duration alone could reach approximately 65% correct. As the number of speakers increased (e.g., ≥ 4), tone recognition performance dropped to approximately 45% correct. In conversational speech, the maximum likelihood estimation of tones based on duration cues was only 23% correct. The tone duration provided little useful cue to differentiate Mandarin tonal identity in everyday situations",
    "checked": true,
    "id": "5bb0dd554dd188ef10ce6889d1466ac3ff954f73",
    "semantic_title": "on the duration of mandarin tones",
    "citation_count": 23
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ewald17_interspeech.html": {
    "title": "The Formant Dynamics of Long Close Vowels in Three Varieties of Swedish",
    "volume": "main",
    "abstract": "This study compares the acoustic realisation of /iː yː ʉː uː/ in three varieties of Swedish: Central Swedish, Estonian Swedish, and Finland Swedish. Vowel tokens were extracted from isolated words produced by six elderly female speakers from each variety. Trajectories of the first three formants were modelled with discrete cosine transform (DCT) coefficients, enabling the comparison of the formant means as well as the direction and magnitude of the formant movement. Cross-dialectal differences were found in all measures and in all vowels. The most noteworthy feature of the Estonian Swedish long close vowel inventory is the lack of /yː/. For Finland Swedish it was shown that /iː/ and /yː/ are more close than in Central Swedish. The realisation of /ʉː/ varies from front in Central Swedish, to central in Estonian Swedish, and back in Finland Swedish. On average, the Central Swedish vowels exhibited a higher degree of formant movement than the vowels in the other two varieties. In the present study, regional variation in Swedish vowels was for the first time investigated using DCT coefficients. The results stress the importance of taking formant dynamics into account even in the analysis of nominal monophthongs",
    "checked": true,
    "id": "f3caa781ff7fdd2fcd317348cfb47f9c57ec317c",
    "semantic_title": "the formant dynamics of long close vowels in three varieties of swedish",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2017/qian17_interspeech.html": {
    "title": "Bidirectional LSTM-RNN for Improving Automated Assessment of Non-Native Children's Speech",
    "volume": "main",
    "abstract": "Recent advances in ASR and spoken language processing have led to improved systems for automated assessment for spoken language. However, it is still challenging for automated scoring systems to achieve high performance in terms of the agreement with human experts when applied to non-native children's spontaneous speech. The subpar performance is mainly caused by the relatively low recognition rate on non-native children's speech. In this paper, we investigate different neural network architectures for improving non-native children's speech recognition and the impact of the features extracted from the corresponding ASR output on the automated assessment of speaking proficiency. Experimental results show that bidirectional LSTM-RNN can outperform feed-forward DNN in ASR, with an overall relative WER reduction of 13.4%. The improved speech recognition can then boost the language proficiency assessment performance. Correlations between the rounded automated scores and expert scores range from 0.66 to 0.70 for the three speaking tasks studied, similar to the human-human agreement levels for these tasks",
    "checked": true,
    "id": "c6f72739a51e0fccd6a08aeec667b948f57816ba",
    "semantic_title": "bidirectional lstm-rnn for improving automated assessment of non-native children's speech",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2017/yue17_interspeech.html": {
    "title": "Automatic Scoring of Shadowing Speech Based on DNN Posteriors and Their DTW",
    "volume": "main",
    "abstract": "Shadowing has become a well-known method to improve learners' overall proficiency. Our previous studies realized automatic scoring of shadowing speech using HMM phoneme posteriors, called GOP (Goodness of Pronunciation) and learners' TOEIC scores were predicted adequately. In this study, we enhance our studies from multiple angles: 1) a much larger amount of shadowing speech is collected, 2) manual scoring of these utterances is done by two native teachers, 3) DNN posteriors are introduced instead of HMM ones, 4) language-independent shadowing assessment based on posteriors-based DTW (Dynamic Time Warping) is examined. Experiments suggest that, compared to HMM, DNN can improve teacher-machine correlation largely by 0.37 and DTW based on DNN posteriors shows as high correlation as 0.74 even when posterior calculation is done using a different language from the target language of learning",
    "checked": true,
    "id": "a56ad57da1e5a6892cdc60ae754e2acc63b8b675",
    "semantic_title": "automatic scoring of shadowing speech based on dnn posteriors and their dtw",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lee17b_interspeech.html": {
    "title": "Off-Topic Spoken Response Detection Using Siamese Convolutional Neural Networks",
    "volume": "main",
    "abstract": "In this study, we developed an off-topic response detection system to be used in the context of the automated scoring of non-native English speakers' spontaneous speech. Based on transcriptions generated from an ASR system trained on non-native speakers' speech and various semantic similarity features, the system classified each test response as an on-topic or off-topic response. The recent success of deep neural networks (DNN) in text similarity detection led us to explore DNN-based document similarity features. Specifically, we used a siamese adaptation of the convolutional network, due to its efficiency in learning similarity patterns simultaneously from both responses and questions used to elicit responses. In addition, a baseline system was developed using a standard vector space model (VSM) trained on sample responses for each question. The accuracy of the siamese CNN-based system was 0.97 and there was a 50% relative error reduction compared to the standard VSM-based system. Furthermore, the accuracy of the siamese CNN-based system was consistent across different questions",
    "checked": true,
    "id": "95c07805e04cef09e4cb373d906d0eaadb1afd31",
    "semantic_title": "off-topic spoken response detection using siamese convolutional neural networks",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2017/arora17_interspeech.html": {
    "title": "Phonological Feature Based Mispronunciation Detection and Diagnosis Using Multi-Task DNNs and Active Learning",
    "volume": "main",
    "abstract": "This paper presents a phonological feature based computer aided pronunciation training system for the learners of a new language (L2). Phonological features allow analysing the learners' mispronunciations systematically and rendering the feedback more effectively. The proposed acoustic model consists of a multi-task deep neural network, which uses a shared representation for estimating the phonological features and HMM state probabilities. Moreover, an active learning based scheme is proposed to efficiently deal with the cost of annotation, which is done by expert teachers, by selecting the most informative samples for annotation. Experimental evaluations are carried out for German and Italian native-speakers speaking English. For mispronunciation detection, the proposed feature-based system outperforms conventional GOP measure and classifier based methods, while providing more detailed diagnosis. Evaluations also demonstrate the advantage of active learning based sampling over random sampling",
    "checked": true,
    "id": "ad1df1a960176f429092b34bccea5d99fa556dc4",
    "semantic_title": "phonological feature based mispronunciation detection and diagnosis using multi-task dnns and active learning",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2017/proenca17_interspeech.html": {
    "title": "Detection of Mispronunciations and Disfluencies in Children Reading Aloud",
    "volume": "main",
    "abstract": "To automatically evaluate the performance of children reading aloud or to follow a child's reading in reading tutor applications, different types of reading disfluencies and mispronunciations must be accounted for. In this work, we aim to detect most of these disfluencies in sentence and pseudoword reading. Detecting incorrectly pronounced words, and quantifying the quality of word pronunciations, is arguably the hardest task. We approach the challenge as a two-step process. First, a segmentation using task-specific lattices is performed, while detecting repetitions and false starts and providing candidate segments for words. Then, candidates are classified as mispronounced or not, using multiple features derived from likelihood ratios based on phone decoding and forced alignment, as well as additional meta-information about the word. Several classifiers were explored (linear fit, neural networks, support vector machines) and trained after a feature selection stage to avoid overfitting. Improved results are obtained using feature combination compared to using only the log likelihood ratio of the reference word (22% versus 27% miss rate at constant 5% false alarm rate)",
    "checked": true,
    "id": "db39304c600d9d9f0f4b5dc5a3adfe2137adbc46",
    "semantic_title": "detection of mispronunciations and disfluencies in children reading aloud",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2017/escuderomancebo17_interspeech.html": {
    "title": "Automatic Assessment of Non-Native Prosody by Measuring Distances on Prosodic Label Sequences",
    "volume": "main",
    "abstract": "The aim of this paper is to investigate how automatic prosodic labeling systems contribute to the evaluation of non-native pronunciation. In particular, it examines the efficiency of a group of metrics to evaluate the prosodic competence of non-native speakers, based on the information provided by sequences of labels in the analysis of both native and non-native speech. A group of Sp_ToBI labels were obtained by means of an automatic labeling system for the speech of native and non-native speakers who read the same texts. The metrics assessed the differences in the prosodic labels for both speech samples. The results showed the efficiency of the metrics to set apart both groups of speakers. Furthermore, they exhibited how non-native speakers (American and Japanese speakers) improved their Spanish productions after doing a set of listening and repeating activities. Finally, this study also shows that the results provided by the metrics are correlated with the scores given by human evaluators on the productions of the different speakers",
    "checked": true,
    "id": "10a78a106182d8396dd61a32e6999361049c5e2c",
    "semantic_title": "automatic assessment of non-native prosody by measuring distances on prosodic label sequences",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ward17_interspeech.html": {
    "title": "Inferring Stance from Prosody",
    "volume": "main",
    "abstract": "Speech conveys many things beyond content, including aspects of stance and attitude that have not been much studied. Considering 14 aspects of stance as they occur in radio news stories, we investigated the extent to which they could be inferred from prosody. By using time-spread prosodic features and by aggregating local estimates, many aspects of stance were at least somewhat predictable, with results significantly better than chance for many stance aspects, including, across English, Mandarin and Turkish, good, typical, local, background, new information, and relevant to a large group",
    "checked": true,
    "id": "bb95d34ded54579a2d937c1b593b319b90c2baf1",
    "semantic_title": "inferring stance from prosody",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2017/levow17_interspeech.html": {
    "title": "Exploring Dynamic Measures of Stance in Spoken Interaction",
    "volume": "main",
    "abstract": "Stance-taking, the expression of opinions or attitudes, informs the process of negotiation, argumentation, and decision-making. While receiving significant attention in text materials in work on the related areas of subjectivity and sentiment analysis, the expression of stance in speech remains less explored. Prior analysis of the acoustics of stance-expression in conversational speech has identified some significant differences across dimensions of stance-related behavior. However, that analysis, as in much prior work, relied on simple functionals of pitch, energy, and duration, including maxima, minima, means, and ranges. In contrast, the current work focuses on exploiting measures that capture the dynamics of the pitch and energy contour. We employ features based on subband autocorrelation measures of pitch change and variants of the modulation spectrum. Using a corpus of conversational speech manually annotated for dimensions of stance-taking, we demonstrate that these measures of pitch and energy dynamics can help to characterize and distinguish among stance-related behaviors in speech",
    "checked": true,
    "id": "c9562ee14b07f3923def8d08f3ddf0c82f0cb2ae",
    "semantic_title": "exploring dynamic measures of stance in spoken interaction",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2017/barriere17_interspeech.html": {
    "title": "Opinion Dynamics Modeling for Movie Review Transcripts Classification with Hidden Conditional Random Fields",
    "volume": "main",
    "abstract": "In this paper, the main goal is to detect a movie reviewer's opinion using hidden conditional random fields. This model allows us to capture the dynamics of the reviewer's opinion in the transcripts of long unsegmented audio reviews that are analyzed by our system. High level linguistic features are computed at the level of inter-pausal segments. The features include syntactic features, a statistical word embedding model and subjectivity lexicons. The proposed system is evaluated on the ICT-MMMO corpus. We obtain a F1-score of 82%, which is better than logistic regression and recurrent neural network approaches. We also offer a discussion that sheds some light on the capacity of our system to adapt the word embedding model learned from general written texts data to spoken movie reviews and thus model the dynamics of the opinion",
    "checked": true,
    "id": "8caef345759113bbd3f70db6a9d9d98d86d63164",
    "semantic_title": "opinion dynamics modeling for movie review transcripts classification with hidden conditional random fields",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2017/luo17_interspeech.html": {
    "title": "Transfer Learning Between Concepts for Human Behavior Modeling: An Application to Sincerity and Deception Prediction",
    "volume": "main",
    "abstract": "Transfer learning (TL) involves leveraging information from sources outside the domain at hand for enhancing model performances. Popular TL methods either directly use the data or adapt the models learned on out-of-domain resources and incorporate them within in-domain models. TL methods have shown promise in several applications such as text classification, cross-domain language classification and emotion recognition. In this paper, we propose TL methods to computational human behavioral trait modeling. Many behavioral traits are abstract constructs (e.g., sincerity of an individual), and are often conceptually related to other constructs (e.g., level of deception) making TL methods an attractive option for their modeling. We consider the problem of automatically predicting human sincerity and deception from behavioral data while leveraging transfer of knowledge from each other. We compare our methods against baseline models trained only on in-domain data. Our best models achieve an Unweighted Average Recall (UAR) of 72.02% in classifying deception (baseline: 69.64%). Similarly, applied methods achieve Spearman's/Pearson's correlation values of 49.37%/48.52% between true and predicted sincerity scores (baseline: 46.51%/41.58%), indicating the success and the potential of TL for such human behavior tasks",
    "checked": true,
    "id": "8ec3bf06bc143d48767667e9e83ba7d45f1155c2",
    "semantic_title": "transfer learning between concepts for human behavior modeling: an application to sincerity and deception prediction",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2017/schroder17_interspeech.html": {
    "title": "The Sound of Deception — What Makes a Speaker Credible?",
    "volume": "main",
    "abstract": "The detection of deception in human speech is a difficult task but can be performed above chance level by human listeners even when only audio data is provided. Still, it is highly contested, which speech features could be used to help identify lies. In this study, we examined a set of phonetic and paralinguistic cues and their influence on the credibility of speech using an analysis-by-synthesis approach. 33 linguistically neutral utterances with different manipulated cues (unfilled pauses, phonation type, higher speech rate, tremolo and raised F0) were synthesized using articulatory synthesis. These utterances were presented to 50 subjects who were asked to choose the more credible utterance. From those choices, a credibility score was calculated for each cue. The results show a significant increase in credibility when a tremolo is inserted or the breathiness is increased, and a decrease in credibility when a pause is inserted or the F0 is raised. Other cues also had a significant, but less pronounced influence on the credibility while some only showed trends. In summary, the study showed that the credibility of a factually unverifiable utterance is in parts controlled by the presented paralinguistic cues",
    "checked": false,
    "id": "3dd5c272b2bcadfc06608c2fcd5ae1e0040eb63c",
    "semantic_title": "the sound of deception - what makes a speaker credible?",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mendels17_interspeech.html": {
    "title": "Hybrid Acoustic-Lexical Deep Learning Approach for Deception Detection",
    "volume": "main",
    "abstract": "Automatic deception detection is an important problem with far-reaching implications for many disciplines. We present a series of experiments aimed at automatically detecting deception from speech. We use the Columbia X-Cultural Deception (CXD) Corpus, a large-scale corpus of within-subject deceptive and non-deceptive speech, for training and evaluating our models. We compare the use of spectral, acoustic-prosodic, and lexical feature sets, using different machine learning models. Finally, we design a single hybrid deep model with both acoustic and lexical features trained jointly that achieves state-of-the-art results on the CXD corpus",
    "checked": true,
    "id": "fda72ce8b95866dd924326b09fca35a4a68ecab7",
    "semantic_title": "hybrid acoustic-lexical deep learning approach for deception detection",
    "citation_count": 51
  },
  "https://www.isca-speech.org/archive/interspeech_2017/swart17_interspeech.html": {
    "title": "A Generative Model for Score Normalization in Speaker Recognition",
    "volume": "main",
    "abstract": "We propose a theoretical framework for thinking about score normalization, which confirms that normalization is not needed under (admittedly fragile) ideal conditions. If, however, these conditions are not met, e.g. under data-set shift between training and runtime, our theory reveals dependencies between scores that could be exploited by strategies such as score normalization. Indeed, it has been demonstrated over and over experimentally, that various ad-hoc score normalization recipes do work. We present a first attempt at using probability theory to design a generative score-space normalization model which gives similar improvements to ZT-norm on the text-dependent RSR 2015 database",
    "checked": true,
    "id": "2d1094ae908b23b7b92d286f11e77dd0bc40fb85",
    "semantic_title": "a generative model for score normalization in speaker recognition",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2017/dey17_interspeech.html": {
    "title": "Content Normalization for Text-Dependent Speaker Verification",
    "volume": "main",
    "abstract": "Subspace based techniques, such as i-vector and Joint Factor Analysis (JFA) have shown to provide state-of-the-art performance for fixed phrase based text-dependent speaker verification. However, the error rates of such systems on the random digit task of RSR dataset are higher than that of Gaussian Mixture Model-Universal Background Model (GMM-UBM). In this paper, we aim at improving i-vector system by normalizing the content of the enrollment data to match the test data. We estimate i-vectors for each frames of a speech utterance (also called online i-vectors). The largest similarity scores across frames between enrollment and test are taken using these online i-vectors to obtain speaker verification scores. Experiments on Part3 of RSR corpora show that the proposed approach achieves 12% relative improvement in equal error rate over a GMM-UBM based baseline system",
    "checked": true,
    "id": "556517c3cb5671d551dd4eb19aec4a0e6dff17c4",
    "semantic_title": "content normalization for text-dependent speaker verification",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhang17d_interspeech.html": {
    "title": "End-to-End Text-Independent Speaker Verification with Triplet Loss on Short Utterances",
    "volume": "main",
    "abstract": "Text-independent speaker verification against short utterances is still challenging despite of recent advances in the field of speaker recognition with i-vector framework. In general, to get a robust i-vector representation, a satisfying amount of data is needed in the MAP adaptation step, which is hard to meet under short duration constraint. To overcome this, we present an end-to-end system which directly learns a mapping from speech features to a compact fixed length speaker discriminative embedding where the Euclidean distance is employed for measuring similarity within trials. To learn the feature mapping, a modified Inception Net with residual block is proposed to optimize the triplet loss function. The input of our end-to-end system is a fixed length spectrogram converted from an arbitrary length utterance. Experiments show that our system consistently outperforms a conventional i-vector system on short duration speaker verification tasks. To test the limit under various duration conditions, we also demonstrate how our end-to-end system behaves with different duration from 2s–4s",
    "checked": true,
    "id": "ef8cebdb9e05c4f12e5019531897ad5f2e29afaa",
    "semantic_title": "end-to-end text-independent speaker verification with triplet loss on short utterances",
    "citation_count": 214
  },
  "https://www.isca-speech.org/archive/interspeech_2017/yu17_interspeech.html": {
    "title": "Adversarial Network Bottleneck Features for Noise Robust Speaker Verification",
    "volume": "main",
    "abstract": "In this paper, we propose a noise robust bottleneck feature representation which is generated by an adversarial network (AN). The AN includes two cascade connected networks, an encoding network (EN) and a discriminative network (DN). Mel-frequency cepstral coefficients (MFCCs) of clean and noisy speech are used as input to the EN and the output of the EN is used as the noise robust feature. The EN and DN are trained in turn, namely, when training the DN, noise types are selected as the training labels and when training the EN, all labels are set as the same, i.e., the clean speech label, which aims to make the AN features invariant to noise and thus achieve noise robustness. We evaluate the performance of the proposed feature on a Gaussian Mixture Model-Universal Background Model based speaker verification system, and make comparison to MFCC features of speech enhanced by short-time spectral amplitude minimum mean square error (STSA-MMSE) and deep neural network-based speech enhancement (DNN-SE) methods. Experimental results on the RSR2015 database show that the proposed AN bottleneck feature (AN-BN) dramatically outperforms the STSA-MMSE and DNN-SE based MFCCs for different noise types and signal-to-noise ratios. Furthermore, the AN-BN feature is able to improve the speaker verification performance under the clean condition",
    "checked": true,
    "id": "7f70160a11d62b75009327664dd4532aad8faa7d",
    "semantic_title": "adversarial network bottleneck features for noise robust speaker verification",
    "citation_count": 30
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wang17g_interspeech.html": {
    "title": "What Does the Speaker Embedding Encode?",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "7717d8196c20659707cec12eb857acf1b06525f3",
    "semantic_title": "what does the speaker embedding encode?",
    "citation_count": 52
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ma17d_interspeech.html": {
    "title": "Incorporating Local Acoustic Variability Information into Short Duration Speaker Verification",
    "volume": "main",
    "abstract": "State-of-the-art speaker verification systems are based on the total variability model to compactly represent the acoustic space. However, short duration utterances only contain limited phonetic content, potentially resulting in an incomplete representation being captured by the total variability model thus leading to poor speaker verification performance. In this paper, a technique to incorporate component-wise local acoustic variability information into the speaker verification framework is proposed. Specifically, Gaussian Probabilistic Linear Discriminant Analysis (G-PLDA) of the supervector space, with a block diagonal covariance assumption, is used in conjunction with the traditional total variability model. Experimental results obtained using the NIST SRE 2010 dataset show that the incorporation of the proposed method leads to relative improvements of 20.48% and 18.99% in the 3 second condition for male and female speech respectively",
    "checked": true,
    "id": "598ac69849784409fe2197be1e9b9cb3b807c317",
    "semantic_title": "incorporating local acoustic variability information into short duration speaker verification",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhong17_interspeech.html": {
    "title": "DNN i-Vector Speaker Verification with Short, Text-Constrained Test Utterances",
    "volume": "main",
    "abstract": "We investigate how to improve the performance of DNN i-vector based speaker verification for short, text-constrained test utterances, e.g. connected digit strings. A text-constrained verification, due to its smaller, limited vocabulary, can deliver better performance than a text-independent one for a short utterance. We study the problem with \"phonetically aware\" Deep Neural Net (DNN) in its capability on \"stochastic phonetic-alignment\" in constructing supervectors and estimating the corresponding i-vectors with two speech databases: a large vocabulary, conversational, speaker independent database (Fisher) and a small vocabulary, continuous digit database (RSR2015 Part III). The phonetic alignment efficiency and resultant speaker verification performance are compared with differently sized senone sets which can characterize the phonetic pronunciations of utterances in the two databases. Performance on RSR2015 Part III evaluation shows a relative improvement of EER, i.e., 7.89% for male speakers and 3.54% for female speakers with only digit related senones. The DNN bottleneck features were also studied to investigate their capability of extracting phonetic sensitive information which is useful for text-independent or text-constrained speaker verifications. We found that by tandeming MFCC with bottleneck features, EERs can be further reduced",
    "checked": true,
    "id": "4e926383ad15245d4c60323c89daf5c4e35d9d79",
    "semantic_title": "dnn i-vector speaker verification with short, text-constrained test utterances",
    "citation_count": 25
  },
  "https://www.isca-speech.org/archive/interspeech_2017/vestman17_interspeech.html": {
    "title": "Time-Varying Autoregressions for Speaker Verification in Reverberant Conditions",
    "volume": "main",
    "abstract": "In poor room acoustics conditions, speech signals received by a microphone might become corrupted by the signals' delayed versions that are reflected from the room surfaces (e.g. wall, floor). This phenomenon, reverberation, drops the accuracy of automatic speaker verification systems by causing mismatch between the training and testing. Since reverberation causes temporal smearing to the signal, one way to tackle its effects is to study robust feature extraction, particularly based on long-time temporal feature extraction. This approach has been adopted previously in the form of 2-dimensional autoregressive (2DAR) feature extraction scheme by using frequency domain linear prediction (FDLP). In 2DAR, FDLP processing is followed by time domain linear prediction (TDLP). In the current study, we propose modifying the latter part of the 2DAR feature extraction scheme by replacing TDLP with time-varying linear prediction (TVLP) to add an extra layer of temporal processing. Our speaker verification experiments using the proposed features with the text-dependent RedDots corpus show small but consistent improvements in clean and reverberant conditions (up to 6.5%) over the 2DAR features and large improvements over the MFCC features in reverberant conditions (up to 46.5%)",
    "checked": true,
    "id": "625b8eae05f9f4575788233ec1ec02e05f8357ca",
    "semantic_title": "time-varying autoregressions for speaker verification in reverberant conditions",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bhattacharya17_interspeech.html": {
    "title": "Deep Speaker Embeddings for Short-Duration Speaker Verification",
    "volume": "main",
    "abstract": "The performance of a state-of-the-art speaker verification system is severely degraded when it is presented with trial recordings of short duration. In this work we propose to use deep neural networks to learn short-duration speaker embeddings. We focus on the 5s-5s condition, wherein both sides of a verification trial are 5 seconds long. In our previous work we established that learning a non-linear mapping from i-vectors to speaker labels is beneficial for speaker verification [1]. In this work we take the idea of learning a speaker classifier one step further — we apply deep neural networks directly to time-frequency speech representations. We propose two feed-forward network architectures for this task. Our best model is based on a deep convolutional architecture wherein recordings are treated as images. From our experimental findings we advocate treating utterances as images or ‘speaker snapshots', much like in face recognition. Our convolutional speaker embeddings perform significantly better than i-vectors when scoring is done using cosine distance, where the relative improvement is 23.5%. The proposed deep embeddings combined with cosine distance also outperform a state-of-the-art i-vector verification system by 1%, providing further empirical evidence in favor of our learned speaker features",
    "checked": true,
    "id": "1fc867b43092fe83c4e0bfa38a9a45ffaea86deb",
    "semantic_title": "deep speaker embeddings for short-duration speaker verification",
    "citation_count": 131
  },
  "https://www.isca-speech.org/archive/interspeech_2017/park17b_interspeech.html": {
    "title": "Using Voice Quality Features to Improve Short-Utterance, Text-Independent Speaker Verification Systems",
    "volume": "main",
    "abstract": "Due to within-speaker variability in phonetic content and/or speaking style, the performance of automatic speaker verification (ASV) systems degrades especially when the enrollment and test utterances are short. This study examines how different types of variability influence performance of ASV systems. Speech samples (< 2 sec) from the UCLA Speaker Variability Database containing 5 different read sentences by 200 speakers were used to study content variability. Other samples (about 5 sec) that contained speech directed towards pets, characterized by exaggerated prosody, were used to analyze style variability. Using the i-vector/PLDA framework, the ASV system error rate with MFCCs had a relative increase of at least 265% and 730% in content-mismatched and style-mismatched trials, respectively. A set of features that represents voice quality (F0, F1, F2, F3, H1-H2, H2-H4, H4-H2k, A1, A2, A3, and CPP) was also used. Using score fusion with MFCCs, all conditions saw decreases in error rates. In addition, using the NIST SRE10 database, score fusion provided relative improvements of 11.78% for 5-second utterances, 12.41% for 10-second utterances, and a small improvement for long utterances (about 5 min). These results suggest that voice quality features can improve short-utterance text-independent ASV system performance",
    "checked": true,
    "id": "6aab2715a57882924d1982116931622fe92ce69d",
    "semantic_title": "using voice quality features to improve short-utterance, text-independent speaker verification systems",
    "citation_count": 23
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lee17c_interspeech.html": {
    "title": "Gain Compensation for Fast i-Vector Extraction Over Short Duration",
    "volume": "main",
    "abstract": "I-vector is widely described as a compact and effective representation of speech utterances for speaker recognition. Standard i-vector extraction could be an expensive task for applications where computing resource is limited, for instance, on handheld devices. Fast approximate inference of i-vector aims to reduce the computational cost required in i-vector extraction where run-time requirement is critical. Most fast approaches hinge on certain assumptions to approximate the i-vector inference formulae with little loss of accuracy. In this paper, we analyze the uniform assumption that we had proposed earlier. We show that the assumption generally hold for long utterances but inadequate for utterances of short duration. We then propose to compensate for the negative effects by applying a simple gain factor on the i-vectors estimated from short utterances. The assertion is confirmed through analysis and experiments conducted on NIST SRE'08 and SRE'10 datasets",
    "checked": true,
    "id": "190290991d4017ad9a91456ffcec57db47d067bb",
    "semantic_title": "gain compensation for fast i-vector extraction over short duration",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2017/heo17_interspeech.html": {
    "title": "Joint Training of Expanded End-to-End DNN for Text-Dependent Speaker Verification",
    "volume": "main",
    "abstract": "We propose an expanded end-to-end DNN architecture for speaker verification based on b-vectors as well as d-vectors. We embedded the components of a speaker verification system such as modeling frame-level features, extracting utterance-level features, dimensionality reduction of utterance-level features, and trial-level scoring in an expanded end-to-end DNN architecture. The main contribution of this paper is that, instead of using DNNs as parts of the system trained independently, we train the whole system jointly with a fine-tune cost after pre-training each part. The experimental results show that the proposed system outperforms the baseline d-vector system and i-vector PLDA system",
    "checked": true,
    "id": "adbfc239889c7ed83123774c8b99b10ab011e109",
    "semantic_title": "joint training of expanded end-to-end dnn for text-dependent speaker verification",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17f_interspeech.html": {
    "title": "Speaker Verification via Estimating Total Variability Space Using Probabilistic Partial Least Squares",
    "volume": "main",
    "abstract": "The i-vector framework is one of the most popular methods in speaker verification, and estimating a total variability space (TVS) is a key part in the i-vector framework. Current estimation methods pay less attention on the discrimination of TVS, but the discrimination is so important that it will influence the improvement of performance. So we focus on the discrimination of TVS to achieve a better performance. In this paper, a discriminative estimating method of TVS based on probabilistic partial least squares (PPLS) is proposed. In this method, the discrimination is improved by using the priori information (labels) of speaker, so both the correlation of intra-class and the discrimination of interclass are fully utilized. Meanwhile, it also introduces a probabilistic view of the partial least squares (PLS) method to overcome the disadvantage of high computational complexity and the inability of channel compensation. And also this proposed method can achieve a better performance than the traditional TVS estimation method as well as the PLS-based method",
    "checked": true,
    "id": "a1765f17ecea9f2a6506edbf1ff53a75c6aad4d2",
    "semantic_title": "speaker verification via estimating total variability space using probabilistic partial least squares",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2017/li17g_interspeech.html": {
    "title": "Deep Speaker Feature Learning for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "Recently deep neural networks (DNNs) have been used to learn speaker features. However, the quality of the learned features is not sufficiently good, so a complex back-end model, either neural or probabilistic, has to be used to address the residual uncertainty when applied to speaker verification. This paper presents a convolutional time-delay deep neural network structure (CT-DNN) for speaker feature learning. Our experimental results on the Fisher database demonstrated that this CT-DNN can produce high-quality speaker features: even with a single feature (0.3 seconds including the context), the EER can be as low as 7.68%. This effectively confirmed that the speaker trait is largely a deterministic short-time property rather than a long-time distributional pattern, and therefore can be extracted from just dozens of frames",
    "checked": true,
    "id": "ea1da06d076ca05a7227291897adc8d936f2f47a",
    "semantic_title": "deep speaker feature learning for text-independent speaker verification",
    "citation_count": 81
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bousquet17_interspeech.html": {
    "title": "Duration Mismatch Compensation Using Four-Covariance Model and Deep Neural Network for Speaker Verification",
    "volume": "main",
    "abstract": "Duration mismatch between enrollment and test utterances still remains a major concern for reliability of real-life speaker recognition applications. Two approaches are proposed here to deal with this case when using the i-vector representation. The first one is an adaptation of Gaussian Probabilistic Linear Discriminant Analysis (PLDA) modeling, which can be extended to the case of any shift between i-vectors drawn from two distinct distributions. The second one attempts to map i-vectors of truncated segments of an utterance to the i-vector of the full segment, by the use of deep neural networks (DNN). Our results show that both new approaches outperform the standard PLDA by about 10% relative, noting that these back-end methods could complement those quantifying the i-vector uncertainty during its extraction process, in the case of duration gap",
    "checked": true,
    "id": "436236f43d063539013d28bd813c3f9ce513afc4",
    "semantic_title": "duration mismatch compensation using four-covariance model and deep neural network for speaker verification",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mccree17_interspeech.html": {
    "title": "Extended Variability Modeling and Unsupervised Adaptation for PLDA Speaker Recognition",
    "volume": "main",
    "abstract": "Probabilistic Linear Discriminant Analysis (PLDA) continues to be the most effective approach for speaker recognition in the i-vector space. This paper extends the PLDA model to include both enrollment and test cut duration as well as to distinguish between session and channel variability. In addition, we address the task of unsupervised adaptation to unknown new domains in two ways: speaker-dependent PLDA parameters and cohort score normalization using Bayes rule. Experimental results on the NIST SRE16 task show that these principled techniques provide state-of-the-art performance with negligible increase in complexity over a PLDA baseline",
    "checked": true,
    "id": "3aba10082d9d8e9c82690dae91ef5d5a872d72b1",
    "semantic_title": "extended variability modeling and unsupervised adaptation for plda speaker recognition",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2017/borgstrom17_interspeech.html": {
    "title": "Improving the Effectiveness of Speaker Verification Domain Adaptation with Inadequate In-Domain Data",
    "volume": "main",
    "abstract": "This paper addresses speaker verification domain adaptation with inadequate in-domain data. Specifically, we explore the cases where in-domain data sets do not include speaker labels, contain speakers with few samples, or contain speakers with low channel diversity. Existing domain adaptation methods are reviewed, and their shortcomings are discussed. We derive an unsupervised version of fully Bayesian adaptation which reduces the reliance on rich in-domain data. When applied to domain adaptation with inadequate in-domain data, the proposed approach yields competitive results when the samples per speaker are reduced, and outperforms existing supervised methods when the channel diversity is low, even without requiring speaker labels. These results are validated on the NIST SRE16, which uses a highly inadequate in-domain data set",
    "checked": true,
    "id": "fb9603732bfe851932214df19099411ca7095775",
    "semantic_title": "improving the effectiveness of speaker verification domain adaptation with inadequate in-domain data",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tan17_interspeech.html": {
    "title": "i-Vector DNN Scoring and Calibration for Noise Robust Speaker Verification",
    "volume": "main",
    "abstract": "This paper proposes applying multi-task learning to train deep neural networks (DNNs) for calibrating the PLDA scores of speaker verification systems under noisy environments. To facilitate the DNNs to learn the main task (calibration), several auxiliary tasks were introduced, including the prediction of SNR and duration from i-vectors and classifying whether an i-vector pair belongs to the same speaker or not. The possibility of replacing the PLDA model by a DNN during the scoring stage is also explored. Evaluations on noise contaminated speech suggest that the auxiliary tasks are important for the DNNs to learn the main calibration task and that the uncalibrated PLDA scores are an essential input to the DNNs. Without this input, the DNNs can only predict the score shifts accurately, suggesting that the PLDA model is indispensable",
    "checked": true,
    "id": "090390bdb38787ae125cb4c4a62c810006cba66b",
    "semantic_title": "i-vector dnn scoring and calibration for noise robust speaker verification",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2017/matejka17_interspeech.html": {
    "title": "Analysis of Score Normalization in Multilingual Speaker Recognition",
    "volume": "main",
    "abstract": "NIST Speaker Recognition Evaluation 2016 has revealed the importance of score normalization for mismatched data conditions. This paper analyzes several score normalization techniques for test conditions with multiple languages. The best performing one for a PLDA classifier is an adaptive s-norm with 30% relative improvement over the system without any score normalization. The analysis shows that the adaptive score normalization (using top scoring files per trial) selects cohorts that in 68% contain recordings from the same language and in 92% of the same gender as the enrollment and test recordings. Our results suggest that the data to select score normalization cohorts should be a pool of several languages and channels and if possible, its subset should contain data from the target domain",
    "checked": true,
    "id": "295b4823f8baa9b8c8b24ae7d28875344062731b",
    "semantic_title": "analysis of score normalization in multilingual speaker recognition",
    "citation_count": 91
  },
  "https://www.isca-speech.org/archive/interspeech_2017/silnova17_interspeech.html": {
    "title": "Alternative Approaches to Neural Network Based Speaker Verification",
    "volume": "main",
    "abstract": "Just like in other areas of automatic speech processing, feature extraction based on bottleneck neural networks was recently found very effective for the speaker verification task. However, better results are usually reported with more complex neural network architectures (e.g. stacked bottlenecks), which are difficult to reproduce. In this work, we experiment with the so called deep features, which are based on a simple feed-forward neural network architecture. We study various forms of applying deep features to i-vector/PDA based speaker verification. With proper settings, better verification performance can be obtained by means of this simple architecture as compared to the more elaborate bottleneck features. Also, we further experiment with multi-task training, where the neural network is trained for both speaker recognition and senone recognition objectives. Results indicate that, with a careful weighting of the two objectives, multi-task training can result in significantly better performing deep features",
    "checked": true,
    "id": "2f54aeebac612f5bcc56130862975df4676632fa",
    "semantic_title": "alternative approaches to neural network based speaker verification",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2017/travadi17_interspeech.html": {
    "title": "A Distribution Free Formulation of the Total Variability Model",
    "volume": "main",
    "abstract": "The Total Variability Model (TVM) [1] has been widely used in audio signal processing as a framework for capturing differences in feature space distributions across variable length sequences by mapping them into a fixed-dimensional representation. Its formulation requires making an assumption about the source data distribution being a Gaussian Mixture Model (GMM). In this paper, we show that it is possible to arrive at the same model formulation without requiring such an assumption about distribution of the data, by showing asymptotic normality of the statistics used to estimate the model. We highlight some connections between TVM and heteroscedastic Principal Component Analysis (PCA), as well as the matrix completion problem, which lead to a computationally efficient formulation of the Maximum Likelihood estimation problem for the model",
    "checked": true,
    "id": "88bfbef4d2b7631b9dd1cae7935560e8f90f5107",
    "semantic_title": "a distribution free formulation of the total variability model",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2017/rahman17_interspeech.html": {
    "title": "Domain Mismatch Modeling of Out-Domain i-Vectors for PLDA Speaker Verification",
    "volume": "main",
    "abstract": "The state-of-the-art i-vector based probabilistic linear discriminant analysis (PLDA) trained on non-target (or out-domain) data significantly affects the speaker verification performance due to the domain mismatch between training and evaluation data. To improve the speaker verification performance, sufficient amount of domain mismatch compensated out-domain data must be used to train the PLDA models successfully. In this paper, we propose a domain mismatch modeling (DMM) technique using maximum-a-posteriori (MAP) estimation to model and compensate the domain variability from the out-domain training i-vectors. From our experimental results, we found that the DMM technique can achieve at least a 24% improvement in EER over an out-domain only baseline when speaker labels are available. Further improvement of 3% is obtained when combining DMM with domain-invariant covariance normalization (DICN) approach. The DMM/DICN combined technique is shown to perform better than in-domain PLDA system with only 200 labeled speakers or 2,000 unlabeled i-vectors",
    "checked": true,
    "id": "74f6118b95467b239ee8b215fe6c41de384e7ff1",
    "semantic_title": "domain mismatch modeling of out-domain i-vectors for plda speaker verification",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2017/cheng17_interspeech.html": {
    "title": "An Exploration of Dropout with LSTMs",
    "volume": "main",
    "abstract": "Long Short-Term Memory networks (LSTMs) are a component of many state-of-the-art DNN-based speech recognition systems. Dropout is a popular method to improve generalization in DNN training. In this paper we describe extensive experiments in which we investigated the best way to combine dropout with LSTMs — specifically, projected LSTMs (LSTMP). We investigated various locations in the LSTM to place the dropout (and various combinations of locations), and a variety of dropout schedules. Our optimized recipe gives consistent improvements in WER across a range of datasets, including Switchboard, TED-LIUM and AMI",
    "checked": true,
    "id": "b0316d17fef2a42fba426426e5ea090a83205aaa",
    "semantic_title": "an exploration of dropout with lstms",
    "citation_count": 99
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kim17e_interspeech.html": {
    "title": "Residual LSTM: Design of a Deep Recurrent Architecture for Distant Speech Recognition",
    "volume": "main",
    "abstract": "In this paper, a novel architecture for a deep recurrent neural network, residual LSTM is introduced. A plain LSTM has an internal memory cell that can learn long term dependencies of sequential data. It also provides a temporal shortcut path to avoid vanishing or exploding gradients in the temporal domain. The residual LSTM provides an additional spatial shortcut path from lower layers for efficient training of deep networks with multiple LSTM layers. Compared with the previous work, highway LSTM, residual LSTM separates a spatial shortcut path with temporal one by using output layers, which can help to avoid a conflict between spatial and temporal-domain gradient flows. Furthermore, residual LSTM reuses the output projection matrix and the output gate of LSTM to control the spatial information flow instead of additional gate networks, which effectively reduces more than 10% of network parameters. An experiment for distant speech recognition on the AMI SDM corpus shows that 10-layer plain and highway LSTM networks presented 13.7% and 6.2% increase in WER over 3-layer baselines, respectively. On the contrary, 10-layer residual LSTM networks provided the lowest WER 41.0%, which corresponds to 3.3% and 2.8% WER reduction over plain and highway LSTM networks, respectively",
    "checked": true,
    "id": "097fb68f360195d438126a7cd9b988fac31b1228",
    "semantic_title": "residual lstm: design of a deep recurrent architecture for distant speech recognition",
    "citation_count": 160
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tran17_interspeech.html": {
    "title": "Unfolded Deep Recurrent Convolutional Neural Network with Jump Ahead Connections for Acoustic Modeling",
    "volume": "main",
    "abstract": "Recurrent neural networks (RNNs) with jump ahead connections have been used in the computer vision tasks. Still, they have not been investigated well for automatic speech recognition (ASR) tasks. In other words, unfolded RNN has been shown to be an effective model for acoustic modeling tasks. This paper investigates how to elaborate a sophisticated unfolded deep RNN architecture in which recurrent connections use a convolutional neural network (CNN) to model a short-term dependence between hidden states. In this study, our unfolded RNN architecture is a CNN that process a sequence of input features sequentially. Each time step, the CNN inputs a small block of the input features and the output of the hidden layer from the preceding block in order to compute the output of its hidden layer. In addition, by exploiting either one or multiple jump ahead connections between time steps, our network can learn long-term dependencies more effectively. We carried experiments on the CHiME 3 task showing the effectiveness of our proposed approach",
    "checked": true,
    "id": "3640a44aebed60570e7ab76bce5d7b681a1db218",
    "semantic_title": "unfolded deep recurrent convolutional neural network with jump ahead connections for acoustic modeling",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2017/karita17_interspeech.html": {
    "title": "Forward-Backward Convolutional LSTM for Acoustic Modeling",
    "volume": "main",
    "abstract": "An automatic speech recognition (ASR) performance has greatly improved with the introduction of convolutional neural network (CNN) or long-short term memory (LSTM) for acoustic modeling. Recently, a convolutional LSTM (CLSTM) has been proposed to directly use convolution operation within the LSTM blocks and combine the advantages of both CNN and LSTM structures into a single architecture. This paper presents the first attempt to use CLSTMs for acoustic modeling. In addition, we propose a new forward-backward architecture to exploit long-term left/right context efficiently. The proposed scheme combines forward and backward LSTMs at different time points of an utterance with the aim of modeling long term frame invariant information such as speaker characteristics, channel etc. Furthermore, the proposed forward-backward architecture can be trained with truncated back-propagation-through-time unlike conventional bidirectional LSTM (BLSTM) architectures. Therefore, we are able to train deeply stacked CLSTM acoustic models, which is practically challenging with conventional BLSTMs. Experimental results show that both CLSTM and forward-backward LSTM improve word error rates significantly compared to standard CNN and LSTM architectures",
    "checked": true,
    "id": "7cf26ee9541433356a3bcf46cc62df9505c9b297",
    "semantic_title": "forward-backward convolutional lstm for acoustic modeling",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ark17_interspeech.html": {
    "title": "Convolutional Recurrent Neural Networks for Small-Footprint Keyword Spotting",
    "volume": "main",
    "abstract": "Keyword spotting (KWS) constitutes a major component of human-technology interfaces. Maximizing the detection accuracy at a low false alarm (FA) rate, while minimizing the footprint size, latency and complexity are the goals for KWS. Towards achieving them, we study Convolutional Recurrent Neural Networks (CRNNs). Inspired by large-scale state-of-the-art speech recognition systems, we combine the strengths of convolutional layers and recurrent layers to exploit local structure and long-range context. We analyze the effect of architecture parameters, and propose training strategies to improve performance. With only ~230k parameters, our CRNN model yields acceptably low latency, and achieves 97.71% accuracy at 0.5 FA/hour for 5 dB signal-to-noise ratio",
    "checked": true,
    "id": "db46b939874e01fb861e7d0a8670cb83092f0ca5",
    "semantic_title": "convolutional recurrent neural networks for small-footprint keyword spotting",
    "citation_count": 158
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wu17c_interspeech.html": {
    "title": "Deep Activation Mixture Model for Speech Recognition",
    "volume": "main",
    "abstract": "Deep learning approaches achieve state-of-the-art performance in a range of applications, including speech recognition. However, the parameters of the deep neural network (DNN) are hard to interpret, which makes regularisation and adaptation to speaker or acoustic conditions challenging. This paper proposes the deep activation mixture model (DAMM) to address these problems. The output of one hidden layer is modelled as the sum of a mixture and residual models. The mixture model forms an activation function contour while the residual one models fluctuations around the contour. The use of the mixture model gives two advantages: First, it introduces a novel regularisation on the DNN. Second, it allows novel adaptation schemes. The proposed approach is evaluated on a large-vocabulary U.S. English broadcast news task. It yields a slightly better performance than the DNN baselines, and on the utterance-level unsupervised adaptation, the adapted DAMM acquires further performance gains",
    "checked": true,
    "id": "805adcbf223e4453c7c2ba9adff809581883427e",
    "semantic_title": "deep activation mixture model for speech recognition",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2017/heck17_interspeech.html": {
    "title": "Ensembles of Multi-Scale VGG Acoustic Models",
    "volume": "main",
    "abstract": "We present our work on constructing multi-scale deep convolutional neural networks for automatic speech recognition. Several VGG nets have been trained that differ solely in the kernel size of the convolutional layers. The general idea is that receptive fields of varying sizes match structures of different scales, thus supporting more robust recognition when combined appropriately. We construct a large multi-scale system by means of system combination. We use ROVER and the fusion of posterior predictions as examples of late combination, and knowledge distillation using soft labels from a model ensemble as a way of early combination. In this work, distillation is approached from the perspective of knowledge transfer pre-training, which is followed by a fine-tuning on the original hard labels. Our results show that it is possible to bundle the individual recognition strengths of the VGGs in a much simpler CNN architecture that yields equal performance with the best late combination",
    "checked": true,
    "id": "fc7117259ab273da111533f3fdf824b4ed19ddbd",
    "semantic_title": "ensembles of multi-scale vgg acoustic models",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2017/grosz17_interspeech.html": {
    "title": "Training Context-Dependent DNN Acoustic Models Using Probabilistic Sampling",
    "volume": "main",
    "abstract": "In current HMM/DNN speech recognition systems, the purpose of the DNN component is to estimate the posterior probabilities of tied triphone states. In most cases the distribution of these states is uneven, meaning that we have a markedly different number of training samples for the various states. This imbalance of the training data is a source of suboptimality for most machine learning algorithms, and DNNs are no exception. A straightforward solution is to re-sample the data, either by upsampling the rarer classes or by downsampling the more common classes. Here, we experiment with the so-called probabilistic sampling method that applies downsampling and upsampling at the same time. For this, it defines a new class distribution for the training data, which is a linear combination of the original and the uniform class distributions. As an extension to previous studies, we propose a new method to re-estimate the class priors, which is required to remedy the mismatch between the training and the test data distributions introduced by re-sampling. Using probabilistic sampling and the proposed modification we report 5% and 6% relative error rate reductions on the TED-LIUM and on the AMI corpora, respectively",
    "checked": true,
    "id": "7768c3bb97bbe8d37720b5089f8da5c3eb7f851c",
    "semantic_title": "training context-dependent dnn acoustic models using probabilistic sampling",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2017/grosz17b_interspeech.html": {
    "title": "A Comparative Evaluation of GMM-Free State Tying Methods for ASR",
    "volume": "main",
    "abstract": "Deep neural network (DNN) based speech recognizers have recently replaced Gaussian mixture (GMM) based systems as the state-of-the-art. While some of the modeling techniques developed for the GMM based framework may directly be applied to HMM/DNN systems, others may be inappropriate. One such example is the creation of context-dependent tied states, for which an efficient decision tree state tying method exists. The tied states used to train DNNs are usually obtained using the same tying algorithm, even though it is based on likelihoods of Gaussians, hence it is more appropriate for HMM/GMMs. Recently, however, several refinements have been published which seek to adapt the state tying algorithm to the HMM/DNN hybrid architecture. Unfortunately, these studies reported results on different (and sometimes very small) datasets, which does not allow their direct comparison. Here, we tested four of these methods on the same LVCSR task, and compared their performance under the same circumstances. We found that, besides changing the input of the context-dependent state tying algorithm, it is worth adjusting the tying criterion as well. The methods which utilized a decision criterion designed directly for neural networks consistently, and significantly, outperformed those which employed the standard Gaussian-based algorithm",
    "checked": true,
    "id": "76fa912b181a4ac6917a1fa0cca78a13dfcb081e",
    "semantic_title": "a comparative evaluation of gmm-free state tying methods for asr",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wang17h_interspeech.html": {
    "title": "Backstitch: Counteracting Finite-Sample Bias via Negative Steps",
    "volume": "main",
    "abstract": "In this paper we describe a modification to Stochastic Gradient Descent (SGD) that improves generalization to unseen data. It consists of doing two steps for each minibatch: a backward step with a small negative learning rate, followed by a forward step with a larger learning rate. The idea was initially inspired by ideas from adversarial training, but we show that it can be viewed as a crude way of canceling out certain systematic biases that come from training on finite data sets. The method gives ~ 10% relative improvement over our best acoustic models based on lattice-free MMI, across multiple datasets with 100–300 hours of data",
    "checked": true,
    "id": "aeec5e369572d5f2cd88e4f5166de439558af933",
    "semantic_title": "backstitch: counteracting finite-sample bias via negative steps",
    "citation_count": 27
  },
  "https://www.isca-speech.org/archive/interspeech_2017/takeda17_interspeech.html": {
    "title": "Node Pruning Based on Entropy of Weights and Node Activity for Small-Footprint Acoustic Model Based on Deep Neural Networks",
    "volume": "main",
    "abstract": "This paper describes a node-pruning method for an acoustic model based on deep neural networks (DNNs). Node pruning is a promising method to reduce the memory usage and computational cost of DNNs. A score function is defined to measure the importance of each node, and less important nodes are pruned. The entropy of the activity of each node has been used as a score function to find nodes with outputs that do not change at all. We introduce entropy of weights of each node to consider the number of weights and their patterns of each node. Because the number of weights and the patterns differ at each layer, the importance of the node should also be measured using the related weights of the target node. We then propose a score function that integrates the entropy of weights and node activity, which will prune less important nodes more efficiently. Experimental results showed that the proposed pruning method successfully reduced the number of parameters by about 6% without any accuracy loss compared with a score function based only on the entropy of node activity",
    "checked": true,
    "id": "fdd97f0260eabb3f6b28cdb0a8b35e71cd710a8f",
    "semantic_title": "node pruning based on entropy of weights and node activity for small-footprint acoustic model based on deep neural networks",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2017/variani17_interspeech.html": {
    "title": "End-to-End Training of Acoustic Models for Large Vocabulary Continuous Speech Recognition with TensorFlow",
    "volume": "main",
    "abstract": "This article discusses strategies for end-to-end training of state-of-the-art acoustic models for Large Vocabulary Continuous Speech Recognition (LVCSR), with the goal of leveraging TensorFlow components so as to make efficient use of large-scale training sets, large model sizes, and high-speed computation units such as Graphical Processing Units (GPUs). Benchmarks are presented that evaluate the efficiency of different approaches to batching of training data, unrolling of recurrent acoustic models, and device placement of TensorFlow variables and operations. An overall training architecture developed in light of those findings is then described. The approach makes it possible to take advantage of both data parallelism and high speed computation on GPU for state-of-the-art sequence training of acoustic models. The effectiveness of the design is evaluated for different training schemes and model sizes, on a 15,000 hour Voice Search task",
    "checked": true,
    "id": "fab90790ab3a3880e6fec274b3760f4417e76f0b",
    "semantic_title": "end-to-end training of acoustic models for large vocabulary continuous speech recognition with tensorflow",
    "citation_count": 26
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sim17_interspeech.html": {
    "title": "An Efficient Phone N-Gram Forward-Backward Computation Using Dense Matrix Multiplication",
    "volume": "main",
    "abstract": "The forward-backward algorithm is commonly used to train neural network acoustic models when optimizing a sequence objective like MMI and sMBR. Recent work on lattice-free MMI training of neural network acoustic models shows that the forward-backward algorithm can be computed efficiently in the probability domain as a series of sparse matrix multiplications using GPUs. In this paper, we present a more efficient way of computing forward-backward using a dense matrix multiplication approach. We do this by exploiting the block-diagonal structure of the n-gram state transition matrix; instead of multiplying large sparse matrices, the proposed method involves a series of smaller dense matrix multiplications, which can be computed in parallel. Efficient implementation can be easily achieved by leveraging on the optimized matrix multiplication routines provided by standard libraries, such as NumPy and TensorFlow. Runtime benchmarks show that the dense multiplication method is consistently faster than the sparse multiplication method (on both CPUs and GPUs), when applied to a 4-gram phone language model. This is still the case even when the sparse multiplication method uses a more compact finite state model representation by excluding unseen n-grams",
    "checked": true,
    "id": "ed527c5fe826d17aa578adc07c63c7b9d6f134e6",
    "semantic_title": "an efficient phone n-gram forward-backward computation using dense matrix multiplication",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tuske17_interspeech.html": {
    "title": "Parallel Neural Network Features for Improved Tandem Acoustic Modeling",
    "volume": "main",
    "abstract": "The combination of acoustic models or features is a standard approach to exploit various knowledge sources. This paper investigates the concatenation of different bottleneck (BN) neural network (NN) outputs for tandem acoustic modeling. Thus, combination of NN features is performed via Gaussian mixture models (GMM). Complementarity between the NN feature representations is attained by using various network topologies: LSTM recurrent, feed-forward, and hierarchical, as well as different non-linearities: hyperbolic tangent, sigmoid, and rectified linear units. Speech recognition experiments are carried out on various tasks: telephone conversations, Skype calls, as well as broadcast news and conversations. Results indicate that LSTM based tandem approach is still competitive, and such tandem model can challenge comparable hybrid systems. The traditional steps of tandem modeling, speaker adaptive and sequence discriminative GMM training, improve the tandem results further. Furthermore, these \"old-fashioned\" steps remain applicable after the concatenation of multiple neural network feature streams. Exploiting the parallel processing of input feature streams, it is shown that 2–5% relative improvement could be achieved over the single best BN feature set. Finally, we also report results after neural network based language model rescoring and examine the system combination possibilities using such complex tandem models",
    "checked": true,
    "id": "44ca9b3d070ea55056093f6fb928927a23405e29",
    "semantic_title": "parallel neural network features for improved tandem acoustic modeling",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tang17_interspeech.html": {
    "title": "Acoustic Feature Learning via Deep Variational Canonical Correlation Analysis",
    "volume": "main",
    "abstract": "We study the problem of acoustic feature learning in the setting where we have access to another (non-acoustic) modality for feature learning but not at test time. We use deep variational canonical correlation analysis (VCCA), a recently proposed deep generative method for multi-view representation learning. We also extend VCCA with improved latent variable priors and with adversarial learning. Compared to other techniques for multi-view feature learning, VCCA's advantages include an intuitive latent variable interpretation and a variational lower bound objective that can be trained end-to-end efficiently. We compare VCCA and its extensions with previous feature learning methods on the University of Wisconsin X-ray Microbeam Database, and show that VCCA-based feature learning improves over previous methods for speaker-independent phonetic recognition",
    "checked": true,
    "id": "47025a0f8f8d375479d98dc6340959360c52d74a",
    "semantic_title": "acoustic feature learning via deep variational canonical correlation analysis",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2017/masumura17_interspeech.html": {
    "title": "Online End-of-Turn Detection from Speech Based on Stacked Time-Asynchronous Sequential Networks",
    "volume": "main",
    "abstract": "This paper presents a novel modeling called stacked time-asynchronous sequential networks (STASNs) for online end-of-turn detection. An online end-of-turn detection that determines turn-taking points in a real-time manner is an essential component for human-computer interaction systems. In this study, we use long-range sequential information of multiple time-asynchronous sequential features, such as prosodic, phonetic, and lexical sequential features, to enhance online end-of-turn detection performance. Our key idea is to embed individual sequential features in a fixed-length continuous representation by using sequential networks. This enables us to simultaneously handle multiple time-asynchronous sequential features for end-of-turn detection. STASNs can embed all of the sequential information between a start-of-conversation and the current end-of-utterance in a fixed-length continuous representation that can be directly used for classification by stacking multiple sequential networks. Experiments show that STASNs outperforms conventional modeling with limited sequential information. Furthermore, STASNs with senone bottleneck features extracted using senone-based deep neural networks have superior performance without requiring lexical features decoded by an automatic speech recognition process",
    "checked": true,
    "id": "a6d0c679deb1e951fba165a3ca45ff730ba94662",
    "semantic_title": "online end-of-turn detection from speech based on stacked time-asynchronous sequential networks",
    "citation_count": 27
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wodarczak17_interspeech.html": {
    "title": "Improving Prediction of Speech Activity Using Multi-Participant Respiratory State",
    "volume": "main",
    "abstract": "One consequence of situated face-to-face conversation is the co-observability of participants' respiratory movements and sounds. We explore whether this information can be exploited in predicting incipient speech activity. Using a methodology called stochastic turn-taking modeling, we compare the performance of a model trained on speech activity alone to one additionally trained on static and dynamic lung volume features. The methodology permits automatic discovery of temporal dependencies across participants and feature types. Our experiments show that respiratory information substantially lowers cross-entropy rates, and that this generalizes to unseen data",
    "checked": true,
    "id": "b8ed3daffef928d9bf5161a2dc5362953711f352",
    "semantic_title": "improving prediction of speech activity using multi-participant respiratory state",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2017/heeman17_interspeech.html": {
    "title": "Turn-Taking Offsets and Dialogue Context",
    "volume": "main",
    "abstract": "A number of researchers have studied turn-taking offsets in human-human dialogues. However, that work collapses over a wide number of different turn-taking contexts. In this work, we delve into the turn-taking delays based on different contexts. We show that turn-taking behavior, both who tends to take the turn next, and the turn-taking delays, are dependent on the previous speech act type, the upcoming speech act, and the nature of the dialogue. This strongly suggests that in studying turn-taking, all turn-taking events should not be grouped together. This also suggests that delays are due to cognitive processing of what to say, rather than whether a speaker should take the turn",
    "checked": true,
    "id": "74e3ab6dce67c71fe90006f2bba28327a5702181",
    "semantic_title": "turn-taking offsets and dialogue context",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2017/maier17_interspeech.html": {
    "title": "Towards Deep End-of-Turn Prediction for Situated Spoken Dialogue Systems",
    "volume": "main",
    "abstract": "We address the challenge of improving live end-of-turn detection for situated spoken dialogue systems. While traditionally silence thresholds have been used to detect the user's end-of-turn, such an approach limits the system's potential fluidity in interaction, restricting it to a purely reactive paradigm. By contrast, here we present a system which takes a predictive approach. The user's end-of-turn is predicted live as acoustic features and words are consumed by the system. We compare the benefits of live lexical and acoustic information by feature analysis and testing equivalent models with different feature sets with a common deep learning architecture, a Long Short-Term Memory (LSTM) network. We show the usefulness of incremental enriched language model features in particular. Training and testing onWizard-of-Oz data collected to train an agent in a simple virtual world, we are successful in improving over a reactive baseline in terms of reducing latency whilst minimising the cut-in rate",
    "checked": true,
    "id": "1a527fa6c5e97e13112d7503cde891de8260719c",
    "semantic_title": "towards deep end-of-turn prediction for situated spoken dialogue systems",
    "citation_count": 31
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ishimoto17_interspeech.html": {
    "title": "End-of-Utterance Prediction by Prosodic Features and Phrase-Dependency Structure in Spontaneous Japanese Speech",
    "volume": "main",
    "abstract": "This study is aimed at uncovering a way that participants in conversation predict end-of-utterance for spontaneous Japanese speech. In spontaneous everyday conversation, the participants must predict the ends of utterances of a speaker to perform smooth turn-taking without too much gap. We consider that they utilize not only syntactic factors but also prosodic factors for the end-of-utterance prediction because of the difficulty of prediction of a syntactic completion point in spontaneous Japanese. In previous studies, we found that prosodic features changed significantly in the final accentual phrase. However, it is not clear what prosodic features support the prediction. In this paper, we focused on dependency structure among bunsetsu-phrases as the syntactic factor, and investigated the relation between the phrase-dependency and prosodic features. The results showed that the average fundamental frequency and the average intensity for accentual phrases did not decline until the modified phrase appeared. Next, to predict the end of utterance from the syntactic and prosodic features, we constructed a generalized linear mixed model. The model provided higher accuracy than using the prosodic features only. These suggest the possibility that prosodic changes and phrase-dependency relations inform the hearer that the utterance is approaching its end",
    "checked": true,
    "id": "7a10cd0fdd32c093da38ada5e16adf149a66b5f5",
    "semantic_title": "end-of-utterance prediction by prosodic features and phrase-dependency structure in spontaneous japanese speech",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2017/liu17_interspeech.html": {
    "title": "Turn-Taking Estimation Model Based on Joint Embedding of Lexical and Prosodic Contents",
    "volume": "main",
    "abstract": "A natural conversation involves rapid exchanges of turns while talking. Taking turns at appropriate timing or intervals is a requisite feature for a dialog system as a conversation partner. This paper proposes a model that estimates the timing of turn-taking during verbal interactions. Unlike previous studies, our proposed model does not rely on a silence region between sentences since a dialog system must respond without large gaps or overlaps. We propose a Recurrent Neural Network (RNN) based model that takes the joint embedding of lexical and prosodic contents as its input to classify utterances into turn-taking related classes and estimates the turn-taking timing. To this end, we trained a neural network to embed the lexical contents, the fundamental frequencies, and the speech power into a joint embedding space. To learn meaningful embedding spaces, the prosodic features from each single utterance are pre-trained using RNN and combined with utterance lexical embedding as the input of our proposed model. We tested this model on a spontaneous conversation dataset and confirmed that it outperformed the use of word embedding-based features",
    "checked": true,
    "id": "f87f58d4f0707f6d0d41827fc8888dd236cfcec5",
    "semantic_title": "turn-taking estimation model based on joint embedding of lexical and prosodic contents",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2017/inaguma17_interspeech.html": {
    "title": "Social Signal Detection in Spontaneous Dialogue Using Bidirectional LSTM-CTC",
    "volume": "main",
    "abstract": "Non-verbal speech cues such as laughter and fillers, which are collectively called social signals, play an important role in human communication. Therefore, detection of them would be useful for dialogue systems to infer speaker's intentions, emotions and engagements. The conventional approaches are based on frame-wise classifiers, which require precise time-alignment of these events for training. This work investigates the Connectionist Temporal Classification (CTC) approach which can learn an alignment between the input and its target label sequence. This allows for robust detection of the events and efficient training without precise time information. Experimental evaluations with various settings demonstrate that CTC based on bidirectional LSTM outperforms the conventional DNN and HMM based methods",
    "checked": true,
    "id": "1fb43ff5f1292d54ef1aa307f76b59957c2d4ff3",
    "semantic_title": "social signal detection in spontaneous dialogue using bidirectional lstm-ctc",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2017/rahimi17_interspeech.html": {
    "title": "Entrainment in Multi-Party Spoken Dialogues at Multiple Linguistic Levels",
    "volume": "main",
    "abstract": "Linguistic entrainment, the phenomena whereby dialogue partners speak more similarly to each other in a variety of dimensions, is key to the success and naturalness of interactions. While there is considerable evidence for both lexical and acoustic-prosodic entrainment, little work has been conducted to investigate the relationship between these two different modalities using the same measures in the same dialogues, specifically in multi-party dialogue. In this paper, we measure lexical and acoustic-prosodic entrainment for multi-party teams to explore whether entrainment occurs at multiple levels during conversation and to understand the relationship between these two modalities",
    "checked": true,
    "id": "3c75da1569689f4083f33aa3b5e30f4f960af9b0",
    "semantic_title": "entrainment in multi-party spoken dialogues at multiple linguistic levels",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2017/reverdy17_interspeech.html": {
    "title": "Measuring Synchrony in Task-Based Dialogues",
    "volume": "main",
    "abstract": "In many contexts from casual everyday conversations to formal discussions, people tend to repeat their interlocutors, and themselves. This phenomenon not only yields random repetitions one might expect from a natural Zipfian distribution of linguistic forms, but also projects underlying discourse mechanisms and rhythms that researchers have suggested establishes conversational involvement and may support communicative progress towards mutual understanding. In this paper, advances in an automated method for assessing interlocutor synchrony in task-based Human-to-Human interactions are reported. The method focuses on dialogue structure, rather than temporal distance, measuring repetition between speakers and their interlocutors last n-turns (n = 1, however far back in the conversation that might have been) rather than utterances during a prior window fixed by duration. The significance of distinct linguistic levels of repetition are assessed by observing contrasts between actual and randomized dialogues, in order to provide a quantifying measure of communicative success. Definite patterns of repetitions where identified, notably in contrasting the role of participants (as information giver or follower). The extent to which those interacted sometime surprisingly with gender, eye-contact and familiarity is the principal contribution of this work",
    "checked": true,
    "id": "fc8d74d6579c7c7e23dedbddee463c6bb8d5de5d",
    "semantic_title": "measuring synchrony in task-based dialogues",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2017/crook17_interspeech.html": {
    "title": "Sequence to Sequence Modeling for User Simulation in Dialog Systems",
    "volume": "main",
    "abstract": "User simulators are a principal offline method for training and evaluating human-computer dialog systems. In this paper, we examine simple sequence-to-sequence neural network architectures for training end-to-end, natural language to natural language, user simulators, using only raw logs of previous interactions without any additional human labelling. We compare the neural network-based simulators with a language model (LM)-based approach for creating natural language user simulators. Using both an automatic evaluation using LM perplexity and a human evaluation, we demonstrate that the sequence-to-sequence approaches outperform the LM-based method. We show correlation between LM perplexity and the human evaluation on this task, and discuss the benefits of different neural network architecture variations",
    "checked": true,
    "id": "7b4efd142ce80de9a485f2d5aede00354e64862d",
    "semantic_title": "sequence to sequence modeling for user simulation in dialog systems",
    "citation_count": 24
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ramanarayanan17b_interspeech.html": {
    "title": "Human and Automated Scoring of Fluency, Pronunciation and Intonation During Human–Machine Spoken Dialog Interactions",
    "volume": "main",
    "abstract": "We present a spoken dialog-based framework for the computer-assisted language learning (CALL) of conversational English. In particular, we leveraged the open-source HALEF dialog framework to develop a job interview conversational application. We then used crowdsourcing to collect multiple interactions with the system from non-native English speakers. We analyzed human-rated scores of the recorded dialog data on three different scoring dimensions critical to the delivery of conversational English — fluency, pronunciation and intonation/stress — and further examined the efficacy of automatically-extracted, hand-curated speech features in predicting each of these sub-scores. Machine learning experiments showed that trained scoring models generally perform at par with the human inter-rater agreement baseline in predicting human-rated scores of conversational proficiency",
    "checked": false,
    "id": "78912b8661473f4f1f24ed8db11da8dcf7569fbd",
    "semantic_title": "human and automated scoring of fluency, pronunciation and intonation during human-machine spoken dialog interactions",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ando17_interspeech.html": {
    "title": "Hierarchical LSTMs with Joint Learning for Estimating Customer Satisfaction from Contact Center Calls",
    "volume": "main",
    "abstract": "This paper presents a joint modeling of both turn-level and call-level customer satisfaction in contact center dialogue. Our key idea is to directly apply turn-level estimation results to call-level estimation and optimize them jointly; previous work treated both estimations as being independent. Proposed joint modeling is achieved by stacking two types of long short-term memory recurrent neural networks (LSTM-RNNs). The lower layer employs LSTM-RNN for sequential labeling of turn-level customer satisfaction in which each label is estimated from context information extracted from not only the target turn but also the surrounding turns. The upper layer uses another LSTM-RNN to estimate call-level customer satisfaction labels from all information of estimated turn-level customer satisfaction. These two networks can be efficiently optimized by joint learning of both types of labels. Experiments show that the proposed method outperforms a conventional support vector machine based method in terms of both turn-level and call-level customer satisfaction with relative error reductions of over 20%",
    "checked": true,
    "id": "fafeddb84afc6e923f866a54bfc611408ddff9e0",
    "semantic_title": "hierarchical lstms with joint learning for estimating customer satisfaction from contact center calls",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ultes17_interspeech.html": {
    "title": "Domain-Independent User Satisfaction Reward Estimation for Dialogue Policy Learning",
    "volume": "main",
    "abstract": "Learning suitable and well-performing dialogue behaviour in statistical spoken dialogue systems has been in the focus of research for many years. While most work which is based on reinforcement learning employs an objective measure like task success for modelling the reward signal, we propose to use a reward based on user satisfaction. We will show in simulated experiments that a live user satisfaction estimation model may be applied resulting in higher estimated satisfaction whilst achieving similar success rates. Moreover, we will show that one satisfaction estimation model which has been trained on one domain may be applied in many other domains which cover a similar task. We will verify our findings by employing the model to one of the domains for learning a policy from real users and compare its performance to policies using the user satisfaction and task success acquired directly from the users as reward",
    "checked": true,
    "id": "95c402e37f3d44c378b7af831712a6a5ccfbccf6",
    "semantic_title": "domain-independent user satisfaction reward estimation for dialogue policy learning",
    "citation_count": 31
  },
  "https://www.isca-speech.org/archive/interspeech_2017/nakamura17_interspeech.html": {
    "title": "Analysis of the Relationship Between Prosodic Features of Fillers and its Forms or Occurrence Positions",
    "volume": "main",
    "abstract": "Fillers are involved in the ease of understanding by listeners and turn-taking. However, the knowledge about its prosodic features is insufficient, and its modeling has not been done either. For these reasons, there is insufficient knowledge to generate natural and appropriate fillers in a dialog system at present. Therefore, for the purpose of clarifying the prosodic features of fillers, its relationship with occurrence positions or forms were analyzed in this research. ‘Ano' and ‘Eto' were used as forms, non-/boundary of Dialog Act and non-/turn-taking for occurrence positions. Duration, F0, and intensity were utilized as prosodic features. As a result, the followings were found out: the prosodic features are different depending on the difference of the occurrence positions even for fillers of the same form, and similar prosodic features are found between the same occurrence positions even in different forms",
    "checked": true,
    "id": "1900205d42d46f37b8afef2e9790500e73a5f242",
    "semantic_title": "analysis of the relationship between prosodic features of fillers and its forms or occurrence positions",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2017/fatima17_interspeech.html": {
    "title": "Cross-Subject Continuous Emotion Recognition Using Speech and Body Motion in Dyadic Interactions",
    "volume": "main",
    "abstract": "Dyadic interactions encapsulate rich emotional exchange between interlocutors suggesting a multimodal, cross-speaker and cross-dimensional continuous emotion dependency. This study explores the dynamic inter-attribute emotional dependency at the cross-subject level with implications to continuous emotion recognition based on speech and body motion cues. We propose a novel two-stage Gaussian Mixture Model mapping framework for the continuous emotion recognition problem. In the first stage, we perform continuous emotion recognition (CER) of both speakers from speech and body motion modalities to estimate activation, valence and dominance (AVD) attributes. In the second stage, we improve the first stage estimates by performing CER of the selected speaker using her/his speech and body motion modalities as well as using the estimated affective attribute(s) of the other speaker. Our experimental evaluations indicate that the second stage, cross-subject continuous emotion recognition (CSCER), provides complementary information to recognize the affective state, and delivers promising improvements for the continuous emotion recognition problem",
    "checked": true,
    "id": "bc8750893faa247bbdf5a0ce752cee8cf7a45b8e",
    "semantic_title": "cross-subject continuous emotion recognition using speech and body motion in dyadic interactions",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2017/elsner17_interspeech.html": {
    "title": "An Automatically Aligned Corpus of Child-Directed Speech",
    "volume": "main",
    "abstract": "Forced alignment would enable phonetic analyses of child directed speech (CDS) corpora which have existing transcriptions. But existing alignment systems are inaccurate due to the atypical phonetics of CDS. We adapt a Kaldi forced alignment system to CDS by extending the dictionary and providing it with heuristically-derived hints for vowel locations. Using this system, we present a new time-aligned CDS corpus with a million aligned segments. We manually correct a subset of the corpus and demonstrate that our system is 70% accurate. Both our automatic and manually corrected alignments are publically available at osf.io/ke44q",
    "checked": true,
    "id": "926547b837d2c424dd9fb39b9ec760452b1e5349",
    "semantic_title": "an automatically aligned corpus of child-directed speech",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bohn17_interspeech.html": {
    "title": "A Comparison of Danish Listeners' Processing Cost in Judging the Truth Value of Norwegian, Swedish, and English Sentences",
    "volume": "main",
    "abstract": "The present study used a sentence verification task to assess the processing cost involved in native Danish listeners' attempts to comprehend true/false statements spoken in Danish, Norwegian, Swedish, and English. Three groups of native Danish listeners heard 40 sentences each which were translation equivalents, and assessed the truth value of these statements. Group 1 heard sentences in Danish and Norwegian, Group 2 in Danish and Swedish, and Group 3 in Danish and English. Response time and proportion of correct responses were used as indices of processing cost. Both measures indicate that the processing cost for native Danish listeners in comprehending Danish and English statements is equivalent, whereas Norwegian and Swedish statements incur a much higher cost, both in terms of response time and correct assessments. The results are discussed with regard to the costs of inter-Scandinavian and English lingua franca communication",
    "checked": true,
    "id": "c14039a245d155a6f2bcd6e37b138d243e7f266b",
    "semantic_title": "a comparison of danish listeners' processing cost in judging the truth value of norwegian, swedish, and english sentences",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kleber17_interspeech.html": {
    "title": "On the Role of Temporal Variability in the Acquisition of the German Vowel Length Contrast",
    "volume": "main",
    "abstract": "This study is part of a larger project investigating the acquisition of stable vowel-plus-consonant timing patterns needed to convey the phonemic vowel length and the voicing contrast in German. The research is motivated by findings showing greater temporal variability in children until the age of 12. The specific aims of the current study were to test (1) whether temporal variability in the production of the vowel length contrast decreases with increasing age (in general and more so when the variability is speech rate induced) and (2) whether duration cues are perceived more categorically with increasing age. Production and perception data were obtained from eleven preschool, five school children and eleven adults. Results revealed that children produce the quantity contrast with temporal patterns that are similar to adults' patterns, although vowel duration was overall longer and variability slightly higher in faster speech and younger children. Apart from that, the two groups of children did not differ in production. In perception, however, school children's response patterns to a continuum from a long vowel to a short vowel word were in between those of adults and preschool children. Findings are discussed with respect to motor control and phonemic abstraction",
    "checked": true,
    "id": "9f16f29d3f043debab5c18cb83a51ffc1a09dcc2",
    "semantic_title": "on the role of temporal variability in the acquisition of the german vowel length contrast",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2017/reidy17_interspeech.html": {
    "title": "A Data-Driven Approach for Perceptually Validated Acoustic Features for Children's Sibilant Fricative Productions",
    "volume": "main",
    "abstract": "Both perceptual and acoustic studies of children's speech independently suggest that phonological contrasts are continuously refined during acquisition. This paper considers two traditional acoustic features for the ‘s'-vs.-‘sh' contrast (centroid and peak frequencies) and a novel feature learned from data, evaluating these features relative to perceptual ratings of children's productions Productions of sibilant fricatives were elicited from 16 adults and 69 preschool children. A second group of adults rated the children's productions on a visual analog scale (VAS). Each production was rated by multiple listeners; mean VAS score for each production was used as its perceptual goodness rating. For each production from the repetition task, a psychoacoustic spectrum was estimated by passing it through a filter bank that modeled the auditory periphery. From these spectra centroid and peak frequencies were computed, two traditional features for a sibilant fricative's place of articulation. A novel acoustic measure was derived by inputting the spectra to a graph-based dimensionality-reduction algorithm Simple regression analyses indicated that a greater amount of variance in the VAS scores was explained by the novel feature (adjusted R = 0.569) than by either centroid (adjusted R = 0.468) or peak frequency (adjusted R = 0.254)",
    "checked": true,
    "id": "ef173ac50b2f9e7dcba6d60615f4246550213168",
    "semantic_title": "a data-driven approach for perceptually validated acoustic features for children's sibilant fricative productions",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2017/xiao17_interspeech.html": {
    "title": "Proficiency Assessment of ESL Learner's Sentence Prosody with TTS Synthesized Voice as Reference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17g_interspeech.html": {
    "title": "Mechanisms of Tone Sandhi Rule Application by Non-Native Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wiener17_interspeech.html": {
    "title": "Changes in Early L2 Cue-Weighting of Non-Native Speech: Evidence from Learners of Mandarin Chinese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17h_interspeech.html": {
    "title": "Directing Attention During Perceptual Training: A Preliminary Study of Phonetic Learning in Southern Min by Mandarin Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/luo17b_interspeech.html": {
    "title": "Prosody Analysis of L2 English for Naturalness Evaluation Through Speech Modification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/grigonyte17_interspeech.html": {
    "title": "Measuring Encoding Efficiency in Swedish and English Language Learner Speech Production",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hanulikova17_interspeech.html": {
    "title": "Lexical Adaptation to a Novel Accent in German: A Comparison Between German, Swedish, and Finnish Listeners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/fernandez17_interspeech.html": {
    "title": "Qualitative Differences in L3 Learners' Neurophysiological Response to L1 versus L2 Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sjons17_interspeech.html": {
    "title": "Articulation Rate in Swedish Child-Directed Speech Increases as a Function of the Age of the Child Even When Surprisal is Controlled for",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhang17e_interspeech.html": {
    "title": "The Relationship Between the Perception and Production of Non-Native Tones",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/marklund17_interspeech.html": {
    "title": "MMN Responses in Adults After Exposure to Bimodal and Unimodal Frequency Distributions of Rotated Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/berisha17_interspeech.html": {
    "title": "Float Like a Butterfly Sting Like a Bee: Changes in Speech Preceded Parkinsonism Diagnosis for Muhammad Ali",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/castellana17_interspeech.html": {
    "title": "Cepstral and Entropy Analyses in Vowels Excerpted from Continuous Speech of Dysphonic and Control Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bandini17b_interspeech.html": {
    "title": "Classification of Bulbar ALS from Kinematic Features of the Jaw and Lips: Towards Computer-Mediated Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/adiga17_interspeech.html": {
    "title": "Zero Frequency Filter Based Analysis of Voice Disorders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/k17_interspeech.html": {
    "title": "Hypernasality Severity Analysis in Cleft Lip and Palate Speech Using Vowel Space Area",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/laaridh17_interspeech.html": {
    "title": "Automatic Prediction of Speech Evaluation Metrics for Dysarthric Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/klumpp17_interspeech.html": {
    "title": "Apkinson — A Mobile Monitoring Solution for Parkinson's Disease",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hlavnicka17_interspeech.html": {
    "title": "Dysprosody Differentiate Between Parkinson's Disease, Progressive Supranuclear Palsy, and Multiple System Atrophy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tu17b_interspeech.html": {
    "title": "Interpretable Objective Assessment of Dysarthric Speech Based on Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/vachhani17_interspeech.html": {
    "title": "Deep Autoencoder Based Speech Features for Improved Dysarthric Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lilley17_interspeech.html": {
    "title": "Prediction of Speech Delay from Acoustic Measurements",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/li17h_interspeech.html": {
    "title": "The Frequency Range of \"The Ling Six Sounds\" in Standard Chinese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gu17b_interspeech.html": {
    "title": "Production of Sustained Vowels and Categorical Perception of Tones in Mandarin Among Cochlear-Implanted Children",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kumar17b_interspeech.html": {
    "title": "Audio Content Based Geotagging in Multimedia",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/huang17d_interspeech.html": {
    "title": "Time Delay Histogram Based Speech Source Separation Using a Planar Array",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/pradhan17_interspeech.html": {
    "title": "Excitation Source Features for Improving the Detection of Vowel Onset and Offset Points in a Speech Sequence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gao17_interspeech.html": {
    "title": "A Contrast Function and Algorithm for Blind Separation of Audio Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/xu17_interspeech.html": {
    "title": "Weighted Spatial Covariance Matrix Estimation for MUSIC Based TDOA Estimation of Speech Source",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/guo17b_interspeech.html": {
    "title": "Speaker Direction-of-Arrival Estimation Based on Frequency-Independent Beampattern",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wang17i_interspeech.html": {
    "title": "A Mask Estimation Method Integrating Data Field Model for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/shannon17_interspeech.html": {
    "title": "Improved End-of-Query Detection for Streaming Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/he17_interspeech.html": {
    "title": "Using Approximated Auditory Roughness as a Pre-Filtering Feature for Human Screaming and Affective Speech AED",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zegers17_interspeech.html": {
    "title": "Improving Source Separation via Multi-Speaker Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/yang17b_interspeech.html": {
    "title": "Multiple Sound Source Counting and Localization Based on Spatial Principal Eigenvector",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/karthik17_interspeech.html": {
    "title": "Subband Selection for Binaural Speech Source Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17i_interspeech.html": {
    "title": "Unmixing Convolutive Mixtures by Exploiting Amplitude Co-Modulation: Methods and Evaluation on Mandarin Speech Recordings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tao17_interspeech.html": {
    "title": "Bimodal Recurrent Neural Network for Audiovisual Voice Activity Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/maas17_interspeech.html": {
    "title": "Domain-Specific Utterance End-Point Detection for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kothapally17_interspeech.html": {
    "title": "Speech Detection and Enhancement Using Single Microphone for Distant Speech Applications in Reverberant Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wu17d_interspeech.html": {
    "title": "A Post-Filtering Approach Based on Locally Linear Embedding Difference Compensation for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhang17f_interspeech.html": {
    "title": "Multi-Target Ensemble Learning for Monaural Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ogawa17_interspeech.html": {
    "title": "Improved Example-Based Speech Enhancement by Using Deep Neural Network Acoustic Model for Noise Robust Example Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gelderblom17_interspeech.html": {
    "title": "Subjective Intelligibility of Deep Neural Network-Based Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/koutsogiannaki17_interspeech.html": {
    "title": "Real-Time Modulation Enhancement of Temporal Envelopes for Increasing Speech Intelligibility",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hirsch17_interspeech.html": {
    "title": "On the Influence of Modifying Magnitude and Phase Spectrum to Enhance Noisy Speech Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/rehr17_interspeech.html": {
    "title": "MixMax Approximation as a Super-Gaussian Log-Spectral Amplitude Estimator for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/marxer17_interspeech.html": {
    "title": "Binary Mask Estimation Strategies for Constrained Imputation-Based Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/park17c_interspeech.html": {
    "title": "A Fully Convolutional Neural Network for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/li17i_interspeech.html": {
    "title": "Speech Enhancement Using Non-Negative Spectrogram Models with Mel-Generalized Cepstral Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/websdale17_interspeech.html": {
    "title": "A Comparison of Perceptually Motivated Loss Functions for Binary Mask Estimation in Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/michelsanti17_interspeech.html": {
    "title": "Conditional Generative Adversarial Networks for Speech Enhancement and Noise-Robust Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/qian17b_interspeech.html": {
    "title": "Speech Enhancement Using Bayesian Wavenet",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhang17g_interspeech.html": {
    "title": "Binaural Reverberant Speech Separation Based on Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zorila17_interspeech.html": {
    "title": "On the Quality and Intelligibility of Noisy Speech Processed for Near-End Listening Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/meermeier17_interspeech.html": {
    "title": "Applications of the BBN Sage Speech Processing Platform",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/cernak17_interspeech.html": {
    "title": "Bob Speaks Kaldi",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lenarczyk17_interspeech.html": {
    "title": "Real Time Pitch Shifting with Formant Structure Preservation Using the Phase Vocoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chennupati17_interspeech.html": {
    "title": "A Signal Processing Approach for Speaker Separation Using SFF Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/stemmer17_interspeech.html": {
    "title": "Speech Recognition and Understanding on Hardware-Accelerated DSP",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tsuji17_interspeech.html": {
    "title": "MetaLab: A Repository for Meta-Analyses on Language Development, and More",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/daniel17_interspeech.html": {
    "title": "Evolving Recurrent Neural Networks That Process and Classify Raw Audio in a Streaming Fashion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/milosevic17_interspeech.html": {
    "title": "Combining Gaussian Mixture Models and Segmental Feature Models for Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hagerer17_interspeech.html": {
    "title": "Did you laugh enough today?\" — Deep Neural Networks for Mobile and Wearable Laughter Trackers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jeon17_interspeech.html": {
    "title": "Low-Frequency Ultrasonic Communication for Speech Broadcasting in Public Transportation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wood17_interspeech.html": {
    "title": "Real-Time Speech Enhancement with GCC-NMF: Demonstration on the Raspberry Pi and NVIDIA Jetson",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/rouhe17_interspeech.html": {
    "title": "Reading Validation for Pronunciation Evaluation in the Digitala Project",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/pelachaud17_interspeech.html": {
    "title": "Conversing with Social Agents That Smile and Laugh",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/papadopoulos17_interspeech.html": {
    "title": "Team ELISA System for DARPA LORELEI Speech Evaluation 2016",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mihajlik17_interspeech.html": {
    "title": "First Results in Developing a Medieval Latin Language Charter Dictation System for the East-Central Europe Region",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/watson17_interspeech.html": {
    "title": "The Motivation and Development of MPAi, a Māori Pronunciation Aid",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/feng17_interspeech.html": {
    "title": "On the Linguistic Relevance of Speech Units Learned by Unsupervised Acoustic Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/das17_interspeech.html": {
    "title": "Deep Auto-Encoder Based Multi-Task Learning Using Probabilistic Transcriptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gutkin17_interspeech.html": {
    "title": "Areal and Phylogenetic Features for Multilingual Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hall17_interspeech.html": {
    "title": "SLPAnnotator: Tools for Implementing Sign Language Phonetic Annotation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/schwarz17_interspeech.html": {
    "title": "The LENA System Applied to Swedish: Reliability of the Adult Word Count Estimate",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/casillas17_interspeech.html": {
    "title": "What do Babies Hear? Analyses of Child- and Adult-Directed Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/casillas17b_interspeech.html": {
    "title": "A New Workflow for Semi-Automatized Annotations: Tests with Long-Form Naturalistic Recordings of Childrens Language Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bergmann17_interspeech.html": {
    "title": "Top-Down versus Bottom-Up Theories of Phonological Acquisition: A Big Data Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tsuji17b_interspeech.html": {
    "title": "Which Acoustic and Phonological Factors Shape Infants' Vowel Discrimination? Exploiting Natural Variation in InPhonDB",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chasaide17b_interspeech.html": {
    "title": "The ABAIR Initiative: Bringing Spoken Irish into the Digital Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/saeb17_interspeech.html": {
    "title": "Very Low Resource Radio Browsing for Agile Developmental and Humanitarian Monitoring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/malandrakis17_interspeech.html": {
    "title": "Extracting Situation Frames from Non-English Speech: Evaluation Framework and Pilot Results",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kocharov17_interspeech.html": {
    "title": "Eliciting Meaningful Units from Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bhati17_interspeech.html": {
    "title": "Unsupervised Speech Signal to Symbol Transformation for Zero Resource Speech Applications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gauthier17_interspeech.html": {
    "title": "Machine Assisted Analysis of Vowel Length Contrasts in Wolof",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/glarner17_interspeech.html": {
    "title": "Leveraging Text Data for Word Segmentation for Underresourced Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhuang17_interspeech.html": {
    "title": "Improving DNN Bluetooth Narrowband Acoustic Models by Cross-Bandwidth and Cross-Lingual Initialization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/abraham17_interspeech.html": {
    "title": "Joint Estimation of Articulatory Features and Acoustic Models for Low-Resource Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/abraham17b_interspeech.html": {
    "title": "Transfer Learning and Distillation Techniques to Improve the Acoustic Modeling of Low Resource Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/helgadottir17_interspeech.html": {
    "title": "Building an ASR Corpus Using Althingi's Parliamentary Speeches",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/alumae17_interspeech.html": {
    "title": "Implementation of a Radiology Speech Recognition System for Estonian Using Open Source Software",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gunason17_interspeech.html": {
    "title": "Building ASR Corpora Using Eyra",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/niekerk17_interspeech.html": {
    "title": "Rapid Development of TTS Corpora for Four South African Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gutkin17b_interspeech.html": {
    "title": "Uniform Multilingual Multi-Speaker Acoustic Model for Statistical Parametric Speech Synthesis of Low-Resourced Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mendelson17b_interspeech.html": {
    "title": "Nativization of Foreign Names in TTS for Automatic Reading of World News in Swahili",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tong17b_interspeech.html": {
    "title": "Multi-Task Learning for Mispronunciation Detection on Singapore Children's Mandarin Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/larsen17_interspeech.html": {
    "title": "Relating Unsupervised Word Segmentation to Reported Vocabulary Acquisition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wiren17_interspeech.html": {
    "title": "Modelling the Informativeness of Non-Verbal Cues in Parent-Child Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/marklund17b_interspeech.html": {
    "title": "Computational Simulations of Temporal Vocalization Behavior in Adult-Child Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/strombergsson17_interspeech.html": {
    "title": "Approximating Phonotactic Input in Children's Linguistic Environments from Orthographic Transcripts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chaabouni17_interspeech.html": {
    "title": "Learning Weakly Supervised Multimodal Phoneme Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/obuchi17_interspeech.html": {
    "title": "Personalized Quantification of Voice Attractiveness in Multidimensional Merit Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bosker17_interspeech.html": {
    "title": "The Role of Temporal Amplitude Modulations in the Political Arena: Hillary Clinton vs. Donald Trump",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gallardo17b_interspeech.html": {
    "title": "Perceptual Ratings of Voice Likability Collected Through In-Lab Listening Tests vs. Mobile-Based Crowdsourcing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/trouvain17_interspeech.html": {
    "title": "Attractiveness of French Voices for German Listeners — Results from Native and Non-Native Read Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/schweitzer17b_interspeech.html": {
    "title": "Social Attractiveness in Dialogs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/novaktot17_interspeech.html": {
    "title": "A Gender Bias in the Acoustic-Melodic Features of Charismatic Speech?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/michalsky17_interspeech.html": {
    "title": "Pitch Convergence as an Effect of Perceived Attractiveness and Likability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jiao17_interspeech.html": {
    "title": "Does Posh English Sound Attractive?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/baumann17_interspeech.html": {
    "title": "Large-Scale Speaker Ranking from Crowdsourced Pairwise Listener Ratings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/signorello17_interspeech.html": {
    "title": "Aerodynamic Features of French Fricatives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/serrurier17_interspeech.html": {
    "title": "Inter-Speaker Variability: Speaker Normalisation and Quantitative Estimation of Articulatory Invariants in Speech Production for French",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/patil17b_interspeech.html": {
    "title": "Comparison of Basic Beatboxing Articulations Between Expert and Novice Artists Using Real-Time Magnetic Resonance Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tang17b_interspeech.html": {
    "title": "Speaker-Specific Biomechanical Model-Based Investigation of a Simple Speech Task Based on Tagged-MRI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/blaylock17_interspeech.html": {
    "title": "Sounds of the Human Vocal Tract",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/uezu17_interspeech.html": {
    "title": "A Simulation Study on the Effect of Glottal Boundary Conditions on Vocal Tract Formants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gangamohan17_interspeech.html": {
    "title": "A Robust and Alternative Approach to Zero Frequency Filtering Method for Epoch Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hua17_interspeech.html": {
    "title": "Improving YANGsaf F0 Estimator with Adaptive Kalman Filter",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/dhiman17_interspeech.html": {
    "title": "A Spectro-Temporal Demodulation Technique for Pitch Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/miwa17_interspeech.html": {
    "title": "Robust Method for Estimating F0 of Complex Tone Based on Pitch Perception of Amplitude Modulated Signal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/graf17_interspeech.html": {
    "title": "Low-Complexity Pitch Estimation Based on Phase Differences Between Low-Resolution Spectra",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/morise17b_interspeech.html": {
    "title": "Harvest: A High-Performance Fundamental Frequency Estimator from Speech Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/stehwien17_interspeech.html": {
    "title": "Prosodic Event Recognition Using Convolutional Neural Networks with Context Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/galvez17_interspeech.html": {
    "title": "Prosodic Facilitation and Interference While Judging on the Veracity of Synthesized Statements",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zellers17_interspeech.html": {
    "title": "An Investigation of Pitch Matching Across Adjacent Turns in a Corpus of Spontaneous German",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mukherjee17_interspeech.html": {
    "title": "The Relationship Between F0 Synchrony and Speech Convergence in Dyadic Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/luque17_interspeech.html": {
    "title": "The Role of Linguistic and Prosodic Cues on the Prediction of Self-Reported Satisfaction in Contact Centre Phone Calls",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/brusco17_interspeech.html": {
    "title": "Cross-Linguistic Study of the Production of Turn-Taking Cues in American English and Argentine Spanish",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/egorow17_interspeech.html": {
    "title": "Emotional Features for Speech Overlaps Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17j_interspeech.html": {
    "title": "Computing Multimodal Dyadic Behaviors During Spontaneous Diagnosis Interviews Toward Automatic Categorization of Autism Spectrum Disorder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lin17_interspeech.html": {
    "title": "Deriving Dyad-Level Interaction Representation Using Interlocutors Structural and Expressive Multimodal Behavior Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/brueckner17_interspeech.html": {
    "title": "Spotting Social Signals in Conversational Speech over IP: A Deep Learning Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gosztolya17_interspeech.html": {
    "title": "Optimized Time Series Filters for Detecting Laughter and Filler Events",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/haider17_interspeech.html": {
    "title": "Visual, Laughter, Applause and Spoken Expression Features for Predicting Engagement Within TED Talks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/li17j_interspeech.html": {
    "title": "Large-Scale Domain Adaptation via Teacher-Student Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ahmad17_interspeech.html": {
    "title": "Improving Children's Speech Recognition Through Explicit Pitch Scaling Based on Iterative Spectrogram Inversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/xie17_interspeech.html": {
    "title": "RNN-LDA Clustering for Feature Based DNN Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/arsikere17_interspeech.html": {
    "title": "Robust Online i-Vectors for Unsupervised Adaptation of DNN Acoustic Models: A Study in the Context of Digital Voice Assistants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/srinivasamurthy17_interspeech.html": {
    "title": "Semi-Supervised Learning with Semantic Knowledge Extraction for Improved Speech Recognition in Air Traffic Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kim17f_interspeech.html": {
    "title": "Dynamic Layer Normalization for Adaptive Neural Acoustic Modeling in Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bosker17b_interspeech.html": {
    "title": "An Entrained Rhythm's Frequency, Not Phase, Influences Temporal Sampling of Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wang17j_interspeech.html": {
    "title": "Context Regularity Indexed by Auditory N1 and P2 Event-Related Potentials",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/verma17_interspeech.html": {
    "title": "Discovering Language in Marmoset Vocalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/watanabe17_interspeech.html": {
    "title": "Subject-Independent Classification of Japanese Spoken Sentences by Multiple Frequency Bands Phase Pattern of EEG Response During Speech Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/rietmolen17_interspeech.html": {
    "title": "The Phonological Status of the French Initial Accent and its Role in Semantic Processing: An Event-Related Potentials Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhao17_interspeech.html": {
    "title": "A Neuro-Experimental Evidence for the Motor Theory of Speech Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/agrawal17_interspeech.html": {
    "title": "Speech Representation Learning Using Unsupervised Data-Driven Modulation Filtering for Robust ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mimura17_interspeech.html": {
    "title": "Combined Multi-Channel NMF-Based Robust Beamforming for Noisy Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/yu17b_interspeech.html": {
    "title": "Recognizing Multi-Talker Speech with Permutation Invariant Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tachioka17_interspeech.html": {
    "title": "Coupled Initialization of Multi-Channel Non-Negative Matrix Factorization Based on Spatial and Spectral Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/loweimi17b_interspeech.html": {
    "title": "Channel Compensation in the Generalised Vector Taylor Series Approach to Robust ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/king17_interspeech.html": {
    "title": "Robust Speech Recognition via Anchor Word Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bapna17_interspeech.html": {
    "title": "Towards Zero-Shot Frame Semantic Parsing for Domain Scaling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/georgiadou17_interspeech.html": {
    "title": "ClockWork-RNN Based Architectures for Slot Filling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jannet17_interspeech.html": {
    "title": "Investigating the Effect of ASR Tuning on Named Entity Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/dinarelli17_interspeech.html": {
    "title": "Label-Dependency Coding in Simple Recurrent Networks for Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/meng17_interspeech.html": {
    "title": "Minimum Semantic Error Cost Training of Deep Long Short-Term Memory Networks for Topic Spotting on Conversational Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/liu17b_interspeech.html": {
    "title": "Topic Identification for Speech Without ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/liu17c_interspeech.html": {
    "title": "An End-to-End Trainable Neural Network Model with Belief Tracking for Task-Oriented Dialog",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/cuayahuitl17_interspeech.html": {
    "title": "Deep Reinforcement Learning of Dialogue Policies with Less Weight Updates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bayer17_interspeech.html": {
    "title": "Towards End-to-End Spoken Dialogue Systems with Turn Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/akhtiamov17_interspeech.html": {
    "title": "Speech and Text Analysis for Multimodal Addressee Detection in Human-Human-Computer Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ramanarayanan17c_interspeech.html": {
    "title": "Rushing to Judgement: How do Laypeople Rate Caller Engagement in Thin-Slice Videos of Human–Machine Dialog?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kraljevski17_interspeech.html": {
    "title": "Hyperarticulation of Corrections in Multilingual Dialogue Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/milde17_interspeech.html": {
    "title": "Multitask Sequence-to-Sequence Models for Grapheme-to-Phoneme Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhang17h_interspeech.html": {
    "title": "Acoustic Data-Driven Lexicon Learning Based on a Greedy Pronunciation Selection Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/shinozaki17_interspeech.html": {
    "title": "Semi-Supervised Learning of a Pronunciation Dictionary from Disjoint Phonemic Transcripts and Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/smit17_interspeech.html": {
    "title": "Improved Subword Modeling for WFST-Based Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bruguier17_interspeech.html": {
    "title": "Pronunciation Learning with RNN-Transducers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/naaman17_interspeech.html": {
    "title": "Learning Similarity Functions for Pronunciation Variations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gelly17_interspeech.html": {
    "title": "Spoken Language Identification Using LSTM-Based Angular Proximity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jin17_interspeech.html": {
    "title": "End-to-End Language Identification Using High-Order Utterance Representation with Bilinear Pooling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhang17i_interspeech.html": {
    "title": "Dialect Recognition Based on Unsupervised Bottleneck Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/irtza17_interspeech.html": {
    "title": "Investigating Scalability in Hierarchical Language Identification System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/qian17c_interspeech.html": {
    "title": "Improving Sub-Phone Modeling for Better Native Language Identification with Non-Native English Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/khurana17_interspeech.html": {
    "title": "QMDIS: QCRI-MIT Advanced Dialect Identification System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/alluri17b_interspeech.html": {
    "title": "Detection of Replay Attacks Using Single Frequency Filtering Cepstral Coefficients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sailor17_interspeech.html": {
    "title": "Unsupervised Representation Learning Using Convolutional Restricted Boltzmann Machine for Spoof Speech Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/suthokumar17_interspeech.html": {
    "title": "Independent Modelling of High and Low Energy Speech Frames for Spoofing Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sarkar17_interspeech.html": {
    "title": "Improving Speaker Verification Performance in Presence of Spoofing Attacks Using Out-of-Domain Spoofed Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/nagrani17_interspeech.html": {
    "title": "VoxCeleb: A Large-Scale Speaker Identification Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jones17b_interspeech.html": {
    "title": "Call My Net Corpus: A Multilingual Corpus for Evaluation of Speaker Recognition Technology",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/weiss17_interspeech.html": {
    "title": "Sequence-to-Sequence Models Can Directly Translate Foreign Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kano17_interspeech.html": {
    "title": "Structured-Based Curriculum Learning for End-to-End English-Japanese Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ruiz17_interspeech.html": {
    "title": "Assessing the Tolerance of Neural Machine Translation Systems Against Speech Recognition Errors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/do17b_interspeech.html": {
    "title": "Toward Expressive Speech Translation: A Unified Sequence-to-Sequence LSTMs Approach for Translating Words and Emphasis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/cho17_interspeech.html": {
    "title": "NMT-Based Segmentation and Punctuation Insertion for Real-Time Spoken Language Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/drude17_interspeech.html": {
    "title": "Tight Integration of Spatial and Spectral Features for BSS with Deep Clustering Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zmolikova17_interspeech.html": {
    "title": "Speaker-Aware Neural Network Based Beamformer for Speaker Extraction in Speech Mixtures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/pfeifenberger17_interspeech.html": {
    "title": "Eigenvector-Based Speech Mask Estimation Using Logistic Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wood17b_interspeech.html": {
    "title": "Real-Time Speech Enhancement with GCC-NMF",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ji17b_interspeech.html": {
    "title": "Coherence-Based Dual-Channel Noise Reduction Algorithm in a Complex Noisy Environment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhang17j_interspeech.html": {
    "title": "Glottal Model Based Speech Beamforming for ad-hoc Microphone Arrays",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/liu17d_interspeech.html": {
    "title": "Acoustic Assessment of Disordered Voice with Continuous Speech Based on Utterance-Level ASR Posterior Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ylmaz17c_interspeech.html": {
    "title": "Multi-Stage DNN Training for Automatic Recognition of Dysarthric Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/smith17_interspeech.html": {
    "title": "Improving Child Speech Disorder Assessment by Incorporating Out-of-Domain Adult Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/joy17b_interspeech.html": {
    "title": "On Improving Acoustic Models for TORGO Dysarthric Speech Database",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/simantiraki17_interspeech.html": {
    "title": "Glottal Source Features for Automatic Speech-Based Depression Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sadeghian17_interspeech.html": {
    "title": "Speech Processing Approach for Diagnosing Dementia in an Early Stage",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/biadsy17_interspeech.html": {
    "title": "Effectively Building Tera Scale MaxEnt Language Models Incorporating Non-Linguistic Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/deena17_interspeech.html": {
    "title": "Semi-Supervised Adaptation of RNNLMs by Fine-Tuning with Domain-Specific Auxiliary Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/singh17_interspeech.html": {
    "title": "Approximated and Domain-Adapted LSTM Language Models for First-Pass Decoding in Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chelba17_interspeech.html": {
    "title": "Sparse Non-Negative Matrix Language Modeling: Maximum Entropy Flexibility on the Cheap",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kumar17c_interspeech.html": {
    "title": "Multi-Scale Context Adaptation for Improving Child Automatic Speech Recognition in Child-Adult Spoken Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhu17_interspeech.html": {
    "title": "Using Knowledge Graph and Search Query Click Logs in Statistical Language Model for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/dimitriadis17_interspeech.html": {
    "title": "Developing On-Line Speaker Diarization System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/seshadri17_interspeech.html": {
    "title": "Comparison of Non-Parametric Bayesian Mixture Models for Syllable Clustering and Zero-Resource Speech Processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/proenca17b_interspeech.html": {
    "title": "Automatic Evaluation of Children Reading Aloud on Sentences and Pseudowords",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/yoon17_interspeech.html": {
    "title": "Off-Topic Spoken Response Detection with Word Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/li17k_interspeech.html": {
    "title": "Improving Mispronunciation Detection for Non-Native Learners with Multisource Information and LSTM-Based Deep Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tsujimura17_interspeech.html": {
    "title": "Automatic Explanation Spot Estimation Method Targeted at Text and Figures in Lecture Slides",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kim17g_interspeech.html": {
    "title": "Multiview Representation Learning via Deep CCA for Silent Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/knill17_interspeech.html": {
    "title": "Use of Graphemic Lexicons for Spoken Language Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/yi17_interspeech.html": {
    "title": "Distilling Knowledge from an Ensemble of Models for Punctuation Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/pusateri17_interspeech.html": {
    "title": "A Mostly Data-Driven Approach to Inverse Text Normalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17k_interspeech.html": {
    "title": "Mismatched Crowdsourcing from Multiple Annotator Languages for Recognizing Zero-Resourced Languages: A Nullspace Clustering Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gale17_interspeech.html": {
    "title": "Experiments in Character-Level Neural Network Models for Punctuation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kaushik17_interspeech.html": {
    "title": "Multi-Channel Apollo Mission Speech Transcripts Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mclaren17_interspeech.html": {
    "title": "Calibration Approaches for Language Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/fernando17_interspeech.html": {
    "title": "Bidirectional Modelling for Short Duration Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/shen17b_interspeech.html": {
    "title": "Conditional Generative Adversarial Nets Classifier for Spoken Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/miguel17_interspeech.html": {
    "title": "Tied Hidden Factors in Neural Networks for End-to-End Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/yun17_interspeech.html": {
    "title": "Speaker Clustering by Iteratively Finding Discriminative Feature Space and Cluster Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/vinals17_interspeech.html": {
    "title": "Domain Adaptation of PLDA Models in Broadcast Diarization by Means of Unsupervised Speaker Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/india17_interspeech.html": {
    "title": "LSTM Neural Network-Based Speaker Segmentation Using Acoustic and Language Modelling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gresse17_interspeech.html": {
    "title": "Acoustic Pairing of Original and Dubbed Voices in the Context of Video Game Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ajili17_interspeech.html": {
    "title": "Homogeneity Measure Impact on Target and Non-Target Trials in Forensic Voice Comparison",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/solewicz17_interspeech.html": {
    "title": "Null-Hypothesis LLR: A Proposal for Forensic Automatic Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/liu17e_interspeech.html": {
    "title": "The Opensesame NIST 2016 Speaker Recognition Evaluation System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kumar17d_interspeech.html": {
    "title": "IITG-Indigo System for NIST 2016 SRE Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/misra17_interspeech.html": {
    "title": "Locally Weighted Linear Discriminant Analysis for Robust Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/shon17b_interspeech.html": {
    "title": "Recursive Whitening Transformation for Speaker Recognition on Language Mismatched Condition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/settle17_interspeech.html": {
    "title": "Query-by-Example Search with Discriminative Neural Acoustic Word Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kaneko17b_interspeech.html": {
    "title": "Constructing Acoustic Distances Between Subwords and States Obtained from a Deep Neural Network for Spoken Term Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/khokhlov17_interspeech.html": {
    "title": "Fast and Accurate OOV Decoder on High-Level Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17l_interspeech.html": {
    "title": "Exploring the Use of Significant Words Language Modeling for Spoken Document Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tasaki17_interspeech.html": {
    "title": "Incorporating Acoustic Features for Spontaneous Speech Driven Content Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lu17b_interspeech.html": {
    "title": "Order-Preserving Abstractive Summarization for Spoken Content Based on Connectionist Temporal Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tsuchiya17_interspeech.html": {
    "title": "Automatic Alignment Between Classroom Lecture Utterances and Slide Components",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lopezotero17_interspeech.html": {
    "title": "Compensating Gender Variability in Query-by-Example Search on Speech Using Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kumar17e_interspeech.html": {
    "title": "Zero-Shot Learning Across Heterogeneous Overlapping Domains",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tsunoo17_interspeech.html": {
    "title": "Hierarchical Recurrent Neural Network for Story Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bouchekif17_interspeech.html": {
    "title": "Evaluating Automatic Topic Segmentation as a Segment Retrieval Task",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bang17_interspeech.html": {
    "title": "Improving Speech Recognizers by Refining Broadcast Data with Inaccurate Subtitle Timestamps",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/svec17_interspeech.html": {
    "title": "A Relevance Score Estimation for Spoken Term Detection Based on RNN-Generated Pronunciation Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gallardo17c_interspeech.html": {
    "title": "Predicting Automatic Speech Recognition Performance Over Communication Channels from Instrumental Speech Quality and Intelligibility Scores",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/botinhao17_interspeech.html": {
    "title": "Speech Intelligibility in Cars: The Effect of Speaking Style, Noise and Listener Age",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/yamamoto17_interspeech.html": {
    "title": "Predicting Speech Intelligibility Using a Gammachirp Envelope Distortion Index Based on the Signal-to-Distortion Ratio",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17m_interspeech.html": {
    "title": "Intelligibilities of Mandarin Chinese Sentences with Spectral \"Holes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ward17b_interspeech.html": {
    "title": "The Effect of Situation-Specific Non-Speech Acoustic Cues on the Intelligibility of Speech in Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/andersen17_interspeech.html": {
    "title": "On the Use of Band Importance Weighting in the Short-Time Objective Intelligibility Measure",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/spille17_interspeech.html": {
    "title": "Listening in the Dips: Comparing Relevant Features for Speech Recognition in Humans and Machines",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sugai17_interspeech.html": {
    "title": "Mental Representation of Japanese Mora; Focusing on its Intrinsic Duration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ying17_interspeech.html": {
    "title": "Temporal Dynamics of Lateral Channel Formation in /l/: 3D EMA Data from Australian English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/klingler17_interspeech.html": {
    "title": "Vowel and Consonant Sequences in three Bavarian Dialects of Austria",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/issa17_interspeech.html": {
    "title": "Acoustic Cues to the Singleton-Geminate Contrast: The Case of Libyan Arabic Sonorants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/brandt17_interspeech.html": {
    "title": "Mel-Cepstral Distortion of German Vowels in Different Information Density Contexts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/boril17_interspeech.html": {
    "title": "Effect of Formant and F0 Discontinuity on Perceived Vowel Duration: Impacts for Concatenative Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tabain17_interspeech.html": {
    "title": "An Ultrasound Study of Alveolar and Retroflex Consonants in Arrernte: Stressed and Unstressed Syllables",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gobl17_interspeech.html": {
    "title": "Reshaping the Transformed LF Model: Generating the Glottal Source from the Waveshape Parameter Rd",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/benus17_interspeech.html": {
    "title": "Kinematic Signatures of Prosody in Lombard Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jochim17b_interspeech.html": {
    "title": "What do Finnish and Central Bavarian Have in Common? Towards an Acoustically Based Quantity Typology",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/nellore17_interspeech.html": {
    "title": "Locating Burst Onsets Using SFF Envelope and Phase Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ding17_interspeech.html": {
    "title": "A Preliminary Phonetic Investigation of Alphabetic Words in Mandarin Chinese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/schatz17_interspeech.html": {
    "title": "A Quantitative Measure of the Impact of Coarticulation on Phone Discriminability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lin17b_interspeech.html": {
    "title": "Sinusoidal Partials Tracking for Singing Analysis Using the Heuristic of the Minimal Frequency and Magnitude Difference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/phan17_interspeech.html": {
    "title": "Audio Scene Classification with Deep Recurrent Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sandsten17_interspeech.html": {
    "title": "Automatic Time-Frequency Analysis of Echolocation Signals Using the Matched Gaussian Multitaper Spectrogram",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/matousek17_interspeech.html": {
    "title": "Classification-Based Detection of Glottal Closure Instants from Speech Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/qi17_interspeech.html": {
    "title": "A Domain Knowledge-Assisted Nonlinear Model for Head-Related Transfer Functions Based on Bottleneck Deep Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jesus17_interspeech.html": {
    "title": "Laryngeal Articulation During Trumpet Performance: An Exploratory Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/guan17_interspeech.html": {
    "title": "Matrix of Polynomials Model Based Polynomial Dictionary Learning Method for Acoustic Impulse Response Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hyder17_interspeech.html": {
    "title": "Acoustic Scene Classification Using a CNN-SuperVector System Trained with Auditory and Spectrogram Image Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/feng17b_interspeech.html": {
    "title": "An Environmental Feature Representation for Robust Speech Recognition and for Environment Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/xu17b_interspeech.html": {
    "title": "Attention and Localization Based on a Deep Convolutional Recurrent Model for Weakly Supervised Audio Tagging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/pan17_interspeech.html": {
    "title": "An Audio Based Piano Performance Evaluation Method Using Deep Neural Network Based Acoustic Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chowdhury17_interspeech.html": {
    "title": "Music Tempo Estimation Using Sub-Band Synchrony",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wang17k_interspeech.html": {
    "title": "A Transfer Learning Based Feature Extractor for Polyphonic Sound Event Detection Using Connectionist Temporal Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mostafa17_interspeech.html": {
    "title": "A Note Based Query By Humming System Using Convolutional Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sailor17b_interspeech.html": {
    "title": "Unsupervised Filterbank Learning Using Convolutional Restricted Boltzmann Machine for Environmental Sound Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/soni17_interspeech.html": {
    "title": "Novel Shifted Real Spectrum for Exact Signal Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/weiner17_interspeech.html": {
    "title": "Manual and Automatic Transcriptions in Dementia Detection from Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gupta17_interspeech.html": {
    "title": "An Affect Prediction Approach Through Depression Severity Parameter Incorporation in Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gillespie17_interspeech.html": {
    "title": "Cross-Database Models for the Classification of Dysarthria Presence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/novotny17_interspeech.html": {
    "title": "Acoustic Evaluation of Nasality in Cerebellar Syndromes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hantke17_interspeech.html": {
    "title": "Emotional Speech of Mentally and Physically Disabled Individuals: Introducing the EmotAsS Database and First Findings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/agurto17_interspeech.html": {
    "title": "Phonological Markers of Oxytocin and MDMA Ingestion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mirheidari17_interspeech.html": {
    "title": "An Avatar-Based System for Identifying Individuals Likely to Develop Dementia",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhang17k_interspeech.html": {
    "title": "Cross-Domain Classification of Drowsiness in Speech: The Case of Alcohol Intoxication and Sleep Deprivation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lopezotero17b_interspeech.html": {
    "title": "Depression Detection Using Automatic Transcriptions of De-Identified Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wankerl17_interspeech.html": {
    "title": "An N-Gram Based Approach to the Automatic Diagnosis of Alzheimer's Disease from Spoken Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mundnich17_interspeech.html": {
    "title": "Exploiting Intra-Annotator Rating Consistency Through Copeland's Method for Estimation of Ground Truth Labels in Couples' Therapy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/pettorino17_interspeech.html": {
    "title": "Rhythmic Characteristics of Parkinsonian Speech: A Study on Mandarin and Polish",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tu17c_interspeech.html": {
    "title": "Trisyllabic Tone 3 Sandhi Patterns in Mandarin Produced by Cantonese Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sahkai17_interspeech.html": {
    "title": "Intonation of Contrastive Topic in Estonian",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hao17_interspeech.html": {
    "title": "Reanalyze Fundamental Frequency Peak Delay in Mandarin",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/michelas17_interspeech.html": {
    "title": "How Does the Absence of Shared Knowledge Between Interlocutors Affect the Production of French Prosodic Forms?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wagner17_interspeech.html": {
    "title": "Three Dimensions of Sentence Prosody and Their (Non-)Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kleinhans17_interspeech.html": {
    "title": "Using Prosody to Classify Discourse Relations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/godoy17_interspeech.html": {
    "title": "Canonical Correlation Analysis and Prediction of Perceived Rhythmic Prominences and Pitch Tones in Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kakouros17_interspeech.html": {
    "title": "Evaluation of Spectral Tilt Measures for Sentence Prominence Under Different Noise Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kuang17_interspeech.html": {
    "title": "Creaky Voice as a Function of Tonal Categories and Prosodic Boundaries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/skarnitzl17_interspeech.html": {
    "title": "The Acoustics of Word Stress in Czech as a Function of Speaking Style",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wagner17b_interspeech.html": {
    "title": "What You See is What You Get Prosodically Less — Visibility Shapes Prosodic Prominence Production in Spontaneous Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hsu17b_interspeech.html": {
    "title": "Focus Acoustics in Mandarin Nominals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lundmark17_interspeech.html": {
    "title": "Exploring Multidimensionality: Acoustic and Articulatory Correlates of Swedish Word Accents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/puga17_interspeech.html": {
    "title": "The Perception of English Intonation Patterns by German L2 Speakers of English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/paradacabaleiro17_interspeech.html": {
    "title": "The Perception of Emotions in Noisified Nonsense Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gibson17_interspeech.html": {
    "title": "Attention Networks for Modeling Behaviors in Addiction Counseling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wortwein17_interspeech.html": {
    "title": "Computational Analysis of Acoustic Descriptors in Psychotic Patients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wu17e_interspeech.html": {
    "title": "Modeling Perceivers Neural-Responses Using Lobe-Dependent Convolutional Neural Network to Improve Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/vlasenko17_interspeech.html": {
    "title": "Implementing Gender-Dependent Vowel-Level Analysis for Boosting Speech-Based Depression Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/siddique17_interspeech.html": {
    "title": "Bilingual Word Embeddings for Cross-Lingual Personality Recognition Using Convolutional Neural Nets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/arimoto17_interspeech.html": {
    "title": "Emotion Category Mapping to Emotional Space by Cross-Corpus Emotion Labeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/fayet17_interspeech.html": {
    "title": "Big Five vs. Prosodic Features as Cues to Detect Abnormality in SSPNET-Personality Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/akira17_interspeech.html": {
    "title": "Speech Rate Comparison When Talking to a System and Talking to a Human: A Study from a Speech-to-Speech, Machine Translation Mediated Map Task",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tseng17_interspeech.html": {
    "title": "Approaching Human Performance in Behavior Estimation in Couples Therapy Using Deep Sentence Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/nasir17_interspeech.html": {
    "title": "Complexity in Speech and its Relation to Emotional Bond in Therapist-Patient Interactions During Suicide Risk Assessment Interviews",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/huang17e_interspeech.html": {
    "title": "An Investigation of Emotion Dynamics and Kalman Filtering for Speech-Based Emotion Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sadamitsu17_interspeech.html": {
    "title": "Zero-Shot Learning for Natural Language Understanding Using Domain-Independent Sequential Structure and Question Types",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sawada17_interspeech.html": {
    "title": "Parallel Hierarchical Attention Networks with Shared Memory Reader for Multi-Stream Conversational Document Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/morchid17_interspeech.html": {
    "title": "Internal Memory Gate for Recurrent Neural Networks with Application to Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/korpusik17_interspeech.html": {
    "title": "Character-Based Embedding Models and Reranking Strategies for Understanding Natural Language Meal Descriptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/parcollet17_interspeech.html": {
    "title": "Quaternion Denoising Encoder-Decoder for Theme Identification of Telephone Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/simonnet17_interspeech.html": {
    "title": "ASR Error Management for Improving Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ma17e_interspeech.html": {
    "title": "Jointly Trained Sequential Labeling and Classification by Sparse Attention Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/nayak17_interspeech.html": {
    "title": "To Plan or not to Plan? Discourse Planning in Slot-Value Informed Sequence to Sequence Models for Language Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/riou17_interspeech.html": {
    "title": "Online Adaptation of an Attention-Based Neural Network for Natural Language Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/martinezhinarejos17_interspeech.html": {
    "title": "Spanish Sign Language Recognition with Different Topology Hidden Markov Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/morales17_interspeech.html": {
    "title": "OpenMM: An Open-Source Multimodal Feature Extraction Tool",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/huang17f_interspeech.html": {
    "title": "Speaker Dependency Analysis, Audiovisual Fusion Cues and a Multimodal BLSTM for Conversational Engagement Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hsu17c_interspeech.html": {
    "title": "Voice Conversion from Unaligned Corpora Using Variational Autoencoding Wasserstein Generative Adversarial Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/nakashika17_interspeech.html": {
    "title": "CAB: An Energy-Based Speaker Clustering Model for Rapid Adaptation in Non-Parallel Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/aihara17_interspeech.html": {
    "title": "Phoneme-Discriminative Features for Dysarthric Speech Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wu17f_interspeech.html": {
    "title": "Denoising Recurrent Neural Network for Deep Bidirectional LSTM Based Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tanaka17b_interspeech.html": {
    "title": "Speaker Dependent Approach for Enhancing a Glossectomy Patient's Speech via GMM-Based Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kaneko17c_interspeech.html": {
    "title": "Generative Adversarial Network-Based Postfilter for STFT Spectrograms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bollepalli17_interspeech.html": {
    "title": "Generative Adversarial Network-Based Glottal Waveform Model for Statistical Parametric Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/luo17c_interspeech.html": {
    "title": "Emotional Voice Conversion with Adaptive Scales F0 Based on Wavelet Transform Using Limited Amount of Emotional Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/doddipatla17_interspeech.html": {
    "title": "Speaker Adaptation in DNN-Based Speech Synthesis Using d-Vectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/li17l_interspeech.html": {
    "title": "Spectro-Temporal Modelling with Time-Frequency LSTM and Structured Output Layer for Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ramos17_interspeech.html": {
    "title": "Segment Level Voice Conversion with Recurrent Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/moore17_interspeech.html": {
    "title": "Creating a Voice for MiRo, the World's First Commercial Biomimetic Robot",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/dominguez17_interspeech.html": {
    "title": "A Thematicity-Based Prosody Enrichment Tool for CTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gruber17_interspeech.html": {
    "title": "WebSubDub — Experimental System for Creating High-Quality Alternative Audio Track for TV Broadcasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/juzova17_interspeech.html": {
    "title": "Voice Conservation and TTS System for People Facing Total Laryngectomy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ghone17_interspeech.html": {
    "title": "TBT (Toolkit to Build TTS): A High Performance Framework to Build Multiple Language HTS Voice",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/karhila17_interspeech.html": {
    "title": "SIAK — A Game for Foreign Language Pronunciation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/larsson17_interspeech.html": {
    "title": "Integrating the Talkamatic Dialogue Manager with Alexa",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ahmed17_interspeech.html": {
    "title": "A Robust Medical Speech-to-Speech/Speech-to-Sign Phraselator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/duckhorn17_interspeech.html": {
    "title": "Towards an Autarkic Embedded Cognitive User Interface",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/winata17_interspeech.html": {
    "title": "Nora the Empathetic Psychologist",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/alam17_interspeech.html": {
    "title": "Modifying Amazon's Alexa ASR Grammar and Lexicon — A Case Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lindblom17_interspeech.html": {
    "title": "Re-Inventing Speech — The Biological Way",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/schuller17_interspeech.html": {
    "title": "The INTERSPEECH 2017 Computational Paralinguistics Challenge: Addressee, Cold & Snoring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/krajewski17_interspeech.html": {
    "title": "Description of the Upper Respiratory Tract Infection Corpus (URTIC)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/janott17_interspeech.html": {
    "title": "Description of the Munich-Passau Snore Sound Corpus (MPSSC)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bergelson17_interspeech.html": {
    "title": "Description of the Homebank Child/Adult Addressee Corpus (HB-CHAAC)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/huckvale17_interspeech.html": {
    "title": "It Sounds Like You Have a Cold! Testing Voice Features for the Interspeech 2017 Computational Paralinguistics Cold Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/cai17b_interspeech.html": {
    "title": "End-to-End Deep Learning Framework for Speech Paralinguistics Detection Based on Perception Aware Spectrum",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wagner17c_interspeech.html": {
    "title": "Infected Phonemes: How a Cold Impairs Speech on a Phonetic Level",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/suresh17_interspeech.html": {
    "title": "Phoneme State Posteriorgram Features for Speech Based Automatic Classification of Speakers in Cold and Healthy Condition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/nwe17_interspeech.html": {
    "title": "An Integrated Solution for Snoring Sound Classification Using Bhattacharyya Distance Based GMM Supervectors with SVM, Feature Selection with Random Forest and Spectrogram with CNN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kitamura17_interspeech.html": {
    "title": "Acoustic Analysis of Detailed Three-Dimensional Shape of the Human Nasal Cavity and Paranasal Sinuses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/arnela17_interspeech.html": {
    "title": "A Semi-Polar Grid Strategy for the Three-Dimensional Finite Element Simulation of Vowel-Vowel Sequences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/vasudevan17_interspeech.html": {
    "title": "A Fast Robust 1D Flow Model for a Self-Oscillating Coupled 2D FEM Vocal Fold Simulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/murtola17_interspeech.html": {
    "title": "Waveform Patterns in Pitch Glides Near a Vocal Tract Resonance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/degirmenci17_interspeech.html": {
    "title": "A Unified Numerical Simulation of Vowel Production That Comprises Phonation and the Emitted Sound",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/dabbaghchian17_interspeech.html": {
    "title": "Synthesis of VV Utterances from Muscle Activation to Sound with a 3D Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mv17_interspeech.html": {
    "title": "A Dual Source-Filter Model of Snore Audio for Snorer Group Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/freitag17_interspeech.html": {
    "title": "An ‘End-to-Evolution' Hybrid Approach for Snore Sound Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/amiriparian17_interspeech.html": {
    "title": "Snore Sound Classification Using Image-Based Deep Spectrum Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tavarez17_interspeech.html": {
    "title": "Exploring Fusion Methods and Feature Space for the Classification of Paralinguistic Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gosztolya17b_interspeech.html": {
    "title": "DNN-Based Feature Extraction and Classifier Combination for Child-Directed Speech, Cold and Snoring Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kaya17_interspeech.html": {
    "title": "Introducing Weighted Kernel Classifiers for Handling Imbalanced Paralinguistic Corpora: Snoring, Addressee and Cold",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/steidl17_interspeech.html": {
    "title": "The INTERSPEECH 2017 Computational Paralinguistics Challenge: A Summary of Results",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/schuller17b_interspeech.html": {
    "title": "Discussion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/toshniwal17_interspeech.html": {
    "title": "Multitask Learning with Low-Level Auxiliary Tasks for Encoder-Decoder Based Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/shannon17b_interspeech.html": {
    "title": "Optimizing Expected Word Error Rate via Sampling for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sainath17_interspeech.html": {
    "title": "Annealed f-Smoothing as a Mechanism to Speed up Neural Network Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/meng17b_interspeech.html": {
    "title": "Non-Uniform MCE Training of Deep Long Short-Term Memory Recurrent Neural Networks for Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/dighe17_interspeech.html": {
    "title": "Exploiting Eigenposteriors for Semi-Supervised Training of DNN Acoustic Models with Sequence Discrimination",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/yang17c_interspeech.html": {
    "title": "Discriminative Autoencoders for Acoustic Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zajic17_interspeech.html": {
    "title": "Speaker Diarization Using Convolutional Neural Network for Statistics Accumulation Refinement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/jati17_interspeech.html": {
    "title": "Speaker2Vec: Unsupervised Learning and Adaptation of a Speaker Manifold Using Deep Neural Networks with an Evaluation on Speaker Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/lan17_interspeech.html": {
    "title": "A Triplet Ranking-Based Neural Network for Speaker Diarization and Linking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/cohen17_interspeech.html": {
    "title": "Estimating Speaker Clustering Quality Using Logistic Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wisniewksi17_interspeech.html": {
    "title": "Combining Speaker Turn Embedding and Incremental Structure Prediction for Low-Latency Speaker Diarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bredin17_interspeech.html": {
    "title": "pyannote.metrics: A Toolkit for Reproducible Evaluation, Diagnostic, and Error Analysis of Speaker Diarization Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17n_interspeech.html": {
    "title": "A Rescoring Approach for Keyword Search Using Lattice Context Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/trmal17_interspeech.html": {
    "title": "The Kaldi OpenKWS System: Improving Low Resource Keyword Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/khokhlov17b_interspeech.html": {
    "title": "The STC Keyword Search System for OpenKWS 2016 Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/sun17_interspeech.html": {
    "title": "Compressed Time Delay Neural Network for Small-Footprint Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/suzuki17b_interspeech.html": {
    "title": "Symbol Sequence Search from Telephone Conversation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gundogdu17_interspeech.html": {
    "title": "Similarity Learning Based Query Modeling for Keyword Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/samui17_interspeech.html": {
    "title": "Deep Recurrent Neural Network Based Monaural Speech Separation Using Recurrent Temporal Restricted Boltzmann Machines",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/huang17g_interspeech.html": {
    "title": "Improved Codebook-Based Speech Enhancement Based on MBE Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chen17o_interspeech.html": {
    "title": "Improving Mask Learning Based Speech Enhancement System with Restoration Layers and Residual Connection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/yan17_interspeech.html": {
    "title": "Exploring Low-Dimensional Structures of Modulation Spectra for Robust Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/pascual17_interspeech.html": {
    "title": "SEGAN: Speech Enhancement Generative Adversarial Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/maiti17_interspeech.html": {
    "title": "Concatenative Resynthesis Using Twin Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/stafylakis17_interspeech.html": {
    "title": "Combining Residual Networks with LSTMs for Lipreading",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/thangthai17_interspeech.html": {
    "title": "Improving Computer Lipreading via DNN Sequence Discriminative Training Techniques",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wand17_interspeech.html": {
    "title": "Improving Speaker-Independent Lipreading with Domain-Adversarial Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/abdelaziz17_interspeech.html": {
    "title": "Turbo Decoders for Audio-Visual Continuous Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/csapo17_interspeech.html": {
    "title": "DNN-Based Ultrasound-to-Speech Conversion for a Silent Speech Interface",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kamper17_interspeech.html": {
    "title": "Visually Grounded Learning of Keyword Prediction from Untranscribed Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chien17d_interspeech.html": {
    "title": "Deep Neural Factorization for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/vesely17_interspeech.html": {
    "title": "Semi-Supervised DNN Training with Word Selection for ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hou17b_interspeech.html": {
    "title": "Gaussian Prediction Based Attention for Online End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/fukuda17_interspeech.html": {
    "title": "Efficient Knowledge Distillation from an Ensemble of Teachers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/prabhavalkar17b_interspeech.html": {
    "title": "An Analysis of \"Attention\" in Sequence-to-Sequence Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/soltau17_interspeech.html": {
    "title": "Neural Speech Recognizer: Acoustic-to-Word LSTM Model for Large Vocabulary Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/guo17c_interspeech.html": {
    "title": "CNN-Based Joint Mapping of Short and Long Utterance i-Vectors for Speaker Verification Using Short Utterances",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ranjan17b_interspeech.html": {
    "title": "Curriculum Learning Based Probabilistic Linear Discriminant Analysis for Noise Robust Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mahto17_interspeech.html": {
    "title": "i-Vector Transformation Using a Novel Discriminative Denoising Autoencoder for Noise-Robust Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wang17l_interspeech.html": {
    "title": "Unsupervised Discriminative Training of PLDA for Domain Adaptation in Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/alam17b_interspeech.html": {
    "title": "Speaker Verification Under Adverse Conditions Using i-Vector Adaptation and Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/castan17_interspeech.html": {
    "title": "Improving Robustness of Speaker Recognition to New Conditions Using Unlabeled Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/abidi17_interspeech.html": {
    "title": "CALYOU: A Comparable Spoken Algerian Corpus Harvested from YouTube",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/narwekar17_interspeech.html": {
    "title": "PRAV: A Phonetically Rich Audio Visual Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/abdelaziz17b_interspeech.html": {
    "title": "NTCD-TIMIT: A New Database and Baseline for Noise-Robust Audio-Visual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/howcroft17_interspeech.html": {
    "title": "The Extended SPaRKy Restaurant Corpus: Designing a Corpus with Variable Information Density",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mansikkaniemi17_interspeech.html": {
    "title": "Automatic Construction of the Finnish Parliament Speech Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/abdo17_interspeech.html": {
    "title": "Building Audio-Visual Phonetically Annotated Arabic Corpus for Expressive Text to Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hughes17_interspeech.html": {
    "title": "What is the Relevant Population? Considerations for the Computation of Likelihood Ratios in Forensic Voice Comparison",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/delvaux17_interspeech.html": {
    "title": "Voice Disguise vs. Impersonation: Acoustic and Perceptual Measurements of Vocal Flexibility in Non Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wu17g_interspeech.html": {
    "title": "Schwa Realization in French: Using Automatic Speech Processing to Study Phonological and Socio-Linguistic Factors in Large Corpora",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/duran17_interspeech.html": {
    "title": "The Social Life of Setswana Ejectives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kohtz17_interspeech.html": {
    "title": "How Long is Too Long? How Pause Features After Requests Affect the Perceived Willingness of Affirmative Answers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gessinger17_interspeech.html": {
    "title": "Shadowing Synthesized Speech — Segmental Analysis of Phonetic Convergence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ghaffarzadegan17_interspeech.html": {
    "title": "Occupancy Detection in Commercial and Residential Environments Using Audio Signal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tran17b_interspeech.html": {
    "title": "Data Augmentation, Missing Feature Mask and Kernel Classification for Through-the-Wall Acoustic Surveillance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/chang17_interspeech.html": {
    "title": "Endpoint Detection Using Grid Long Short-Term Memory Networks for Streaming Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/baby17_interspeech.html": {
    "title": "Deep Learning Techniques in Tandem with Signal Processing Cues for Phonetic Segmentation for Text to Speech Synthesis in Indian Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wang17m_interspeech.html": {
    "title": "Gate Activation Signal Analysis for Gated Recurrent Neural Networks and its Correlation with Phoneme Boundaries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/yin17_interspeech.html": {
    "title": "Speaker Change Detection in Broadcast TV Using Bidirectional Long Short-Term Memory Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/do17c_interspeech.html": {
    "title": "Improved Automatic Speech Recognition Using Subband Temporal Envelope Features and Time-Delay Neural Network Denoising Autoencoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/fujimoto17_interspeech.html": {
    "title": "Factored Deep Convolutional Neural Networks for Noise Robust Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/papadopoulos17b_interspeech.html": {
    "title": "Global SNR Estimation of Speech Signals for Unknown Noise Conditions Using Noise Adapted Non-Linear Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/ge17_interspeech.html": {
    "title": "Joint Training of Multi-Channel-Condition Dereverberation and Acoustic Modeling of Microphone Array Speech for Robust Distant Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/tran17c_interspeech.html": {
    "title": "Uncertainty Decoding with Adaptive Sampling for Noise Robust DNN-Based Acoustic Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zhang17l_interspeech.html": {
    "title": "Attention-Based LSTM with Multi-Task Learning for Distant Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/huang17h_interspeech.html": {
    "title": "To Improve the Robustness of LSTM-RNN Acoustic Models Using Higher-Order Feedback from Multiple Histories",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/kim17h_interspeech.html": {
    "title": "End-to-End Speech Recognition with Auditory Attention for Multi-Microphone Distance Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/menon17_interspeech.html": {
    "title": "Robust Speech Recognition Based on Binaural Auditory Processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/caroselli17_interspeech.html": {
    "title": "Adaptive Multichannel Dereverberation for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zihlmann17_interspeech.html": {
    "title": "The Effects of Real and Placebo Alcohol on Deaffrication",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/mcauliffe17b_interspeech.html": {
    "title": "Polyglot and Speech Corpus Tools: A System for Representing, Integrating, and Querying Speech Corpora",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hughes17b_interspeech.html": {
    "title": "Mapping Across Feature Spaces in Forensic Voice Comparison: The Contribution of Auditory-Based Voice Quality to (Semi-)Automatic System Testing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/arantes17_interspeech.html": {
    "title": "Effect of Language, Speaking Style and Speaker on Long-Term F0 Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/volin17_interspeech.html": {
    "title": "Stability of Prosodic Characteristics Across Age and Gender Groups",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/plantehebert17_interspeech.html": {
    "title": "Electrophysiological Correlates of Familiar Voice Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/cooperleavitt17_interspeech.html": {
    "title": "Developing an Embosi (Bantu C25) Speech Variant Dictionary to Model Vowel Elision and Morpheme Deletion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/murphy17_interspeech.html": {
    "title": "Rd as a Control Parameter to Explore Affective Correlates of the Tense-Lax Continuum",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/barbosa17_interspeech.html": {
    "title": "Cross-Linguistic Distinctions Between Professional and Non-Professional Speaking Styles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gendrot17_interspeech.html": {
    "title": "Perception and Production of Word-Final /ʁ/ in French",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/narendra17_interspeech.html": {
    "title": "Glottal Source Estimation from Coded Telephone Speech Using a Deep Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/christodoulides17_interspeech.html": {
    "title": "Automatic Labelling of Prosodic Prominence, Phrasing and Disfluencies in French Speech by Simulating the Perception of Naïve and Expert Listeners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/levit17_interspeech.html": {
    "title": "Don't Count on ASR to Transcribe for You: Breaking Bias with Two Crowds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/airaksinen17_interspeech.html": {
    "title": "Effects of Training Data Variety in Generating Glottal Pulses from Acoustic Features with DNNs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/hantke17b_interspeech.html": {
    "title": "Towards Intelligent Crowdsourcing for Audio Data Annotation: Integrating Active Learning in the Real World",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/henter17_interspeech.html": {
    "title": "Principles for Learning Controllable TTS from Annotated and Latent Variation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/takamichi17_interspeech.html": {
    "title": "Sampling-Based Speech Parameter Generation Using Moment-Matching Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/pollet17_interspeech.html": {
    "title": "Unit Selection with Hierarchical Cascaded Long Short Term Memory Bidirectional Recurrent Neural Nets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/cooper17_interspeech.html": {
    "title": "Utterance Selection for Optimizing Intelligibility of TTS Voices Trained on ASR Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/rosenberg17_interspeech.html": {
    "title": "Bias and Statistical Significance in Evaluating Speech Synthesis with Mean Opinion Scores",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/adiga17b_interspeech.html": {
    "title": "Phase Modeling Using Integrated Linear Prediction Residual for Statistical Parametric Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/gonzalez17_interspeech.html": {
    "title": "Evaluation of a Silent Speech Interface Based on Magnetic Sensing and Deep Learning for a Phonetically Rich Vocabulary",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/greenwood17_interspeech.html": {
    "title": "Predicting Head Pose from Speech with a Conditional Variational Autoencoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wester17_interspeech.html": {
    "title": "Real-Time Reactive Speech Synthesis: Incorporating Interruptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/blaauw17_interspeech.html": {
    "title": "A Neural Parametric Singing Synthesizer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/wang17n_interspeech.html": {
    "title": "Tacotron: Towards End-to-End Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/capes17_interspeech.html": {
    "title": "Siri On-Device Deep Learning-Guided Unit Selection Text-to-Speech System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/esch17_interspeech.html": {
    "title": "An Expanded Taxonomy of Semiotic Classes for Text Normalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/nakashika17b_interspeech.html": {
    "title": "Complex-Valued Restricted Boltzmann Machine for Direct Learning of Frequency Spectra",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/zioko17_interspeech.html": {
    "title": "Soundtracing for Realtime Speech Adjustment to Environmental Conditions in 3D Simulations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/arai17b_interspeech.html": {
    "title": "Vocal-Tract Model with Static Articulators: Lips, Teeth, Tongue, and More",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/masudakatsuse17_interspeech.html": {
    "title": "Remote Articulation Test System Based on WebRTC",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/bunnell17_interspeech.html": {
    "title": "The ModelTalker Project: A Web-Based Voice Banking Pipeline for ALS/MND Patients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2017/heeringa17_interspeech.html": {
    "title": "Visible Vowels: A Tool for the Visualization of Vowel Variation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  }
}