{
  "https://jmlr.org/papers/v18/15-619.html": {
    "title": "On Binary Embedding using Circulant Matrices",
    "abstract": "Binary embeddings provide efficient and powerful ways to perform operations on large scale data. However binary embedding typically requires long codes in order to preserve the discriminative power of the input space. Thus binary coding methods traditionally suffer from high computation and storage costs in such a scenario. To address this problem, we propose Circulant Binary Embedding (CBE) which generates binary codes by projecting the data with a circulant matrix. The circulant structure allows us to use Fast Fourier Transform algorithms to speed up the computation. For obtaining $k$-bit binary codes from $d$-dimensional data, our method improves the time complexity from $\\mathcal{O}(dk)$ to $\\mathcal{O}(d\\log{d})$, and the space complexity from $\\mathcal{O}(dk)$ to $\\mathcal{O}(d)$. We study two settings, which differ in the way we choose the parameters of the circulant matrix. In the first, the parameters are chosen randomly and in the second, the parameters are learned using the data. For randomized CBE, we give a theoretical analysis comparing it with binary embedding using an unstructured random projection matrix. The challenge here is to show that the dependencies in the entries of the circulant matrix do not lead to a loss in performance. In the second setting, we design a novel time-frequency alternating optimization to learn data-dependent circulant projections, which alternatively minimizes the objective in original and Fourier domains. In both the settings, we show by extensive experiments that the CBE approach gives much better performance than the state-of-the-art approaches if we fix a running time, and provides much faster computation with negligible performance degradation if we fix the number of bits in the embedding",
    "volume": "main",
    "checked": true,
    "id": "1edb498c7cef4b43783809b0a3e86ac857c69573",
    "citation_count": 25
  },
  "https://jmlr.org/papers/v18/16-579.html": {
    "title": "Variational Fourier Features for Gaussian Processes",
    "abstract": "This work brings together two powerful concepts in Gaussian processes: the variational approach to sparse approximation and the spectral representation of Gaussian processes. This gives rise to an approximation that inherits the benefits of the variational approach but with the representational power and computational scalability of spectral representations. The work hinges on a key result that there exist spectral features related to a finite domain of the Gaussian process which exhibit almost-independent covariances. We derive these expressions for MatÃ©rn kernels in one dimension, and generalize to more dimensions using kernels with specific structures. Under the assumption of additive Gaussian noise, our method requires only a single pass through the data set, making for very fast and accurate computation. We fit a model to 4 million training points in just a few minutes on a standard laptop. With non- conjugate likelihoods, our MCMC scheme reduces the cost of computation from $\\mathcal{O}(NM^2)$ (for a sparse Gaussian process) to $\\mathcal{O}(NM)$ per iteration, where $N$ is the number of data and $M$ is the number of features",
    "volume": "main",
    "checked": true,
    "id": "a9fae3cbd13c2d0a80a1f0a625167884d2738344",
    "citation_count": 162
  },
  "https://jmlr.org/papers/v18/17-434.html": {
    "title": "HyperTools: a Python Toolbox for Gaining Geometric Insights into High-Dimensional Data",
    "abstract": "Dimensionality reduction algorithms have played a foundational role in facilitating the deep understanding of complex high- dimensional data. One particularly useful application of dimensionality reduction techniques is in data visualization. Low-dimensional visualizations can help practitioners understand where machine learning algorithms might leverage the geometric properties of a dataset to improve performance. Another challenge is to generalize insights across datasets [e.g. data from multiple modalities describing the same system (Haxby et al., 2011), artwork or photographs of similar content in different styles (Zhu et al., 2017), etc.]. Several recently developed techniques(e.g. Haxby et al., 2011; Chen et al., 2015) use the procrustean transformation (Schonemann, 1966) to align the geometries of two or more spaces so that data with different axes may be plotted in a common space. We propose that each of these techniques (dimensionality reduction, alignment, and visualization) applied in sequence should be cast as a single conceptual hyperplot operation for gaining geometric insights into high-dimensional data. Our Python toolbox enables this operation in a single (highly flexible) function call",
    "volume": "MLOSS",
    "checked": true,
    "id": "71602749a2f9c449b032f06498edcc8c8dd5a944",
    "citation_count": 18
  },
  "https://jmlr.org/papers/v18/17-468.html": {
    "title": "Automatic Differentiation in Machine Learning: a Survey",
    "abstract": "Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply Ã¢ÂÂautodiffÃ¢ÂÂ, is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational fluid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other's results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names Ã¢ÂÂdynamic computational graphsÃ¢ÂÂ and Ã¢ÂÂdifferentiable programmingÃ¢ÂÂ. We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms Ã¢ÂÂautodiffÃ¢ÂÂ, Ã¢ÂÂautomatic differentiationÃ¢ÂÂ, and Ã¢ÂÂsymbolic differentiationÃ¢ÂÂ as these are encountered more and more in machine learning settings",
    "volume": "main",
    "checked": true,
    "id": "da118b8aa99699edd7609fbbd081d5b93bc2e87b",
    "citation_count": 1809
  },
  "https://jmlr.org/papers/v18/15-154.html": {
    "title": "Normal Bandits of Unknown Means and Variances",
    "abstract": "Consider the problem of sampling sequentially from a finite number of $N \\geq 2$ populations, specified by random variables $X^i_k$, $ i = 1,\\ldots , N,$ and $k = 1, 2, \\ldots$; where $X^i_k$ denotes the outcome from population $i$ the $k^{th}$ time it is sampled. It is assumed that for each fixed $i$, $\\{ X^i_k \\}_{k \\geq 1}$ is a sequence of i.i.d. normal random variables, with unknown mean $\\mu_i$ and unknown variance $\\sigma_i^2$. The objective is to have a policy $\\pi$ for deciding from which of the $N$ populations to sample from at any time $t=1,2,\\ldots$ so as to maximize the expected sum of outcomes of $n$ total samples or equivalently to minimize the regret due to lack on information of the parameters $\\mu_i$ and $\\sigma_i^2$. In this paper, we present a simple inflated sample mean (ISM) index policy that is asymptotically optimal in the sense of Theorem 4 below. This resolves a standing open problem from \\cite{bkmab96}. Additionally, finite horizon regret bounds are given",
    "volume": "main",
    "checked": true,
    "id": "769791248102153914658300fb5fce43ca91d8b6",
    "citation_count": 63
  },
  "https://jmlr.org/papers/v18/15-226.html": {
    "title": "Cost-Sensitive Learning with Noisy Labels",
    "abstract": "We study binary classification in the presence of \\emph{class- conditional} random noise, where the learner gets to see labels that are flipped independently with some probability, and where the flip probability depends on the class. Our goal is to devise learning algorithms that are efficient and statistically consistent with respect to commonly used utility measures. In particular, we look at a family of measures motivated by their application in domains where cost-sensitive learning is necessary (for example, when there is class imbalance). In contrast to most of the existing literature on consistent classification that are limited to the classical 0-1 loss, our analysis includes more general utility measures such as the AM measure (arithmetic mean of True Positive Rate and True Negative Rate). For this problem of cost-sensitive learning under class- conditional random noise, we develop two approaches that are based on suitably modifying surrogate losses. First, we provide a simple unbiased estimator of any loss, and obtain performance bounds for empirical utility maximization in the presence of i.i.d. data with noisy labels. If the loss function satisfies a simple symmetry condition, we show that using unbiased estimator leads to an efficient algorithm for empirical maximization. Second, by leveraging a reduction of risk minimization under noisy labels to classification with weighted 0-1 loss, we suggest the use of a simple weighted surrogate loss, for which we are able to obtain strong utility bounds. This approach implies that methods already used in practice, such as biased SVM and weighted logistic regression, are provably noise- tolerant. For two practically important measures in our family, we show that the proposed methods are competitive with respect to recently proposed methods for dealing with label noise in several benchmark data sets",
    "volume": "main",
    "checked": true,
    "id": "362510c9e6a97e35074eb01c2528b856143886f4",
    "citation_count": 49
  },
  "https://jmlr.org/papers/v18/15-233.html": {
    "title": "Provably Correct Algorithms for Matrix Column Subset Selection with Selectively Sampled Data",
    "abstract": "We consider the problem of matrix column subset selection, which selects a subset of columns from an input matrix such that the input can be well approximated by the span of the selected columns. Column subset selection has been applied to numerous real-world data applications such as population genetics summarization, electronic circuits testing and recommendation systems. In many applications the complete data matrix is unavailable and one needs to select representative columns by inspecting only a small portion of the input matrix. In this paper we propose the first provably correct column subset selection algorithms for partially observed data matrices. Our proposed algorithms exhibit different merits and limitations in terms of statistical accuracy, computational efficiency, sample complexity and sampling schemes, which provides a nice exploration of the tradeoff between these desired properties for column subset selection. The proposed methods employ the idea of feedback driven sampling and are inspired by several sampling schemes previously introduced for low-rank matrix approximation tasks (Drineas et al., 2008; Frieze et al., 2004; Deshpande and Vempala, 2006; Krishnamurthy and Singh, 2014). Our analysis shows that, under the assumption that the input data matrix has incoherent rows but possibly coherent columns, all algorithms provably converge to the best low-rank approximation of the original data as number of selected columns increases. Furthermore, two of the proposed algorithms enjoy a relative error bound, which is preferred for column subset selection and matrix approximation purposes. We also demonstrate through both theoretical and empirical analysis the power of feedback driven sampling compared to uniform random sampling on input matrices with highly correlated columns",
    "volume": "main",
    "checked": true,
    "id": "ee87ea9d42996b7ca307ebd1e8867c2981924dcc",
    "citation_count": 15
  },
  "https://jmlr.org/papers/v18/15-373.html": {
    "title": "A Study of the Classification of Low-Dimensional Data with Supervised Manifold Learning",
    "abstract": "Supervised manifold learning methods learn data representations by preserving the geometric structure of data while enhancing the separation between data samples from different classes. In this work, we propose a theoretical study of supervised manifold learning for classification. We consider nonlinear dimensionality reduction algorithms that yield linearly separable embeddings of training data and present generalization bounds for this type of algorithms. A necessary condition for satisfactory generalization performance is that the embedding allow the construction of a sufficiently regular interpolation function in relation with the separation margin of the embedding. We show that for supervised embeddings satisfying this condition, the classification error decays at an exponential rate with the number of training samples. Finally, we examine the separability of supervised nonlinear embeddings that aim to preserve the low-dimensional geometric structure of data based on graph representations. The proposed analysis is supported by experiments on several real data sets",
    "volume": "main",
    "checked": true,
    "id": "5d72144f4b414b6ad6f3198b42df36c69be7ce38",
    "citation_count": 25
  },
  "https://jmlr.org/papers/v18/15-481.html": {
    "title": "Probabilistic preference learning with the Mallows rank model",
    "abstract": "Ranking and comparing items is crucial for collecting information about preferences in many areas, from marketing to politics. The Mallows rank model is among the most successful approaches to analyse rank data, but its computational complexity has limited its use to a particular form based on Kendall distance. We develop new computationally tractable methods for Bayesian inference in Mallows models that work with any right-invariant distance. Our method performs inference on the consensus ranking of the items, also when based on partial rankings, such as top-$k$ items or pairwise comparisons. We prove that items that none of the assessors has ranked do not influence the maximum a posteriori consensus ranking, and can therefore be ignored. When assessors are many or heterogeneous, we propose a mixture model for clustering them in homogeneous subgroups, with cluster-specific consensus rankings. We develop approximate stochastic algorithms that allow a fully probabilistic analysis, leading to coherent quantifications of uncertainties. We make probabilistic predictions on the class membership of assessors based on their ranking of just some items, and predict missing individual preferences, as needed in recommendation systems. We test our approach using several experimental and benchmark datasets",
    "volume": "main",
    "checked": true,
    "id": "4df6b6751421724d18bc70cb9d4f2ef6d15d6136",
    "citation_count": 80
  },
  "https://jmlr.org/papers/v18/15-484.html": {
    "title": "Robust Topological Inference: Distance To a Measure and Kernel Distance",
    "abstract": "Let $P$ be a distribution with support $S$. The salient features of $S$ can be quantified with persistent homology, which summarizes topological features of the sublevel sets of the distance function (the distance of any point $x$ to $S$). Given a sample from $P$ we can infer the persistent homology using an empirical version of the distance function. However, the empirical distance function is highly non-robust to noise and outliers. Even one outlier is deadly. The distance-to-a-measure (DTM), introduced by \\cite{chazal2011geometric}, and the kernel distance, introduced by \\cite{phillips2014goemetric}, are smooth functions that provide useful topological information but are robust to noise and outliers. \\cite{massart2014} derived concentration bounds for DTM. Building on these results, we derive limiting distributions and confidence sets, and we propose a method for choosing tuning parameters",
    "volume": "main",
    "checked": true,
    "id": "c9c8cabbb15c7341324c010fe205c8c676fbb375",
    "citation_count": 161
  },
  "https://jmlr.org/papers/v18/15-506.html": {
    "title": "Training Gaussian Mixture Models at Scale via Coresets",
    "abstract": "How can we train a statistical mixture model on a massive data set? In this work we show how to construct \\emph{coresets} for mixtures of Gaussians. A coreset is a weighted subset of the data, which guarantees that models fitting the coreset also provide a good fit for the original data set. We show that, perhaps surprisingly, Gaussian mixtures admit coresets of size polynomial in dimension and the number of mixture components, while being \\emph{independent} of the data set size. Hence, one can harness computationally intensive algorithms to compute a good approximation on a significantly smaller data set. More importantly, such coresets can be efficiently constructed both in distributed and streaming settings and do not impose restrictions on the data generating process. Our results rely on a novel reduction of statistical estimation to problems in computational geometry and new combinatorial complexity results for mixtures of Gaussians. Empirical evaluation on several real- world data sets suggests that our coreset-based approach enables significant reduction in training-time with negligible approximation error",
    "volume": "main",
    "checked": true,
    "id": "4baebac87436a44f6f06a84ffc54f89ed9d6b46b",
    "citation_count": 75
  },
  "https://jmlr.org/papers/v18/15-592.html": {
    "title": "Gradient Estimation with Simultaneous Perturbation and Compressive Sensing",
    "abstract": "We propose a scheme for finding a \"good\" estimator for the gradient of a function on a high-dimensional space with few function evaluations, for applications where function evaluations are expensive and the function under consideration is not sensitive in all coordinates locally, making its gradient almost sparse. Exploiting the latter aspect, our method combines ideas from Spall's Simultaneous Perturbation Stochastic Approximation with compressive sensing. We theoretically justify its computational advantages and illustrate them empirically by numerical experiments. In particular, applications to estimating gradient outer product matrix as well as standard optimization problems are illustrated via simulations",
    "volume": "main",
    "checked": true,
    "id": "c0bda50db615f1041f27b12df0e18ed4b050e0f0",
    "citation_count": 6
  },
  "https://jmlr.org/papers/v18/15-595.html": {
    "title": "Principled Selection of Hyperparameters in the Latent Dirichlet Allocation Model",
    "abstract": "Latent Dirichlet Allocation (LDA) is a well known topic model that is often used to make inference regarding the properties of collections of text documents. LDA is a hierarchical Bayesian model, and involves a prior distribution on a set of latent topic variables. The prior is indexed by certain hyperparameters, and even though these have a large impact on inference, they are usually chosen either in an ad-hoc manner, or by applying an algorithm whose theoretical basis has not been firmly established. We present a method, based on a combination of Markov chain Monte Carlo and importance sampling, for estimating the maximum likelihood estimate of the hyperparameters. The method may be viewed as a computational scheme for implementation of an empirical Bayes analysis. It comes with theoretical guarantees, and a key feature of our approach is that we provide theoretically-valid error margins for our estimates. Experiments on both synthetic and real data show good performance of our methodology",
    "volume": "main",
    "checked": true,
    "id": "faf1ae7711186c3408ecef288cc04bcd94fd621b",
    "citation_count": 45
  },
  "https://jmlr.org/papers/v18/17-527.html": {
    "title": "Deep Learning the Ising Model Near Criticality",
    "abstract": "It is well established that neural networks with deep architectures perform better than shallow networks for many tasks in machine learning. In statistical physics, while there has been recent interest in representing physical data with generative modelling, the focus has been on shallow neural networks. A natural question to ask is whether deep neural networks hold any advantage over shallow networks in representing such data. We investigate this question by using unsupervised, generative graphical models to learn the probability distribution of a two-dimensional Ising system. Deep Boltzmann machines, deep belief networks, and deep restricted Boltzmann networks are trained on thermal spin configurations from this system, and compared to the shallow architecture of the restricted Boltzmann machine. We benchmark the models, focussing on the accuracy of generating energetic observables near the phase transition, where these quantities are most difficult to approximate. Interestingly, after training the generative networks, we observe that the accuracy essentially depends only on the number of neurons in the first hidden layer of the network, and not on other model details such as network depth or model type. This is evidence that shallow networks are more efficient than deep networks at representing physical probability distributions associated with Ising systems near criticality",
    "volume": "main",
    "checked": true,
    "id": "81fc34df88b2293981380454e7259629ff645320",
    "citation_count": 79
  },
  "https://jmlr.org/papers/v18/17-636.html": {
    "title": "pomegranate: Fast and Flexible Probabilistic Modeling in Python",
    "abstract": "We present pomegranate, an open source machine learning package for probabilistic modeling in Python. Probabilistic modeling encompasses a wide range of methods that explicitly describe uncertainty using probability distributions. Three widely used probabilistic models implemented in pomegranate are general mixture models, hidden Markov models, and Bayesian networks. A primary focus of pomegranate is to abstract away the complexities of training models from their definition. This allows users to focus on specifying the correct model for their application instead of being limited by their understanding of the underlying algorithms. An aspect of this focus involves the collection of additive sufficient statistics from data sets as a strategy for training models. This approach trivially enables many useful learning strategies, such as out-of-core learning, minibatch learning, and semi-supervised learning, without requiring the user to consider how to partition data or modify the algorithms to handle these tasks themselves. pomegranate is written in Cython to speed up calculations and releases the global interpreter lock to allow for built-in multithreaded parallelism, making it competitive with---or outperform---other implementations of similar algorithms. This paper presents an overview of the design choices in pomegranate, and how they have enabled complex features to be supported by simple code. The code is available at \\url{https://github.com/jmschrei/pomegranate}",
    "volume": "main",
    "checked": true,
    "id": "65e1ada2360b42368a5f9f5e40ff436051c6fa84",
    "citation_count": 152
  },
  "https://jmlr.org/papers/v18/17-653.html": {
    "title": "Maximum Principle Based Algorithms for Deep Learning",
    "abstract": "The continuous dynamical system approach to deep learning is explored in order to devise alternative frameworks for training algorithms. Training is recast as a control problem and this allows us to formulate necessary optimality conditions in continuous time using the Pontryagin's maximum principle (PMP). A modification of the method of successive approximations is then used to solve the PMP, giving rise to an alternative training algorithm for deep learning. This approach has the advantage that rigorous error estimates and convergence results can be established. We also show that it may avoid some pitfalls of gradient-based methods, such as slow convergence on flat landscapes near saddle points. Furthermore, we demonstrate that it obtains favorable initial convergence rate per-iteration, provided Hamiltonian maximization can be efficiently carried out - a step which is still in need of improvement. Overall, the approach opens up new avenues to attack problems associated with deep learning, such as trapping in slow manifolds and inapplicability of gradient-based methods for discrete trainable variables",
    "volume": "main",
    "checked": true,
    "id": "5e3fd9e6e7bcfc37fa751385ea3c8c7c7ac80c43",
    "citation_count": 188
  },
  "https://jmlr.org/papers/v18/14-415.html": {
    "title": "Gradient Hard Thresholding Pursuit",
    "abstract": "Hard Thresholding Pursuit (HTP) is an iterative greedy selection procedure for finding sparse solutions of underdetermined linear systems. This method has been shown to have strong theoretical guarantee and impressive numerical performance. In this article, we generalize HTP from compressed sensing to a generic problem setup of sparsity-constrained convex optimization. The proposed algorithm iterates between a standard gradient descent step and a hard-thresholding step with or without debiasing. We analyze the parameter estimation and sparsity recovery performance of the proposed method. Extensive numerical results confirm our theoretical predictions and demonstrate the superiority of our method to the state-of-the-art greedy selection methods in sparse linear regression, sparse logistic regression and sparse precision matrix estimation problems.\\footnote{A conference version of this work appeared in ICML 2014 \\citep{Yuan- ICML-2014}.}",
    "volume": "main",
    "checked": true,
    "id": "346ad27591f1b65ad9b21ecb28def7763d832d61",
    "citation_count": 74
  },
  "https://jmlr.org/papers/v18/15-636.html": {
    "title": "Risk-Constrained Reinforcement Learning with Percentile Risk Criteria",
    "abstract": "In many sequential decision-making problems one is interested in minimizing an expected cumulative cost while taking into account risk, i.e., increased awareness of events of small probability and high consequences. Accordingly, the objective of this paper is to present efficient reinforcement learning algorithms for risk-constrained Markov decision processes (MDPs), where risk is represented via a chance constraint or a constraint on the conditional value-at-risk (CVaR) of the cumulative cost. We collectively refer to such problems as percentile risk-constrained MDPs. Specifically, we first derive a formula for computing the gradient of the Lagrangian function for percentile risk-constrained MDPs. Then, we devise policy gradient and actor-critic algorithms that (1) estimate such gradient, (2) update the policy in the descent direction, and (3) update the Lagrange multiplier in the ascent direction. For these algorithms we prove convergence to locally optimal policies. Finally, we demonstrate the effectiveness of our algorithms in an optimal stopping problem and an online marketing application",
    "volume": "main",
    "checked": true,
    "id": "759bbd8dd50cb4790cad7a3bccbdfcbfee5e3e89",
    "citation_count": 328
  },
  "https://jmlr.org/papers/v18/16-119.html": {
    "title": "Local Identifiability of $\\ell_1$-minimization Dictionary Learning: a Sufficient and Almost Necessary Condition",
    "abstract": "We study the theoretical properties of learning a dictionary from $N$ signals $\\mathbf{x}_i\\in \\mathbb R^K$ for $i=1,\\ldots,N$ via $\\ell_1$-minimization. We assume that $\\mathbf{x}_i$'s are $i.i.d.$ random linear combinations of the $K$ columns from a complete (i.e., square and invertible) reference dictionary $\\mathbf{D}_0 \\in \\mathbb R^{K\\times K}$. Here, the random linear coefficients are generated from either the $s$-sparse Gaussian model or the Bernoulli-Gaussian model. First, for the population case, we establish a sufficient and almost necessary condition for the reference dictionary $\\mathbf{D}_0$ to be locally identifiable, i.e., a strict local minimum of the expected $\\ell_1$-norm objective function. Our condition covers both sparse and dense cases of the random linear coefficients and significantly improves the sufficient condition by Gribonval and Schnass (2010). In addition, we show that for a complete $\\mu$-coherent reference dictionary, i.e., a dictionary with absolute pairwise column inner-product at most $\\mu\\in[0,1)$, local identifiability holds even when the random linear coefficient vector has up to $O(\\mu^{-2})$ nonzero entries. Moreover, our local identifiability results also translate to the finite sample case with high probability provided that the number of signals $N$ scales as $O(K\\log K)$",
    "volume": "main",
    "checked": true,
    "id": "572b96d8ce584e75a96200c929ad03432ba88331",
    "citation_count": 14
  },
  "https://jmlr.org/papers/v18/17-069.html": {
    "title": "In Search of Coherence and Consensus: Measuring the Interpretability of Statistical Topics",
    "abstract": "Topic modeling is an important tool in natural language processing. Topic models provide two forms of output. The first is a predictive model. This type of model has the ability to predict unseen documents (e.g., their categories). When topic models are used in this way, there are ample measures to assess their performance. The second output of these models is the topics themselves. Topics are lists of keywords that describe the top words pertaining to each topic. Often, these lists of keywords are presented to a human subject who then assesses the meaning of the topic, which is ultimately subjective. One of the fundamental problems of topic models lies in assessing the quality of the topics from the perspective of human interpretability. Naturally, human subjects need to be employed to evaluate interpretability of a topic. Lately, crowdsourcing approaches are widely used to serve the role of human subjects in evaluation. In this work we study measures of interpretability and propose to measure topic interpretability from two perspectives: topic coherence and topic consensus. We start with an existing measure for topic coherence---model precision. It evaluates coherence of a topic by introducing an intruded word and measuring how well a human subject or a crowdsourcing approach could identify the intruded word: if it is easy to identify, the topic is coherent. We then investigate how we can measure coherence comprehensively by examining dimensions of topic coherence. For the second perspective of topic interpretability, we suggest topic consensus that measures how well the results of a crowdsourcing approach matches those given categories of topics. Good topics should lead to good categories, thus, high topic consensus. Therefore, if there is low topic consensus in terms of categories, topics could be of low interpretability. We then further discuss how topic coherence and topic consensus assess different aspects of topic interpretability and hope that this work can pave way for comprehensive measures of topic interpretability",
    "volume": "main",
    "checked": true,
    "id": "f992072f620b559cc0c49eb0112c84bbf5cab44a",
    "citation_count": 19
  },
  "https://jmlr.org/papers/v18/17-151.html": {
    "title": "On the Behavior of Intrinsically High-Dimensional Spaces: Distances, Direct and Reverse Nearest Neighbors, and Hubness",
    "abstract": "Over the years, different characterizations of the curse of dimensionality have been provided, usually stating the conditions under which, in the limit of the infinite dimensionality, distances become indistinguishable. However, these characterizations almost never address the form of associated distributions in the finite, although high- dimensional, case. This work aims to contribute in this respect by investigating the distribution of distances, and of direct and reverse nearest neighbors, in intrinsically high-dimensional spaces. Indeed, we derive a closed form for the distribution of distances from a given point, for the expected distance from a given point to its $k$th nearest neighbor, and for the expected size of the approximate set of neighbors of a given point in finite high-dimensional spaces. Additionally, the hubness problem is considered, which is related to the form of the function $N_k$ representing the number of points that have a given point as one of their $k$ nearest neighbors, which is also called the number of $k$-occurrences. Despite the extensive use of this function, the precise characterization of its form is a longstanding problem. We derive a closed form for the number of $k$-occurrences associated with a given point in finite high- dimensional spaces, together with the associated limiting probability distribution. By investigating the relationships with the hubness phenomenon emerging in network science, we find that the distribution of node (in-)degrees of some real-life, large-scale networks has connections with the distribution of $k$-occurrences described herein",
    "volume": "main",
    "checked": true,
    "id": "3646bd0f042ee386361426ff01f88b7f1a8d39f9",
    "citation_count": 21
  },
  "https://jmlr.org/papers/v18/17-457.html": {
    "title": "Convergence of Unregularized Online Learning Algorithms",
    "abstract": "In this paper we study the convergence of online gradient descent algorithms in reproducing kernel Hilbert spaces (RKHSs) without regularization. We establish a sufficient condition and a necessary condition for the convergence of excess generalization errors in expectation. A sufficient condition for the almost sure convergence is also given. With high probability, we provide explicit convergence rates of the excess generalization errors for both averaged iterates and the last iterate, which in turn also imply convergence rates with probability one. To our best knowledge, this is the first high- probability convergence rate for the last iterate of online gradient descent algorithms in the general convex setting. Without any boundedness assumptions on iterates, our results are derived by a novel use of two measures of the algorithm's one- step progress, respectively by generalization errors and by distances in RKHSs, where the variances of the involved martingales are cancelled out by the descent property of the algorithm",
    "volume": "main",
    "checked": true,
    "id": "4f7642c396ebfa115444ab5ba6819b5068398f1b",
    "citation_count": 11
  },
  "https://jmlr.org/papers/v18/16-556.html": {
    "title": "Convergence Analysis of Distributed Inference with Vector-Valued Gaussian Belief Propagation",
    "abstract": "This paper considers inference over distributed linear Gaussian models using factor graphs and Gaussian belief propagation (BP). The distributed inference algorithm involves only local computation of the information matrix and of the mean vector, and message passing between neighbors. Under broad conditions, it is shown that the message information matrix converges to a unique positive definite limit matrix for arbitrary positive semidefinite initialization, and it approaches an arbitrarily small neighborhood of this limit matrix at an exponential rate. A necessary and sufficient convergence condition for the belief mean vector to converge to the optimal centralized estimator is provided under the assumption that the message information matrix is initialized as a positive semidefinite matrix. Further, it is shown that Gaussian BP always converges when the underlying factor graph is given by the union of a forest and a single loop. The proposed convergence condition in the setup of distributed linear Gaussian models is shown to be strictly weaker than other existing convergence conditions and requirements, including the Gaussian Markov random field based walk-summability condition, and applicable to a large class of scenarios",
    "volume": "main",
    "checked": true,
    "id": "0424813e783d90ec7c5b8994301991a991e912f1",
    "citation_count": 51
  },
  "https://jmlr.org/papers/v18/17-406.html": {
    "title": "auDeep: Unsupervised Learning of Representations from Audio with Deep Recurrent Neural Networks",
    "abstract": "auDeep is a Python toolkit for deep unsupervised representation learning from acoustic data. It is based on a recurrent sequence to sequence autoencoder approach which can learn representations of time series data by taking into account their temporal dynamics. We provide an extensive command line interface in addition to a Python API for users and developers, both of which are comprehensively documented and publicly available at https://github.com/auDeep/auDeep. Experimental results indicate that auDeep features are competitive with state-of-the art audio classification",
    "volume": "main",
    "checked": true,
    "id": "589b9bdabddfa3a8d03fcbcf9b45514b128cde1e",
    "citation_count": 124
  },
  "https://jmlr.org/papers/v18/17-514.html": {
    "title": "On the Stability of Feature Selection Algorithms",
    "abstract": "Feature Selection is central to modern data science, from exploratory data analysis to predictive model-building. The Ã¢ÂÂstabilityÃ¢ÂÂ of a feature selection algorithm refers to the robustness of its feature preferences, with respect to data sampling and to its stochastic nature. An algorithm is `unstable' if a small change in data leads to large changes in the chosen feature subset. Whilst the idea is simple, quantifying this has proven more challenging---we note numerous proposals in the literature, each with different motivation and justification. We present a rigorous statistical treatment for this issue. In particular, with this work we consolidate the literature and provide (1) a deeper understanding of existing work based on a small set of properties, and (2) a clearly justified statistical approach with several novel benefits. This approach serves to identify a stability measure obeying all desirable properties, and (for the first time in the literature) allowing confidence intervals and hypothesis tests on the stability, enabling rigorous experimental comparison of feature selection algorithms",
    "volume": "main",
    "checked": true,
    "id": "456f688d742ad3c086a9372c4e35251094904210",
    "citation_count": 20
  },
  "https://jmlr.org/papers/v18/16-657.html": {
    "title": "Maximum Likelihood Estimation for Mixtures of Spherical Gaussians is NP-hard",
    "abstract": "This paper presents NP-hardness and hardness of approximation results for maximum likelihood estimation of mixtures of spherical Gaussians",
    "volume": "main",
    "checked": true,
    "id": "0ce6a7f1b086bb06b5ae3b287db459721499d03e",
    "citation_count": 7
  },
  "https://jmlr.org/papers/v18/16-532.html": {
    "title": "The DFS Fused Lasso: Linear-Time Denoising over General Graphs",
    "abstract": "The fused lasso, also known as (anisotropic) total variation denoising, is widely used for piecewise constant signal estimation with respect to a given undirected graph. The fused lasso estimate is highly nontrivial to compute when the underlying graph is large and has an arbitrary structure. But for a special graph structure, namely, the chain graph, the fused lasso---or simply, 1d fused lasso---can be computed in linear time. In this paper, we revisit a result recently established in the online classification literature (Herbster et al., 2009; Cesa-Bianchi et al., 2013) and show that it has important implications for signal denoising on graphs. The result can be translated to our setting as follows. Given a general graph, if we run the standard depth-first search (DFS) traversal algorithm, then the total variation of any signal over the chain graph induced by DFS is no more than twice its total variation over the original graph. This result leads to several interesting theoretical and computational conclusions. Letting $m$ and $n$ denote the number of edges and nodes, respectively, of the graph in consideration, it implies that for an underlying signal with total variation $t$ over the graph, the fused lasso (properly tuned) achieves a mean squared error rate of $t^{2/3} n^{-2/3}$. Moreover, precisely the same mean squared error rate is achieved by running the 1d fused lasso on the DFS-induced chain graph. Importantly, the latter estimator is simple and computationally cheap, requiring $O(m)$ operations to construct the DFS-induced chain and $O(n)$ operations to compute the 1d fused lasso solution over this chain. Further, for trees that have bounded maximum degree, the error rate of $t^{2/3} n^{-2/3}$ cannot be improved, in the sense that it is the minimax rate for signals that have total variation $t$ over the tree. Finally, several related results also hold---for example, the analogous result holds for a roughness measure defined by the $\\ell_0$ norm of differences across edges in place of the total variation metric",
    "volume": "main",
    "checked": true,
    "id": "4443a96f79cfbd5bc4170f84db4fd6f7df6edaab",
    "citation_count": 54
  },
  "https://jmlr.org/papers/v18/16-480.html": {
    "title": "Community Detection and Stochastic Block Models: Recent Developments",
    "abstract": "The stochastic block model (SBM) is a random graph model with planted clusters. It is widely employed as a canonical model to study clustering and community detection, and provides generally a fertile ground to study the statistical and computational tradeoffs that arise in network and data sciences. This note surveys the recent developments that establish the fundamental limits for community detection in the SBM, both with respect to information-theoretic and computational thresholds, and for various recovery requirements such as exact, partial and weak recovery (a.k.a., detection). The main results discussed are the phase transitions for exact recovery at the Chernoff-Hellinger threshold, the phase transition for weak recovery at the Kesten- Stigum threshold, the optimal distortion-SNR tradeoff for partial recovery, the learning of the SBM parameters and the gap between information-theoretic and computational thresholds. The note also covers some of the algorithms developed in the quest of achieving the limits, in particular two-round algorithms via graph-splitting, semi-definite programming, linearized belief propagation, classical and nonbacktracking spectral methods. A few open problems are also discussed",
    "volume": "main",
    "checked": true,
    "id": "41f6bbf03e9c2bf1bb2e3cea442a0f08b6d807d0",
    "citation_count": 890
  },
  "https://jmlr.org/papers/v18/16-587.html": {
    "title": "On $b$-bit Min-wise Hashing for Large-scale Regression and Classification with Sparse Data",
    "abstract": "Large-scale regression problems where both the number of variables, $p$, and the number of observations, $n$, may be large and in the order of millions or more, are becoming increasingly more common. Typically the data are sparse: only a fraction of a percent of the entries in the design matrix are non-zero. Nevertheless, often the only computationally feasible approach is to perform dimension reduction to obtain a new design matrix with far fewer columns and then work with this compressed data. $b$-bit min-wise hashing (Li and KÃÂ¶nig, 2011; Li et al., 2011) is a promising dimension reduction scheme for sparse matrices which produces a set of random features such that regression on the resulting design matrix approximates a kernel regression with the resemblance kernel. In this work, we derive bounds on the prediction error of such regressions. For both linear and logistic models, we show that the average prediction error vanishes asymptotically as long as $q \\|\\boldsymbol{\\beta}^*\\|_2^2 /n \\rightarrow 0$, where $q$ is the average number of non-zero entries in each row of the design matrix and $\\boldsymbol{\\beta}^*$ is the coefficient of the linear predictor. We also show that ordinary least squares or ridge regression applied to the reduced data can in fact allow us fit more flexible models. We obtain non-asymptotic prediction error bounds for interaction models and for models where an unknown row normalisation must be applied in order for the signal to be linear in the predictors",
    "volume": "main",
    "checked": true,
    "id": "7f3dfc6c1d6049714c4934369d7aecd06eea78ef",
    "citation_count": 5
  },
  "https://jmlr.org/papers/v18/17-078.html": {
    "title": "Efficient Learning with a Family of Nonconvex Regularizers by Redistributing Nonconvexity",
    "abstract": "The use of convex regularizers allows for easy optimization, though they often produce biased estimation and inferior prediction performance. Recently, nonconvex regularizers have attracted a lot of attention and outperformed convex ones. However, the resultant optimization problem is much harder. In this paper, a popular subclass of $\\ell_1$-based nonconvex sparsity-inducing and low-rank regularizers is considered. This includes nonconvex variants of lasso, sparse group lasso, tree- structured lasso, nuclear norm and total variation regularizers. We propose to move the nonconvexity from the regularizer to the loss. The nonconvex regularizer is then transformed to a familiar convex one, while the resultant loss function can still be guaranteed to be smooth. Learning with the convexified regularizer can be performed by existing efficient algorithms originally designed for convex regularizers (such as the proximal algorithm, Frank-Wolfe algorithm, alternating direction method of multipliers and stochastic gradient descent). This is further extended to consider cases where the convexified regularizer does not have a closed-form proximal step, and when the loss function is nonconvex nonsmooth. Extensive experiments on a variety of machine learning application scenarios show that optimizing the transformed problem is much faster than running the state-of-the-art on the original problem",
    "volume": "main",
    "checked": true,
    "id": "9b71b0a5b177ded248381dba4430944886ebb3e3",
    "citation_count": 46
  },
  "https://jmlr.org/papers/v18/17-380.html": {
    "title": "Mode-Seeking Clustering and Density Ridge Estimation via Direct Estimation of Density-Derivative-Ratios",
    "abstract": "Modes and ridges of the probability density function behind observed data are useful geometric features. Mode-seeking clustering assigns cluster labels by associating data samples with the nearest modes, and estimation of density ridges enables us to find lower-dimensional structures hidden in data. A key technical challenge both in mode-seeking clustering and density ridge estimation is accurate estimation of the ratios of the first- and second-order density derivatives to the density. A naive approach takes a three-step approach of first estimating the data density, then computing its derivatives, and finally taking their ratios. However, this three-step approach can be unreliable because a good density estimator does not necessarily mean a good density derivative estimator, and division by the estimated density could significantly magnify the estimation error. To cope with these problems, we propose a novel estimator for the density-derivative-ratios. The proposed estimator does not involve density estimation, but rather directly approximates the ratios of density derivatives of any order. Moreover, we establish a convergence rate of the proposed estimator. Based on the proposed estimator, novel methods both for mode-seeking clustering and density ridge estimation are developed, and the respective convergence rates to the mode and ridge of the underlying density are also established. Finally, we experimentally demonstrate that the developed methods significantly outperform existing methods, particularly for relatively high-dimensional data",
    "volume": "main",
    "checked": true,
    "id": "738da70f6b1fb3657c7a0ae6e9742fe035be3a9c",
    "citation_count": 5
  },
  "https://jmlr.org/papers/v18/17-269.html": {
    "title": "To Tune or Not to Tune the Number of Trees in Random Forest",
    "abstract": "The number of trees $T$ in the random forest (RF) algorithm for supervised learning has to be set by the user. It is unclear whether $T$ should simply be set to the largest computationally manageable value or whether a smaller $T$ may be sufficient or in some cases even better. While the principle underlying bagging is that more trees are better, in practice the classification error rate sometimes reaches a minimum before increasing again for increasing number of trees. The goal of this paper is four-fold: (i) providing theoretical results showing that the expected error rate may be a non-monotonous function of the number of trees and explaining under which circumstances this happens; (ii) providing theoretical results showing that such non-monotonous patterns cannot be observed for other performance measures such as the Brier score and the logarithmic loss (for classification) and the mean squared error (for regression); (iii) illustrating the extent of the problem through an application to a large number (n = 306) of datasets from the public database OpenML; (iv) finally arguing in favor of setting $T$ to a computationally feasible large number as long as classical error measures based on average loss are considered",
    "volume": "main",
    "checked": false,
    "id": "d25de110afec3131a87c2342af7bba50e5e2cc17",
    "citation_count": 259
  },
  "https://jmlr.org/papers/v18/17-343.html": {
    "title": "Divide-and-Conquer for Debiased $l_1$-norm Support Vector Machine in Ultra-high Dimensions",
    "abstract": "$1$-norm support vector machine (SVM) generally has competitive performance compared to standard $2$-norm support vector machine in classification problems, with the advantage of automatically selecting relevant features. We propose a divide-and-conquer approach in the large sample size and high-dimensional setting by splitting the data set across multiple machines, and then averaging the debiased estimators. Extension of existing theoretical studies to SVM is challenging in estimation of the inverse Hessian matrix that requires approximating the Dirac delta function via smoothing. We show that under appropriate conditions the aggregated estimator can obtain the same convergence rate as the central estimator utilizing all observations",
    "volume": "main",
    "checked": false,
    "id": "be50c4fe10616af1b39645eef8d7010c72957cea",
    "citation_count": 18
  },
  "https://jmlr.org/papers/v18/17-364.html": {
    "title": "Beyond the Hazard Rate: More Perturbation Algorithms for Adversarial Multi-armed Bandits",
    "abstract": "Recent work on follow the perturbed leader (FTPL) algorithms for the adversarial multi-armed bandit problem has highlighted the role of the hazard rate of the distribution generating the perturbations. Assuming that the hazard rate is bounded, it is possible to provide regret analyses for a variety of FTPL algorithms for the multi-armed bandit problem. This paper pushes the inquiry into regret bounds for FTPL algorithms beyond the bounded hazard rate condition. There are good reasons to do so: natural distributions such as the uniform and Gaussian violate the condition. We give regret bounds for both bounded support and unbounded support distributions without assuming the hazard rate condition. We also disprove a conjecture that the Gaussian distribution cannot lead to a low-regret algorithm. In fact, it turns out that it leads to near optimal regret, up to logarithmic factors. A key ingredient in our approach is the introduction of a new notion called the generalized hazard rate",
    "volume": "main",
    "checked": true,
    "id": "bc56c74bc3c6111745dcf7542450393be2421c2e",
    "citation_count": 1
  },
  "https://jmlr.org/papers/v18/17-157.html": {
    "title": "On Faster Convergence of Cyclic Block Coordinate Descent-type Methods for Strongly Convex Minimization",
    "abstract": "The cyclic block coordinate descent-type (CBCD-type) methods, which perform iterative updates for a few coordinates (a block) simultaneously throughout the procedure, have shown remarkable computational performance for solving strongly convex minimization problems. Typical applications include many popular statistical machine learning methods such as elastic-net regression, ridge penalized logistic regression, and sparse additive regression. Existing optimization literature has shown that for strongly convex minimization, the CBCD-type methods attain iteration complexity of $\\mathcal{O}(p\\log(1/\\epsilon))$, where $\\epsilon$ is a pre-specified accuracy of the objective value, and $p$ is the number of blocks. However, such iteration complexity explicitly depends on $p$, and therefore is at least $p$ times worse than the complexity $\\mathcal{O}(\\log(1/\\epsilon))$ of gradient descent (GD) methods. To bridge this theoretical gap, we propose an improved convergence analysis for the CBCD-type methods. In particular, we first show that for a family of quadratic minimization problems, the iteration complexity $\\mathcal{O}(\\log^2(p)\\cdot\\log(1/\\epsilon))$ of the CBCD-type methods matches that of the GD methods in term of dependency on $p$, up to a $\\log^2 p$ factor. Thus our complexity bounds are sharper than the existing bounds by at least a factor of $p/\\log^2(p)$. We also provide a lower bound to confirm that our improved complexity bounds are tight (up to a $\\log^2 (p)$ factor), under the assumption that the largest and smallest eigenvalues of the Hessian matrix do not scale with $p$. Finally, we generalize our analysis to other strongly convex minimization problems beyond quadratic ones",
    "volume": "main",
    "checked": true,
    "id": "486956c45fa19b2d0d794501ec296f607ed1eeb5",
    "citation_count": 25
  },
  "https://jmlr.org/papers/v18/16-558.html": {
    "title": "Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization",
    "abstract": "Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation and early-stopping. We formulate hyperparameter optimization as a pure-exploration non-stochastic infinite-armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce a novel algorithm, ÃÂ¸uralg , for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare ÃÂ¸uralg with popular Bayesian optimization methods on a suite of hyperparameter optimization problems. We observe that ÃÂ¸uralg can provide over an order-of-magnitude speedup over our competitor set on a variety of deep-learning and kernel-based learning problems",
    "volume": "main",
    "checked": true,
    "id": "892f9a2f69241feec647856cd26bed37e04fd747",
    "citation_count": 1604
  },
  "https://jmlr.org/papers/v18/17-297.html": {
    "title": "Submatrix localization via message passing",
    "abstract": "The principal submatrix localization problem deals with recovering a $K\\times K$ principal submatrix of elevated mean $\\mu$ in a large $n\\times n$ symmetric matrix subject to additive standard Gaussian noise, or more generally, mean zero, variance one, subgaussian noise. This problem serves as a prototypical example for community detection, in which the community corresponds to the support of the submatrix. The main result of this paper is that in the regime $\\Omega(\\sqrt{n}) \\leq K \\leq o(n)$, the support of the submatrix can be weakly recovered (with $o(K)$ misclassification errors on average) by an optimized message passing algorithm if $\\lambda = \\mu^2K^2/n$, the signal-to-noise ratio, exceeds $1/e$. This extends a result by Deshpande and Montanari previously obtained for $K=\\Theta(\\sqrt{n})$ and $\\mu=\\Theta(1).$ In addition, the algorithm can be combined with a voting procedure to achieve the information-theoretic limit of exact recovery with sharp constants for all $K \\geq \\frac{n}{\\log n} (\\frac{1}{8e} + o(1))$. The total running time of the algorithm is $O(n^2\\log n)$. Another version of the submatrix localization problem, known as noisy biclustering, aims to recover a $K_1\\times K_2$ submatrix of elevated mean $\\mu$ in a large $n_1\\times n_2$ Gaussian matrix. The optimized message passing algorithm and its analysis are adapted to the bicluster problem assuming $\\Omega(\\sqrt{n_i}) \\leq K_i \\leq o(n_i)$ and $K_1\\asymp K_2.$ A sharp information-theoretic condition for the weak recovery of both clusters is also identified",
    "volume": "main",
    "checked": true,
    "id": "168cf8023d7a934d2c264a755de668235373e16f",
    "citation_count": 22
  },
  "https://jmlr.org/papers/v18/16-456.html": {
    "title": "Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations",
    "abstract": "We introduce a method to train Quantized Neural Networks (QNNs) --- neural networks with extremely low precision (e.g., 1-bit) weights and activations, at run-time. At train-time the quantized weights and activations are used for computing the parameter gradients. During the forward pass, QNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations. As a result, power consumption is expected to be drastically reduced. We trained QNNs over the MNIST, CIFAR-10, SVHN and ImageNet datasets. The resulting QNNs achieve prediction accuracy comparable to their 32-bit counterparts. For example, our quantized version of AlexNet with 1-bit weights and 2-bit activations achieves $51\\%$ top-1 accuracy. Moreover, we quantize the parameter gradients to 6-bits as well which enables gradients computation using only bit-wise operation. Quantized recurrent neural networks were tested over the Penn Treebank dataset, and achieved comparable accuracy as their 32-bit counterparts using only 4-bits. Last but not least, we programmed a binary matrix multiplication GPU kernel with which it is possible to run our MNIST QNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The QNN code is available online",
    "volume": "main",
    "checked": true,
    "id": "d2e4147eecae6f914e9e1e9aece8fdd2eaed809f",
    "citation_count": 1552
  },
  "https://jmlr.org/papers/v18/17-377.html": {
    "title": "Significance-based community detection in weighted networks",
    "abstract": "Community detection is the process of grouping strongly connected nodes in a network. Many community detection methods for un-weighted networks have a theoretical basis in a null model. Communities discovered by these methods therefore have interpretations in terms of statistical significance. In this paper, we introduce a null for weighted networks called the continuous configuration model. First, we propose a community extraction algorithm for weighted networks which incorporates iterative hypothesis testing under the null. We prove a central limit theorem for edge-weight sums and asymptotic consistency of the algorithm under a weighted stochastic block model. We then incorporate the algorithm in a community detection method called CCME. To benchmark the method, we provide a simulation framework involving the null to plant Ã¢ÂÂbackground\" nodes in weighted networks with communities. We show that the empirical performance of CCME on these simulations is competitive with existing methods, particularly when overlapping communities and background nodes are present. To further validate the method, we present two real-world networks with potential background nodes and analyze them with CCME, yielding results that reveal macro- features of the corresponding systems",
    "volume": "main",
    "checked": true,
    "id": "522ff723cfebc7c8153cb876f39d03be2746e43a",
    "citation_count": 28
  },
  "https://jmlr.org/papers/v18/17-317.html": {
    "title": "Kernel Method for Persistence Diagrams via Kernel Embedding and Weight Factor",
    "abstract": "Topological data analysis (TDA) is an emerging mathematical concept for characterizing shapes in complicated data. In TDA, persistence diagrams are widely recognized as a useful descriptor of data, distinguishing robust and noisy topological properties. This paper introduces a kernel method for persistence diagrams to develop a statistical framework in TDA. The proposed kernel is stable under perturbation of data, enables one to explicitly control the effect of persistence by a weight function, and allows an efficient and accurate approximate computation. The method is applied into practical data on granular systems, oxide glasses and proteins, showing advantages of our method compared to other relevant methods for persistence diagrams",
    "volume": "main",
    "checked": true,
    "id": "87f3028a5cda86db2664b046f7aa587dbfda60cb",
    "citation_count": 70
  },
  "https://jmlr.org/papers/v18/17-228.html": {
    "title": "Pycobra: A Python Toolbox for Ensemble Learning and Visualisation",
    "abstract": "We introduce pycobra, a Python library devoted to ensemble learning (regression and classification) and visualisation. Its main assets are the implementation of several ensemble learning algorithms, a flexible and generic interface to compare and blend any existing machine learning algorithm available in Python libraries (as long as a predict method is given), and visualisation tools such as Voronoi tessellations. pycobra is fully scikit-learn compatible and is released under the MIT open-source license. pycobra can be downloaded from the Python Package Index (PyPi) and Machine Learning Open Source Software (MLOSS). The current version (along with Jupyter notebooks, extensive documentation, and continuous integration tests) is available at https://github.com/bhargavvader/pycobra and official documentation website is https://modal.lille.inria.fr/pycobra",
    "volume": "MLOSS",
    "checked": true,
    "id": "5deb9b967302c2541fe9322773615aa813cd8861",
    "citation_count": 17
  },
  "https://jmlr.org/papers/v18/16-087.html": {
    "title": "KELP: a Kernel-based Learning Platform",
    "abstract": "KELP is a Java framework that enables fast and easy implementation of kernel functions over discrete data, such as strings, trees or graphs and their combination with standard vectorial kernels. Additionally, it provides several kernel- based algorithms, e.g., online and batch kernel machines for classification, regression and clustering, and a Java environment for easy implementation of new algorithms. KELP is a versatile toolkit, very appealing both to experts and practitioners of machine learning and Java language programming, who can find extensive documentation, tutorials and examples of increasing complexity on the accompanying website. Interestingly, KELP can be also used without any knowledge of Java programming through command line tools and JSON/XML interfaces enabling the declaration and instantiation of articulated learning models using simple templates. Finally, the extensive use of modularity and interfaces in KELP enables developers to easily extend it with their own kernels and algorithms",
    "volume": "MLOSS",
    "checked": true,
    "id": "023baa8fdcc3c5269226d328695907ad7ad35f12",
    "citation_count": 35
  },
  "https://jmlr.org/papers/v18/17-284.html": {
    "title": "Uncovering Causality from Multivariate Hawkes Integrated Cumulants",
    "abstract": "We design a new nonparametric method that allows one to estimate the matrix of integrated kernels of a multivariate Hawkes process. This matrix not only encodes the mutual influences of each node of the process, but also disentangles the causality relationships between them. Our approach is the first that leads to an estimation of this matrix without any parametric modeling and estimation of the kernels themselves. As a consequence, it can give an estimation of causality relationships between nodes (or users), based on their activity timestamps (on a social network for instance), without knowing or estimating the shape of the activities lifetime. For that purpose, we introduce a moment matching method that fits the second-order and the third-order integrated cumulants of the process. A theoretical analysis allows us to prove that this new estimation technique is consistent. Moreover, we show, on numerical experiments, that our approach is indeed very robust with respect to the shape of the kernels and gives appealing results on the MemeTracker database and on financial order book data",
    "volume": "main",
    "checked": true,
    "id": "46325ef55d59f5b28fec01ffacdeca609158d0d1",
    "citation_count": 71
  },
  "https://jmlr.org/papers/v18/17-234.html": {
    "title": "Making Better Use of the Crowd: How Crowdsourcing Can Advance Machine Learning Research",
    "abstract": "This survey provides a comprehensive overview of the landscape of crowdsourcing research, targeted at the machine learning community. We begin with an overview of the ways in which crowdsourcing can be used to advance machine learning research, focusing on four application areas: 1) data generation, 2) evaluation and debugging of models, 3) hybrid intelligence systems that leverage the complementary strengths of humans and machines to expand the capabilities of AI, and 4) crowdsourced behavioral experiments that improve our understanding of how humans interact with machine learning systems and technology more broadly. We next review the extensive literature on the behavior of crowdworkers themselves. This research, which explores the prevalence of dishonesty among crowdworkers, how workers respond to both monetary incentives and intrinsic forms of motivation, and how crowdworkers interact with each other, has immediate implications that we distill into best practices that researchers should follow when using crowdsourcing in their own research. We conclude with a discussion of additional tips and best practices that are crucial to the success of any project that uses crowdsourcing, but rarely mentioned in the literature",
    "volume": "main",
    "checked": true,
    "id": "7e6ce5bd878f24839ba38fe8fb91f30aab3cd863",
    "citation_count": 124
  },
  "https://jmlr.org/papers/v18/17-563.html": {
    "title": "Enhancing Identification of Causal Effects by Pruning",
    "abstract": "Causal models communicate our assumptions about causes and effects in real-world phenomena. Often the interest lies in the identification of the effect of an action which means deriving an expression from the observed probability distribution for the interventional distribution resulting from the action. In many cases an identifiability algorithm may return a complicated expression that contains variables that are in fact unnecessary. In practice this can lead to additional computational burden and increased bias or inefficiency of estimates when dealing with measurement error or missing data. We present graphical criteria to detect variables which are redundant in identifying causal effects. We also provide an improved version of a well-known identifiability algorithm that implements these criteria",
    "volume": "main",
    "checked": true,
    "id": "463a035b6437ae0d572355b40e6edc2e4727008a",
    "citation_count": 10
  },
  "https://jmlr.org/papers/v18/16-499.html": {
    "title": "Active Nearest-Neighbor Learning in Metric Spaces",
    "abstract": "We propose a pool-based non-parametric active learning algorithm for general metric spaces, called MArgin Regularized Metric Active Nearest Neighbor (MARMANN), which outputs a nearest-neighbor classifier. We give prediction error guarantees that depend on the noisy-margin properties of the input sample, and are competitive with those obtained by previously proposed passive learners. We prove that the label complexity of MARMANN is significantly lower than that of any passive learner with similar error guarantees. MARMANN is based on a generalized sample compression scheme, and a new label-efficient active model-selection procedure",
    "volume": "main",
    "checked": true,
    "id": "f4902d60dd70aaf41862615a1ed97a8a36be56a2",
    "citation_count": 34
  },
  "https://jmlr.org/papers/v18/17-073.html": {
    "title": "From Predictive Methods to Missing Data Imputation: An Optimization Approach",
    "abstract": "Missing data is a common problem in real-world settings and for this reason has attracted significant attention in the statistical literature. We propose a flexible framework based on formal optimization to impute missing data with mixed continuous and categorical variables. This framework can readily incorporate various predictive models including $K$-nearest neighbors, support vector machines, and decision tree based methods, and can be adapted for multiple imputation. We derive fast first-order methods that obtain high quality solutions in seconds following a general imputation algorithm opt.impute presented in this paper. We demonstrate that our proposed method improves out-of-sample accuracy in large-scale computational experiments across a sample of 84 data sets taken from the UCI Machine Learning Repository. In all scenarios of missing at random mechanisms and various missing percentages, opt.impute produces the best overall imputation in most data sets benchmarked against five other methods: mean impute, $K$-nearest neighbors, iterative knn, Bayesian PCA, and predictive-mean matching, with an average reduction in mean absolute error of 8.3$\\%$ against the best cross-validated benchmark method. Moreover, opt.impute leads to improved out-of-sample performance of learning algorithms trained using the imputed data, demonstrated by computational experiments on 10 downstream tasks. For models trained using opt.impute single imputations with 50$\\%$ data missing, the average out-of- sample $R^2$ is 0.339 in the regression tasks and the average out-of-sample accuracy is 86.1$\\%$ in the classification tasks, compared to 0.315 and 84.4$\\%$ for the best cross-validated benchmark method. In the multiple imputation setting, downstream models trained using opt.impute obtain a statistically significant improvement over models trained using multivariate imputation by chained equations (mice) in 8/10 missing data scenarios considered",
    "volume": "main",
    "checked": true,
    "id": "b6f1be7f52d9f3dfaf5bb7ad726958e5d0ac92e9",
    "citation_count": 187
  },
  "https://jmlr.org/papers/v18/17-178.html": {
    "title": "Saturating Splines and Feature Selection",
    "abstract": "We extend the adaptive regression spline model by incorporating saturation, the natural requirement that a function extend as a constant outside a certain range. We fit saturating splines to data via a convex optimization problem over a space of measures, which we solve using an efficient algorithm based on the conditional gradient method. Unlike many existing approaches, our algorithm solves the original infinite- dimensional (for splines of degree at least two) optimization problem without pre-specified knot locations. We then adapt our algorithm to fit generalized additive models with saturating splines as coordinate functions and show that the saturation requirement allows our model to simultaneously perform feature selection and nonlinear function fitting. Finally, we briefly sketch how the method can be extended to higher order splines and to different requirements on the extension outside the data range",
    "volume": "main",
    "checked": true,
    "id": "f92ec07d952263fd5b105ff6d4c18c3d57e455a4",
    "citation_count": 14
  },
  "https://jmlr.org/papers/v18/17-347.html": {
    "title": "Nonasymptotic convergence of stochastic proximal point methods for constrained convex optimization",
    "abstract": "A popular approach for solving stochastic optimization problems is the stochastic gradient descent (SGD) method. Although the SGD iteration is computationally cheap and its practical performance may be satisfactory under certain circumstances, there is recent evidence of its convergence difficulties and instability for unappropriate choice of parameters. To avoid some of the drawbacks of SGD, stochastic proximal point (SPP) algorithms have been recently considered. We introduce a new variant of the SPP method for solving stochastic convex problems subject to (in)finite intersection of constraints satisfying a linear regularity condition. For the newly introduced SPP scheme we prove new nonasymptotic convergence results. In particular, for convex Lipschitz continuous objective functions, we prove nonasymptotic convergence rates in terms of the expected value function gap of order $\\mathcal{O}\\left(\\frac{1}{k^{1/2}}\\right)$, where $k$ is the iteration counter. We also derive better nonasymptotic convergence rates in terms of expected quadratic distance from the iterates to the optimal solution for smooth strongly convex objective functions, which in the best case is of order $\\mathcal{O}\\left(\\frac{1}{k}\\right)$. Since these convergence rates can be attained by our SPP algorithm only under some natural restrictions on the stepsize, we also introduce a restarting variant of SPP that overcomes these difficulties and derive the corresponding nonasymptotic convergence rates. Numerical evidence supports the effectiveness of our methods in real problems",
    "volume": "main",
    "checked": true,
    "id": "5b9e83af8c74f834420e68406e3a378062a0c57f",
    "citation_count": 74
  },
  "https://jmlr.org/papers/v18/16-206.html": {
    "title": "Simple, Robust and Optimal Ranking from Pairwise Comparisons",
    "abstract": "We consider data in the form of pairwise comparisons of $n$ items, with the goal of identifying the top $k$ items for some value of $k < n$, or alternatively, recovering a ranking of all the items. We analyze the Borda counting algorithm that ranks the items in order of the number of pairwise comparisons won, and show it has three attractive features: (a) it is an optimal method achieving the information-theoretic limits up to constant factors; (b) it is robust in that its optimality holds without imposing conditions on the underlying matrix of pairwise-comparison probabilities, in contrast to some prior work that applies only to the BTL parametric model; and (c) its computational efficiency leads to speed-ups of several orders of magnitude. We address the problem of exact recovery, and for the top-$k$ recovery problem we also extend our results to obtain sharp guarantees for approximate recovery under the Hamming distortion metric, and more generally, to any arbitrary error requirement that satisfies a simple and natural monotonicity condition. In doing so, we introduce a general framework that allows us to treat a variety of problems in the literature in an unified manner",
    "volume": "main",
    "checked": true,
    "id": "f79361dda56ee755fc56ab83cf0d9f12d42b2d5e",
    "citation_count": 141
  },
  "https://jmlr.org/papers/v18/16-549.html": {
    "title": "Surprising properties of dropout in deep networks",
    "abstract": "We analyze dropout in deep networks with rectified linear units and the quadratic loss. Our results expose surprising differences between the behavior of dropout and more traditional regularizers like weight decay. For example, on some simple data sets dropout training produces negative weights even though the output is the sum of the inputs. This provides a counterpoint to the suggestion that dropout discourages co-adaptation of weights. We also show that the dropout penalty can grow exponentially in the depth of the network while the weight-decay penalty remains essentially linear, and that dropout is insensitive to various re-scalings of the input features, outputs, and network weights. This last insensitivity implies that there are no isolated local minima of the dropout training criterion. Our work uncovers new properties of dropout, extends our understanding of why dropout succeeds, and lays the foundation for further progress",
    "volume": "main",
    "checked": true,
    "id": "73b36c2a58ea9e2c156b08f09721414e573d22ab",
    "citation_count": 33
  },
  "https://jmlr.org/papers/v18/16-256.html": {
    "title": "Exact Learning of Lightweight Description Logic Ontologies",
    "abstract": "We study the problem of learning description logic (DL) ontologies in Angluin et al.'s framework of exact learning via queries. We admit membership queries (Ã¢ÂÂis a given subsumption entailed by the target ontology?Ã¢ÂÂ) and equivalence queries (Ã¢ÂÂis a given ontology equivalent to the target ontology?Ã¢ÂÂ). We present three main results: (1) ontologies formulated in (two relevant versions of) the description logic DL-Lite can be learned with polynomially many queries of polynomial size; (2) this is not the case for ontologies formulated in the description logic $\\mathcal{E}\\mathcal{L}$, even when only acyclic ontologies are admitted; and (3) ontologies formulated in a fragment of $\\mathcal{E}\\mathcal{L}$ related to the web ontology language OWL 2 RL can be learned in polynomial time. We also show that neither membership nor equivalence queries alone are sufficient in cases (1) and (3)",
    "volume": "main",
    "checked": true,
    "id": "293f3e37edf1174eec171c9240fccea3f45c405f",
    "citation_count": 39
  },
  "https://jmlr.org/papers/v18/17-159.html": {
    "title": "Sparse Concordance-assisted Learning for Optimal Treatment Decision",
    "abstract": "To find optimal decision rule, Fan et al. (2016) proposed an innovative concordance-assisted learning algorithm which is based on maximum rank correlation estimator. It makes better use of the available information through pairwise comparison. However the objective function is discontinuous and computationally hard to optimize. In this paper, we consider a convex surrogate loss function to solve this problem. In addition, our algorithm ensures sparsity of decision rule and renders easy interpretation. We derive the $L_2$ error bound of the estimated coefficients under ultra-high dimension. Simulation results of various settings and application to STAR*D both illustrate that the proposed method can still estimate optimal treatment regime successfully when the number of covariates is large",
    "volume": "main",
    "checked": true,
    "id": "94e2175d11108bca97212fcbebab3cf06f88942a",
    "citation_count": 26
  },
  "https://jmlr.org/papers/v18/17-145.html": {
    "title": "Post-Regularization Inference for Time-Varying Nonparanormal Graphical Models",
    "abstract": "We propose a novel class of time-varying nonparanormal graphical models, which allows us to model high dimensional heavy-tailed systems and the evolution of their latent network structures. Under this model we develop statistical tests for presence of edges both locally at a fixed index value and globally over a range of values. The tests are developed for a high-dimensional regime, are robust to model selection mistakes and do not require commonly assumed minimum signal strength. The testing procedures are based on a high dimensional, debiasing-free moment estimator, which uses a novel kernel smoothed Kendall's tau correlation matrix as an input statistic. The estimator consistently estimates the latent inverse Pearson correlation matrix uniformly in both the index variable and kernel bandwidth. Its rate of convergence is shown to be minimax optimal. Our method is supported by thorough numerical simulations and an application to a neural imaging data set",
    "volume": "main",
    "checked": true,
    "id": "3756325b07829a4b8957c07a0fb4c1af989e6727",
    "citation_count": 27
  },
  "https://jmlr.org/papers/v18/17-409.html": {
    "title": "Permuted and Augmented Stick-Breaking Bayesian Multinomial Regression",
    "abstract": "To model categorical response variables given their covariates, we propose a permuted and augmented stick-breaking (paSB) construction that one-to-one maps the observed categories to randomly permuted latent sticks. This new construction transforms multinomial regression into regression analysis of stick-specific binary random variables that are mutually independent given their covariate-dependent stick success probabilities, which are parameterized by the regression coefficients of their corresponding categories. The paSB construction allows transforming an arbitrary cross-entropy-loss binary classifier into a Bayesian multinomial one. Specifically, we parameterize the negative logarithms of the stick failure probabilities with a family of covariate-dependent softplus functions to construct nonparametric Bayesian multinomial softplus regression, and transform Bayesian support vector machine (SVM) into Bayesian multinomial SVM. These Bayesian multinomial regression models are not only capable of providing probability estimates, quantifying uncertainty, increasing robustness, and producing nonlinear classification decision boundaries, but also amenable to posterior simulation. Example results demonstrate their attractive properties and performance",
    "volume": "main",
    "checked": true,
    "id": "cbf3f558f1edd43853604491f6a0f9121d8d7d01",
    "citation_count": 9
  },
  "https://jmlr.org/papers/v18/17-416.html": {
    "title": "Steering Social Activity: A Stochastic Optimal Control Point Of View",
    "abstract": "User engagement in online social networking depends critically on the level of social activity in the corresponding platform---the number of online actions, such as posts, shares or replies, taken by their users. Can we design data-driven algorithms to increase social activity? At a user level, such algorithms may increase activity by helping users decide when to take an action to be more likely to be noticed by their peers. At a network level, they may increase activity by incentivizing a few influential users to take more actions, which in turn will trigger additional actions by other users. In this paper, we model social activity using the framework of marked temporal point processes, derive an alternate representation of these processes using stochastic differential equations (SDEs) with jumps and, exploiting this alternate representation, develop two efficient online algorithms with provable guarantees to steer social activity both at a user and at a network level. In doing so, we establish a previously unexplored connection between optimal control of jump SDEs and doubly stochastic marked temporal point processes, which is of independent interest. Finally, we experiment both with synthetic and real data gathered from Twitter and show that our algorithms consistently steer social activity more effectively than the state of the art",
    "volume": "main",
    "checked": true,
    "id": "e3db80d03afe62bd0d174c33492f08a49d23bc4e",
    "citation_count": 30
  },
  "https://jmlr.org/papers/v18/16-483.html": {
    "title": "The Search Problem in Mixture Models",
    "abstract": "We consider the task of learning the parameters of a  single component of a mixture model, for the case when we are given side information about that component; we call this the Ã¢ÂÂsearch problem\" in mixture models. We would like to solve this with computational and sample complexity lower than solving the overall original problem, where one learns parameters of all components. Our main contributions are the development of a simple but general model for the notion of side information, and a corresponding simple matrix-based algorithm for solving the search problem in this general setting. We then specialize this model and algorithm to four common scenarios: Gaussian mixture models, LDA topic models, subspace clustering, and mixed linear regression. For each one of these we show that if (and only if) the side information is informative, we obtain parameter estimates with greater accuracy, and also improved computation complexity than existing moment based mixture model algorithms (e.g. tensor methods). We also illustrate several natural ways one can obtain such side information, for specific problem instances. Our experiments on real data sets (NY Times, Yelp, BSDS500) further demonstrate the practicality of our algorithms showing significant improvement in runtime and accuracy",
    "volume": "main",
    "checked": true,
    "id": "6af4785e872029b349be27245d29fb027827fa0c",
    "citation_count": 7
  },
  "https://jmlr.org/papers/v18/16-140.html": {
    "title": "An $\\ell_{\\infty}$ Eigenvector Perturbation Bound and Its Application",
    "abstract": "In statistics and machine learning, we are interested in the eigenvectors (or singular vectors) of certain matrices (e.g. covariance matrices, data matrices, etc). However, those matrices are usually perturbed by noises or statistical errors, either from random sampling or structural patterns. The Davis- Kahan $\\sin \\theta$ theorem is often used to bound the difference between the eigenvectors of a matrix $A$ and those of a perturbed matrix $\\widetilde{A} = A + E$, in terms of $\\ell_2$ norm. In this paper, we prove that when $A$ is a low-rank and incoherent matrix, the $\\ell_{\\infty}$ norm perturbation bound of singular vectors (or eigenvectors in the symmetric case) is smaller by a factor of $\\sqrt{d_1}$ or $\\sqrt{d_2}$ for left and right vectors, where $d_1$ and $d_2$ are the matrix dimensions. The power of this new perturbation result is shown in robust covariance estimation, particularly when random variables have heavy tails. There, we propose new robust covariance estimators and establish their asymptotic properties using the newly developed perturbation bound. Our theoretical results are verified through extensive numerical experiments",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://jmlr.org/papers/v18/16-299.html": {
    "title": "A Tight Bound of Hard Thresholding",
    "abstract": "This paper is concerned with the hard thresholding operator which sets all but the $k$ largest absolute elements of a vector to zero. We establish a tight bound to quantitatively characterize the deviation of the thresholded solution from a given signal. Our theoretical result is universal in the sense that it holds for all choices of parameters, and the underlying analysis depends only on fundamental arguments in mathematical optimization. We discuss the implications for two domains: Compressed Sensing. On account of the crucial estimate, we bridge the connection between the restricted isometry property (RIP) and the sparsity parameter for a vast volume of hard thresholding based algorithms, which renders an improvement on the RIP condition especially when the true sparsity is unknown. This suggests that in essence, many more kinds of sensing matrices or fewer measurements are admissible for the data acquisition procedure. Machine Learning. In terms of large-scale machine learning, a significant yet challenging problem is learning accurate sparse models in an efficient manner. In stark contrast to prior work that attempted the $\\ell_1$-relaxation for promoting sparsity, we present a novel stochastic algorithm which performs hard thresholding in each iteration, hence ensuring such parsimonious solutions. Equipped with the developed bound, we prove the {\\em global linear convergence} for a number of prevalent statistical models under mild assumptions, even though the problem turns out to be non-convex",
    "volume": "main",
    "checked": true,
    "id": "8f029eada934e433a4f2bd6720e18f7d0930bc0a",
    "citation_count": 79
  },
  "https://jmlr.org/papers/v18/16-486.html": {
    "title": "Estimation of Graphical Models through Structured Norm Minimization",
    "abstract": "Estimation of Markov Random Field and covariance models from high-dimensional data represents a canonical problem that has received a lot of attention in the literature. A key assumption, widely employed, is that of sparsity of the underlying model. In this paper, we study the problem of estimating such models exhibiting a more intricate structure comprising simultaneously of sparse, structured sparse and dense components. Such structures naturally arise in several scientific fields, including molecular biology, finance and political science. We introduce a general framework based on a novel structured norm that enables us to estimate such complex structures from high-dimensional data. The resulting optimization problem is convex and we introduce a linearized multi-block alternating direction method of multipliers (ADMM) algorithm to solve it efficiently. We illustrate the superior performance of the proposed framework on a number of synthetic data sets generated from both random and structured networks. Further, we apply the method to a number of real data sets and discuss the results",
    "volume": "main",
    "checked": true,
    "id": "e53ef762bd887def2750aec863cf88e1d75a9a73",
    "citation_count": 17
  },
  "https://jmlr.org/papers/v18/16-421.html": {
    "title": "Sparse Exchangeable Graphs and Their Limits via Graphon Processes",
    "abstract": "In a recent paper, Caron and Fox suggest a probabilistic model for sparse graphs which are exchangeable when associating each vertex with a time parameter in $\\mathbb{R}_+$. Here we show that by generalizing the classical definition of graphons as functions over probability spaces to functions over $\\sigma$-finite measure spaces, we can model a large family of exchangeable graphs, including the Caron-Fox graphs and the traditional exchangeable dense graphs as special cases. Explicitly, modelling the underlying space of features by a $\\sigma$-finite measure space $(S,\\mathcal{S},\\mu)$ and the connection probabilities by an integrable function $W\\colon S\\times S\\to [0,1]$, we construct a random family $(G_t)_{t\\geq 0}$ of growing graphs such that the vertices of $G_t$ are given by a Poisson point process on $S$ with intensity $t\\mu$, with two points $x,y$ of the point process connected with probability $W(x,y)$. We call such a random family a graphon process. We prove that a graphon process has convergent subgraph frequencies (with possibly infinite limits) and that, in the natural extension of the cut metric to our setting, the sequence converges to the generating graphon. We also show that the underlying graphon is identifiable only as an equivalence class over graphons with cut distance zero. More generally, we study metric convergence for arbitrary (not necessarily random) sequences of graphs, and show that a sequence of graphs has a convergent subsequence if and only if it has a subsequence satisfying a property we call uniform regularity of tails. Finally, we prove that every graphon is equivalent to a graphon on $\\mathbb{R}_+$ equipped with Lebesgue measure",
    "volume": "main",
    "checked": true,
    "id": "878afff9b207bba6b46968af5e65aa92c19ed083",
    "citation_count": 97
  },
  "https://jmlr.org/papers/v18/17-044.html": {
    "title": "Weighted SGD for $\\ell_p$ Regression with Randomized Preconditioning",
    "abstract": "In recent years, stochastic gradient descent (SGD) methods and randomized linear algebra (RLA) algorithms have been applied to many large-scale problems in machine learning and data analysis. SGD methods are easy to implement and applicable to a wide range of convex optimization problems. In contrast, RLA algorithms provide much stronger performance guarantees but are applicable to a narrower class of problems. We aim to bridge the gap between these two methods in solving constrained overdetermined linear regression problems---e.g., $\\ell_2$ and $\\ell_1$ regression problems",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://jmlr.org/papers/v18/17-748.html": {
    "title": "Catalyst Acceleration for First-order Convex Optimization: from Theory to Practice",
    "abstract": "We introduce a generic scheme for accelerating gradient-based optimization methods in the sense of Nesterov. The approach, called Catalyst, builds upon the inexact accelerated proximal point algorithm for minimizing a convex objective function, and consists of approximately solving a sequence of well-chosen auxiliary problems, leading to faster convergence. One of the keys to achieve acceleration in theory and in practice is to solve these sub-problems with appropriate accuracy by using the right stopping criterion and the right warm-start strategy. We give practical guidelines to use Catalyst and present a comprehensive analysis of its global complexity. We show that Catalyst applies to a large class of algorithms, including gradient descent, block coordinate descent, incremental algorithms such as SAG, SAGA, SDCA, SVRG, MISO/Finito, and their proximal variants. For all of these methods, we establish faster rates using the Catalyst acceleration, for strongly convex and non-strongly convex objectives. We conclude with extensive experiments showing that acceleration is useful in practice, especially for ill-conditioned problems",
    "volume": "main",
    "checked": true,
    "id": "1d7f90010b79c9d0ba1245c33b0122183d7c53a6",
    "citation_count": 119
  },
  "https://jmlr.org/papers/v18/17-398.html": {
    "title": "Gaussian Lower Bound for the Information Bottleneck Limit",
    "abstract": "The Information Bottleneck (IB) is a conceptual method for extracting the most compact, yet informative, representation of a set of variables, with respect to the target. It generalizes the notion of minimal sufficient statistics from classical parametric statistics to a broader information-theoretic sense. The IB curve defines the optimal trade-off between representation complexity and its predictive power. Specifically, it is achieved by minimizing the level of mutual information (MI) between the representation and the original variables, subject to a minimal level of MI between the representation and the target. This problem is shown to be in general NP hard. One important exception is the multivariate Gaussian case, for which the Gaussian IB (GIB) is known to obtain an analytical closed form solution, similar to Canonical Correlation Analysis (CCA). In this work we introduce a Gaussian lower bound to the IB curve; we find an embedding of the data which maximizes its Ã¢ÂÂGaussian part\", on which we apply the GIB. This embedding provides an efficient (and practical) representation of any arbitrary data-set (in the IB sense), which in addition holds the favorable properties of a Gaussian distribution. Importantly, we show that the optimal Gaussian embedding is bounded from above by non-linear CCA. This allows a fundamental limit for our ability to Gaussianize arbitrary data- sets and solve complex problems by linear methods",
    "volume": "main",
    "checked": true,
    "id": "5932781fedbdd113b9aa6b0f696880bc0fadaee7",
    "citation_count": 12
  },
  "https://jmlr.org/papers/v18/17-381.html": {
    "title": "tick: a Python Library for Statistical Learning, with an emphasis on Hawkes Processes and Time-Dependent Models",
    "abstract": "This paper introduces tick, is a statistical learning library for Python 3, with a particular emphasis on time-dependent models, such as point processes, tools for generalized linear models and survival analysis. The core of the library provides model computational classes, solvers and proximal operators for regularization. It relies on a C++ implementation and state- of-the-art optimization algorithms to provide very fast computations in a single node multi-core setting. Source code and documentation can be downloaded from https://github.com/X-DataInitiative/tick",
    "volume": "MLOSS",
    "checked": true,
    "id": "93cf61011906392b8aba7efbdac83066d852d222",
    "citation_count": 91
  },
  "https://jmlr.org/papers/v18/17-632.html": {
    "title": "SGDLibrary: A MATLAB library for stochastic optimization algorithms",
    "abstract": "We consider the problem of finding the minimizer of a function $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ of the finite-sum form $\\min f(w) = 1/n\\sum_{i}^n f_i(w)$. This problem has been studied intensively in recent years in the field of machine learning (ML). One promising approach for large-scale data is to use a stochastic optimization algorithm to solve the problem. SGDLibrary is a readable, flexible and extensible pure-MATLAB library of a collection of stochastic optimization algorithms. The purpose of the library is to provide researchers and implementers a comprehensive evaluation environment for the use of these algorithms on various ML problems",
    "volume": "MLOSS",
    "checked": true,
    "id": "af8f59a3e20277b8bc7c66f6b1f95a8a126c5382",
    "citation_count": 36
  },
  "https://jmlr.org/papers/v18/16-340.html": {
    "title": "Reward Maximization Under Uncertainty: Leveraging Side-Observations on Networks",
    "abstract": "We study the stochastic multi-armed bandit (MAB) problem in the presence of side-observations across actions that occur as a result of an underlying network structure. In our model, a bipartite graph captures the relationship between actions and a common set of unknowns such that choosing an action reveals observations for the unknowns that it is connected to. This models a common scenario in online social networks where users respond to their friends' activity, thus providing side information about each other's preferences. Our contributions are as follows: 1) We derive an asymptotic lower bound (with respect to time) as a function of the bi-partite network structure on the regret of any uniformly good policy that achieves the maximum long-term average reward. 2) We propose two policies - a randomized policy; and a policy based on the well- known upper confidence bound (UCB) policies - both of which explore each action at a rate that is a function of its network position. We show, under mild assumptions, that these policies achieve the asymptotic lower bound on the regret up to a multiplicative factor, independent of the network structure. Finally, we use numerical examples on a real-world social network and a routing example network to demonstrate the benefits obtained by our policies over other existing policies",
    "volume": "main",
    "checked": true,
    "id": "95797bfe76daebfe780db13c94643712a9d88e84",
    "citation_count": 24
  },
  "https://jmlr.org/papers/v18/17-019.html": {
    "title": "Simultaneous Clustering and Estimation of Heterogeneous Graphical Models",
    "abstract": "We consider joint estimation of multiple graphical models arising from heterogeneous and high-dimensional observations. Unlike most previous approaches which assume that the cluster structure is given in advance, an appealing feature of our method is to learn cluster structure while estimating heterogeneous graphical models. This is achieved via a high dimensional version of Expectation Conditional Maximization (ECM) algorithm  (Meng and Rubin,  1993). A joint graphical lasso penalty is imposed on the conditional maximization step to extract both homogeneity and heterogeneity components across all clusters. Our algorithm is computationally efficient due to fast sparse learning routines and can be implemented without unsupervised learning knowledge. The superior performance of our method is demonstrated by extensive experiments and its application to a Glioblastoma cancer dataset reveals some new insights in understanding the Glioblastoma cancer. In theory, a non-asymptotic error bound is established for the output directly from our high dimensional ECM algorithm, and it consists of two quantities: statistical error (statistical accuracy) and optimization error (computational complexity). Such a result gives a theoretical guideline in terminating our ECM iterations",
    "volume": "main",
    "checked": true,
    "id": "1ff73da2c28dc2b028a4f0d53db66d2d35ba8a3b",
    "citation_count": 62
  },
  "https://jmlr.org/papers/v18/17-313.html": {
    "title": "Sketched Ridge Regression: Optimization Perspective, Statistical Perspective, and Model Averaging",
    "abstract": "We address the statistical and optimization impacts of the classical sketch and Hessian sketch used to approximately solve the Matrix Ridge Regression (MRR) problem. Prior research has quantified the effects of classical sketch on the strictly simpler least squares regression (LSR) problem. We establish that classical sketch has a similar effect upon the optimization properties of MRR as it does on those of LSR: namely, it recovers nearly optimal solutions. By contrast, Hessian sketch does not have this guarantee; instead, the approximation error is governed by a subtle interplay between the Ã¢ÂÂmassÃ¢ÂÂ in the responses and the optimal objective value. For both types of approximation, the regularization in the sketched MRR problem results in significantly different statistical properties from those of the sketched LSR problem. In particular, there is a bias-variance trade-off in sketched MRR that is not present in sketched LSR. We provide upper and lower bounds on the bias and variance of sketched MRR; these bounds show that classical sketch significantly increases the variance, while Hessian sketch significantly increases the bias. Empirically, sketched MRR solutions can have risks that are higher by an order-of- magnitude than those of the optimal MRR solutions. We establish theoretically and empirically that model averaging greatly decreases the gap between the risks of the true and sketched solutions to the MRR problem. Thus, in parallel or distributed settings, sketching combined with model averaging is a powerful technique that quickly obtains near-optimal solutions to the MRR problem while greatly mitigating the increased statistical risk incurred by sketching",
    "volume": "main",
    "checked": true,
    "id": "bdc0063ad883585d072f8462a2a3730adf1ebe87",
    "citation_count": 79
  },
  "https://jmlr.org/papers/v18/16-147.html": {
    "title": "Compact Convex Projections",
    "abstract": "We study the usefulness of conditional gradient like methods for determining projections onto convex sets, in particular, projections onto naturally arising convex sets in reproducing kernel Hilbert spaces. Our work is motivated by the recently introduced kernel herding algorithm which is closely related to the Conditional Gradient Method (CGM). It is known that the herding algorithm converges with a rate of $1/t$, where $t$ counts the number of iterations, when a point in the interior of a convex set is approximated. We generalize this result and we provide a necessary and sufficient condition for the algorithm to approximate projections with a rate of $1/t$. The CGM, which is in general vastly superior to the herding algorithm, achieves only an inferior rate of $1/\\sqrt{t}$ in this setting. We study the usefulness of such projection algorithms further by exploring ways to use these for solving concrete machine learning problems. In particular, we derive non-parametric regression algorithms which use at their core a slightly modified kernel herding algorithm to determine projections. We derive bounds to control approximation errors of these methods and we demonstrate via experiments that the developed regressors are en-par with state-of-the-art regression algorithms for large scale problems",
    "volume": "main",
    "checked": true,
    "id": "b743bd84b236625fb7c9bb36aaf5a9f41b4ab269",
    "citation_count": 1
  },
  "https://jmlr.org/papers/v18/16-319.html": {
    "title": "Complete Graphical Characterization and Construction of Adjustment Sets in Markov Equivalence Classes of Ancestral Graphs",
    "abstract": "We present a graphical criterion for covariate adjustment that is sound and complete for four different classes of causal graphical models: directed acyclic graphs (DAGs), maximal ancestral graphs (MAGs), completed partially directed acyclic graphs (CPDAGs), and partial ancestral graphs (PAGs). Our criterion unifies covariate adjustment for a large set of graph classes. Moreover, we define an explicit set that satisfies our criterion, if there is any set that satisfies our criterion. We also give efficient algorithms for constructing all sets that fulfill our criterion, implemented in the R package dagitty. Finally, we discuss the relationship between our criterion and other criteria for adjustment, and we provide new soundness and completeness proofs for the adjustment criterion for DAGs",
    "volume": "main",
    "checked": true,
    "id": "7e8b21f4eae979e1e8642805674acdea81905978",
    "citation_count": 107
  },
  "https://jmlr.org/papers/v18/16-410.html": {
    "title": "Katyusha: The First Direct Acceleration of Stochastic Gradient Methods",
    "abstract": "Nesterov's momentum trick is famously known for accelerating gradient descent, and has been proven useful in building fast iterative algorithms. However, in the stochastic setting, counterexamples exist and prevent Nesterov's momentum from providing similar acceleration, even if the underlying problem is convex and finite-sum. We introduce $\\mathtt{Katyusha}$, a direct, primal-only stochastic gradient method to fix this issue. In convex finite-sum stochastic optimization, $\\mathtt{Katyusha}$ has an optimal accelerated convergence rate, and enjoys an optimal parallel linear speedup in the mini-batch setting. The main ingredient is $\\textit{Katyusha momentum}$, a novel Ã¢ÂÂnegative momentumÃ¢ÂÂ on top of Nesterov's momentum. It can be incorporated into a variance-reduction based algorithm and speed it up, both in terms of $\\textit{sequential and parallel}$ performance. Since variance reduction has been successfully applied to a growing list of practical problems, our paper suggests that in each of such cases, one could potentially try to give Katyusha a hug",
    "volume": "main",
    "checked": true,
    "id": "06fc893d0a70249e6350df1495755b79f0870ad0",
    "citation_count": 438
  },
  "https://jmlr.org/papers/v18/16-503.html": {
    "title": "Average Stability is Invariant to Data Preconditioning. Implications to Exp-concave Empirical Risk Minimization",
    "abstract": "We show that the average stability notion introduced by Kearns and Ron (1999); Bousquet and Elisseeff (2002) is invariant to data preconditioning, for a wide class of generalized linear models that includes most of the known exp-concave losses. In other words, when analyzing the stability rate of a given algorithm, we may assume the optimal preconditioning of the data. This implies that, at least from a statistical perspective, explicit regularization is not required in order to compensate for ill-conditioned data, which stands in contrast to a widely common approach that includes a regularization for analyzing the sample complexity of generalized linear models. Several important implications of our findings include: a) We demonstrate that the excess risk of empirical risk minimization (ERM) is controlled by the preconditioned stability rate. This immediately yields a relatively short and elegant proof for the fast rates attained by ERM in our context. b) We complement the recent bounds of Hardt et al. (2015) on the stability rate of the Stochastic Gradient Descent algorithm",
    "volume": "main",
    "checked": true,
    "id": "727fa9a2fa05d2ce4d4baf7b27cf099147d51087",
    "citation_count": 21
  },
  "https://jmlr.org/papers/v18/16-595.html": {
    "title": "Parallelizing Stochastic Gradient Descent for Least Squares Regression: Mini-batching, Averaging, and Model Misspecification",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "6bb5c2180948902448388ac66e1d04010c3d9965",
    "citation_count": 122
  },
  "https://jmlr.org/papers/v18/17-243.html": {
    "title": "Learning Quadratic Variance Function (QVF) DAG Models via OverDispersion Scoring (ODS)",
    "abstract": "Learning DAG or Bayesian network models is an important problem in multi-variate causal inference. However, a number of challenges arises in learning large-scale DAG models including model identifiability and computational complexity since the space of directed graphs is huge. In this paper, we address these issues in a number of steps for a broad class of DAG models where the noise or variance is signal-dependent. Firstly we introduce a new class of identifiable DAG models, where each node has a distribution where the variance is a quadratic function of the mean (QVF DAG models). Our QVF DAG models include many interesting classes of distributions such as Poisson, Binomial, Geometric, Exponential, Gamma and many other distributions in which the noise variance depends on the mean. We prove that this class of QVF DAG models is identifiable, and introduce a new algorithm, the OverDispersion Scoring (ODS) algorithm, for learning large-scale QVF DAG models. Our algorithm is based on firstly learning the moralized or undirected graphical model representation of the DAG to reduce the DAG search-space, and then exploiting the quadratic variance property to learn the ordering. We show through theoretical results and simulations that our algorithm is statistically consistent in the high-dimensional $p>n$ setting provided that the degree of the moralized graph is bounded and performs well compared to state-of-the-art DAG-learning algorithms. We also demonstrate through a real data example involving multi-variate count data, that our ODS algorithm is well-suited to estimating DAG models for count data in comparison to other methods used for discrete data",
    "volume": "main",
    "checked": true,
    "id": "e5c2d4a715fb17b78b16d09cec6e08d521528dc8",
    "citation_count": 30
  },
  "https://jmlr.org/papers/v18/17-247.html": {
    "title": "Improved spectral community detection in large heterogeneous networks",
    "abstract": "In this article, we propose and study the performance of spectral community detection for a family of Ã¢ÂÂ$\\alpha$-normalizedÃ¢ÂÂ adjacency matrices $\\bf A$, of the type $ {\\bf D}^{-\\alpha}{\\bf A}{\\bf D}^{-\\alpha}$ with $\\bf D$ the degree matrix, in heterogeneous dense graph models. We show that the previously used normalization methods based on ${\\bf A}$ or $ {\\bf D}^{-1}{\\bf A}{\\bf D}^{-1} $ are in general suboptimal in terms of correct recovery rates and, relying on advanced random matrix methods, we prove instead the existence of an optimal value $ \\alpha_{\\rm opt} $ of the parameter $ \\alpha $ in our generic model; we further provide an online estimation of $ \\alpha_{\\rm opt} $ only based on the node degrees in the graph. Numerical simulations show that the proposed method outperforms state-of-the-art spectral approaches on moderately dense to dense heterogeneous graphs",
    "volume": "main",
    "checked": true,
    "id": "3efe2c23f90ec6988c359afc8669b6f94c821a68",
    "citation_count": 28
  },
  "https://jmlr.org/papers/v18/17-448.html": {
    "title": "Statistical Inference on Random Dot Product Graphs: a Survey",
    "abstract": "The random dot product graph (RDPG) is an independent-edge random graph that is analytically tractable and, simultaneously, either encompasses or can successfully approximate a wide range of random graphs, from relatively simple stochastic block models to complex latent position graphs. In this survey paper, we describe a comprehensive paradigm for statistical inference on random dot product graphs, a paradigm centered on spectral embeddings of adjacency and Laplacian matrices. We examine the graph-inferential analogues of several canonical tenets of classical Euclidean inference. In particular, we summarize a body of existing results on the consistency and asymptotic normality of the adjacency and Laplacian spectral embeddings, and the role these spectral embeddings can play in the construction of single- and multi-sample hypothesis tests for graph data. We investigate several real-world applications, including community detection and classification in large social networks and the determination of functional and biologically relevant network properties from an exploratory data analysis of the Drosophila connectome. We outline requisite background and current open problems in spectral graph inference",
    "volume": "main",
    "checked": true,
    "id": "d4697ad12ea937b4306455eb053a0a0abd247b6d",
    "citation_count": 191
  },
  "https://jmlr.org/papers/v18/17-755.html": {
    "title": "Rate of Convergence of $k$-Nearest-Neighbor Classification Rule",
    "abstract": "A binary classification problem is considered. The excess error probability of the $k$-nearest-neighbor classification rule according to the error probability of the Bayes decision is revisited by a decomposition of the excess error probability into approximation and estimation errors. Under a weak margin condition and under a modified Lipschitz condition or a local Lipschitz condition, tight upper bounds are presented such that one avoids the condition that the feature vector is bounded. The concept of modified Lipschitz condition is applied for discrete distributions, too. As a consequence of both concepts, we present the rate of convergence of $L_2$ error for the corresponding nearest neighbor regression estimate",
    "volume": "main",
    "checked": true,
    "id": "45a1e7b6d91e3b9332e84f1f7a823df455630003",
    "citation_count": 32
  },
  "https://jmlr.org/papers/v18/16-315.html": {
    "title": "A Theory of Learning with Corrupted Labels",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "90ba5eb1021fdf05edbbcb1424c477d86dd3a216",
    "citation_count": 67
  },
  "https://jmlr.org/papers/v18/16-424.html": {
    "title": "Interactive Algorithms: Pool, Stream and Precognitive Stream",
    "abstract": "We consider interactive algorithms in the pool-based setting, and in the stream-based setting. Interactive algorithms observe suggested elements (representing actions or queries), and interactively select some of them and receive responses. Pool- based algorithms can select elements at any order, while stream- based algorithms observe elements in sequence, and can only select elements immediately after observing them. We further consider an intermediate setting, which we term precognitive stream, in which the algorithm knows in advance the identity of all the elements in the sequence, but can select them only in the order of their appearance. For all settings, we assume that the suggested elements are generated independently from some source distribution, and ask what is the stream size required for emulating a pool algorithm with a given pool size, in the stream-based setting and in the precognitive stream setting. We provide algorithms and matching lower bounds for general pool algorithms, and for utility-based pool algorithms. We further derive nearly matching upper and lower bounds on the gap between the two settings for the special case of active learning for binary classification",
    "volume": "main",
    "checked": true,
    "id": "0d2b67a5e93d53748fbac783683e388585666a25",
    "citation_count": 4
  },
  "https://jmlr.org/papers/v18/16-512.html": {
    "title": "CoCoA: A General Framework for Communication-Efficient Distributed Optimization",
    "abstract": "The scale of modern datasets necessitates the development of efficient distributed optimization methods for machine learning. We present a general-purpose framework for distributed computing environments, CoCoA, that has an efficient communication scheme and is applicable to a wide variety of problems in machine learning and signal processing. We extend the framework to cover general non-strongly-convex regularizers, including L1-regularized problems like lasso, sparse logistic regression, and elastic net regularization, and show how earlier work can be derived as a special case. We provide convergence guarantees for the class of convex regularized loss minimization objectives, leveraging a novel approach in handling non-strongly-convex regularizers and non-smooth loss functions. The resulting framework has markedly improved performance over state-of-the- art methods, as we illustrate with an extensive set of experiments on real distributed datasets",
    "volume": "main",
    "checked": true,
    "id": "1d720f25d5d2c6dcaebfe108ead3036b7fe06940",
    "citation_count": 234
  },
  "https://jmlr.org/papers/v18/17-012.html": {
    "title": "Concentration inequalities for empirical processes of linear time series",
    "abstract": "The paper considers suprema of empirical processes for linear time series indexed by functional classes. We derive an upper bound for the tail probability of the suprema under conditions on the size of the function class, the sample size, temporal dependence and the moment conditions of the underlying time series. Due to the dependence and heavy-tailness, our tail probability bound is substantially different from those classical exponential bounds obtained under the independence assumption in that it involves an extra polynomial decaying term. We allow both short- and long-range dependent processes. For empirical processes indexed by half intervals, our tail probability inequality is sharp up to a multiplicative constant",
    "volume": "main",
    "checked": true,
    "id": "98f9e6ff032d10a23d25f4795e36779f496fe775",
    "citation_count": 12
  },
  "https://jmlr.org/papers/v18/17-445.html": {
    "title": "A Cluster Elastic Net for Multivariate Regression",
    "abstract": "We propose a method for simultaneously estimating regression coefficients and clustering response variables in a multivariate regression model, to increase prediction accuracy and give insights into the relationship between response variables. The estimates of the regression coefficients and clusters are found by using a penalized likelihood estimator, which includes a cluster fusion penalty, to shrink the difference in fitted values from responses in the same cluster, and an $L_1$ penalty for simultaneous variable selection and estimation. We propose a two-step algorithm, that iterates between k-means clustering and solving the penalized likelihood function assuming the clusters are known, which has desirable parallel computational properties obtained by using the cluster fusion penalty. If the response variable clusters are known a priori then the algorithm reduces to just solving the penalized likelihood problem. Theoretical results are presented for the penalized least squares case, including asymptotic results allowing for $p \\gg n$. We extend our method to the setting where the responses are binomial variables. We propose a coordinate descent algorithm for the normal likelihood and a proximal gradient descent algorithm for the binomial likelihood, which can easily be extended to other generalized linear model (GLM) settings. Simulations and data examples from business operations and genomics are presented to show the merits of both the least squares and binomial methods",
    "volume": "main",
    "checked": true,
    "id": "a9efac350ca4e267480b95157a71a13444c7fa06",
    "citation_count": 15
  },
  "https://jmlr.org/papers/v18/17-492.html": {
    "title": "Characteristic and Universal Tensor Product Kernels",
    "abstract": "Maximum mean discrepancy (MMD), also called energy distance or N-distance in statistics and Hilbert-Schmidt independence criterion (HSIC), specifically distance covariance in statistics, are among the most popular and successful approaches to quantify the difference and independence of random variables, respectively. Thanks to their kernel-based foundations, MMD and HSIC are applicable on a wide variety of domains. Despite their tremendous success, quite little is known about when HSIC characterizes independence and when MMD with tensor product kernel can discriminate probability distributions. In this paper, we answer these questions by studying various notions of characteristic property of the tensor product kernel",
    "volume": "main",
    "checked": true,
    "id": "ab2de69e9ea9e376fddd74137c51d7ebc36214c4",
    "citation_count": 54
  },
  "https://jmlr.org/papers/v18/17-716.html": {
    "title": "Learning Certifiably Optimal Rule Lists for Categorical Data",
    "abstract": "We present the design and implementation of a custom discrete optimization technique for building rule lists over a categorical feature space. Our algorithm produces rule lists with optimal training performance, according to the regularized empirical risk, with a certificate of optimality. By leveraging algorithmic bounds, efficient data structures, and computational reuse, we achieve several orders of magnitude speedup in time and a massive reduction of memory consumption. We demonstrate that our approach produces optimal rule lists on practical problems in seconds. Our results indicate that it is possible to construct optimal sparse rule lists that are approximately as accurate as the COMPAS proprietary risk prediction tool on data from Broward County, Florida, but that are completely interpretable. This framework is a novel alternative to CART and other decision tree methods for interpretable modeling",
    "volume": "main",
    "checked": true,
    "id": "36ea2ee3c4d6ff388dea3c7d685e7fe2019d8911",
    "citation_count": 193
  },
  "https://jmlr.org/papers/v19/16-210.html": {
    "title": "Numerical Analysis near Singularities in RBF Networks",
    "abstract": "The existence of singularities often affects the learning dynamics in feedforward neural networks. In this paper, based on theoretical analysis results, we numerically analyze the learning dynamics of radial basis function (RBF) networks near singularities to understand to what extent singularities influence the learning dynamics. First, we show the explicit expression of the Fisher information matrix for RBF networks. Second, we demonstrate through numerical simulations that the singularities have a significant impact on the learning dynamics of RBF networks. Our results show that overlap singularities mainly have influence on the low dimensional RBF networks and elimination singularities have a more significant impact to the learning processes than overlap singularities in both low and high dimensional RBF networks, whereas the plateau phenomena are mainly caused by the elimination singularities. The results can also be the foundation to investigate the singular learning dynamics in deep feedforward neural networks",
    "volume": "main",
    "checked": true,
    "id": "c33cc603f5ec57d8ff1d6e3bb39a04c44153c4e4",
    "citation_count": 8
  },
  "https://jmlr.org/papers/v19/16-225.html": {
    "title": "A Two-Stage Penalized Least Squares Method for Constructing Large Systems of Structural Equations",
    "abstract": "We propose a two-stage penalized least squares method to build large systems of structural equations based on the instrumental variables view of the classical two-stage least squares method. We show that, with large numbers of endogenous and exogenous variables, the system can be constructed via consistent estimation of a set of conditional expectations at the first stage, and consistent selection of regulatory effects at the second stage. While the consistent estimation at the first stage can be obtained via the ridge regression, the adaptive lasso is employed at the second stage to achieve the consistent selection. This method is computationally fast and allows for parallel implementation. We demonstrate its effectiveness via simulation studies and real data analysis",
    "volume": "main",
    "checked": true,
    "id": "dae03d7ee2cc63d25ffde7d4d20c93abf8d8d95c",
    "citation_count": 15
  },
  "https://jmlr.org/papers/v19/16-534.html": {
    "title": "Approximate Submodularity and its Applications: Subset Selection, Sparse Approximation and Dictionary Selection",
    "abstract": "We introduce the submodularity ratio as a measure of how “close” to submodular a set function $f$ is. We show that when $f$ has submodularity ratio $\\gamma$, the greedy algorithm for maximizing $f$ provides a $(1-e^{-\\gamma})$-approximation. Furthermore, when $\\gamma$ is bounded away from 0, the greedy algorithm for minimum submodular cover also provides essentially an $O(\\log n)$ approximation for a universe of $n$ elements. As a main application of this framework, we study the problem of selecting a subset of $k$ random variables from a large set, in order to obtain the best linear prediction of another variable of interest. We analyze the performance of widely used greedy heuristics; in particular, by showing that the submodularity ratio is lower-bounded by the smallest $2k$-sparse eigenvalue of the covariance matrix, we obtain the strongest known approximation guarantees for the Forward Regression and Orthogonal Matching Pursuit algorithms. As a second application, we analyze greedy algorithms for the dictionary selection problem, and significantly improve the previously known guarantees. Our theoretical analysis is complemented by experiments on real-world and synthetic data sets; in particular, we focus on an analysis of how tight various spectral parameters and the submodularity ratio are in terms of predicting the performance of the greedy algorithms",
    "volume": "main",
    "checked": true,
    "id": "9e3e8d6f833e8abae13662ad43d1b1f2152a8510",
    "citation_count": 73
  },
  "https://jmlr.org/papers/v19/16-656.html": {
    "title": "A Hidden Absorbing Semi-Markov Model for Informatively Censored Temporal Data: Learning and Inference",
    "abstract": "Modeling continuous-time physiological processes that manifest a patient's evolving clinical states is a key step in approaching many problems in healthcare. In this paper, we develop the Hidden Absorbing Semi-Markov Model (HASMM): a versatile probabilistic model that is capable of capturing the modern electronic health record (EHR) data. Unlike existing models, the HASMM accommodates irregularly sampled, temporally correlated, and informatively censored physiological data, and can describe non-stationary clinical state transitions. Learning the HASMM parameters from the EHR data is achieved via a novel  forward-filtering backward-sampling Monte-Carlo EM algorithm that exploits the knowledge of the end-point clinical outcomes (informative censoring) in the EHR data, and implements the E-step by sequentially sampling the patients' clinical states in the reverse-time direction while conditioning on the future states. Real-time inferences are drawn via a forward-filtering algorithm that operates on a virtually constructed discrete-time embedded Markov chain that mirrors the patient's continuous-time state trajectory. We demonstrate the prognostic utility of the HASMM in a critical care prognosis setting using a real-world dataset for patients admitted to the Ronald Reagan UCLA Medical Center. In particular, we show that using HASMMs, a patient's clinical deterioration can be predicted 8-9 hours prior to intensive care unit admission, with a 22$\\%$ AUC gain compared to the Rothman index, which is the state-of-the-art critical care risk scoring technology",
    "volume": "main",
    "checked": true,
    "id": "61de14e45e99f279ba6e9f85d8e00d28dcdfa5b4",
    "citation_count": 26
  },
  "https://jmlr.org/papers/v19/17-006.html": {
    "title": "Can We Trust the Bootstrap in High-dimensions? The Case of Linear Models",
    "abstract": "We consider the performance of the bootstrap in high-dimensions for the setting of linear regression, where $p\\lt n$ but $p/n$ is not close to zero. We consider ordinary least-squares as well as robust regression methods and adopt a minimalist performance requirement: can the bootstrap give us good confidence intervals for a single coordinate of $\\beta$ (where $\\beta$ is the true regression vector)? We show through a mix of numerical and theoretical work that the bootstrap is fraught with problems. Both of the most commonly used methods of bootstrapping for regression–residual bootstrap and pairs bootstrap–give very poor inference on $\\beta$ as the ratio $p/n$ grows. We find that the residual bootstrap tend to give anti-conservative estimates (inflated Type I error), while the pairs bootstrap gives very conservative estimates (severe loss of power) as the ratio $p/n$ grows. We also show that the jackknife resampling technique for estimating the variance of $\\hat{\\beta}$ severely overestimates the variance in high dimensions. We contribute alternative procedures based on our theoretical results that result in dimensionality adaptive and robust bootstrap methods",
    "volume": "main",
    "checked": true,
    "id": "c8a1b796891fb33eaee1cfb8b11e9bd73c2f7e06",
    "citation_count": 33
  },
  "https://jmlr.org/papers/v19/17-016.html": {
    "title": "RSG: Beating Subgradient Method without Smoothness and Strong Convexity",
    "abstract": "In this paper, we study the efficiency of a Restarted SubGradient (RSG) method that periodically restarts the standard subgradient method (SG). We show that, when applied to a broad class of convex optimization problems, RSG method can find an $\\epsilon$-optimal solution with a lower complexity than the SG method. In particular, we first show that RSG can reduce the dependence of SG's iteration complexity on the distance between the initial solution and the optimal set to that between the $\\epsilon$-level set and the optimal set {multiplied by a logarithmic factor}. Moreover, we show the advantages of RSG over SG in solving a broad family of problems that satisfy a local error bound condition, and also demonstrate its advantages for three specific families of convex optimization problems with different power constants in the local error bound condition. (a) For the problems whose epigraph is a polyhedron, RSG is shown to converge linearly. (b) For the problems with local quadratic growth property in the $\\epsilon$-sublevel set, RSG has an $O(\\frac{1}{\\epsilon}\\log(\\frac{1}{\\epsilon}))$ iteration complexity. (c) For the problems that admit a local Kurdyka-{\\L}ojasiewicz property with a power constant of $\\beta\\in[0,1)$, RSG has an $O(\\frac{1}{\\epsilon^{2\\beta}}\\log(\\frac{1}{\\epsilon}))$ iteration complexity. The novelty of our analysis lies at exploiting the lower bound of the first-order optimality residual at the $\\epsilon$-level set. It is this novelty that allows us to explore the local properties of functions (e.g., local quadratic growth property, local Kurdyka-\\L ojasiewicz property, more generally local error bound conditions) to develop the improved convergence of RSG. We also develop a practical variant of RSG enjoying faster convergence than the SG method, which can be run without knowing the involved parameters in the local error bound condition. We demonstrate the effectiveness of the proposed algorithms on several machine learning tasks including regression, classification and matrix completion",
    "volume": "main",
    "checked": true,
    "id": "6fa9bf229d8e48b868024fd14d334b88b18a018d",
    "citation_count": 73
  },
  "https://jmlr.org/papers/v19/17-042.html": {
    "title": "Patchwork Kriging for Large-scale Gaussian Process Regression",
    "abstract": "This paper presents a new approach for Gaussian process (GP) regression for large datasets. The approach involves partitioning the regression input domain into multiple local regions with a different local GP model fitted in each region. Unlike existing local partitioned GP approaches, we introduce a technique for patching together the local GP models nearly seamlessly to ensure that the local GP models for two neighboring regions produce nearly the same response prediction and prediction error variance on the boundary between the two regions. This largely mitigates the well-known discontinuity problem that degrades the prediction accuracy of existing local partitioned GP methods over regional boundaries. Our main innovation is to represent the continuity conditions as additional pseudo-observations that the differences between neighboring GP responses are identically zero at an appropriately chosen set of boundary input locations. To predict the response at any input location, we simply augment the actual response observations with the pseudo-observations and apply standard GP prediction methods to the augmented data. In contrast to heuristic continuity adjustments, this has an advantage of working within a formal GP framework, so that the GP-based predictive uncertainty quantification remains valid. Our approach also inherits a sparse block-like structure for the sample covariance matrix, which results in computationally efficient closed-form expressions for the predictive mean and variance. In addition, we provide a new spatial partitioning scheme based on a recursive space partitioning along local principal component directions, which makes the proposed approach applicable for regression domains having more than two dimensions. Using three spatial datasets and three higher dimensional datasets, we investigate the numerical performance of the approach and compare it to several state-of-the-art approaches",
    "volume": "main",
    "checked": true,
    "id": "203f8db95b6de48f438a46dad83ee0158b6c05ab",
    "citation_count": 63
  },
  "https://jmlr.org/papers/v19/17-084.html": {
    "title": "Scalable Bayes via Barycenter in Wasserstein Space",
    "abstract": "Divide-and-conquer based methods for Bayesian inference provide a general approach for tractable posterior inference when the sample size is large. These methods divide the data into smaller subsets, sample from the posterior distribution of parameters in parallel on all the subsets, and combine posterior samples from all the subsets to approximate the full data posterior distribution. The smaller size of any subset compared to the full data implies that posterior sampling on any subset is computationally more efficient than sampling from the true posterior distribution. Since the combination step takes negligible time relative to sampling, posterior computations can be scaled to massive data by dividing the full data into sufficiently large number of data subsets. One such approach relies on the geometry of posterior distributions estimated across different subsets and combines them through their barycenter in a Wasserstein space of probability measures. We provide theoretical guarantees on the accuracy of approximation that are valid in many applications. We show that the geometric method approximates the full data posterior distribution better than its competitors across diverse simulations and reproduces known results when applied to a movie ratings database",
    "volume": "main",
    "checked": true,
    "id": "293b14d6f5bf8c0ac754e0472207b880ec06e770",
    "citation_count": 136
  },
  "https://jmlr.org/papers/v19/17-131.html": {
    "title": "Experience Selection in Deep Reinforcement Learning for Control",
    "abstract": "Experience replay is a technique that allows off-policy reinforcement-learning methods to reuse past experiences. The stability and speed of convergence of reinforcement learning, as well as the eventual performance of the learned policy, are strongly dependent on the experiences being replayed. Which experiences are replayed depends on two important choices. The first is which and how many experiences to retain in the experience replay buffer. The second choice is how to sample the experiences that are to be replayed from that buffer. We propose new methods for the combined problem of experience retention and experience sampling. We refer to the combination as experience selection. We focus our investigation specifically on the control of physical systems, such as robots, where exploration is costly. To determine which experiences to keep and which to replay, we investigate different proxies for their immediate and long-term utility. These proxies include age, temporal difference error and the strength of the applied exploration noise. Since no currently available method works in all situations, we propose guidelines for using prior knowledge about the characteristics of the control problem at hand to choose the appropriate experience replay strategy",
    "volume": "main",
    "checked": true,
    "id": "6a17b2ba75d8fca0b06fa5329b18694ed985959f",
    "citation_count": 57
  },
  "https://jmlr.org/papers/v19/17-194.html": {
    "title": "A Constructive Approach to $L_0$ Penalized Regression",
    "abstract": "We propose a constructive approach to estimating sparse, high-dimensional linear regression models. The approach is a computational algorithm motivated from the KKT conditions for the $\\ell_0$-penalized least squares solutions. It generates a sequence of solutions iteratively, based on support detection using primal and dual information and root finding. We refer to the algorithm as SDAR for brevity. Under a sparse Riesz condition on the design matrix and certain other conditions, we show that with high probability, the $\\ell_2$ estimation error of the solution sequence decays exponentially to the minimax error bound in $O(\\log(R\\sqrt{J}))$ iterations, where $J$ is the number of important predictors and $R$ is the relative magnitude of the nonzero target coefficients; and under a mutual coherence condition and certain other conditions, the $\\ell_{\\infty}$ estimation error decays to the optimal error bound in $O(\\log(R))$ iterations. Moreover the SDAR solution recovers the oracle least squares estimator within a finite number of iterations with high probability if the sparsity level is known. Computational complexity analysis shows that the cost of SDAR is $O(np)$ per iteration. We also consider an adaptive version of SDAR for use in practical applications where the true sparsity level is unknown. Simulation studies demonstrate that SDAR outperforms Lasso, MCP and two greedy methods in accuracy and efficiency",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://jmlr.org/papers/v19/17-218.html": {
    "title": "Change-Point Computation for Large Graphical Models: A Scalable Algorithm for Gaussian Graphical Models with Change-Points",
    "abstract": "Graphical models with change-points are computationally challenging to fit, particularly in cases where the number of observation points and the number of nodes in the graph are large. Focusing on Gaussian graphical models, we introduce an approximate majorize-minimize (MM) algorithm that can be useful for computing change-points in large graphical models. The proposed algorithm is an order of magnitude faster than a brute force search. Under some regularity conditions on the data generating process, we show that with high probability, the algorithm converges to a value that is within statistical error of the true change-point. A fast implementation of the algorithm using Markov Chain Monte Carlo is also introduced. The performances of the proposed algorithms are evaluated on synthetic data sets and the algorithm is also used to analyze structural changes in the S{\\&}P 500 over the period 2000-2016",
    "volume": "main",
    "checked": true,
    "id": "15029a6ba1a50e7943dbac73a6bfbecc6e1cf05a",
    "citation_count": 18
  },
  "https://jmlr.org/papers/v19/17-291.html": {
    "title": "Statistical Analysis and Parameter Selection for Mapper",
    "abstract": "In this article, we study the question of the statistical convergence of the 1-dimensional Mapper to its continuous analogue, the Reeb graph. We show that the Mapper is an optimal estimator of the Reeb graph, which gives, as a byproduct, a method to automatically tune its parameters and compute confidence regions on its topological features, such as its loops and flares. This allows to circumvent the issue of testing a large grid of parameters and keeping the most stable ones in the brute-force setting, which is widely used in visualization, clustering and feature selection with the Mapper",
    "volume": "main",
    "checked": true,
    "id": "47ad46286319b680544d6bb04ce462cb51a710cf",
    "citation_count": 69
  },
  "https://jmlr.org/papers/v19/17-295.html": {
    "title": "A Robust Learning Approach for Regression Models Based on Distributionally Robust Optimization",
    "abstract": "We present a Distributionally Robust Optimization (DRO) approach to estimate a robustified regression plane in a linear regression setting, when the observed samples are potentially contaminated with adversarially corrupted outliers. Our approach mitigates the impact of outliers by hedging against a family of probability distributions on the observed data, some of which assign very low probabilities to the outliers. The set of distributions under consideration are close to the empirical distribution in the sense of the Wasserstein metric. We show that this DRO formulation can be relaxed to a convex optimization problem which encompasses a class of models. By selecting proper norm spaces for the Wasserstein metric, we are able to recover several commonly used regularized regression models. We provide new insights into the regularization term and give guidance on the selection of the regularization coefficient from the standpoint of a confidence region. We establish two types of performance guarantees for the solution to our formulation under mild conditions. One is related to its out-of-sample behavior (prediction bias), and the other concerns the discrepancy between the estimated and true regression planes (estimation bias). Extensive numerical results demonstrate the superiority of our approach to a host of regression models, in terms of the prediction and estimation accuracies. We also consider the application of our robust learning procedure to outlier detection, and show that our approach achieves a much higher AUC (Area Under the ROC Curve) than M-estimation (Huber, 1964, 1973)",
    "volume": "main",
    "checked": true,
    "id": "3d61c35547b3fe5a8529a46d0ef6f8e6c5bcb153",
    "citation_count": 86
  },
  "https://jmlr.org/papers/v19/17-329.html": {
    "title": "Model-Free Trajectory-based Policy Optimization with Monotonic Improvement",
    "abstract": "Many of the recent trajectory optimization algorithms alternate between linear approximation of the system dynamics around the mean trajectory and conservative policy update. One way of constraining the policy change is by bounding the Kullback-Leibler (KL) divergence between successive policies. These approaches already demonstrated great experimental success in challenging problems such as end-to-end control of physical systems. However, the linear approximation of the system dynamics can introduce a bias in the policy update and prevent convergence to the optimal policy. In this article, we propose a new model-free trajectory-based policy optimization algorithm with guaranteed monotonic improvement. The algorithm backpropagates a local, quadratic and time-dependent \\qfunc learned from trajectory data instead of a model of the system dynamics. Our policy update ensures exact KL-constraint satisfaction without simplifying assumptions on the system dynamics. We experimentally demonstrate on highly non-linear control tasks the improvement in performance of our algorithm in comparison to approaches linearizing the system dynamics. In order to show the monotonic improvement of our algorithm, we additionally conduct a theoretical analysis of our policy update scheme to derive a lower bound of the change in policy return between successive iterations",
    "volume": "main",
    "checked": true,
    "id": "4d6fdb281bb14439531a0c95e6266e1247fb37c4",
    "citation_count": 26
  },
  "https://jmlr.org/papers/v19/17-361.html": {
    "title": "Regularized Optimal Transport and the Rot Mover's Distance",
    "abstract": "This paper presents a unified framework for smooth convex regularization of discrete optimal transport problems. In this context, the regularized optimal transport turns out to be equivalent to a matrix nearness problem with respect to Bregman divergences. Our framework thus naturally generalizes a previously proposed regularization based on the Boltzmann-Shannon entropy related to the Kullback-Leibler divergence, and solved with the Sinkhorn-Knopp algorithm. We call the regularized optimal transport distance the rot mover's distance in reference to the classical earth mover's distance. By exploiting alternate Bregman projections, we develop the alternate scaling algorithm and non-negative alternate scaling algorithm, to compute efficiently the regularized optimal plans depending on whether the domain of the regularizer lies within the non-negative orthant or not. We further enhance the separable case with a sparse extension to deal with high data dimensions. We also instantiate our framework and discuss the inherent specificities for well-known regularizers and statistical divergences in the machine learning and information geometry communities. Finally, we demonstrate the merits of our methods with experiments using synthetic data to illustrate the effect of different regularizers, penalties and dimensions, as well as real-world data for a pattern recognition application to audio scene classification",
    "volume": "main",
    "checked": true,
    "id": "9610ec402a1205cdad534ca7402d6e269e35fd07",
    "citation_count": 70
  },
  "https://jmlr.org/papers/v19/17-374.html": {
    "title": "ELFI: Engine for Likelihood-Free Inference",
    "abstract": "Engine for Likelihood-Free Inference (ELFI) is a Python software library for performing likelihood-free inference (LFI). ELFI provides a convenient syntax for arranging components in LFI, such as priors, simulators, summaries or distances, to a network called ELFI graph. The components can be implemented in a wide variety of languages. The stand-alone ELFI graph can be used with any of the available inference methods without modifications. A central method implemented in ELFI is Bayesian Optimization for Likelihood-Free Inference (BOLFI), which has recently been shown to accelerate likelihood-free inference up to several orders of magnitude by surrogate-modelling the distance. ELFI also has an inbuilt support for output data storing for reuse and analysis, and supports parallelization of computation from multiple cores up to a cluster environment. ELFI is designed to be extensible and provides interfaces for widening its functionality. This makes the adding of new inference methods to ELFI straightforward and automatically compatible with the inbuilt features",
    "volume": "MLOSS",
    "checked": false,
    "id": "54bc637f34f0b38df7ffbc0e6aef8b55644364e2",
    "citation_count": 59
  },
  "https://jmlr.org/papers/v19/17-404.html": {
    "title": "Streaming kernel regression with provably adaptive mean, variance, and regularization",
    "abstract": "We consider the problem of streaming kernel regression, when the observations arrive sequentially and the goal is to recover the underlying mean function, assumed to belong to an RKHS. The variance of the noise is not assumed to be known. In this context, we tackle the problem of tuning the regularization parameter adaptively at each time step, while maintaining tight confidence bounds estimates on the value of the mean function at each point. To this end, we first generalize existing results for finite-dimensional linear regression with fixed regularization and known variance to the kernel setup with a regularization parameter allowed to be a measurable function of past observations. Then, using appropriate self-normalized inequalities we build upper and lower bound estimates for the variance, leading to Bernstein-like concentration bounds. The latter is used in order to define the adaptive regularization. The bounds resulting from our technique are valid uniformly over all observation points and all time steps, and are compared against the literature with numerical experiments. Finally, the potential of these tools is illustrated by an application to kernelized bandits, where we revisit the Kernel UCB and Kernel Thompson Sampling procedures, and show the benefits of the novel adaptive kernel tuning strategy",
    "volume": "main",
    "checked": true,
    "id": "424d50c7a73f30f94e1af910f6f78a68ac1769b7",
    "citation_count": 34
  },
  "https://jmlr.org/papers/v19/17-436.html": {
    "title": "Dual Principal Component Pursuit",
    "abstract": "We consider the problem of learning a linear subspace from data corrupted by outliers. Classical approaches are typically designed for the case in which the subspace dimension is small relative to the ambient dimension. Our approach works with a dual representation of the subspace and hence aims to find its orthogonal complement; as such, it is particularly suitable for subspaces whose dimension is close to the ambient dimension (subspaces of high relative dimension). We pose the problem of computing normal vectors to the inlier subspace as a non-convex $\\ell_1$ minimization problem on the sphere, which we call Dual Principal Component Pursuit (DPCP) problem. We provide theoretical guarantees under which every global solution to DPCP is a vector in the orthogonal complement of the inlier subspace. Moreover, we relax the non-convex DPCP problem to a recursion of linear programs whose solutions are shown to converge in a finite number of steps to a vector orthogonal to the subspace. In particular, when the inlier subspace is a hyperplane, the solutions to the recursion of linear programs converge to the global minimum of the non-convex DPCP problem in a finite number of steps. We also propose algorithms based on alternating minimization and iteratively re-weighted least squares, which are suitable for dealing with large-scale data. Experiments on synthetic data show that the proposed methods are able to handle more outliers and higher relative dimensions than current state-of-the-art methods, while experiments in the context of the three-view geometry problem in computer vision suggest that the proposed methods can be a useful or even superior alternative to traditional RANSAC-based approaches for computer vision and other applications",
    "volume": "main",
    "checked": true,
    "id": "9dc822d8c41d1091b3ae200fe5efa6f98bd4ecf3",
    "citation_count": 87
  },
  "https://jmlr.org/papers/v19/17-444.html": {
    "title": "Distributed Proximal Gradient Algorithm for Partially Asynchronous Computer Clusters",
    "abstract": "With ever growing data volume and model size, an error-tolerant, communication efficient, yet versatile distributed algorithm has become vital for the success of many large-scale machine learning applications. In this work we propose m-PAPG, an implementation of the flexible proximal gradient algorithm in model parallel systems equipped with the partially asynchronous communication protocol. The worker machines communicate asynchronously with a controlled staleness bound $s$ and operate at different frequencies. We characterize various convergence properties of m-PAPG: 1) Under a general non-smooth and non-convex setting, we prove that every limit point of the sequence generated by m-PAPG is a critical point of the objective function; 2) Under an error bound condition of convex objective functions, we prove that the optimality gap decays linearly for every $s$ steps; 3) Under the Kurdyka-{\\L}ojasiewicz inequality and a sufficient decrease assumption, we prove that the sequences generated by m-PAPG converge to the same critical point, provided that a proximal Lipschitz condition is satisfied",
    "volume": "main",
    "checked": true,
    "id": "4b9e2ed2257033f8c82df2ed9ec4bada4017713b",
    "citation_count": 24
  },
  "https://jmlr.org/papers/v19/17-513.html": {
    "title": "Refining the Confidence Level for Optimistic Bandit Strategies",
    "abstract": "This paper introduces the first strategy for stochastic bandits with unit variance Gaussian noise that is simultaneously minimax optimal up to constant factors, asymptotically optimal, and never worse than the classical upper confidence bound strategy up to universal constant factors. Preliminary empirical evidence is also promising. Besides this, a conjecture on the optimal form of the regret is shown to be false and a finite-time lower bound on the regret of any strategy is presented that very nearly matches the finite-time upper bound of the newly proposed strategy",
    "volume": "main",
    "checked": true,
    "id": "f767585cc69d29cf78070b51065a6c4cb46ea7ac",
    "citation_count": 41
  },
  "https://jmlr.org/papers/v19/17-740.html": {
    "title": "ThunderSVM: A Fast SVM Library on GPUs and CPUs",
    "abstract": "Support Vector Machines (SVMs) are classic supervised learning models for classification, regression and distribution estimation. A survey conducted by Kaggle in 2017 shows that 26% of the data mining and machine learning practitioners are users of SVMs. However, SVM training and prediction are very expensive computationally for large and complex problems. This paper presents an efficient and open source SVM software toolkit called ThunderSVM which exploits the high-performance of Graphics Processing Units (GPUs) and multi-core CPUs. ThunderSVM supports all the functionalities–including classification (SVC), regression (SVR) and one-class SVMs–of LibSVM and uses identical command line options, such that existing LibSVM users can easily apply our toolkit. ThunderSVM can be used through multiple language interfaces including C/C++, Python, R and MATLAB. Our experimental results show that ThunderSVM is generally an order of magnitude faster than LibSVM while producing identical SVMs. In addition to the high efficiency, we design our convex optimization solver in a general way such that SVC, SVR, and one-class SVMs share the same solver for the ease of maintenance. Documentation, examples, and more about ThunderSVM are available at https://github.com/zeyiwen/thundersvm",
    "volume": "MLOSS",
    "checked": true,
    "id": "93e583e5abdf699fdbd0bf26f645f883a9b64f07",
    "citation_count": 143
  },
  "https://jmlr.org/papers/v19/17-777.html": {
    "title": "Robust Synthetic Control",
    "abstract": "We present a robust generalization of the synthetic control method for comparative case studies. Like the classical method cf. \\cite{abadie3}, we present an algorithm to estimate the unobservable counterfactual of a treatment unit. A distinguishing feature of our algorithm is that of de-noising the data matrix via singular value thresholding, which renders our approach robust in multiple facets: it automatically identifies a good subset of donors for the synthetic control, overcomes the challenges of missing data, and continues to work well in settings where covariate information may not be provided. We posit that the setting can be viewed as an instance of the Latent Variable Model and provide the first finite sample analysis (coupled with asymptotic results) for the estimation of the counterfactual. Our algorithm accurately imputes missing entries and filters corrupted observations in producing a consistent estimator of the underlying signal matrix, provided $p = \\Omega( T^{-1 + \\zeta})$ for some $\\zeta > 0$; here, $p$ is the fraction of observed data and $T$ is the time interval of interest. Under the same proportion of observations, we demonstrate that the mean-squared error in our counterfactual estimation scales as $\\mathcal{O}(\\sigma^2/p + 1/\\sqrt{T})$, where $\\sigma^2$ is the variance of the inherent noise. Additionally, we introduce a Bayesian framework to quantify the estimation uncertainty. Our experiments, using both synthetic and real-world datasets, demonstrate that our robust generalization yields an improvement over the classical synthetic control method",
    "volume": "main",
    "checked": true,
    "id": "ef8d3a3a0b9d75ab918f8da7e4a4343e9e50efba",
    "citation_count": 121
  },
  "https://jmlr.org/papers/v19/17-781.html": {
    "title": "Reverse Iterative Volume Sampling for Linear Regression",
    "abstract": "We study the following basic machine learning task: Given a fixed set of input points in $\\mathbb{R}^d$ for a linear regression problem, we wish to predict a hidden response value for each of the points. We can only afford to attain the responses for a small subset of the points that are then used to construct linear predictions for all points in the dataset. The performance of the predictions is evaluated by the total square loss on all responses (the attained as well as the remaining hidden ones). We show that a good approximate solution to this least squares problem can be obtained from just dimension $d$ many responses by using a joint sampling technique called volume sampling. Moreover, the least squares solution obtained for the volume sampled subproblem is an unbiased estimator of optimal solution based on all $n$ responses. This unbiasedness is a desirable property that is not shared by other common subset selection techniques. Motivated by these basic properties, we develop a theoretical framework for studying volume sampling, resulting in a number of new matrix expectation equalities and statistical guarantees which are of importance not only to least squares regression but also to numerical linear algebra in general. Our methods also lead to a regularized variant of volume sampling, and we propose the first efficient algorithm for volume sampling which makes this technique a practical tool in the machine learning toolbox. Finally, we provide experimental evidence which confirms our theoretical findings",
    "volume": "main",
    "checked": true,
    "id": "053d3f6d1ae2bb23dbd4fe26150c56a04facfba2",
    "citation_count": 43
  },
  "https://jmlr.org/papers/v19/18-020.html": {
    "title": "Universal discrete-time reservoir computers with stochastic inputs and linear readouts using non-homogeneous state-affine systems",
    "abstract": "A new class of non-homogeneous state-affine systems is introduced for use in reservoir computing. Sufficient conditions are identified that guarantee first, that the associated reservoir computers with linear readouts are causal, time-invariant, and satisfy the fading memory property and second, that a subset of this class is universal in the category of fading memory filters with stochastic almost surely uniformly bounded inputs. This means that any discrete-time filter that satisfies the fading memory property with random inputs of that type can be uniformly approximated by elements in the non-homogeneous state-affine family",
    "volume": "main",
    "checked": true,
    "id": "b05338b8dce4e23af78e413b27b107ae62dcd647",
    "citation_count": 44
  },
  "https://jmlr.org/papers/v19/18-046.html": {
    "title": "Deep Hidden Physics Models: Deep Learning of Nonlinear Partial Differential Equations",
    "abstract": "We put forth a deep learning approach for discovering nonlinear partial differential equations from scattered and potentially noisy observations in space and time. Specifically, we approximate the unknown solution as well as the nonlinear dynamics by two deep neural networks. The first network acts as a prior on the unknown solution and essentially enables us to avoid numerical differentiations which are inherently ill-conditioned and unstable. The second network represents the nonlinear dynamics and helps us distill the mechanisms that govern the evolution of a given spatiotemporal data-set. We test the effectiveness of our approach for several benchmark problems spanning a number of scientific domains and demonstrate how the proposed framework can help us accurately learn the underlying dynamics and forecast future states of the system. In particular, we study the Burgers', Korteweg-de Vries (KdV), Kuramoto-Sivashinsky, nonlinear Schr\\\"{o}dinger, and Navier-Stokes equations",
    "volume": "main",
    "checked": true,
    "id": "ebcc0e71ef6a77d05e7ab064435bc2da87c55e91",
    "citation_count": 553
  },
  "https://jmlr.org/papers/v19/18-100.html": {
    "title": "OpenEnsembles: A Python Resource for Ensemble Clustering",
    "abstract": "In this paper we introduce OpenEnsembles, a Python toolkit for performing and analyzing ensemble clustering. Ensemble clustering is the process of creating many clustering solutions for a given dataset and utilizing the relationships observed across the ensemble to identify final solutions, which are more robust, stable or better than the individual solutions within the ensemble. The OpenEnsembles library provides a unified interface for applying transformations to data, clustering data, visualizing individual clustering solutions, visualizing and finishing the ensemble, and calculating validation metrics for a clustering solution for any given partitioning of the data. We have documented examples of using OpenEnsembles to create, analyze, and visualize a number of different types of ensemble approaches on toy and example datasets. OpenEnsembles is released under the GNU General Public License version 3, can be installed via Conda or the Python Package Index (pip), and is available at https://github.com/NaegleLab/OpenEnsembles",
    "volume": "MLOSS",
    "checked": true,
    "id": "8a02122e3af3fe4b73ca29cf97efe4f622cab2c4",
    "citation_count": 19
  },
  "https://jmlr.org/papers/v19/16-241.html": {
    "title": "Importance Sampling for Minibatches",
    "abstract": "Minibatching is a very well studied and highly popular technique in supervised learning, used by practitioners due to its ability to accelerate training through better utilization of parallel processing power and reduction of stochastic variance. Another popular technique is importance sampling–a strategy for preferential sampling of more important examples also capable of accelerating the training process. However, despite considerable effort by the community in these areas, and due to the inherent technical difficulty of the problem, there is virtually no existing work combining the power of importance sampling with the strength of minibatching. In this paper we propose the first practical importance sampling for minibatches and give simple and rigorous complexity analysis of its performance. We illustrate on synthetic problems that for training data of certain properties, our sampling can lead to several orders of magnitude improvement in training time. We then test the new sampling on several popular data sets, and show that the improvement can reach an order of magnitude",
    "volume": "main",
    "checked": true,
    "id": "5eecb1357f1a81df2196240126e69c899ef259e8",
    "citation_count": 105
  },
  "https://jmlr.org/papers/v19/16-412.html": {
    "title": "Generalized Rank-Breaking: Computational and Statistical Tradeoffs",
    "abstract": "For massive and heterogeneous modern datasets, it is of fundamental interest to provide guarantees on the accuracy of estimation when computational resources are limited. In the application of rank aggregation, for the Plackett-Luce model, we provide a hierarchy of rank-breaking mechanisms ordered by the complexity in thus generated sketch of the data. This allows the number of data points collected to be gracefully traded off against computational resources available, while guaranteeing the desired level of accuracy. Theoretical guarantees on the proposed generalized rank-breaking implicitly provide such trade-offs, which can be explicitly characterized under certain canonical scenarios on the structure of the data. Further, the proposed generalized rank-breaking algorithm involves set-wise comparisons as opposed to traditional pairwise comparisons. The maximum likelihood estimate of pairwise comparisons is computed efficiently using the celebrated minorization maximization algorithm (Hunter, 2004). To compute the pseudo-maximum likelihood estimate of the set-wise comparisons, we provide a generalization of the minorization maximization algorithm and give guarantees on its convergence",
    "volume": "main",
    "checked": true,
    "id": "bbf3ff5e523487c5da75fd7633c6a22fb6626f96",
    "citation_count": 5
  },
  "https://jmlr.org/papers/v19/16-465.html": {
    "title": "Gradient Descent Learns Linear Dynamical Systems",
    "abstract": "We prove that stochastic gradient descent efficiently converges to the global optimizer of the maximum likelihood objective of an unknown linear time-invariant dynamical system from a sequence of noisy observations generated by the system. Even though the objective function is non-convex, we provide polynomial running time and sample complexity bounds under strong but natural assumptions. Linear systems identification has been studied for many decades, yet, to the best of our knowledge, these are the first polynomial guarantees for the problem we consider",
    "volume": "main",
    "checked": true,
    "id": "8a765725a44b91b60d414551dc555175cfff3cd9",
    "citation_count": 191
  },
  "https://jmlr.org/papers/v19/16-569.html": {
    "title": "Parallelizing Spectrally Regularized Kernel Algorithms",
    "abstract": "We consider a distributed learning approach in supervised learning for a large class of spectral regularization methods in an reproducing kernel Hilbert space (RKHS) framework. The data set of size $n$ is partitioned into $m=O(n^\\alpha)$, $\\alpha < \\frac{1}{2}$, disjoint subsamples. On each subsample, some spectral regularization method (belonging to a large class, including in particular Kernel Ridge Regression, $L^2$-boosting and spectral cut-off) is applied. The regression function $f$ is then estimated via simple averaging, leading to a substantial reduction in computation time. We show that minimax optimal rates of convergence are preserved if $m$ grows sufficiently slowly (corresponding to an upper bound for $\\alpha$) as $n \\to \\infty$, depending on the smoothness assumptions on $f$ and the intrinsic dimensionality. In spirit, the analysis relies on a classical bias/stochastic error analysis",
    "volume": "main",
    "checked": true,
    "id": "17c27a387ce0c6fa5d1f820db35b6c53b21ac1ec",
    "citation_count": 39
  },
  "https://jmlr.org/papers/v19/17-285.html": {
    "title": "A Direct Approach for Sparse Quadratic Discriminant Analysis",
    "abstract": "Quadratic discriminant analysis (QDA) is a standard tool for classification due to its simplicity and flexibility. Because the number of its parameters scales quadratically with the number of the variables, QDA is not practical, however, when the dimensionality is relatively large. To address this, we propose a novel procedure named DA-QDA for QDA in analyzing high-dimensional data. Formulated in a simple and coherent framework, DA-QDA aims to directly estimate the key quantities in the Bayes discriminant function including quadratic interactions and a linear index of the variables for classification. Under appropriate sparsity assumptions, we establish consistency results for estimating the interactions and the linear index, and further demonstrate that the misclassification rate of our procedure converges to the optimal Bayes risk, even when the dimensionality is exponentially high with respect to the sample size. An efficient algorithm based on the alternating direction method of multipliers (ADMM) is developed for finding interactions, which is much faster than its competitor in the literature. The promising performance of DA-QDA is illustrated via extensive simulation studies and the analysis of four real datasets",
    "volume": "main",
    "checked": true,
    "id": "02219e75a6aa9448e7f39e80ca9c1781c059122c",
    "citation_count": 29
  },
  "https://jmlr.org/papers/v19/17-537.html": {
    "title": "Distribution-Specific Hardness of Learning Neural Networks",
    "abstract": "Although neural networks are routinely and successfully trained in practice using simple gradient-based methods, most existing theoretical results are negative, showing that learning such networks is difficult, in a worst-case sense over all data distributions. In this paper, we take a more nuanced view, and consider whether specific assumptions on the ânicenessâ of the input distribution, or ânicenessâ of the target function (e.g. in terms of smoothness, non-degeneracy, incoherence, random choice of parameters etc.), are sufficient to guarantee learnability using gradient-based methods. We provide evidence that neither class of assumptions alone is sufficient: On the one hand, for any member of a class of âniceâ target functions, there are difficult input distributions. On the other hand, we identify a family of simple target functions, which are difficult to learn even if the input distribution is âniceâ. To prove our results, we develop some tools which may be of independent interest, such as extending Fourier-based hardness techniques developed in the context of statistical queries (Blum et al., 1994), from the Boolean cube to Euclidean space and to more general classes of functions",
    "volume": "main",
    "checked": true,
    "id": "0e1a7fc4ec20c16d7b2ecb30dfd2c0df1dfe8ced",
    "citation_count": 106
  },
  "https://jmlr.org/papers/v19/17-624.html": {
    "title": "Goodness-of-Fit Tests for Random Partitions via Symmetric Polynomials",
    "abstract": "We consider goodness-of-fit tests with i.i.d. samples generated from a categorical distribution $(p_1,...,p_k)$. For a given $(q_1,...,q_k)$, we test the null hypothesis whether $p_j=q_{\\pi(j)}$ for some label permutation $\\pi$. The uncertainty of label permutation implies that the null hypothesis is composite instead of being singular. In this paper, we construct a testing procedure using statistics that are defined as indefinite integrals of some symmetric polynomials. This method is aimed directly at the invariance of the problem, and avoids the need of matching the unknown labels. The asymptotic distribution of the testing statistic is shown to be chi-squared, and its power is proved to be nearly optimal under a local alternative hypothesis. Various degenerate structures of the null hypothesis are carefully analyzed in the paper. A two-sample version of the test is also studied",
    "volume": "main",
    "checked": true,
    "id": "646efc06d9636255b8f3391779334cd18ff2b2fd",
    "citation_count": 2
  },
  "https://jmlr.org/papers/v19/17-735.html": {
    "title": "A Spectral Approach for the Design of Experiments: Design, Analysis and Algorithms",
    "abstract": "This paper proposes a new approach to construct high quality space-filling sample designs. First, we propose a novel technique to quantify the space-filling property and optimally trade-off uniformity and randomness in sample designs in arbitrary dimensions. Second, we connect the proposed metric (defined in the spatial domain) to the quality metric of the design performance (defined in the spectral domain). This connection serves as an analytic framework for evaluating the qualitative properties of space-filling designs in general. Using the theoretical insights provided by this spatial-spectral analysis, we derive the notion of optimal space-filling designs, which we refer to as space-filling spectral designs. Third, we propose an efficient estimator to evaluate the space-filling properties of sample designs in arbitrary dimensions and use it to develop an optimization framework for generating high quality space-filling designs. Finally, we carry out a detailed performance comparison on two different applications in varying dimensions: a) image reconstruction and b) surrogate modeling for several benchmark optimization functions and a physics simulation code for inertial confinement fusion (ICF). Our results clearly evidence the superiority of the proposed space-filling designs over existing approaches, particularly in high dimensions",
    "volume": "main",
    "checked": true,
    "id": "79c6713c41b4fedf9c7454b7e2bb48d0aeb1ae0f",
    "citation_count": 18
  },
  "https://jmlr.org/papers/v19/16-349.html": {
    "title": "Kernel Density Estimation for Dynamical Systems",
    "abstract": "We study the density estimation problem with observations generated by certain dynamical systems that admit a unique underlying invariant Lebesgue density. Observations drawn from dynamical systems are not independent and moreover, usual mixing concepts may not be appropriate for measuring the dependence among these observations. By employing the $\\mathcal{C}$-mixing concept to measure the dependence, we conduct statistical analysis on the consistency and convergence of the kernel density estimator. Our main results are as follows: First, we show that with properly chosen bandwidth, the kernel density estimator is universally consistent under $L_1$-norm; Second, we establish convergence rates for the estimator with respect to several classes of dynamical systems under $L_1$-norm. In the analysis, the density function $f$ is only assumed to be H\\\"{o}lder continuous or pointwise H\\\"{o}lder controllable which is a weak assumption in the literature of nonparametric density estimation and also more realistic in the dynamical system context. Last but not least, we prove that the same convergence rates of the estimator under $L_\\infty$-norm and $L_1$-norm can be achieved when the density function is H\\\"{o}lder continuous, compactly supported, and bounded. The bandwidth selection problem of the kernel density estimator for dynamical system is also discussed in our study via numerical simulations",
    "volume": "main",
    "checked": true,
    "id": "5f5cf1b4bb21e01b7d8e062ea6a82b45f5fc9832",
    "citation_count": 16
  },
  "https://jmlr.org/papers/v19/16-432.html": {
    "title": "Invariant Models for Causal Transfer Learning",
    "abstract": "Methods of transfer learning try to combine knowledge from several related tasks (or domains) to improve performance on a test task. Inspired by causal methodology, we relax the usual covariate shift assumption and assume that it holds true for a subset of predictor variables: the conditional distribution of the target variable given this subset of predictors is invariant over all tasks. We show how this assumption can be motivated from ideas in the field of causality. We focus on the problem of Domain Generalization, in which no examples from the test task are observed. We prove that in an adversarial setting using this subset for prediction is optimal in Domain Generalization; we further provide examples, in which the tasks are sufficiently diverse and the estimator therefore outperforms pooling the data, even on average. If examples from the test task are available, we also provide a method to transfer knowledge from the training tasks and exploit all available features for prediction. However, we provide no guarantees for this method. We introduce a practical method which allows for automatic inference of the above subset and provide corresponding code. We present results on synthetic data sets and a gene deletion data set",
    "volume": "main",
    "checked": true,
    "id": "1a60d4122ef0ac6972ef9b4a3752ac1657de482c",
    "citation_count": 261
  },
  "https://jmlr.org/papers/v19/16-515.html": {
    "title": "The xyz algorithm for fast interaction search in high-dimensional data",
    "abstract": "When performing regression on a data set with $p$ variables, it is often of interest to go beyond using main linear effects and include interactions as products between individual variables. For small-scale problems, these interactions can be computed explicitly but this leads to a computational complexity of at least $\\mathcal{O}(p^2)$ if done naively. This cost can be prohibitive if $p$ is very large. We introduce a new randomised algorithm that is able to discover interactions with high probability and under mild conditions has a runtime that is subquadratic in $p$. We show that strong interactions can be discovered in almost linear time, whilst finding weaker interactions requires $\\mathcal{O}(p^\\alpha)$ operations for $1<\\alpha<2$ depending on their strength. The underlying idea is to transform interaction search into a closest pair problem which can be solved efficiently in subquadratic time. The algorithm is called $xyz$ and is implemented in the language R. We demonstrate its efficiency for application to genome-wide association studies, where more than $10^{11}$ interactions can be screened in under $280$ seconds with a single-core $1.2$ GHz CPU",
    "volume": "main",
    "checked": true,
    "id": "842a864eca199871e4dd99821d1586568637d4e4",
    "citation_count": 22
  },
  "https://jmlr.org/papers/v19/17-144.html": {
    "title": "Local Rademacher Complexity-based Learning Guarantees for Multi-Task Learning",
    "abstract": "We show a Talagrand-type concentration inequality for Multi-Task Learning (MTL), with which we establish sharp excess risk bounds for MTL in terms of the Local Rademacher Complexity (LRC). We also give a new bound on the (LRC) for any norm regularized hypothesis classes, which applies not only to MTL, but also to the standard Single-Task Learning (STL) setting. By combining both results, one can easily derive fast-rate bounds on the excess risk for many prominent MTL methods, including–as we demonstrate–Schatten norm, group norm, and graph regularized MTL. The derived bounds reflect a relationship akin to a conservation law of asymptotic convergence rates. When compared to the rates obtained via a traditional, global Rademacher analysis, this very relationship allows for trading off slower rates with respect to the number of tasks for faster rates with respect to the number of available samples per task",
    "volume": "main",
    "checked": true,
    "id": "90505b029bb710efa0278188fdc611cad6f10dcc",
    "citation_count": 15
  },
  "https://jmlr.org/papers/v19/17-345.html": {
    "title": "State-by-state Minimax Adaptive Estimation for Nonparametric Hidden {M}arkov Models",
    "abstract": "In this paper, we introduce a new estimator for the emission densities of a nonparametric hidden Markov model. It is adaptive and minimax with respect to each state's regularity–as opposed to globally minimax estimators, which adapt to the worst regularity among the emission densities. Our method is based on Goldenshluger and Lepski's methodology. It is computationally efficient and only requires a family of preliminary estimators, without any restriction on the type of estimators considered. We present two such estimators that allow to reach minimax rates up to a logarithmic term: a spectral estimator and a least squares estimator. We show how to calibrate it in practice and assess its performance on simulations and on real data",
    "volume": "main",
    "checked": false,
    "id": "7a841ce039429fdb3aace3bd99def017dcd0ac1b",
    "citation_count": 9
  },
  "https://jmlr.org/papers/v19/17-607.html": {
    "title": "Learning from Comparisons and Choices",
    "abstract": "When tracking user-specific online activities, each user's preference is revealed in the form of choices and comparisons. For example, a user's purchase history is a record of her choices, i.e. which item was chosen among a subset of offerings. A user's preferences can be observed either explicitly as in movie ratings or implicitly as in viewing times of news articles. Given such individualized ordinal data in the form of comparisons and choices, we address the problem of collaboratively learning representations of the users and the items. The learned features can be used to predict a user's preference of an unseen item to be used in recommendation systems. This also allows one to compute similarities among users and items to be used for categorization and search. Motivated by the empirical successes of the MultiNomial Logit (MNL) model in marketing and transportation, and also more recent successes in word embedding and crowdsourced image embedding, we pose this problem as learning the MNL model parameters that best explain the data. We propose a convex relaxation for learning the MNL model, and show that it is minimax optimal up to a logarithmic factor by comparing its performance to a fundamental lower bound. This characterizes the minimax sample complexity of the problem, and proves that the proposed estimator cannot be improved upon other than by a logarithmic factor. Further, the analysis identifies how the accuracy depends on the topology of sampling via the spectrum of the sampling graph. This provides a guideline for designing surveys when one can choose which items are to be compared. This is accompanied by numerical simulations on synthetic and real data sets, confirming our theoretical predictions",
    "volume": "main",
    "checked": true,
    "id": "49643375881ee50d02935c335ab209226387fc1d",
    "citation_count": 33
  },
  "https://jmlr.org/papers/v19/17-704.html": {
    "title": "Connections with Robust PCA and the Role of Emergent Sparsity in Variational Autoencoder Models",
    "abstract": "Variational autoencoders (VAE) represent a popular, flexible form of deep generative model that can be stochastically fit to samples from a given random process using an information-theoretic variational bound on the true underlying distribution. Once so-obtained, the model can be putatively used to generate new samples from this distribution, or to provide a low-dimensional latent representation of existing samples. While quite effective in numerous application domains, certain important mechanisms which govern the behavior of the VAE are obfuscated by the intractable integrals and resulting stochastic approximations involved. Moreover, as a highly non-convex model, it remains unclear exactly how minima of the underlying energy relate to original design purposes. We attempt to better quantify these issues by analyzing a series of tractable special cases of increasing complexity. In doing so, we unveil interesting connections with more traditional dimensionality reduction models, as well as an intrinsic yet underappreciated propensity for robustly dismissing sparse outliers when estimating latent manifolds. With respect to the latter, we demonstrate that the VAE can be viewed as the natural evolution of recent robust PCA models, capable of learning nonlinear manifolds of unknown dimension obscured by gross corruptions",
    "volume": "main",
    "checked": true,
    "id": "ce73e21ad34832bd17648cae483b960fc8c47644",
    "citation_count": 71
  },
  "https://jmlr.org/papers/v19/18-117.html": {
    "title": "An Efficient and Effective Generic Agglomerative Hierarchical Clustering Approach",
    "abstract": "We introduce an agglomerative hierarchical clustering (AHC) framework which is generic, efficient and effective. Our approach embeds a sub-family of Lance-Williams (LW) clusterings and relies on inner-products instead of squared Euclidean distances. We carry out a constrained bottom-up merging procedure on a sparsified normalized inner-product matrix. Our method is named SNK-AHC for Sparsified Normalized Kernel matrix based AHC. SNK-AHC is more scalable than the classic dissimilarity matrix based AHC. It can also produce better results when clusters have arbitrary shapes. Artificial and real-world benchmarks are used to exemplify these points. From a theoretical standpoint, SNK-AHC provides another interpretation of the classic techniques which relies on the concept of weighted penalized similarities. The differences between group average, Mcquitty, centroid, median and Ward, can be explained by their distinct averaging strategies for aggregating clusters inter-similarities and intra-similarities. Other features of SNK-AHC are examined. We provide sufficient conditions in order to have monotonic dendrograms, we elaborate a stored data matrix approach for centroid and median, we underline the diagonal translation invariance property of group average, Mcquitty and Ward and we show to what extent SNK-AHC can determine the number of clusters",
    "volume": "main",
    "checked": true,
    "id": "5dce9a740a8a187ec0608faa25521d1b51b8743c",
    "citation_count": 6
  },
  "https://jmlr.org/papers/v19/14-033.html": {
    "title": "Markov Blanket and Markov Boundary of Multiple Variables",
    "abstract": "Markov blanket (Mb) and Markov boundary (MB) are two key concepts in Bayesian networks (BNs). In this paper, we study the problem of Mb and MB for multiple variables. First, we show that Mb possesses the additivity property under the local intersection assumption, that is, an Mb of multiple targets can be constructed by simply taking the union of Mbs of the individual targets and removing the targets themselves. MB is also proven to have additivity under the local intersection assumption. Second, we analyze the cases of violating additivity of Mb and MB and then put forward the notions of Markov blanket supplementary (MbS) and Markov boundary supplementary (MBS). The properties of MbS and MBS are studied in detail. Third, we build two MB discovery algorithms and prove their correctness under the local composition assumption. We also discuss the ways of practically doing conditional independence tests and analyze the complexities of the algorithms. Finally, we make a benchmarking study based on six synthetic BNs and then apply MB discovery to multi-class prediction based on a real data set. The experimental results reveal our algorithms have higher accuracies and lower complexities than existing algorithms",
    "volume": "main",
    "checked": true,
    "id": "62dc6551a94736e7d33d6dc54d874917ab083ef4",
    "citation_count": 8
  },
  "https://jmlr.org/papers/v19/16-291.html": {
    "title": "Kernel Distribution Embeddings: Universal Kernels, Characteristic Kernels and Kernel Metrics on Distributions",
    "abstract": "Kernel mean embeddings have become a popular tool in machine learning. They map probability measures to functions in a reproducing kernel Hilbert space. The distance between two mapped measures defines a semi-distance over the probability measures known as the maximum mean discrepancy (MMD). Its properties depend on the underlying kernel and have been linked to three fundamental concepts of the kernel literature: universal, characteristic and strictly positive definite kernels. The contributions of this paper are three-fold. First, by slightly extending the usual definitions of universal, characteristic and strictly positive definite kernels, we show that these three concepts are essentially equivalent. Second, we give the first complete characterization of those kernels whose associated MMD-distance metrizes the weak convergence of probability measures. Third, we show that kernel mean embeddings can be extended from probability measures to generalized measures called Schwartz-distributions and analyze a few properties of these distribution embeddings",
    "volume": "main",
    "checked": true,
    "id": "9cd04aee5dac5b6f48d178a44f1d7b74c5f8a9a7",
    "citation_count": 70
  },
  "https://jmlr.org/papers/v19/16-474.html": {
    "title": "Random Forests, Decision Trees, and Categorical Predictors: The \"Absent Levels\" Problem",
    "abstract": "One advantage of decision tree based methods like random forests is their ability to natively handle categorical predictors without having to first transform them (e.g., by using feature engineering techniques). However, in this paper, we show how this capability can lead to an inherent âabsent levelsâ problem for decision tree based methods that has never been thoroughly discussed, and whose consequences have never been carefully explored. This problem occurs whenever there is an indeterminacy over how to handle an observation that has reached a categorical split which was determined when the observation in question's level was absent during training. Although these incidents may appear to be innocuous, by using Leo Breiman and Adele Cutler's random forests FORTRAN code and the randomForest R package  (Liaw and Wiener, 2002) as motivating case studies, we examine how overlooking the absent levels problem can systematically bias a model. Furthermore, by using three real data examples, we illustrate how absent levels can dramatically alter a model's performance in practice, and we empirically demonstrate how some simple heuristics can be used to help mitigate the effects of the absent levels problem until a more robust theoretical solution is found",
    "volume": "main",
    "checked": true,
    "id": "d1f56f2add1da28eeafc57ebad96bc18a1209584",
    "citation_count": 35
  },
  "https://jmlr.org/papers/v19/17-025.html": {
    "title": "On Tight Bounds for the Lasso",
    "abstract": "We present upper and lower bounds for the prediction error of the Lasso. For the case of random Gaussian design, we show that under mild conditions the prediction error of the Lasso is up to smaller order terms dominated by the prediction error of its noiseless counterpart. We then provide exact expressions for the prediction error of the latter, in terms of compatibility constants. Here, we assume the active components of the underlying regression function satisfy some âbetamin\" condition. For the case of fixed design, we provide upper and lower bounds, again in terms of compatibility constants. As an example, we give an up to a logarithmic term tight bound for the least squares estimator with total variation penalty",
    "volume": "main",
    "checked": true,
    "id": "8baf8085e6444f28109fffa34919096c881ab992",
    "citation_count": 15
  },
  "https://jmlr.org/papers/v19/17-244.html": {
    "title": "Harmonic Mean Iteratively Reweighted Least Squares for Low-Rank Matrix Recovery",
    "abstract": "We propose a new iteratively reweighted least squares (IRLS) algorithm for the recovery of a matrix $X \\in \\mathbb{C}^{d_1\\times d_2}$ of rank $r \\ll\\min(d_1,d_2)$ from incomplete linear observations, solving a sequence of low complexity linear problems. The easily implementable algorithm, which we call harmonic mean iteratively reweighted least squares (HM-IRLS), optimizes a non-convex Schatten-$p$ quasi-norm penalization to promote low-rankness and carries three major strengths, in particular for the matrix completion setting. First, we observe a remarkable {global convergence behavior} of the algorithm's iterates to the low-rank matrix for relevant, interesting cases, for which any other state-of-the-art optimization approach fails the recovery. Secondly, HM-IRLS exhibits an empirical recovery probability close to $1$ even for a number of measurements very close to the theoretical lower bound $r (d_1 +d_2 -r)$, i.e., already for significantly fewer linear observations than any other tractable approach in the literature. Thirdly, HM-IRLS exhibits a locally superlinear rate of convergence (of order $2-p$) if the linear observations fulfill a suitable null space property. While for the first two properties we have so far only strong empirical evidence, we prove the third property as our main theoretical result",
    "volume": "main",
    "checked": true,
    "id": "fa2e9266298690b5ea51e0e27788637f7153bcc3",
    "citation_count": 26
  }
}