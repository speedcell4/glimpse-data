{
  "https://openreview.net/forum?id=B1gabhRcYX": {
    "title": "BA-Net: Dense Bundle Adjustment Networks",
    "volume": "oral",
    "abstract": "",
    "checked": false,
    "id": "4ba326228728c7ea03a4de6778d0b428b035d5cd",
    "semantic_title": "ba-net: dense bundle adjustment network",
    "citation_count": 289,
    "authors": []
  },
  "https://openreview.net/forum?id=HygBZnRctX": {
    "title": "Transferring Knowledge across Learning Processes",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "0c37a1feac9a09a2fa5554cb7ff735d4bcd6ed5b",
    "semantic_title": "transferring knowledge across learning processes",
    "citation_count": 64,
    "authors": []
  },
  "https://openreview.net/forum?id=rJl-b3RcF7": {
    "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "21937ecd9d66567184b83eca3d3e09eb4e6fbd60",
    "semantic_title": "the lottery ticket hypothesis: finding sparse, trainable neural networks",
    "citation_count": 3489,
    "authors": []
  },
  "https://openreview.net/forum?id=SkVhlh09tX": {
    "title": "Pay Less Attention with Lightweight and Dynamic Convolutions",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "fea820b7d953d32069e189af2961c28fd213470b",
    "semantic_title": "pay less attention with lightweight and dynamic convolutions",
    "citation_count": 610,
    "authors": []
  },
  "https://openreview.net/forum?id=S1x4ghC9tQ": {
    "title": "Temporal Difference Variational Auto-Encoder",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "9d671a4de50b98c3f00623ee597e37c9f00ba0cc",
    "semantic_title": "temporal difference variational auto-encoder",
    "citation_count": 127,
    "authors": []
  },
  "https://openreview.net/forum?id=rJgMlhRctm": {
    "title": "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "50f76736c3090c6effac25400e5e40cc0b7b5ad9",
    "semantic_title": "the neuro-symbolic concept learner: interpreting scenes, words, and sentences from natural supervision",
    "citation_count": 704,
    "authors": []
  },
  "https://openreview.net/forum?id=Byg3y3C9Km": {
    "title": "Learning Protein Structure with a Differentiable Simulator",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "5909909bd3f6d6d15d5c8d9a64c8031cddf604db",
    "semantic_title": "learning protein structure with a differentiable simulator",
    "citation_count": 148,
    "authors": []
  },
  "https://openreview.net/forum?id=rJxgknCcK7": {
    "title": "FFJORD: Free-Form Continuous Dynamics for Scalable Reversible Generative Models",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "8afa6dd9f9ac46462a1fb70a757c4ae1cd45bbf6",
    "semantic_title": "ffjord: free-form continuous dynamics for scalable reversible generative models",
    "citation_count": 882,
    "authors": []
  },
  "https://openreview.net/forum?id=r1lYRjC9F7": {
    "title": "Enabling Factorized Piano Music Modeling and Generation with the MAESTRO Dataset",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "2603a68b4503ba949c91c7e00cd342624b4aae2f",
    "semantic_title": "enabling factorized piano music modeling and generation with the maestro dataset",
    "citation_count": 452,
    "authors": []
  },
  "https://openreview.net/forum?id=ryGs6iA5Km": {
    "title": "How Powerful are Graph Neural Networks?",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "62ed9bf1d83c8db1f9cbf92ea2f57ea90ef683d9",
    "semantic_title": "how powerful are graph neural networks?",
    "citation_count": 7711,
    "authors": []
  },
  "https://openreview.net/forum?id=HylzTiC5Km": {
    "title": "GENERATING HIGH FIDELITY IMAGES WITH SUBSCALE PIXEL NETWORKS AND MULTIDIMENSIONAL UPSCALING",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "e8fd01d3a2ae47827cc8e008d658bca830d99415",
    "semantic_title": "generating high fidelity images with subscale pixel networks and multidimensional upscaling",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S1xq3oR5tQ": {
    "title": "A Unified Theory of Early Visual Representations from Retina to Cortex through Anatomically Constrained Deep CNNs",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "ad0466d3f1c128691416bab7244ace11a6716c7d",
    "semantic_title": "a unified theory of early visual representations from retina to cortex through anatomically constrained deep cnns",
    "citation_count": 72,
    "authors": []
  },
  "https://openreview.net/forum?id=Bklr3j0cKX": {
    "title": "Learning deep representations by mutual information estimation and maximization",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "af3825437b627db1a99f946f7aa773ba8b03befd",
    "semantic_title": "learning deep representations by mutual information estimation and maximization",
    "citation_count": 2675,
    "authors": []
  },
  "https://openreview.net/forum?id=rJEjjoR9K7": {
    "title": "Learning Robust Representations by Projecting Superficial Statistics Out",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "96b32b204a62777bef66eea595de2c47b4e9d6e9",
    "semantic_title": "learning robust representations by projecting superficial statistics out",
    "citation_count": 238,
    "authors": []
  },
  "https://openreview.net/forum?id=HkNDsiC9KQ": {
    "title": "Meta-Learning Update Rules for Unsupervised Representation Learning",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "c6509a450bdda7ca5b8567103dfe9671dbf3b567",
    "semantic_title": "meta-learning update rules for unsupervised representation learning",
    "citation_count": 123,
    "authors": []
  },
  "https://openreview.net/forum?id=B1l6qiR5F7": {
    "title": "Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "15d6f3d815d0ff176fafb14a3f46e5723ebac723",
    "semantic_title": "ordered neurons: integrating tree structures into recurrent neural networks",
    "citation_count": 325,
    "authors": []
  },
  "https://openreview.net/forum?id=Bygh9j09KX": {
    "title": "ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "0f50b7483f1b200ebf88c4dd7698de986399a0f3",
    "semantic_title": "imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness",
    "citation_count": 2678,
    "authors": []
  },
  "https://openreview.net/forum?id=B1xsqj09Fm": {
    "title": "Large Scale GAN Training for High Fidelity Natural Image Synthesis",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "22aab110058ebbd198edb1f1e7b4f69fb13c0613",
    "semantic_title": "large scale gan training for high fidelity natural image synthesis",
    "citation_count": 5412,
    "authors": []
  },
  "https://openreview.net/forum?id=ByeZ5jC5YQ": {
    "title": "KnockoffGAN: Generating Knockoffs for Feature Selection using Generative Adversarial Networks",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "7b767e7cb5b67589646f0c1162423ceeb2daa3a7",
    "semantic_title": "knockoffgan: generating knockoffs for feature selection using generative adversarial networks",
    "citation_count": 58,
    "authors": []
  },
  "https://openreview.net/forum?id=r1xlvi0qYm": {
    "title": "Learning to Remember More with Less Memorization",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "dc836eda6649fa9e0534a7ba4ddafd97a579926b",
    "semantic_title": "learning to remember more with less memorization",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=B1l08oAct7": {
    "title": "Deterministic Variational Inference for Robust Bayesian Neural Networks",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "51bb7f2681605651a4df725ac6c9e18377d36ac5",
    "semantic_title": "deterministic variational inference for robust bayesian neural networks",
    "citation_count": 165,
    "authors": []
  },
  "https://openreview.net/forum?id=rJVorjCcKQ": {
    "title": "Slalom: Fast, Verifiable and Private Execution of Neural Networks in Trusted Hardware",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "662f3ca1f074e24803d33fcd6c7d19564de107f2",
    "semantic_title": "slalom: fast, verifiable and private execution of neural networks in trusted hardware",
    "citation_count": 401,
    "authors": []
  },
  "https://openreview.net/forum?id=HJx54i05tX": {
    "title": "On Random Deep Weight-Tied Autoencoders: Exact Asymptotic Analysis, Phase Transitions, and Implications to Training",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "5a0772885ed113e9da7aa643c877feea387d160e",
    "semantic_title": "on random deep weight-tied autoencoders: exact asymptotic analysis, phase transitions, and implications to training",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=H1xSNiRcF7": {
    "title": "Smoothing the Geometry of Probabilistic Box Embeddings",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "63bcdc392c7135ccc2ded4cd53ef4e2f65ba80a0",
    "semantic_title": "smoothing the geometry of probabilistic box embeddings",
    "citation_count": 85,
    "authors": []
  },
  "https://openreview.net/forum?id=rJl0r3R9KX": {
    "title": "Regularized Learning for Domain Adaptation under Label Shifts",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6d64363e52cd7ecad99d7ce6ae849f245dfbbf92",
    "semantic_title": "regularized learning for domain adaptation under label shifts",
    "citation_count": 208,
    "authors": []
  },
  "https://openreview.net/forum?id=SylCrnCcFX": {
    "title": "Towards Robust, Locally Linear Deep Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a403099192180450edc594d2fed7a19296e9ff48",
    "semantic_title": "towards robust, locally linear deep networks",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=HylTBhA5tQ": {
    "title": "The Limitations of Adversarial Training and the Blind-Spot Attack",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e749e8c947550485eddf864f8efeb870b894e4ce",
    "semantic_title": "the limitations of adversarial training and the blind-spot attack",
    "citation_count": 145,
    "authors": []
  },
  "https://openreview.net/forum?id=B1gTShAct7": {
    "title": "Learning to Learn without Forgetting by Maximizing Transfer and Minimizing Interference",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2b877889ac31b73d1ede70b00eb4c7118ef8eca2",
    "semantic_title": "learning to learn without forgetting by maximizing transfer and minimizing interference",
    "citation_count": 790,
    "authors": []
  },
  "https://openreview.net/forum?id=ryxnHhRqFm": {
    "title": "Global-to-local Memory Pointer Networks for Task-Oriented Dialogue",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "924acc8b76779a5211cc04bbe6cf13bd0bc6e7f8",
    "semantic_title": "global-to-local memory pointer networks for task-oriented dialogue",
    "citation_count": 168,
    "authors": []
  },
  "https://openreview.net/forum?id=rJlnB3C5Ym": {
    "title": "Rethinking the Value of Network Pruning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4a1004ecd34118116344633c7cdcc34493c423ee",
    "semantic_title": "rethinking the value of network pruning",
    "citation_count": 1480,
    "authors": []
  },
  "https://openreview.net/forum?id=ByzcS3AcYX": {
    "title": "Neural TTS Stylization with Adversarial and Collaborative Games",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c978ee204c0f6ab6e1a8c635a93d16612927e24b",
    "semantic_title": "neural tts stylization with adversarial and collaborative games",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=SJe9rh0cFX": {
    "title": "On the Universal Approximability and Complexity Bounds of Quantized ReLU Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "169b173ed1a397e8b47d9dbac7eefb770cb30ce9",
    "semantic_title": "on the universal approximability and complexity bounds of quantized relu neural networks",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=Ske5r3AqK7": {
    "title": "Poincare Glove: Hyperbolic Word Embeddings",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "e8fa823c17aeb8d08fe9aa5fc2bc0eaacb9edcdf",
    "semantic_title": "poincar√© glove: hyperbolic word embeddings",
    "citation_count": 288,
    "authors": []
  },
  "https://openreview.net/forum?id=B1lKS2AqtX": {
    "title": "Eidetic 3D LSTM: A Model for Video Prediction and Beyond",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "82a646e1cb33124f672beba451f5039e9e32fb6d",
    "semantic_title": "eidetic 3d lstm: a model for video prediction and beyond",
    "citation_count": 361,
    "authors": []
  },
  "https://openreview.net/forum?id=HkxKH2AcFm": {
    "title": "Towards GAN Benchmarks Which Require Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c7dbce9513f4fc902da181d6630995627ba83192",
    "semantic_title": "towards gan benchmarks which require generalization",
    "citation_count": 59,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgKBhA5Y7": {
    "title": "There Are Many Consistent Explanations of Unlabeled Data: Why You Should Average",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "98286df6d923d787f26e034bbaf3a5a64ac29cb1",
    "semantic_title": "there are many consistent explanations of unlabeled data: why you should average",
    "citation_count": 244,
    "authors": []
  },
  "https://openreview.net/forum?id=ryeOSnAqYm": {
    "title": "Synthetic Datasets for Neural Program Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a15763582df784b43548c6d53edfd55568c35168",
    "semantic_title": "synthetic datasets for neural program synthesis",
    "citation_count": 46,
    "authors": []
  },
  "https://openreview.net/forum?id=r1xdH3CcKX": {
    "title": "Stochastic Prediction of Multi-Agent Interactions from Partial Observations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "906c03e6e188d301b85ca6521955a8584f9babe7",
    "semantic_title": "stochastic prediction of multi-agent interactions from partial observations",
    "citation_count": 89,
    "authors": []
  },
  "https://openreview.net/forum?id=HyePrhR5KX": {
    "title": "DyRep: Learning Representations over Dynamic Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5593001e49474a475dbcae99be350a8d527c05a3",
    "semantic_title": "dyrep: learning representations over dynamic graphs",
    "citation_count": 520,
    "authors": []
  },
  "https://openreview.net/forum?id=rkxwShA9Ym": {
    "title": "Label super-resolution networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "938bd120c083170c785be0dbb3a67edbb6e5356a",
    "semantic_title": "label super-resolution networks",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=HkfPSh05K7": {
    "title": "Multi-step Retriever-Reader Interaction for Scalable Open-domain Question Answering",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e7512b84e923372ae410d7614e71224d573ed2ef",
    "semantic_title": "multi-step retriever-reader interaction for scalable open-domain question answering",
    "citation_count": 165,
    "authors": []
  },
  "https://openreview.net/forum?id=Byl8BnRcYm": {
    "title": "Capsule Graph Neural Network",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "67de0bc5d1dfb458efc9aa192c879bf00ee667b3",
    "semantic_title": "capsule graph neural network",
    "citation_count": 227,
    "authors": []
  },
  "https://openreview.net/forum?id=Syl8Sn0cK7": {
    "title": "Learning a Meta-Solver for Syntax-Guided Program Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f262ad9e3a84bcea08acd1c97ee9740cd78ee256",
    "semantic_title": "learning a meta-solver for syntax-guided program synthesis",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=H1eSS3CcKX": {
    "title": "Stochastic Optimization of Sorting Networks via Continuous Relaxations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "38e3a9620f575dc16cf26957c3cafa035b8f1895",
    "semantic_title": "stochastic optimization of sorting networks via continuous relaxations",
    "citation_count": 174,
    "authors": []
  },
  "https://openreview.net/forum?id=BylBr3C9K7": {
    "title": "Energy-Constrained Compression for Deep Neural Networks via Weighted Sparse Projection and Layer Input Masking",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0986f2ac6755df5d196ceb09b5bdf19593cbbaef",
    "semantic_title": "energy-constrained compression for deep neural networks via weighted sparse projection and layer input masking",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=rygrBhC5tQ": {
    "title": "Composing Complex Skills by Learning Transition Policies",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "74e12851de2d542aa2aef7b8a39ef021a5802689",
    "semantic_title": "composing complex skills by learning transition policies",
    "citation_count": 92,
    "authors": []
  },
  "https://openreview.net/forum?id=ryxSrhC9KX": {
    "title": "Revealing interpretable object representations from human behavior",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bfd9f718c8e7ec5e3ef47e7cc388fa9918a86525",
    "semantic_title": "revealing interpretable object representations from human behavior",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=HylVB3AqYm": {
    "title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f323407464c4cd492d3fc1afd7170eab08f44d9b",
    "semantic_title": "proxylessnas: direct neural architecture search on target task and hardware",
    "citation_count": 1877,
    "authors": []
  },
  "https://openreview.net/forum?id=r1x4BnCqKX": {
    "title": "A Generative Model For Electron Paths",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6e3e17ac9a5a25ca718bd2da025a7117c4f03634",
    "semantic_title": "a generative model for electron paths",
    "citation_count": 56,
    "authors": []
  },
  "https://openreview.net/forum?id=rylNH20qFQ": {
    "title": "Learning to Infer and Execute 3D Shape Programs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7e2f5eca9465cf114043ed6c95ea59d9dbea45a1",
    "semantic_title": "learning to infer and execute 3d shape programs",
    "citation_count": 147,
    "authors": []
  },
  "https://openreview.net/forum?id=rJe4ShAcF7": {
    "title": "Music Transformer: Generating Music with Long-Term Structure",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "fb507ada871d1e8c29e376dbf7b7879689aa89f9",
    "semantic_title": "music transformer: generating music with long-term structure",
    "citation_count": 486,
    "authors": []
  },
  "https://openreview.net/forum?id=SkgQBn0cF7": {
    "title": "Modeling the Long Term Future in Model-Based Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c4e02b75e525c4e8c4616d2e9dfeb4a638142c51",
    "semantic_title": "modeling the long term future in model-based reinforcement learning",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=HygQBn0cYm": {
    "title": "Model-Predictive Policy Learning with Uncertainty Regularization for Driving in Dense Traffic",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2eeace98cf3c105a8d37884dc8d33c50ae4b7ddb",
    "semantic_title": "model-predictive policy learning with uncertainty regularization for driving in dense traffic",
    "citation_count": 123,
    "authors": []
  },
  "https://openreview.net/forum?id=ByeMB3Act7": {
    "title": "Learning to Screen for Fast Softmax Inference on Large Vocabulary Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "dccf55bad7c065fba1f25e56699584392895b05a",
    "semantic_title": "learning to screen for fast softmax inference on large vocabulary neural networks",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=r1efr3C9Ym": {
    "title": "Interpolation-Prediction Networks for Irregularly Sampled Time Series",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "07334ab67ee57811f32da13588f900e8dc3211c6",
    "semantic_title": "interpolation-prediction networks for irregularly sampled time series",
    "citation_count": 148,
    "authors": []
  },
  "https://openreview.net/forum?id=HyxGB2AcY7": {
    "title": "Contingency-Aware Exploration in Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bf604ae3ddd5adec55554921b37f04035b7350a7",
    "semantic_title": "contingency-aware exploration in reinforcement learning",
    "citation_count": 73,
    "authors": []
  },
  "https://openreview.net/forum?id=BkgWHnR5tm": {
    "title": "Neural Graph Evolution: Towards Efficient Automatic Robot Design",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5b3a45b76e144fed2478419a7229930b28151f46",
    "semantic_title": "neural graph evolution: towards efficient automatic robot design",
    "citation_count": 63,
    "authors": []
  },
  "https://openreview.net/forum?id=Bkxbrn0cYX": {
    "title": "Selfless Sequential Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "974e22699cb165b0285a7c72b5cc6c3d42010143",
    "semantic_title": "selfless sequential learning",
    "citation_count": 114,
    "authors": []
  },
  "https://openreview.net/forum?id=rJgbSn09Ym": {
    "title": "Learning Particle Dynamics for Manipulating Rigid Bodies, Deformable Objects, and Fluids",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d40531ed81859ce40f119c1bbc1d1cb50af498fd",
    "semantic_title": "learning particle dynamics for manipulating rigid bodies, deformable objects, and fluids",
    "citation_count": 399,
    "authors": []
  },
  "https://openreview.net/forum?id=H1zeHnA9KX": {
    "title": "Representing Formal Languages: A Comparison Between Finite Automata and Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9fac39c702724732188ff61090d2e2b498a34eb2",
    "semantic_title": "representing formal languages: a comparison between finite automata and recurrent neural networks",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=B1exrnCcF7": {
    "title": "Disjoint Mapping Network for Cross-modal Matching of Voices and Faces",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4188f289eb85bb047dbbc15acbd79fae6abe25f5",
    "semantic_title": "disjoint mapping network for cross-modal matching of voices and faces",
    "citation_count": 71,
    "authors": []
  },
  "https://openreview.net/forum?id=ByleB2CcKm": {
    "title": "Learning Procedural Abstractions and Evaluating Discrete Latent Temporal Structure",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a92a616aa46f5d06cab7a02c337370ec0e19b4f4",
    "semantic_title": "learning procedural abstractions and evaluating discrete latent temporal structure",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=ByfyHh05tQ": {
    "title": "Learning to Design RNA",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "60b115f0c1ea9565928ebced8d0606f4c31d6640",
    "semantic_title": "learning to design rna",
    "citation_count": 72,
    "authors": []
  },
  "https://openreview.net/forum?id=BygANhA9tQ": {
    "title": "Cost-Sensitive Robustness against Adversarial Examples",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "75d59ae0ed3ce51e37b383985cfff310251f591a",
    "semantic_title": "cost-sensitive robustness against adversarial examples",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=S1lTEh09FQ": {
    "title": "Combinatorial Attacks on Binarized Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e51b580d595a74616272ec3a1a45ed05989a3453",
    "semantic_title": "combinatorial attacks on binarized neural networks",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=r1laEnA5Ym": {
    "title": "A Variational Inequality Perspective on Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "641f0891e63ea033332be6428fa815beaecb61e2",
    "semantic_title": "a variational inequality perspective on generative adversarial networks",
    "citation_count": 353,
    "authors": []
  },
  "https://openreview.net/forum?id=H1g2NhC5KQ": {
    "title": "Multiple-Attribute Text Rewriting",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "49c34076336a5de8e195a9af933b925b5e16d46a",
    "semantic_title": "multiple-attribute text rewriting",
    "citation_count": 236,
    "authors": []
  },
  "https://openreview.net/forum?id=HyGhN2A5tm": {
    "title": "Multi-Agent Dual Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4f00db92bd7c1fd7b41d361c13797b9ff45a5b91",
    "semantic_title": "multi-agent dual learning",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=SJxsV2R5FQ": {
    "title": "Learning sparse relational transition models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b445e8f3d812f59b68f6ed70890d14a882f99b89",
    "semantic_title": "learning sparse relational transition models",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=rkxoNnC5FQ": {
    "title": "SPIGAN: Privileged Adversarial Learning from Simulation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1962791d77a5f7555e8921ab9e92f7cd1fd95736",
    "semantic_title": "spigan: privileged adversarial learning from simulation",
    "citation_count": 105,
    "authors": []
  },
  "https://openreview.net/forum?id=Syx5V2CcFm": {
    "title": "Universal Stagewise Learning for Non-Convex Problems with Convergence on Averaged Solutions",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a048662aab04b4f683cb6778100356892527fb1b",
    "semantic_title": "universal stagewise learning for non-convex problems with convergence on averaged solutions",
    "citation_count": 58,
    "authors": []
  },
  "https://openreview.net/forum?id=HJx9EhC9tQ": {
    "title": "Reasoning About Physical Interactions with Object-Oriented Prediction and Planning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "91cb47c26dffa7e3fcc339abdaab9fa229b37d95",
    "semantic_title": "reasoning about physical interactions with object-oriented prediction and planning",
    "citation_count": 129,
    "authors": []
  },
  "https://openreview.net/forum?id=BkltNhC9FX": {
    "title": "Posterior Attention Models for Sequence to Sequence Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "086cef88cbac0b733e1d0b7d4756600df953851f",
    "semantic_title": "posterior attention models for sequence to sequence learning",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=HJeu43ActQ": {
    "title": "NOODL: Provable Online Dictionary Learning and Sparse Coding",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "23e4f9ee3c7f6d201081cc895ddb5f33476b9058",
    "semantic_title": "noodl: provable online dictionary learning and sparse coding",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=rJedV3R5tm": {
    "title": "RelGAN: Relational Generative Adversarial Networks for Text Generation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0e8b9dc48e4c8be508a7797fd7742506fe59875c",
    "semantic_title": "relgan: relational generative adversarial networks for text generation",
    "citation_count": 176,
    "authors": []
  },
  "https://openreview.net/forum?id=H1xwNhCcYm": {
    "title": "Do Deep Generative Models Know What They Don't Know?",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6507909a8f77c88144c3a67b9336bd1c85e84cac",
    "semantic_title": "do deep generative models know what they don't know?",
    "citation_count": 759,
    "authors": []
  },
  "https://openreview.net/forum?id=BJxvEh0cFQ": {
    "title": "K for the Price of 1: Parameter-efficient Multi-task and Transfer Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "e685efa9ab0672bd83b01cc61e5cc753aa33db68",
    "semantic_title": "k for the price of 1: parameter efficient multi-task and transfer learning",
    "citation_count": 69,
    "authors": []
  },
  "https://openreview.net/forum?id=S1lDV3RcKm": {
    "title": "MisGAN: Learning from Incomplete Data with Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0a869336c65185f078ba473d7ca5b86a371ab929",
    "semantic_title": "misgan: learning from incomplete data with generative adversarial networks",
    "citation_count": 171,
    "authors": []
  },
  "https://openreview.net/forum?id=S1xLN3C9YX": {
    "title": "Learnable Embedding Space for Efficient Neural Architecture Compression",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8111960524288d0e33079be9249d73aab7667c85",
    "semantic_title": "learnable embedding space for efficient neural architecture compression",
    "citation_count": 43,
    "authors": []
  },
  "https://openreview.net/forum?id=HkgSEnA5KQ": {
    "title": "Guiding Policies with Language via Meta-Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "34f5d2f039558ba0b6a5103553ad68321fa6eabd",
    "semantic_title": "guiding policies with language via meta-learning",
    "citation_count": 64,
    "authors": []
  },
  "https://openreview.net/forum?id=HJfSEnRqKQ": {
    "title": "Active Learning with Partial Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "58c04126a5196deb57ae31d6174cd4aae154f138",
    "semantic_title": "active learning with partial feedback",
    "citation_count": 67,
    "authors": []
  },
  "https://openreview.net/forum?id=S1xNEhR9KX": {
    "title": "On the Sensitivity of Adversarial Robustness to Input Data Distributions",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "508dad538d5c63eb6c07fd10794510357a951a58",
    "semantic_title": "on the sensitivity of adversarial robustness to input data distributions",
    "citation_count": 60,
    "authors": []
  },
  "https://openreview.net/forum?id=ByME42AqK7": {
    "title": "Efficient Multi-Objective Neural Architecture Search via Lamarckian Evolution",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "77e5aa8c33a9cb9dd3f0874b09fd84389360e88e",
    "semantic_title": "efficient multi-objective neural architecture search via lamarckian evolution",
    "citation_count": 503,
    "authors": []
  },
  "https://openreview.net/forum?id=SklEEnC5tQ": {
    "title": "DISTRIBUTIONAL CONCAVITY REGULARIZATION FOR GANS",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "81473ba213bae1aa188e03e54f91319914159c9c",
    "semantic_title": "distributional concavity regularization for gans",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=r1g4E3C9t7": {
    "title": "Characterizing Audio Adversarial Examples Using Temporal Dependency",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d5c92fafb030f76d27c749e36550c390c8b3d63b",
    "semantic_title": "characterizing audio adversarial examples using temporal dependency",
    "citation_count": 165,
    "authors": []
  },
  "https://openreview.net/forum?id=H1xQVn09FX": {
    "title": "GANSynth: Adversarial Neural Audio Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "725c650ae8db8d9a57e4a7b15a555dbe69b67054",
    "semantic_title": "gansynth: adversarial neural audio synthesis",
    "citation_count": 392,
    "authors": []
  },
  "https://openreview.net/forum?id=BylQV305YQ": {
    "title": "Toward Understanding the Impact of Staleness in Distributed Machine Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "39f593a6ba742751f46011d97661d20d7437d016",
    "semantic_title": "toward understanding the impact of staleness in distributed machine learning",
    "citation_count": 81,
    "authors": []
  },
  "https://openreview.net/forum?id=r1xX42R5Fm": {
    "title": "Beyond Greedy Ranking: Slate Optimization via List-CVAE",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7fb66090fe4b7e9a56a91bf977e6f40b49d569ac",
    "semantic_title": "beyond greedy ranking: slate optimization via list-cvae",
    "citation_count": 50,
    "authors": []
  },
  "https://openreview.net/forum?id=SyxfEn09Y7": {
    "title": "G-SGD: Optimizing ReLU Neural Networks in its Positively Scale-Invariant Space",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "15cfd11abb010eb2d84dbbe9b80dd80f2e98922d",
    "semantic_title": "g-sgd: optimizing relu neural networks in its positively scale-invariant space",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=Bkl-43C9FQ": {
    "title": "Spherical CNNs on Unstructured Grids",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4aa2c7ab9646b9f988d2dd2e2375060b3158dc2a",
    "semantic_title": "spherical cnns on unstructured grids",
    "citation_count": 184,
    "authors": []
  },
  "https://openreview.net/forum?id=HJgeEh09KQ": {
    "title": "Boosting Robustness Certification of Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4963fe1027aebe63217bd2904decf24f59379e1f",
    "semantic_title": "boosting robustness certification of neural networks",
    "citation_count": 170,
    "authors": []
  },
  "https://openreview.net/forum?id=rJleN20qK7": {
    "title": "Two-Timescale Networks for Nonlinear Value Function Approximation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e4eed0e966c134bbff6a5a2a38ac2b3e44900906",
    "semantic_title": "two-timescale networks for nonlinear value function approximation",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=BJlgNh0qKQ": {
    "title": "Differentiable Perturb-and-Parse: Semi-Supervised Parsing with a Structured Variational Autoencoder",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "29927b4e90810e05c3f329a3a9324d4f6859a93c",
    "semantic_title": "differentiable perturb-and-parse: semi-supervised parsing with a structured variational autoencoder",
    "citation_count": 56,
    "authors": []
  },
  "https://openreview.net/forum?id=BJe1E2R5KX": {
    "title": "Algorithmic Framework for Model-based Deep Reinforcement Learning with Theoretical Guarantees",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d333f99881b09426283a9c7a1d25f7ac30d63062",
    "semantic_title": "algorithmic framework for model-based deep reinforcement learning with theoretical guarantees",
    "citation_count": 227,
    "authors": []
  },
  "https://openreview.net/forum?id=HkzRQhR9YX": {
    "title": "Tree-Structured Recurrent Switching Linear Dynamical Systems for Multi-Scale Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "349f1028e6587604108fd720025cf4b190da2dec",
    "semantic_title": "tree-structured recurrent switching linear dynamical systems for multi-scale modeling",
    "citation_count": 73,
    "authors": []
  },
  "https://openreview.net/forum?id=B1e0X3C9tQ": {
    "title": "Diagnosing and Enhancing VAE Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f6a201eed70e8b48e2f60d97c98cfc8fe3b7b175",
    "semantic_title": "diagnosing and enhancing vae models",
    "citation_count": 381,
    "authors": []
  },
  "https://openreview.net/forum?id=HylTXn0qYX": {
    "title": "Efficiently testing local optimality and escaping saddles for ReLU networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "04a98e1fd56e0d66eeb5f66e5b6a4d310759b3eb",
    "semantic_title": "efficiently testing local optimality and escaping saddles for relu networks",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=HJE6X305Fm": {
    "title": "Don't let your Discriminator be fooled",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1cbbaa7c718cb9abae10955a0ad6dcd3b1e123bb",
    "semantic_title": "don't let your discriminator be fooled",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=B1xhQhRcK7": {
    "title": "Rigorous Agent Evaluation: An Adversarial Approach to Uncover Catastrophic Failures",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5ad2c21ad1c3267e4ce484543bb699d558066d9a",
    "semantic_title": "rigorous agent evaluation: an adversarial approach to uncover catastrophic failures",
    "citation_count": 81,
    "authors": []
  },
  "https://openreview.net/forum?id=Sklsm20ctX": {
    "title": "Competitive experience replay",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ee248883b124a80bdbbc686e906145ccdeadc834",
    "semantic_title": "competitive experience replay",
    "citation_count": 53,
    "authors": []
  },
  "https://openreview.net/forum?id=BJej72AqF7": {
    "title": "A Max-Affine Spline Perspective of Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1bd7d872d54a650affb8b0e1101c0c52e2a912da",
    "semantic_title": "a max-affine spline perspective of recurrent neural networks",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=Byg5QhR5FQ": {
    "title": "Top-Down Neural Model For Formulae",
    "volume": "poster",
    "abstract": "We present a simple neural model that given a formula and a property tries to answer the question whether the formula has the given property, for example whether a propositional formula is always true. The structure of the formula is captured by a feedforward neural network recursively built for the given formula in a top-down manner. The results of this network are then processed by two recurrent neural networks. One of the interesting aspects of our model is how propositional atoms are treated. For example, the model is insensitive to their names, it only matters whether they are the same or distinct",
    "checked": true,
    "id": "f6b394f714ef1b9ace8879356c1db2badd21ba82",
    "semantic_title": "top-down neural model for formulae",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=S1ecm2C9K7": {
    "title": "Feature-Wise Bias Amplification",
    "volume": "poster",
    "abstract": "We study the phenomenon of bias amplification in classifiers, wherein a machine learning model learns to predict classes with a greater disparity than the underlying ground truth. We demonstrate that bias amplification can arise via inductive bias in gradient descent methods resulting in overestimation of importance of moderately-predictive ``weak'' features if insufficient training data is available. This overestimation gives rise to feature-wise bias amplification -- a previously unreported form of bias that can be traced back to the features of a trained model. Through analysis and experiments, we show that the while some bias cannot be mitigated without sacrificing accuracy, feature-wise bias amplification can be mitigated through targeted feature selection. We present two new feature selection algorithms for mitigating bias amplification in linear models, and show how they can be adapted to convolutional neural networks efficiently. Our experiments on synthetic and real data demonstrate that these algorithms consistently lead to reduced bias without harming accuracy, in some cases eliminating predictive bias altogether while providing modest gains in accuracy",
    "checked": true,
    "id": "941ac3f9c1402bbbc1c61f328eb242e99cde5a43",
    "semantic_title": "feature-wise bias amplification",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=HkgYmhR9KX": {
    "title": "AD-VAT: An Asymmetric Dueling mechanism for learning Visual Active Tracking",
    "volume": "poster",
    "abstract": "Visual Active Tracking (VAT) aims at following a target object by autonomously controlling the motion system of a tracker given visual observations. Previous work has shown that the tracker can be trained in a simulator via reinforcement learning and deployed in real-world scenarios. However, during training, such a method requires manually specifying the moving path of the target object to be tracked, which cannot ensure the tracker's generalization on the unseen object moving patterns. To learn a robust tracker for VAT, in this paper, we propose a novel adversarial RL method which adopts an Asymmetric Dueling mechanism, referred to as AD-VAT. In AD-VAT, both the tracker and the target are approximated by end-to-end neural networks, and are trained via RL in a dueling/competitive manner: i.e., the tracker intends to lockup the target, while the target tries to escape from the tracker. They are asymmetric in that the target is aware of the tracker, but not vice versa. Specifically, besides its own observation, the target is fed with the tracker's observation and action, and learns to predict the tracker's reward as an auxiliary task. We show that such an asymmetric dueling mechanism produces a stronger target, which in turn induces a more robust tracker. To stabilize the training, we also propose a novel partial zero-sum reward for the tracker/target. The experimental results, in both 2D and 3D environments, demonstrate that the proposed method leads to a faster convergence in training and yields more robust tracking behaviors in different testing scenarios. For supplementary videos, see: https://www.youtube.com/playlist?list=PL9rZj4Mea7wOZkdajK1TsprRg8iUf51BS The code is available at https://github.com/zfw1226/active_tracking_rl",
    "checked": true,
    "id": "1433d442139fd876001093f5a02fdf9f4f07425a",
    "semantic_title": "ad-vat: an asymmetric dueling mechanism for learning visual active tracking",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=BJfOXnActQ": {
    "title": "Learning to Learn with Conditional Class Dependencies",
    "volume": "poster",
    "abstract": "Neural networks can learn to extract statistical properties from data, but they seldom make use of structured information from the label space to help representation learning. Although some label structure can implicitly be obtained when training on huge amounts of data, in a few-shot learning context where little data is available, making explicit use of the label structure can inform the model to reshape the representation space to reflect a global sense of class dependencies. We propose a meta-learning framework, Conditional class-Aware Meta-Learning (CAML), that conditionally transforms feature representations based on a metric space that is trained to capture inter-class dependencies. This enables a conditional modulation of the feature representations of the base-learner to impose regularities informed by the label space. Experiments show that the conditional transformation in CAML leads to more disentangled representations and achieves competitive results on the miniImageNet benchmark",
    "checked": true,
    "id": "cb80484b34f62d9d7db0bb9214c5374f6bcb861d",
    "semantic_title": "learning to learn with conditional class dependencies",
    "citation_count": 81,
    "authors": []
  },
  "https://openreview.net/forum?id=Hyg_X2C5FX": {
    "title": "GAN Dissection: Visualizing and Understanding Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "Generative Adversarial Networks (GANs) have recently achieved impressive results for many real-world applications, and many GAN variants have emerged with improvements in sample quality and training stability. However, visualization and understanding of GANs is largely missing. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models. In this work, we present an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. We first identify a group of interpretable units that are closely related to object concepts with a segmentation-based network dissection method. Then, we quantify the causal effect of interpretable units by measuring the ability of interventions to control objects in the output. Finally, we examine the contextual relationship between these units and their surrounding by inserting the discovered object concepts into new images. We show several practical applications enabled by our framework, from comparing internal representations across different layers, models, and datasets, to improving GANs by locating and removing artifact-causing units, to interactively manipulating objects in the scene. We provide open source interpretation tools to help peer researchers and practitioners better understand their GAN models",
    "checked": true,
    "id": "df7ad8eeb595da5f7774e91dae06075be952acff",
    "semantic_title": "gan dissection: visualizing and understanding generative adversarial networks",
    "citation_count": 469,
    "authors": []
  },
  "https://openreview.net/forum?id=S1lvm305YQ": {
    "title": "TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre Transfer",
    "volume": "poster",
    "abstract": "In this work, we address the problem of musical timbre transfer, where the goal is to manipulate the timbre of a sound sample from one instrument to match another instrument while preserving other musical content, such as pitch, rhythm, and loudness. In principle, one could apply image-based style transfer techniques to a time-frequency representation of an audio signal, but this depends on having a representation that allows independent manipulation of timbre as well as high-quality waveform generation. We introduce TimbreTron, a method for musical timbre transfer which applies \"image\" domain style transfer to a time-frequency representation of the audio signal, and then produces a high-quality waveform using a conditional WaveNet synthesizer. We show that the Constant Q Transform (CQT) representation is particularly well-suited to convolutional architectures due to its approximate pitch equivariance. Based on human perceptual evaluations, we confirmed that TimbreTron recognizably transferred the timbre while otherwise preserving the musical content, for both monophonic and polyphonic samples. We made an accompanying demo video here: https://www.cs.toronto.edu/~huang/TimbreTron/index.html which we strongly encourage you to watch before reading the paper",
    "checked": true,
    "id": "f1fc78d559a4b6cf02fd2bbe579b9473ae4ae213",
    "semantic_title": "timbretron: a wavenet(cyclegan(cqt(audio))) pipeline for musical timbre transfer",
    "citation_count": 98,
    "authors": []
  },
  "https://openreview.net/forum?id=SyMDXnCcF7": {
    "title": "A Mean Field Theory of Batch Normalization",
    "volume": "poster",
    "abstract": "We develop a mean field theory for batch normalization in fully-connected feedforward neural networks. In so doing, we provide a precise characterization of signal propagation and gradient backpropagation in wide batch-normalized networks at initialization. Our theory shows that gradient signals grow exponentially in depth and that these exploding gradients cannot be eliminated by tuning the initial weight variances or by adjusting the nonlinear activation function. Indeed, batch normalization itself is the cause of gradient explosion. As a result, vanilla batch-normalized networks without skip connections are not trainable at large depths for common initialization schemes, a prediction that we verify with a variety of empirical simulations. While gradient explosion cannot be eliminated, it can be reduced by tuning the network close to the linear regime, which improves the trainability of deep batch-normalized networks without residual connections. Finally, we investigate the learning dynamics of batch-normalized networks and observe that after a single step of optimization the networks achieve a relatively stable equilibrium in which gradients have dramatically smaller dynamic range. Our theory leverages Laplace, Fourier, and Gegenbauer transforms and we derive new identities that may be of independent interest",
    "checked": true,
    "id": "e5b7c1ce5a46e059fce96249c0c034afdd3c287a",
    "semantic_title": "a mean field theory of batch normalization",
    "citation_count": 180,
    "authors": []
  },
  "https://openreview.net/forum?id=HkxLXnAcFQ": {
    "title": "A Closer Look at Few-shot Classification",
    "volume": "poster",
    "abstract": "Few-shot classiÔ¨Åcation aims to learn a classiÔ¨Åer to recognize unseen classes during training with limited labeled examples. While signiÔ¨Åcant progress has been made, the growing complexity of network designs, meta-learning algorithms, and differences in implementation details make a fair comparison difÔ¨Åcult. In this paper, we present 1) a consistent comparative analysis of several representative few-shot classiÔ¨Åcation algorithms, with results showing that deeper backbones signiÔ¨Åcantly reduce the gap across methods including the baseline, 2) a slightly modiÔ¨Åed baseline method that surprisingly achieves competitive performance when compared with the state-of-the-art on both the mini-ImageNet and the CUB datasets, and 3) a new experimental setting for evaluating the cross-domain generalization ability for few-shot classiÔ¨Åcation algorithms. Our results reveal that reducing intra-class variation is an important factor when the feature backbone is shallow, but not as critical when using deeper backbones. In a realistic, cross-domain evaluation setting, we show that a baseline method with a standard Ô¨Åne-tuning practice compares favorably against other state-of-the-art few-shot learning algorithms",
    "checked": true,
    "id": "9d5ec23154fb278a765f47ba5ee5150bd441d0de",
    "semantic_title": "a closer look at few-shot classification",
    "citation_count": 1770,
    "authors": []
  },
  "https://openreview.net/forum?id=HkzSQhCcK7": {
    "title": "STCN: Stochastic Temporal Convolutional Networks",
    "volume": "poster",
    "abstract": "Convolutional architectures have recently been shown to be competitive on many sequence modelling tasks when compared to the de-facto standard of recurrent neural networks (RNNs) while providing computational and modelling advantages due to inherent parallelism. However, currently, there remains a performance gap to more expressive stochastic RNN variants, especially those with several layers of dependent random variables. In this work, we propose stochastic temporal convolutional networks (STCNs), a novel architecture that combines the computational advantages of temporal convolutional networks (TCN) with the representational power and robustness of stochastic latent spaces. In particular, we propose a hierarchy of stochastic latent variables that captures temporal dependencies at different time-scales. The architecture is modular and flexible due to the decoupling of the deterministic and stochastic layers. We show that the proposed architecture achieves state of the art log-likelihoods across several tasks. Finally, the model is capable of predicting high-quality synthetic samples over a long-range temporal horizon in modelling of handwritten text",
    "checked": true,
    "id": "b9357ef8753b07253b144617e0ef798569192bed",
    "semantic_title": "stcn: stochastic temporal convolutional networks",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=HkgEQnRqYQ": {
    "title": "RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space",
    "volume": "poster",
    "abstract": "We study the problem of learning representations of entities and relations in knowledge graphs for predicting missing links. The success of such a task heavily relies on the ability of modeling and inferring the patterns of (or between) the relations. In this paper, we present a new approach for knowledge graph embedding called RotatE, which is able to model and infer various relation patterns including: symmetry/antisymmetry, inversion, and composition. Specifically, the RotatE model defines each relation as a rotation from the source entity to the target entity in the complex vector space. In addition, we propose a novel self-adversarial negative sampling technique for efficiently and effectively training the RotatE model. Experimental results on multiple benchmark knowledge graphs show that the proposed RotatE model is not only scalable, but also able to infer and model various relation patterns and significantly outperform existing state-of-the-art models for link prediction",
    "checked": true,
    "id": "8f096071a09701012c9c279aee2a88143a295935",
    "semantic_title": "rotate: knowledge graph embedding by relational rotation in complex space",
    "citation_count": 2147,
    "authors": []
  },
  "https://openreview.net/forum?id=BJemQ209FQ": {
    "title": "Learning to Navigate the Web",
    "volume": "poster",
    "abstract": "Learning in environments with large state and action spaces, and sparse rewards, can hinder a Reinforcement Learning (RL) agent's learning through trial-and-error. For instance, following natural language instructions on the Web (such as booking a flight ticket) leads to RL settings where input vocabulary and number of actionable elements on a page can grow very large. Even though recent approaches improve the success rate on relatively simple environments with the help of human demonstrations to guide the exploration, they still fail in environments where the set of possible instructions can reach millions. We approach the aforementioned problems from a different perspective and propose guided RL approaches that can generate unbounded amount of experience for an agent to learn from. Instead of learning from a complicated instruction with a large vocabulary, we decompose it into multiple sub-instructions and schedule a curriculum in which an agent is tasked with a gradually increasing subset of these relatively easier sub-instructions. In addition, when the expert demonstrations are not available, we propose a novel meta-learning framework that generates new instruction following tasks and trains the agent more effectively. We train DQN, deep reinforcement learning agent, with Q-value function approximated with a novel QWeb neural network architecture on these smaller, synthetic instructions. We evaluate the ability of our agent to generalize to new instructions onWorld of Bits benchmark, on forms with up to 100 elements, supporting 14 million possible instructions. The QWeb agent outperforms the baseline without using any human demonstration achieving 100% success rate on several difficult environments",
    "checked": true,
    "id": "a7038473320c50df76fa950aca486015c5659503",
    "semantic_title": "learning to navigate the web",
    "citation_count": 65,
    "authors": []
  },
  "https://openreview.net/forum?id=r1xQQhAqKX": {
    "title": "Modeling Uncertainty with Hedged Instance Embeddings",
    "volume": "poster",
    "abstract": "Instance embeddings are an efficient and versatile image representation that facilitates applications like recognition, verification, retrieval, and clustering. Many metric learning methods represent the input as a single point in the embedding space. Often the distance between points is used as a proxy for match confidence. However, this can fail to represent uncertainty which can arise when the input is ambiguous, e.g., due to occlusion or blurriness. This work addresses this issue and explicitly models the uncertainty by \"hedging\" the location of each input in the embedding space. We introduce the hedged instance embedding (HIB) in which embeddings are modeled as random variables and the model is trained under the variational information bottleneck principle (Alemi et al., 2016; Achille & Soatto, 2018). Empirical results on our new N-digit MNIST dataset show that our method leads to the desired behavior of \"hedging its bets\" across the embedding space upon encountering ambiguous inputs. This results in improved performance for image matching and classification tasks, more structure in the learned embedding space, and an ability to compute a per-exemplar uncertainty measure which is correlated with downstream performance",
    "checked": true,
    "id": "99e702dead5fa8e7edbb95d8f27925000bd9b145",
    "semantic_title": "modeling uncertainty with hedged instance embeddings",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=B1ffQnRcKX": {
    "title": "Automatically Composing Representation Transformations as a Means for Generalization",
    "volume": "poster",
    "abstract": "A generally intelligent learner should generalize to more complex tasks than it has previously encountered, but the two common paradigms in machine learning -- either training a separate learner per task or training a single learner for all tasks -- both have difficulty with such generalization because they do not leverage the compositional structure of the task distribution. This paper introduces the compositional problem graph as a broadly applicable formalism to relate tasks of different complexity in terms of problems with shared subproblems. We propose the compositional generalization problem for measuring how readily old knowledge can be reused and hence built upon. As a first step for tackling compositional generalization, we introduce the compositional recursive learner, a domain-general framework for learning algorithmic procedures for composing representation transformations, producing a learner that reasons about what computation to execute by making analogies to previously seen problems. We show on a symbolic and a high-dimensional domain that our compositional approach can generalize to more complex problems than the learner has previously encountered, whereas baselines that are not explicitly compositional do not",
    "checked": true,
    "id": "1766648967f6206a944a4bd18bbbd92a74c164bd",
    "semantic_title": "automatically composing representation transformations as a means for generalization",
    "citation_count": 70,
    "authors": []
  },
  "https://openreview.net/forum?id=HkezXnA9YX": {
    "title": "Systematic Generalization: What Is Required and Can It Be Learned?",
    "volume": "poster",
    "abstract": "Numerous models for grounded language understanding have been recently proposed, including (i) generic models that can be easily adapted to any given task and (ii) intuitively appealing modular models that require background knowledge to be instantiated. We compare both types of models in how much they lend themselves to a particular form of systematic generalization. Using a synthetic VQA test, we evaluate which models are capable of reasoning about all possible object pairs after training on only a small subset of them. Our findings show that the generalization of modular models is much more systematic and that it is highly sensitive to the module layout, i.e. to how exactly the modules are connected. We furthermore investigate if modular models that generalize well could be made more end-to-end by learning their layout and parametrization. We find that end-to-end methods from prior work often learn inappropriate layouts or parametrizations that do not facilitate systematic generalization. Our results suggest that, in addition to modularity, systematic generalization in language understanding may require explicit regularizers or priors",
    "checked": null,
    "id": "6c7494a47cc5421a7b636c244e13586dc2dab007",
    "semantic_title": "systematic generalization: what is required and can it be learned?",
    "citation_count": 162,
    "authors": []
  },
  "https://openreview.net/forum?id=ByxZX20qFQ": {
    "title": "Adaptive Input Representations for Neural Language Modeling",
    "volume": "poster",
    "abstract": "We introduce adaptive input representations for neural language modeling which extend the adaptive softmax of Grave et al. (2017) to input representations of variable capacity. There are several choices on how to factorize the input and output layers, and whether to model words, characters or sub-word units. We perform a systematic comparison of popular choices for a self-attentional architecture. Our experiments show that models equipped with adaptive embeddings are more than twice as fast to train than the popular character input CNN while having a lower number of parameters. On the WikiText-103 benchmark we achieve 18.7 perplexity, an improvement of 10.5 perplexity compared to the previously best published result and on the Billion Word benchmark, we achieve 23.02 perplexity",
    "checked": true,
    "id": "d170bd486e4c0fe82601e322b0e9e0dde63ab299",
    "semantic_title": "adaptive input representations for neural language modeling",
    "citation_count": 390,
    "authors": []
  },
  "https://openreview.net/forum?id=H1MW72AcK7": {
    "title": "Optimal Control Via Neural Networks: A Convex Approach",
    "volume": "poster",
    "abstract": "Control of complex systems involves both system identification and controller design. Deep neural networks have proven to be successful in many identification tasks, however, from model-based control perspective, these networks are difficult to work with because they are typically nonlinear and nonconvex. Therefore many systems are still identified and controlled based on simple linear models despite their poor representation capability. In this paper we bridge the gap between model accuracy and control tractability faced by neural networks, by explicitly constructing networks that are convex with respect to their inputs. We show that these input convex networks can be trained to obtain accurate models of complex physical systems. In particular, we design input convex recurrent neural networks to capture temporal behavior of dynamical systems. Then optimal controllers can be achieved via solving a convex model predictive control problem. Experiment results demonstrate the good potential of the proposed input convex neural network based approach in a variety of control applications. In particular we show that in the MuJoCo locomotion tasks, we could achieve over 10% higher performance using 5 times less time compared with state-of-the-art model-based reinforcement learning method; and in the building HVAC control example, our method achieved up to 20% energy reduction compared with classic linear models",
    "checked": true,
    "id": "e20b0a274d62a94fb1259ce3f2166ecae4673a7e",
    "semantic_title": "optimal control via neural networks: a convex approach",
    "citation_count": 189,
    "authors": []
  },
  "https://openreview.net/forum?id=BJlxm30cKm": {
    "title": "An Empirical Study of Example Forgetting during Deep Neural Network Learning",
    "volume": "poster",
    "abstract": "Inspired by the phenomenon of catastrophic forgetting, we investigate the learning dynamics of neural networks as they train on single classification tasks. Our goal is to understand whether a related phenomenon occurs when data does not undergo a clear distributional shift. We define a ``forgetting event'' to have occurred when an individual training example transitions from being classified correctly to incorrectly over the course of learning. Across several benchmark data sets, we find that: (i) certain examples are forgotten with high frequency, and some not at all; (ii) a data set's (un)forgettable examples generalize across neural architectures; and (iii) based on forgetting dynamics, a significant fraction of examples can be omitted from the training data set while still maintaining state-of-the-art generalization performance",
    "checked": true,
    "id": "a2b5d224895d96bfe2e384e2dcf1ebd136ac3782",
    "semantic_title": "an empirical study of example forgetting during deep neural network learning",
    "citation_count": 743,
    "authors": []
  },
  "https://openreview.net/forum?id=rJ4km2R5t7": {
    "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    "volume": "poster",
    "abstract": "For natural language understanding (NLU) technology to be maximally useful, it must be able to process language in a way that is not exclusive to a single task, genre, or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation (GLUE) benchmark, a collection of tools for evaluating the performance of models across a diverse set of existing NLU tasks. By including tasks with limited training data, GLUE is designed to favor and encourage models that share general linguistic knowledge across tasks. GLUE also includes a hand-crafted diagnostic test suite that enables detailed linguistic analysis of models. We evaluate baselines based on current methods for transfer and representation learning and find that multi-task training on all tasks performs better than training a separate model per task. However, the low absolute performance of our best model indicates the need for improved general NLU systems",
    "checked": true,
    "id": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c",
    "semantic_title": "glue: a multi-task benchmark and analysis platform for natural language understanding",
    "citation_count": 7208,
    "authors": []
  },
  "https://openreview.net/forum?id=Byey7n05FQ": {
    "title": "Plan Online, Learn Offline: Efficient Learning and Exploration via Model-Based Control",
    "volume": "poster",
    "abstract": "We propose a \"plan online and learn offline\" framework for the setting where an agent, with an internal model, needs to continually act and learn in the world. Our work builds on the synergistic relationship between local model-based control, global value function learning, and exploration. We study how local trajectory optimization can cope with approximation errors in the value function, and can stabilize and accelerate value function learning. Conversely, we also study how approximate value functions can help reduce the planning horizon and allow for better policies beyond local solutions. Finally, we also demonstrate how trajectory optimization can be used to perform temporally coordinated exploration in conjunction with estimating uncertainty in value function approximation. This exploration is critical for fast and stable learning of the value function. Combining these components enable solutions to complex control tasks, like humanoid locomotion and dexterous in-hand manipulation, in the equivalent of a few minutes of experience in the real world",
    "checked": true,
    "id": "6a9013a8cdd84e423223f76a903028011c84c4ab",
    "semantic_title": "plan online, learn offline: efficient learning and exploration via model-based control",
    "citation_count": 229,
    "authors": []
  },
  "https://openreview.net/forum?id=Syx0Mh05YQ": {
    "title": "Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion",
    "volume": "poster",
    "abstract": "This paper proposes a representational model for grid cells. In this model, the 2D self-position of the agent is represented by a high-dimensional vector, and the 2D self-motion or displacement of the agent is represented by a matrix that transforms the vector. Each component of the vector is a unit or a cell. The model consists of the following three sub-models. (1) Vector-matrix multiplication. The movement from the current position to the next position is modeled by matrix-vector multi- plication, i.e., the vector of the next position is obtained by multiplying the matrix of the motion to the vector of the current position. (2) Magnified local isometry. The angle between two nearby vectors equals the Euclidean distance between the two corresponding positions multiplied by a magnifying factor. (3) Global adjacency kernel. The inner product between two vectors measures the adjacency between the two corresponding positions, which is defined by a kernel function of the Euclidean distance between the two positions. Our representational model has explicit algebra and geometry. It can learn hexagon patterns of grid cells, and it is capable of error correction, path integral and path planning",
    "checked": true,
    "id": "2abc9d603a87b300a251a3796a12b8a2d21746df",
    "semantic_title": "learning grid cells as vector representation of self-position coupled with matrix representation of self-motion",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=BJe0Gn0cY7": {
    "title": "Preventing Posterior Collapse with delta-VAEs",
    "volume": "poster",
    "abstract": "Due to the phenomenon of \"posterior collapse,\" current latent variable generative models pose a challenging design choice that either weakens the capacity of the decoder or requires altering the training objective. We develop an alternative that utilizes the most powerful generative models as decoders, optimize the variational lower bound, and ensures that the latent variables preserve and encode useful information. Our proposed Œ¥-VAEs achieve this by constraining the variational family for the posterior to have a minimum distance to the prior. For sequential latent variable models, our approach resembles the classic representation learning approach of slow feature analysis. We demonstrate our method's efficacy at modeling text on LM1B and modeling images: learning representations, improving sample quality, and achieving state of the art log-likelihood on CIFAR-10 and ImageNet 32 √ó 32",
    "checked": true,
    "id": "097b7ad748e0f5fb8eeae9fec9ccb344801f87e1",
    "semantic_title": "preventing posterior collapse with delta-vaes",
    "citation_count": 171,
    "authors": []
  },
  "https://openreview.net/forum?id=HyxAfnA5tm": {
    "title": "Deep Online Learning Via Meta-Learning: Continual Adaptation for Model-Based RL",
    "volume": "poster",
    "abstract": "Humans and animals can learn complex predictive models that allow them to accurately and reliably reason about real-world phenomena, and they can adapt such models extremely quickly in the face of unexpected changes. Deep neural network models allow us to represent very complex functions, but lack this capacity for rapid online adaptation. The goal in this paper is to develop a method for continual online learning from an incoming stream of data, using deep neural network models. We formulate an online learning procedure that uses stochastic gradient descent to update model parameters, and an expectation maximization algorithm with a Chinese restaurant process prior to develop and maintain a mixture of models to handle non-stationary task distributions. This allows for all models to be adapted as necessary, with new models instantiated for task changes and old models recalled when previously seen tasks are encountered again. Furthermore, we observe that meta-learning can be used to meta-train a model such that this direct online adaptation with SGD is effective, which is otherwise not the case for large function approximators. We apply our method to model-based reinforcement learning, where adapting the predictive model is critical for control; we demonstrate that our online learning via meta-learning algorithm outperforms alternative prior methods, and enables effective continuous adaptation in non-stationary task distributions such as varying terrains, motor failures, and unexpected disturbances",
    "checked": true,
    "id": "42de54e614110c0c0a0bbbfee045e11e53eb4a7d",
    "semantic_title": "deep online learning via meta-learning: continual adaptation for model-based rl",
    "citation_count": 191,
    "authors": []
  },
  "https://openreview.net/forum?id=SJG6G2RqtX": {
    "title": "Value Propagation Networks",
    "volume": "poster",
    "abstract": "We present Value Propagation (VProp), a set of parameter-efficient differentiable planning modules built on Value Iteration which can successfully be trained using reinforcement learning to solve unseen tasks, has the capability to generalize to larger map sizes, and can learn to navigate in dynamic environments. We show that the modules enable learning to plan when the environment also includes stochastic elements, providing a cost-efficient learning system to build low-level size-invariant planners for a variety of interactive navigation problems. We evaluate on static and dynamic configurations of MazeBase grid-worlds, with randomly generated environments of several different sizes, and on a StarCraft navigation scenario, with more complex dynamics, and pixels as input",
    "checked": true,
    "id": "e2662442770457da538b90cb9a0d42782059ef8c",
    "semantic_title": "value propagation networks",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=Byxpfh0cFm": {
    "title": "Efficient Augmentation via Data Subsampling",
    "volume": "poster",
    "abstract": "Data augmentation is commonly used to encode invariances in learning methods. However, this process is often performed in an inefficient manner, as artificial examples are created by applying a number of transformations to all points in the training set. The resulting explosion of the dataset size can be an issue in terms of storage and training costs, as well as in selecting and tuning the optimal set of transformations to apply. In this work, we demonstrate that it is possible to significantly reduce the number of data points included in data augmentation while realizing the same accuracy and invariance benefits of augmenting the entire dataset. We propose a novel set of subsampling policies, based on model influence and loss, that can achieve a 90% reduction in augmentation set size while maintaining the accuracy gains of standard data augmentation",
    "checked": true,
    "id": "526ef02a3a4624f1dbd0f2ce51778eb92a893e8a",
    "semantic_title": "efficient augmentation via data subsampling",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=B1lnzn0ctQ": {
    "title": "ALISTA: Analytic Weights Are As Good As Learned Weights in LISTA",
    "volume": "poster",
    "abstract": "Deep neural networks based on unfolding an iterative algorithm, for example, LISTA (learned iterative shrinkage thresholding algorithm), have been an empirical success for sparse signal recovery. The weights of these neural networks are currently determined by data-driven \"black-box\" training. In this work, we propose Analytic LISTA (ALISTA), where the weight matrix in LISTA is computed as the solution to a data-free optimization problem, leaving only the stepsize and threshold parameters to data-driven learning. This signiÔ¨Åcantly simpliÔ¨Åes the training. SpeciÔ¨Åcally, the data-free optimization problem is based on coherence minimization. We show our ALISTA retains the optimal linear convergence proved in (Chen et al., 2018) and has a performance comparable to LISTA. Furthermore, we extend ALISTA to convolutional linear operators, again determined in a data-free manner. We also propose a feed-forward framework that combines the data-free optimization and ALISTA networks from end to end, one that can be jointly trained to gain robustness to small perturbations in the encoding model",
    "checked": true,
    "id": "e15e022cb7874912313e2601d38b944fc93bbdd9",
    "semantic_title": "alista: analytic weights are as good as learned weights in lista",
    "citation_count": 178,
    "authors": []
  },
  "https://openreview.net/forum?id=HygsfnR9Ym": {
    "title": "Recall Traces: Backtracking Models for Efficient Reinforcement Learning",
    "volume": "poster",
    "abstract": "In many environments only a tiny subset of all states yield high reward. In these cases, few of the interactions with the environment provide a relevant learning signal. Hence, we may want to preferentially train on those high-reward states and the probable trajectories leading to them. To this end, we advocate for the use of a \\textit{backtracking model} that predicts the preceding states that terminate at a given high-reward state. We can train a model which, starting from a high value state (or one that is estimated to have high value), predicts and samples which (state, action)-tuples may have led to that high value state. These traces of (state, action) pairs, which we refer to as Recall Traces, sampled from this backtracking model starting from a high value state, are informative as they terminate in good states, and hence we can use these traces to improve a policy. We provide a variational interpretation for this idea and a practical algorithm in which the backtracking model samples from an approximate posterior distribution over trajectories which lead to large rewards. Our method improves the sample efficiency of both on- and off-policy RL algorithms across several environments and tasks",
    "checked": true,
    "id": "29aab768e642588352134a03c0368e1bdc1f1e8d",
    "semantic_title": "recall traces: backtracking models for efficient reinforcement learning",
    "citation_count": 68,
    "authors": []
  },
  "https://openreview.net/forum?id=H1gsz30cKX": {
    "title": "Fixup Initialization: Residual Learning Without Normalization",
    "volume": "poster",
    "abstract": "Normalization layers are a staple in state-of-the-art deep neural network architectures. They are widely believed to stabilize training, enable higher learning rate, accelerate convergence and improve generalization, though the reason for their effectiveness is still an active research topic. In this work, we challenge the commonly-held beliefs by showing that none of the perceived benefits is unique to normalization. Specifically, we propose fixed-update initialization (Fixup), an initialization motivated by solving the exploding and vanishing gradient problem at the beginning of training via properly rescaling a standard initialization. We find training residual networks with Fixup to be as stable as training with normalization -- even for networks with 10,000 layers. Furthermore, with proper regularization, Fixup enables residual networks without normalization to achieve state-of-the-art performance in image classification and machine translation",
    "checked": true,
    "id": "96c82727dd5a80fef93007f888bb8569feb6bd85",
    "semantic_title": "fixup initialization: residual learning without normalization",
    "citation_count": 351,
    "authors": []
  },
  "https://openreview.net/forum?id=rJliMh09F7": {
    "title": "Diversity-Sensitive Conditional Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "We propose a simple yet highly effective method that addresses the mode-collapse problem in the Conditional Generative Adversarial Network (cGAN). Although conditional distributions are multi-modal (i.e., having many modes) in practice, most cGAN approaches tend to learn an overly simplified distribution where an input is always mapped to a single output regardless of variations in latent code. To address such issue, we propose to explicitly regularize the generator to produce diverse outputs depending on latent codes. The proposed regularization is simple, general, and can be easily integrated into most conditional GAN objectives. Additionally, explicit regularization on generator allows our method to control a balance between visual quality and diversity. We demonstrate the effectiveness of our method on three conditional generation tasks: image-to-image translation, image inpainting, and future video prediction. We show that simple addition of our regularization to existing models leads to surprisingly diverse generations, substantially outperforming the previous approaches for multi-modal conditional generation specifically designed in each individual task",
    "checked": true,
    "id": "cc8a7a229151d17b1088d059fc471da2cea718c3",
    "semantic_title": "diversity-sensitive conditional generative adversarial networks",
    "citation_count": 216,
    "authors": []
  },
  "https://openreview.net/forum?id=S1lqMn05Ym": {
    "title": "Information asymmetry in KL-regularized RL",
    "volume": "poster",
    "abstract": "Many real world tasks exhibit rich structure that is repeated across different parts of the state space or in time. In this work we study the possibility of leveraging such repeated structure to speed up and regularize learning. We start from the KL regularized expected reward objective which introduces an additional component, a default policy. Instead of relying on a fixed default policy, we learn it from data. But crucially, we restrict the amount of information the default policy receives, forcing it to learn reusable behaviors that help the policy learn faster. We formalize this strategy and discuss connections to information bottleneck approaches and to the variational EM algorithm. We present empirical results in both discrete and continuous action domains and demonstrate that, for certain tasks, learning a default policy alongside the policy can significantly speed up and improve learning. Please watch the video demonstrating learned experts and default policies on several continuous control tasks ( https://youtu.be/U2qA3llzus8 )",
    "checked": true,
    "id": "549c9dfb32e85d9ef5a48566767be42ad132a3c4",
    "semantic_title": "information asymmetry in kl-regularized rl",
    "citation_count": 104,
    "authors": []
  },
  "https://openreview.net/forum?id=ByftGnR9KX": {
    "title": "FlowQA: Grasping Flow in History for Conversational Machine Comprehension",
    "volume": "poster",
    "abstract": "Conversational machine comprehension requires a deep understanding of the conversation history. To enable traditional, single-turn models to encode the history comprehensively, we introduce Flow, a mechanism that can incorporate intermediate representations generated during the process of answering previous questions, through an alternating parallel processing structure. Compared to shallow approaches that concatenate previous questions/answers as input, Flow integrates the latent semantics of the conversation history more deeply. Our model, FlowQA, shows superior performance on two recently proposed conversational challenges (+7.2% F1 on CoQA and +4.0% on QuAC). The effectiveness of Flow also shows in other tasks. By reducing sequential instruction understanding to conversational machine comprehension, FlowQA outperforms the best models on all three domains in SCONE, with +1.8% to +4.4% improvement in accuracy",
    "checked": true,
    "id": "9065ace5366ef548cf81bd9f239f1d132c1ef412",
    "semantic_title": "flowqa: grasping flow in history for conversational machine comprehension",
    "citation_count": 98,
    "authors": []
  },
  "https://openreview.net/forum?id=ByetGn0cYX": {
    "title": "Probabilistic Planning with Sequential Monte Carlo methods",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "c6b040f3f8215f32e362ca7a483c48070585b029",
    "semantic_title": "probabilistic planning with sequential monte carlo methods",
    "citation_count": 45,
    "authors": []
  },
  "https://openreview.net/forum?id=SkGuG2R5tm": {
    "title": "Spreading vectors for similarity search",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6ac386b9f77c3e4d84c06ec8b66475b1a6eada67",
    "semantic_title": "spreading vectors for similarity search",
    "citation_count": 121,
    "authors": []
  },
  "https://openreview.net/forum?id=BkedznAqKQ": {
    "title": "LanczosNet: Multi-Scale Deep Graph Convolutional Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5098df13be6d1f2a31c9fbf85703336ef77a9665",
    "semantic_title": "lanczosnet: multi-scale deep graph convolutional networks",
    "citation_count": 228,
    "authors": []
  },
  "https://openreview.net/forum?id=SylPMnR9Ym": {
    "title": "Learning what you can do before doing anything",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a53874f5c63b31468ad2fe3f5dea558a6ce35820",
    "semantic_title": "learning what you can do before doing anything",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=rylDfnCqF7": {
    "title": "Lagging Inference Networks and Posterior Collapse in Variational Autoencoders",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "159078136930f3963e01d694faa1b6b51f93c7ec",
    "semantic_title": "lagging inference networks and posterior collapse in variational autoencoders",
    "citation_count": 273,
    "authors": []
  },
  "https://openreview.net/forum?id=rkevMnRqYQ": {
    "title": "Preferences Implicit in the State of the World",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d35f9c78fc6d656d530aac2ed9f2aae6137b9041",
    "semantic_title": "preferences implicit in the state of the world",
    "citation_count": 55,
    "authors": []
  },
  "https://openreview.net/forum?id=S1lIMn05F7": {
    "title": "A Direct Approach to Robust Deep Learning Using Adversarial Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "04ee17ea05341aadc8643a21d21746cb67993a9d",
    "semantic_title": "a direct approach to robust deep learning using adversarial networks",
    "citation_count": 77,
    "authors": []
  },
  "https://openreview.net/forum?id=SyfIfnC5Ym": {
    "title": "Improving the Generalization of Adversarial Training with Domain Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "428c2e5992d6ed3186c087cba0fdba2ab6a468b2",
    "semantic_title": "improving the generalization of adversarial training with domain adaptation",
    "citation_count": 132,
    "authors": []
  },
  "https://openreview.net/forum?id=HklSf3CqKm": {
    "title": "Subgradient Descent Learns Orthogonal Dictionaries",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "15411d45ac13f432c3505c3ce825dfa00d88f8a6",
    "semantic_title": "subgradient descent learns orthogonal dictionaries",
    "citation_count": 51,
    "authors": []
  },
  "https://openreview.net/forum?id=HyGEM3C9KQ": {
    "title": "Improving Differentiable Neural Computers Through Memory Masking, De-allocation, and Link Distribution Sharpness Control",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d5535d4da15a7a8dfbeb34f61cddb4874bbc56e0",
    "semantic_title": "improving differentiable neural computers through memory masking, de-allocation, and link distribution sharpness control",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=r1eVMnA9K7": {
    "title": "Unsupervised Control Through Non-Parametric Discriminative Rewards",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "b65a6be07ce9c86797e6917258cf5ba45273ee73",
    "semantic_title": "unsupervised control through non-parametric discriminative rewards",
    "citation_count": 177,
    "authors": []
  },
  "https://openreview.net/forum?id=r1eEG20qKQ": {
    "title": "Self-Tuning Networks: Bilevel Optimization of Hyperparameters using Structured Best-Response Functions",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0ef6910cb25305a447aaa8c4be89ead9879a5016",
    "semantic_title": "self-tuning networks: bilevel optimization of hyperparameters using structured best-response functions",
    "citation_count": 164,
    "authors": []
  },
  "https://openreview.net/forum?id=B1MXz20cYQ": {
    "title": "Explaining Image Classifiers by Counterfactual Generation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f1173ca43481c1b33f4e7891ce77200e51eecba2",
    "semantic_title": "explaining image classifiers by counterfactual generation",
    "citation_count": 265,
    "authors": []
  },
  "https://openreview.net/forum?id=HJlQfnCqKX": {
    "title": "Predicting the Generalization Gap in Deep Networks with Margin Distributions",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4f409f21d04347db66e3cc09a32c4ef0d5b66094",
    "semantic_title": "predicting the generalization gap in deep networks with margin distributions",
    "citation_count": 199,
    "authors": []
  },
  "https://openreview.net/forum?id=HyN-M2Rctm": {
    "title": "Mode Normalization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b578b10ef5e271087200ef6182b9806c472c223d",
    "semantic_title": "mode normalization",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=r1GbfhRqF7": {
    "title": "Kernel Change-point Detection with Auxiliary Deep Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "fb327a4cc8a7e3de79362228b6904848d007e61d",
    "semantic_title": "kernel change-point detection with auxiliary deep generative models",
    "citation_count": 71,
    "authors": []
  },
  "https://openreview.net/forum?id=BJzbG20cFQ": {
    "title": "Towards Metamerism via Foveated Style Transfer",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7f3df93baabaf66bebbf7a24257f4ada252bc959",
    "semantic_title": "towards metamerism via foveated style transfer",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=BJxgz2R9t7": {
    "title": "Learning To Solve Circuit-SAT: An Unsupervised Differentiable Approach",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2b64300879d6fcaabe932e87ecb412066359b286",
    "semantic_title": "learning to solve circuit-sat: an unsupervised differentiable approach",
    "citation_count": 99,
    "authors": []
  },
  "https://openreview.net/forum?id=Hyg1G2AqtQ": {
    "title": "Variance Reduction for Reinforcement Learning in Input-Driven Environments",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c470f1d1e7d377b4e0d01890ae418a918c0e7093",
    "semantic_title": "variance reduction for reinforcement learning in input-driven environments",
    "citation_count": 95,
    "authors": []
  },
  "https://openreview.net/forum?id=SyxAb30cY7": {
    "title": "Robustness May Be at Odds with Accuracy",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1b9c6022598085dd892f360122c0fa4c630b3f18",
    "semantic_title": "robustness may be at odds with accuracy",
    "citation_count": 1786,
    "authors": []
  },
  "https://openreview.net/forum?id=H1g0Z3A9Fm": {
    "title": "Supervised Community Detection with Line Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "11222e4f5262c774bf9c006420eb647b951624b2",
    "semantic_title": "supervised community detection with line graph neural networks",
    "citation_count": 321,
    "authors": []
  },
  "https://openreview.net/forum?id=S1M6Z2Cctm": {
    "title": "Harmonic Unpaired Image-to-image Translation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c2f32f0ea55d287ea7dee6bc214e7b96fc2c2763",
    "semantic_title": "harmonic unpaired image-to-image translation",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=rklaWn0qK7": {
    "title": "Learning Neural PDE Solvers with Convergence Guarantees",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e2be7cd38737099a741184ad62587a3fd226db55",
    "semantic_title": "learning neural pde solvers with convergence guarantees",
    "citation_count": 145,
    "authors": []
  },
  "https://openreview.net/forum?id=HyxnZh0ct7": {
    "title": "Meta-learning with differentiable closed-form solvers",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "208cd4b25768f0096fb2e80e7690473da0e2a563",
    "semantic_title": "meta-learning with differentiable closed-form solvers",
    "citation_count": 931,
    "authors": []
  },
  "https://openreview.net/forum?id=S1lhbnRqF7": {
    "title": "Building Dynamic Knowledge Graphs from Text using Machine Reading Comprehension",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e8443666cf927806576ace54b9351af72a1d2d9b",
    "semantic_title": "building dynamic knowledge graphs from text using machine reading comprehension",
    "citation_count": 79,
    "authors": []
  },
  "https://openreview.net/forum?id=BkMiWhR5K7": {
    "title": "Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "10ab21b120e305b6d3cbf81c5a906d36521152f1",
    "semantic_title": "prior convictions: black-box adversarial attacks with bandits and priors",
    "citation_count": 375,
    "authors": []
  },
  "https://openreview.net/forum?id=Byf5-30qFX": {
    "title": "DHER: Hindsight Experience Replay for Dynamic Goals",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "154a19b77f2302d6b9bab354d3160ef527d29f55",
    "semantic_title": "dher: hindsight experience replay for dynamic goals",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=HJf9ZhC9FX": {
    "title": "Stochastic Gradient/Mirror Descent: Minimax Optimality and Implicit Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1d57045bfa02ac54df0481c1a977829b63564334",
    "semantic_title": "stochastic gradient/mirror descent: minimax optimality and implicit regularization",
    "citation_count": 64,
    "authors": []
  },
  "https://openreview.net/forum?id=H1lqZhRcFm": {
    "title": "Unsupervised Learning of the Set of Local Maxima",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "46d5cf70365d48ad38ed549fb44cf76810e8f8da",
    "semantic_title": "unsupervised learning of the set of local maxima",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=B1xY-hRctX": {
    "title": "Neural Logic Machines",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3a6447361b20c249f5306ae17dee43f645430e31",
    "semantic_title": "neural logic machines",
    "citation_count": 250,
    "authors": []
  },
  "https://openreview.net/forum?id=ryetZ20ctX": {
    "title": "Defensive Quantization: When Efficiency Meets Robustness",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ed1f55fb7ba0e3196913027840c4e23155e2e80c",
    "semantic_title": "defensive quantization: when efficiency meets robustness",
    "citation_count": 204,
    "authors": []
  },
  "https://openreview.net/forum?id=SygD-hCcF7": {
    "title": "Dimensionality Reduction for Representing the Knowledge of Probabilistic Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "effc2e95fb3ca77bba896439a1d9cf49f18f4af0",
    "semantic_title": "dimensionality reduction for representing the knowledge of probabilistic models",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=SygvZ209F7": {
    "title": "Biologically-Plausible Learning Algorithms Can Scale to Large Datasets",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2c26cca611ef1403ca81df238d04d0ef4a584a86",
    "semantic_title": "biologically-plausible learning algorithms can scale to large datasets",
    "citation_count": 76,
    "authors": []
  },
  "https://openreview.net/forum?id=rkxw-hAcFQ": {
    "title": "Generating Multi-Agent Trajectories using Programmatic Weak Supervision",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1dd5e6ea1cff5644f1fb80272d25ab37b3ef64bf",
    "semantic_title": "generating multi-agent trajectories using programmatic weak supervision",
    "citation_count": 90,
    "authors": []
  },
  "https://openreview.net/forum?id=H1gL-2A9Ym": {
    "title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "ac225094aab9e7b629bc5b3343e026dea0200c70",
    "semantic_title": "predict then propagate: graph neural networks meet personalized pagerank",
    "citation_count": 1683,
    "authors": []
  },
  "https://openreview.net/forum?id=ryl8-3AcFX": {
    "title": "Environment Probing Interaction Policies",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "633266814150ab66f0474d7b9a6807b729c7e0af",
    "semantic_title": "environment probing interaction policies",
    "citation_count": 67,
    "authors": []
  },
  "https://openreview.net/forum?id=SyzVb3CcFX": {
    "title": "Time-Agnostic Prediction: Predicting Predictable Video Frames",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "973e116b2b2949fdb8c533a8d84ddd811c0920cf",
    "semantic_title": "time-agnostic prediction: predicting predictable video frames",
    "citation_count": 89,
    "authors": []
  },
  "https://openreview.net/forum?id=BJg4Z3RqF7": {
    "title": "Unsupervised Adversarial Image Reconstruction",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "59ae0a50c5ebc8aa9c408e9d75f56c195ac7276e",
    "semantic_title": "unsupervised adversarial image reconstruction",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=rylV-2C9KQ": {
    "title": "Deep Decoder: Concise Image Representations from Untrained Non-convolutional Networks",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "f73802729a2fad366fecc9ed6221d2aa2a8a6f38",
    "semantic_title": "deep decoder: concise image representations from untrained non-convolutional networks",
    "citation_count": 284,
    "authors": []
  },
  "https://openreview.net/forum?id=S1xNb2A9YX": {
    "title": "Minimal Images in Deep Neural Networks: Fragile Object Recognition in Natural Images",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "3b523a15853e99dafc16962827e6e6f467d41a03",
    "semantic_title": "minimal images in deep neural networks: fragile object recognition in natural images",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=Hkg4W2AcFm": {
    "title": "Overcoming the Disentanglement vs Reconstruction Trade-off via Jacobian Supervision",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "aee543a4859821b750b31cc03a0c39f9b4daacc0",
    "semantic_title": "overcoming the disentanglement vs reconstruction trade-off via jacobian supervision",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=H1l7bnR5Ym": {
    "title": "ProbGAN: Towards Probabilistic GAN with Theoretical Guarantees",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "99c870a6d94d1295a6ed1d004235cb4f8f676fbe",
    "semantic_title": "probgan: towards probabilistic gan with theoretical guarantees",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=rkxQ-nA9FX": {
    "title": "Theoretical Analysis of Auto Rate-Tuning by Batch Normalization",
    "volume": "poster",
    "abstract": "Batch Normalization (BN) has become a cornerstone of deep learning across diverse architectures, appearing to help optimization as well as generalization. While the idea makes intuitive sense, theoretical analysis of its effectiveness has been lacking. Here theoretical support is provided for one of its conjectured properties, namely, the ability to allow gradient descent to succeed with less tuning of learning rates. It is shown that even if we fix the learning rate of scale-invariant parameters (e.g., weights of each layer with BN) to a constant (say, 0.3), gradient descent still approaches a stationary point (i.e., a solution where gradient is zero) in the rate of T^{‚àí1/2} in T iterations, asymptotically matching the best bound for gradient descent with well-tuned learning rates. A similar result with convergence rate T^{‚àí1/4} is also shown for stochastic gradient descent",
    "checked": null,
    "id": "3dfb0a18ab5a5413c50d911e49b3c83b1a9383a3",
    "semantic_title": "theoretical analysis of auto rate-tuning by batch normalization",
    "citation_count": 131,
    "authors": []
  },
  "https://openreview.net/forum?id=B1lz-3Rct7": {
    "title": "Three Mechanisms of Weight Decay Regularization",
    "volume": "poster",
    "abstract": "Weight decay is one of the standard tricks in the neural network toolbox, but the reasons for its regularization effect are poorly understood, and recent results have cast doubt on the traditional interpretation in terms of $L_2$ regularization. Literal weight decay has been shown to outperform $L_2$ regularization for optimizers for which they differ. We empirically investigate weight decay for three optimization algorithms (SGD, Adam, and K-FAC) and a variety of network architectures. We identify three distinct mechanisms by which weight decay exerts a regularization effect, depending on the particular optimization algorithm and architecture: (1) increasing the effective learning rate, (2) approximately regularizing the input-output Jacobian norm, and (3) reducing the effective damping coefficient for second-order optimization. Our results provide insight into how to improve the regularization of neural networks",
    "checked": null,
    "id": "ba618ec05a9dbef75310c5e4bcce8a559e0270b5",
    "semantic_title": "three mechanisms of weight decay regularization",
    "citation_count": 258,
    "authors": []
  },
  "https://openreview.net/forum?id=SkfMWhAqYQ": {
    "title": "Approximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet",
    "volume": "poster",
    "abstract": "Deep Neural Networks (DNNs) excel on many complex perceptual tasks but it has proven notoriously difficult to understand how they reach their decisions. We here introduce a high-performance DNN architecture on ImageNet whose decisions are considerably easier to explain. Our model, a simple variant of the ResNet-50 architecture called BagNet, classifies an image based on the occurrences of small local image features without taking into account their spatial ordering. This strategy is closely related to the bag-of-feature (BoF) models popular before the onset of deep learning and reaches a surprisingly high accuracy on ImageNet (87.6% top-5 for 32 x 32 px features and Alexnet performance for 16 x16 px features). The constraint on local features makes it straight-forward to analyse how exactly each part of the image influences the classification. Furthermore, the BagNets behave similar to state-of-the art deep neural networks such as VGG-16, ResNet-152 or DenseNet-169 in terms of feature sensitivity, error distribution and interactions between image parts. This suggests that the improvements of DNNs over previous bag-of-feature classifiers in the last few years is mostly achieved by better fine-tuning rather than by qualitatively different decision strategies",
    "checked": null,
    "id": "810ae452a3a1f673ea241bd540f9551b2996ed5b",
    "semantic_title": "approximating cnns with bag-of-local-features models works surprisingly well on imagenet",
    "citation_count": 561,
    "authors": []
  },
  "https://openreview.net/forum?id=SyMWn05F7": {
    "title": "Learning Exploration Policies for Navigation",
    "volume": "poster",
    "abstract": "Numerous past works have tackled the problem of task-driven navigation. But, how to effectively explore a new environment to enable a variety of down-stream tasks has received much less attention. In this work, we study how agents can autonomously explore realistic and complex 3D environments without the context of task-rewards. We propose a learning-based approach and investigate different policy architectures, reward functions, and training paradigms. We find that use of policies with spatial memory that are bootstrapped with imitation learning and finally finetuned with coverage rewards derived purely from on-board sensors can be effective at exploring novel environments. We show that our learned exploration policies can explore better than classical approaches based on geometry alone and generic learning-based exploration techniques. Finally, we also show how such task-agnostic exploration can be used for down-stream tasks. Videos are available at https://sites.google.com/view/exploration-for-nav/",
    "checked": null,
    "id": "48182d7620a9278d7e9cd880a961fa14d22a0281",
    "semantic_title": "learning exploration policies for navigation",
    "citation_count": 237,
    "authors": []
  },
  "https://openreview.net/forum?id=SJggZnRcFQ": {
    "title": "Learning Programmatically Structured Representations with Perceptor Gradients",
    "volume": "poster",
    "abstract": "We present the perceptor gradients algorithm -- a novel approach to learning symbolic representations based on the idea of decomposing an agent's policy into i) a perceptor network extracting symbols from raw observation data and ii) a task encoding program which maps the input symbols to output actions. We show that the proposed algorithm is able to learn representations that can be directly fed into a Linear-Quadratic Regulator (LQR) or a general purpose A* planner. Our experimental results confirm that the perceptor gradients algorithm is able to efficiently learn transferable symbolic representations as well as generate new observations according to a semantically meaningful specification",
    "checked": null,
    "id": "7e2d24fc41325e19f175dc9dc35db76d56402a72",
    "semantic_title": "learning programmatically structured representations with perceptor gradients",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=HJxeWnCcF7": {
    "title": "Learning Mixed-Curvature Representations in Product Spaces",
    "volume": "poster",
    "abstract": "The quality of the representations achieved by embeddings is determined by how well the geometry of the embedding space matches the structure of the data. Euclidean space has been the workhorse for embeddings; recently hyperbolic and spherical spaces have gained popularity due to their ability to better embed new types of structured data---such as hierarchical data---but most data is not structured so uniformly. We address this problem by proposing learning embeddings in a product manifold combining multiple copies of these model spaces (spherical, hyperbolic, Euclidean), providing a space of heterogeneous curvature suitable for a wide variety of structures. We introduce a heuristic to estimate the sectional curvature of graph data and directly determine an appropriate signature---the number of component spaces and their dimensions---of the product manifold. Empirically, we jointly learn the curvature and the embedding in the product space via Riemannian optimization. We discuss how to define and compute intrinsic quantities such as means---a challenging notion for product manifolds---and provably learnable optimization functions. On a range of datasets and reconstruction tasks, our product space embeddings outperform single Euclidean or hyperbolic spaces used in previous works, reducing distortion by 32.55% on a Facebook social network dataset. We learn word embeddings and find that a product of hyperbolic spaces in 50 dimensions consistently improves on baseline Euclidean and hyperbolic embeddings, by 2.6 points in Spearman rank correlation on similarity tasks and 3.4 points on analogy accuracy",
    "checked": null,
    "id": "779ad52e8c27b77c10d14d536133da61c2c1f9b2",
    "semantic_title": "learning mixed-curvature representations in product spaces",
    "citation_count": 200,
    "authors": []
  },
  "https://openreview.net/forum?id=Hygxb2CqKm": {
    "title": "Stable Recurrent Models",
    "volume": "poster",
    "abstract": "Stability is a fundamental property of dynamical systems, yet to this date it has had little bearing on the practice of recurrent neural networks. In this work, we conduct a thorough investigation of stable recurrent models. Theoretically, we prove stable recurrent neural networks are well approximated by feed-forward networks for the purpose of both inference and training by gradient descent. Empirically, we demonstrate stable recurrent models often perform as well as their unstable counterparts on benchmark sequence tasks. Taken together, these findings shed light on the effective power of recurrent networks and suggest much of sequence learning happens, or can be made to happen, in the stable regime. Moreover, our results help to explain why in many cases practitioners succeed in replacing recurrent models by feed-forward models",
    "checked": null,
    "id": "e77099681374e940ea45821fd7e406394721552f",
    "semantic_title": "stable recurrent models",
    "citation_count": 119,
    "authors": []
  },
  "https://openreview.net/forum?id=HyxCxhRcY7": {
    "title": "Deep Anomaly Detection with Outlier Exposure",
    "volume": "poster",
    "abstract": "It is important to detect anomalous inputs when deploying machine learning systems. The use of larger and more complex inputs in deep learning magnifies the difficulty of distinguishing between anomalous and in-distribution examples. At the same time, diverse image and text data are available in enormous quantities. We propose leveraging these data to improve deep anomaly detection by training anomaly detectors against an auxiliary dataset of outliers, an approach we call Outlier Exposure (OE). This enables anomaly detectors to generalize and detect unseen anomalies. In extensive experiments on natural language processing and small- and large-scale vision tasks, we find that Outlier Exposure significantly improves detection performance. We also observe that cutting-edge generative models trained on CIFAR-10 may assign higher likelihoods to SVHN images than to CIFAR-10 images; we use OE to mitigate this issue. We also analyze the flexibility and robustness of Outlier Exposure, and identify characteristics of the auxiliary dataset that improve performance",
    "checked": null,
    "id": "2d8c97db4bae00ff243d122b957091a236a697a7",
    "semantic_title": "deep anomaly detection with outlier exposure",
    "citation_count": 1475,
    "authors": []
  },
  "https://openreview.net/forum?id=S1lTg3RqYQ": {
    "title": "Exemplar Guided Unsupervised Image-to-Image Translation with Semantic Consistency",
    "volume": "poster",
    "abstract": "Image-to-image translation has recently received significant attention due to advances in deep learning. Most works focus on learning either a one-to-one mapping in an unsupervised way or a many-to-many mapping in a supervised way. However, a more practical setting is many-to-many mapping in an unsupervised way, which is harder due to the lack of supervision and the complex inner- and cross-domain variations. To alleviate these issues, we propose the Exemplar Guided & Semantically Consistent Image-to-image Translation (EGSC-IT) network which conditions the translation process on an exemplar image in the target domain. We assume that an image comprises of a content component which is shared across domains, and a style component specific to each domain. Under the guidance of an exemplar from the target domain we apply Adaptive Instance Normalization to the shared content component, which allows us to transfer the style information of the target domain to the source domain. To avoid semantic inconsistencies during translation that naturally appear due to the large inner- and cross-domain variations, we introduce the concept of feature masks that provide coarse semantic guidance without requiring the use of any semantic labels. Experimental results on various datasets show that EGSC-IT does not only translate the source image to diverse instances in the target domain, but also preserves the semantic consistency during the process",
    "checked": null,
    "id": "7242666492464a5408fc072f5c4f51502e0d0e5b",
    "semantic_title": "exemplar guided unsupervised image-to-image translation",
    "citation_count": 140,
    "authors": []
  },
  "https://openreview.net/forum?id=HkG3e205K7": {
    "title": "Doubly Reparameterized Gradient Estimators for Monte Carlo Objectives",
    "volume": "poster",
    "abstract": "Deep latent variable models have become a popular model choice due to the scalable learning algorithms introduced by (Kingma & Welling 2013, Rezende et al. 2014). These approaches maximize a variational lower bound on the intractable log likelihood of the observed data. Burda et al. (2015) introduced a multi-sample variational bound, IWAE, that is at least as tight as the standard variational lower bound and becomes increasingly tight as the number of samples increases. Counterintuitively, the typical inference network gradient estimator for the IWAE bound performs poorly as the number of samples increases (Rainforth et al. 2018, Le et al. 2018). Roeder et a. (2017) propose an improved gradient estimator, however, are unable to show it is unbiased. We show that it is in fact biased and that the bias can be estimated efficiently with a second application of the reparameterization trick. The doubly reparameterized gradient (DReG) estimator does not suffer as the number of samples increases, resolving the previously raised issues. The same idea can be used to improve many recently introduced training techniques for latent variable models. In particular, we show that this estimator reduces the variance of the IWAE gradient, the reweighted wake-sleep update (RWS) (Bornschein & Bengio 2014), and the jackknife variational inference (JVI) gradient (Nowozin 2018). Finally, we show that this computationally efficient, drop-in estimator translates to improved performance for all three objectives on several modeling tasks",
    "checked": null,
    "id": "d8778f909eb027bcb7c5122bca23acb680ded647",
    "semantic_title": "doubly reparameterized gradient estimators for monte carlo objectives",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=Bylnx209YX": {
    "title": "Adversarial Attacks on Graph Neural Networks via Meta Learning",
    "volume": "poster",
    "abstract": "Deep learning models for graphs have advanced the state of the art on many tasks. Despite their recent success, little is known about their robustness. We investigate training time attacks on graph neural networks for node classification that perturb the discrete graph structure. Our core principle is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize. Our experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings. Remarkably, the perturbations created by our algorithm can misguide the graph neural networks such that they perform worse than a simple baseline that ignores all relational information. Our attacks do not assume any knowledge about or access to the target classifiers",
    "checked": null,
    "id": "6f5b1076ebacd30849d86e5f5787e3d43b65911f",
    "semantic_title": "adversarial attacks on graph neural networks via meta learning",
    "citation_count": 571,
    "authors": []
  },
  "https://openreview.net/forum?id=Bkg3g2R9FX": {
    "title": "Adaptive Gradient Methods with Dynamic Bound of Learning Rate",
    "volume": "poster",
    "abstract": "Adaptive optimization methods such as AdaGrad, RMSprop and Adam have been proposed to achieve a rapid training process with an element-wise scaling term on learning rates. Though prevailing, they are observed to generalize poorly compared with SGD or even fail to converge due to unstable and extreme learning rates. Recent work has put forward some algorithms such as AMSGrad to tackle this issue but they failed to achieve considerable improvement over existing methods. In our paper, we demonstrate that extreme learning rates can lead to poor performance. We provide new variants of Adam and AMSGrad, called AdaBound and AMSBound respectively, which employ dynamic bounds on learning rates to achieve a gradual and smooth transition from adaptive methods to SGD and give a theoretical proof of convergence. We further conduct experiments on various popular tasks and models, which is often insufficient in previous work. Experimental results show that new variants can eliminate the generalization gap between adaptive methods and SGD and maintain higher learning speed early in training at the same time. Moreover, they can bring significant improvement over their prototypes, especially on complex deep networks. The implementation of the algorithm can be found at https://github.com/Luolc/AdaBound",
    "checked": null,
    "id": "03af562fb8e69677865dbe94910e464443dd4623",
    "semantic_title": "adaptive gradient methods with dynamic bound of learning rate",
    "citation_count": 602,
    "authors": []
  },
  "https://openreview.net/forum?id=HyGcghRct7": {
    "title": "Random mesh projectors for inverse problems",
    "volume": "poster",
    "abstract": "We propose a new learning-based approach to solve ill-posed inverse problems in imaging. We address the case where ground truth training samples are rare and the problem is severely ill-posed---both because of the underlying physics and because we can only get few measurements. This setting is common in geophysical imaging and remote sensing. We show that in this case the common approach to directly learn the mapping from the measured data to the reconstruction becomes unstable. Instead, we propose to first learn an ensemble of simpler mappings from the data to projections of the unknown image into random piecewise-constant subspaces. We then combine the projections to form a final reconstruction by solving a deconvolution-like problem. We show experimentally that the proposed method is more robust to measurement noise and corruptions not seen during training than a directly learned inverse",
    "checked": null,
    "id": "e2c18e8f9c808dd5ccc8df752da6efe69619fe34",
    "semantic_title": "random mesh projectors for inverse problems",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=S1xcx3C5FX": {
    "title": "A Statistical Approach to Assessing Neural Network Robustness",
    "volume": "poster",
    "abstract": "We present a new approach to assessing the robustness of neural networks based on estimating the proportion of inputs for which a property is violated. Specifically, we estimate the probability of the event that the property is violated under an input model. Our approach critically varies from the formal verification framework in that when the property can be violated, it provides an informative notion of how robust the network is, rather than just the conventional assertion that the network is not verifiable. Furthermore, it provides an ability to scale to larger networks than formal verification approaches. Though the framework still provides a formal guarantee of satisfiability whenever it successfully finds one or more violations, these advantages do come at the cost of only providing a statistical estimate of unsatisfiability whenever no violation is found. Key to the practical success of our approach is an adaptation of multi-level splitting, a Monte Carlo approach for estimating the probability of rare events, to our statistical robustness framework. We demonstrate that our approach is able to emulate formal verification procedures on benchmark problems, while scaling to larger networks and providing reliable additional information in the form of accurate estimates of the violation probability",
    "checked": null,
    "id": "85df5b9fcd85c41ec0e1eb6c1ab15b8d7147c885",
    "semantic_title": "a statistical approach to assessing neural network robustness",
    "citation_count": 83,
    "authors": []
  },
  "https://openreview.net/forum?id=Hye9lnCct7": {
    "title": "Learning Actionable Representations with Goal Conditioned Policies",
    "volume": "poster",
    "abstract": "Representation learning is a central challenge across a range of machine learning areas. In reinforcement learning, effective and functional representations have the potential to tremendously accelerate learning progress and solve more challenging problems. Most prior work on representation learning has focused on generative approaches, learning representations that capture all the underlying factors of variation in the observation space in a more disentangled or well-ordered manner. In this paper, we instead aim to learn functionally salient representations: representations that are not necessarily complete in terms of capturing all factors of variation in the observation space, but rather aim to capture those factors of variation that are important for decision making -- that are \"actionable\". These representations are aware of the dynamics of the environment, and capture only the elements of the observation that are necessary for decision making rather than all factors of variation, eliminating the need for explicit reconstruction. We show how these learned representations can be useful to improve exploration for sparse reward problems, to enable long horizon hierarchical reinforcement learning, and as a state representation for learning policies for downstream tasks. We evaluate our method on a number of simulated environments, and compare it to prior methods for representation learning, exploration, and hierarchical reinforcement learning",
    "checked": null,
    "id": "c46d80f83813fba0e8363a0ab36a19fba062540e",
    "semantic_title": "learning actionable representations with goal-conditioned policies",
    "citation_count": 109,
    "authors": []
  },
  "https://openreview.net/forum?id=rJgYxn09Fm": {
    "title": "Learning Implicitly Recurrent CNNs Through Parameter Sharing",
    "volume": "poster",
    "abstract": "We introduce a parameter sharing scheme, in which different layers of a convolutional neural network (CNN) are defined by a learned linear combination of parameter tensors from a global bank of templates. Restricting the number of templates yields a flexible hybridization of traditional CNNs and recurrent networks. Compared to traditional CNNs, we demonstrate substantial parameter savings on standard image classification tasks, while maintaining accuracy. Our simple parameter sharing scheme, though defined via soft weights, in practice often yields trained networks with near strict recurrent structure; with negligible side effects, they convert into networks with actual loops. Training these networks thus implicitly involves discovery of suitable recurrent architectures. Though considering only the aspect of recurrent links, our trained networks achieve accuracy competitive with those built using state-of-the-art neural architecture search (NAS) procedures. Our hybridization of recurrent and convolutional networks may also represent a beneficial architectural bias. Specifically, on synthetic tasks which are algorithmic in nature, our hybrid networks both train faster and extrapolate better to test examples outside the span of the training set",
    "checked": null,
    "id": "359cdea86e4203f73de4c12356fd9139dba9a745",
    "semantic_title": "learning implicitly recurrent cnns through parameter sharing",
    "citation_count": 70,
    "authors": []
  },
  "https://openreview.net/forum?id=ByldlhAqYQ": {
    "title": "Transfer Learning for Sequences via Learning to Collocate",
    "volume": "poster",
    "abstract": "Transfer learning aims to solve the data sparsity for a specific domain by applying information of another domain. Given a sequence (e.g. a natural language sentence), the transfer learning, usually enabled by recurrent neural network (RNN), represent the sequential information transfer. RNN uses a chain of repeating cells to model the sequence data. However, previous studies of neural network based transfer learning simply transfer the information across the whole layers, which are unfeasible for seq2seq and sequence labeling. Meanwhile, such layer-wise transfer learning mechanisms also lose the fine-grained cell-level information from the source domain. In this paper, we proposed the aligned recurrent transfer, ART, to achieve cell-level information transfer. ART is in a recurrent manner that different cells share the same parameters. Besides transferring the corresponding information at the same position, ART transfers information from all collocated words in the source domain. This strategy enables ART to capture the word collocation across domains in a more flexible way. We conducted extensive experiments on both sequence labeling tasks (POS tagging, NER) and sentence classification (sentiment analysis). ART outperforms the state-of-the-arts over all experiments",
    "checked": null,
    "id": "3b81ba581733e0c1e9c76b317645d2ee973fb710",
    "semantic_title": "transfer learning for sequences via learning to collocate",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=HJGven05Y7": {
    "title": "How to train your MAML",
    "volume": "poster",
    "abstract": "The field of few-shot learning has recently seen substantial advancements. Most of these advancements came from casting few-shot learning as a meta-learning problem.Model Agnostic Meta Learning or MAML is currently one of the best approaches for few-shot learning via meta-learning. MAML is simple, elegant and very powerful, however, it has a variety of issues, such as being very sensitive to neural network architectures, often leading to instability during training, requiring arduous hyperparameter searches to stabilize training and achieve high generalization and being very computationally expensive at both training and inference times. In this paper, we propose various modifications to MAML that not only stabilize the system, but also substantially improve the generalization performance, convergence speed and computational overhead of MAML, which we call MAML++",
    "checked": null,
    "id": "ba5267c42ec429cf9533d6ce07d8d3d03177b9ec",
    "semantic_title": "how to train your maml",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HyxPx3R9tm": {
    "title": "Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow",
    "volume": "poster",
    "abstract": "Adversarial learning methods have been proposed for a wide range of applications, but the training of adversarial models can be notoriously unstable. Effectively balancing the performance of the generator and discriminator is critical, since a discriminator that achieves very high accuracy will produce relatively uninformative gradients. In this work, we propose a simple and general technique to constrain information flow in the discriminator by means of an information bottleneck. By enforcing a constraint on the mutual information between the observations and the discriminator's internal representation, we can effectively modulate the discriminator's accuracy and maintain useful and informative gradients. We demonstrate that our proposed variational discriminator bottleneck (VDB) leads to significant improvements across three distinct application areas for adversarial learning algorithms. Our primary evaluation studies the applicability of the VDB to imitation learning of dynamic continuous control skills, such as running. We show that our method can learn such skills directly from raw video demonstrations, substantially outperforming prior adversarial imitation learning methods. The VDB can also be combined with adversarial inverse reinforcement learning to learn parsimonious reward functions that can be transferred and re-optimized in new settings. Finally, we demonstrate that VDB can train GANs more effectively for image generation, improving upon a number of prior stabilization methods",
    "checked": null,
    "id": "4b59846404c085f5c9523c69cd790537613a3df5",
    "semantic_title": "variational discriminator bottleneck: improving imitation learning, inverse rl, and gans by constraining information flow",
    "citation_count": 216,
    "authors": []
  },
  "https://openreview.net/forum?id=BJgLg3R9KQ": {
    "title": "Learning what and where to attend",
    "volume": "poster",
    "abstract": "Most recent gains in visual recognition have originated from the inclusion of attention mechanisms in deep convolutional networks (DCNs). Because these networks are optimized for object recognition, they learn where to attend using only a weak form of supervision derived from image class labels. Here, we demonstrate the benefit of using stronger supervisory signals by teaching DCNs to attend to image regions that humans deem important for object recognition. We first describe a large-scale online experiment (ClickMe) used to supplement ImageNet with nearly half a million human-derived \"top-down\" attention maps. Using human psychophysics, we confirm that the identified top-down features from ClickMe are more diagnostic than \"bottom-up\" saliency features for rapid image categorization. As a proof of concept, we extend a state-of-the-art attention network and demonstrate that adding ClickMe supervision significantly improves its accuracy and yields visual features that are more interpretable and more similar to those used by human observers",
    "checked": null,
    "id": "136c96810238657bf0c6f0d4b56b0e40e24f3c47",
    "semantic_title": "learning what and where to attend",
    "citation_count": 140,
    "authors": []
  },
  "https://openreview.net/forum?id=SygLehCqtm": {
    "title": "Learning protein sequence embeddings using information from structure",
    "volume": "poster",
    "abstract": "Inferring the structural properties of a protein from its amino acid sequence is a challenging yet important problem in biology. Structures are not known for the vast majority of protein sequences, but structure is critical for understanding function. Existing approaches for detecting structural similarity between proteins from sequence are unable to recognize and exploit structural patterns when sequences have diverged too far, limiting our ability to transfer knowledge between structurally related proteins. We newly approach this problem through the lens of representation learning. We introduce a framework that maps any protein sequence to a sequence of vector embeddings --- one per amino acid position --- that encode structural information. We train bidirectional long short-term memory (LSTM) models on protein sequences with a two-part feedback mechanism that incorporates information from (i) global structural similarity between proteins and (ii) pairwise residue contact maps for individual proteins. To enable learning from structural similarity information, we define a novel similarity measure between arbitrary-length sequences of vector embeddings based on a soft symmetric alignment (SSA) between them. Our method is able to learn useful position-specific embeddings despite lacking direct observations of position-level correspondence between sequences. We show empirically that our multi-task framework outperforms other sequence-based methods and even a top-performing structure-based alignment method when predicting structural similarity, our goal. Finally, we demonstrate that our learned embeddings can be transferred to other protein sequence problems, improving the state-of-the-art in transmembrane domain prediction",
    "checked": null,
    "id": "3637fcccc758786ae1c6529ab22fe85ea98e9c36",
    "semantic_title": "learning protein sequence embeddings using information from structure",
    "citation_count": 289,
    "authors": []
  },
  "https://openreview.net/forum?id=SJzSgnRcKX": {
    "title": "What do you learn from context? Probing for sentence structure in contextualized word representations",
    "volume": "poster",
    "abstract": "Contextualized representation models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline",
    "checked": null,
    "id": "e2587eddd57bc4ba286d91b27c185083f16f40ee",
    "semantic_title": "what do you learn from context? probing for sentence structure in contextualized word representations",
    "citation_count": 857,
    "authors": []
  },
  "https://openreview.net/forum?id=rye4g3AqFm": {
    "title": "Deep learning generalizes because the parameter-function map is biased towards simple functions",
    "volume": "poster",
    "abstract": "Deep neural networks (DNNs) generalize remarkably well without explicit regularization even in the strongly over-parametrized regime where classical learning theory would instead predict that they would severely overfit. While many proposals for some kind of implicit regularization have been made to rationalise this success, there is no consensus for the fundamental reason why DNNs do not strongly overfit. In this paper, we provide a new explanation. By applying a very general probability-complexity bound recently derived from algorithmic information theory (AIT), we argue that the parameter-function map of many DNNs should be exponentially biased towards simple functions. We then provide clear evidence for this strong simplicity bias in a model DNN for Boolean functions, as well as in much larger fully connected and convolutional networks trained on CIFAR10 and MNIST. As the target functions in many real problems are expected to be highly structured, this intrinsic simplicity bias helps explain why deep networks generalize well on real world problems. This picture also facilitates a novel PAC-Bayes approach where the prior is taken over the DNN input-output function space, rather than the more conventional prior over parameter space. If we assume that the training algorithm samples parameters close to uniformly within the zero-error region then the PAC-Bayes theorem can be used to guarantee good expected generalization for target functions producing high-likelihood training sets. By exploiting recently discovered connections between DNNs and Gaussian processes to estimate the marginal likelihood, we produce relatively tight generalization PAC-Bayes error bounds which correlate well with the true error on realistic datasets such as MNIST and CIFAR10 and for architectures including convolutional and fully connected networks",
    "checked": null,
    "id": "7a327286d3549be1607c1ce153b041b08be151f8",
    "semantic_title": "deep learning generalizes because the parameter-function map is biased towards simple functions",
    "citation_count": 231,
    "authors": []
  },
  "https://openreview.net/forum?id=SJgEl3A5tm": {
    "title": "CAMOU: Learning Physical Vehicle Camouflages to Adversarially Attack Detectors in the Wild",
    "volume": "poster",
    "abstract": "In this paper, we conduct an intriguing experimental study about the physical adversarial attack on object detectors in the wild. In particular, we learn a camouflage pattern to hide vehicles from being detected by state-of-the-art convolutional neural network based detectors. Our approach alternates between two threads. In the first, we train a neural approximation function to imitate how a simulator applies a camouflage to vehicles and how a vehicle detector performs given images of the camouflaged vehicles. In the second, we minimize the approximated detection score by searching for the optimal camouflage. Experiments show that the learned camouflage can not only hide a vehicle from the image-based detectors under many test cases but also generalizes to different environments, vehicles, and object detectors",
    "checked": null,
    "id": "2ee77a5fa75717118b62833f4cf111cfd0293aec",
    "semantic_title": "camou: learning physical vehicle camouflages to adversarially attack detectors in the wild",
    "citation_count": 104,
    "authors": []
  },
  "https://openreview.net/forum?id=Hke4l2AcKQ": {
    "title": "MAE: Mutual Posterior-Divergence Regularization for Variational AutoEncoders",
    "volume": "poster",
    "abstract": "Variational Autoencoder (VAE), a simple and effective deep generative model, has led to a number of impressive empirical successes and spawned many advanced variants and theoretical investigations. However, recent studies demonstrate that, when equipped with expressive generative distributions (aka. decoders), VAE suffers from learning uninformative latent representations with the observation called KL Varnishing, in which case VAE collapses into an unconditional generative model. In this work, we introduce mutual posterior-divergence regularization, a novel regularization that is able to control the geometry of the latent space to accomplish meaningful representation learning, while achieving comparable or superior capability of density estimation.Experiments on three image benchmark datasets demonstrate that, when equipped with powerful decoders, our model performs well both on density estimation and representation learning",
    "checked": null,
    "id": "c6d2a216fae7043e9251e5e87141482a1ffc68cd",
    "semantic_title": "mae: mutual posterior-divergence regularization for variational autoencoders",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=SkMQg3C5K7": {
    "title": "A Convergence Analysis of Gradient Descent for Deep Linear Neural Networks",
    "volume": "poster",
    "abstract": "We analyze speed of convergence to global optimum for gradient descent training a deep linear neural network by minimizing the L2 loss over whitened data. Convergence at a linear rate is guaranteed when the following hold: (i) dimensions of hidden layers are at least the minimum of the input and output dimensions; (ii) weight matrices at initialization are approximately balanced; and (iii) the initial loss is smaller than the loss of any rank-deficient solution. The assumptions on initialization (conditions (ii) and (iii)) are necessary, in the sense that violating any one of them may lead to convergence failure. Moreover, in the important case of output dimension 1, i.e. scalar regression, they are met, and thus convergence to global optimum holds, with constant probability under a random initialization scheme. Our results significantly extend previous analyses, e.g., of deep linear residual networks (Bartlett et al., 2018)",
    "checked": null,
    "id": "134c165953e23a6dc7d4f0d86989e92362ca4335",
    "semantic_title": "a convergence analysis of gradient descent for deep linear neural networks",
    "citation_count": 290,
    "authors": []
  },
  "https://openreview.net/forum?id=SkxXg2C5FX": {
    "title": "Don't Settle for Average, Go for the Max: Fuzzy Sets and Max-Pooled Word Vectors",
    "volume": "poster",
    "abstract": "Recent literature suggests that averaged word vectors followed by simple post-processing outperform many deep learning methods on semantic textual similarity tasks. Furthermore, when averaged word vectors are trained supervised on large corpora of paraphrases, they achieve state-of-the-art results on standard STS benchmarks. Inspired by these insights, we push the limits of word embeddings even further. We propose a novel fuzzy bag-of-words (FBoW) representation for text that contains all the words in the vocabulary simultaneously but with different degrees of membership, which are derived from similarities between word vectors. We show that max-pooled word vectors are only a special case of fuzzy BoW and should be compared via fuzzy Jaccard index rather than cosine similarity. Finally, we propose DynaMax, a completely unsupervised and non-parametric similarity measure that dynamically extracts and max-pools good features depending on the sentence pair. This method is both efficient and easy to implement, yet outperforms current baselines on STS tasks by a large margin and is even competitive with supervised word vectors trained to directly optimise cosine similarity",
    "checked": null,
    "id": "117b08d70961242283afdb412d9ab9004262d9d5",
    "semantic_title": "don't settle for average, go for the max: fuzzy sets and max-pooled word vectors",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=BygfghAcYX": {
    "title": "The role of over-parametrization in generalization of neural networks",
    "volume": "poster",
    "abstract": "Despite existing work on ensuring generalization of neural networks in terms of scale sensitive complexity measures, such as norms, margin and sharpness, these complexity measures do not offer an explanation of why neural networks generalize better with over-parametrization. In this work we suggest a novel complexity measure based on unit-wise capacities resulting in a tighter generalization bound for two layer ReLU networks. Our capacity bound correlates with the behavior of test error with increasing network sizes (within the range reported in the experiments), and could partly explain the improvement in generalization with over-parametrization. We further present a matching lower bound for the Rademacher complexity that improves over previous capacity lower bounds for neural networks",
    "checked": null,
    "id": "51063caeb691f9a20e34feb721f14660a0df968e",
    "semantic_title": "towards understanding the role of over-parametrization in generalization of neural networks",
    "citation_count": 383,
    "authors": []
  },
  "https://openreview.net/forum?id=H1x-x309tm": {
    "title": "On the Convergence of A Class of Adam-Type Algorithms for Non-Convex Optimization",
    "volume": "poster",
    "abstract": "This paper studies a class of adaptive gradient based momentum algorithms that update the search directions and learning rates simultaneously using past gradients. This class, which we refer to as the ''``Adam-type'', includes the popular algorithms such as Adam, AMSGrad, AdaGrad. Despite their popularity in training deep neural networks (DNNs), the convergence of these algorithms for solving non-convex problems remains an open question. In this paper, we develop an analysis framework and a set of mild sufficient conditions that guarantee the convergence of the Adam-type methods, with a convergence rate of order $O(\\log{T}/\\sqrt{T})$ for non-convex stochastic optimization. Our convergence analysis applies to a new algorithm called AdaFom (AdaGrad with First Order Momentum). We show that the conditions are essential, by identifying concrete examples in which violating the conditions makes an algorithm diverge. Besides providing one of the first comprehensive analysis for Adam-type methods in the non-convex setting, our results can also help the practitioners to easily monitor the progress of algorithms and determine their convergence behavior",
    "checked": null,
    "id": "c215b9ac79f07c8a43782b224f4416943837ffa8",
    "semantic_title": "on the convergence of a class of adam-type algorithms for non-convex optimization",
    "citation_count": 323,
    "authors": []
  },
  "https://openreview.net/forum?id=HJflg30qKX": {
    "title": "Gradient descent aligns the layers of deep linear networks",
    "volume": "poster",
    "abstract": "This paper establishes risk convergence and asymptotic weight matrix alignment --- a form of implicit regularization --- of gradient flow and gradient descent when applied to deep linear networks on linearly separable data. In more detail, for gradient flow applied to strictly decreasing loss functions (with similar results for gradient descent with particular decreasing step sizes): (i) the risk converges to 0; (ii) the normalized i-th weight matrix asymptotically equals its rank-1 approximation u_iv_i^T; (iii) these rank-1 matrices are aligned across layers, meaning |v_{i+1}^T u_i| -> 1. In the case of the logistic loss (binary cross entropy), more can be said: the linear function induced by the network --- the product of its weight matrices --- converges to the same direction as the maximum margin solution. This last property was identified in prior work, but only under assumptions on gradient descent which here are implied by the alignment phenomenon",
    "checked": null,
    "id": "5786917220aab2f6d0b00606eee9fe0ad0700f1b",
    "semantic_title": "gradient descent aligns the layers of deep linear networks",
    "citation_count": 254,
    "authors": []
  },
  "https://openreview.net/forum?id=SJz1x20cFQ": {
    "title": "Hierarchical RL Using an Ensemble of Proprioceptive Periodic Policies",
    "volume": "poster",
    "abstract": "In this paper we introduce a simple, robust approach to hierarchically training an agent in the setting of sparse reward tasks. The agent is split into a low-level and a high-level policy. The low-level policy only accesses internal, proprioceptive dimensions of the state observation. The low-level policies are trained with a simple reward that encourages changing the values of the non-proprioceptive dimensions. Furthermore, it is induced to be periodic with the use a ``phase function.'' The high-level policy is trained using a sparse, task-dependent reward, and operates by choosing which of the low-level policies to run at any given time. Using this approach, we solve difficult maze and navigation tasks with sparse rewards using the Mujoco Ant and Humanoid agents and show improvement over recent hierarchical methods",
    "checked": null,
    "id": "622cd0cc96d437481d8ca1de5ea5400e655efcc3",
    "semantic_title": "hierarchical rl using an ensemble of proprioceptive periodic policies",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=HJgkx2Aqt7": {
    "title": "Learning To Simulate",
    "volume": "poster",
    "abstract": "Simulation is a useful tool in situations where training data for machine learning models is costly to annotate or even hard to acquire. In this work, we propose a reinforcement learning-based method for automatically adjusting the parameters of any (non-differentiable) simulator, thereby controlling the distribution of synthesized data in order to maximize the accuracy of a model trained on that data. In contrast to prior art that hand-crafts these simulation parameters or adjusts only parts of the available parameters, our approach fully controls the simulator with the actual underlying goal of maximizing accuracy, rather than mimicking the real data distribution or randomly generating a large volume of data. We find that our approach (i) quickly converges to the optimal simulation parameters in controlled experiments and (ii) can indeed discover good sets of parameters for an image rendering simulator in actual computer vision applications",
    "checked": null,
    "id": "73b226ea356f14e6751f16871da2e357a7989990",
    "semantic_title": "learning to simulate",
    "citation_count": 119,
    "authors": []
  },
  "https://openreview.net/forum?id=Skeke3C5Fm": {
    "title": "Multilingual Neural Machine Translation With Soft Decoupled Encoding",
    "volume": "poster",
    "abstract": "Multilingual training of neural machine translation (NMT) systems has led to impressive accuracy improvements on low-resource languages. However, there are still significant challenges in efficiently learning word representations in the face of paucity of data. In this paper, we propose Soft Decoupled Encoding (SDE), a multilingual lexicon encoding framework specifically designed to share lexical-level information intelligently without requiring heuristic preprocessing such as pre-segmenting the data. SDE represents a word by its spelling through a character encoding, and its semantic meaning through a latent embedding space shared by all languages. Experiments on a standard dataset of four low-resource languages show consistent improvements over strong multilingual NMT baselines, with gains of up to 2 BLEU on one of the tested languages, achieving the new state-of-the-art on all four language pairs",
    "checked": null,
    "id": "be312e930f6739a709e60547aa0dfb9c3dc44497",
    "semantic_title": "multilingual neural machine translation with soft decoupled encoding",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=BJgklhAcK7": {
    "title": "Meta-Learning with Latent Embedding Optimization",
    "volume": "poster",
    "abstract": "Gradient-based meta-learning techniques are both widely applicable and proficient at solving challenging few-shot learning and fast adaptation problems. However, they have practical difficulties when operating on high-dimensional parameter spaces in extreme low-data regimes. We show that it is possible to bypass these limitations by learning a data-dependent latent generative representation of model parameters, and performing gradient-based meta-learning in this low-dimensional latent space. The resulting approach, latent embedding optimization (LEO), decouples the gradient-based adaptation procedure from the underlying high-dimensional space of model parameters. Our evaluation shows that LEO can achieve state-of-the-art performance on the competitive miniImageNet and tieredImageNet few-shot classification tasks. Further analysis indicates LEO is able to capture uncertainty in the data, and can perform adaptation more effectively by optimizing in latent space",
    "checked": null,
    "id": "04f739a0c29b75877243731aeead512bf0ed1dff",
    "semantic_title": "meta-learning with latent embedding optimization",
    "citation_count": 1370,
    "authors": []
  },
  "https://openreview.net/forum?id=HJeRkh05Km": {
    "title": "Visual Semantic Navigation using Scene Priors",
    "volume": "poster",
    "abstract": "How do humans navigate to target objects in novel scenes? Do we use the semantic/functional priors we have built over years to efficiently search and navigate? For example, to search for mugs, we search cabinets near the coffee machine and for fruits we try the fridge. In this work, we focus on incorporating semantic priors in the task of semantic navigation. We propose to use Graph Convolutional Networks for incorporating the prior knowledge into a deep reinforcement learning framework. The agent uses the features from the knowledge graph to predict the actions. For evaluation, we use the AI2-THOR framework. Our experiments show how semantic knowledge improves the performance significantly. More importantly, we show improvement in generalization to unseen scenes and/or objects",
    "checked": null,
    "id": "c7aea4b653d4e12cb47438960f5689f5f835e073",
    "semantic_title": "visual semantic navigation using scene priors",
    "citation_count": 323,
    "authors": []
  },
  "https://openreview.net/forum?id=HkgTkhRcKQ": {
    "title": "AdaShift: Decorrelation and Convergence of Adaptive Learning Rate Methods",
    "volume": "poster",
    "abstract": "Adam is shown not being able to converge to the optimal solution in certain cases. Researchers recently propose several algorithms to avoid the issue of non-convergence of Adam, but their efficiency turns out to be unsatisfactory in practice. In this paper, we provide a new insight into the non-convergence issue of Adam as well as other adaptive learning rate methods. We argue that there exists an inappropriate correlation between gradient $g_t$ and the second moment term $v_t$ in Adam ($t$ is the timestep), which results in that a large gradient is likely to have small step size while a small gradient may have a large step size. We demonstrate that such unbalanced step sizes are the fundamental cause of non-convergence of Adam, and we further prove that decorrelating $v_t$ and $g_t$ will lead to unbiased step size for each gradient, thus solving the non-convergence problem of Adam. Finally, we propose AdaShift, a novel adaptive learning rate method that decorrelates $v_t$ and $g_t$ by temporal shifting, i.e., using temporally shifted gradient $g_{t-n}$ to calculate $v_t$. The experiment results demonstrate that AdaShift is able to address the non-convergence issue of Adam, while still maintaining a competitive performance with Adam in terms of both training speed and generalization",
    "checked": null,
    "id": "9f88722cbc4107e3c3d0e1c7934cc7f1d5ae4fdb",
    "semantic_title": "adashift: decorrelation and convergence of adaptive learning rate methods",
    "citation_count": 66,
    "authors": []
  },
  "https://openreview.net/forum?id=H1xaJn05FQ": {
    "title": "Sliced Wasserstein Auto-Encoders",
    "volume": "poster",
    "abstract": "In this paper we use the geometric properties of the optimal transport (OT) problem and the Wasserstein distances to define a prior distribution for the latent space of an auto-encoder. We introduce Sliced-Wasserstein Auto-Encoders (SWAE), that enable one to shape the distribution of the latent space into any samplable probability distribution without the need for training an adversarial network or having a likelihood function specified. In short, we regularize the auto-encoder loss with the sliced-Wasserstein distance between the distribution of the encoded training samples and a samplable prior distribution. We show that the proposed formulation has an efficient numerical solution that provides similar capabilities to Wasserstein Auto-Encoders (WAE) and Variational Auto-Encoders (VAE), while benefiting from an embarrassingly simple implementation. We provide extensive error analysis for our algorithm, and show its merits on three benchmark datasets",
    "checked": null,
    "id": "da798da448db9dd8707b55efb856b5c7ad5b6c00",
    "semantic_title": "sliced wasserstein auto-encoders",
    "citation_count": 148,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgpy3C5tX": {
    "title": "Amortized Bayesian Meta-Learning",
    "volume": "poster",
    "abstract": "Meta-learning, or learning-to-learn, has proven to be a successful strategy in attacking problems in supervised learning and reinforcement learning that involve small amounts of data. State-of-the-art solutions involve learning an initialization and/or learning algorithm using a set of training episodes so that the meta learner can generalize to an evaluation episode quickly. These methods perform well but often lack good quantification of uncertainty, which can be vital to real-world applications when data is lacking. We propose a meta-learning method which efficiently amortizes hierarchical variational inference across tasks, learning a prior distribution over neural network weights so that a few steps of Bayes by Backprop will produce a good task-specific approximate posterior. We show that our method produces good uncertainty estimates on contextual bandit and few-shot learning benchmarks",
    "checked": null,
    "id": "2686aef553af3ebd43cb0ea98a538a9435f7dd5f",
    "semantic_title": "amortized bayesian meta-learning",
    "citation_count": 137,
    "authors": []
  },
  "https://openreview.net/forum?id=S1g2JnRcFX": {
    "title": "Local SGD Converges Fast and Communicates Little",
    "volume": "poster",
    "abstract": "Mini-batch stochastic gradient descent (SGD) is state of the art in large scale distributed training. The scheme can reach a linear speed-up with respect to the number of workers, but this is rarely seen in practice as the scheme often suffers from large network delays and bandwidth limits. To overcome this communication bottleneck recent works propose to reduce the communication frequency. An algorithm of this type is local SGD that runs SGD independently in parallel on different workers and averages the sequences only once in a while. This scheme shows promising results in practice, but eluded thorough theoretical analysis. We prove concise convergence rates for local SGD on convex problems and show that it converges at the same rate as mini-batch SGD in terms of number of evaluated gradients, that is, the scheme achieves linear speed-up in the number of workers and mini-batch size. The number of communication rounds can be reduced up to a factor of T^{1/2}---where T denotes the number of total steps---compared to mini-batch SGD. This also holds for asynchronous implementations. Local SGD can also be used for large scale training of deep learning models. The results shown here aim serving as a guideline to further explore the theoretical and practical aspects of local SGD in these applications",
    "checked": null,
    "id": "7cfa76a82be96c74b2eff514265b7fd271a179cd",
    "semantic_title": "local sgd converges fast and communicates little",
    "citation_count": 891,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgoyn09KQ": {
    "title": "textTOvec: DEEP CONTEXTUALIZED NEURAL AUTOREGRESSIVE TOPIC MODELS OF LANGUAGE WITH DISTRIBUTED COMPOSITIONAL PRIOR",
    "volume": "poster",
    "abstract": "We address two challenges of probabilistic topic modelling in order to better estimate the probability of a word in a given context, i.e., P(wordjcontext) : (1) No Language Structure in Context: Probabilistic topic models ignore word order by summarizing a given context as a \"bag-of-word\" and consequently the semantics of words in the context is lost. In this work, we incorporate language structure by combining a neural autoregressive topic model (TM) with a LSTM based language model (LSTM-LM) in a single probabilistic framework. The LSTM-LM learns a vector-space representation of each word by accounting for word order in local collocation patterns, while the TM simultaneously learns a latent representation from the entire document. In addition, the LSTM-LM models complex characteristics of language (e.g., syntax and semantics), while the TM discovers the underlying thematic structure in a collection of documents. We unite two complementary paradigms of learning the meaning of word occurrences by combining a topic model and a language model in a unified probabilistic framework, named as ctx-DocNADE. (2) Limited Context and/or Smaller training corpus of documents: In settings with a small number of word occurrences (i.e., lack of context) in short text or data sparsity in a corpus of few documents, the application of TMs is challenging. We address this challenge by incorporating external knowledge into neural autoregressive topic models via a language modelling approach: we use word embeddings as input of a LSTM-LM with the aim to improve the wordtopic mapping on a smaller and/or short-text corpus. The proposed DocNADE extension is named as ctx-DocNADEe. We present novel neural autoregressive topic model variants coupled with neural language models and embeddings priors that consistently outperform state-of-theart generative topic models in terms of generalization (perplexity), interpretability (topic coherence) and applicability (retrieval and classification) over 6 long-text and 8 short-text datasets from diverse domains",
    "checked": null,
    "id": "ed01746601d9cb7724568645d8b53876f6cd7c16",
    "semantic_title": "texttovec: deep contextualized neural autoregressive models of language with distributed compositional prior",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=ByloJ20qtm": {
    "title": "Neural Program Repair by Jointly Learning to Localize and Repair",
    "volume": "poster",
    "abstract": "Due to its potential to improve programmer productivity and software quality, automated program repair has been an active topic of research. Newer techniques harness neural networks to learn directly from examples of buggy programs and their fixes. In this work, we consider a recently identified class of bugs called variable-misuse bugs. The state-of-the-art solution for variable misuse enumerates potential fixes for all possible bug locations in a program, before selecting the best prediction. We show that it is beneficial to train a model that jointly and directly localizes and repairs variable-misuse bugs. We present multi-headed pointer networks for this purpose, with one head each for localization and repair. The experimental results show that the joint model significantly outperforms an enumerative solution that uses a pointer based model for repair alone",
    "checked": null,
    "id": "564bce85c8ad9a50f4652a4d05e1ed0aaa22df49",
    "semantic_title": "neural program repair by jointly learning to localize and repair",
    "citation_count": 131,
    "authors": []
  },
  "https://openreview.net/forum?id=ryl5khRcKm": {
    "title": "Human-level Protein Localization with Convolutional Neural Networks",
    "volume": "poster",
    "abstract": "Localizing a specific protein in a human cell is essential for understanding cellular functions and biological processes of underlying diseases. A promising, low-cost,and time-efficient biotechnology for localizing proteins is high-throughput fluorescence microscopy imaging (HTI). This imaging technique stains the protein of interest in a cell with fluorescent antibodies and subsequently takes a microscopic image. Together with images of other stained proteins or cell organelles and the annotation by the Human Protein Atlas project, these images provide a rich source of information on the protein location which can be utilized by computational methods. It is yet unclear how precise such methods are and whether they can compete with human experts. We here focus on deep learning image analysis methods and, in particular, on Convolutional Neural Networks (CNNs)since they showed overwhelming success across different imaging tasks. We pro-pose a novel CNN architecture \"GapNet-PL\" that has been designed to tackle the characteristics of HTI data and uses global averages of filters at different abstraction levels. We present the largest comparison of CNN architectures including GapNet-PL for protein localization in HTI images of human cells. GapNet-PL outperforms all other competing methods and reaches close to perfect localization in all 13 tasks with an average AUC of 98% and F1 score of 78%. On a separate test set the performance of GapNet-PL was compared with three human experts and 25 scholars. GapNet-PL achieved an accuracy of 91%, significantly (p-value 1.1e‚àí6) outperforming the best human expert with an accuracy of 72%",
    "checked": null,
    "id": "6a65652fd2735d546281dd54c912d6eeaa17546c",
    "semantic_title": "human-level protein localization with convolutional neural networks",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=r1lq1hRqYQ": {
    "title": "From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following",
    "volume": "poster",
    "abstract": "Reinforcement learning is a promising framework for solving control problems, but its use in practical situations is hampered by the fact that reward functions are often difficult to engineer. Specifying goals and tasks for autonomous machines, such as robots, is a significant challenge: conventionally, reward functions and goal states have been used to communicate objectives. But people can communicate objectives to each other simply by describing or demonstrating them. How can we build learning algorithms that will allow us to tell machines what we want them to do? In this work, we investigate the problem of grounding language commands as reward functions using inverse reinforcement learning, and argue that language-conditioned rewards are more transferable than language-conditioned policies to new environments. We propose language-conditioned reward learning (LC-RL), which grounds language commands as a reward function represented by a deep neural network. We demonstrate that our model learns rewards that transfer to novel tasks and environments on realistic, high-dimensional visual environments with natural language commands, whereas directly learning a language-conditioned policy leads to poor performance",
    "checked": null,
    "id": "758311575a6385bb15d4f9af8c0e671cb98184b4",
    "semantic_title": "from language to goals: inverse reinforcement learning for vision-based instruction following",
    "citation_count": 125,
    "authors": []
  },
  "https://openreview.net/forum?id=HklY120cYm": {
    "title": "ClariNet: Parallel Wave Generation in End-to-End Text-to-Speech",
    "volume": "poster",
    "abstract": "In this work, we propose a new solution for parallel wave generation by WaveNet. In contrast to parallel WaveNet (van Oord et al., 2018), we distill a Gaussian inverse autoregressive flow from the autoregressive WaveNet by minimizing a regularized KL divergence between their highly-peaked output distributions. Our method computes the KL divergence in closed-form, which simplifies the training algorithm and provides very efficient distillation. In addition, we introduce the first text-to-wave neural architecture for speech synthesis, which is fully convolutional and enables fast end-to-end training from scratch. It significantly outperforms the previous pipeline that connects a text-to-spectrogram model to a separately trained WaveNet (Ping et al., 2018). We also successfully distill a parallel waveform synthesizer conditioned on the hidden representation in this end-to-end model",
    "checked": null,
    "id": "5e3a59695261f03aa3f09a8a5ac6166fb63e0a2e",
    "semantic_title": "clarinet: parallel wave generation in end-to-end text-to-speech",
    "citation_count": 346,
    "authors": []
  },
  "https://openreview.net/forum?id=SyxtJh0qYm": {
    "title": "Variational Autoencoder with Arbitrary Conditioning",
    "volume": "poster",
    "abstract": "We propose a single neural probabilistic model based on variational autoencoder that can be conditioned on an arbitrary subset of observed features and then sample the remaining features in \"one shot\". The features may be both real-valued and categorical. Training of the model is performed by stochastic variational Bayes. The experimental evaluation on synthetic data, as well as feature imputation and image inpainting problems, shows the effectiveness of the proposed approach and diversity of the generated samples",
    "checked": null,
    "id": "c61b43acd00d73e58fade68b8eb7c3ae875fc60c",
    "semantic_title": "variational autoencoder with arbitrary conditioning",
    "citation_count": 147,
    "authors": []
  },
  "https://openreview.net/forum?id=BJluy2RcFm": {
    "title": "Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs",
    "volume": "poster",
    "abstract": "We consider a simple and overarching representation for permutation-invariant functions of sequences (or set functions). Our approach, which we call Janossy pooling, expresses a permutation-invariant function as the average of a permutation-sensitive function applied to all reorderings of the input sequence. This allows us to leverage the rich and mature literature on permutation-sensitive functions to construct novel and flexible permutation-invariant functions. If carried out naively, Janossy pooling can be computationally prohibitive. To allow computational tractability, we consider three kinds of approximations: canonical orderings of sequences, functions with k-order interactions, and stochastic optimization algorithms with random permutations. Our framework unifies a variety of existing work in the literature, and suggests possible modeling and algorithmic extensions. We explore a few in our experiments, which demonstrate improved performance over current state-of-the-art methods",
    "checked": null,
    "id": "de9550945b2f631c541c299114a770c4f47f9616",
    "semantic_title": "janossy pooling: learning deep permutation-invariant functions for variable-size inputs",
    "citation_count": 190,
    "authors": []
  },
  "https://openreview.net/forum?id=ByGuynAct7": {
    "title": "The Deep Weight Prior",
    "volume": "poster",
    "abstract": "Bayesian inference is known to provide a general framework for incorporating prior knowledge or specific properties into machine learning models via carefully choosing a prior distribution. In this work, we propose a new type of prior distributions for convolutional neural networks, deep weight prior (DWP), that exploit generative models to encourage a specific structure of trained convolutional filters e.g., spatial correlations of weights. We define DWP in the form of an implicit distribution and propose a method for variational inference with such type of implicit priors. In experiments, we show that DWP improves the performance of Bayesian neural networks when training data are limited, and initialization of weights with samples from DWP accelerates training of conventional convolutional neural networks",
    "checked": null,
    "id": "cf9d672680312c086b2277dbaa04bd37cb295d6e",
    "semantic_title": "the deep weight prior",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=rkluJ2R9KQ": {
    "title": "A new dog learns old tricks: RL finds classic optimization algorithms",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "0029ccaa0047d8af36e34bf555c33cd09ce4cb31",
    "semantic_title": "a new dog learns old tricks: rl finds classic optimization algorithms",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=HJgd1nAqFX": {
    "title": "DOM-Q-NET: Grounded RL on Structured Language",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "d7d5406eabff0e0e7fc5f7d8706297d85446e4b2",
    "semantic_title": "dom-q-net: grounded rl on structured language",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=ryxwJhC9YX": {
    "title": "InstaGAN: Instance-aware Image-to-Image Translation",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "af7994e85e6900c0eaf48a5d80fbf5c127f58644",
    "semantic_title": "instagan: instance-aware image-to-image translation",
    "citation_count": 157,
    "authors": []
  },
  "https://openreview.net/forum?id=SyNPk2R9K7": {
    "title": "Learning to Describe Scenes with Programs",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "58dcc24c63ed179d9a9b458d5f1284a7a297c5d6",
    "semantic_title": "learning to describe scenes with programs",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=HJfwJ2A5KX": {
    "title": "Data-Dependent Coresets for Compressing Neural Networks with Applications to Generalization Bounds",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "f61fc6c49d3b22f297b7ccb72bbfe6f7c1344e78",
    "semantic_title": "data-dependent coresets for compressing neural networks with applications to generalization bounds",
    "citation_count": 79,
    "authors": []
  },
  "https://openreview.net/forum?id=rJg8yhAqKm": {
    "title": "InfoBot: Transfer and Exploration via the Information Bottleneck",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "bf7f1ada5feecc0992f71b39c1ebeccb19ae631b",
    "semantic_title": "infobot: transfer and exploration via the information bottleneck",
    "citation_count": 166,
    "authors": []
  },
  "https://openreview.net/forum?id=BylE1205Fm": {
    "title": "Emerging Disentanglement in Auto-Encoder Based Unsupervised Image Content Transfer",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "98f0f57e5c9425c81e24618b00b364bbcb61338b",
    "semantic_title": "emerging disentanglement in auto-encoder based unsupervised image content transfer",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=rJg4J3CqFm": {
    "title": "Learning Embeddings into Entropic Wasserstein Spaces",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "8fec21b4553cb8eb72345aff30b35bcffd456378",
    "semantic_title": "learning embeddings into entropic wasserstein spaces",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=H1g4k309F7": {
    "title": "Wasserstein Barycenter Model Ensembling",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "3c50284257e0180133d5d71da20bd1c7e089b85c",
    "semantic_title": "wasserstein barycenter model ensembling",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=Hyx4knR9Ym": {
    "title": "Generalizable Adversarial Training via Spectral Normalization",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "3a606480406886742572a956e221e986c65d94c1",
    "semantic_title": "generalizable adversarial training via spectral normalization",
    "citation_count": 139,
    "authors": []
  },
  "https://openreview.net/forum?id=Bylmkh05KX": {
    "title": "Unsupervised Speech Recognition via Segmental Empirical Output Distribution Matching",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "ad4b7c0e1c71e26c1cf352069e9fd5cb06f70148",
    "semantic_title": "unsupervised speech recognition via segmental empirical output distribution matching",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=rye7knCqK7": {
    "title": "Learning when to Communicate at Scale in Multiagent Cooperative and Competitive Tasks",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "320f80ad1a95de5824b9477c4e3cec37ee007d10",
    "semantic_title": "learning when to communicate at scale in multiagent cooperative and competitive tasks",
    "citation_count": 243,
    "authors": []
  },
  "https://openreview.net/forum?id=HyzMyhCcK7": {
    "title": "ProxQuant: Quantized Neural Networks via Proximal Operators",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "7c42d7ff616efc45a42b264b0da6c74e8141a9ed",
    "semantic_title": "proxquant: quantized neural networks via proximal operators",
    "citation_count": 117,
    "authors": []
  },
  "https://openreview.net/forum?id=rkMW1hRqKX": {
    "title": "Optimal Completion Distillation for Sequence Learning",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "6ffe584f5edad5e343a30892ec54f1ef8b8e740e",
    "semantic_title": "optimal completion distillation for sequence learning",
    "citation_count": 45,
    "authors": []
  },
  "https://openreview.net/forum?id=SyxZJn05YX": {
    "title": "Feature Intertwiner for Object Detection",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "d2420d9ce64d101b28641660b4641c415fc7a6c9",
    "semantic_title": "feature intertwiner for object detection",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=BkxWJnC9tX": {
    "title": "Diversity and Depth in Per-Example Routing Models",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "7b7482c433da5e3b52224524206f36bda4c14edf",
    "semantic_title": "diversity and depth in per-example routing models",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=Hke-JhA9Y7": {
    "title": "Learning concise representations for regression by evolving networks of trees",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "a7dc6bb1ffb17fa9428dc12303a9d762c959fd00",
    "semantic_title": "learning concise representations for regression by evolving networks of trees",
    "citation_count": 58,
    "authors": []
  },
  "https://openreview.net/forum?id=H1lJJnR5Ym": {
    "title": "Exploration by random network distillation",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "4cb3fd057949624aa4f0bbe7a6dcc8777ff04758",
    "semantic_title": "exploration by random network distillation",
    "citation_count": 1327,
    "authors": []
  },
  "https://openreview.net/forum?id=rygkk305YQ": {
    "title": "Hierarchical Generative Modeling for Controllable Speech Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "3321263fd0b2be6011f20d7b74b8ae801741eb21",
    "semantic_title": "hierarchical generative modeling for controllable speech synthesis",
    "citation_count": 260,
    "authors": []
  },
  "https://openreview.net/forum?id=Bkx0RjA9tX": {
    "title": "Generative Question Answering: Learning to Answer the Whole Question",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "6182aed90596acd1573bd5ccbc2284b1e8a7291b",
    "semantic_title": "generative question answering: learning to answer the whole question",
    "citation_count": 73,
    "authors": []
  },
  "https://openreview.net/forum?id=Bkg6RiCqY7": {
    "title": "Decoupled Weight Decay Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "d07284a6811f1b2745d91bdb06b040b57f226882",
    "semantic_title": "decoupled weight decay regularization",
    "citation_count": 22860,
    "authors": []
  },
  "https://openreview.net/forum?id=rkl6As0cF7": {
    "title": "Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "bb905c6b8d9abe84adad1c6784e6037dc9516e95",
    "semantic_title": "probabilistic recursive reasoning for multi-agent reinforcement learning",
    "citation_count": 115,
    "authors": []
  },
  "https://openreview.net/forum?id=BJl6AjC5F7": {
    "title": "Learning to Represent Edits",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "f6224ed8b4969aee883c8c0a57444b79f681d499",
    "semantic_title": "learning to represent edits",
    "citation_count": 112,
    "authors": []
  },
  "https://openreview.net/forum?id=BklhAj09K7": {
    "title": "Unsupervised Domain Adaptation for Distance Metric Learning",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "c9c3e58677b95a110220ad89946fd941fa1b287e",
    "semantic_title": "unsupervised domain adaptation for distance metric learning",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=B1g30j0qF7": {
    "title": "Bayesian Deep Convolutional Networks with Many Channels are Gaussian Processes",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "9ff05248a26c637a4fc9a11e08608574467d6320",
    "semantic_title": "bayesian deep convolutional networks with many channels are gaussian processes",
    "citation_count": 308,
    "authors": []
  },
  "https://openreview.net/forum?id=Hke20iA9Y7": {
    "title": "Efficient Training on Very Large Corpora via Gramian Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "88c31abd700b40e3237472f8c296365cd9f079b1",
    "semantic_title": "efficient training on very large corpora via gramian estimation",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=BkloRs0qK7": {
    "title": "A comprehensive, application-oriented study of catastrophic forgetting in DNNs",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "eaf6a986ef90b442d6363ee54852d9b5b4788b0f",
    "semantic_title": "a comprehensive, application-oriented study of catastrophic forgetting in dnns",
    "citation_count": 90,
    "authors": []
  },
  "https://openreview.net/forum?id=SJgsCjCqt7": {
    "title": "Variational Autoencoders with Jointly Optimized Latent Dependency Structure",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "0f2d473dd476f4e8775081af57e0ef6f9d4c70bb",
    "semantic_title": "variational autoencoders with jointly optimized latent dependency structure",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=HyeFAsRctQ": {
    "title": "Verification of Non-Linear Specifications for Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "ad640ea420d49a969a995632aed22c21251891df",
    "semantic_title": "verification of non-linear specifications for neural networks",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=S1xtAjR5tX": {
    "title": "Improving Sequence-to-Sequence Learning via Optimal Transport",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "717daba98eb57b898687fc013b705f763eb2916b",
    "semantic_title": "improving sequence-to-sequence learning via optimal transport",
    "citation_count": 94,
    "authors": []
  },
  "https://openreview.net/forum?id=SyVuRiC5K7": {
    "title": "LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "a3d7257cf0ba5c501d2f6b8ddab931bcd588dfbf",
    "semantic_title": "learning to propagate labels: transductive propagation network for few-shot learning",
    "citation_count": 667,
    "authors": []
  },
  "https://openreview.net/forum?id=HyzdRiR9Y7": {
    "title": "Universal Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "ac4dafdef1d2b685b7f28a11837414573d39ff4e",
    "semantic_title": "universal transformers",
    "citation_count": 752,
    "authors": []
  },
  "https://openreview.net/forum?id=rJfUCoR5KX": {
    "title": "An Empirical study of Binary Neural Networks' Optimisation",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "a2980c4ab69e017e72ca36db0ac3a0a427ff1aee",
    "semantic_title": "an empirical study of binary neural networks' optimisation",
    "citation_count": 90,
    "authors": []
  },
  "https://openreview.net/forum?id=rylIAsCqYm": {
    "title": "A2BCD: Asynchronous Acceleration with Optimal Complexity",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "81f7e93492a1b3effb239d482d41b76093c44ec7",
    "semantic_title": "a2bcd: asynchronous acceleration with optimal complexity",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=r1lrAiA5Ym": {
    "title": "Backpropamine: training self-modifying neural networks with differentiable neuromodulated plasticity",
    "volume": "poster",
    "abstract": "The impressive lifelong learning in animal brains is primarily enabled by plastic changes in synaptic connectivity. Importantly, these changes are not passive, but are actively controlled by neuromodulation, which is itself under the control of the brain. The resulting self-modifying abilities of the brain play an important role in learning and adaptation, and are a major basis for biological reinforcement learning. Here we show for the first time that artificial neural networks with such neuromodulated plasticity can be trained with gradient descent. Extending previous work on differentiable Hebbian plasticity, we propose a differentiable formulation for the neuromodulation of plasticity. We show that neuromodulated plasticity improves the performance of neural networks on both reinforcement learning and supervised learning tasks. In one task, neuromodulated plastic LSTMs with millions of parameters outperform standard LSTMs on a benchmark language modeling task (controlling for the number of parameters). We conclude that differentiable neuromodulation of plasticity offers a powerful new framework for training neural networks",
    "checked": null,
    "id": "146c231532d4e38de95e63368dcd09d0f8cea291",
    "semantic_title": "backpropamine: training self-modifying neural networks with differentiable neuromodulated plasticity",
    "citation_count": 90,
    "authors": []
  },
  "https://openreview.net/forum?id=S1EERs09YQ": {
    "title": "Discovery of Natural Language Concepts in Individual Units of CNNs",
    "volume": "poster",
    "abstract": "Although deep convolutional networks have achieved improved performance in many natural language tasks, they have been treated as black boxes because they are difficult to interpret. Especially, little is known about how they represent language in their intermediate layers. In an attempt to understand the representations of deep convolutional networks trained on language tasks, we show that individual units are selectively responsive to specific morphemes, words, and phrases, rather than responding to arbitrary and uninterpretable patterns. In order to quantitatively analyze such intriguing phenomenon, we propose a concept alignment method based on how units respond to replicated text. We conduct analyses with different architectures on multiple datasets for classification and translation tasks and provide new insights into how deep models understand natural language",
    "checked": null,
    "id": "55c1222c959435867c12e87e2a5c9a7f3a5d1e4b",
    "semantic_title": "discovery of natural language concepts in individual units of cnns",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=ryzECoAcY7": {
    "title": "Learning Multi-Level Hierarchies with Hindsight",
    "volume": "poster",
    "abstract": "Hierarchical agents have the potential to solve sequential decision making tasks with greater sample efficiency than their non-hierarchical counterparts because hierarchical agents can break down tasks into sets of subtasks that only require short sequences of decisions. In order to realize this potential of faster learning, hierarchical agents need to be able to learn their multiple levels of policies in parallel so these simpler subproblems can be solved simultaneously. Yet, learning multiple levels of policies in parallel is hard because it is inherently unstable: changes in a policy at one level of the hierarchy may cause changes in the transition and reward functions at higher levels in the hierarchy, making it difficult to jointly learn multiple levels of policies. In this paper, we introduce a new Hierarchical Reinforcement Learning (HRL) framework, Hierarchical Actor-Critic (HAC), that can overcome the instability issues that arise when agents try to jointly learn multiple levels of policies. The main idea behind HAC is to train each level of the hierarchy independently of the lower levels by training each level as if the lower level policies are already optimal. We demonstrate experimentally in both grid world and simulated robotics domains that our approach can significantly accelerate learning relative to other non-hierarchical and hierarchical methods. Indeed, our framework is the first to successfully learn 3-level hierarchies in parallel in tasks with continuous state and action spaces",
    "checked": null,
    "id": "17704b148b5c20ddf92acbaf1addda134ecbb474",
    "semantic_title": "learning multi-level hierarchies with hindsight",
    "citation_count": 313,
    "authors": []
  },
  "https://openreview.net/forum?id=SkxXCi0qFX": {
    "title": "ProMP: Proximal Meta-Policy Search",
    "volume": "poster",
    "abstract": "Credit assignment in Meta-reinforcement learning (Meta-RL) is still poorly understood. Existing methods either neglect credit assignment to pre-adaptation behavior or implement it naively. This leads to poor sample-efficiency during meta-training as well as ineffective task identification strategies. This paper provides a theoretical analysis of credit assignment in gradient-based Meta-RL. Building on the gained insights we develop a novel meta-learning algorithm that overcomes both the issue of poor credit assignment and previous difficulties in estimating meta-policy gradients. By controlling the statistical distance of both pre-adaptation and adapted policies during meta-policy search, the proposed algorithm endows efficient and stable meta-learning. Our approach leads to superior pre-adaptation policy behavior and consistently outperforms previous Meta-RL algorithms in sample-efficiency, wall-clock time, and asymptotic performance",
    "checked": null,
    "id": "c456941a270cb2040eed4abfb39150508caf920c",
    "semantic_title": "promp: proximal meta-policy search",
    "citation_count": 210,
    "authors": []
  },
  "https://openreview.net/forum?id=HyM7AiA5YX": {
    "title": "Complement Objective Training",
    "volume": "poster",
    "abstract": "Learning with a primary objective, such as softmax cross entropy for classification and sequence generation, has been the norm for training deep neural networks for years. Although being a widely-adopted approach, using cross entropy as the primary objective exploits mostly the information from the ground-truth class for maximizing data likelihood, and largely ignores information from the complement (incorrect) classes. We argue that, in addition to the primary objective, training also using a complement objective that leverages information from the complement classes can be effective in improving model performance. This motivates us to study a new training paradigm that maximizes the likelihood of the ground-truth class while neutralizing the probabilities of the complement classes. We conduct extensive experiments on multiple tasks ranging from computer vision to natural language understanding. The experimental results confirm that, compared to the conventional training with just one primary objective, training also with the complement objective further improves the performance of the state-of-the-art models across all tasks. In addition to the accuracy improvement, we also show that models trained with both primary and complement objectives are more robust to single-step adversarial attacks",
    "checked": null,
    "id": "13d96d1f052254e3fe48077f12afd63b6a0c844a",
    "semantic_title": "complement objective training",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=rJeXCo0cYX": {
    "title": "BabyAI: A Platform to Study the Sample Efficiency of Grounded Language Learning",
    "volume": "poster",
    "abstract": "Allowing humans to interactively train artificial agents to understand language instructions is desirable for both practical and scientific reasons. Though, given the lack of sample efficiency in current learning methods, reaching this goal may require substantial research efforts. We introduce the BabyAI research platform, with the goal of supporting investigations towards including humans in the loop for grounded language learning. The BabyAI platform comprises an extensible suite of 19 levels of increasing difficulty. Each level gradually leads the agent towards acquiring a combinatorially rich synthetic language, which is a proper subset of English. The platform also provides a hand-crafted bot agent, which simulates a human teacher. We report estimated amount of supervision required for training neural reinforcement and behavioral-cloning agents on some BabyAI levels. We put forward strong evidence that current deep learning methods are not yet sufficiently sample-efficient in the context of learning a language with compositional properties",
    "checked": null,
    "id": "1b19f433a3e8497e9d9bd67efb108521d16b5b85",
    "semantic_title": "babyai: a platform to study the sample efficiency of grounded language learning",
    "citation_count": 239,
    "authors": []
  },
  "https://openreview.net/forum?id=H1gMCsAqY7": {
    "title": "Slimmable Neural Networks",
    "volume": "poster",
    "abstract": "We present a simple and general method to train a single neural network executable at different widths (number of channels in a layer), permitting instant and adaptive accuracy-efficiency trade-offs at runtime. Instead of training individual networks with different width configurations, we train a shared network with switchable batch normalization. At runtime, the network can adjust its width on the fly according to on-device benchmarks and resource constraints, rather than downloading and offloading different models. Our trained networks, named slimmable neural networks, achieve similar (and in many cases better) ImageNet classification accuracy than individually trained models of MobileNet v1, MobileNet v2, ShuffleNet and ResNet-50 at different widths respectively. We also demonstrate better performance of slimmable models compared with individual ones across a wide range of applications including COCO bounding-box object detection, instance segmentation and person keypoint detection without tuning hyper-parameters. Lastly we visualize and discuss the learned features of slimmable networks. Code and models are available at: https://github.com/JiahuiYu/slimmable_networks",
    "checked": null,
    "id": "120ffccea4787b88f78b55b9302891ff96cb4228",
    "semantic_title": "slimmable neural networks",
    "citation_count": 552,
    "authors": []
  },
  "https://openreview.net/forum?id=HyxzRsR9Y7": {
    "title": "Learning Self-Imitating Diverse Policies",
    "volume": "poster",
    "abstract": "The success of popular algorithms for deep reinforcement learning, such as policy-gradients and Q-learning, relies heavily on the availability of an informative reward signal at each timestep of the sequential decision-making process. When rewards are only sparsely available during an episode, or a rewarding feedback is provided only after episode termination, these algorithms perform sub-optimally due to the difficultly in credit assignment. Alternatively, trajectory-based policy optimization methods, such as cross-entropy method and evolution strategies, do not require per-timestep rewards, but have been found to suffer from high sample complexity by completing forgoing the temporal nature of the problem. Improving the efficiency of RL algorithms in real-world problems with sparse or episodic rewards is therefore a pressing need. In this work, we introduce a self-imitation learning algorithm that exploits and explores well in the sparse and episodic reward settings. We view each policy as a state-action visitation distribution and formulate policy optimization as a divergence minimization problem. We show that with Jensen-Shannon divergence, this divergence minimization problem can be reduced into a policy-gradient algorithm with shaped rewards learned from experience replays. Experimental results indicate that our algorithm works comparable to existing algorithms in environments with dense rewards, and significantly better in environments with sparse and episodic rewards. We then discuss limitations of self-imitation learning, and propose to solve them by using Stein variational policy gradient descent with the Jensen-Shannon kernel to learn multiple diverse policies. We demonstrate its effectiveness on a challenging variant of continuous-control MuJoCo locomotion tasks",
    "checked": null,
    "id": "653bdfb3c35621ee04ee5d5253dc7e3a422d69e1",
    "semantic_title": "learning self-imitating diverse policies",
    "citation_count": 67,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgW0oA9FX": {
    "title": "Graph HyperNetworks for Neural Architecture Search",
    "volume": "poster",
    "abstract": "Neural architecture search (NAS) automatically finds the best task-specific neural network topology, outperforming many manual architecture designs. However, it can be prohibitively expensive as the search requires training thousands of different networks, while each training run can last for hours. In this work, we propose the Graph HyperNetwork (GHN) to amortize the search cost: given an architecture, it directly generates the weights by running inference on a graph neural network. GHNs model the topology of an architecture and therefore can predict network performance more accurately than regular hypernetworks and premature early stopping. To perform NAS, we randomly sample architectures and use the validation accuracy of networks with GHN generated weights as the surrogate search signal. GHNs are fast - they can search nearly 10√ó faster than other random search methods on CIFAR-10 and ImageNet. GHNs can be further extended to the anytime prediction setting, where they have found networks with better speed-accuracy tradeoff than the state-of-the-art manual designs",
    "checked": null,
    "id": "89c10e08902cb90abbe1276a3042b93c2f9c78b4",
    "semantic_title": "graph hypernetworks for neural architecture search",
    "citation_count": 278,
    "authors": []
  },
  "https://openreview.net/forum?id=ryxxCiRqYX": {
    "title": "Deep Layers as Stochastic Solvers",
    "volume": "poster",
    "abstract": "We provide a novel perspective on the forward pass through a block of layers in a deep network. In particular, we show that a forward pass through a standard dropout layer followed by a linear layer and a non-linear activation is equivalent to optimizing a convex objective with a single iteration of a $\\tau$-nice Proximal Stochastic Gradient method. We further show that replacing standard Bernoulli dropout with additive dropout is equivalent to optimizing the same convex objective with a variance-reduced proximal method. By expressing both fully-connected and convolutional layers as special cases of a high-order tensor product, we unify the underlying convex optimization problem in the tensor setting and derive a formula for the Lipschitz constant $L$ used to determine the optimal step size of the above proximal methods. We conduct experiments with standard convolutional networks applied to the CIFAR-10 and CIFAR-100 datasets and show that replacing a block of layers with multiple iterations of the corresponding solver, with step size set via $L$, consistently improves classification accuracy",
    "checked": null,
    "id": "b9df003fc73d4b6d3171c74cf1cd8ba35d92dda0",
    "semantic_title": "deep layers as stochastic solvers",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=S1lg0jAcYm": {
    "title": "ARM: Augment-REINFORCE-Merge Gradient for Stochastic Binary Networks",
    "volume": "poster",
    "abstract": "To backpropagate the gradients through stochastic binary layers, we propose the augment-REINFORCE-merge (ARM) estimator that is unbiased, exhibits low variance, and has low computational complexity. Exploiting variable augmentation, REINFORCE, and reparameterization, the ARM estimator achieves adaptive variance reduction for Monte Carlo integration by merging two expectations via common random numbers. The variance-reduction mechanism of the ARM estimator can also be attributed to either antithetic sampling in an augmented space, or the use of an optimal anti-symmetric \"self-control\" baseline function together with the REINFORCE estimator in that augmented space. Experimental results show the ARM estimator provides state-of-the-art performance in auto-encoding variational inference and maximum likelihood estimation, for discrete latent variable models with one or multiple stochastic binary layers. Python code for reproducible research is publicly available",
    "checked": null,
    "id": "97f0cff278f9131c22a2e26748a46511b761014b",
    "semantic_title": "arm: augment-reinforce-merge gradient for stochastic binary networks",
    "citation_count": 63,
    "authors": []
  },
  "https://openreview.net/forum?id=HyexAiA5Fm": {
    "title": "Scalable Unbalanced Optimal Transport using Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "Generative adversarial networks (GANs) are an expressive class of neural generative models with tremendous success in modeling high-dimensional continuous measures. In this paper, we present a scalable method for unbalanced optimal transport (OT) based on the generative-adversarial framework. We formulate unbalanced OT as a problem of simultaneously learning a transport map and a scaling factor that push a source measure to a target measure in a cost-optimal manner. We provide theoretical justification for this formulation, showing that it is closely related to an existing static formulation by Liero et al. (2018). We then propose an algorithm for solving this problem based on stochastic alternating gradient updates, similar in practice to GANs, and perform numerical experiments demonstrating how this methodology can be applied to population modeling",
    "checked": null,
    "id": "bdd151d4c522b924b2971b1139cd05c5b76c1513",
    "semantic_title": "scalable unbalanced optimal transport using generative adversarial networks",
    "citation_count": 76,
    "authors": []
  },
  "https://openreview.net/forum?id=rJe10iC5K7": {
    "title": "Unsupervised Discovery of Parts, Structure, and Dynamics",
    "volume": "poster",
    "abstract": "Humans easily recognize object parts and their hierarchical structure by watching how they move; they can then predict how each part moves in the future. In this paper, we propose a novel formulation that simultaneously learns a hierarchical, disentangled object representation and a dynamics model for object parts from unlabeled videos. Our Parts, Structure, and Dynamics (PSD) model learns to, first, recognize the object parts via a layered image representation; second, predict hierarchy via a structural descriptor that composes low-level concepts into a hierarchical structure; and third, model the system dynamics by predicting the future. Experiments on multiple real and synthetic datasets demonstrate that our PSD model works well on all three tasks: segmenting object parts, building their hierarchical structure, and capturing their motion distributions",
    "checked": null,
    "id": "4b64cd1fb5ee85e874a792c49271e4bf5314d6d7",
    "semantic_title": "unsupervised discovery of parts, structure, and dynamics",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=B1xJAsA5F7": {
    "title": "Learning Multimodal Graph-to-Graph Translation for Molecule Optimization",
    "volume": "poster",
    "abstract": "We view molecule optimization as a graph-to-graph translation problem. The goal is to learn to map from one molecular graph to another with better properties based on an available corpus of paired molecules. Since molecules can be optimized in different ways, there are multiple viable translations for each input graph. A key challenge is therefore to model diverse translation outputs. Our primary contributions include a junction tree encoder-decoder for learning diverse graph translations along with a novel adversarial training method for aligning distributions of molecules. Diverse output distributions in our model are explicitly realized by low-dimensional latent vectors that modulate the translation process. We evaluate our model on multiple molecule optimization tasks and show that our model outperforms previous state-of-the-art baselines by a significant margin",
    "checked": null,
    "id": "7975918889eb92138f0b06e86279b4b0ca3db303",
    "semantic_title": "learning multimodal graph-to-graph translation for molecular optimization",
    "citation_count": 228,
    "authors": []
  },
  "https://openreview.net/forum?id=HJxyAjRcFX": {
    "title": "Harmonizing Maximum Likelihood with GANs for Multimodal Conditional Generation",
    "volume": "poster",
    "abstract": "Recent advances in conditional image generation tasks, such as image-to-image translation and image inpainting, are largely accounted to the success of conditional GAN models, which are often optimized by the joint use of the GAN loss with the reconstruction loss. However, we reveal that this training recipe shared by almost all existing methods causes one critical side effect: lack of diversity in output samples. In order to accomplish both training stability and multimodal output generation, we propose novel training schemes with a new set of losses named moment reconstruction losses that simply replace the reconstruction loss. We show that our approach is applicable to any conditional generation tasks by performing thorough experiments on image-to-image translation, super-resolution and image inpainting using Cityscapes and CelebA dataset. Quantitative evaluations also confirm that our methods achieve a great diversity in outputs while retaining or even improving the visual fidelity of generated samples",
    "checked": null,
    "id": "0dc02a63400c4f1899c952afdc620479238abedf",
    "semantic_title": "harmonizing maximum likelihood with gans for multimodal conditional generation",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=SkeRTsAcYm": {
    "title": "Phase-Aware Speech Enhancement with Deep Complex U-Net",
    "volume": "poster",
    "abstract": "Most deep learning-based models for speech enhancement have mainly focused on estimating the magnitude of spectrogram while reusing the phase from noisy speech for reconstruction. This is due to the difficulty of estimating the phase of clean speech. To improve speech enhancement performance, we tackle the phase estimation problem in three ways. First, we propose Deep Complex U-Net, an advanced U-Net structured model incorporating well-defined complex-valued building blocks to deal with complex-valued spectrograms. Second, we propose a polar coordinate-wise complex-valued masking method to reflect the distribution of complex ideal ratio masks. Third, we define a novel loss function, weighted source-to-distortion ratio (wSDR) loss, which is designed to directly correlate with a quantitative evaluation measure. Our model was evaluated on a mixture of the Voice Bank corpus and DEMAND database, which has been widely used by many deep learning models for speech enhancement. Ablation experiments were conducted on the mixed dataset showing that all three proposed approaches are empirically valid. Experimental results show that the proposed method achieves state-of-the-art performance in all metrics, outperforming previous approaches by a large margin",
    "checked": null,
    "id": "101757dd6c7cb3704a54b25fd8ae0e8c0daee248",
    "semantic_title": "phase-aware speech enhancement with deep complex u-net",
    "citation_count": 331,
    "authors": []
  },
  "https://openreview.net/forum?id=rJgTTjA9tX": {
    "title": "The Comparative Power of ReLU Networks and Polynomial Kernels in the Presence of Sparse Latent Structure",
    "volume": "poster",
    "abstract": "There has been a large amount of interest, both in the past and particularly recently, into the relative advantage of different families of universal function approximators, for instance neural networks, polynomials, rational functions, etc. However, current research has focused almost exclusively on understanding this problem in a worst case setting: e.g. characterizing the best L1 or L_{infty} approximation in a box (or sometimes, even under an adversarially constructed data distribution.) In this setting many classical tools from approximation theory can be effectively used. However, in typical applications we expect data to be high dimensional, but structured -- so, it would only be important to approximate the desired function well on the relevant part of its domain, e.g. a small manifold on which real input data actually lies. Moreover, even within this domain the desired quality of approximation may not be uniform; for instance in classification problems, the approximation needs to be more accurate near the decision boundary. These issues, to the best of our knowledge, have remain unexplored until now. With this in mind, we analyze the performance of neural networks and polynomial kernels in a natural regression setting where the data enjoys sparse latent structure, and the labels depend in a simple way on the latent variables. We give an almost-tight theoretical analysis of the performance of both neural networks and polynomials for this problem, as well as verify our theory with simulations. Our results both involve new (complex-analytic) techniques, which may be of independent interest, and show substantial qualitative differences with what is known in the worst-case setting",
    "checked": null,
    "id": "99b4267421b63f1c018581bc87b263c0e5132910",
    "semantic_title": "the comparative power of relu networks and polynomial kernels in the presence of sparse latent structure",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=BJl6TjRcY7": {
    "title": "Neural Probabilistic Motor Primitives for Humanoid Control",
    "volume": "poster",
    "abstract": "We focus on the problem of learning a single motor module that can flexibly express a range of behaviors for the control of high-dimensional physically simulated humanoids. To do this, we propose a motor architecture that has the general structure of an inverse model with a latent-variable bottleneck. We show that it is possible to train this model entirely offline to compress thousands of expert policies and learn a motor primitive embedding space. The trained neural probabilistic motor primitive system can perform one-shot imitation of whole-body humanoid behaviors, robustly mimicking unseen trajectories. Additionally, we demonstrate that it is also straightforward to train controllers to reuse the learned motor primitive space to solve tasks, and the resulting movements are relatively naturalistic. To support the training of our model, we compare two approaches for offline policy cloning, including an experience efficient method which we call linear feedback policy cloning. We encourage readers to view a supplementary video (https://youtu.be/CaDEf-QcKwA ) summarizing our results",
    "checked": null,
    "id": "96ae5d3ac1a1dcc365684bc92fcfa4d40d802bca",
    "semantic_title": "neural probabilistic motor primitives for humanoid control",
    "citation_count": 160,
    "authors": []
  },
  "https://openreview.net/forum?id=H1xipsA5K7": {
    "title": "Learning Two-layer Neural Networks with Symmetric Inputs",
    "volume": "poster",
    "abstract": "We give a new algorithm for learning a two-layer neural network under a very general class of input distributions. Assuming there is a ground-truth two-layer network y = A \\sigma(Wx) + \\xi, where A, W are weight matrices, \\xi represents noise, and the number of neurons in the hidden layer is no larger than the input or output, our algorithm is guaranteed to recover the parameters A, W of the ground-truth network. The only requirement on the input x is that it is symmetric, which still allows highly complicated and structured input. Our algorithm is based on the method-of-moments framework and extends several results in tensor decompositions. We use spectral algorithms to avoid the complicated non-convex optimization in learning neural networks. Experiments show that our algorithm can robustly learn the ground-truth neural network with a small number of samples for many symmetric input distributions",
    "checked": null,
    "id": "40c36298ff685a0e1a2c83b52590250ed44664bd",
    "semantic_title": "learning two-layer neural networks with symmetric inputs",
    "citation_count": 57,
    "authors": []
  },
  "https://openreview.net/forum?id=SJzqpj09YQ": {
    "title": "Spectral Inference Networks: Unifying Deep and Spectral Learning",
    "volume": "poster",
    "abstract": "We present Spectral Inference Networks, a framework for learning eigenfunctions of linear operators by stochastic optimization. Spectral Inference Networks generalize Slow Feature Analysis to generic symmetric operators, and are closely related to Variational Monte Carlo methods from computational physics. As such, they can be a powerful tool for unsupervised representation learning from video or graph-structured data. We cast training Spectral Inference Networks as a bilevel optimization problem, which allows for online learning of multiple eigenfunctions. We show results of training Spectral Inference Networks on problems in quantum mechanics and feature learning for videos on synthetic datasets. Our results demonstrate that Spectral Inference Networks accurately recover eigenfunctions of linear operators and can discover interpretable representations from video in a fully unsupervised manner",
    "checked": null,
    "id": "8910e3211989fdeb6123b542667ba4c717523596",
    "semantic_title": "spectral inference networks: unifying deep and spectral learning",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=Hkl5aoR5tm": {
    "title": "On Self Modulation for Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "Training Generative Adversarial Networks (GANs) is notoriously challenging. We propose and study an architectural modification, self-modulation, which improves GAN performance across different data sets, architectures, losses, regularizers, and hyperparameter settings. Intuitively, self-modulation allows the intermediate feature maps of a generator to change as a function of the input noise vector. While reminiscent of other conditioning techniques, it requires no labeled data. In a large-scale empirical study we observe a relative decrease of 5%-35% in FID. Furthermore, all else being equal, adding this modification to the generator leads to improved performance in 124/144 (86%) of the studied settings. Self-modulation is a simple architectural change that requires no additional parameter tuning, which suggests that it can be applied readily to any GAN",
    "checked": null,
    "id": "f91ff6c0aeba78f94f02b761446d5d911e6ab390",
    "semantic_title": "on self modulation for generative adversarial networks",
    "citation_count": 104,
    "authors": []
  },
  "https://openreview.net/forum?id=BJgK6iA5KX": {
    "title": "AutoLoss: Learning Discrete Schedule for Alternate Optimization",
    "volume": "poster",
    "abstract": "Many machine learning problems involve iteratively and alternately optimizing different task objectives with respect to different sets of parameters. Appropriately scheduling the optimization of a task objective or a set of parameters is usually crucial to the quality of convergence. In this paper, we present AutoLoss, a meta-learning framework that automatically learns and determines the optimization schedule. AutoLoss provides a generic way to represent and learn the discrete optimization schedule from metadata, allows for a dynamic and data-driven schedule in ML problems that involve alternating updates of different parameters or from different loss objectives. We apply AutoLoss on four ML tasks: d-ary quadratic regression, classification using a multi-layer perceptron (MLP), image generation using GANs, and multi-task neural machine translation (NMT). We show that the AutoLoss controller is able to capture the distribution of better optimization schedules that result in higher quality of convergence on all four tasks. The trained AutoLoss controller is generalizable -- it can guide and improve the learning of a new task model with different specifications, or on different datasets",
    "checked": null,
    "id": "84f6f00c6b82415b73fcce4036530bd71b6dd2b7",
    "semantic_title": "autoloss: learning discrete schedules for alternate optimization",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=rJed6j0cKX": {
    "title": "Analyzing Inverse Problems with Invertible Neural Networks",
    "volume": "poster",
    "abstract": "For many applications, in particular in natural science, the task is to determine hidden system parameters from a set of measurements. Often, the forward process from parameter- to measurement-space is well-defined, whereas the inverse problem is ambiguous: multiple parameter sets can result in the same measurement. To fully characterize this ambiguity, the full posterior parameter distribution, conditioned on an observed measurement, has to be determined. We argue that a particular class of neural networks is well suited for this task ‚Äì so-called Invertible Neural Networks (INNs). Unlike classical neural networks, which attempt to solve the ambiguous inverse problem directly, INNs focus on learning the forward process, using additional latent output variables to capture the information otherwise lost. Due to invertibility, a model of the corresponding inverse process is learned implicitly. Given a specific measurement and the distribution of the latent variables, the inverse pass of the INN provides the full posterior over parameter space. We prove theoretically and verify experimentally, on artificial data and real-world problems from medicine and astrophysics, that INNs are a powerful analysis tool to find multi-modalities in parameter space, uncover parameter correlations, and identify unrecoverable parameters",
    "checked": null,
    "id": "25e433197844c239742f67fbb4171e913e0b9fe2",
    "semantic_title": "analyzing inverse problems with invertible neural networks",
    "citation_count": 489,
    "authors": []
  },
  "https://openreview.net/forum?id=S1gOpsCctm": {
    "title": "Learning Finite State Representations of Recurrent Policy Networks",
    "volume": "poster",
    "abstract": "Recurrent neural networks (RNNs) are an effective representation of control policies for a wide range of reinforcement and imitation learning problems. RNN policies, however, are particularly difficult to explain, understand, and analyze due to their use of continuous-valued memory vectors and observation features. In this paper, we introduce a new technique, Quantized Bottleneck Insertion, to learn finite representations of these vectors and features. The result is a quantized representation of the RNN that can be analyzed to improve our understanding of memory use and general behavior. We present results of this approach on synthetic environments and six Atari games. The resulting finite representations are surprisingly small in some cases, using as few as 3 discrete memory states and 10 observations for a perfect Pong policy. We also show that these finite policy representations lead to improved interpretability",
    "checked": null,
    "id": "bd858cef3d94faf6121b7f2bd6bb6c308db4aa16",
    "semantic_title": "learning finite state representations of recurrent policy networks",
    "citation_count": 88,
    "authors": []
  },
  "https://openreview.net/forum?id=SkMwpiR9Y7": {
    "title": "Measuring and regularizing networks in function space",
    "volume": "poster",
    "abstract": "To optimize a neural network one often thinks of optimizing its parameters, but it is ultimately a matter of optimizing the function that maps inputs to outputs. Since a change in the parameters might serve as a poor proxy for the change in the function, it is of some concern that primacy is given to parameters but that the correspondence has not been tested. Here, we show that it is simple and computationally feasible to calculate distances between functions in a $L^2$ Hilbert space. We examine how typical networks behave in this space, and compare how parameter $\\ell^2$ distances compare to function $L^2$ distances between various points of an optimization trajectory. We find that the two distances are nontrivially related. In particular, the $L^2/\\ell^2$ ratio decreases throughout optimization, reaching a steady value around when test error plateaus. We then investigate how the $L^2$ distance could be applied directly to optimization. We first propose that in multitask learning, one can avoid catastrophic forgetting by directly limiting how much the input/output function changes between tasks. Secondly, we propose a new learning rule that constrains the distance a network can travel through $L^2$-space in any one update. This allows new examples to be learned in a way that minimally interferes with what has previously been learned. These applications demonstrate how one can measure and regularize function distances directly, without relying on parameters or local approximations like loss curvature",
    "checked": null,
    "id": "e3fee9244fc47aa9e80006e39352af90f64631fe",
    "semantic_title": "measuring and regularizing networks in function space",
    "citation_count": 139,
    "authors": []
  },
  "https://openreview.net/forum?id=BkgPajAcY7": {
    "title": "No Training Required: Exploring Random Encoders for Sentence Classification",
    "volume": "poster",
    "abstract": "We explore various methods for computing sentence representations from pre-trained word embeddings without any training, i.e., using nothing but random parameterizations. Our aim is to put sentence embeddings on more solid footing by 1) looking at how much modern sentence embeddings gain over random methods---as it turns out, surprisingly little; and by 2) providing the field with more appropriate baselines going forward---which are, as it turns out, quite strong. We also make important observations about proper experimental protocol for sentence classification evaluation, together with recommendations for future research",
    "checked": null,
    "id": "7a8f8109e65ed9a6048859681a825eb5655e5dd2",
    "semantic_title": "no training required: exploring random encoders for sentence classification",
    "citation_count": 99,
    "authors": []
  },
  "https://openreview.net/forum?id=SyVU6s05K7": {
    "title": "Deep Frank-Wolfe For Neural Network Optimization",
    "volume": "poster",
    "abstract": "Learning a deep neural network requires solving a challenging optimization problem: it is a high-dimensional, non-convex and non-smooth minimization problem with a large number of terms. The current practice in neural network optimization is to rely on the stochastic gradient descent (SGD) algorithm or its adaptive variants. However, SGD requires a hand-designed schedule for the learning rate. In addition, its adaptive variants tend to produce solutions that generalize less well on unseen data than SGD with a hand-designed schedule. We present an optimization method that offers empirically the best of both worlds: our algorithm yields good generalization performance while requiring only one hyper-parameter. Our approach is based on a composite proximal framework, which exploits the compositional nature of deep neural networks and can leverage powerful convex optimization algorithms by design. Specifically, we employ the Frank-Wolfe (FW) algorithm for SVM, which computes an optimal step-size in closed-form at each time-step. We further show that the descent direction is given by a simple backward pass in the network, yielding the same computational cost per iteration as SGD. We present experiments on the CIFAR and SNLI data sets, where we demonstrate the significant superiority of our method over Adam, Adagrad, as well as the recently proposed BPGrad and AMSGrad. Furthermore, we compare our algorithm to SGD with a hand-designed learning rate schedule, and show that it provides similar generalization while often converging faster. The code is publicly available at https://github.com/oval-group/dfw",
    "checked": null,
    "id": "a97d86772364b7244eca65d48c5f9225f6fdc162",
    "semantic_title": "deep frank-wolfe for neural network optimization",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=S1fUpoR5FQ": {
    "title": "Quasi-hyperbolic momentum and Adam for deep learning",
    "volume": "poster",
    "abstract": "Momentum-based acceleration of stochastic gradient descent (SGD) is widely used in deep learning. We propose the quasi-hyperbolic momentum algorithm (QHM) as an extremely simple alteration of momentum SGD, averaging a plain SGD step with a momentum step. We describe numerous connections to and identities with other algorithms, and we characterize the set of two-state optimization algorithms that QHM can recover. Finally, we propose a QH variant of Adam called QHAdam, and we empirically demonstrate that our algorithms lead to significantly improved training in a variety of settings, including a new state-of-the-art result on WMT16 EN-DE. We hope that these empirical results, combined with the conceptual and practical simplicity of QHM and QHAdam, will spur interest from both practitioners and researchers. Code is immediately available",
    "checked": null,
    "id": "8c62519ba7567b0c0016f9d49eefa2584e5df2cb",
    "semantic_title": "quasi-hyperbolic momentum and adam for deep learning",
    "citation_count": 130,
    "authors": []
  },
  "https://openreview.net/forum?id=rJNH6sAqY7": {
    "title": "On Computation and Generalization of Generative Adversarial Networks under Spectrum Control",
    "volume": "poster",
    "abstract": "Generative Adversarial Networks (GANs), though powerful, is hard to train. Several recent works (Brock et al., 2016; Miyato et al., 2018) suggest that controlling the spectra of weight matrices in the discriminator can significantly improve the training of GANs. Motivated by their discovery, we propose a new framework for training GANs, which allows more flexible spectrum control (e.g., making the weight matrices of the discriminator have slow singular value decays). Specifically, we propose a new reparameterization approach for the weight matrices of the discriminator in GANs, which allows us to directly manipulate the spectra of the weight matrices through various regularizers and constraints, without intensively computing singular value decompositions. Theoretically, we further show that the spectrum control improves the generalization ability of GANs. Our experiments on CIFAR-10, STL-10, and ImgaeNet datasets confirm that compared to other competitors, our proposed method is capable of generating images with better or equal quality by utilizing spectral normalization and encouraging the slow singular value decay",
    "checked": null,
    "id": "4292986f497324293091189f86003d47cab3f664",
    "semantic_title": "on computation and generalization of generative adversarial networks under spectrum control",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=HJMHpjC9Ym": {
    "title": "Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition",
    "volume": "poster",
    "abstract": "In this paper, we propose a novel Convolutional Neural Network (CNN) architecture for learning multi-scale feature representations with good tradeoffs between speed and accuracy. This is achieved by using a multi-branch network, which has different computational complexity at different branches with different resolutions. Through frequent merging of features from branches at distinct scales, our model obtains multi-scale features while using less computation. The proposed approach demonstrates improvement of model efficiency and performance on both object recognition and speech recognition tasks, using popular architectures including ResNet, ResNeXt and SEResNeXt. For object recognition, our approach reduces computation by 1/3 while improving accuracy significantly over 1% point than the baselines, and the computational savings can be higher up to 1/2 without compromising the accuracy. Our model also surpasses state-of-the-art CNN acceleration approaches by a large margin in terms of accuracy and FLOPs. On the task of speech recognition, our proposed multi-scale CNNs save 30% FLOPs with slightly better word error rates, showing good generalization across domains",
    "checked": null,
    "id": "425032e4fdd531cd7a5d3e7effc12d16f9c076d9",
    "semantic_title": "big-little net: an efficient multi-scale feature representation for visual and speech recognition",
    "citation_count": 96,
    "authors": []
  },
  "https://openreview.net/forum?id=BklHpjCqKm": {
    "title": "Deep Lagrangian Networks: Using Physics as Model Prior for Deep Learning",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "e129e344083b307e005c5342ba49524d9981a420",
    "semantic_title": "deep lagrangian networks: using physics as model prior for deep learning",
    "citation_count": 377,
    "authors": []
  },
  "https://openreview.net/forum?id=ByMVTsR5KQ": {
    "title": "Adversarial Audio Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "0d3bbe47fe67e5a125a2547913ac0e4a30f18c8d",
    "semantic_title": "adversarial audio synthesis",
    "citation_count": 610,
    "authors": []
  },
  "https://openreview.net/forum?id=B1xVTjCqKQ": {
    "title": "A Data-Driven and Distributed Approach to Sparse Signal Representation and Recovery",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "e34755989bc59b1092bd83393f21d42cbd68623e",
    "semantic_title": "a data-driven and distributed approach to sparse signal representation and recovery",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=HJlNpoA5YQ": {
    "title": "The Laplacian in RL: Learning Representations with Efficient Approximations",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "af03168ecb019c4ba427b8386b9c0bc3695e917e",
    "semantic_title": "the laplacian in rl: learning representations with efficient approximations",
    "citation_count": 87,
    "authors": []
  },
  "https://openreview.net/forum?id=SkgEaj05t7": {
    "title": "On the Relation Between the Sharpest Directions of DNN Loss and the SGD Step Length",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "5f9e2f6d4a844189b2e34da8fd0ba282f3f36c6f",
    "semantic_title": "on the relation between the sharpest directions of dnn loss and the sgd step length",
    "citation_count": 118,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk4fpoA5Km": {
    "title": "Discriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "876053977063ab843dd24c78425cbad1779a62ed",
    "semantic_title": "discriminator-actor-critic: addressing sample inefficiency and reward bias in adversarial imitation learning",
    "citation_count": 259,
    "authors": []
  },
  "https://openreview.net/forum?id=BkfbpsAcF7": {
    "title": "Excessive Invariance Causes Adversarial Vulnerability",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "67b72e427187b1113c787f9265926322e3d123e8",
    "semantic_title": "excessive invariance causes adversarial vulnerability",
    "citation_count": 166,
    "authors": []
  },
  "https://openreview.net/forum?id=H1ebTsActm": {
    "title": "Adaptivity of deep ReLU network for learning in Besov and mixed smooth Besov spaces: optimal rate and curse of dimensionality",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "9f56bda06700297fe8a8aab4bb429451cee9a441",
    "semantic_title": "adaptivity of deep relu network for learning in besov and mixed smooth besov spaces: optimal rate and curse of dimensionality",
    "citation_count": 243,
    "authors": []
  },
  "https://openreview.net/forum?id=ryxepo0cFX": {
    "title": "AntisymmetricRNN: A Dynamical System View on Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "e2c8a6b49cd999b16ac4dcfdc375563a6932b1c7",
    "semantic_title": "antisymmetricrnn: a dynamical system view on recurrent neural networks",
    "citation_count": 204,
    "authors": []
  },
  "https://openreview.net/forum?id=S1GkToR5tm": {
    "title": "Discriminator Rejection Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "866aa9bcb15cf4a23a0afed515fa2f6b93f91d11",
    "semantic_title": "discriminator rejection sampling",
    "citation_count": 131,
    "authors": []
  },
  "https://openreview.net/forum?id=r1My6sR9tX": {
    "title": "Unsupervised Learning via Meta-Learning",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "37b3d9ab049c5671fed29f56cacee858d98c2ea8",
    "semantic_title": "unsupervised learning via meta-learning",
    "citation_count": 230,
    "authors": []
  },
  "https://openreview.net/forum?id=r1lyTjAqYX": {
    "title": "Recurrent Experience Replay in Distributed Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "8ede7ddf99986d69562455bc8d69222fc3e27350",
    "semantic_title": "recurrent experience replay in distributed reinforcement learning",
    "citation_count": 488,
    "authors": []
  },
  "https://openreview.net/forum?id=rJlk6iRqKX": {
    "title": "Query-Efficient Hard-label Black-box Attack: An Optimization-based Approach",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "b862efa06baea0b032214675eb3c3645d5d69d46",
    "semantic_title": "query-efficient hard-label black-box attack: an optimization-based approach",
    "citation_count": 311,
    "authors": []
  },
  "https://openreview.net/forum?id=SJzR2iRcK7": {
    "title": "Multi-class classification without multi-class labels",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "5761b8de288b7622e94eb5addd6718313860f68b",
    "semantic_title": "multi-class classification without multi-class labels",
    "citation_count": 166,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgT3jRct7": {
    "title": "Large-Scale Answerer in Questioner's Mind for Visual Dialog Question Generation",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "0dde473b9280b115ae4405e5dc936a6e913aa573",
    "semantic_title": "large-scale answerer in questioner's mind for visual dialog question generation",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=HJe62s09tX": {
    "title": "Unsupervised Hyper-alignment for Multilingual Word Embeddings",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "115d6dffb5d6866c03ddec57a1779a1ba032613b",
    "semantic_title": "unsupervised hyperalignment for multilingual word embeddings",
    "citation_count": 71,
    "authors": []
  },
  "https://openreview.net/forum?id=SJx63jRqFm": {
    "title": "Diversity is All You Need: Learning Skills without a Reward Function",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "5b01eaef54a653ba03ddd5a978690380fbc19bfc",
    "semantic_title": "diversity is all you need: learning skills without a reward function",
    "citation_count": 1084,
    "authors": []
  },
  "https://openreview.net/forum?id=Hyfn2jCcKm": {
    "title": "Solving the Rubik's Cube with Approximate Policy Iteration",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "ff0dfe776b6c4b5ab2bc40ce7480736017b87127",
    "semantic_title": "solving the rubik's cube with approximate policy iteration",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=BJxh2j0qYm": {
    "title": "Dynamic Channel Pruning: Feature Boosting and Suppression",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "a055b9917759dd75811edbc8500ca247b457c5b2",
    "semantic_title": "dynamic channel pruning: feature boosting and suppression",
    "citation_count": 316,
    "authors": []
  },
  "https://openreview.net/forum?id=SJl2niR9KQ": {
    "title": "Beyond Pixel Norm-Balls: Parametric Adversaries using an Analytically Differentiable Renderer",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "742af5aa06ed8de12f8f77e248ff3fe5700061ff",
    "semantic_title": "beyond pixel norm-balls: parametric adversaries using an analytically differentiable renderer",
    "citation_count": 114,
    "authors": []
  },
  "https://openreview.net/forum?id=Hygn2o0qKX": {
    "title": "Deterministic PAC-Bayesian generalization bounds for deep networks via generalizing noise-resilience",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "0204871837acb118871e8d1bb59407da73142333",
    "semantic_title": "deterministic pac-bayesian generalization bounds for deep networks via generalizing noise-resilience",
    "citation_count": 100,
    "authors": []
  },
  "https://openreview.net/forum?id=r1lohoCqY7": {
    "title": "Learning-Based Frequency Estimation Algorithms",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "72c969a5dc5b236d511fbdaae88c443d14145ae8",
    "semantic_title": "learning-based frequency estimation algorithms",
    "citation_count": 159,
    "authors": []
  },
  "https://openreview.net/forum?id=Syxt2jC5FX": {
    "title": "From Hard to Soft: Understanding Deep Network Nonlinearities via Vector Quantization and Statistical Inference",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "c630f37bf7d5ab6979216ca426e4620f104fba89",
    "semantic_title": "from hard to soft: understanding deep network nonlinearities via vector quantization and statistical inference",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=SkeK3s0qKQ": {
    "title": "Episodic Curiosity through Reachability",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "fdfeeb14bbde2ab31b18e56b92d362dcd1b14f71",
    "semantic_title": "episodic curiosity through reachability",
    "citation_count": 268,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgK3oC5Fm": {
    "title": "Bayesian Prediction of Future Street Scenes using Synthetic Likelihoods",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "1c57f1d3c1191df543f81fb56c2ec5ded1bc5488",
    "semantic_title": "bayesian prediction of future street scenes using synthetic likelihoods",
    "citation_count": 46,
    "authors": []
  },
  "https://openreview.net/forum?id=S1eK3i09YQ": {
    "title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "d6f6d1504cfedde4efb23e7ec0f42f006062c6a0",
    "semantic_title": "gradient descent provably optimizes over-parameterized neural networks",
    "citation_count": 1270,
    "authors": []
  },
  "https://openreview.net/forum?id=SJGvns0qK7": {
    "title": "Bayesian Policy Optimization for Model Uncertainty",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "4722d3e408c3687028e7f52ea6b7bf6b69981764",
    "semantic_title": "bayesian policy optimization for model uncertainty",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=rJlDnoA5Y7": {
    "title": "Von Mises-Fisher Loss for Training Sequence to Sequence Models with Continuous Outputs",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "3f67f9c8a97dc2a0b7c40d1ced73294133e67aa1",
    "semantic_title": "von mises-fisher loss for training sequence to sequence models with continuous outputs",
    "citation_count": 70,
    "authors": []
  },
  "https://openreview.net/forum?id=Byx83s09Km": {
    "title": "Information-Directed Exploration for Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "a25b645c3d24f91164230a0ac5bb2d4ec88c1538",
    "semantic_title": "information-directed exploration for deep reinforcement learning",
    "citation_count": 70,
    "authors": []
  },
  "https://openreview.net/forum?id=r1gNni0qtm": {
    "title": "Generalized Tensor Models for Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "bfd01ae65d568801b908c6d5bb0e0704c9c5feab",
    "semantic_title": "generalized tensor models for recurrent neural networks",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=Syx72jC9tm": {
    "title": "Invariant and Equivariant Graph Networks",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "6541afa4b4061a7d5c8387514bedea9dc249fd80",
    "semantic_title": "invariant and equivariant graph networks",
    "citation_count": 502,
    "authors": []
  },
  "https://openreview.net/forum?id=r1l73iRqKm": {
    "title": "Wizard of Wikipedia: Knowledge-Powered Conversational Agents",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "227458886343b86bd15adf58c769be326b4b058a",
    "semantic_title": "wizard of wikipedia: knowledge-powered conversational agents",
    "citation_count": 945,
    "authors": []
  },
  "https://openreview.net/forum?id=HkeGhoA5FX": {
    "title": "Residual Non-local Attention Networks for Image Restoration",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "34439db81b482cd562e1cdba974c70a2b89cd6d4",
    "semantic_title": "residual non-local attention networks for image restoration",
    "citation_count": 673,
    "authors": []
  },
  "https://openreview.net/forum?id=ryGfnoC5KQ": {
    "title": "Kernel RNN Learning (KeRNL)",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "c3e47eaff722e3950f5e2f9d8eb00fabf4d4d636",
    "semantic_title": "kernel rnn learning (kernl)",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=S1zz2i0cY7": {
    "title": "Integer Networks for Data Compression with Latent-Variable Models",
    "volume": "poster",
    "abstract": "We consider the problem of using variational latent-variable models for data compression. For such models to produce a compressed binary sequence, which is the universal data representation in a digital world, the latent representation needs to be subjected to entropy coding. Range coding as an entropy coding technique is optimal, but it can fail catastrophically if the computation of the prior differs even slightly between the sending and the receiving side. Unfortunately, this is a common scenario when floating point math is used and the sender and receiver operate on different hardware or software platforms, as numerical round-off is often platform dependent. We propose using integer networks as a universal solution to this problem, and demonstrate that they enable reliable cross-platform encoding and decoding of images using variational models",
    "checked": null,
    "id": "2405cb80949231747aa09af3fa8a11d5fff512d4",
    "semantic_title": "integer networks for data compression with latent-variable models",
    "citation_count": 71,
    "authors": []
  },
  "https://openreview.net/forum?id=BkgzniCqY7": {
    "title": "Structured Adversarial Attack: Towards General Implementation and Better Interpretability",
    "volume": "poster",
    "abstract": "When generating adversarial examples to attack deep neural networks (DNNs), Lp norm of the added perturbation is usually used to measure the similarity between original image and adversarial example. However, such adversarial attacks perturbing the raw input spaces may fail to capture structural information hidden in the input. This work develops a more general attack model, i.e., the structured attack (StrAttack), which explores group sparsity in adversarial perturbation by sliding a mask through images aiming for extracting key spatial structures. An ADMM (alternating direction method of multipliers)-based framework is proposed that can split the original problem into a sequence of analytically solvable subproblems and can be generalized to implement other attacking methods. Strong group sparsity is achieved in adversarial perturbations even with the same level of Lp-norm distortion (p‚àà {1,2,‚àû}) as the state-of-the-art attacks. We demonstrate the effectiveness of StrAttack by extensive experimental results on MNIST, CIFAR-10 and ImageNet. We also show that StrAttack provides better interpretability (i.e., better correspondence with discriminative image regions) through adversarial saliency map (Paper-not et al., 2016b) and class activation map (Zhou et al., 2016)",
    "checked": null,
    "id": "5bc67a8a47c796053d5ed77aaecd3cbbd4c5d4c1",
    "semantic_title": "structured adversarial attack: towards general implementation and better interpretability",
    "citation_count": 161,
    "authors": []
  },
  "https://openreview.net/forum?id=r1e13s05YX": {
    "title": "Neural network gradient-based learning of black-box function interfaces",
    "volume": "poster",
    "abstract": "Deep neural networks work well at approximating complicated functions when provided with data and trained by gradient descent methods. At the same time, there is a vast amount of existing functions that programmatically solve different tasks in a precise manner eliminating the need for training. In many cases, it is possible to decompose a task to a series of functions, of which for some we may prefer to use a neural network to learn the functionality, while for others the preferred method would be to use existing black-box functions. We propose a method for end-to-end training of a base neural network that integrates calls to existing black-box functions. We do so by approximating the black-box functionality with a differentiable neural network in a way that drives the base network to comply with the black-box function interface during the end-to-end optimization process. At inference time, we replace the differentiable estimator with its external black-box non-differentiable counterpart such that the base network output matches the input arguments of the black-box function. Using this ``Estimate and Replace'' paradigm, we train a neural network, end to end, to compute the input to black-box functionality while eliminating the need for intermediate labels. We show that by leveraging the existing precise black-box function during inference, the integrated model generalizes better than a fully differentiable model, and learns more efficiently compared to RL-based methods",
    "checked": null,
    "id": "25d5a513f7534c157ab1381d51ffe294773ffc91",
    "semantic_title": "neural network gradient-based learning of black-box function interfaces",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=BJx0sjC5FX": {
    "title": "RNNs implicitly implement tensor-product representations",
    "volume": "poster",
    "abstract": "Recurrent neural networks (RNNs) can learn continuous vector representations of symbolic structures such as sequences and sentences; these representations often exhibit linear regularities (analogies). Such regularities motivate our hypothesis that RNNs that show such regularities implicitly compile symbolic structures into tensor product representations (TPRs; Smolensky, 1990), which additively combine tensor products of vectors representing roles (e.g., sequence positions) and vectors representing fillers (e.g., particular words). To test this hypothesis, we introduce Tensor Product Decomposition Networks (TPDNs), which use TPRs to approximate existing vector representations. We demonstrate using synthetic data that TPDNs can successfully approximate linear and tree-based RNN autoencoder representations, suggesting that these representations exhibit interpretable compositional structure; we explore the settings that lead RNNs to induce such structure-sensitive representations. By contrast, further TPDN experiments show that the representations of four models trained to encode naturally-occurring sentences can be largely approximated with a bag of words, with only marginal improvements from more sophisticated structures. We conclude that TPDNs provide a powerful method for interpreting vector representations, and that standard RNNs can induce compositional sequence representations that are remarkably well approximated byTPRs; at the same time, existing training tasks for sentence representation learning may not be sufficient for inducing robust structural representations",
    "checked": null,
    "id": "d3ded34ff3378aadaa9a7c10e51cef6d04391a86",
    "semantic_title": "rnns implicitly implement tensor product representations",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=r1GAsjC5Fm": {
    "title": "Self-Monitoring Navigation Agent via Auxiliary Progress Estimation",
    "volume": "poster",
    "abstract": "The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at https://github.com/chihyaoma/selfmonitoring-agent",
    "checked": null,
    "id": "29e13746fa5aed13e51558a521a39aaeaa99c1b1",
    "semantic_title": "self-monitoring navigation agent via auxiliary progress estimation",
    "citation_count": 276,
    "authors": []
  },
  "https://openreview.net/forum?id=H1g6osRcFQ": {
    "title": "Policy Transfer with Strategy Optimization",
    "volume": "poster",
    "abstract": "Computer simulation provides an automatic and safe way for training robotic control policies to achieve complex tasks such as locomotion. However, a policy trained in simulation usually does not transfer directly to the real hardware due to the differences between the two environments. Transfer learning using domain randomization is a promising approach, but it usually assumes that the target environment is close to the distribution of the training environments, thus relying heavily on accurate system identification. In this paper, we present a different approach that leverages domain randomization for transferring control policies to unknown environments. The key idea that, instead of learning a single policy in the simulation, we simultaneously learn a family of policies that exhibit different behaviors. When tested in the target environment, we directly search for the best policy in the family based on the task performance, without the need to identify the dynamic parameters. We evaluate our method on five simulated robotic control problems with different discrepancies in the training and testing environment and demonstrate that our method can overcome larger modeling errors compared to training a robust policy or an adaptive policy",
    "checked": null,
    "id": "0e31f993257d0b25d7c18eb75a4ba39f7563bd81",
    "semantic_title": "policy transfer with strategy optimization",
    "citation_count": 80,
    "authors": []
  },
  "https://openreview.net/forum?id=rJg6ssC5Y7": {
    "title": "DeepOBS: A Deep Learning Optimizer Benchmark Suite",
    "volume": "poster",
    "abstract": "Because the choice and tuning of the optimizer affects the speed, and ultimately the performance of deep learning, there is significant past and recent research in this area. Yet, perhaps surprisingly, there is no generally agreed-upon protocol for the quantitative and reproducible evaluation of optimization strategies for deep learning. We suggest routines and benchmarks for stochastic optimization, with special focus on the unique aspects of deep learning, such as stochasticity, tunability and generalization. As the primary contribution, we present DeepOBS, a Python package of deep learning optimization benchmarks. The package addresses key challenges in the quantitative assessment of stochastic optimizers, and automates most steps of benchmarking. The library includes a wide and extensible set of ready-to-use realistic optimization problems, such as training Residual Networks for image classification on ImageNet or character-level language prediction models, as well as popular classics like MNIST and CIFAR-10. The package also provides realistic baseline results for the most popular optimizers on these test problems, ensuring a fair comparison to the competition when benchmarking new optimizers, and without having to run costly experiments. It comes with output back-ends that directly produce LaTeX code for inclusion in academic publications. It supports TensorFlow and is available open source",
    "checked": null,
    "id": "5794c7f8d703b91804c22f1ce37cb9288eaedc60",
    "semantic_title": "deepobs: a deep learning optimizer benchmark suite",
    "citation_count": 71,
    "authors": []
  },
  "https://openreview.net/forum?id=BJxhijAcY7": {
    "title": "signSGD with Majority Vote is Communication Efficient and Fault Tolerant",
    "volume": "poster",
    "abstract": "Training neural networks on large datasets can be accelerated by distributing the workload over a network of machines. As datasets grow ever larger, networks of hundreds or thousands of machines become economically viable. The time cost of communicating gradients limits the effectiveness of using such large machine counts, as may the increased chance of network faults. We explore a particularly simple algorithm for robust, communication-efficient learning---signSGD. Workers transmit only the sign of their gradient vector to a server, and the overall update is decided by a majority vote. This algorithm uses 32x less communication per iteration than full-precision, distributed SGD. Under natural conditions verified by experiment, we prove that signSGD converges in the large and mini-batch settings, establishing convergence for a parameter regime of Adam as a byproduct. Aggregating sign gradients by majority vote means that no individual worker has too much power. We prove that unlike SGD, majority vote is robust when up to 50% of workers behave adversarially. The class of adversaries we consider includes as special cases those that invert or randomise their gradient estimate. On the practical side, we built our distributed training system in Pytorch. Benchmarking against the state of the art collective communications library (NCCL), our framework---with the parameter server housed entirely on one machine---led to a 25% reduction in time for training resnet50 on Imagenet when using 15 AWS p3.2xlarge machines",
    "checked": null,
    "id": "dbca9dbe14e9933515d2005dc1163ae2c24d9afd",
    "semantic_title": "signsgd with majority vote is communication efficient and fault tolerant",
    "citation_count": 243,
    "authors": []
  },
  "https://openreview.net/forum?id=HkljioCcFQ": {
    "title": "MARGINALIZED AVERAGE ATTENTIONAL NETWORK FOR WEAKLY-SUPERVISED LEARNING",
    "volume": "poster",
    "abstract": "In weakly-supervised temporal action localization, previous works have failed to locate dense and integral regions for each entire action due to the overestimation of the most salient regions. To alleviate this issue, we propose a marginalized average attentional network (MAAN) to suppress the dominant response of the most salient regions in a principled manner. The MAAN employs a novel marginalized average aggregation (MAA) module and learns a set of latent discriminative probabilities in an end-to-end fashion. MAA samples multiple subsets from the video snippet features according to a set of latent discriminative probabilities and takes the expectation over all the averaged subset features. Theoretically, we prove that the MAA module with learned latent discriminative probabilities successfully reduces the difference in responses between the most salient regions and the others. Therefore, MAAN is able to generate better class activation sequences and identify dense and integral action regions in the videos. Moreover, we propose a fast algorithm to reduce the complexity of constructing MAA from $O(2^T)$ to $O(T^2)$. Extensive experiments on two large-scale video datasets show that our MAAN achieves a superior performance on weakly-supervised temporal action localization",
    "checked": null,
    "id": "6c97556edbc192896cc55395f8f21fe0ff148580",
    "semantic_title": "marginalized average attentional network for weakly-supervised learning",
    "citation_count": 79,
    "authors": []
  },
  "https://openreview.net/forum?id=SyGjjsC5tQ": {
    "title": "Stable Opponent Shaping in Differentiable Games",
    "volume": "poster",
    "abstract": "A growing number of learning methods are actually differentiable games whose players optimise multiple, interdependent objectives in parallel ‚Äì from GANs and intrinsic curiosity to multi-agent RL. Opponent shaping is a powerful approach to improve learning dynamics in these games, accounting for player influence on others' updates. Learning with Opponent-Learning Awareness (LOLA) is a recent algorithm that exploits this response and leads to cooperation in settings like the Iterated Prisoner's Dilemma. Although experimentally successful, we show that LOLA agents can exhibit ‚Äòarrogant' behaviour directly at odds with convergence. In fact, remarkably few algorithms have theoretical guarantees applying across all (n-player, non-convex) games. In this paper we present Stable Opponent Shaping (SOS), a new method that interpolates between LOLA and a stable variant named LookAhead. We prove that LookAhead converges locally to equilibria and avoids strict saddles in all differentiable games. SOS inherits these essential guarantees, while also shaping the learning of opponents and consistently either matching or outperforming LOLA experimentally",
    "checked": null,
    "id": "3cf906d2cc57f07244cd1f74ccb1ace8e70073cc",
    "semantic_title": "stable opponent shaping in differentiable games",
    "citation_count": 110,
    "authors": []
  },
  "https://openreview.net/forum?id=BJxssoA5KX": {
    "title": "Bounce and Learn: Modeling Scene Dynamics with Real-World Bounces",
    "volume": "poster",
    "abstract": "We introduce an approach to model surface properties governing bounces in everyday scenes. Our model learns end-to-end, starting from sensor inputs, to predict post-bounce trajectories and infer two underlying physical properties that govern bouncing - restitution and effective collision normals. Our model, Bounce and Learn, comprises two modules -- a Physics Inference Module (PIM) and a Visual Inference Module (VIM). VIM learns to infer physical parameters for locations in a scene given a single still image, while PIM learns to model physical interactions for the prediction task given physical parameters and observed pre-collision 3D trajectories. To achieve our results, we introduce the Bounce Dataset comprising 5K RGB-D videos of bouncing trajectories of a foam ball to probe surfaces of varying shapes and materials in everyday scenes including homes and offices. Our proposed model learns from our collected dataset of real-world bounces and is bootstrapped with additional information from simple physics simulations. We show on our newly collected dataset that our model out-performs baselines, including trajectory fitting with Newtonian physics, in predicting post-bounce trajectories and inferring physical properties of a scene",
    "checked": null,
    "id": "b39399b1b7c8d2950109b645552439a712913bf1",
    "semantic_title": "bounce and learn: modeling scene dynamics with real-world bounces",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=H1eqjiCctX": {
    "title": "Understanding Composition of Word Embeddings via Tensor Decomposition",
    "volume": "poster",
    "abstract": "Word embedding is a powerful tool in natural language processing. In this paper we consider the problem of word embedding composition \\--- given vector representations of two words, compute a vector for the entire phrase. We give a generative model that can capture specific syntactic relations between words. Under our model, we prove that the correlations between three words (measured by their PMI) form a tensor that has an approximate low rank Tucker decomposition. The result of the Tucker decomposition gives the word embeddings as well as a core tensor, which can be used to produce better compositions of the word embeddings. We also complement our theoretical results with experiments that verify our assumptions, and demonstrate the effectiveness of the new composition method",
    "checked": null,
    "id": "1ee096afc761526e4be9e3af11d6287d4a0a5393",
    "semantic_title": "understanding composition of word embeddings via tensor decomposition",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=rylqooRqK7": {
    "title": "SNAS: stochastic neural architecture search",
    "volume": "poster",
    "abstract": "We propose Stochastic Neural Architecture Search (SNAS), an economical end-to-end solution to Neural Architecture Search (NAS) that trains neural operation parameters and architecture distribution parameters in same round of back-propagation, while maintaining the completeness and differentiability of the NAS pipeline. In this work, NAS is reformulated as an optimization problem on parameters of a joint distribution for the search space in a cell. To leverage the gradient information in generic differentiable loss for architecture search, a novel search gradient is proposed. We prove that this search gradient optimizes the same objective as reinforcement-learning-based NAS, but assigns credits to structural decisions more efficiently. This credit assignment is further augmented with locally decomposable reward to enforce a resource-efficient constraint. In experiments on CIFAR-10, SNAS takes less epochs to find a cell architecture with state-of-the-art accuracy than non-differentiable evolution-based and reinforcement-learning-based NAS, which is also transferable to ImageNet. It is also shown that child networks of SNAS can maintain the validation accuracy in searching, with which attention-based NAS requires parameter retraining to compete, exhibiting potentials to stride towards efficient NAS on big datasets",
    "checked": null,
    "id": "3f0a2de309f21a957b4741dd68007eb08d9b12e3",
    "semantic_title": "snas: stochastic neural architecture search",
    "citation_count": 935,
    "authors": []
  },
  "https://openreview.net/forum?id=HJGciiR5Y7": {
    "title": "Latent Convolutional Models",
    "volume": "poster",
    "abstract": "We present a new latent model of natural images that can be learned on large-scale datasets. The learning process provides a latent embedding for every image in the training dataset, as well as a deep convolutional network that maps the latent space to the image space. After training, the new model provides a strong and universal image prior for a variety of image restoration tasks such as large-hole inpainting, superresolution, and colorization. To model high-resolution natural images, our approach uses latent spaces of very high dimensionality (one to two orders of magnitude higher than previous latent image models). To tackle this high dimensionality, we use latent spaces with a special manifold structure (convolutional manifolds) parameterized by a ConvNet of a certain architecture. In the experiments, we compare the learned latent models with latent models learned by autoencoders, advanced variants of generative adversarial networks, and a strong baseline system using simpler parameterization of the latent space. Our model outperforms the competing approaches over a range of restoration tasks",
    "checked": null,
    "id": "dcf4a22445f4486932fffd7014c320fa9bd91011",
    "semantic_title": "latent convolutional models",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=rkxciiC9tm": {
    "title": "NADPEx: An on-policy temporally consistent exploration method for deep reinforcement learning",
    "volume": "poster",
    "abstract": "Reinforcement learning agents need exploratory behaviors to escape from local optima. These behaviors may include both immediate dithering perturbation and temporally consistent exploration. To achieve these, a stochastic policy model that is inherently consistent through a period of time is in desire, especially for tasks with either sparse rewards or long term information. In this work, we introduce a novel on-policy temporally consistent exploration strategy - Neural Adaptive Dropout Policy Exploration (NADPEx) - for deep reinforcement learning agents. Modeled as a global random variable for conditional distribution, dropout is incorporated to reinforcement learning policies, equipping them with inherent temporal consistency, even when the reward signals are sparse. Two factors, gradients' alignment with the objective and KL constraint in policy space, are discussed to guarantee NADPEx policy's stable improvement. Our experiments demonstrate that NADPEx solves tasks with sparse reward while naive exploration and parameter noise fail. It yields as well or even faster convergence in the standard mujoco benchmark for continuous control",
    "checked": null,
    "id": "a88d54c168a84ed8a04d2a32be0b5939586b5792",
    "semantic_title": "nadpex: an on-policy temporally consistent exploration method for deep reinforcement learning",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=SkEYojRqtm": {
    "title": "Representation Degeneration Problem in Training Natural Language Generation Models",
    "volume": "poster",
    "abstract": "We study an interesting problem in training neural network-based models for natural language generation tasks, which we call the \\emph{representation degeneration problem}. We observe that when training a model for natural language generation tasks through likelihood maximization with the weight tying trick, especially with big training datasets, most of the learnt word embeddings tend to degenerate and be distributed into a narrow cone, which largely limits the representation power of word embeddings. We analyze the conditions and causes of this problem and propose a novel regularization method to address it. Experiments on language modeling and machine translation show that our method can largely mitigate the representation degeneration problem and achieve better performance than baseline algorithms",
    "checked": null,
    "id": "55e960535f643637161b2e99a8c21a92c0d13757",
    "semantic_title": "representation degeneration problem in training natural language generation models",
    "citation_count": 266,
    "authors": []
  },
  "https://openreview.net/forum?id=HyEtjoCqFX": {
    "title": "Soft Q-Learning with Mutual-Information Regularization",
    "volume": "poster",
    "abstract": "We propose a reinforcement learning (RL) algorithm that uses mutual-information regularization to optimize a prior action distribution for better performance and exploration. Entropy-based regularization has previously been shown to improve both exploration and robustness in challenging sequential decision-making tasks. It does so by encouraging policies to put probability mass on all actions. However, entropy regularization might be undesirable when actions have significantly different importance. In this paper, we propose a theoretically motivated framework that dynamically weights the importance of actions by using the mutual-information. In particular, we express the RL problem as an inference problem where the prior probability distribution over actions is subject to optimization. We show that the prior optimization introduces a mutual-information regularizer in the RL objective. This regularizer encourages the policy to be close to a non-uniform distribution that assigns higher probability mass to more important actions. We empirically demonstrate that our method significantly improves over entropy regularization methods and unregularized methods",
    "checked": null,
    "id": "b90793b029b2fa64e94d1ee7d5d2c7e1d1fcaaa9",
    "semantic_title": "soft q-learning with mutual-information regularization",
    "citation_count": 62,
    "authors": []
  },
  "https://openreview.net/forum?id=HyztsoC5Y7": {
    "title": "Learning to Adapt in Dynamic, Real-World Environments through Meta-Reinforcement Learning",
    "volume": "poster",
    "abstract": "Although reinforcement learning methods can achieve impressive results in simulation, the real world presents two major challenges: generating samples is exceedingly expensive, and unexpected perturbations or unseen situations cause proficient but specialized policies to fail at test time. Given that it is impractical to train separate policies to accommodate all situations the agent may see in the real world, this work proposes to learn how to quickly and effectively adapt online to new tasks. To enable sample-efficient learning, we consider learning online adaptation in the context of model-based reinforcement learning. Our approach uses meta-learning to train a dynamics model prior such that, when combined with recent data, this prior can be rapidly adapted to the local context. Our experiments demonstrate online adaptation for continuous control tasks on both simulated and real-world agents. We first show simulated agents adapting their behavior online to novel terrains, crippled body parts, and highly-dynamic environments. We also illustrate the importance of incorporating online adaptation into autonomous agents that operate in the real world by applying our method to a real dynamic legged millirobot: We demonstrate the agent's learned ability to quickly adapt online to a missing leg, adjust to novel terrains and slopes, account for miscalibration or errors in pose estimation, and compensate for pulling payloads",
    "checked": null,
    "id": "944bd3b472c8a30163bbfc1b5cbab8545693c3e0",
    "semantic_title": "learning to adapt in dynamic, real-world environments through meta-reinforcement learning",
    "citation_count": 548,
    "authors": []
  },
  "https://openreview.net/forum?id=SylKoo0cKm": {
    "title": "How Important is a Neuron",
    "volume": "poster",
    "abstract": "The problem of attributing a deep network's prediction to its input/base features is well-studied (cf. Simonyan et al. (2013)). We introduce the notion of conductance to extend the notion of attribution to understanding the importance of hidden units. Informally, the conductance of a hidden unit of a deep network is the flow of attribution via this hidden unit. We can use conductance to understand the importance of a hidden unit to the prediction for a specific input, or over a set of inputs. We justify conductance in multiple ways via a qualitative comparison with other methods, via some axiomatic results, and via an empirical evaluation based on a feature selection task. The empirical evaluations are done using the Inception network over ImageNet data, and a convolutinal network over text data. In both cases, we demonstrate the effectiveness of conductance in identifying interesting insights about the internal workings of these networks",
    "checked": null,
    "id": "eb322f6f798fd1b381896d0b79f5498d89585b1f",
    "semantic_title": "how important is a neuron?",
    "citation_count": 130,
    "authors": []
  },
  "https://openreview.net/forum?id=BJeOioA9Y7": {
    "title": "Knowledge Flow: Improve Upon Your Teachers",
    "volume": "poster",
    "abstract": "A zoo of deep nets is available these days for almost any given task, and it is increasingly unclear which net to start with when addressing a new task, or which net to use as an initialization for fine-tuning a new model. To address this issue, in this paper, we develop knowledge flow which moves ‚Äòknowledge' from multiple deep nets, referred to as teachers, to a new deep net model, called the student. The structure of the teachers and the student can differ arbitrarily and they can be trained on entirely different tasks with different output spaces too. Upon training with knowledge flow the student is independent of the teachers. We demonstrate our approach on a variety of supervised and reinforcement learning tasks, outperforming fine-tuning and other ‚Äòknowledge exchange' methods",
    "checked": null,
    "id": "6dd986b2621d7f420ca80be5b71e12dece41e877",
    "semantic_title": "knowledge flow: improve upon your teachers",
    "citation_count": 62,
    "authors": []
  },
  "https://openreview.net/forum?id=BkG8sjR5Km": {
    "title": "Emergent Coordination Through Competition",
    "volume": "poster",
    "abstract": "We study the emergence of cooperative behaviors in reinforcement learning agents by introducing a challenging competitive multi-agent soccer environment with continuous simulated physics. We demonstrate that decentralized, population-based training with co-play can lead to a progression in agents' behaviors: from random, to simple ball chasing, and finally showing evidence of cooperation. Our study highlights several of the challenges encountered in large scale multi-agent training in continuous control. In particular, we demonstrate that the automatic optimization of simple shaping rewards, not themselves conducive to co-operative behavior, can lead to long-horizon team behavior. We further apply an evaluation scheme, grounded by game theoretic principals, that can assess agent performance in the absence of pre-defined evaluation tasks or human baselines",
    "checked": null,
    "id": "8a5c62f9c49a943b66fb1ae379442497609c8596",
    "semantic_title": "emergent coordination through competition",
    "citation_count": 149,
    "authors": []
  },
  "https://openreview.net/forum?id=Bkg8jjC9KQ": {
    "title": "Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile",
    "volume": "poster",
    "abstract": "Owing to their connection with generative adversarial networks (GANs), saddle-point problems have recently attracted considerable interest in machine learning and beyond. By necessity, most theoretical guarantees revolve around convex-concave (or even linear) problems; however, making theoretical inroads towards efficient GAN training depends crucially on moving beyond this classic framework. To make piecemeal progress along these lines, we analyze the behavior of mirror descent (MD) in a class of non-monotone problems whose solutions coincide with those of a naturally associated variational inequality ‚Äì a property which we call coherence. We first show that ordinary, \"vanilla\" MD converges under a strict version of this condition, but not otherwise; in particular, it may fail to converge even in bilinear models with a unique solution. We then show that this deficiency is mitigated by optimism: by taking an \"extra-gradient\" step, optimistic mirror descent (OMD) converges in all coherent problems. Our analysis generalizes and extends the results of Daskalakis et al. [2018] for optimistic gradient descent (OGD) in bilinear problems, and makes concrete headway for provable convergence beyond convex-concave games. We also provide stochastic analogues of these results, and we validate our analysis by numerical experiments in a wide array of GAN models (including Gaussian mixture models, and the CelebA and CIFAR-10 datasets)",
    "checked": null,
    "id": "0ffb2d5bbbc4f2cc547569337bd20cc77c1111d7",
    "semantic_title": "optimistic mirror descent in saddle-point problems: going the extra (gradient) mile",
    "citation_count": 295,
    "authors": []
  },
  "https://openreview.net/forum?id=S1gUsoR9YX": {
    "title": "Multilingual Neural Machine Translation with Knowledge Distillation",
    "volume": "poster",
    "abstract": "Multilingual machine translation, which translates multiple languages with a single model, has attracted much attention due to its efficiency of offline training and online serving. However, traditional multilingual translation usually yields inferior accuracy compared with the counterpart using individual models for each language pair, due to language diversity and model capacity limitations. In this paper, we propose a distillation-based approach to boost the accuracy of multilingual machine translation. Specifically, individual models are first trained and regarded as teachers, and then the multilingual model is trained to fit the training data and match the outputs of individual models simultaneously through knowledge distillation. Experiments on IWSLT, WMT and Ted talk translation datasets demonstrate the effectiveness of our method. Particularly, we show that one model is enough to handle multiple languages (up to 44 languages in our experiment), with comparable or even better accuracy than individual models",
    "checked": null,
    "id": "1b24b7b4ac2427d20ab60c8451563eb8d99caf9c",
    "semantic_title": "multilingual neural machine translation with knowledge distillation",
    "citation_count": 250,
    "authors": []
  },
  "https://openreview.net/forum?id=H1ersoRqtm": {
    "title": "Structured Neural Summarization",
    "volume": "poster",
    "abstract": "Summarization of long sequences into a concise statement is a core problem in natural language processing, requiring non-trivial understanding of the input. Based on the promising results of graph neural networks on highly structured data, we develop a framework to extend existing sequence encoders with a graph component that can reason about long-distance relationships in weakly structured data such as text. In an extensive evaluation, we show that the resulting hybrid sequence-graph models outperform both pure sequence models as well as pure graph models on a range of summarization tasks",
    "checked": null,
    "id": "2225b7c480dc627e68f03e5321383f27e12cb1d7",
    "semantic_title": "structured neural summarization",
    "citation_count": 212,
    "authors": []
  },
  "https://openreview.net/forum?id=rJxHsjRqFQ": {
    "title": "Hyperbolic Attention Networks",
    "volume": "poster",
    "abstract": "Recent approaches have successfully demonstrated the benefits of learning the parameters of shallow networks in hyperbolic space. We extend this line of work by imposing hyperbolic geometry on the embeddings used to compute the ubiquitous attention mechanisms for different neural networks architectures. By only changing the geometry of embedding of object representations, we can use the embedding space more efficiently without increasing the number of parameters of the model. Mainly as the number of objects grows exponentially for any semantic distance from the query, hyperbolic geometry --as opposed to Euclidean geometry-- can encode those objects without having any interference. Our method shows improvements in generalization on neural machine translation on WMT'14 (English to German), learning on graphs (both on synthetic and real-world graph tasks) and visual question answering (CLEVR) tasks while keeping the neural representations compact",
    "checked": null,
    "id": "ebff4eb2f94dcf38171a5ca6a24ee95bc8e88c10",
    "semantic_title": "hyperbolic attention networks",
    "citation_count": 223,
    "authors": []
  },
  "https://openreview.net/forum?id=rkeSiiA5Fm": {
    "title": "Deep Learning 3D Shapes Using Alt-az Anisotropic 2-Sphere Convolution",
    "volume": "poster",
    "abstract": "The ground-breaking performance obtained by deep convolutional neural networks (CNNs) for image processing tasks is inspiring research efforts attempting to extend it for 3D geometric tasks. One of the main challenge in applying CNNs to 3D shape analysis is how to define a natural convolution operator on non-euclidean surfaces. In this paper, we present a method for applying deep learning to 3D surfaces using their spherical descriptors and alt-az anisotropic convolution on 2-sphere. A cascade set of geodesic disk filters rotate on the 2-sphere and collect spherical patterns and so to extract geometric features for various 3D shape analysis tasks. We demonstrate theoretically and experimentally that our proposed method has the possibility to bridge the gap between 2D images and 3D shapes with the desired rotation equivariance/invariance, and its effectiveness is evaluated in applications of non-rigid/ rigid shape classification and shape retrieval",
    "checked": null,
    "id": "6410edc9a0b4c1a71696b70d7bd2853566e13738",
    "semantic_title": "deep learning 3d shapes using alt-az anisotropic 2-sphere convolution",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=SkeVsiAcYm": {
    "title": "Generative predecessor models for sample-efficient imitation learning",
    "volume": "poster",
    "abstract": "We propose Generative Predecessor Models for Imitation Learning (GPRIL), a novel imitation learning algorithm that matches the state-action distribution to the distribution observed in expert demonstrations, using generative models to reason probabilistically about alternative histories of demonstrated states. We show that this approach allows an agent to learn robust policies using only a small number of expert demonstrations and self-supervised interactions with the environment. We derive this approach from first principles and compare it empirically to a state-of-the-art imitation learning method, showing that it outperforms or matches its performance on two simulated robot manipulation tasks and demonstrate significantly higher sample efficiency by applying the algorithm on a real robot",
    "checked": null,
    "id": "b6eae428eed3e5f1965a14dd2c26acf5400df473",
    "semantic_title": "generative predecessor models for sample-efficient imitation learning",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=rJlEojAqFm": {
    "title": "Relational Forward Models for Multi-Agent Learning",
    "volume": "poster",
    "abstract": "The behavioral dynamics of multi-agent systems have a rich and orderly structure, which can be leveraged to understand these systems, and to improve how artificial agents learn to operate in them. Here we introduce Relational Forward Models (RFM) for multi-agent learning, networks that can learn to make accurate predictions of agents' future behavior in multi-agent environments. Because these models operate on the discrete entities and relations present in the environment, they produce interpretable intermediate representations which offer insights into what drives agents' behavior, and what events mediate the intensity and valence of social interactions. Furthermore, we show that embedding RFM modules inside agents results in faster learning systems compared to non-augmented baselines. As more and more of the autonomous systems we develop and interact with become multi-agent in nature, developing richer analysis tools for characterizing how and why agents make decisions is increasingly necessary. Moreover, developing artificial agents that quickly and safely learn to coordinate with one another, and with humans in shared environments, is crucial",
    "checked": null,
    "id": "6ce08c7259b1dca6dd6b1e19f1037e974b12621e",
    "semantic_title": "relational forward models for multi-agent learning",
    "citation_count": 70,
    "authors": []
  },
  "https://openreview.net/forum?id=SJVmjjR9FX": {
    "title": "Variational Bayesian Phylogenetic Inference",
    "volume": "poster",
    "abstract": "Bayesian phylogenetic inference is currently done via Markov chain Monte Carlo with simple mechanisms for proposing new states, which hinders exploration efficiency and often requires long runs to deliver accurate posterior estimates. In this paper we present an alternative approach: a variational framework for Bayesian phylogenetic analysis. We approximate the true posterior using an expressive graphical model for tree distributions, called a subsplit Bayesian network, together with appropriate branch length distributions. We train the variational approximation via stochastic gradient ascent and adopt multi-sample based gradient estimators for different latent variables separately to handle the composite latent space of phylogenetic models. We show that our structured variational approximations are flexible enough to provide comparable posterior estimation to MCMC, while requiring less computation due to a more efficient tree exploration mechanism enabled by variational inference. Moreover, the variational approximations can be readily used for further statistical analysis such as marginal likelihood estimation for model comparison via importance sampling. Experiments on both synthetic data and real data Bayesian phylogenetic inference problems demonstrate the effectiveness and efficiency of our methods",
    "checked": null,
    "id": "c8d136aacf10bfade0630d796ca7f8716d867b51",
    "semantic_title": "variational bayesian phylogenetic inference",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=rk4Qso0cKm": {
    "title": "Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network",
    "volume": "poster",
    "abstract": "We present a new algorithm to train a robust neural network against adversarial attacks. Our algorithm is motivated by the following two ideas. First, although recent work has demonstrated that fusing randomness can improve the robustness of neural networks (Liu 2017), we noticed that adding noise blindly to all the layers is not the optimal way to incorporate randomness. Instead, we model randomness under the framework of Bayesian Neural Network (BNN) to formally learn the posterior distribution of models in a scalable way. Second, we formulate the mini-max problem in BNN to learn the best model distribution under adversarial attacks, leading to an adversarial-trained Bayesian neural net. Experiment results demonstrate that the proposed algorithm achieves state-of-the-art performance under strong attacks. On CIFAR-10 with VGG network, our model leads to 14% accuracy improvement compared with adversarial training (Madry 2017) and random self-ensemble (Liu, 2017) under PGD attack with 0.035 distortion, and the gap becomes even larger on a subset of ImageNet",
    "checked": null,
    "id": "c1f76891bdfa07d9a61ad11a15de13b139b20d2a",
    "semantic_title": "adv-bnn: improved adversarial defense through robust bayesian neural network",
    "citation_count": 171,
    "authors": []
  },
  "https://openreview.net/forum?id=ryf7ioRqFX": {
    "title": "h-detach: Modifying the LSTM Gradient Towards Better Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "916a119cf9365fd5d5d7d8befdc95e1e69e895de",
    "semantic_title": "h-detach: modifying the lstm gradient towards better optimization",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=HJgXsjA5tQ": {
    "title": "On the loss landscape of a class of deep neural networks with no bad local valleys",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "e3300e3800fae17aa4a4e1e684359cbd272421d0",
    "semantic_title": "on the loss landscape of a class of deep neural networks with no bad local valleys",
    "citation_count": 87,
    "authors": []
  },
  "https://openreview.net/forum?id=BklMjsRqY7": {
    "title": "Accumulation Bit-Width Scaling For Ultra-Low Precision Training Of Deep Networks",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "e931f4444f634695bfab5a6e57c817da52fc512b",
    "semantic_title": "accumulation bit-width scaling for ultra-low precision training of deep networks",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=Bklfsi0cKm": {
    "title": "Deep Convolutional Networks as shallow Gaussian Processes",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "43e33e80d74205e860dd4b8e26b7c458c60e201a",
    "semantic_title": "deep convolutional networks as shallow gaussian processes",
    "citation_count": 271,
    "authors": []
  },
  "https://openreview.net/forum?id=S1VWjiRcKX": {
    "title": "Universal Successor Features Approximators",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "894536f2ac4728850bc18705daeeda6e88f3d6f1",
    "semantic_title": "universal successor features approximators",
    "citation_count": 116,
    "authors": []
  },
  "https://openreview.net/forum?id=SkeZisA5t7": {
    "title": "Adaptive Estimators Show Information Compression in Deep Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "46f41e44bfc53b7297134b2734980494c0833874",
    "semantic_title": "adaptive estimators show information compression in deep neural networks",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=H1MgjoR9tQ": {
    "title": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "a35532918341dd582f1ebef56652ff7593f7d769",
    "semantic_title": "cbow is not all you need: combining cbow with the compositional matrix space model",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=SkNksoRctQ": {
    "title": "Fluctuation-dissipation relations for stochastic gradient descent",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "3eb0f85ff84e4869387beffea2846c9243351aa7",
    "semantic_title": "fluctuation-dissipation relations for stochastic gradient descent",
    "citation_count": 75,
    "authors": []
  },
  "https://openreview.net/forum?id=HJGkisCcKm": {
    "title": "A Universal Music Translation Network",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "45ce9fce4a4eea9f72688885182aee0c84786fab",
    "semantic_title": "a universal music translation network",
    "citation_count": 110,
    "authors": []
  },
  "https://openreview.net/forum?id=ByxkijC5FQ": {
    "title": "Neural Persistence: A Complexity Measure for Deep Neural Networks Using Algebraic Topology",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "2484ebfa2053999b4481ab2cd979eccb47cb0321",
    "semantic_title": "neural persistence: a complexity measure for deep neural networks using algebraic topology",
    "citation_count": 111,
    "authors": []
  },
  "https://openreview.net/forum?id=HyNA5iRcFQ": {
    "title": "Detecting Egregious Responses in Neural Sequence-to-sequence Models",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "2272d5b2ebffa289bea56cf1c11efd6fef0c4c6f",
    "semantic_title": "detecting egregious responses in neural sequence-to-sequence models",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=HJz05o0qK7": {
    "title": "Measuring Compositionality in Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "ba4cf6046d420af0ae86e2f4b587a8d50d219be3",
    "semantic_title": "measuring compositionality in representation learning",
    "citation_count": 149,
    "authors": []
  },
  "https://openreview.net/forum?id=HJMCcjAcYX": {
    "title": "Learning Representations of Sets through Optimized Permutations",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "8083cfb0e76358ab54f92eedbe13ed6a874a48e5",
    "semantic_title": "learning representations of sets through optimized permutations",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=H1gR5iR5FX": {
    "title": "Analysing Mathematical Reasoning Abilities of Neural Models",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "afed6dc6900d3b37e528b9086661bba583d60bf6",
    "semantic_title": "analysing mathematical reasoning abilities of neural models",
    "citation_count": 428,
    "authors": []
  },
  "https://openreview.net/forum?id=rkxacs0qY7": {
    "title": "FUNCTIONAL VARIATIONAL BAYESIAN NEURAL NETWORKS",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "69555845bf26bf930ecbfc223fa0ee454b2d58df",
    "semantic_title": "functional variational bayesian neural networks",
    "citation_count": 235,
    "authors": []
  },
  "https://openreview.net/forum?id=HygjqjR9Km": {
    "title": "Improving MMD-GAN Training with Repulsive Loss Function",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "15cc54ed7b1582b2efd71bedf28b23634d82991b",
    "semantic_title": "improving mmd-gan training with repulsive loss function",
    "citation_count": 80,
    "authors": []
  },
  "https://openreview.net/forum?id=rygjcsR9Y7": {
    "title": "SOM-VAE: Interpretable Discrete Representation Learning on Time Series",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "e8c5f091420cb75013121dd02ee417ae974ecee1",
    "semantic_title": "som-vae: interpretable discrete representation learning on time series",
    "citation_count": 139,
    "authors": []
  },
  "https://openreview.net/forum?id=r1eiqi09K7": {
    "title": "Riemannian Adaptive Optimization Methods",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "1f992463673ebd65840fe8d763da98dc94e04c17",
    "semantic_title": "riemannian adaptive optimization methods",
    "citation_count": 256,
    "authors": []
  },
  "https://openreview.net/forum?id=BJgqqsAct7": {
    "title": "Non-vacuous Generalization Bounds at the ImageNet Scale: a PAC-Bayesian Compression Approach",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "74fe26e60f04055af52a88a513a1d6229ade5781",
    "semantic_title": "non-vacuous generalization bounds at the imagenet scale: a pac-bayesian compression approach",
    "citation_count": 213,
    "authors": []
  },
  "https://openreview.net/forum?id=rygqqsA9KX": {
    "title": "Learning Factorized Multimodal Representations",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "034f1c5589644a6b42f50bf61b1628a1c5607fd9",
    "semantic_title": "learning factorized multimodal representations",
    "citation_count": 313,
    "authors": []
  },
  "https://openreview.net/forum?id=Syxt5oC5YQ": {
    "title": "Aggregated Momentum: Stability Through Passive Damping",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "1de35bfced22b716db622d8946a66ac9c5d39690",
    "semantic_title": "aggregated momentum: stability through passive damping",
    "citation_count": 68,
    "authors": []
  },
  "https://openreview.net/forum?id=SJxu5iR9KQ": {
    "title": "Learning to Schedule Communication in Multi-agent Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "0732925ffaf7ee525a57786f7c9791491ef084db",
    "semantic_title": "learning to schedule communication in multi-agent reinforcement learning",
    "citation_count": 208,
    "authors": []
  },
  "https://openreview.net/forum?id=H1xD9sR5Fm": {
    "title": "Minimum Divergence vs. Maximum Margin: an Empirical Comparison on Seq2Seq Models",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "ea49cef629b6e5a87a596944ad4ea9144651bfdd",
    "semantic_title": "minimum divergence vs. maximum margin: an empirical comparison on seq2seq models",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=ryGvcoA5YX": {
    "title": "Overcoming Catastrophic Forgetting for Continual Learning via Model Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "48518df15244bb691b20e827aca53e407c63adef",
    "semantic_title": "overcoming catastrophic forgetting for continual learning via model adaptation",
    "citation_count": 127,
    "authors": []
  },
  "https://openreview.net/forum?id=Sklv5iRqYX": {
    "title": "Multi-Domain Adversarial Learning",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "78e7507af6aa29d542eb4ee983a973157094b726",
    "semantic_title": "multi-domain adversarial learning",
    "citation_count": 60,
    "authors": []
  },
  "https://openreview.net/forum?id=rJzLciCqKm": {
    "title": "Learning from Positive and Unlabeled Data with a Selection Bias",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "f4b74295a2aeaff7fab34a414e1feb5e52d52cb8",
    "semantic_title": "learning from positive and unlabeled data with a selection bias",
    "citation_count": 100,
    "authors": []
  },
  "https://openreview.net/forum?id=BkeU5j0ctQ": {
    "title": "CEM-RL: Combining evolutionary and gradient-based methods for policy search",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "d9e08e872c69e5acfed2a93ec6f6d623cfd0c680",
    "semantic_title": "cem-rl: combining evolutionary and gradient-based methods for policy search",
    "citation_count": 161,
    "authors": []
  },
  "https://openreview.net/forum?id=BylIciRcYQ": {
    "title": "SGD Converges to Global Minimum in Deep Learning via Star-convex Path",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "b4a4d4c12e49110176847baaf6dd88b0dfeed285",
    "semantic_title": "sgd converges to global minimum in deep learning via star-convex path",
    "citation_count": 74,
    "authors": []
  },
  "https://openreview.net/forum?id=HJxB5sRcFQ": {
    "title": "LayoutGAN: Generating Graphic Layouts with Wireframe Discriminators",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "f3f4e8f1871622bd937a592b23fa7cc0a41bfbe0",
    "semantic_title": "layoutgan: generating graphic layouts with wireframe discriminators",
    "citation_count": 229,
    "authors": []
  },
  "https://openreview.net/forum?id=r1gEqiC9FX": {
    "title": "Equi-normalization of Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "8dd29bd8bd4197a97d28614f3c13b18fc6c9e2b5",
    "semantic_title": "equi-normalization of neural networks",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=rkemqsC9Fm": {
    "title": "Information Theoretic lower bounds on negative log likelihood",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "e0ac65da87d98a1af4a43a37a020a001ad478c3c",
    "semantic_title": "information theoretic lower bounds on negative log likelihood",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=B1xf9jAqFQ": {
    "title": "Neural Speed Reading with Structural-Jump-LSTM",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "d647a64de005113f7bb5859347f5edca81bc0eec",
    "semantic_title": "neural speed reading with structural-jump-lstm",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=rklz9iAcKQ": {
    "title": "Deep Graph Infomax",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "967a21a111757d6af7f7a25ca7ea2bdf6d505098",
    "semantic_title": "deep graph infomax",
    "citation_count": 2382,
    "authors": []
  },
  "https://openreview.net/forum?id=B1VZqjAcYX": {
    "title": "SNIP: SINGLE-SHOT NETWORK PRUNING BASED ON CONNECTION SENSITIVITY",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "cf440ccce4a7a8681e238b4f26d5b95109add55d",
    "semantic_title": "snip: single-shot network pruning based on connection sensitivity",
    "citation_count": 1196,
    "authors": []
  },
  "https://openreview.net/forum?id=SJfb5jCqKm": {
    "title": "Bias-Reduced Uncertainty Estimation for Deep Neural Classifiers",
    "volume": "poster",
    "abstract": "We consider the problem of uncertainty estimation in the context of (non-Bayesian) deep neural classification. In this context, all known methods are based on extracting uncertainty signals from a trained network optimized to solve the classification problem at hand. We demonstrate that such techniques tend to introduce biased estimates for instances whose predictions are supposed to be highly confident. We argue that this deficiency is an artifact of the dynamics of training with SGD-like optimizers, and it has some properties similar to overfitting. Based on this observation, we develop an uncertainty estimation algorithm that selectively estimates the uncertainty of highly confident points, using earlier snapshots of the trained model, before their estimates are jittered (and way before they are ready for actual classification). We present extensive experiments indicating that the proposed algorithm provides uncertainty estimates that are consistently better than all known methods",
    "checked": null,
    "id": "dfb58733b367005945a2f2c2028145e4678efd7a",
    "semantic_title": "bias-reduced uncertainty estimation for deep neural classifiers",
    "citation_count": 140,
    "authors": []
  },
  "https://openreview.net/forum?id=rJfW5oA5KQ": {
    "title": "Approximability of Discriminators Implies Diversity in GANs",
    "volume": "poster",
    "abstract": "While Generative Adversarial Networks (GANs) have empirically produced impressive results on learning complex real-world distributions, recent works have shown that they suffer from lack of diversity or mode collapse. The theoretical work of Arora et al. (2017a) suggests a dilemma about GANs' statistical properties: powerful discriminators cause overfitting, whereas weak discriminators cannot detect mode collapse. By contrast, we show in this paper that GANs can in principle learn distributions in Wasserstein distance (or KL-divergence in many cases) with polynomial sample complexity, if the discriminator class has strong distinguishing power against the particular generator class (instead of against all possible generators). For various generator classes such as mixture of Gaussians, exponential families, and invertible and injective neural networks generators, we design corresponding discriminators (which are often neural nets of specific architectures) such that the Integral Probability Metric (IPM) induced by the discriminators can provably approximate the Wasserstein distance and/or KL-divergence. This implies that if the training is successful, then the learned distribution is close to the true distribution in Wasserstein distance or KL divergence, and thus cannot drop modes. Our preliminary experiments show that on synthetic datasets the test IPM is well correlated with KL divergence or the Wasserstein distance, indicating that the lack of diversity in GANs may be caused by the sub-optimality in optimization instead of statistical inefficiency",
    "checked": null,
    "id": "c3f03c67e119900e6f8f86740c445b46d0c5bcf3",
    "semantic_title": "approximability of discriminators implies diversity in gans",
    "citation_count": 74,
    "authors": []
  },
  "https://openreview.net/forum?id=B1xWcj0qYm": {
    "title": "On the Minimal Supervision for Training Any Binary Classifier from Only Unlabeled Data",
    "volume": "poster",
    "abstract": "Empirical risk minimization (ERM), with proper loss function and regularization, is the common practice of supervised classification. In this paper, we study training arbitrary (from linear to deep) binary classifier from only unlabeled (U) data by ERM. We prove that it is impossible to estimate the risk of an arbitrary binary classifier in an unbiased manner given a single set of U data, but it becomes possible given two sets of U data with different class priors. These two facts answer a fundamental question---what the minimal supervision is for training any binary classifier from only U data. Following these findings, we propose an ERM-based learning method from two sets of U data, and then prove it is consistent. Experiments demonstrate the proposed method could train deep models and outperform state-of-the-art methods for learning from two sets of U data",
    "checked": null,
    "id": "570f3c52e4e9608d65afd00076e784800c286524",
    "semantic_title": "on the minimal supervision for training any binary classifier from only unlabeled data",
    "citation_count": 87,
    "authors": []
  },
  "https://openreview.net/forum?id=r1NJqsRctX": {
    "title": "Auxiliary Variational MCMC",
    "volume": "poster",
    "abstract": "We introduce Auxiliary Variational MCMC, a novel framework for learning MCMC kernels that combines recent advances in variational inference with insights drawn from traditional auxiliary variable MCMC methods such as Hamiltonian Monte Carlo. Our framework exploits low dimensional structure in the target distribution in order to learn a more efficient MCMC sampler. The resulting sampler is able to suppress random walk behaviour and mix between modes efficiently, without the need to compute gradients of the target distribution. We test our sampler on a number of challenging distributions, where the underlying structure is known, and on the task of posterior sampling in Bayesian logistic regression. Code to reproduce all experiments is available at https://github.com/AVMCMC/AuxiliaryVariationalMCMC",
    "checked": null,
    "id": "ebbab9a45b241de9600f55d68025e2890333a942",
    "semantic_title": "auxiliary variational mcmc",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=S1zk9iRqF7": {
    "title": "PATE-GAN: Generating Synthetic Data with Differential Privacy Guarantees",
    "volume": "poster",
    "abstract": "Machine learning has the potential to assist many communities in using the large datasets that are becoming more and more available. Unfortunately, much of that potential is not being realized because it would require sharing data in a way that compromises privacy. In this paper, we investigate a method for ensuring (differential) privacy of the generator of the Generative Adversarial Nets (GAN) framework. The resulting model can be used for generating synthetic data on which algorithms can be trained and validated, and on which competitions can be conducted, without compromising the privacy of the original dataset. Our method modifies the Private Aggregation of Teacher Ensembles (PATE) framework and applies it to GANs. Our modified framework (which we call PATE-GAN) allows us to tightly bound the influence of any individual sample on the model, resulting in tight differential privacy guarantees and thus an improved performance over models with the same guarantees. We also look at measuring the quality of synthetic data from a new angle; we assert that for the synthetic data to be useful for machine learning researchers, the relative performance of two algorithms (trained and tested) on the synthetic dataset should be the same as their relative performance (when trained and tested) on the original dataset. Our experiments, on various datasets, demonstrate that PATE-GAN consistently outperforms the state-of-the-art method with respect to this and other notions of synthetic data quality",
    "checked": null,
    "id": "af1841e1db6579f1f1777a59c7e9e4658d2ac466",
    "semantic_title": "pate-gan: generating synthetic data with differential privacy guarantees",
    "citation_count": 658,
    "authors": []
  },
  "https://openreview.net/forum?id=r1f0YiCctm": {
    "title": "Minimal Random Code Learning: Getting Bits Back from Compressed Model Parameters",
    "volume": "poster",
    "abstract": "While deep neural networks are a highly successful model class, their large memory footprint puts considerable strain on energy consumption, communication bandwidth, and storage requirements. Consequently, model size reduction has become an utmost goal in deep learning. A typical approach is to train a set of deterministic weights, while applying certain techniques such as pruning and quantization, in order that the empirical weight distribution becomes amenable to Shannon-style coding schemes. However, as shown in this paper, relaxing weight determinism and using a full variational distribution over weights allows for more efficient coding schemes and consequently higher compression rates. In particular, following the classical bits-back argument, we encode the network weights using a random sample, requiring only a number of bits corresponding to the Kullback-Leibler divergence between the sampled variational distribution and the encoding distribution. By imposing a constraint on the Kullback-Leibler divergence, we are able to explicitly control the compression rate, while optimizing the expected loss on the training set. The employed encoding scheme can be shown to be close to the optimal information-theoretical lower bound, with respect to the employed variational family. Our method sets new state-of-the-art in neural network compression, as it strictly dominates previous approaches in a Pareto sense: On the benchmarks LeNet-5/MNIST and VGG-16/CIFAR-10, our approach yields the best test performance for a fixed memory budget, and vice versa, it achieves the highest compression rates for a fixed test performance",
    "checked": null,
    "id": "90b60322be13e63c9903b40b3407861e001eba1d",
    "semantic_title": "minimal random code learning: getting bits back from compressed model parameters",
    "citation_count": 80,
    "authors": []
  },
  "https://openreview.net/forum?id=ryf6Fs09YX": {
    "title": "GO Gradient for Expectation-Based Objectives",
    "volume": "poster",
    "abstract": "Within many machine learning algorithms, a fundamental problem concerns efficient calculation of an unbiased gradient wrt parameters $\\boldsymbol{\\gamma}$ for expectation-based objectives $\\mathbb{E}_{q_{\\boldsymbol{\\gamma}} (\\boldsymbol{y})} [f (\\boldsymbol{y}) ]$. Most existing methods either ($i$) suffer from high variance, seeking help from (often) complicated variance-reduction techniques; or ($ii$) they only apply to reparameterizable continuous random variables and employ a reparameterization trick. To address these limitations, we propose a General and One-sample (GO) gradient that ($i$) applies to many distributions associated with non-reparameterizable continuous {\\em or} discrete random variables, and ($ii$) has the same low-variance as the reparameterization trick. We find that the GO gradient often works well in practice based on only one Monte Carlo sample (although one can of course use more samples if desired). Alongside the GO gradient, we develop a means of propagating the chain rule through distributions, yielding statistical back-propagation, coupling neural networks to common random variables",
    "checked": null,
    "id": "15d0330ef349310db06e7f9babd8f09970905b51",
    "semantic_title": "go gradient for expectation-based objectives",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=HJz6tiCqYm": {
    "title": "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations",
    "volume": "poster",
    "abstract": "In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, ImageNet-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called ImageNet-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize",
    "checked": null,
    "id": "49b64383fe36268410c430352637ed23b16820c5",
    "semantic_title": "benchmarking neural network robustness to common corruptions and perturbations",
    "citation_count": 3425,
    "authors": []
  },
  "https://openreview.net/forum?id=HkxaFoC9KQ": {
    "title": "Deep reinforcement learning with relational inductive biases",
    "volume": "poster",
    "abstract": "We introduce an approach for augmenting model-free deep reinforcement learning agents with a mechanism for relational reasoning over structured representations, which improves performance, learning efficiency, generalization, and interpretability. Our architecture encodes an image as a set of vectors, and applies an iterative message-passing procedure to discover and reason about relevant entities and relations in a scene. In six of seven StarCraft II Learning Environment mini-games, our agent achieved state-of-the-art performance, and surpassed human grandmaster-level on four. In a novel navigation and planning task, our agent's performance and learning efficiency far exceeded non-relational baselines, it was able to generalize to more complex scenes than it had experienced during training. Moreover, when we examined its learned internal representations, they reflected important structure about the problem and the agent's intentions. The main contribution of this work is to introduce techniques for representing and reasoning about states in model-free deep reinforcement learning agents via relational inductive biases. Our experiments show this approach can offer advantages in efficiency, generalization, and interpretability, and can scale up to meet some of the most challenging test environments in modern artificial intelligence",
    "checked": null,
    "id": "9ea92ebeb7462f2db346cfa3281ad7497b1063d6",
    "semantic_title": "deep reinforcement learning with relational inductive biases",
    "citation_count": 208,
    "authors": []
  },
  "https://openreview.net/forum?id=S1E3Ko09F7": {
    "title": "L-Shapley and C-Shapley: Efficient Model Interpretation for Structured Data",
    "volume": "poster",
    "abstract": "Instancewise feature scoring is a method for model interpretation, which yields, for each test instance, a vector of importance scores associated with features. Methods based on the Shapley score have been proposed as a fair way of computing feature attributions, but incur an exponential complexity in the number of features. This combinatorial explosion arises from the definition of Shapley value and prevents these methods from being scalable to large data sets and complex models. We focus on settings in which the data have a graph structure, and the contribution of features to the target variable is well-approximated by a graph-structured factorization. In such settings, we develop two algorithms with linear complexity for instancewise feature importance scoring on black-box models. We establish the relationship of our methods to the Shapley value and a closely related concept known as the Myerson value from cooperative game theory. We demonstrate on both language and image data that our algorithms compare favorably with other methods using both quantitative metrics and human evaluation",
    "checked": null,
    "id": "85634f8a458f13d7cb80e946eec23880295ba1a3",
    "semantic_title": "l-shapley and c-shapley: efficient model interpretation for structured data",
    "citation_count": 214,
    "authors": []
  },
  "https://openreview.net/forum?id=S1x2Fj0qKQ": {
    "title": "Whitening and Coloring Batch Transform for GANs",
    "volume": "poster",
    "abstract": "Batch Normalization (BN) is a common technique used to speed-up and stabilize training. On the other hand, the learnable parameters of BN are commonly used in conditional Generative Adversarial Networks (cGANs) for representing class-specific information using conditional Batch Normalization (cBN). In this paper we propose to generalize both BN and cBN using a Whitening and Coloring based batch normalization. We show that our conditional Coloring can represent categorical conditioning information which largely helps the cGAN qualitative results. Moreover, we show that full-feature whitening is important in a general GAN scenario in which the training process is known to be highly unstable. We test our approach on different datasets and using different GAN networks and training protocols, showing a consistent improvement in all the tested frameworks. Our CIFAR-10 conditioned results are higher than all previous works on this dataset",
    "checked": null,
    "id": "e5cf28063f76ad1a3cda56f68b08ff264a0aa198",
    "semantic_title": "whitening and coloring batch transform for gans",
    "citation_count": 50,
    "authors": []
  },
  "https://openreview.net/forum?id=Sk4jFoA9K7": {
    "title": "PeerNets: Exploiting Peer Wisdom Against Adversarial Attacks",
    "volume": "poster",
    "abstract": "Deep learning systems have become ubiquitous in many aspects of our lives. Unfortunately, it has been shown that such systems are vulnerable to adversarial attacks, making them prone to potential unlawful uses. Designing deep neural networks that are robust to adversarial attacks is a fundamental step in making such systems safer and deployable in a broader variety of applications (e.g. autonomous driving), but more importantly is a necessary step to design novel and more advanced architectures built on new computational paradigms rather than marginally building on the existing ones. In this paper we introduce PeerNets, a novel family of convolutional networks alternating classical Euclidean convolutions with graph convolutions to harness information from a graph of peer samples. This results in a form of non-local forward propagation in the model, where latent features are conditioned on the global structure induced by the graph, that is up to 3 times more robust to a variety of white- and black-box adversarial attacks compared to conventional architectures with almost no drop in accuracy",
    "checked": null,
    "id": "bfb0d179916c000d54f27e7a9ea18b6269963e74",
    "semantic_title": "peernets: exploiting peer wisdom against adversarial attacks",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=B1gstsCqt7": {
    "title": "Sparse Dictionary Learning by Dynamical Neural Networks",
    "volume": "poster",
    "abstract": "A dynamical neural network consists of a set of interconnected neurons that interact over time continuously. It can exhibit computational properties in the sense that the dynamical system's evolution and/or limit points in the associated state space can correspond to numerical solutions to certain mathematical optimization or learning problems. Such a computational system is particularly attractive in that it can be mapped to a massively parallel computer architecture for power and throughput efficiency, especially if each neuron can rely solely on local information (i.e., local memory). Deriving gradients from the dynamical network's various states while conforming to this last constraint, however, is challenging. We show that by combining ideas of top-down feedback and contrastive learning, a dynamical network for solving the l1-minimizing dictionary learning problem can be constructed, and the true gradients for learning are provably computable by individual neurons. Using spiking neurons to construct our dynamical network, we present a learning process, its rigorous mathematical analysis, and numerical results on several dictionary learning problems",
    "checked": null,
    "id": "33cacec6bc67f52f611845b8c6a88feaa5f88d09",
    "semantic_title": "sparse dictionary learning by dynamical neural networks",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=HkxjYoCqKX": {
    "title": "Relaxed Quantization for Discretized Neural Networks",
    "volume": "poster",
    "abstract": "Neural network quantization has become an important research area due to its great impact on deployment of large models on resource constrained devices. In order to train networks that can be effectively discretized without loss of performance, we introduce a differentiable quantization procedure. Differentiability can be achieved by transforming continuous distributions over the weights and activations of the network to categorical distributions over the quantization grid. These are subsequently relaxed to continuous surrogates that can allow for efficient gradient-based optimization. We further show that stochastic rounding can be seen as a special case of the proposed approach and that under this formulation the quantization grid itself can also be optimized with gradient descent. We experimentally validate the performance of our method on MNIST, CIFAR 10 and Imagenet classification",
    "checked": null,
    "id": "8bf8aaea18d73577b7f6835a9dc821cb8e32a0d1",
    "semantic_title": "relaxed quantization for discretized neural networks",
    "citation_count": 132,
    "authors": []
  },
  "https://openreview.net/forum?id=HkgqFiAcFm": {
    "title": "Marginal Policy Gradients: A Unified Family of Estimators for Bounded Action Spaces with Applications",
    "volume": "poster",
    "abstract": "Many complex domains, such as robotics control and real-time strategy (RTS) games, require an agent to learn a continuous control. In the former, an agent learns a policy over R^d and in the latter, over a discrete set of actions each of which is parametrized by a continuous parameter. Such problems are naturally solved using policy based reinforcement learning (RL) methods, but unfortunately these often suffer from high variance leading to instability and slow convergence. Unnecessary variance is introduced whenever policies over bounded action spaces are modeled using distributions with unbounded support by applying a transformation T to the sampled action before execution in the environment. Recently, the variance reduced clipped action policy gradient (CAPG) was introduced for actions in bounded intervals, but to date no variance reduced methods exist when the action is a direction, something often seen in RTS games. To this end we introduce the angular policy gradient (APG), a stochastic policy gradient method for directional control. With the marginal policy gradients family of estimators we present a unified analysis of the variance reduction properties of APG and CAPG; our results provide a stronger guarantee than existing analyses for CAPG. Experimental results on a popular RTS game and a navigation task show that the APG estimator offers a substantial improvement over the standard policy gradient",
    "checked": null,
    "id": "a2c7c3b8ffe918912f25cf7f476d3b6f603e8153",
    "semantic_title": "marginal policy gradients: a unified family of estimators for bounded action spaces with applications",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=H1gKYo09tX": {
    "title": "code2seq: Generating Sequences from Structured Representations of Code",
    "volume": "poster",
    "abstract": "The ability to generate natural language sequences from source code snippets has a variety of applications such as code summarization, documentation, and retrieval. Sequence-to-sequence (seq2seq) models, adopted from neural machine translation (NMT), have achieved state-of-the-art performance on these tasks by treating source code as a sequence of tokens. We present code2seq: an alternative approach that leverages the syntactic structure of programming languages to better encode source code. Our model represents a code snippet as the set of compositional paths in its abstract syntax tree (AST) and uses attention to select the relevant paths while decoding. We demonstrate the effectiveness of our approach for two tasks, two programming languages, and four datasets of up to 16M examples. Our model significantly outperforms previous models that were specifically designed for programming languages, as well as general state-of-the-art NMT models. An interactive online demo of our model is available at http://code2seq.org. Our code, data and trained models are available at http://github.com/tech-srl/code2seq",
    "checked": null,
    "id": "98d1307bed619b58b4a44acd8e65ac58495776c2",
    "semantic_title": "code2seq: generating sequences from structured representations of code",
    "citation_count": 700,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk4dFjR5K7": {
    "title": "ADef: an Iterative Algorithm to Construct Adversarial Deformations",
    "volume": "poster",
    "abstract": "While deep neural networks have proven to be a powerful tool for many recognition and classification tasks, their stability properties are still not well understood. In the past, image classifiers have been shown to be vulnerable to so-called adversarial attacks, which are created by additively perturbing the correctly classified image. In this paper, we propose the ADef algorithm to construct a different kind of adversarial attack created by iteratively applying small deformations to the image, found through a gradient descent step. We demonstrate our results on MNIST with convolutional neural networks and on ImageNet with Inception-v3 and ResNet-101",
    "checked": null,
    "id": "3ecda636a99ec93acc941c0217a65c9a3af9562f",
    "semantic_title": "adef: an iterative algorithm to construct adversarial deformations",
    "citation_count": 97,
    "authors": []
  },
  "https://openreview.net/forum?id=rke_YiRct7": {
    "title": "Small nonlinearities in activation functions create bad local minima in neural networks",
    "volume": "poster",
    "abstract": "We investigate the loss surface of neural networks. We prove that even for one-hidden-layer networks with \"slightest\" nonlinearity, the empirical risks have spurious local minima in most cases. Our results thus indicate that in general \"no spurious local minim\" is a property limited to deep linear networks, and insights obtained from linear networks may not be robust. Specifically, for ReLU(-like) networks we constructively prove that for almost all practical datasets there exist infinitely many local minima. We also present a counterexample for more general activations (sigmoid, tanh, arctan, ReLU, etc.), for which there exists a bad local minimum. Our results make the least restrictive assumptions relative to existing results on spurious local optima in neural networks. We complete our discussion by presenting a comprehensive characterization of global optimality for deep linear networks, which unifies other results on this topic",
    "checked": null,
    "id": "ab0813895b5a7df2bffd34808a8974d7220ac36b",
    "semantic_title": "small nonlinearities in activation functions create bad local minima in neural networks",
    "citation_count": 95,
    "authors": []
  },
  "https://openreview.net/forum?id=SyNvti09KQ": {
    "title": "Visceral Machines: Risk-Aversion in Reinforcement Learning with Intrinsic Physiological Rewards",
    "volume": "poster",
    "abstract": "As people learn to navigate the world, autonomic nervous system (e.g., ``fight or flight) responses provide intrinsic feedback about the potential consequence of action choices (e.g., becoming nervous when close to a cliff edge or driving fast around a bend.) Physiological changes are correlated with these biological preparations to protect one-self from danger. We present a novel approach to reinforcement learning that leverages a task-independent intrinsic reward function trained on peripheral pulse measurements that are correlated with human autonomic nervous system responses. Our hypothesis is that such reward functions can circumvent the challenges associated with sparse and skewed rewards in reinforcement learning settings and can help improve sample efficiency. We test this in a simulated driving environment and show that it can increase the speed of learning and reduce the number of collisions during the learning stage",
    "checked": null,
    "id": "56cf1351c926a3f837d0c53eda7d2f58ececf6d6",
    "semantic_title": "visceral machines: risk-aversion in reinforcement learning with intrinsic physiological rewards",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=SJfPFjA9Fm": {
    "title": "ACCELERATING NONCONVEX LEARNING VIA REPLICA EXCHANGE LANGEVIN DIFFUSION",
    "volume": "poster",
    "abstract": "Langevin diffusion is a powerful method for nonconvex optimization, which enables the escape from local minima by injecting noise into the gradient. In particular, the temperature parameter controlling the noise level gives rise to a tradeoff between ``global exploration'' and ``local exploitation'', which correspond to high and low temperatures. To attain the advantages of both regimes, we propose to use replica exchange, which swaps between two Langevin diffusions with different temperatures. We theoretically analyze the acceleration effect of replica exchange from two perspectives: (i) the convergence in $\\chi^2$-divergence, and (ii) the large deviation principle. Such an acceleration effect allows us to faster approach the global minima. Furthermore, by discretizing the replica exchange Langevin diffusion, we obtain a discrete-time algorithm. For such an algorithm, we quantify its discretization error in theory and demonstrate its acceleration effect in practice",
    "checked": null,
    "id": "d2eff663ca365f3f4d05ff3819a81dabf7400a9a",
    "semantic_title": "accelerating nonconvex learning via replica exchange langevin diffusion",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=rJevYoA9Fm": {
    "title": "The Singular Values of Convolutional Layers",
    "volume": "poster",
    "abstract": "We characterize the singular values of the linear transformation associated with a standard 2D multi-channel convolutional layer, enabling their efficient computation. This characterization also leads to an algorithm for projecting a convolutional layer onto an operator-norm ball. We show that this is an effective regularizer; for example, it improves the test error of a deep residual network using batch normalization on CIFAR-10 from 6.2% to 5.3%",
    "checked": null,
    "id": "fb2edf25484c9e9e5f94b719c55dc1faf7591bfa",
    "semantic_title": "the singular values of convolutional layers",
    "citation_count": 202,
    "authors": []
  },
  "https://openreview.net/forum?id=ByxPYjC5KQ": {
    "title": "Improving Generalization and Stability of Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "Generative Adversarial Networks (GANs) are one of the most popular tools for learning complex high dimensional distributions. However, generalization properties of GANs have not been well understood. In this paper, we analyze the generalization of GANs in practical settings. We show that discriminators trained on discrete datasets with the original GAN loss have poor generalization capability and do not approximate the theoretically optimal discriminator. We propose a zero-centered gradient penalty for improving the generalization of the discriminator by pushing it toward the optimal discriminator. The penalty guarantees the generalization and convergence of GANs. Experiments on synthetic and large scale datasets verify our theoretical analysis",
    "checked": null,
    "id": "a7dbdcac11bc300d73e0352fc0422b57ff075f51",
    "semantic_title": "improving generalization and stability of generative adversarial networks",
    "citation_count": 122,
    "authors": []
  },
  "https://openreview.net/forum?id=r1xwKoR9Y7": {
    "title": "GamePad: A Learning Environment for Theorem Proving",
    "volume": "poster",
    "abstract": "In this paper, we introduce a system called GamePad that can be used to explore the application of machine learning methods to theorem proving in the Coq proof assistant. Interactive theorem provers such as Coq enable users to construct machine-checkable proofs in a step-by-step manner. Hence, they provide an opportunity to explore theorem proving with human supervision. We use GamePad to synthesize proofs for a simple algebraic rewrite problem and train baseline models for a formalization of the Feit-Thompson theorem. We address position evaluation (i.e., predict the number of proof steps left) and tactic prediction (i.e., predict the next proof step) tasks, which arise naturally in tactic-based theorem proving",
    "checked": null,
    "id": "87c425f23bcac2f082968abda64a971f91522f73",
    "semantic_title": "gamepad: a learning environment for theorem proving",
    "citation_count": 110,
    "authors": []
  },
  "https://openreview.net/forum?id=HJlLKjR9FQ": {
    "title": "Towards Understanding Regularization in Batch Normalization",
    "volume": "poster",
    "abstract": "Batch Normalization (BN) improves both convergence and generalization in training neural networks. This work understands these phenomena theoretically. We analyze BN by using a basic block of neural networks, consisting of a kernel layer, a BN layer, and a nonlinear activation function. This basic network helps us understand the impacts of BN in three aspects. First, by viewing BN as an implicit regularizer, BN can be decomposed into population normalization (PN) and gamma decay as an explicit regularization. Second, learning dynamics of BN and the regularization show that training converged with large maximum and effective learning rate. Third, generalization of BN is explored by using statistical mechanics. Experiments demonstrate that BN in convolutional neural networks share the same traits of regularization as the above analyses",
    "checked": null,
    "id": "9fa42ea422fe1729e04a98fa8072a7b48ce91bc8",
    "semantic_title": "towards understanding regularization in batch normalization",
    "citation_count": 180,
    "authors": []
  },
  "https://openreview.net/forum?id=SylLYsCcFm": {
    "title": "Learning to Make Analogies by Contrasting Abstract Relational Structure",
    "volume": "poster",
    "abstract": "Analogical reasoning has been a principal focus of various waves of AI research. Analogy is particularly challenging for machines because it requires relational structures to be represented such that they can be flexibly applied across diverse domains of experience. Here, we study how analogical reasoning can be induced in neural networks that learn to perceive and reason about raw visual data. We find that the critical factor for inducing such a capacity is not an elaborate architecture, but rather, careful attention to the choice of data and the manner in which it is presented to the model. The most robust capacity for analogical reasoning is induced when networks learn analogies by contrasting abstract relational structures in their input domains, a training method that uses only the input data to force models to learn about important abstract features. Using this technique we demonstrate capacities for complex, visual and symbolic analogy making and generalisation in even the simplest neural network architectures",
    "checked": null,
    "id": "0d16b0fbe1e2bf6ef02aea2e058f2e13c3a83fa2",
    "semantic_title": "learning to make analogies by contrasting abstract relational structure",
    "citation_count": 99,
    "authors": []
  },
  "https://openreview.net/forum?id=ByxBFsRqYm": {
    "title": "Attention, Learn to Solve Routing Problems!",
    "volume": "poster",
    "abstract": "The recently presented idea to learn heuristics for combinatorial optimization problems is promising as it can save costly development. However, to push this idea towards practical implementation, we need better models and better ways of training. We contribute in both directions: we propose a model based on attention layers with benefits over the Pointer Network and we show how to train this model using REINFORCE with a simple baseline based on a deterministic greedy rollout, which we find is more efficient than using a value function. We significantly improve over recent learned heuristics for the Travelling Salesman Problem (TSP), getting close to optimal results for problems up to 100 nodes. With the same hyperparameters, we learn strong heuristics for two variants of the Vehicle Routing Problem (VRP), the Orienteering Problem (OP) and (a stochastic variant of) the Prize Collecting TSP (PCTSP), outperforming a wide range of baselines and getting results close to highly optimized and specialized algorithms",
    "checked": null,
    "id": "ce4f001c1d8ddb9a95cf54e14240ef02c44bd329",
    "semantic_title": "attention, learn to solve routing problems!",
    "citation_count": 1202,
    "authors": []
  },
  "https://openreview.net/forum?id=BkeStsCcKQ": {
    "title": "Critical Learning Periods in Deep Networks",
    "volume": "poster",
    "abstract": "Similar to humans and animals, deep artificial neural networks exhibit critical periods during which a temporary stimulus deficit can impair the development of a skill. The extent of the impairment depends on the onset and length of the deficit window, as in animal models, and on the size of the neural network. Deficits that do not affect low-level statistics, such as vertical flipping of the images, have no lasting effect on performance and can be overcome with further training. To better understand this phenomenon, we use the Fisher Information of the weights to measure the effective connectivity between layers of a network during training. Counterintuitively, information rises rapidly in the early phases of training, and then decreases, preventing redistribution of information resources in a phenomenon we refer to as a loss of \"Information Plasticity\". Our analysis suggests that the first few epochs are critical for the creation of strong connections that are optimal relative to the input data distribution. Once such strong connections are created, they do not appear to change during additional training. These findings suggest that the initial learning transient, under-scrutinized compared to asymptotic behavior, plays a key role in determining the outcome of the training process. Our findings, combined with recent theoretical results in the literature, also suggest that forgetting (decrease of information in the weights) is critical to achieving invariance and disentanglement in representation learning. Finally, critical periods are not restricted to biological systems, but can emerge naturally in learning systems, whether biological or artificial, due to fundamental constrains arising from learning dynamics and information processing",
    "checked": null,
    "id": "1802a7870a642d414f435273dd9e9190a0dc4fcb",
    "semantic_title": "critical learning periods in deep networks",
    "citation_count": 135,
    "authors": []
  },
  "https://openreview.net/forum?id=HkxStoC5F7": {
    "title": "Meta-Learning Probabilistic Inference for Prediction",
    "volume": "poster",
    "abstract": "This paper introduces a new framework for data efficient and versatile learning. Specifically: 1) We develop ML-PIP, a general framework for Meta-Learning approximate Probabilistic Inference for Prediction. ML-PIP extends existing probabilistic interpretations of meta-learning to cover a broad class of methods. 2) We introduce \\Versa{}, an instance of the framework employing a flexible and versatile amortization network that takes few-shot learning datasets as inputs, with arbitrary numbers of shots, and outputs a distribution over task-specific parameters in a single forward pass. \\Versa{} substitutes optimization at test time with forward passes through inference networks, amortizing the cost of inference and relieving the need for second derivatives during training. 3) We evaluate \\Versa{} on benchmark datasets where the method sets new state-of-the-art results, and can handle arbitrary number of shots, and for classification, arbitrary numbers of classes at train and test time. The power of the approach is then demonstrated through a challenging few-shot ShapeNet view reconstruction task",
    "checked": null,
    "id": "7b0aad12a6917b7d444ba2f87c7f8ccc5357797a",
    "semantic_title": "meta-learning probabilistic inference for prediction",
    "citation_count": 264,
    "authors": []
  },
  "https://openreview.net/forum?id=HyeVtoRqtQ": {
    "title": "Trellis Networks for Sequence Modeling",
    "volume": "poster",
    "abstract": "We present trellis networks, a new architecture for sequence modeling. On the one hand, a trellis network is a temporal convolutional network with special structure, characterized by weight tying across depth and direct injection of the input into deep layers. On the other hand, we show that truncated recurrent networks are equivalent to trellis networks with special sparsity structure in their weight matrices. Thus trellis networks with general weight matrices generalize truncated recurrent networks. We leverage these connections to design high-performing trellis networks that absorb structural and algorithmic elements from both recurrent and convolutional models. Experiments demonstrate that trellis networks outperform the current state of the art methods on a variety of challenging benchmarks, including word-level language modeling and character-level language modeling tasks, and stress tests designed to evaluate long-term memory retention. The code is available at https://github.com/locuslab/trellisnet",
    "checked": null,
    "id": "a14af711aaa3ae83eb64d1f517b024b8c3094a8a",
    "semantic_title": "trellis networks for sequence modeling",
    "citation_count": 146,
    "authors": []
  },
  "https://openreview.net/forum?id=Bke4KsA5FX": {
    "title": "Generative Code Modeling with Graphs",
    "volume": "poster",
    "abstract": "Generative models forsource code are an interesting structured prediction problem, requiring to reason about both hard syntactic and semantic constraints as well as about natural, likely programs. We present a novel model for this problem that uses a graph to represent the intermediate state of the generated output. Our model generates code by interleaving grammar-driven expansion steps with graph augmentation and neural message passing steps. An experimental evaluation shows that our new model can generate semantically meaningful expressions, outperforming a range of strong baselines",
    "checked": null,
    "id": "75c0d369c7151b925155cfe1b3f01dd7d0503981",
    "semantic_title": "generative code modeling with graphs",
    "citation_count": 178,
    "authors": []
  },
  "https://openreview.net/forum?id=HkNGYjR9FX": {
    "title": "Learning Recurrent Binary/Ternary Weights",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "ce98b1ec4c79235eb83539ff1379c0f79dd3e2b9",
    "semantic_title": "learning recurrent binary/ternary weights",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=SJfZKiC5FX": {
    "title": "Dynamically Unfolding Recurrent Restorer: A Moving Endpoint Control Method for Image Restoration",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "f2ef09c15eeb7d197c8ec23fb85ce3bf4be6bd63",
    "semantic_title": "dynamically unfolding recurrent restorer: a moving endpoint control method for image restoration",
    "citation_count": 51,
    "authors": []
  },
  "https://openreview.net/forum?id=HJMC_iA5tm": {
    "title": "Learning a SAT Solver from Single-Bit Supervision",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "fe257027193ea4a74fdab99d7509ce4002ad7de6",
    "semantic_title": "learning a sat solver from single-bit supervision",
    "citation_count": 422,
    "authors": []
  },
  "https://openreview.net/forum?id=BklCusRct7": {
    "title": "Optimal Transport Maps For Distribution Preserving Operations on Latent Spaces of Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "fcb68849a63000c05c9b8cd592302a82b723dc32",
    "semantic_title": "optimal transport maps for distribution preserving operations on latent spaces of generative models",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=Hkf2_sC5FX": {
    "title": "Efficient Lifelong Learning with A-GEM",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "4a954b3e72a61968ab235076bcc242aca3a05520",
    "semantic_title": "efficient lifelong learning with a-gem",
    "citation_count": 1449,
    "authors": []
  },
  "https://openreview.net/forum?id=HkeoOo09YX": {
    "title": "Meta-Learning For Stochastic Gradient MCMC",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "43410e1d4b8790582717013542313c994588590c",
    "semantic_title": "meta-learning for stochastic gradient mcmc",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=B1G9doA9F7": {
    "title": "Augmented Cyclic Adversarial Learning for Low Resource Domain Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "653435e90a6dc9c331e03f70ed32906eb3f6810d",
    "semantic_title": "augmented cyclic adversarial learning for low resource domain adaptation",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=HklKui0ct7": {
    "title": "Off-Policy Evaluation and Learning from Logged Bandit Feedback: Error Reduction via Surrogate Policy",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "91727b55133f77dafed05603da0d4752a7606019",
    "semantic_title": "off-policy evaluation and learning from logged bandit feedback: error reduction via surrogate policy",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=HkfYOoCcYX": {
    "title": "Double Viterbi: Weight Encoding for High Compression Ratio and Fast On-Chip Reconstruction for Deep Neural Network",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "1f7fa36fe81a6447bdcd06a84f72228e5dc0524a",
    "semantic_title": "double viterbi: weight encoding for high compression ratio and fast on-chip reconstruction for deep neural network",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=H1ewdiR5tQ": {
    "title": "Graph Wavelet Neural Network",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "9f91568d8ec72c417d9146a551b61e69ccf1f848",
    "semantic_title": "graph wavelet neural network",
    "citation_count": 330,
    "authors": []
  },
  "https://openreview.net/forum?id=SJgw_sRqFQ": {
    "title": "The Unusual Effectiveness of Averaging in GAN Training",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "fed5546113d945807e2f24317c0e560d1d21068c",
    "semantic_title": "the unusual effectiveness of averaging in gan training",
    "citation_count": 175,
    "authors": []
  },
  "https://openreview.net/forum?id=HyGIdiRqtm": {
    "title": "Evaluating Robustness of Neural Networks with Mixed Integer Programming",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "9de69a46e6c619255eeffbfbb6c7b7163690eb48",
    "semantic_title": "evaluating robustness of neural networks with mixed integer programming",
    "citation_count": 837,
    "authors": []
  },
  "https://openreview.net/forum?id=S1EHOsC9tX": {
    "title": "Towards the first adversarially robust neural network model on MNIST",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "fd7789de401811fd8692466b8d49230e7184655f",
    "semantic_title": "towards the first adversarially robust neural network model on mnist",
    "citation_count": 369,
    "authors": []
  },
  "https://openreview.net/forum?id=HyGBdo0qFm": {
    "title": "On the Turing Completeness of Modern Neural Network Architectures",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "3694381e74445a8b9f8cb8d373e39626e47191b5",
    "semantic_title": "on the turing completeness of modern neural network architectures",
    "citation_count": 145,
    "authors": []
  },
  "https://openreview.net/forum?id=ByeSdsC9Km": {
    "title": "Adaptive Posterior Learning: few-shot learning with a surprise-based memory module",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "f3bcab5b23eb8999ea3c3c29140c75861255cfea",
    "semantic_title": "adaptive posterior learning: few-shot learning with a surprise-based memory module",
    "citation_count": 77,
    "authors": []
  },
  "https://openreview.net/forum?id=r14EOsCqKX": {
    "title": "A Closer Look at Deep Learning Heuristics: Learning rate restarts, Warmup and Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "c4ab32dc966bff2de35723374f7410eeab85053f",
    "semantic_title": "a closer look at deep learning heuristics: learning rate restarts, warmup and distillation",
    "citation_count": 276,
    "authors": []
  },
  "https://openreview.net/forum?id=Syl7OsRqY7": {
    "title": "Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "e17c13217e1fad11bae46820c4acae1745f69b43",
    "semantic_title": "coarse-grain fine-grain coattention network for multi-evidence question answering",
    "citation_count": 63,
    "authors": []
  },
  "https://openreview.net/forum?id=H1emus0qF7": {
    "title": "Near-Optimal Representation Learning for Hierarchical Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "e4a89a978f747d0b548f5887b2380c5f618061f0",
    "semantic_title": "near-optimal representation learning for hierarchical reinforcement learning",
    "citation_count": 210,
    "authors": []
  },
  "https://openreview.net/forum?id=H1gfOiAqYm": {
    "title": "Execution-Guided Neural Program Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "6c41bedc4637f3fd504c68baa3b3d8881e056ac1",
    "semantic_title": "execution-guided neural program synthesis",
    "citation_count": 161,
    "authors": []
  },
  "https://openreview.net/forum?id=rJlWOj0qF7": {
    "title": "Imposing Category Trees Onto Word-Embeddings Using A Geometric Construction",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "df97d99457cac7ba4cac120018174790f1e1bc1c",
    "semantic_title": "imposing category trees onto word-embeddings using a geometric construction",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=BJgRDjR9tQ": {
    "title": "ROBUST ESTIMATION VIA GENERATIVE ADVERSARIAL NETWORKS",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "a78d516846e1931b251ae873d9e78f2717c2ccec",
    "semantic_title": "robust estimation via generative adversarial networks",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=BJG0voC9YQ": {
    "title": "Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "745a134eca192982e8e0c16d6f36cfe24f9bdd08",
    "semantic_title": "woulda, coulda, shoulda: counterfactually-guided policy search",
    "citation_count": 137,
    "authors": []
  },
  "https://openreview.net/forum?id=Byg0DsCqYQ": {
    "title": "Robust Conditional Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "0251807835a2d863c809c25b3d5899d6431dbe89",
    "semantic_title": "robust conditional generative adversarial networks",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=SkE6PjC9KX": {
    "title": "Attentive Neural Processes",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "9b396268f367917211bbd33947325e72b7742d36",
    "semantic_title": "attentive neural processes",
    "citation_count": 440,
    "authors": []
  },
  "https://openreview.net/forum?id=B1fpDsAqt7": {
    "title": "Visual Reasoning by Progressive Module Networks",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "7af4a37e6e63b5f06e7bfb6e7c8910322774efb9",
    "semantic_title": "visual reasoning by progressive module networks",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=Bkg2viA5FQ": {
    "title": "Hindsight policy gradients",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "1fa1f04b80f057e477549e6b9798fab7c7e57db5",
    "semantic_title": "hindsight policy gradients",
    "citation_count": 68,
    "authors": []
  },
  "https://openreview.net/forum?id=SkloDjAqYm": {
    "title": "LeMoNADe: Learned Motif and Neuronal Assembly Detection in calcium imaging videos",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "bc652086b359a16dd8493270db22cc34f51e2ba8",
    "semantic_title": "lemonade: learned motif and neuronal assembly detection in calcium imaging videos",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=H1ziPjC5Fm": {
    "title": "Visual Explanation by Interpretation: Improving Visual Feedback Capabilities of Deep Neural Networks",
    "volume": "poster",
    "abstract": "Visual Interpretation and explanation of deep models is critical towards wide adoption of systems that rely on them. In this paper, we propose a novel scheme for both interpretation as well as explanation in which, given a pretrained model, we automatically identify internal features relevant for the set of classes considered by the model, without relying on additional annotations. We interpret the model through average visualizations of this reduced set of features. Then, at test time, we explain the network prediction by accompanying the predicted class label with supporting visualizations derived from the identified features. In addition, we propose a method to address the artifacts introduced by strided operations in deconvNet-based visualizations. Moreover, we introduce an8Flower , a dataset specifically designed for objective quantitative evaluation of methods for visual explanation. Experiments on the MNIST , ILSVRC 12, Fashion 144k and an8Flower datasets show that our method produces detailed explanations with good coverage of relevant features of the classes of interest",
    "checked": null,
    "id": "8d8bc608da14bc0ce32c3a5d1fdfbe037993626d",
    "semantic_title": "visual explanation by interpretation: improving visual feedback capabilities of deep neural networks",
    "citation_count": 62,
    "authors": []
  },
  "https://openreview.net/forum?id=BJg9DoR9t7": {
    "title": "Max-MIG: an Information Theoretic Approach for Joint Learning from Crowds",
    "volume": "poster",
    "abstract": "Eliciting labels from crowds is a potential way to obtain large labeled data. Despite a variety of methods developed for learning from crowds, a key challenge remains unsolved: \\emph{learning from crowds without knowing the information structure among the crowds a priori, when some people of the crowds make highly correlated mistakes and some of them label effortlessly (e.g. randomly)}. We propose an information theoretic approach, Max-MIG, for joint learning from crowds, with a common assumption: the crowdsourced labels and the data are independent conditioning on the ground truth. Max-MIG simultaneously aggregates the crowdsourced labels and learns an accurate data classifier. Furthermore, we devise an accurate data-crowds forecaster that employs both the data and the crowdsourced labels to forecast the ground truth. To the best of our knowledge, this is the first algorithm that solves the aforementioned challenge of learning from crowds. In addition to the theoretical validation, we also empirically show that our algorithm achieves the new state-of-the-art results in most settings, including the real-world data, and is the first algorithm that is robust to various information structures. Codes are available at https://github.com/Newbeeer/Max-MIG",
    "checked": null,
    "id": "789e1c58964f98f67471fe3552ed9934f24d10ff",
    "semantic_title": "max-mig: an information theoretic approach for joint learning from crowds",
    "citation_count": 55,
    "authors": []
  },
  "https://openreview.net/forum?id=BJfYvo09Y7": {
    "title": "Hierarchical Visuomotor Control of Humanoids",
    "volume": "poster",
    "abstract": "We aim to build complex humanoid agents that integrate perception, motor control, and memory. In this work, we partly factor this problem into low-level motor control from proprioception and high-level coordination of the low-level skills informed by vision. We develop an architecture capable of surprisingly flexible, task-directed motor control of a relatively high-DoF humanoid body by combining pre-training of low-level motor controllers with a high-level, task-focused controller that switches among low-level sub-policies. The resulting system is able to control a physically-simulated humanoid body to solve tasks that require coupling visual perception from an unstabilized egocentric RGB camera during locomotion in the environment. Supplementary video link: https://youtu.be/fBoir7PNxPk",
    "checked": null,
    "id": "a8a7219ee83cfd7ca258e20b5826a0c0786dcb73",
    "semantic_title": "hierarchical visuomotor control of humanoids",
    "citation_count": 97,
    "authors": []
  },
  "https://openreview.net/forum?id=BkgtDsCcKQ": {
    "title": "Function Space Particle Optimization for Bayesian Neural Networks",
    "volume": "poster",
    "abstract": "While Bayesian neural networks (BNNs) have drawn increasing attention, their posterior inference remains challenging, due to the high-dimensional and over-parameterized nature. To address this issue, several highly flexible and scalable variational inference procedures based on the idea of particle optimization have been proposed. These methods directly optimize a set of particles to approximate the target posterior. However, their application to BNNs often yields sub-optimal performance, as such methods have a particular failure mode on over-parameterized models. In this paper, we propose to solve this issue by performing particle optimization directly in the space of regression functions. We demonstrate through extensive experiments that our method successfully overcomes this issue, and outperforms strong baselines in a variety of tasks including prediction, defense against adversarial examples, and reinforcement learning",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SkMuPjRcKQ": {
    "title": "Feed-forward Propagation in Probabilistic Neural Networks with Categorical and Max Layers",
    "volume": "poster",
    "abstract": "Probabilistic Neural Networks deal with various sources of stochasticity: input noise, dropout, stochastic neurons, parameter uncertainties modeled as random variables, etc. In this paper we revisit a feed-forward propagation approach that allows one to estimate for each neuron its mean and variance w.r.t. all mentioned sources of stochasticity. In contrast, standard NNs propagate only point estimates, discarding the uncertainty. Methods propagating also the variance have been proposed by several authors in different context. The view presented here attempts to clarify the assumptions and derivation behind such methods, relate them to classical NNs and broaden their scope of applicability. The main technical contributions are new approximations for the distributions of argmax and max-related transforms, which allow for fully analytic uncertainty propagation in networks with softmax and max-pooling layers as well as leaky ReLU activations. We evaluate the accuracy of the approximation and suggest a simple calibration. Applying the method to networks with dropout allows for faster training and gives improved test likelihoods without the need of sampling",
    "checked": null,
    "id": "219ad53f77cb3a1953b8a766ec28c62ea8da49f8",
    "semantic_title": "feed-forward propagation in probabilistic neural networks with categorical and max layers",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=Hyl_vjC5KQ": {
    "title": "Hierarchical Reinforcement Learning via Advantage-Weighted Information Maximization",
    "volume": "poster",
    "abstract": "Real-world tasks are often highly structured. Hierarchical reinforcement learning (HRL) has attracted research interest as an approach for leveraging the hierarchical structure of a given task in reinforcement learning (RL). However, identifying the hierarchical policy structure that enhances the performance of RL is not a trivial task. In this paper, we propose an HRL method that learns a latent variable of a hierarchical policy using mutual information maximization. Our approach can be interpreted as a way to learn a discrete and latent representation of the state-action space. To learn option policies that correspond to modes of the advantage function, we introduce advantage-weighted importance sampling. In our HRL method, the gating policy learns to select option policies based on an option-value function, and these option policies are optimized based on the deterministic policy gradient method. This framework is derived by leveraging the analogy between a monolithic policy in standard RL and a hierarchical policy in HRL by using a deterministic option policy. Experimental results indicate that our HRL approach can learn a diversity of options and that it can enhance the performance of RL in continuous control tasks",
    "checked": null,
    "id": "1447cb195033be291674a44a07eb18ee894c23eb",
    "semantic_title": "hierarchical reinforcement learning via advantage-weighted information maximization",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=rJNwDjAqYX": {
    "title": "Large-Scale Study of Curiosity-Driven Learning",
    "volume": "poster",
    "abstract": "Reinforcement learning algorithms rely on carefully engineered rewards from the environment that are extrinsic to the agent. However, annotating each environment with hand-designed, dense rewards is difficult and not scalable, motivating the need for developing reward functions that are intrinsic to the agent. Curiosity is such intrinsic reward function which uses prediction error as a reward signal. In this paper: (a) We perform the first large-scale study of purely curiosity-driven learning, i.e. {\\em without any extrinsic rewards}, across $54$ standard benchmark environments, including the Atari game suite. Our results show surprisingly good performance as well as a high degree of alignment between the intrinsic curiosity objective and the hand-designed extrinsic rewards of many games. (b) We investigate the effect of using different feature spaces for computing prediction error and show that random features are sufficient for many popular RL game benchmarks, but learned features appear to generalize better (e.g. to novel game levels in Super Mario Bros.). (c) We demonstrate limitations of the prediction-based rewards in stochastic setups. Game-play videos and code are at https://doubleblindsupplementary.github.io/large-curiosity/",
    "checked": null,
    "id": "ca14dce53be20d3d23d4f0db844a8389ab619db3",
    "semantic_title": "large-scale study of curiosity-driven learning",
    "citation_count": 702,
    "authors": []
  },
  "https://openreview.net/forum?id=HJxwDiActX": {
    "title": "StrokeNet: A Neural Painting Environment",
    "volume": "poster",
    "abstract": "We've seen tremendous success of image generating models these years. Generating images through a neural network is usually pixel-based, which is fundamentally different from how humans create artwork using brushes. To imitate human drawing, interactions between the environment and the agent is required to allow trials. However, the environment is usually non-differentiable, leading to slow convergence and massive computation. In this paper we try to address the discrete nature of software environment with an intermediate, differentiable simulation. We present StrokeNet, a novel model where the agent is trained upon a well-crafted neural approximation of the painting environment. With this approach, our agent was able to learn to write characters such as MNIST digits faster than reinforcement learning approaches in an unsupervised manner. Our primary contribution is the neural simulation of a real-world environment. Furthermore, the agent trained with the emulated environment is able to directly transfer its skills to real-world software",
    "checked": null,
    "id": "412e4a122f5abc23ca9b8856d07d3dd962a93e5a",
    "semantic_title": "strokenet: a neural painting environment",
    "citation_count": 76,
    "authors": []
  },
  "https://openreview.net/forum?id=BkgBvsC9FQ": {
    "title": "DialogWAE: Multimodal Response Generation with Conditional Wasserstein Auto-Encoder",
    "volume": "poster",
    "abstract": "Variational autoencoders (VAEs) have shown a promise in data-driven conversation modeling. However, most VAE conversation models match the approximate posterior distribution over the latent variables to a simple prior such as standard normal distribution, thereby restricting the generated responses to a relatively simple (e.g., single-modal) scope. In this paper, we propose DialogWAE, a conditional Wasserstein autoencoder (WAE) specially designed for dialogue modeling. Unlike VAEs that impose a simple distribution over the latent variables, DialogWAE models the distribution of data by training a GAN within the latent variable space. Specifically, our model samples from the prior and posterior distributions over the latent variables by transforming context-dependent random noise using neural networks and minimizes the Wasserstein distance between the two distributions. We further develop a Gaussian mixture prior network to enrich the latent space. Experiments on two popular datasets show that DialogWAE outperforms the state-of-the-art approaches in generating more coherent, informative and diverse responses",
    "checked": null,
    "id": "77b9505e6967cfb45eaf6ec8ac7746cbaaab6e0d",
    "semantic_title": "dialogwae: multimodal response generation with conditional wasserstein auto-encoder",
    "citation_count": 129,
    "authors": []
  },
  "https://openreview.net/forum?id=ByMHvs0cFQ": {
    "title": "Quaternion Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "Recurrent neural networks (RNNs) are powerful architectures to model sequential data, due to their capability to learn short and long-term dependencies between the basic elements of a sequence. Nonetheless, popular tasks such as speech or images recognition, involve multi-dimensional input features that are characterized by strong internal dependencies between the dimensions of the input vector. We propose a novel quaternion recurrent neural network (QRNN), alongside with a quaternion long-short term memory neural network (QLSTM), that take into account both the external relations and these internal structural dependencies with the quaternion algebra. Similarly to capsules, quaternions allow the QRNN to code internal dependencies by composing and processing multidimensional features as single entities, while the recurrent operation reveals correlations between the elements composing the sequence. We show that both QRNN and QLSTM achieve better performances than RNN and LSTM in a realistic application of automatic speech recognition. Finally, we show that QRNN and QLSTM reduce by a maximum factor of 3.3x the number of free parameters needed, compared to real-valued RNNs and LSTMs to reach better results, leading to a more compact representation of the relevant information",
    "checked": null,
    "id": "31a857249f9f3bcdeb8a3b2944620fc16f128f64",
    "semantic_title": "quaternion recurrent neural networks",
    "citation_count": 131,
    "authors": []
  },
  "https://openreview.net/forum?id=SkfrvsA9FX": {
    "title": "Reward Constrained Policy Optimization",
    "volume": "poster",
    "abstract": "Solving tasks in Reinforcement Learning is no easy feat. As the goal of the agent is to maximize the accumulated reward, it often learns to exploit loopholes and misspecifications in the reward signal resulting in unwanted behavior. While constraints may solve this issue, there is no closed form solution for general constraints. In this work we present a novel multi-timescale approach for constrained policy optimization, called `Reward Constrained Policy Optimization' (RCPO), which uses an alternative penalty signal to guide the policy towards a constraint satisfying one. We prove the convergence of our approach and provide empirical evidence of its ability to train constraint satisfying policies",
    "checked": null,
    "id": "cb7c479a36520da1caeeec67db10772351a390c6",
    "semantic_title": "reward constrained policy optimization",
    "citation_count": 540,
    "authors": []
  },
  "https://openreview.net/forum?id=SJgNwi09Km": {
    "title": "Learning Latent Superstructures in Variational Autoencoders for Deep Multidimensional Clustering",
    "volume": "poster",
    "abstract": "We investigate a variant of variational autoencoders where there is a superstructure of discrete latent variables on top of the latent features. In general, our superstructure is a tree structure of multiple super latent variables and it is automatically learned from data. When there is only one latent variable in the superstructure, our model reduces to one that assumes the latent features to be generated from a Gaussian mixture model. We call our model the latent tree variational autoencoder (LTVAE). Whereas previous deep learning methods for clustering produce only one partition of data, LTVAE produces multiple partitions of data, each being given by one super latent variable. This is desirable because high dimensional data usually have many different natural facets and can be meaningfully partitioned in multiple ways",
    "checked": null,
    "id": "7b85357834e398437a291906aded59caff5151eb",
    "semantic_title": "learning latent superstructures in variational autoencoders for deep multidimensional clustering",
    "citation_count": 51,
    "authors": []
  },
  "https://openreview.net/forum?id=SygQvs0cFQ": {
    "title": "Variational Smoothing in Recurrent Neural Network Language Models",
    "volume": "poster",
    "abstract": "We present a new theoretical perspective of data noising in recurrent neural network language models (Xie et al., 2017). We show that each variant of data noising is an instance of Bayesian recurrent neural networks with a particular variational distribution (i.e., a mixture of Gaussians whose weights depend on statistics derived from the corpus such as the unigram distribution). We use this insight to propose a more principled method to apply at prediction time and propose natural extensions to data noising under the variational framework. In particular, we propose variational smoothing with tied input and output embedding matrices and an element-wise variational smoothing method. We empirically verify our analysis on two benchmark language modeling datasets and demonstrate performance improvements over existing data noising methods",
    "checked": null,
    "id": "08736d66224afbcd5514947a79a08a424a6f0576",
    "semantic_title": "variational smoothing in recurrent neural network language models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=B1GMDsR5tm": {
    "title": "Initialized Equilibrium Propagation for Backprop-Free Training",
    "volume": "poster",
    "abstract": "Deep neural networks are almost universally trained with reverse-mode automatic differentiation (a.k.a. backpropagation). Biological networks, on the other hand, appear to lack any mechanism for sending gradients back to their input neurons, and thus cannot be learning in this way. In response to this, Scellier & Bengio (2017) proposed Equilibrium Propagation - a method for gradient-based train- ing of neural networks which uses only local learning rules and, crucially, does not rely on neurons having a mechanism for back-propagating an error gradient. Equilibrium propagation, however, has a major practical limitation: inference involves doing an iterative optimization of neural activations to find a fixed-point, and the number of steps required to closely approximate this fixed point scales poorly with the depth of the network. In response to this problem, we propose Initialized Equilibrium Propagation, which trains a feedforward network to initialize the iterative inference procedure for Equilibrium propagation. This feed-forward network learns to approximate the state of the fixed-point using a local learning rule. After training, we can simply use this initializing network for inference, resulting in a learned feedforward network. Our experiments show that this network appears to work as well or better than the original version of Equilibrium propagation. This shows how we might go about training deep networks without using backpropagation",
    "checked": null,
    "id": "6811848615502f5347b3330110be6496bd2c6e4f",
    "semantic_title": "initialized equilibrium propagation for backprop-free training",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=H1z-PsR5KX": {
    "title": "Identifying and Controlling Important Neurons in Neural Machine Translation",
    "volume": "poster",
    "abstract": "Neural machine translation (NMT) models learn representations containing substantial linguistic information. However, it is not clear if such information is fully distributed or if some of it can be attributed to individual neurons. We develop unsupervised methods for discovering important neurons in NMT models. Our methods rely on the intuition that different models learn similar properties, and do not require any costly external supervision. We show experimentally that translation quality depends on the discovered neurons, and find that many of them capture common linguistic phenomena. Finally, we show how to control NMT translations in predictable ways, by modifying activations of individual neurons",
    "checked": null,
    "id": "c5489d244bfc1e9b0d8c94bf6dd774ee1aca2def",
    "semantic_title": "identifying and controlling important neurons in neural machine translation",
    "citation_count": 183,
    "authors": []
  },
  "https://openreview.net/forum?id=BJe-DsC5Fm": {
    "title": "signSGD via Zeroth-Order Oracle",
    "volume": "poster",
    "abstract": "In this paper, we design and analyze a new zeroth-order (ZO) stochastic optimization algorithm, ZO-signSGD, which enjoys dual advantages of gradient-free operations and signSGD. The latter requires only the sign information of gradient estimates but is able to achieve a comparable or even better convergence speed than SGD-type algorithms. Our study shows that ZO signSGD requires $\\sqrt{d}$ times more iterations than signSGD, leading to a convergence rate of $O(\\sqrt{d}/\\sqrt{T})$ under mild conditions, where $d$ is the number of optimization variables, and $T$ is the number of iterations. In addition, we analyze the effects of different types of gradient estimators on the convergence of ZO-signSGD, and propose two variants of ZO-signSGD that at least achieve $O(\\sqrt{d}/\\sqrt{T})$ convergence rate. On the application side we explore the connection between ZO-signSGD and black-box adversarial attacks in robust deep learning. Our empirical evaluations on image classification datasets MNIST and CIFAR-10 demonstrate the superior performance of ZO-signSGD on the generation of adversarial examples from black-box neural networks",
    "checked": null,
    "id": "3ae1544865a18d8649a5c4939f9eb17165b23bea",
    "semantic_title": "signsgd via zeroth-order oracle",
    "citation_count": 134,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgbwsAcYm": {
    "title": "DELTA: DEEP LEARNING TRANSFER USING FEATURE MAP WITH ATTENTION FOR CONVOLUTIONAL NETWORKS",
    "volume": "poster",
    "abstract": "Transfer learning through fine-tuning a pre-trained neural network with an extremely large dataset, such as ImageNet, can significantly accelerate training while the accuracy is frequently bottlenecked by the limited dataset size of the new target task. To solve the problem, some regularization methods, constraining the outer layer weights of the target network using the starting point as references (SPAR), have been studied. In this paper, we propose a novel regularized transfer learning framework DELTA, namely DEep Learning Transfer using Feature Map with Attention. Instead of constraining the weights of neural network, DELTA aims to preserve the outer layer outputs of the target network. Specifically, in addition to minimizing the empirical loss, DELTA intends to align the outer layer outputs of two networks, through constraining a subset of feature maps that are precisely selected by attention that has been learned in an supervised learning manner. We evaluate DELTA with the state-of-the-art algorithms, including L2 and L2-SP. The experiment results show that our proposed method outperforms these baselines with higher accuracy for new tasks",
    "checked": null,
    "id": "98627a34761bad5bd0582a7b03988de780b2d06b",
    "semantic_title": "delta: deep learning transfer using feature map with attention for convolutional networks",
    "citation_count": 172,
    "authors": []
  },
  "https://openreview.net/forum?id=B1GAUs0cKQ": {
    "title": "Variance Networks: When Expectation Does Not Meet Your Expectations",
    "volume": "poster",
    "abstract": "Ordinary stochastic neural networks mostly rely on the expected values of their weights to make predictions, whereas the induced noise is mostly used to capture the uncertainty, prevent overfitting and slightly boost the performance through test-time averaging. In this paper, we introduce variance layers, a different kind of stochastic layers. Each weight of a variance layer follows a zero-mean distribution and is only parameterized by its variance. It means that each object is represented by a zero-mean distribution in the space of the activations. We show that such layers can learn surprisingly well, can serve as an efficient exploration tool in reinforcement learning tasks and provide a decent defense against adversarial attacks. We also show that a number of conventional Bayesian neural networks naturally converge to such zero-mean posteriors. We observe that in these cases such zero-mean parameterization leads to a much better training objective than more flexible conventional parameterizations where the mean is being learned",
    "checked": null,
    "id": "58b8a5d5e46192c9bb0cf720cbe225f205ad1b3d",
    "semantic_title": "variance networks: when expectation does not meet your expectations",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=ryepUj0qtX": {
    "title": "Conditional Network Embeddings",
    "volume": "poster",
    "abstract": "Network Embeddings (NEs) map the nodes of a given network into $d$-dimensional Euclidean space $\\mathbb{R}^d$. Ideally, this mapping is such that 'similar' nodes are mapped onto nearby points, such that the NE can be used for purposes such as link prediction (if 'similar' means being 'more likely to be connected') or classification (if 'similar' means 'being more likely to have the same label'). In recent years various methods for NE have been introduced, all following a similar strategy: defining a notion of similarity between nodes (typically some distance measure within the network), a distance measure in the embedding space, and a loss function that penalizes large distances for similar nodes and small distances for dissimilar nodes. A difficulty faced by existing methods is that certain networks are fundamentally hard to embed due to their structural properties: (approximate) multipartiteness, certain degree distributions, assortativity, etc. To overcome this, we introduce a conceptual innovation to the NE literature and propose to create \\emph{Conditional Network Embeddings} (CNEs); embeddings that maximally add information with respect to given structural properties (e.g. node degrees, block densities, etc.). We use a simple Bayesian approach to achieve this, and propose a block stochastic gradient descent algorithm for fitting it efficiently. We demonstrate that CNEs are superior for link prediction and multi-label classification when compared to state-of-the-art methods, and this without adding significant mathematical or computational complexity. Finally, we illustrate the potential of CNE for network visualization",
    "checked": null,
    "id": "6625204c81fac0adba369d9ec924ac214e6eebc6",
    "semantic_title": "conditional network embeddings",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=SyMhLo0qKQ": {
    "title": "Distribution-Interpolation Trade off in Generative Models",
    "volume": "poster",
    "abstract": "We investigate the properties of multidimensional probability distributions in the context of latent space prior distributions of implicit generative models. Our work revolves around the phenomena arising while decoding linear interpolations between two random latent vectors -- regions of latent space in close proximity to the origin of the space are oversampled, which restricts the usability of linear interpolations as a tool to analyse the latent space. We show that the distribution mismatch can be eliminated completely by a proper choice of the latent probability distribution or using non-linear interpolations. We prove that there is a trade off between the interpolation being linear, and the latent distribution having even the most basic properties required for stable training, such as finite mean. We use the multidimensional Cauchy distribution as an example of the prior distribution, and also provide a general method of creating non-linear interpolations, that is easily applicable to a large family of commonly used latent distributions",
    "checked": null,
    "id": "3fda72294f9ca4f8afac856fbff26e83b0695e7a",
    "semantic_title": "distribution-interpolation trade off in generative models",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=rkzjUoAcFX": {
    "title": "Sample Efficient Adaptive Text-to-Speech",
    "volume": "poster",
    "abstract": "We present a meta-learning approach for adaptive text-to-speech (TTS) with few data. During training, we learn a multi-speaker model using a shared conditional WaveNet core and independent learned embeddings for each speaker. The aim of training is not to produce a neural network with fixed weights, which is then deployed as a TTS system. Instead, the aim is to produce a network that requires few data at deployment time to rapidly adapt to new speakers. We introduce and benchmark three strategies: (i) learning the speaker embedding while keeping the WaveNet core fixed, (ii) fine-tuning the entire architecture with stochastic gradient descent, and (iii) predicting the speaker embedding with a trained neural network encoder. The experiments show that these approaches are successful at adapting the multi-speaker neural network to new speakers, obtaining state-of-the-art results in both sample naturalness and voice similarity with merely a few minutes of audio data from new speakers",
    "checked": null,
    "id": "031c6baabd61f5b654ef4892f8f6ed737ec52511",
    "semantic_title": "sample efficient adaptive text-to-speech",
    "citation_count": 149,
    "authors": []
  },
  "https://openreview.net/forum?id=ByloIiCqYQ": {
    "title": "Maximal Divergence Sequential Autoencoder for Binary Software Vulnerability Detection",
    "volume": "poster",
    "abstract": "Due to the sharp increase in the severity of the threat imposed by software vulnerabilities, the detection of vulnerabilities in binary code has become an important concern in the software industry, such as the embedded systems industry, and in the field of computer security. However, most of the work in binary code vulnerability detection has relied on handcrafted features which are manually chosen by a select few, knowledgeable domain experts. In this paper, we attempt to alleviate this severe binary vulnerability detection bottleneck by leveraging recent advances in deep learning representations and propose the Maximal Divergence Sequential Auto-Encoder. In particular, latent codes representing vulnerable and non-vulnerable binaries are encouraged to be maximally divergent, while still being able to maintain crucial information from the original binaries. We conducted extensive experiments to compare and contrast our proposed methods with the baselines, and the results show that our proposed methods outperform the baselines in all performance measures of interest",
    "checked": null,
    "id": "89272d09aaf902e70de0d68e5e9c15d810c8f418",
    "semantic_title": "maximal divergence sequential autoencoder for binary software vulnerability detection",
    "citation_count": 53,
    "authors": []
  },
  "https://openreview.net/forum?id=BkN5UoAqF7": {
    "title": "Sample Efficient Imitation Learning for Continuous Control",
    "volume": "poster",
    "abstract": "The goal of imitation learning (IL) is to enable a learner to imitate expert behavior given expert demonstrations. Recently, generative adversarial imitation learning (GAIL) has shown significant progress on IL for complex continuous tasks. However, GAIL and its extensions require a large number of environment interactions during training. In real-world environments, the more an IL method requires the learner to interact with the environment for better imitation, the more training time it requires, and the more damage it causes to the environments and the learner itself. We believe that IL algorithms could be more applicable to real-world problems if the number of interactions could be reduced. In this paper, we propose a model-free IL algorithm for continuous control. Our algorithm is made up mainly three changes to the existing adversarial imitation learning (AIL) methods ‚Äì (a) adopting off-policy actor-critic (Off-PAC) algorithm to optimize the learner policy, (b) estimating the state-action value using off-policy samples without learning reward functions, and (c) representing the stochastic policy function so that its outputs are bounded. Experimental results show that our algorithm achieves competitive results with GAIL while significantly reducing the environment interactions",
    "checked": null,
    "id": "21946808f15dab6c73a76e89d1fa5869df515a3f",
    "semantic_title": "sample efficient imitation learning for continuous control",
    "citation_count": 58,
    "authors": []
  },
  "https://openreview.net/forum?id=ryE98iR5tm": {
    "title": "Practical lossless compression with latent variables using bits back coding",
    "volume": "poster",
    "abstract": "Deep latent variable models have seen recent success in many data domains. Lossless compression is an application of these models which, despite having the potential to be highly useful, has yet to be implemented in a practical manner. We present '`Bits Back with ANS' (BB-ANS), a scheme to perform lossless compression with latent variable models at a near optimal rate. We demonstrate this scheme by using it to compress the MNIST dataset with a variational auto-encoder model (VAE), achieving compression rates superior to standard methods with only a simple VAE. Given that the scheme is highly amenable to parallelization, we conclude that with a sufficiently high quality generative model this scheme could be used to achieve substantial improvements in compression rate with acceptable running time. We make our implementation available open source at https://github.com/bits-back/bits-back",
    "checked": null,
    "id": "c2ed34facd63d72e5d03ba13a6a3956ed6b2ac6c",
    "semantic_title": "practical lossless compression with latent variables using bits back coding",
    "citation_count": 142,
    "authors": []
  },
  "https://openreview.net/forum?id=HyxKIiAqYQ": {
    "title": "Context-adaptive Entropy Model for End-to-end Optimized Image Compression",
    "volume": "poster",
    "abstract": "We propose a context-adaptive entropy model for use in end-to-end optimized image compression. Our model exploits two types of contexts, bit-consuming contexts and bit-free contexts, distinguished based upon whether additional bit allocation is required. Based on these contexts, we allow the model to more accurately estimate the distribution of each latent representation with a more generalized form of the approximation models, which accordingly leads to an enhanced compression performance. Based on the experimental results, the proposed method outperforms the traditional image codecs, such as BPG and JPEG2000, as well as other previous artificial-neural-network (ANN) based approaches, in terms of the peak signal-to-noise ratio (PSNR) and multi-scale structural similarity (MS-SSIM) index. The test code is publicly available at https://github.com/JooyoungLeeETRI/CA_Entropy_Model",
    "checked": null,
    "id": "3b2409b9cc59b8f9b2d7baa200c1ac9e98b397da",
    "semantic_title": "context-adaptive entropy model for end-to-end optimized image compression",
    "citation_count": 396,
    "authors": []
  },
  "https://openreview.net/forum?id=ryM_IoAqYX": {
    "title": "Analysis of Quantized Models",
    "volume": "poster",
    "abstract": "Deep neural networks are usually huge, which significantly limits the deployment on low-end devices. In recent years, many weight-quantized models have been proposed. They have small storage and fast inference, but training can still be time-consuming. This can be improved with distributed learning. To reduce the high communication cost due to worker-server synchronization, recently gradient quantization has also been proposed to train deep networks with full-precision weights. In this paper, we theoretically study how the combination of both weight and gradient quantization affects convergence. We show that (i) weight-quantized models converge to an error related to the weight quantization resolution and weight dimension; (ii) quantizing gradients slows convergence by a factor related to the gradient quantization resolution and dimension; and (iii) clipping the gradient before quantization renders this factor dimension-free, thus allowing the use of fewer bits for gradient quantization. Empirical experiments confirm the theoretical convergence results, and demonstrate that quantized networks can speed up training and have comparable performance as full-precision networks",
    "checked": null,
    "id": "02dae4e3494fe8f1b6591d4697a4e584605a3f6b",
    "semantic_title": "analysis of quantized models",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=H1edIiA9KQ": {
    "title": "Generating Multiple Objects at Spatially Distinct Locations",
    "volume": "poster",
    "abstract": "Recent improvements to Generative Adversarial Networks (GANs) have made it possible to generate realistic images in high resolution based on natural language descriptions such as image captions. Furthermore, conditional GANs allow us to control the image generation process through labels or even natural language descriptions. However, fine-grained control of the image layout, i.e. where in the image specific objects should be located, is still difficult to achieve. This is especially true for images that should contain multiple distinct objects at different spatial locations. We introduce a new approach which allows us to control the location of arbitrarily many objects within an image by adding an object pathway to both the generator and the discriminator. Our approach does not need a detailed semantic layout but only bounding boxes and the respective labels of the desired objects are needed. The object pathway focuses solely on the individual objects and is iteratively applied at the locations specified by the bounding boxes. The global pathway focuses on the image background and the general image layout. We perform experiments on the Multi-MNIST, CLEVR, and the more complex MS-COCO data set. Our experiments show that through the use of the object pathway we can control object locations within images and can model complex scenes with multiple objects at various locations. We further show that the object pathway focuses on the individual objects and learns features relevant for these, while the global pathway focuses on global image characteristics and the image background",
    "checked": null,
    "id": "15170be53e9b6d63df11ac231bdf20d9f550afdc",
    "semantic_title": "generating multiple objects at spatially distinct locations",
    "citation_count": 103,
    "authors": []
  },
  "https://openreview.net/forum?id=rkzDIiA5YQ": {
    "title": "ANYTIME MINIBATCH: EXPLOITING STRAGGLERS IN ONLINE DISTRIBUTED OPTIMIZATION",
    "volume": "poster",
    "abstract": "Distributed optimization is vital in solving large-scale machine learning problems. A widely-shared feature of distributed optimization techniques is the requirement that all nodes complete their assigned tasks in each computational epoch before the system can proceed to the next epoch. In such settings, slow nodes, called stragglers, can greatly slow progress. To mitigate the impact of stragglers, we propose an online distributed optimization method called Anytime Minibatch. In this approach, all nodes are given a fixed time to compute the gradients of as many data samples as possible. The result is a variable per-node minibatch size. Workers then get a fixed communication time to average their minibatch gradients via several rounds of consensus, which are then used to update primal variables via dual averaging. Anytime Minibatch prevents stragglers from holding up the system without wasting the work that stragglers can complete. We present a convergence analysis and analyze the wall time performance. Our numerical results show that our approach is up to 1.5 times faster in Amazon EC2 and it is up to five times faster when there is greater variability in compute node performance",
    "checked": null,
    "id": "93a94c2c91da45bafe767a5d742011bb262f81b3",
    "semantic_title": "anytime minibatch: exploiting stragglers in online distributed optimization",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=H1fU8iAqKX": {
    "title": "A rotation-equivariant convolutional neural network model of primary visual cortex",
    "volume": "poster",
    "abstract": "Classical models describe primary visual cortex (V1) as a filter bank of orientation-selective linear-nonlinear (LN) or energy models, but these models fail to predict neural responses to natural stimuli accurately. Recent work shows that convolutional neural networks (CNNs) can be trained to predict V1 activity more accurately, but it remains unclear which features are extracted by V1 neurons beyond orientation selectivity and phase invariance. Here we work towards systematically studying V1 computations by categorizing neurons into groups that perform similar computations. We present a framework for identifying common features independent of individual neurons' orientation selectivity by using a rotation-equivariant convolutional neural network, which automatically extracts every feature at multiple different orientations. We fit this rotation-equivariant CNN to responses of a population of 6000 neurons to natural images recorded in mouse primary visual cortex using two-photon imaging. We show that our rotation-equivariant network outperforms a regular CNN with the same number of feature maps and reveals a number of common features, which are shared by many V1 neurons and are pooled sparsely to predict neural activity. Our findings are a first step towards a powerful new tool to study the nonlinear functional organization of visual cortex",
    "checked": null,
    "id": "70c73ea3e534a46b6b2c1b91449b9e17678337ef",
    "semantic_title": "a rotation-equivariant convolutional neural network model of primary visual cortex",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=ryfMLoCqtQ": {
    "title": "An analytic theory of generalization dynamics and transfer learning in deep linear networks",
    "volume": "poster",
    "abstract": "Much attention has been devoted recently to the generalization puzzle in deep learning: large, deep networks can generalize well, but existing theories bounding generalization error are exceedingly loose, and thus cannot explain this striking performance. Furthermore, a major hope is that knowledge may transfer across tasks, so that multi-task learning can improve generalization on individual tasks. However we lack analytic theories that can quantitatively predict how the degree of knowledge transfer depends on the relationship between the tasks. We develop an analytic theory of the nonlinear dynamics of generalization in deep linear networks, both within and across tasks. In particular, our theory provides analytic solutions to the training and testing error of deep networks as a function of training time, number of examples, network size and initialization, and the task structure and SNR. Our theory reveals that deep networks progressively learn the most important task structure first, so that generalization error at the early stopping time primarily depends on task structure and is independent of network size. This suggests any tight bound on generalization error must take into account task structure, and explains observations about real data being learned faster than random data. Intriguingly our theory also reveals the existence of a learning algorithm that proveably out-performs neural network training through gradient descent. Finally, for transfer learning, our theory reveals that knowledge transfer depends sensitively, but computably, on the SNRs and input feature alignments of pairs of tasks",
    "checked": null,
    "id": "a77dc75ab9d3477cab828f492af638e5c27b5f4a",
    "semantic_title": "an analytic theory of generalization dynamics and transfer learning in deep linear networks",
    "citation_count": 131,
    "authors": []
  },
  "https://openreview.net/forum?id=r1lWUoA9FQ": {
    "title": "Are adversarial examples inevitable?",
    "volume": "poster",
    "abstract": "A wide range of defenses have been proposed to harden neural networks against adversarial attacks. However, a pattern has emerged in which the majority of adversarial defenses are quickly broken by new attacks. Given the lack of success at generating robust defenses, we are led to ask a fundamental question: Are adversarial attacks inevitable? This paper analyzes adversarial examples from a theoretical perspective, and identifies fundamental bounds on the susceptibility of a classifier to adversarial attacks. We show that, for certain classes of problems, adversarial examples are inescapable. Using experiments, we explore the implications of theoretical guarantees for real-world problems and discuss how factors such as dimensionality and image complexity limit a classifier's robustness against adversarial examples",
    "checked": null,
    "id": "fd02c5b49bab02fb814c6999ebf161f3be377c75",
    "semantic_title": "are adversarial examples inevitable?",
    "citation_count": 282,
    "authors": []
  },
  "https://openreview.net/forum?id=BJeWUs05KQ": {
    "title": "Directed-Info GAIL: Learning Hierarchical Policies from Unsegmented Demonstrations using Directed Information",
    "volume": "poster",
    "abstract": "The use of imitation learning to learn a single policy for a complex task that has multiple modes or hierarchical structure can be challenging. In fact, previous work has shown that when the modes are known, learning separate policies for each mode or sub-task can greatly improve the performance of imitation learning. In this work, we discover the interaction between sub-tasks from their resulting state-action trajectory sequences using a directed graphical model. We propose a new algorithm based on the generative adversarial imitation learning framework which automatically learns sub-task policies from unsegmented demonstrations. Our approach maximizes the directed information flow in the graphical model between sub-task latent variables and their generated trajectories. We also show how our approach connects with the existing Options framework, which is commonly used to learn hierarchical policies",
    "checked": null,
    "id": "b43d8c8b25bc65cbf3097480e9000649c79b7a51",
    "semantic_title": "directed-info gail: learning hierarchical policies from unsegmented demonstrations using directed information",
    "citation_count": 68,
    "authors": []
  },
  "https://openreview.net/forum?id=BkzeUiRcY7": {
    "title": "M^3RL: Mind-aware Multi-agent Management Reinforcement Learning",
    "volume": "poster",
    "abstract": "Most of the prior work on multi-agent reinforcement learning (MARL) achieves optimal collaboration by directly learning a policy for each agent to maximize a common reward. In this paper, we aim to address this from a different angle. In particular, we consider scenarios where there are self-interested agents (i.e., worker agents) which have their own minds (preferences, intentions, skills, etc.) and can not be dictated to perform tasks they do not want to do. For achieving optimal coordination among these agents, we train a super agent (i.e., the manager) to manage them by first inferring their minds based on both current and past observations and then initiating contracts to assign suitable tasks to workers and promise to reward them with corresponding bonuses so that they will agree to work together. The objective of the manager is to maximize the overall productivity as well as minimize payments made to the workers for ad-hoc worker teaming. To train the manager, we propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL), which consists of agent modeling and policy learning. We have evaluated our approach in two environments, Resource Collection and Crafting, to simulate multi-agent management problems with various task settings and multiple designs for the worker agents. The experimental results have validated the effectiveness of our approach in modeling worker agents' minds online, and in achieving optimal ad-hoc teaming with good generalization and fast adaptation",
    "checked": null,
    "id": "d341764a2020dcc3389f27d6508679e0c3cab486",
    "semantic_title": "m^3rl: mind-aware multi-agent management reinforcement learning",
    "citation_count": 53,
    "authors": []
  },
  "https://openreview.net/forum?id=ryggIs0cYQ": {
    "title": "Differentiable Learning-to-Normalize via Switchable Normalization",
    "volume": "poster",
    "abstract": "We address a learning-to-normalize problem by proposing Switchable Normalization (SN), which learns to select different normalizers for different normalization layers of a deep neural network. SN employs three distinct scopes to compute statistics (means and variances) including a channel, a layer, and a minibatch. SN switches between them by learning their importance weights in an end-to-end manner. It has several good properties. First, it adapts to various network architectures and tasks (see Fig.1). Second, it is robust to a wide range of batch sizes, maintaining high performance even when small minibatch is presented (e.g. 2 images/GPU). Third, SN does not have sensitive hyper-parameter, unlike group normalization that searches the number of groups as a hyper-parameter. Without bells and whistles, SN outperforms its counterparts on various challenging benchmarks, such as ImageNet, COCO, CityScapes, ADE20K, and Kinetics. Analyses of SN are also presented. We hope SN will help ease the usage and understand the normalization techniques in deep learning. The code of SN will be released",
    "checked": null,
    "id": "1c42f8ab39e22225ffd3222baeba4863435220a0",
    "semantic_title": "differentiable learning-to-normalize via switchable normalization",
    "citation_count": 177,
    "authors": []
  },
  "https://openreview.net/forum?id=SJxTroR9F7": {
    "title": "Supervised Policy Update for Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "We propose a new sample-efficient methodology, called Supervised Policy Update (SPU), for deep reinforcement learning. Starting with data generated by the current policy, SPU formulates and solves a constrained optimization problem in the non-parameterized proximal policy space. Using supervised regression, it then converts the optimal non-parameterized policy to a parameterized policy, from which it draws new samples. The methodology is general in that it applies to both discrete and continuous action spaces, and can handle a wide variety of proximity constraints for the non-parameterized optimization problem. We show how the Natural Policy Gradient and Trust Region Policy Optimization (NPG/TRPO) problems, and the Proximal Policy Optimization (PPO) problem can be addressed by this methodology. The SPU implementation is much simpler than TRPO. In terms of sample efficiency, our extensive experiments show SPU outperforms TRPO in Mujoco simulated robotic tasks and outperforms PPO in Atari video game tasks",
    "checked": null,
    "id": "c4ecab02877c91f06340877640d9274e1f85a7a1",
    "semantic_title": "supervised policy update for deep reinforcement learning",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=Hyx6Bi0qYm": {
    "title": "Adversarial Domain Adaptation for Stable Brain-Machine Interfaces",
    "volume": "poster",
    "abstract": "Brain-Machine Interfaces (BMIs) have recently emerged as a clinically viable option to restore voluntary movements after paralysis. These devices are based on the ability to extract information about movement intent from neural signals recorded using multi-electrode arrays chronically implanted in the motor cortices of the brain. However, the inherent loss and turnover of recorded neurons requires repeated recalibrations of the interface, which can potentially alter the day-to-day user experience. The resulting need for continued user adaptation interferes with the natural, subconscious use of the BMI. Here, we introduce a new computational approach that decodes movement intent from a low-dimensional latent representation of the neural data. We implement various domain adaptation methods to stabilize the interface over significantly long times. This includes Canonical Correlation Analysis used to align the latent variables across days; this method requires prior point-to-point correspondence of the time series across domains. Alternatively, we match the empirical probability distributions of the latent variables across days through the minimization of their Kullback-Leibler divergence. These two methods provide a significant and comparable improvement in the performance of the interface. However, implementation of an Adversarial Domain Adaptation Network trained to match the empirical probability distribution of the residuals of the reconstructed neural signals outperforms the two methods based on latent variables, while requiring remarkably few data points to solve the domain adaptation problem",
    "checked": null,
    "id": "301201dc73384b9dbc45a02e69db95210f93e6b8",
    "semantic_title": "adversarial domain adaptation for stable brain-machine interfaces",
    "citation_count": 75,
    "authors": []
  },
  "https://openreview.net/forum?id=SJe3HiC5KX": {
    "title": "LEARNING FACTORIZED REPRESENTATIONS FOR OPEN-SET DOMAIN ADAPTATION",
    "volume": "poster",
    "abstract": "Domain adaptation for visual recognition has undergone great progress in the past few years. Nevertheless, most existing methods work in the so-called closed-set scenario, assuming that the classes depicted by the target images are exactly the same as those of the source domain. In this paper, we tackle the more challenging, yet more realistic case of open-set domain adaptation, where new, unknown classes can be present in the target data. While, in the unsupervised scenario, one cannot expect to be able to identify each specific new class, we aim to automatically detect which samples belong to these new classes and discard them from the recognition process. To this end, we rely on the intuition that the source and target samples depicting the known classes can be generated by a shared subspace, whereas the target samples from unknown classes come from a different, private subspace. We therefore introduce a framework that factorizes the data into shared and private parts, while encouraging the shared representation to be discriminative. Our experiments on standard benchmarks evidence that our approach significantly outperforms the state-of-the-art in open-set domain adaptation",
    "checked": null,
    "id": "211c42a6d949d60fa7eaaf5a7b7b28ebef36da9f",
    "semantic_title": "learning factorized representations for open-set domain adaptation",
    "citation_count": 53,
    "authors": []
  },
  "https://openreview.net/forum?id=H1xsSjC9Ym": {
    "title": "Learning to Understand Goal Specifications by Modelling Reward",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "4a4b71ff918ca8eeffa5dfe66be2db7fcc1291da",
    "semantic_title": "learning to understand goal specifications by modelling reward",
    "citation_count": 159,
    "authors": []
  },
  "https://openreview.net/forum?id=H1goBoR9F7": {
    "title": "Dynamic Sparse Graph for Efficient Deep Learning",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "ba6985ee462ed8b09385a924aded5a45f74e7a59",
    "semantic_title": "dynamic sparse graph for efficient deep learning",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=SkEqro0ctQ": {
    "title": "Hierarchical interpretations for neural network predictions",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "b3cedde36a6841b43162fc406b688e51bec68d36",
    "semantic_title": "hierarchical interpretations for neural network predictions",
    "citation_count": 145,
    "authors": []
  },
  "https://openreview.net/forum?id=BkG5SjR5YQ": {
    "title": "Post Selection Inference with Incomplete Maximum Mean Discrepancy Estimator",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "0d93de3e89ea7ed04fe44be572472a244b97c678",
    "semantic_title": "post selection inference with incomplete maximum mean discrepancy estimator",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=BygqBiRcFQ": {
    "title": "Diffusion Scattering Transforms on Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "080b8de51221395038feb656f33783b65fa48434",
    "semantic_title": "diffusion scattering transforms on graphs",
    "citation_count": 103,
    "authors": []
  },
  "https://openreview.net/forum?id=Bye5SiAqKX": {
    "title": "Preconditioner on Matrix Lie Group for SGD",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "448491374325bd2c3f8cbc381b67b7742329144b",
    "semantic_title": "preconditioner on matrix lie group for sgd",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=ryeYHi0ctQ": {
    "title": "DPSNet: End-to-end Deep Plane Sweep Stereo",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "01dd15f339e8da4eb9a23e47983c1b7c480c7196",
    "semantic_title": "dpsnet: end-to-end deep plane sweep stereo",
    "citation_count": 227,
    "authors": []
  },
  "https://openreview.net/forum?id=S1eYHoC5FX": {
    "title": "DARTS: Differentiable Architecture Search",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "c1f457e31b611da727f9aef76c283a18157dfa83",
    "semantic_title": "darts: differentiable architecture search",
    "citation_count": 4347,
    "authors": []
  },
  "https://openreview.net/forum?id=Syx_Ss05tm": {
    "title": "Adversarial Reprogramming of Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "c8ed4a46e6aa565ea757e0ff5b7c160419699026",
    "semantic_title": "adversarial reprogramming of neural networks",
    "citation_count": 183,
    "authors": []
  },
  "https://openreview.net/forum?id=S1eOHo09KX": {
    "title": "Opportunistic Learning: Budgeted Cost-Sensitive Learning from Data Streams",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "ac87ce247e9e1d5cf9c394e7cf2ad7a239542d89",
    "semantic_title": "opportunistic learning: budgeted cost-sensitive learning from data streams",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=BJg_roAcK7": {
    "title": "INVASE: Instance-wise Variable Selection using Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "0e5793fc79f155a619a50aab2be4f577e87e3f99",
    "semantic_title": "invase: instance-wise variable selection using neural networks",
    "citation_count": 166,
    "authors": []
  },
  "https://openreview.net/forum?id=S1erHoR5t7": {
    "title": "The relativistic discriminator: a key element missing from standard GAN",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "dd2ebc42a1a4491b4179dec0ca8686d5c66f6bfe",
    "semantic_title": "the relativistic discriminator: a key element missing from standard gan",
    "citation_count": 974,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgBHoCqYX": {
    "title": "A Kernel Random Matrix-Based Approach for Sparse PCA",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "c2b13b979556d6fa1e7c0adc7cf37911e38cebe8",
    "semantic_title": "a kernel random matrix-based approach for sparse pca",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=rke4HiAcY7": {
    "title": "Caveats for information bottleneck in deterministic scenarios",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "7c789f89fd2a53d281cb67506a78a94f5c932333",
    "semantic_title": "caveats for information bottleneck in deterministic scenarios",
    "citation_count": 83,
    "authors": []
  },
  "https://openreview.net/forum?id=SJeXSo09FQ": {
    "title": "Learning Localized Generative Models for 3D Point Clouds via Graph Convolution",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "32b918246037976ba5f8363104ec042f56db42d6",
    "semantic_title": "learning localized generative models for 3d point clouds via graph convolution",
    "citation_count": 184,
    "authors": []
  },
  "https://openreview.net/forum?id=S1fQSiCcYm": {
    "title": "Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "3517b9824def42f3c723c6c63eda7ade12d25538",
    "semantic_title": "understanding and improving interpolation in autoencoders via an adversarial regularizer",
    "citation_count": 264,
    "authors": []
  },
  "https://openreview.net/forum?id=HJlmHoR5tQ": {
    "title": "Adversarial Imitation via Variational Inverse Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "c58a10a4923f59d02bd2dc2f7b9b13e7198d3383",
    "semantic_title": "adversarial imitation via variational inverse reinforcement learning",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=HyeGBj09Fm": {
    "title": "Generating Liquid Simulations with Deformation-aware Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "73a7e8165e83c5cfe2150c03b927c55102b2e815",
    "semantic_title": "generating liquid simulations with deformation-aware neural networks",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=ByxGSsR9FQ": {
    "title": "L2-Nonexpansive Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "ef2ec69e7c94b4194ba01719ac76d4595e6b4bdf",
    "semantic_title": "l2-nonexpansive neural networks",
    "citation_count": 74,
    "authors": []
  },
  "https://openreview.net/forum?id=ryGgSsAcFQ": {
    "title": "Deep, Skinny Neural Networks are not Universal Approximators",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "6b7c16120ef3324bdca4b8433fb1f89c761e4dfc",
    "semantic_title": "deep, skinny neural networks are not universal approximators",
    "citation_count": 66,
    "authors": []
  },
  "https://openreview.net/forum?id=ryGkSo0qYm": {
    "title": "Large Scale Graph Learning From Smooth Signals",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "6a432ad86a4c3c37005f8e46212ec4e0abea5482",
    "semantic_title": "large scale graph learning from smooth signals",
    "citation_count": 82,
    "authors": []
  },
  "https://openreview.net/forum?id=H1gTEj09FX": {
    "title": "RotDCF: Decomposition of Convolutional Filters for Rotation-Equivariant Deep Networks",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "8ccde9d80706a59e606f6e6d48d4260b60ccc736",
    "semantic_title": "rotdcf: decomposition of convolutional filters for rotation-equivariant deep networks",
    "citation_count": 43,
    "authors": []
  },
  "https://openreview.net/forum?id=rkxaNjA9Ym": {
    "title": "Per-Tensor Fixed-Point Quantization of the Back-Propagation Algorithm",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "24c338254c60e9a53443af4d4d99b8333a2a0e81",
    "semantic_title": "per-tensor fixed-point quantization of the back-propagation algorithm",
    "citation_count": 43,
    "authors": []
  },
  "https://openreview.net/forum?id=Skh4jRcKQ": {
    "title": "Understanding Straight-Through Estimator in Training Activation Quantized Neural Nets",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "cf0671ec7da36af49699de81bee05e9549140db2",
    "semantic_title": "understanding straight-through estimator in training activation quantized neural nets",
    "citation_count": 314,
    "authors": []
  },
  "https://openreview.net/forum?id=B1G5ViAqFm": {
    "title": "Convolutional Neural Networks on Non-uniform Geometrical Signals Using Euclidean Spectral Transformation",
    "volume": "poster",
    "abstract": "Convolutional Neural Networks (CNN) have been successful in processing data signals that are uniformly sampled in the spatial domain (e.g., images). However, most data signals do not natively exist on a grid, and in the process of being sampled onto a uniform physical grid suffer significant aliasing error and information loss. Moreover, signals can exist in different topological structures as, for example, points, lines, surfaces and volumes. It has been challenging to analyze signals with mixed topologies (for example, point cloud with surface mesh). To this end, we develop mathematical formulations for Non-Uniform Fourier Transforms (NUFT) to directly, and optimally, sample nonuniform data signals of different topologies defined on a simplex mesh into the spectral domain with no spatial sampling error. The spectral transform is performed in the Euclidean space, which removes the translation ambiguity from works on the graph spectrum. Our representation has four distinct advantages: (1) the process causes no spatial sampling error during initial sampling, (2) the generality of this approach provides a unified framework for using CNNs to analyze signals of mixed topologies, (3) it allows us to leverage state-of-the-art backbone CNN architectures for effective learning without having to design a particular architecture for a particular data structure in an ad-hoc fashion, and (4) the representation allows weighted meshes where each element has a different weight (i.e., texture) indicating local properties. We achieve good results on-par with state-of-the-art for 3D shape retrieval task, and new state-of-the-art for point cloud to surface reconstruction task",
    "checked": null,
    "id": "c7ae7cb97e954b33878c893ed237886c2bfc9e7d",
    "semantic_title": "convolutional neural networks on non-uniform geometrical signals using euclidean spectral transformation",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=BJfIVjAcKm": {
    "title": "Training for Faster Adversarial Robustness Verification via Inducing ReLU Stability",
    "volume": "poster",
    "abstract": "We explore the concept of co-design in the context of neural network verification. Specifically, we aim to train deep neural networks that not only are robust to adversarial perturbations but also whose robustness can be verified more easily. To this end, we identify two properties of network models - weight sparsity and so-called ReLU stability - that turn out to significantly impact the complexity of the corresponding verification task. We demonstrate that improving weight sparsity alone already enables us to turn computationally intractable verification problems into tractable ones. Then, improving ReLU stability leads to an additional 4-13x speedup in verification times. An important feature of our methodology is its \"universality,\" in the sense that it can be used with a broad range of training procedures and verification approaches",
    "checked": null,
    "id": "de49430578bb3f8de3e610423255662c45f17610",
    "semantic_title": "training for faster adversarial robustness verification via inducing relu stability",
    "citation_count": 200,
    "authors": []
  }
}