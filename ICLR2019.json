{
  "https://openreview.net/forum?id=B1gabhRcYX": {
    "title": "BA-Net: Dense Bundle Adjustment Networks",
    "volume": "oral",
    "abstract": "This paper introduces a network architecture to solve the structure-from-motion (SfM) problem via feature-metric bundle adjustment (BA), which explicitly enforces multi-view geometry constraints in the form of feature-metric error. The whole pipeline is differentiable, so that the network can learn suitable features that make the BA problem more tractable. Furthermore, this work introduces a novel depth parameterization to recover dense per-pixel depth. The network first generates several basis depth maps according to the input image, and optimizes the final depth as a linear combination of these basis depth maps via feature-metric BA. The basis depth maps generator is also learned via end-to-end training. The whole system nicely combines domain knowledge (i.e. hard-coded multi-view geometry constraints) and deep learning (i.e. feature learning and basis depth maps learning) to address the challenging dense SfM problem. Experiments on large scale real data prove the success of the proposed method",
    "checked": false,
    "id": "4ba326228728c7ea03a4de6778d0b428b035d5cd",
    "semantic_title": "ba-net: dense bundle adjustment network",
    "citation_count": 296,
    "authors": []
  },
  "https://openreview.net/forum?id=HygBZnRctX": {
    "title": "Transferring Knowledge across Learning Processes",
    "volume": "oral",
    "abstract": "In complex transfer learning scenarios new tasks might not be tightly linked to previous tasks. Approaches that transfer information contained only in the final parameters of a source model will therefore struggle. Instead, transfer learning at at higher level of abstraction is needed. We propose Leap, a framework that achieves this by transferring knowledge across learning processes. We associate each task with a manifold on which the training process travels from initialization to final parameters and construct a meta-learning objective that minimizes the expected length of this path. Our framework leverages only information obtained during training and can be computed on the fly at negligible cost. We demonstrate that our framework outperforms competing methods, both in meta-learning and transfer learning, on a set of computer vision tasks. Finally, we demonstrate that Leap can transfer knowledge across learning processes in demanding reinforcement learning environments (Atari) that involve millions of gradient steps",
    "checked": true,
    "id": "0c37a1feac9a09a2fa5554cb7ff735d4bcd6ed5b",
    "semantic_title": "transferring knowledge across learning processes",
    "citation_count": 64,
    "authors": []
  },
  "https://openreview.net/forum?id=rJl-b3RcF7": {
    "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks",
    "volume": "oral",
    "abstract": "Neural network pruning techniques can reduce the parameter counts of trained networks by over 90%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the \"lottery ticket hypothesis:\" dense, randomly-initialized, feed-forward networks contain subnetworks (\"winning tickets\") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy",
    "checked": true,
    "id": "21937ecd9d66567184b83eca3d3e09eb4e6fbd60",
    "semantic_title": "the lottery ticket hypothesis: finding sparse, trainable neural networks",
    "citation_count": 3563,
    "authors": []
  },
  "https://openreview.net/forum?id=SkVhlh09tX": {
    "title": "Pay Less Attention with Lightweight and Dynamic Convolutions",
    "volume": "oral",
    "abstract": "Self-attention is a useful mechanism to build generative models for language and images. It determines the importance of context elements by comparing each element to the current time step. In this paper, we show that a very lightweight convolution can perform competitively to the best reported self-attention results. Next, we introduce dynamic convolutions which are simpler and more efficient than self-attention. We predict separate convolution kernels based solely on the current time-step in order to determine the importance of context elements. The number of operations required by this approach scales linearly in the input length, whereas self-attention is quadratic. Experiments on large-scale machine translation, language modeling and abstractive summarization show that dynamic convolutions improve over strong self-attention models. On the WMT'14 English-German test set dynamic convolutions achieve a new state of the art of 29.7 BLEU",
    "checked": true,
    "id": "fea820b7d953d32069e189af2961c28fd213470b",
    "semantic_title": "pay less attention with lightweight and dynamic convolutions",
    "citation_count": 612,
    "authors": []
  },
  "https://openreview.net/forum?id=S1x4ghC9tQ": {
    "title": "Temporal Difference Variational Auto-Encoder",
    "volume": "oral",
    "abstract": "To act and plan in complex environments, we posit that agents should have a mental simulator of the world with three characteristics: (a) it should build an abstract state representing the condition of the world; (b) it should form a belief which represents uncertainty on the world; (c) it should go beyond simple step-by-step simulation, and exhibit temporal abstraction. Motivated by the absence of a model satisfying all these requirements, we propose TD-VAE, a generative sequence model that learns representations containing explicit beliefs about states several steps into the future, and that can be rolled out directly without single-step transitions. TD-VAE is trained on pairs of temporally separated time points, using an analogue of temporal difference learning used in reinforcement learning",
    "checked": true,
    "id": "9d671a4de50b98c3f00623ee597e37c9f00ba0cc",
    "semantic_title": "temporal difference variational auto-encoder",
    "citation_count": 128,
    "authors": []
  },
  "https://openreview.net/forum?id=rJgMlhRctm": {
    "title": "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision",
    "volume": "oral",
    "abstract": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval",
    "checked": true,
    "id": "50f76736c3090c6effac25400e5e40cc0b7b5ad9",
    "semantic_title": "the neuro-symbolic concept learner: interpreting scenes, words, and sentences from natural supervision",
    "citation_count": 716,
    "authors": []
  },
  "https://openreview.net/forum?id=Byg3y3C9Km": {
    "title": "Learning Protein Structure with a Differentiable Simulator",
    "volume": "oral",
    "abstract": "The Boltzmann distribution is a natural model for many systems, from brains to materials and biomolecules, but is often of limited utility for fitting data because Monte Carlo algorithms are unable to simulate it in available time. This gap between the expressive capabilities and sampling practicalities of energy-based models is exemplified by the protein folding problem, since energy landscapes underlie contemporary knowledge of protein biophysics but computer simulations are challenged to fold all but the smallest proteins from first principles. In this work we aim to bridge the gap between the expressive capacity of energy functions and the practical capabilities of their simulators by using an unrolled Monte Carlo simulation as a model for data. We compose a neural energy function with a novel and efficient simulator based on Langevin dynamics to build an end-to-end-differentiable model of atomic protein structure given amino acid sequence information. We introduce techniques for stabilizing backpropagation under long roll-outs and demonstrate the model's capacity to make multimodal predictions and to, in some cases, generalize to unobserved protein fold types when trained on a large corpus of protein structures",
    "checked": true,
    "id": "5909909bd3f6d6d15d5c8d9a64c8031cddf604db",
    "semantic_title": "learning protein structure with a differentiable simulator",
    "citation_count": 148,
    "authors": []
  },
  "https://openreview.net/forum?id=rJxgknCcK7": {
    "title": "FFJORD: Free-Form Continuous Dynamics for Scalable Reversible Generative Models",
    "volume": "oral",
    "abstract": "A promising class of generative models maps points from a simple distribution to a complex distribution through an invertible neural network. Likelihood-based training of these models requires restricting their architectures to allow cheap computation of Jacobian determinants. Alternatively, the Jacobian trace can be used if the transformation is specified by an ordinary differential equation. In this paper, we use Hutchinson's trace estimator to give a scalable unbiased estimate of the log-density. The result is a continuous-time invertible generative model with unbiased density estimation and one-pass sampling, while allowing unrestricted neural network architectures. We demonstrate our approach on high-dimensional density estimation, image generation, and variational inference, achieving the state-of-the-art among exact likelihood methods with efficient sampling",
    "checked": true,
    "id": "8afa6dd9f9ac46462a1fb70a757c4ae1cd45bbf6",
    "semantic_title": "ffjord: free-form continuous dynamics for scalable reversible generative models",
    "citation_count": 899,
    "authors": []
  },
  "https://openreview.net/forum?id=r1lYRjC9F7": {
    "title": "Enabling Factorized Piano Music Modeling and Generation with the MAESTRO Dataset",
    "volume": "oral",
    "abstract": "Generating musical audio directly with neural networks is notoriously difficult because it requires coherently modeling structure at many different timescales. Fortunately, most music is also highly structured and can be represented as discrete note events played on musical instruments. Herein, we show that by using notes as an intermediate representation, we can train a suite of models capable of transcribing, composing, and synthesizing audio waveforms with coherent musical structure on timescales spanning six orders of magnitude (~0.1 ms to ~100 s), a process we call Wave2Midi2Wave. This large advance in the state of the art is enabled by our release of the new MAESTRO (MIDI and Audio Edited for Synchronous TRacks and Organization) dataset, composed of over 172 hours of virtuosic piano performances captured with fine alignment (~3 ms) between note labels and audio waveforms. The networks and the dataset together present a promising approach toward creating new expressive and interpretable neural models of music",
    "checked": true,
    "id": "2603a68b4503ba949c91c7e00cd342624b4aae2f",
    "semantic_title": "enabling factorized piano music modeling and generation with the maestro dataset",
    "citation_count": 467,
    "authors": []
  },
  "https://openreview.net/forum?id=ryGs6iA5Km": {
    "title": "How Powerful are Graph Neural Networks?",
    "volume": "oral",
    "abstract": "Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance",
    "checked": true,
    "id": "62ed9bf1d83c8db1f9cbf92ea2f57ea90ef683d9",
    "semantic_title": "how powerful are graph neural networks?",
    "citation_count": 7902,
    "authors": []
  },
  "https://openreview.net/forum?id=HylzTiC5Km": {
    "title": "GENERATING HIGH FIDELITY IMAGES WITH SUBSCALE PIXEL NETWORKS AND MULTIDIMENSIONAL UPSCALING",
    "volume": "oral",
    "abstract": "The unconditional generation of high fidelity images is a longstanding benchmark for testing the performance of image decoders. Autoregressive image models have been able to generate small images unconditionally, but the extension of these methods to large images where fidelity can be more readily assessed has remained an open problem. Among the major challenges are the capacity to encode the vast previous context and the sheer difficulty of learning a distribution that preserves both global semantic coherence and exactness of detail. To address the former challenge, we propose the Subscale Pixel Network (SPN), a conditional decoder architecture that generates an image as a sequence of image slices of equal size. The SPN compactly captures image-wide spatial dependencies and requires a fraction of the memory and the computation. To address the latter challenge, we propose to use multidimensional upscaling to grow an image in both size and depth via intermediate stages corresponding to distinct SPNs. We evaluate SPNs on the unconditional generation of CelebAHQ of size 256 and of ImageNet from size 32 to 128. We achieve state-of-the-art likelihood results in multiple settings, set up new benchmark results in previously unexplored settings and are able to generate very high fidelity large scale samples on the basis of both datasets",
    "checked": true,
    "id": "e8fd01d3a2ae47827cc8e008d658bca830d99415",
    "semantic_title": "generating high fidelity images with subscale pixel networks and multidimensional upscaling",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S1xq3oR5tQ": {
    "title": "A Unified Theory of Early Visual Representations from Retina to Cortex through Anatomically Constrained Deep CNNs",
    "volume": "oral",
    "abstract": "The vertebrate visual system is hierarchically organized to process visual information in successive stages. Neural representations vary drastically across the first stages of visual processing: at the output of the retina, ganglion cell receptive fields (RFs) exhibit a clear antagonistic center-surround structure, whereas in the primary visual cortex (V1), typical RFs are sharply tuned to a precise orientation. There is currently no unified theory explaining these differences in representations across layers. Here, using a deep convolutional neural network trained on image recognition as a model of the visual system, we show that such differences in representation can emerge as a direct consequence of different neural resource constraints on the retinal and cortical networks, and for the first time we find a single model from which both geometries spontaneously emerge at the appropriate stages of visual processing. The key constraint is a reduced number of neurons at the retinal output, consistent with the anatomy of the optic nerve as a stringent bottleneck. Second, we find that, for simple downstream cortical networks, visual representations at the retinal output emerge as nonlinear and lossy feature detectors, whereas they emerge as linear and faithful encoders of the visual scene for more complex cortical networks. This result predicts that the retinas of small vertebrates (e.g. salamander, frog) should perform sophisticated nonlinear computations, extracting features directly relevant to behavior, whereas retinas of large animals such as primates should mostly encode the visual scene linearly and respond to a much broader range of stimuli. These predictions could reconcile the two seemingly incompatible views of the retina as either performing feature extraction or efficient coding of natural scenes, by suggesting that all vertebrates lie on a spectrum between these two objectives, depending on the degree of neural resources allocated to their visual system",
    "checked": true,
    "id": "ad0466d3f1c128691416bab7244ace11a6716c7d",
    "semantic_title": "a unified theory of early visual representations from retina to cortex through anatomically constrained deep cnns",
    "citation_count": 73,
    "authors": []
  },
  "https://openreview.net/forum?id=Bklr3j0cKX": {
    "title": "Learning deep representations by mutual information estimation and maximization",
    "volume": "oral",
    "abstract": "This work investigates unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality in the input into the objective can significantly improve a representation's suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning methods and compares favorably with fully-supervised learning on several classification tasks in with some standard architectures. DIM opens new avenues for unsupervised learning of representations and is an important step towards flexible formulations of representation learning objectives for specific end-goals",
    "checked": true,
    "id": "af3825437b627db1a99f946f7aa773ba8b03befd",
    "semantic_title": "learning deep representations by mutual information estimation and maximization",
    "citation_count": 2702,
    "authors": []
  },
  "https://openreview.net/forum?id=rJEjjoR9K7": {
    "title": "Learning Robust Representations by Projecting Superficial Statistics Out",
    "volume": "oral",
    "abstract": "Despite impressive performance as evaluated on i.i.d. holdout data, deep neural networks depend heavily on superficial statistics of the training data and are liable to break under distribution shift. For example, subtle changes to the background or texture of an image can break a seemingly powerful classifier. Building on previous work on domain generalization, we hope to produce a classifier that will generalize to previously unseen domains, even when domain identifiers are not available during training. This setting is challenging because the model may extract many distribution-specific (superficial) signals together with distribution-agnostic (semantic) signals. To overcome this challenge, we incorporate the gray-level co-occurrence matrix (GLCM) to extract patterns that our prior knowledge suggests are superficial: they are sensitive to the texture but unable to capture the gestalt of an image. Then we introduce two techniques for improving our networks' out-of-sample performance. The first method is built on the reverse gradient method that pushes our model to learn representations from which the GLCM representation is not predictable. The second method is built on the independence introduced by projecting the model's representation onto the subspace orthogonal to GLCM representation's. We test our method on the battery of standard domain generalization data sets and, interestingly, achieve comparable or better performance as compared to other domain generalization methods that explicitly require samples from the target distribution for training",
    "checked": true,
    "id": "96b32b204a62777bef66eea595de2c47b4e9d6e9",
    "semantic_title": "learning robust representations by projecting superficial statistics out",
    "citation_count": 242,
    "authors": []
  },
  "https://openreview.net/forum?id=HkNDsiC9KQ": {
    "title": "Meta-Learning Update Rules for Unsupervised Representation Learning",
    "volume": "oral",
    "abstract": "A major goal of unsupervised learning is to discover data representations that are useful for subsequent tasks, without access to supervised labels during training. Typically, this involves minimizing a surrogate objective, such as the negative log likelihood of a generative model, with the hope that representations useful for subsequent tasks will arise as a side effect. In this work, we propose instead to directly target later desired tasks by meta-learning an unsupervised learning rule which leads to representations useful for those tasks. Specifically, we target semi-supervised classification performance, and we meta-learn an algorithm -- an unsupervised weight update rule -- that produces representations useful for this task. Additionally, we constrain our unsupervised update rule to a be a biologically-motivated, neuron-local function, which enables it to generalize to different neural network architectures, datasets, and data modalities. We show that the meta-learned update rule produces useful features and sometimes outperforms existing unsupervised learning techniques. We further show that the meta-learned unsupervised update rule generalizes to train networks with different widths, depths, and nonlinearities. It also generalizes to train on data with randomly permuted input dimensions and even generalizes from image datasets to a text task",
    "checked": true,
    "id": "c6509a450bdda7ca5b8567103dfe9671dbf3b567",
    "semantic_title": "meta-learning update rules for unsupervised representation learning",
    "citation_count": 124,
    "authors": []
  },
  "https://openreview.net/forum?id=B1l6qiR5F7": {
    "title": "Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks",
    "volume": "oral",
    "abstract": "Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. While the standard LSTM architecture allows different neurons to track information at different time scales, it does not have an explicit bias towards modeling a hierarchy of constituents. This paper proposes to add such inductive bias by ordering the neurons; a vector of master input and forget gates ensures that when a given neuron is updated, all the neurons that follow it in the ordering are also updated. Our novel recurrent architecture, ordered neurons LSTM (ON-LSTM), achieves good performance on four different tasks: language modeling, unsupervised parsing, targeted syntactic evaluation, and logical inference",
    "checked": true,
    "id": "15d6f3d815d0ff176fafb14a3f46e5723ebac723",
    "semantic_title": "ordered neurons: integrating tree structures into recurrent neural networks",
    "citation_count": 329,
    "authors": []
  },
  "https://openreview.net/forum?id=Bygh9j09KX": {
    "title": "ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness",
    "volume": "oral",
    "abstract": "Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies suggest a more important role of image textures. We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict. We show that ImageNet-trained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies. We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on 'Stylized-ImageNet', a stylized version of ImageNet. This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation",
    "checked": true,
    "id": "0f50b7483f1b200ebf88c4dd7698de986399a0f3",
    "semantic_title": "imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness",
    "citation_count": 2723,
    "authors": []
  },
  "https://openreview.net/forum?id=B1xsqj09Fm": {
    "title": "Large Scale GAN Training for High Fidelity Natural Image Synthesis",
    "volume": "oral",
    "abstract": "Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple \"truncation trick\", allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.3 and Frechet Inception Distance (FID) of 9.6, improving over the previous best IS of 52.52 and FID of 18.65",
    "checked": true,
    "id": "22aab110058ebbd198edb1f1e7b4f69fb13c0613",
    "semantic_title": "large scale gan training for high fidelity natural image synthesis",
    "citation_count": 5482,
    "authors": []
  },
  "https://openreview.net/forum?id=ByeZ5jC5YQ": {
    "title": "KnockoffGAN: Generating Knockoffs for Feature Selection using Generative Adversarial Networks",
    "volume": "oral",
    "abstract": "Feature selection is a pervasive problem. The discovery of relevant features can be as important for performing a particular task (such as to avoid overfitting in prediction) as it can be for understanding the underlying processes governing the true label (such as discovering relevant genetic factors for a disease). Machine learning driven feature selection can enable discovery from large, high-dimensional, non-linear observational datasets by creating a subset of features for experts to focus on. In order to use expert time most efficiently, we need a principled methodology capable of controlling the False Discovery Rate. In this work, we build on the promising Knockoff framework by developing a flexible knockoff generation model. We adapt the Generative Adversarial Networks framework to allow us to generate knockoffs with no assumptions on the feature distribution. Our model consists of 4 networks, a generator, a discriminator, a stability network and a power network. We demonstrate the capability of our model to perform feature selection, showing that it performs as well as the originally proposed knockoff generation model in the Gaussian setting and that it outperforms the original model in non-Gaussian settings, including on a real-world dataset",
    "checked": true,
    "id": "7b767e7cb5b67589646f0c1162423ceeb2daa3a7",
    "semantic_title": "knockoffgan: generating knockoffs for feature selection using generative adversarial networks",
    "citation_count": 60,
    "authors": []
  },
  "https://openreview.net/forum?id=r1xlvi0qYm": {
    "title": "Learning to Remember More with Less Memorization",
    "volume": "oral",
    "abstract": "Memory-augmented neural networks consisting of a neural controller and an external memory have shown potentials in long-term sequential learning. Current RAM-like memory models maintain memory accessing every timesteps, thus they do not effectively leverage the short-term memory held in the controller. We hypothesize that this scheme of writing is suboptimal in memory utilization and introduces redundant computation. To validate our hypothesis, we derive a theoretical bound on the amount of information stored in a RAM-like system and formulate an optimization problem that maximizes the bound. The proposed solution dubbed Uniform Writing is proved to be optimal under the assumption of equal timestep contributions. To relax this assumption, we introduce modifications to the original solution, resulting in a solution termed Cached Uniform Writing. This method aims to balance between maximizing memorization and forgetting via overwriting mechanisms. Through an extensive set of experiments, we empirically demonstrate the advantages of our solutions over other recurrent architectures, claiming the state-of-the-arts in various sequential modeling tasks",
    "checked": true,
    "id": "dc836eda6649fa9e0534a7ba4ddafd97a579926b",
    "semantic_title": "learning to remember more with less memorization",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=B1l08oAct7": {
    "title": "Deterministic Variational Inference for Robust Bayesian Neural Networks",
    "volume": "oral",
    "abstract": "Bayesian neural networks (BNNs) hold great promise as a flexible and principled solution to deal with uncertainty when learning from finite data. Among approaches to realize probabilistic inference in deep neural networks, variational Bayes (VB) is theoretically grounded, generally applicable, and computationally efficient. With wide recognition of potential advantages, why is it that variational Bayes has seen very limited practical use for BNNs in real applications? We argue that variational inference in neural networks is fragile: successful implementations require careful initialization and tuning of prior variances, as well as controlling the variance of Monte Carlo gradient estimates. We provide two innovations that aim to turn VB into a robust inference tool for Bayesian neural networks: first, we introduce a novel deterministic method to approximate moments in neural networks, eliminating gradient variance; second, we introduce a hierarchical prior for parameters and a novel Empirical Bayes procedure for automatically selecting prior variances. Combining these two innovations, the resulting method is highly efficient and robust. On the application of heteroscedastic regression we demonstrate good predictive performance over alternative approaches",
    "checked": true,
    "id": "51bb7f2681605651a4df725ac6c9e18377d36ac5",
    "semantic_title": "deterministic variational inference for robust bayesian neural networks",
    "citation_count": 169,
    "authors": []
  },
  "https://openreview.net/forum?id=rJVorjCcKQ": {
    "title": "Slalom: Fast, Verifiable and Private Execution of Neural Networks in Trusted Hardware",
    "volume": "oral",
    "abstract": "As Machine Learning (ML) gets applied to security-critical or sensitive domains, there is a growing need for integrity and privacy for outsourced ML computations. A pragmatic solution comes from Trusted Execution Environments (TEEs), which use hardware and software protections to isolate sensitive computations from the untrusted software stack. However, these isolation guarantees come at a price in performance, compared to untrusted alternatives. This paper initiates the study of high performance execution of Deep Neural Networks (DNNs) in TEEs by efficiently partitioning DNN computations between trusted and untrusted devices. Building upon an efficient outsourcing scheme for matrix multiplication, we propose Slalom, a framework that securely delegates execution of all linear layers in a DNN from a TEE (e.g., Intel SGX or Sanctum) to a faster, yet untrusted, co-located processor. We evaluate Slalom by running DNNs in an Intel SGX enclave, which selectively delegates work to an untrusted GPU. For canonical DNNs (VGG16, MobileNet and ResNet variants) we obtain 6x to 20x increases in throughput for verifiable inference, and 4x to 11x for verifiable and private inference",
    "checked": true,
    "id": "662f3ca1f074e24803d33fcd6c7d19564de107f2",
    "semantic_title": "slalom: fast, verifiable and private execution of neural networks in trusted hardware",
    "citation_count": 409,
    "authors": []
  },
  "https://openreview.net/forum?id=HJx54i05tX": {
    "title": "On Random Deep Weight-Tied Autoencoders: Exact Asymptotic Analysis, Phase Transitions, and Implications to Training",
    "volume": "oral",
    "abstract": "We study the behavior of weight-tied multilayer vanilla autoencoders under the assumption of random weights. Via an exact characterization in the limit of large dimensions, our analysis reveals interesting phase transition phenomena when the depth becomes large. This, in particular, provides quantitative answers and insights to three questions that were yet fully understood in the literature. Firstly, we provide a precise answer on how the random deep weight-tied autoencoder model performs \"approximate inference\" as posed by Scellier et al. (2018), and its connection to reversibility considered by several theoretical studies. Secondly, we show that deep autoencoders display a higher degree of sensitivity to perturbations in the parameters, distinct from the shallow counterparts. Thirdly, we obtain insights on pitfalls in training initialization practice, and demonstrate experimentally that it is possible to train a deep autoencoder, even with the tanh activation and a depth as large as 200 layers, without resorting to techniques such as layer-wise pre-training or batch normalization. Our analysis is not specific to any depths or any Lipschitz activations, and our analytical techniques may have broader applicability",
    "checked": true,
    "id": "5a0772885ed113e9da7aa643c877feea387d160e",
    "semantic_title": "on random deep weight-tied autoencoders: exact asymptotic analysis, phase transitions, and implications to training",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=H1xSNiRcF7": {
    "title": "Smoothing the Geometry of Probabilistic Box Embeddings",
    "volume": "oral",
    "abstract": "There is growing interest in geometrically-inspired embeddings for learning hierarchies, partial orders, and lattice structures, with natural applications to transitive relational data such as entailment graphs. Recent work has extended these ideas beyond deterministic hierarchies to probabilistically calibrated models, which enable learning from uncertain supervision and inferring soft-inclusions among concepts, while maintaining the geometric inductive bias of hierarchical embedding models. We build on the Box Lattice model of Vilnis et al. (2018), which showed promising results in modeling soft-inclusions through an overlapping hierarchy of sets, parameterized as high-dimensional hyperrectangles (boxes). However, the hard edges of the boxes present difficulties for standard gradient based optimization; that work employed a special surrogate function for the disjoint case, but we find this method to be fragile. In this work, we present a novel hierarchical embedding model, inspired by a relaxation of box embeddings into parameterized density functions using Gaussian convolutions over the boxes. Our approach provides an alternative surrogate to the original lattice measure that improves the robustness of optimization in the disjoint case, while also preserving the desirable properties with respect to the original lattice. We demonstrate increased or matching performance on WordNet hypernymy prediction, Flickr caption entailment, and a MovieLens-based market basket dataset. We show especially marked improvements in the case of sparse data, where many conditional probabilities should be low, and thus boxes should be nearly disjoint",
    "checked": true,
    "id": "63bcdc392c7135ccc2ded4cd53ef4e2f65ba80a0",
    "semantic_title": "smoothing the geometry of probabilistic box embeddings",
    "citation_count": 86,
    "authors": []
  },
  "https://openreview.net/forum?id=rJl0r3R9KX": {
    "title": "Regularized Learning for Domain Adaptation under Label Shifts",
    "volume": "poster",
    "abstract": "We propose Regularized Learning under Label shifts (RLLS), a principled and a practical domain-adaptation algorithm to correct for shifts in the label distribution between a source and a target domain. We first estimate importance weights using labeled source data and unlabeled target data, and then train a classifier on the weighted source samples. We derive a generalization bound for the classifier on the target domain which is independent of the (ambient) data dimensions, and instead only depends on the complexity of the function class. To the best of our knowledge, this is the first generalization bound for the label-shift problem where the labels in the target domain are not available. Based on this bound, we propose a regularized estimator for the small-sample regime which accounts for the uncertainty in the estimated weights. Experiments on the CIFAR-10 and MNIST datasets show that RLLS improves classification accuracy, especially in the low sample and large-shift regimes, compared to previous methods",
    "checked": true,
    "id": "6d64363e52cd7ecad99d7ce6ae849f245dfbbf92",
    "semantic_title": "regularized learning for domain adaptation under label shifts",
    "citation_count": 207,
    "authors": []
  },
  "https://openreview.net/forum?id=SylCrnCcFX": {
    "title": "Towards Robust, Locally Linear Deep Networks",
    "volume": "poster",
    "abstract": "Deep networks realize complex mappings that are often understood by their locally linear behavior at or around points of interest. For example, we use the derivative of the mapping with respect to its inputs for sensitivity analysis, or to explain (obtain coordinate relevance for) a prediction. One key challenge is that such derivatives are themselves inherently unstable. In this paper, we propose a new learning problem to encourage deep networks to have stable derivatives over larger regions. While the problem is challenging in general, we focus on networks with piecewise linear activation functions. Our algorithm consists of an inference step that identifies a region around a point where linear approximation is provably stable, and an optimization step to expand such regions. We propose a novel relaxation to scale the algorithm to realistic models. We illustrate our method with residual and recurrent networks on image and sequence datasets",
    "checked": true,
    "id": "a403099192180450edc594d2fed7a19296e9ff48",
    "semantic_title": "towards robust, locally linear deep networks",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=HylTBhA5tQ": {
    "title": "The Limitations of Adversarial Training and the Blind-Spot Attack",
    "volume": "poster",
    "abstract": "The adversarial training procedure proposed by Madry et al. (2018) is one of the most effective methods to defend against adversarial examples in deep neural net- works (DNNs). In our paper, we shed some lights on the practicality and the hardness of adversarial training by showing that the effectiveness (robustness on test set) of adversarial training has a strong correlation with the distance between a test point and the manifold of training data embedded by the network. Test examples that are relatively far away from this manifold are more likely to be vulnerable to adversarial attacks. Consequentially, an adversarial training based defense is susceptible to a new class of attacks, the \"blind-spot attack\", where the input images reside in \"blind-spots\" (low density regions) of the empirical distri- bution of training data but is still on the ground-truth data manifold. For MNIST, we found that these blind-spots can be easily found by simply scaling and shifting image pixel values. Most importantly, for large datasets with high dimensional and complex data manifold (CIFAR, ImageNet, etc), the existence of blind-spots in adversarial training makes defending on any valid test examples difficult due to the curse of dimensionality and the scarcity of training data. Additionally, we find that blind-spots also exist on provable defenses including (Kolter & Wong, 2018) and (Sinha et al., 2018) because these trainable robustness certificates can only be practically optimized on a limited set of training data",
    "checked": true,
    "id": "e749e8c947550485eddf864f8efeb870b894e4ce",
    "semantic_title": "the limitations of adversarial training and the blind-spot attack",
    "citation_count": 146,
    "authors": []
  },
  "https://openreview.net/forum?id=B1gTShAct7": {
    "title": "Learning to Learn without Forgetting by Maximizing Transfer and Minimizing Interference",
    "volume": "poster",
    "abstract": "Lack of performance when it comes to continual learning over non-stationary distributions of data remains a major challenge in scaling neural network learning to more human realistic settings. In this work we propose a new conceptualization of the continual learning problem in terms of a temporally symmetric trade-off between transfer and interference that can be optimized by enforcing gradient alignment across examples. We then propose a new algorithm, Meta-Experience Replay (MER), that directly exploits this view by combining experience replay with optimization based meta-learning. This method learns parameters that make interference based on future gradients less likely and transfer based on future gradients more likely. We conduct experiments across continual lifelong supervised learning benchmarks and non-stationary reinforcement learning environments demonstrating that our approach consistently outperforms recently proposed baselines for continual learning. Our experiments show that the gap between the performance of MER and baseline algorithms grows both as the environment gets more non-stationary and as the fraction of the total experiences stored gets smaller",
    "checked": true,
    "id": "2b877889ac31b73d1ede70b00eb4c7118ef8eca2",
    "semantic_title": "learning to learn without forgetting by maximizing transfer and minimizing interference",
    "citation_count": 809,
    "authors": []
  },
  "https://openreview.net/forum?id=ryxnHhRqFm": {
    "title": "Global-to-local Memory Pointer Networks for Task-Oriented Dialogue",
    "volume": "poster",
    "abstract": "End-to-end task-oriented dialogue is challenging since knowledge bases are usually large, dynamic and hard to incorporate into a learning framework. We propose the global-to-local memory pointer (GLMP) networks to address this issue. In our model, a global memory encoder and a local memory decoder are proposed to share external knowledge. The encoder encodes dialogue history, modifies global contextual representation, and generates a global memory pointer. The decoder first generates a sketch response with unfilled slots. Next, it passes the global memory pointer to filter the external knowledge for relevant information, then instantiates the slots via the local memory pointers. We empirically show that our model can improve copy accuracy and mitigate the common out-of-vocabulary problem. As a result, GLMP is able to improve over the previous state-of-the-art models in both simulated bAbI Dialogue dataset and human-human Stanford Multi-domain Dialogue dataset on automatic and human evaluation",
    "checked": true,
    "id": "924acc8b76779a5211cc04bbe6cf13bd0bc6e7f8",
    "semantic_title": "global-to-local memory pointer networks for task-oriented dialogue",
    "citation_count": 168,
    "authors": []
  },
  "https://openreview.net/forum?id=rJlnB3C5Ym": {
    "title": "Rethinking the Value of Network Pruning",
    "volume": "poster",
    "abstract": "Network pruning is widely used for reducing the heavy inference cost of deep models in low-resource settings. A typical pruning algorithm is a three-stage pipeline, i.e., training (a large model), pruning and fine-tuning. During pruning, according to a certain criterion, redundant weights are pruned and important weights are kept to best preserve the accuracy. In this work, we make several surprising observations which contradict common beliefs. For all state-of-the-art structured pruning algorithms we examined, fine-tuning a pruned model only gives comparable or worse performance than training that model with randomly initialized weights. For pruning algorithms which assume a predefined target network architecture, one can get rid of the full pipeline and directly train the target network from scratch. Our observations are consistent for multiple network architectures, datasets, and tasks, which imply that: 1) training a large, over-parameterized model is often not necessary to obtain an efficient final model, 2) learned ``important'' weights of the large model are typically not useful for the small pruned model, 3) the pruned architecture itself, rather than a set of inherited ``important'' weights, is more crucial to the efficiency in the final model, which suggests that in some cases pruning can be useful as an architecture search paradigm. Our results suggest the need for more careful baseline evaluations in future research on structured pruning methods. We also compare with the \"Lottery Ticket Hypothesis\" (Frankle & Carbin 2019), and find that with optimal learning rate, the \"winning ticket\" initialization as used in Frankle & Carbin (2019) does not bring improvement over random initialization",
    "checked": true,
    "id": "4a1004ecd34118116344633c7cdcc34493c423ee",
    "semantic_title": "rethinking the value of network pruning",
    "citation_count": 1500,
    "authors": []
  },
  "https://openreview.net/forum?id=ByzcS3AcYX": {
    "title": "Neural TTS Stylization with Adversarial and Collaborative Games",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c978ee204c0f6ab6e1a8c635a93d16612927e24b",
    "semantic_title": "neural tts stylization with adversarial and collaborative games",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=SJe9rh0cFX": {
    "title": "On the Universal Approximability and Complexity Bounds of Quantized ReLU Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "169b173ed1a397e8b47d9dbac7eefb770cb30ce9",
    "semantic_title": "on the universal approximability and complexity bounds of quantized relu neural networks",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=Ske5r3AqK7": {
    "title": "Poincare Glove: Hyperbolic Word Embeddings",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "e8fa823c17aeb8d08fe9aa5fc2bc0eaacb9edcdf",
    "semantic_title": "poincar√© glove: hyperbolic word embeddings",
    "citation_count": 296,
    "authors": []
  },
  "https://openreview.net/forum?id=B1lKS2AqtX": {
    "title": "Eidetic 3D LSTM: A Model for Video Prediction and Beyond",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "82a646e1cb33124f672beba451f5039e9e32fb6d",
    "semantic_title": "eidetic 3d lstm: a model for video prediction and beyond",
    "citation_count": 364,
    "authors": []
  },
  "https://openreview.net/forum?id=HkxKH2AcFm": {
    "title": "Towards GAN Benchmarks Which Require Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "27228e7e227d1e216950fd7b99ba4178a8730f93",
    "semantic_title": "towards shape biased unsupervised representation learning for domain generalization",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgKBhA5Y7": {
    "title": "There Are Many Consistent Explanations of Unlabeled Data: Why You Should Average",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "98286df6d923d787f26e034bbaf3a5a64ac29cb1",
    "semantic_title": "there are many consistent explanations of unlabeled data: why you should average",
    "citation_count": 246,
    "authors": []
  },
  "https://openreview.net/forum?id=ryeOSnAqYm": {
    "title": "Synthetic Datasets for Neural Program Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a15763582df784b43548c6d53edfd55568c35168",
    "semantic_title": "synthetic datasets for neural program synthesis",
    "citation_count": 46,
    "authors": []
  },
  "https://openreview.net/forum?id=r1xdH3CcKX": {
    "title": "Stochastic Prediction of Multi-Agent Interactions from Partial Observations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "906c03e6e188d301b85ca6521955a8584f9babe7",
    "semantic_title": "stochastic prediction of multi-agent interactions from partial observations",
    "citation_count": 89,
    "authors": []
  },
  "https://openreview.net/forum?id=HyePrhR5KX": {
    "title": "DyRep: Learning Representations over Dynamic Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5593001e49474a475dbcae99be350a8d527c05a3",
    "semantic_title": "dyrep: learning representations over dynamic graphs",
    "citation_count": 526,
    "authors": []
  },
  "https://openreview.net/forum?id=rkxwShA9Ym": {
    "title": "Label super-resolution networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "938bd120c083170c785be0dbb3a67edbb6e5356a",
    "semantic_title": "label super-resolution networks",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=HkfPSh05K7": {
    "title": "Multi-step Retriever-Reader Interaction for Scalable Open-domain Question Answering",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e7512b84e923372ae410d7614e71224d573ed2ef",
    "semantic_title": "multi-step retriever-reader interaction for scalable open-domain question answering",
    "citation_count": 165,
    "authors": []
  },
  "https://openreview.net/forum?id=Byl8BnRcYm": {
    "title": "Capsule Graph Neural Network",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "67de0bc5d1dfb458efc9aa192c879bf00ee667b3",
    "semantic_title": "capsule graph neural network",
    "citation_count": 227,
    "authors": []
  },
  "https://openreview.net/forum?id=Syl8Sn0cK7": {
    "title": "Learning a Meta-Solver for Syntax-Guided Program Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f262ad9e3a84bcea08acd1c97ee9740cd78ee256",
    "semantic_title": "learning a meta-solver for syntax-guided program synthesis",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=H1eSS3CcKX": {
    "title": "Stochastic Optimization of Sorting Networks via Continuous Relaxations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "38e3a9620f575dc16cf26957c3cafa035b8f1895",
    "semantic_title": "stochastic optimization of sorting networks via continuous relaxations",
    "citation_count": 175,
    "authors": []
  },
  "https://openreview.net/forum?id=BylBr3C9K7": {
    "title": "Energy-Constrained Compression for Deep Neural Networks via Weighted Sparse Projection and Layer Input Masking",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0986f2ac6755df5d196ceb09b5bdf19593cbbaef",
    "semantic_title": "energy-constrained compression for deep neural networks via weighted sparse projection and layer input masking",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=rygrBhC5tQ": {
    "title": "Composing Complex Skills by Learning Transition Policies",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "74e12851de2d542aa2aef7b8a39ef021a5802689",
    "semantic_title": "composing complex skills by learning transition policies",
    "citation_count": 93,
    "authors": []
  },
  "https://openreview.net/forum?id=ryxSrhC9KX": {
    "title": "Revealing interpretable object representations from human behavior",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bfd9f718c8e7ec5e3ef47e7cc388fa9918a86525",
    "semantic_title": "revealing interpretable object representations from human behavior",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=HylVB3AqYm": {
    "title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f323407464c4cd492d3fc1afd7170eab08f44d9b",
    "semantic_title": "proxylessnas: direct neural architecture search on target task and hardware",
    "citation_count": 1894,
    "authors": []
  },
  "https://openreview.net/forum?id=r1x4BnCqKX": {
    "title": "A Generative Model For Electron Paths",
    "volume": "poster",
    "abstract": "Chemical reactions can be described as the stepwise redistribution of electrons in molecules. As such, reactions are often depicted using \"arrow-pushing\" diagrams which show this movement as a sequence of arrows. We propose an electron path prediction model (ELECTRO) to learn these sequences directly from raw reaction data. Instead of predicting product molecules directly from reactant molecules in one shot, learning a model of electron movement has the benefits of (a) being easy for chemists to interpret, (b) incorporating constraints of chemistry, such as balanced atom counts before and after the reaction, and (c) naturally encoding the sparsity of chemical reactions, which usually involve changes in only a small number of atoms in the reactants. We design a method to extract approximate reaction paths from any dataset of atom-mapped reaction SMILES strings. Our model achieves excellent performance on an important subset of the USPTO reaction dataset, comparing favorably to the strongest baselines. Furthermore, we show that our model recovers a basic knowledge of chemistry without being explicitly trained to do so",
    "checked": true,
    "id": "6e3e17ac9a5a25ca718bd2da025a7117c4f03634",
    "semantic_title": "a generative model for electron paths",
    "citation_count": 55,
    "authors": []
  },
  "https://openreview.net/forum?id=rylNH20qFQ": {
    "title": "Learning to Infer and Execute 3D Shape Programs",
    "volume": "poster",
    "abstract": "Human perception of 3D shapes goes beyond reconstructing them as a set of points or a composition of geometric primitives: we also effortlessly understand higher-level shape structure such as the repetition and reflective symmetry of object parts. In contrast, recent advances in 3D shape sensing focus more on low-level geometry but less on these higher-level relationships. In this paper, we propose 3D shape programs, integrating bottom-up recognition systems with top-down, symbolic program structure to capture both low-level geometry and high-level structural priors for 3D shapes. Because there are no annotations of shape programs for real shapes, we develop neural modules that not only learn to infer 3D shape programs from raw, unannotated shapes, but also to execute these programs for shape reconstruction. After initial bootstrapping, our end-to-end differentiable model learns 3D shape programs by reconstructing shapes in a self-supervised manner. Experiments demonstrate that our model accurately infers and executes 3D shape programs for highly complex shapes from various categories. It can also be integrated with an image-to-shape module to infer 3D shape programs directly from an RGB image, leading to 3D shape reconstructions that are both more accurate and more physically plausible",
    "checked": true,
    "id": "7e2f5eca9465cf114043ed6c95ea59d9dbea45a1",
    "semantic_title": "learning to infer and execute 3d shape programs",
    "citation_count": 148,
    "authors": []
  },
  "https://openreview.net/forum?id=rJe4ShAcF7": {
    "title": "Music Transformer: Generating Music with Long-Term Structure",
    "volume": "poster",
    "abstract": "Music relies heavily on repetition to build structure and meaning. Self-reference occurs on multiple timescales, from motifs to phrases to reusing of entire sections of music, such as in pieces with ABA structure. The Transformer (Vaswani et al., 2017), a sequence model based on self-attention, has achieved compelling results in many generation tasks that require maintaining long-range coherence. This suggests that self-attention might also be well-suited to modeling music. In musical composition and performance, however, relative timing is critically important. Existing approaches for representing relative positional information in the Transformer modulate attention based on pairwise distance (Shaw et al., 2018). This is impractical for long sequences such as musical compositions since their memory complexity is quadratic in the sequence length. We propose an algorithm that reduces the intermediate memory requirements to linear in the sequence length. This enables us to demonstrate that a Transformer with our modified relative attention mechanism can generate minute-long (thousands of steps) compositions with compelling structure, generate continuations that coherently elaborate on a given motif, and in a seq2seq setup generate accompaniments conditioned on melodies. We evaluate the Transformer with our relative attention mechanism on two datasets, JSB Chorales and Piano-e-competition, and obtain state-of-the-art results on the latter",
    "checked": true,
    "id": "fb507ada871d1e8c29e376dbf7b7879689aa89f9",
    "semantic_title": "music transformer: generating music with long-term structure",
    "citation_count": 495,
    "authors": []
  },
  "https://openreview.net/forum?id=SkgQBn0cF7": {
    "title": "Modeling the Long Term Future in Model-Based Reinforcement Learning",
    "volume": "poster",
    "abstract": "In model-based reinforcement learning, the agent interleaves between model learning and planning. These two components are inextricably intertwined. If the model is not able to provide sensible long-term prediction, the executed planer would exploit model flaws, which can yield catastrophic failures. This paper focuses on building a model that reasons about the long-term future and demonstrates how to use this for efficient planning and exploration. To this end, we build a latent-variable autoregressive model by leveraging recent ideas in variational inference. We argue that forcing latent variables to carry future information through an auxiliary task substantially improves long-term predictions. Moreover, by planning in the latent space, the planner's solution is ensured to be within regions where the model is valid. An exploration strategy can be devised by searching for unlikely trajectories under the model. Our methods achieves higher reward faster compared to baselines on a variety of tasks and environments in both the imitation learning and model-based reinforcement learning settings",
    "checked": true,
    "id": "c4e02b75e525c4e8c4616d2e9dfeb4a638142c51",
    "semantic_title": "modeling the long term future in model-based reinforcement learning",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=HygQBn0cYm": {
    "title": "Model-Predictive Policy Learning with Uncertainty Regularization for Driving in Dense Traffic",
    "volume": "poster",
    "abstract": "Learning a policy using only observational data is challenging because the distribution of states it induces at execution time may differ from the distribution observed during training. In this work, we propose to train a policy while explicitly penalizing the mismatch between these two distributions over a fixed time horizon. We do this by using a learned model of the environment dynamics which is unrolled for multiple time steps, and training a policy network to minimize a differentiable cost over this rolled-out trajectory. This cost contains two terms: a policy cost which represents the objective the policy seeks to optimize, and an uncertainty cost which represents its divergence from the states it is trained on. We propose to measure this second cost by using the uncertainty of the dynamics model about its own predictions, using recent ideas from uncertainty estimation for deep networks. We evaluate our approach using a large-scale observational dataset of driving behavior recorded from traffic cameras, and show that we are able to learn effective driving policies from purely observational data, with no environment interaction",
    "checked": true,
    "id": "2eeace98cf3c105a8d37884dc8d33c50ae4b7ddb",
    "semantic_title": "model-predictive policy learning with uncertainty regularization for driving in dense traffic",
    "citation_count": 123,
    "authors": []
  },
  "https://openreview.net/forum?id=ByeMB3Act7": {
    "title": "Learning to Screen for Fast Softmax Inference on Large Vocabulary Neural Networks",
    "volume": "poster",
    "abstract": "Neural language models have been widely used in various NLP tasks, including machine translation, next word prediction and conversational agents. However, it is challenging to deploy these models on mobile devices due to their slow prediction speed, where the bottleneck is to compute top candidates in the softmax layer. In this paper, we introduce a novel softmax layer approximation algorithm by exploiting the clustering structure of context vectors. Our algorithm uses a light-weight screening model to predict a much smaller set of candidate words based on the given context, and then conducts an exact softmax only within that subset. Training such a procedure end-to-end is challenging as traditional clustering methods are discrete and non-differentiable, and thus unable to be used with back-propagation in the training process. Using the Gumbel softmax, we are able to train the screening model end-to-end on the training set to exploit data distribution. The algorithm achieves an order of magnitude faster inference than the original softmax layer for predicting top-k words in various tasks such as beam search in machine translation or next words prediction. For example, for machine translation task on German to English dataset with around 25K vocabulary, we can achieve 20.4 times speed up with 98.9% precision@1 and 99.3% precision@5 with the original softmax layer prediction, while state-of-the-art (Zhang et al., 2018) only achieves 6.7x speedup with 98.7% precision@1 and 98.1% precision@5 for the same task",
    "checked": true,
    "id": "dccf55bad7c065fba1f25e56699584392895b05a",
    "semantic_title": "learning to screen for fast softmax inference on large vocabulary neural networks",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=r1efr3C9Ym": {
    "title": "Interpolation-Prediction Networks for Irregularly Sampled Time Series",
    "volume": "poster",
    "abstract": "In this paper, we present a new deep learning architecture for addressing the problem of supervised learning with sparse and irregularly sampled multivariate time series. The architecture is based on the use of a semi-parametric interpolation network followed by the application of a prediction network. The interpolation network allows for information to be shared across multiple dimensions of a multivariate time series during the interpolation stage, while any standard deep learning model can be used for the prediction network. This work is motivated by the analysis of physiological time series data in electronic health records, which are sparse, irregularly sampled, and multivariate. We investigate the performance of this architecture on both classification and regression tasks, showing that our approach outperforms a range of baseline and recently proposed models",
    "checked": true,
    "id": "07334ab67ee57811f32da13588f900e8dc3211c6",
    "semantic_title": "interpolation-prediction networks for irregularly sampled time series",
    "citation_count": 148,
    "authors": []
  },
  "https://openreview.net/forum?id=HyxGB2AcY7": {
    "title": "Contingency-Aware Exploration in Reinforcement Learning",
    "volume": "poster",
    "abstract": "This paper investigates whether learning contingency-awareness and controllable aspects of an environment can lead to better exploration in reinforcement learning. To investigate this question, we consider an instantiation of this hypothesis evaluated on the Arcade Learning Element (ALE). In this study, we develop an attentive dynamics model (ADM) that discovers controllable elements of the observations, which are often associated with the location of the character in Atari games. The ADM is trained in a self-supervised fashion to predict the actions taken by the agent. The learned contingency information is used as a part of the state representation for exploration purposes. We demonstrate that combining actor-critic algorithm with count-based exploration using our representation achieves impressive results on a set of notoriously challenging Atari games due to sparse rewards. For example, we report a state-of-the-art score of >11,000 points on Montezuma's Revenge without using expert demonstrations, explicit high-level information (e.g., RAM states), or supervisory data. Our experiments confirm that contingency-awareness is indeed an extremely powerful concept for tackling exploration problems in reinforcement learning and opens up interesting research questions for further investigations",
    "checked": true,
    "id": "bf604ae3ddd5adec55554921b37f04035b7350a7",
    "semantic_title": "contingency-aware exploration in reinforcement learning",
    "citation_count": 75,
    "authors": []
  },
  "https://openreview.net/forum?id=BkgWHnR5tm": {
    "title": "Neural Graph Evolution: Towards Efficient Automatic Robot Design",
    "volume": "poster",
    "abstract": "Despite the recent successes in robotic locomotion control, the design of robot relies heavily on human engineering. Automatic robot design has been a long studied subject, but the recent progress has been slowed due to the large combinatorial search space and the difficulty in evaluating the found candidates. To address the two challenges, we formulate automatic robot design as a graph search problem and perform evolution search in graph space. We propose Neural Graph Evolution (NGE), which performs selection on current candidates and evolves new ones iteratively. Different from previous approaches, NGE uses graph neural networks to parameterize the control policies, which reduces evaluation cost on new candidates with the help of skill transfer from previously evaluated designs. In addition, NGE applies Graph Mutation with Uncertainty (GM-UC) by incorporating model uncertainty, which reduces the search space by balancing exploration and exploitation. We show that NGE significantly outperforms previous methods by an order of magnitude. As shown in experiments, NGE is the first algorithm that can automatically discover kinematically preferred robotic graph structures, such as a fish with two symmetrical flat side-fins and a tail, or a cheetah with athletic front and back legs. Instead of using thousands of cores for weeks, NGE efficiently solves searching problem within a day on a single 64 CPU-core Amazon EC2 machine",
    "checked": true,
    "id": "5b3a45b76e144fed2478419a7229930b28151f46",
    "semantic_title": "neural graph evolution: towards efficient automatic robot design",
    "citation_count": 63,
    "authors": []
  },
  "https://openreview.net/forum?id=Bkxbrn0cYX": {
    "title": "Selfless Sequential Learning",
    "volume": "poster",
    "abstract": "Sequential learning, also called lifelong learning, studies the problem of learning tasks in a sequence with access restricted to only the data of the current task. In this paper we look at a scenario with fixed model capacity, and postulate that the learning process should not be selfish, i.e. it should account for future tasks to be added and thus leave enough capacity for them. To achieve Selfless Sequential Learning we study different regularization strategies and activation functions. We find that imposing sparsity at the level of the representation (i.e. neuron activations) is more beneficial for sequential learning than encouraging parameter sparsity. In particular, we propose a novel regularizer, that encourages representation sparsity by means of neural inhibition. It results in few active neurons which in turn leaves more free neurons to be utilized by upcoming tasks. As neural inhibition over an entire layer can be too drastic, especially for complex tasks requiring strong representations, our regularizer only inhibits other neurons in a local neighbourhood, inspired by lateral inhibition processes in the brain. We combine our novel regularizer with state-of-the-art lifelong learning methods that penalize changes to important previously learned parts of the network. We show that our new regularizer leads to increased sparsity which translates in consistent performance improvement on diverse datasets",
    "checked": true,
    "id": "974e22699cb165b0285a7c72b5cc6c3d42010143",
    "semantic_title": "selfless sequential learning",
    "citation_count": 116,
    "authors": []
  },
  "https://openreview.net/forum?id=rJgbSn09Ym": {
    "title": "Learning Particle Dynamics for Manipulating Rigid Bodies, Deformable Objects, and Fluids",
    "volume": "poster",
    "abstract": "Real-life control tasks involve matters of various substances---rigid or soft bodies, liquid, gas---each with distinct physical behaviors. This poses challenges to traditional rigid-body physics engines. Particle-based simulators have been developed to model the dynamics of these complex scenes; however, relying on approximation techniques, their simulation often deviates from real-world physics, especially in the long term. In this paper, we propose to learn a particle-based simulator for complex control tasks. Combining learning with particle-based systems brings in two major benefits: first, the learned simulator, just like other particle-based systems, acts widely on objects of different materials; second, the particle-based representation poses strong inductive bias for learning: particles of the same type have the same dynamics within. This enables the model to quickly adapt to new environments of unknown dynamics within a few observations. We demonstrate robots achieving complex manipulation tasks using the learned simulator, such as manipulating fluids and deformable foam, with experiments both in simulation and in the real world. Our study helps lay the foundation for robot learning of dynamic scenes with particle-based representations",
    "checked": true,
    "id": "d40531ed81859ce40f119c1bbc1d1cb50af498fd",
    "semantic_title": "learning particle dynamics for manipulating rigid bodies, deformable objects, and fluids",
    "citation_count": 403,
    "authors": []
  },
  "https://openreview.net/forum?id=H1zeHnA9KX": {
    "title": "Representing Formal Languages: A Comparison Between Finite Automata and Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "We investigate the internal representations that a recurrent neural network (RNN) uses while learning to recognize a regular formal language. Specifically, we train a RNN on positive and negative examples from a regular language, and ask if there is a simple decoding function that maps states of this RNN to states of the minimal deterministic finite automaton (MDFA) for the language. Our experiments show that such a decoding function indeed exists, and that it maps states of the RNN not to MDFA states, but to states of an {\\em abstraction} obtained by clustering small sets of MDFA states into ``''superstates''. A qualitative analysis reveals that the abstraction often has a simple interpretation. Overall, the results suggest a strong structural relationship between internal representations used by RNNs and finite automata, and explain the well-known ability of RNNs to recognize formal grammatical structure",
    "checked": true,
    "id": "9fac39c702724732188ff61090d2e2b498a34eb2",
    "semantic_title": "representing formal languages: a comparison between finite automata and recurrent neural networks",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=B1exrnCcF7": {
    "title": "Disjoint Mapping Network for Cross-modal Matching of Voices and Faces",
    "volume": "poster",
    "abstract": "We propose a novel framework, called Disjoint Mapping Network (DIMNet), for cross-modal biometric matching, in particular of voices and faces. Different from the existing methods, DIMNet does not explicitly learn the joint relationship between the modalities. Instead, DIMNet learns a shared representation for different modalities by mapping them individually to their common covariates. These shared representations can then be used to find the correspondences between the modalities. We show empirically that DIMNet is able to achieve better performance than the current state-of-the-art methods, with the additional benefits of being conceptually simpler and less data-intensive",
    "checked": true,
    "id": "4188f289eb85bb047dbbc15acbd79fae6abe25f5",
    "semantic_title": "disjoint mapping network for cross-modal matching of voices and faces",
    "citation_count": 73,
    "authors": []
  },
  "https://openreview.net/forum?id=ByleB2CcKm": {
    "title": "Learning Procedural Abstractions and Evaluating Discrete Latent Temporal Structure",
    "volume": "poster",
    "abstract": "Clustering methods and latent variable models are often used as tools for pattern mining and discovery of latent structure in time-series data. In this work, we consider the problem of learning procedural abstractions from possibly high-dimensional observational sequences, such as video demonstrations. Given a dataset of time-series, the goal is to identify the latent sequence of steps common to them and label each time-series with the temporal extent of these procedural steps. We introduce a hierarchical Bayesian model called Prism that models the realization of a common procedure across multiple time-series, and can recover procedural abstractions with supervision. We also bring to light two characteristics ignored by traditional evaluation criteria when evaluating latent temporal labelings (temporal clusterings) -- segment structure, and repeated structure -- and develop new metrics tailored to their evaluation. We demonstrate that our metrics improve interpretability and ease of analysis for evaluation on benchmark time-series datasets. Results on benchmark and video datasets indicate that Prism outperforms standard sequence models as well as state-of-the-art techniques in identifying procedural abstractions",
    "checked": true,
    "id": "a92a616aa46f5d06cab7a02c337370ec0e19b4f4",
    "semantic_title": "learning procedural abstractions and evaluating discrete latent temporal structure",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=ByfyHh05tQ": {
    "title": "Learning to Design RNA",
    "volume": "poster",
    "abstract": "Designing RNA molecules has garnered recent interest in medicine, synthetic biology, biotechnology and bioinformatics since many functional RNA molecules were shown to be involved in regulatory processes for transcription, epigenetics and translation. Since an RNA's function depends on its structural properties, the RNA Design problem is to find an RNA sequence which satisfies given structural constraints. Here, we propose a new algorithm for the RNA Design problem, dubbed LEARNA. LEARNA uses deep reinforcement learning to train a policy network to sequentially design an entire RNA sequence given a specified target structure. By meta-learning across 65000 different RNA Design tasks for one hour on 20 CPU cores, our extension Meta-LEARNA constructs an RNA Design policy that can be applied out of the box to solve novel RNA Design tasks. Methodologically, for what we believe to be the first time, we jointly optimize over a rich space of architectures for the policy network, the hyperparameters of the training procedure and the formulation of the decision process. Comprehensive empirical results on two widely-used RNA Design benchmarks, as well as a third one that we introduce, show that our approach achieves new state-of-the-art performance on the former while also being orders of magnitudes faster in reaching the previous state-of-the-art performance. In an ablation study, we analyze the importance of our method's different components",
    "checked": true,
    "id": "60b115f0c1ea9565928ebced8d0606f4c31d6640",
    "semantic_title": "learning to design rna",
    "citation_count": 74,
    "authors": []
  },
  "https://openreview.net/forum?id=BygANhA9tQ": {
    "title": "Cost-Sensitive Robustness against Adversarial Examples",
    "volume": "poster",
    "abstract": "Several recent works have developed methods for training classifiers that are certifiably robust against norm-bounded adversarial perturbations. These methods assume that all the adversarial transformations are equally important, which is seldom the case in real-world applications. We advocate for cost-sensitive robustness as the criteria for measuring the classifier's performance for tasks where some adversarial transformation are more important than others. We encode the potential harm of each adversarial transformation in a cost matrix, and propose a general objective function to adapt the robust training method of Wong & Kolter (2018) to optimize for cost-sensitive robustness. Our experiments on simple MNIST and CIFAR10 models with a variety of cost matrices show that the proposed approach can produce models with substantially reduced cost-sensitive robust error, while maintaining classification accuracy",
    "checked": true,
    "id": "75d59ae0ed3ce51e37b383985cfff310251f591a",
    "semantic_title": "cost-sensitive robustness against adversarial examples",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=S1lTEh09FQ": {
    "title": "Combinatorial Attacks on Binarized Neural Networks",
    "volume": "poster",
    "abstract": "Binarized Neural Networks (BNNs) have recently attracted significant interest due to their computational efficiency. Concurrently, it has been shown that neural networks may be overly sensitive to ``attacks\" -- tiny adversarial changes in the input -- which may be detrimental to their use in safety-critical domains. Designing attack algorithms that effectively fool trained models is a key step towards learning robust neural networks. The discrete, non-differentiable nature of BNNs, which distinguishes them from their full-precision counterparts, poses a challenge to gradient-based attacks. In this work, we study the problem of attacking a BNN through the lens of combinatorial and integer optimization. We propose a Mixed Integer Linear Programming (MILP) formulation of the problem. While exact and flexible, the MILP quickly becomes intractable as the network and perturbation space grow. To address this issue, we propose IProp, a decomposition-based algorithm that solves a sequence of much smaller MILP problems. Experimentally, we evaluate both proposed methods against the standard gradient-based attack (PGD) on MNIST and Fashion-MNIST, and show that IProp performs favorably compared to PGD, while scaling beyond the limits of the MILP",
    "checked": true,
    "id": "e51b580d595a74616272ec3a1a45ed05989a3453",
    "semantic_title": "combinatorial attacks on binarized neural networks",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=r1laEnA5Ym": {
    "title": "A Variational Inequality Perspective on Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "Generative adversarial networks (GANs) form a generative modeling approach known for producing appealing samples, but they are notably difficult to train. One common way to tackle this issue has been to propose new formulations of the GAN objective. Yet, surprisingly few studies have looked at optimization methods designed for this adversarial training. In this work, we cast GAN optimization problems in the general variational inequality framework. Tapping into the mathematical programming literature, we counter some common misconceptions about the difficulties of saddle point optimization and propose to extend methods designed for variational inequalities to the training of GANs. We apply averaging, extrapolation and a computationally cheaper variant that we call extrapolation from the past to the stochastic gradient method (SGD) and Adam",
    "checked": true,
    "id": "641f0891e63ea033332be6428fa815beaecb61e2",
    "semantic_title": "a variational inequality perspective on generative adversarial networks",
    "citation_count": 355,
    "authors": []
  },
  "https://openreview.net/forum?id=H1g2NhC5KQ": {
    "title": "Multiple-Attribute Text Rewriting",
    "volume": "poster",
    "abstract": "The dominant approach to unsupervised \"style transfer'' in text is based on the idea of learning a latent representation, which is independent of the attributes specifying its \"style''. In this paper, we show that this condition is not necessary and is not always met in practice, even with domain adversarial training that explicitly aims at learning such disentangled representations. We thus propose a new model that controls several factors of variation in textual data where this condition on disentanglement is replaced with a simpler mechanism based on back-translation. Our method allows control over multiple attributes, like gender, sentiment, product type, etc., and a more fine-grained control on the trade-off between content preservation and change of style with a pooling operator in the latent space. Our experiments demonstrate that the fully entangled model produces better generations, even when tested on new and more challenging benchmarks comprising reviews with multiple sentences and multiple attributes",
    "checked": true,
    "id": "49c34076336a5de8e195a9af933b925b5e16d46a",
    "semantic_title": "multiple-attribute text rewriting",
    "citation_count": 238,
    "authors": []
  },
  "https://openreview.net/forum?id=HyGhN2A5tm": {
    "title": "Multi-Agent Dual Learning",
    "volume": "poster",
    "abstract": "Dual learning has attracted much attention in machine learning, computer vision and natural language processing communities. The core idea of dual learning is to leverage the duality between the primal task (mapping from domain X to domain Y) and dual task (mapping from domain Y to X) to boost the performances of both tasks. Existing dual learning framework forms a system with two agents (one primal model and one dual model) to utilize such duality. In this paper, we extend this framework by introducing multiple primal and dual models, and propose the multi-agent dual learning framework. Experiments on neural machine translation and image translation tasks demonstrate the effectiveness of the new framework. In particular, we set a new record on IWSLT 2014 German-to-English translation with a 35.44 BLEU score, achieve a 31.03 BLEU score on WMT 2014 English-to-German translation with over 2.6 BLEU improvement over the strong Transformer baseline, and set a new record of 49.61 BLEU score on the recent WMT 2018 English-to-German translation",
    "checked": true,
    "id": "4f00db92bd7c1fd7b41d361c13797b9ff45a5b91",
    "semantic_title": "multi-agent dual learning",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=SJxsV2R5FQ": {
    "title": "Learning sparse relational transition models",
    "volume": "poster",
    "abstract": "We present a representation for describing transition models in complex uncertain domains using relational rules. For any action, a rule selects a set of relevant objects and computes a distribution over properties of just those objects in the resulting state given their properties in the previous state. An iterative greedy algorithm is used to construct a set of deictic references that determine which objects are relevant in any given state. Feed-forward neural networks are used to learn the transition distribution on the relevant objects' properties. This strategy is demonstrated to be both more versatile and more sample efficient than learning a monolithic transition model in a simulated domain in which a robot pushes stacks of objects on a cluttered table",
    "checked": true,
    "id": "b445e8f3d812f59b68f6ed70890d14a882f99b89",
    "semantic_title": "learning sparse relational transition models",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=rkxoNnC5FQ": {
    "title": "SPIGAN: Privileged Adversarial Learning from Simulation",
    "volume": "poster",
    "abstract": "Deep Learning for Computer Vision depends mainly on the source of supervision. Photo-realistic simulators can generate large-scale automatically labeled synthetic data, but introduce a domain gap negatively impacting performance. We propose a new unsupervised domain adaptation algorithm, called SPIGAN, relying on Simulator Privileged Information (PI) and Generative Adversarial Networks (GAN). We use internal data from the simulator as PI during the training of a target task network. We experimentally evaluate our approach on semantic segmentation. We train the networks on real-world Cityscapes and Vistas datasets, using only unlabeled real-world images and synthetic labeled data with z-buffer (depth) PI from the SYNTHIA dataset. Our method improves over no adaptation and state-of-the-art unsupervised domain adaptation techniques",
    "checked": true,
    "id": "1962791d77a5f7555e8921ab9e92f7cd1fd95736",
    "semantic_title": "spigan: privileged adversarial learning from simulation",
    "citation_count": 105,
    "authors": []
  },
  "https://openreview.net/forum?id=Syx5V2CcFm": {
    "title": "Universal Stagewise Learning for Non-Convex Problems with Convergence on Averaged Solutions",
    "volume": "poster",
    "abstract": "Although stochastic gradient descent (SGD) method and its variants (e.g., stochastic momentum methods, AdaGrad) are algorithms of choice for solving non-convex problems (especially deep learning), big gaps still remain between the theory and the practice with many questions unresolved. For example, there is still a lack of theories of convergence for SGD and its variants that use stagewise step size and return an averaged solution in practice. In addition, theoretical insights of why adaptive step size of AdaGrad could improve non-adaptive step size of SGD is still missing for non-convex optimization. This paper aims to address these questions and fill the gap between theory and practice. We propose a universal stagewise optimization framework for a broad family of non-smooth non-convex problems with the following key features: (i) at each stage any suitable stochastic convex optimization algorithms (e.g., SGD or AdaGrad) that return an averaged solution can be employed for minimizing a regularized convex problem; (ii) the step size is decreased in a stagewise manner; (iii) an averaged solution is returned as the final solution. % that is selected from all stagewise averaged solutions with sampling probabilities increasing as the stage number. Our theoretical results of stagewise {\\ada} exhibit its adaptive convergence, therefore shed insights on its faster convergence than stagewise SGD for problems with slowly growing cumulative stochastic gradients. To the best of our knowledge, these new results are the first of their kind for addressing the unresolved issues of existing theories mentioned earlier. Besides theoretical contributions, our empirical studies show that our stagewise variants of SGD, AdaGrad improve the generalization performance of existing variants/implementations of SGD and AdaGrad",
    "checked": true,
    "id": "a048662aab04b4f683cb6778100356892527fb1b",
    "semantic_title": "universal stagewise learning for non-convex problems with convergence on averaged solutions",
    "citation_count": 58,
    "authors": []
  },
  "https://openreview.net/forum?id=HJx9EhC9tQ": {
    "title": "Reasoning About Physical Interactions with Object-Oriented Prediction and Planning",
    "volume": "poster",
    "abstract": "Object-based factorizations provide a useful level of abstraction for interacting with the world. Building explicit object representations, however, often requires supervisory signals that are difficult to obtain in practice. We present a paradigm for learning object-centric representations for physical scene understanding without direct supervision of object properties. Our model, Object-Oriented Prediction and Planning (O2P2), jointly learns a perception function to map from image observations to object representations, a pairwise physics interaction function to predict the time evolution of a collection of objects, and a rendering function to map objects back to pixels. For evaluation, we consider not only the accuracy of the physical predictions of the model, but also its utility for downstream tasks that require an actionable representation of intuitive physics. After training our model on an image prediction task, we can use its learned representations to build block towers more complicated than those observed during training",
    "checked": true,
    "id": "91cb47c26dffa7e3fcc339abdaab9fa229b37d95",
    "semantic_title": "reasoning about physical interactions with object-oriented prediction and planning",
    "citation_count": 130,
    "authors": []
  },
  "https://openreview.net/forum?id=BkltNhC9FX": {
    "title": "Posterior Attention Models for Sequence to Sequence Learning",
    "volume": "poster",
    "abstract": "Modern neural architectures critically rely on attention for mapping structured inputs to sequences. In this paper we show that prevalent attention architectures do not adequately model the dependence among the attention and output tokens across a predicted sequence. We present an alternative architecture called Posterior Attention Models that after a principled factorization of the full joint distribution of the attention and output variables, proposes two major changes. First, the position where attention is marginalized is changed from the input to the output. Second, the attention propagated to the next decoding stage is a posterior attention distribution conditioned on the output. Empirically on five translation and two morphological inflection tasks the proposed posterior attention models yield better BLEU score and alignment accuracy than existing attention models",
    "checked": true,
    "id": "086cef88cbac0b733e1d0b7d4756600df953851f",
    "semantic_title": "posterior attention models for sequence to sequence learning",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=HJeu43ActQ": {
    "title": "NOODL: Provable Online Dictionary Learning and Sparse Coding",
    "volume": "poster",
    "abstract": "We consider the dictionary learning problem, where the aim is to model the given data as a linear combination of a few columns of a matrix known as a dictionary, where the sparse weights forming the linear combination are known as coefficients. Since the dictionary and coefficients, parameterizing the linear model are unknown, the corresponding optimization is inherently non-convex. This was a major challenge until recently, when provable algorithms for dictionary learning were proposed. Yet, these provide guarantees only on the recovery of the dictionary, without explicit recovery guarantees on the coefficients. Moreover, any estimation error in the dictionary adversely impacts the ability to successfully localize and estimate the coefficients. This potentially limits the utility of existing provable dictionary learning methods in applications where coefficient recovery is of interest. To this end, we develop NOODL: a simple Neurally plausible alternating Optimization-based Online Dictionary Learning algorithm, which recovers both the dictionary and coefficients exactly at a geometric rate, when initialized appropriately. Our algorithm, NOODL, is also scalable and amenable for large scale distributed implementations in neural architectures, by which we mean that it only involves simple linear and non-linear operations. Finally, we corroborate these theoretical results via experimental evaluation of the proposed algorithm with the current state-of-the-art techniques",
    "checked": true,
    "id": "23e4f9ee3c7f6d201081cc895ddb5f33476b9058",
    "semantic_title": "noodl: provable online dictionary learning and sparse coding",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=rJedV3R5tm": {
    "title": "RelGAN: Relational Generative Adversarial Networks for Text Generation",
    "volume": "poster",
    "abstract": "Generative adversarial networks (GANs) have achieved great success at generating realistic images. However, the text generation still remains a challenging task for modern GAN architectures. In this work, we propose RelGAN, a new GAN architecture for text generation, consisting of three main components: a relational memory based generator for the long-distance dependency modeling, the Gumbel-Softmax relaxation for training GANs on discrete data, and multiple embedded representations in the discriminator to provide a more informative signal for the generator updates. Our experiments show that RelGAN outperforms current state-of-the-art models in terms of sample quality and diversity, and we also reveal via ablation studies that each component of RelGAN contributes critically to its performance improvements. Moreover, a key advantage of our method, that distinguishes it from other GANs, is the ability to control the trade-off between sample quality and diversity via the use of a single adjustable parameter. Finally, RelGAN is the first architecture that makes GANs with Gumbel-Softmax relaxation succeed in generating realistic text",
    "checked": true,
    "id": "0e8b9dc48e4c8be508a7797fd7742506fe59875c",
    "semantic_title": "relgan: relational generative adversarial networks for text generation",
    "citation_count": 177,
    "authors": []
  },
  "https://openreview.net/forum?id=H1xwNhCcYm": {
    "title": "Do Deep Generative Models Know What They Don't Know?",
    "volume": "poster",
    "abstract": "A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data. A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong. Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel, out-of-distribution inputs. In this paper we challenge this assumption. We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former. Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN. To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood. We find such behavior persists even when we restrict the flows to constant-volume transformations. These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood",
    "checked": true,
    "id": "6507909a8f77c88144c3a67b9336bd1c85e84cac",
    "semantic_title": "do deep generative models know what they don't know?",
    "citation_count": 769,
    "authors": []
  },
  "https://openreview.net/forum?id=BJxvEh0cFQ": {
    "title": "K for the Price of 1: Parameter-efficient Multi-task and Transfer Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "e685efa9ab0672bd83b01cc61e5cc753aa33db68",
    "semantic_title": "k for the price of 1: parameter efficient multi-task and transfer learning",
    "citation_count": 70,
    "authors": []
  },
  "https://openreview.net/forum?id=S1lDV3RcKm": {
    "title": "MisGAN: Learning from Incomplete Data with Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0a869336c65185f078ba473d7ca5b86a371ab929",
    "semantic_title": "misgan: learning from incomplete data with generative adversarial networks",
    "citation_count": 172,
    "authors": []
  },
  "https://openreview.net/forum?id=S1xLN3C9YX": {
    "title": "Learnable Embedding Space for Efficient Neural Architecture Compression",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8111960524288d0e33079be9249d73aab7667c85",
    "semantic_title": "learnable embedding space for efficient neural architecture compression",
    "citation_count": 43,
    "authors": []
  },
  "https://openreview.net/forum?id=HkgSEnA5KQ": {
    "title": "Guiding Policies with Language via Meta-Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "34f5d2f039558ba0b6a5103553ad68321fa6eabd",
    "semantic_title": "guiding policies with language via meta-learning",
    "citation_count": 65,
    "authors": []
  },
  "https://openreview.net/forum?id=HJfSEnRqKQ": {
    "title": "Active Learning with Partial Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "58c04126a5196deb57ae31d6174cd4aae154f138",
    "semantic_title": "active learning with partial feedback",
    "citation_count": 67,
    "authors": []
  },
  "https://openreview.net/forum?id=S1xNEhR9KX": {
    "title": "On the Sensitivity of Adversarial Robustness to Input Data Distributions",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "508dad538d5c63eb6c07fd10794510357a951a58",
    "semantic_title": "on the sensitivity of adversarial robustness to input data distributions",
    "citation_count": 60,
    "authors": []
  },
  "https://openreview.net/forum?id=ByME42AqK7": {
    "title": "Efficient Multi-Objective Neural Architecture Search via Lamarckian Evolution",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "77e5aa8c33a9cb9dd3f0874b09fd84389360e88e",
    "semantic_title": "efficient multi-objective neural architecture search via lamarckian evolution",
    "citation_count": 504,
    "authors": []
  },
  "https://openreview.net/forum?id=SklEEnC5tQ": {
    "title": "DISTRIBUTIONAL CONCAVITY REGULARIZATION FOR GANS",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "81473ba213bae1aa188e03e54f91319914159c9c",
    "semantic_title": "distributional concavity regularization for gans",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=r1g4E3C9t7": {
    "title": "Characterizing Audio Adversarial Examples Using Temporal Dependency",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d5c92fafb030f76d27c749e36550c390c8b3d63b",
    "semantic_title": "characterizing audio adversarial examples using temporal dependency",
    "citation_count": 165,
    "authors": []
  },
  "https://openreview.net/forum?id=H1xQVn09FX": {
    "title": "GANSynth: Adversarial Neural Audio Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "725c650ae8db8d9a57e4a7b15a555dbe69b67054",
    "semantic_title": "gansynth: adversarial neural audio synthesis",
    "citation_count": 395,
    "authors": []
  },
  "https://openreview.net/forum?id=BylQV305YQ": {
    "title": "Toward Understanding the Impact of Staleness in Distributed Machine Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "39f593a6ba742751f46011d97661d20d7437d016",
    "semantic_title": "toward understanding the impact of staleness in distributed machine learning",
    "citation_count": 84,
    "authors": []
  },
  "https://openreview.net/forum?id=r1xX42R5Fm": {
    "title": "Beyond Greedy Ranking: Slate Optimization via List-CVAE",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7fb66090fe4b7e9a56a91bf977e6f40b49d569ac",
    "semantic_title": "beyond greedy ranking: slate optimization via list-cvae",
    "citation_count": 50,
    "authors": []
  },
  "https://openreview.net/forum?id=SyxfEn09Y7": {
    "title": "G-SGD: Optimizing ReLU Neural Networks in its Positively Scale-Invariant Space",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "15cfd11abb010eb2d84dbbe9b80dd80f2e98922d",
    "semantic_title": "g-sgd: optimizing relu neural networks in its positively scale-invariant space",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=Bkl-43C9FQ": {
    "title": "Spherical CNNs on Unstructured Grids",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4aa2c7ab9646b9f988d2dd2e2375060b3158dc2a",
    "semantic_title": "spherical cnns on unstructured grids",
    "citation_count": 188,
    "authors": []
  },
  "https://openreview.net/forum?id=HJgeEh09KQ": {
    "title": "Boosting Robustness Certification of Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4963fe1027aebe63217bd2904decf24f59379e1f",
    "semantic_title": "boosting robustness certification of neural networks",
    "citation_count": 171,
    "authors": []
  },
  "https://openreview.net/forum?id=rJleN20qK7": {
    "title": "Two-Timescale Networks for Nonlinear Value Function Approximation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e4eed0e966c134bbff6a5a2a38ac2b3e44900906",
    "semantic_title": "two-timescale networks for nonlinear value function approximation",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=BJlgNh0qKQ": {
    "title": "Differentiable Perturb-and-Parse: Semi-Supervised Parsing with a Structured Variational Autoencoder",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "29927b4e90810e05c3f329a3a9324d4f6859a93c",
    "semantic_title": "differentiable perturb-and-parse: semi-supervised parsing with a structured variational autoencoder",
    "citation_count": 56,
    "authors": []
  },
  "https://openreview.net/forum?id=BJe1E2R5KX": {
    "title": "Algorithmic Framework for Model-based Deep Reinforcement Learning with Theoretical Guarantees",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d333f99881b09426283a9c7a1d25f7ac30d63062",
    "semantic_title": "algorithmic framework for model-based deep reinforcement learning with theoretical guarantees",
    "citation_count": 230,
    "authors": []
  },
  "https://openreview.net/forum?id=HkzRQhR9YX": {
    "title": "Tree-Structured Recurrent Switching Linear Dynamical Systems for Multi-Scale Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "349f1028e6587604108fd720025cf4b190da2dec",
    "semantic_title": "tree-structured recurrent switching linear dynamical systems for multi-scale modeling",
    "citation_count": 75,
    "authors": []
  },
  "https://openreview.net/forum?id=B1e0X3C9tQ": {
    "title": "Diagnosing and Enhancing VAE Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f6a201eed70e8b48e2f60d97c98cfc8fe3b7b175",
    "semantic_title": "diagnosing and enhancing vae models",
    "citation_count": 387,
    "authors": []
  },
  "https://openreview.net/forum?id=HylTXn0qYX": {
    "title": "Efficiently testing local optimality and escaping saddles for ReLU networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "04a98e1fd56e0d66eeb5f66e5b6a4d310759b3eb",
    "semantic_title": "efficiently testing local optimality and escaping saddles for relu networks",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=HJE6X305Fm": {
    "title": "Don't let your Discriminator be fooled",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1cbbaa7c718cb9abae10955a0ad6dcd3b1e123bb",
    "semantic_title": "don't let your discriminator be fooled",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=B1xhQhRcK7": {
    "title": "Rigorous Agent Evaluation: An Adversarial Approach to Uncover Catastrophic Failures",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5ad2c21ad1c3267e4ce484543bb699d558066d9a",
    "semantic_title": "rigorous agent evaluation: an adversarial approach to uncover catastrophic failures",
    "citation_count": 84,
    "authors": []
  },
  "https://openreview.net/forum?id=Sklsm20ctX": {
    "title": "Competitive experience replay",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ee248883b124a80bdbbc686e906145ccdeadc834",
    "semantic_title": "competitive experience replay",
    "citation_count": 53,
    "authors": []
  },
  "https://openreview.net/forum?id=BJej72AqF7": {
    "title": "A Max-Affine Spline Perspective of Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1bd7d872d54a650affb8b0e1101c0c52e2a912da",
    "semantic_title": "a max-affine spline perspective of recurrent neural networks",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=Byg5QhR5FQ": {
    "title": "Top-Down Neural Model For Formulae",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f6b394f714ef1b9ace8879356c1db2badd21ba82",
    "semantic_title": "top-down neural model for formulae",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=S1ecm2C9K7": {
    "title": "Feature-Wise Bias Amplification",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "941ac3f9c1402bbbc1c61f328eb242e99cde5a43",
    "semantic_title": "feature-wise bias amplification",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=HkgYmhR9KX": {
    "title": "AD-VAT: An Asymmetric Dueling mechanism for learning Visual Active Tracking",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1433d442139fd876001093f5a02fdf9f4f07425a",
    "semantic_title": "ad-vat: an asymmetric dueling mechanism for learning visual active tracking",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=BJfOXnActQ": {
    "title": "Learning to Learn with Conditional Class Dependencies",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "cb80484b34f62d9d7db0bb9214c5374f6bcb861d",
    "semantic_title": "learning to learn with conditional class dependencies",
    "citation_count": 82,
    "authors": []
  },
  "https://openreview.net/forum?id=Hyg_X2C5FX": {
    "title": "GAN Dissection: Visualizing and Understanding Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "df7ad8eeb595da5f7774e91dae06075be952acff",
    "semantic_title": "gan dissection: visualizing and understanding generative adversarial networks",
    "citation_count": 471,
    "authors": []
  },
  "https://openreview.net/forum?id=S1lvm305YQ": {
    "title": "TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre Transfer",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f1fc78d559a4b6cf02fd2bbe579b9473ae4ae213",
    "semantic_title": "timbretron: a wavenet(cyclegan(cqt(audio))) pipeline for musical timbre transfer",
    "citation_count": 98,
    "authors": []
  },
  "https://openreview.net/forum?id=SyMDXnCcF7": {
    "title": "A Mean Field Theory of Batch Normalization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e5b7c1ce5a46e059fce96249c0c034afdd3c287a",
    "semantic_title": "a mean field theory of batch normalization",
    "citation_count": 182,
    "authors": []
  },
  "https://openreview.net/forum?id=HkxLXnAcFQ": {
    "title": "A Closer Look at Few-shot Classification",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9d5ec23154fb278a765f47ba5ee5150bd441d0de",
    "semantic_title": "a closer look at few-shot classification",
    "citation_count": 1787,
    "authors": []
  },
  "https://openreview.net/forum?id=HkzSQhCcK7": {
    "title": "STCN: Stochastic Temporal Convolutional Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b9357ef8753b07253b144617e0ef798569192bed",
    "semantic_title": "stcn: stochastic temporal convolutional networks",
    "citation_count": 63,
    "authors": []
  },
  "https://openreview.net/forum?id=HkgEQnRqYQ": {
    "title": "RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space",
    "volume": "poster",
    "abstract": "We study the problem of learning representations of entities and relations in knowledge graphs for predicting missing links. The success of such a task heavily relies on the ability of modeling and inferring the patterns of (or between) the relations. In this paper, we present a new approach for knowledge graph embedding called RotatE, which is able to model and infer various relation patterns including: symmetry/antisymmetry, inversion, and composition. Specifically, the RotatE model defines each relation as a rotation from the source entity to the target entity in the complex vector space. In addition, we propose a novel self-adversarial negative sampling technique for efficiently and effectively training the RotatE model. Experimental results on multiple benchmark knowledge graphs show that the proposed RotatE model is not only scalable, but also able to infer and model various relation patterns and significantly outperform existing state-of-the-art models for link prediction",
    "checked": true,
    "id": "8f096071a09701012c9c279aee2a88143a295935",
    "semantic_title": "rotate: knowledge graph embedding by relational rotation in complex space",
    "citation_count": 2190,
    "authors": []
  },
  "https://openreview.net/forum?id=BJemQ209FQ": {
    "title": "Learning to Navigate the Web",
    "volume": "poster",
    "abstract": "Learning in environments with large state and action spaces, and sparse rewards, can hinder a Reinforcement Learning (RL) agent's learning through trial-and-error. For instance, following natural language instructions on the Web (such as booking a flight ticket) leads to RL settings where input vocabulary and number of actionable elements on a page can grow very large. Even though recent approaches improve the success rate on relatively simple environments with the help of human demonstrations to guide the exploration, they still fail in environments where the set of possible instructions can reach millions. We approach the aforementioned problems from a different perspective and propose guided RL approaches that can generate unbounded amount of experience for an agent to learn from. Instead of learning from a complicated instruction with a large vocabulary, we decompose it into multiple sub-instructions and schedule a curriculum in which an agent is tasked with a gradually increasing subset of these relatively easier sub-instructions. In addition, when the expert demonstrations are not available, we propose a novel meta-learning framework that generates new instruction following tasks and trains the agent more effectively. We train DQN, deep reinforcement learning agent, with Q-value function approximated with a novel QWeb neural network architecture on these smaller, synthetic instructions. We evaluate the ability of our agent to generalize to new instructions onWorld of Bits benchmark, on forms with up to 100 elements, supporting 14 million possible instructions. The QWeb agent outperforms the baseline without using any human demonstration achieving 100% success rate on several difficult environments",
    "checked": true,
    "id": "a7038473320c50df76fa950aca486015c5659503",
    "semantic_title": "learning to navigate the web",
    "citation_count": 65,
    "authors": []
  },
  "https://openreview.net/forum?id=r1xQQhAqKX": {
    "title": "Modeling Uncertainty with Hedged Instance Embeddings",
    "volume": "poster",
    "abstract": "Instance embeddings are an efficient and versatile image representation that facilitates applications like recognition, verification, retrieval, and clustering. Many metric learning methods represent the input as a single point in the embedding space. Often the distance between points is used as a proxy for match confidence. However, this can fail to represent uncertainty which can arise when the input is ambiguous, e.g., due to occlusion or blurriness. This work addresses this issue and explicitly models the uncertainty by \"hedging\" the location of each input in the embedding space. We introduce the hedged instance embedding (HIB) in which embeddings are modeled as random variables and the model is trained under the variational information bottleneck principle (Alemi et al., 2016; Achille & Soatto, 2018). Empirical results on our new N-digit MNIST dataset show that our method leads to the desired behavior of \"hedging its bets\" across the embedding space upon encountering ambiguous inputs. This results in improved performance for image matching and classification tasks, more structure in the learned embedding space, and an ability to compute a per-exemplar uncertainty measure which is correlated with downstream performance",
    "checked": true,
    "id": "99e702dead5fa8e7edbb95d8f27925000bd9b145",
    "semantic_title": "modeling uncertainty with hedged instance embeddings",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=B1ffQnRcKX": {
    "title": "Automatically Composing Representation Transformations as a Means for Generalization",
    "volume": "poster",
    "abstract": "A generally intelligent learner should generalize to more complex tasks than it has previously encountered, but the two common paradigms in machine learning -- either training a separate learner per task or training a single learner for all tasks -- both have difficulty with such generalization because they do not leverage the compositional structure of the task distribution. This paper introduces the compositional problem graph as a broadly applicable formalism to relate tasks of different complexity in terms of problems with shared subproblems. We propose the compositional generalization problem for measuring how readily old knowledge can be reused and hence built upon. As a first step for tackling compositional generalization, we introduce the compositional recursive learner, a domain-general framework for learning algorithmic procedures for composing representation transformations, producing a learner that reasons about what computation to execute by making analogies to previously seen problems. We show on a symbolic and a high-dimensional domain that our compositional approach can generalize to more complex problems than the learner has previously encountered, whereas baselines that are not explicitly compositional do not",
    "checked": true,
    "id": "1766648967f6206a944a4bd18bbbd92a74c164bd",
    "semantic_title": "automatically composing representation transformations as a means for generalization",
    "citation_count": 71,
    "authors": []
  },
  "https://openreview.net/forum?id=HkezXnA9YX": {
    "title": "Systematic Generalization: What Is Required and Can It Be Learned?",
    "volume": "poster",
    "abstract": "Numerous models for grounded language understanding have been recently proposed, including (i) generic models that can be easily adapted to any given task and (ii) intuitively appealing modular models that require background knowledge to be instantiated. We compare both types of models in how much they lend themselves to a particular form of systematic generalization. Using a synthetic VQA test, we evaluate which models are capable of reasoning about all possible object pairs after training on only a small subset of them. Our findings show that the generalization of modular models is much more systematic and that it is highly sensitive to the module layout, i.e. to how exactly the modules are connected. We furthermore investigate if modular models that generalize well could be made more end-to-end by learning their layout and parametrization. We find that end-to-end methods from prior work often learn inappropriate layouts or parametrizations that do not facilitate systematic generalization. Our results suggest that, in addition to modularity, systematic generalization in language understanding may require explicit regularizers or priors",
    "checked": null,
    "id": "6c7494a47cc5421a7b636c244e13586dc2dab007",
    "semantic_title": "systematic generalization: what is required and can it be learned?",
    "citation_count": 162,
    "authors": []
  },
  "https://openreview.net/forum?id=ByxZX20qFQ": {
    "title": "Adaptive Input Representations for Neural Language Modeling",
    "volume": "poster",
    "abstract": "We introduce adaptive input representations for neural language modeling which extend the adaptive softmax of Grave et al. (2017) to input representations of variable capacity. There are several choices on how to factorize the input and output layers, and whether to model words, characters or sub-word units. We perform a systematic comparison of popular choices for a self-attentional architecture. Our experiments show that models equipped with adaptive embeddings are more than twice as fast to train than the popular character input CNN while having a lower number of parameters. On the WikiText-103 benchmark we achieve 18.7 perplexity, an improvement of 10.5 perplexity compared to the previously best published result and on the Billion Word benchmark, we achieve 23.02 perplexity",
    "checked": true,
    "id": "d170bd486e4c0fe82601e322b0e9e0dde63ab299",
    "semantic_title": "adaptive input representations for neural language modeling",
    "citation_count": 394,
    "authors": []
  },
  "https://openreview.net/forum?id=H1MW72AcK7": {
    "title": "Optimal Control Via Neural Networks: A Convex Approach",
    "volume": "poster",
    "abstract": "Control of complex systems involves both system identification and controller design. Deep neural networks have proven to be successful in many identification tasks, however, from model-based control perspective, these networks are difficult to work with because they are typically nonlinear and nonconvex. Therefore many systems are still identified and controlled based on simple linear models despite their poor representation capability. In this paper we bridge the gap between model accuracy and control tractability faced by neural networks, by explicitly constructing networks that are convex with respect to their inputs. We show that these input convex networks can be trained to obtain accurate models of complex physical systems. In particular, we design input convex recurrent neural networks to capture temporal behavior of dynamical systems. Then optimal controllers can be achieved via solving a convex model predictive control problem. Experiment results demonstrate the good potential of the proposed input convex neural network based approach in a variety of control applications. In particular we show that in the MuJoCo locomotion tasks, we could achieve over 10% higher performance using 5 times less time compared with state-of-the-art model-based reinforcement learning method; and in the building HVAC control example, our method achieved up to 20% energy reduction compared with classic linear models",
    "checked": true,
    "id": "e20b0a274d62a94fb1259ce3f2166ecae4673a7e",
    "semantic_title": "optimal control via neural networks: a convex approach",
    "citation_count": 192,
    "authors": []
  },
  "https://openreview.net/forum?id=BJlxm30cKm": {
    "title": "An Empirical Study of Example Forgetting during Deep Neural Network Learning",
    "volume": "poster",
    "abstract": "Inspired by the phenomenon of catastrophic forgetting, we investigate the learning dynamics of neural networks as they train on single classification tasks. Our goal is to understand whether a related phenomenon occurs when data does not undergo a clear distributional shift. We define a ``forgetting event'' to have occurred when an individual training example transitions from being classified correctly to incorrectly over the course of learning. Across several benchmark data sets, we find that: (i) certain examples are forgotten with high frequency, and some not at all; (ii) a data set's (un)forgettable examples generalize across neural architectures; and (iii) based on forgetting dynamics, a significant fraction of examples can be omitted from the training data set while still maintaining state-of-the-art generalization performance",
    "checked": true,
    "id": "a2b5d224895d96bfe2e384e2dcf1ebd136ac3782",
    "semantic_title": "an empirical study of example forgetting during deep neural network learning",
    "citation_count": 760,
    "authors": []
  },
  "https://openreview.net/forum?id=rJ4km2R5t7": {
    "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    "volume": "poster",
    "abstract": "For natural language understanding (NLU) technology to be maximally useful, it must be able to process language in a way that is not exclusive to a single task, genre, or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation (GLUE) benchmark, a collection of tools for evaluating the performance of models across a diverse set of existing NLU tasks. By including tasks with limited training data, GLUE is designed to favor and encourage models that share general linguistic knowledge across tasks. GLUE also includes a hand-crafted diagnostic test suite that enables detailed linguistic analysis of models. We evaluate baselines based on current methods for transfer and representation learning and find that multi-task training on all tasks performs better than training a separate model per task. However, the low absolute performance of our best model indicates the need for improved general NLU systems",
    "checked": true,
    "id": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c",
    "semantic_title": "glue: a multi-task benchmark and analysis platform for natural language understanding",
    "citation_count": 7299,
    "authors": []
  },
  "https://openreview.net/forum?id=Byey7n05FQ": {
    "title": "Plan Online, Learn Offline: Efficient Learning and Exploration via Model-Based Control",
    "volume": "poster",
    "abstract": "We propose a \"plan online and learn offline\" framework for the setting where an agent, with an internal model, needs to continually act and learn in the world. Our work builds on the synergistic relationship between local model-based control, global value function learning, and exploration. We study how local trajectory optimization can cope with approximation errors in the value function, and can stabilize and accelerate value function learning. Conversely, we also study how approximate value functions can help reduce the planning horizon and allow for better policies beyond local solutions. Finally, we also demonstrate how trajectory optimization can be used to perform temporally coordinated exploration in conjunction with estimating uncertainty in value function approximation. This exploration is critical for fast and stable learning of the value function. Combining these components enable solutions to complex control tasks, like humanoid locomotion and dexterous in-hand manipulation, in the equivalent of a few minutes of experience in the real world",
    "checked": true,
    "id": "6a9013a8cdd84e423223f76a903028011c84c4ab",
    "semantic_title": "plan online, learn offline: efficient learning and exploration via model-based control",
    "citation_count": 233,
    "authors": []
  },
  "https://openreview.net/forum?id=Syx0Mh05YQ": {
    "title": "Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion",
    "volume": "poster",
    "abstract": "This paper proposes a representational model for grid cells. In this model, the 2D self-position of the agent is represented by a high-dimensional vector, and the 2D self-motion or displacement of the agent is represented by a matrix that transforms the vector. Each component of the vector is a unit or a cell. The model consists of the following three sub-models. (1) Vector-matrix multiplication. The movement from the current position to the next position is modeled by matrix-vector multi- plication, i.e., the vector of the next position is obtained by multiplying the matrix of the motion to the vector of the current position. (2) Magnified local isometry. The angle between two nearby vectors equals the Euclidean distance between the two corresponding positions multiplied by a magnifying factor. (3) Global adjacency kernel. The inner product between two vectors measures the adjacency between the two corresponding positions, which is defined by a kernel function of the Euclidean distance between the two positions. Our representational model has explicit algebra and geometry. It can learn hexagon patterns of grid cells, and it is capable of error correction, path integral and path planning",
    "checked": true,
    "id": "2abc9d603a87b300a251a3796a12b8a2d21746df",
    "semantic_title": "learning grid cells as vector representation of self-position coupled with matrix representation of self-motion",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=BJe0Gn0cY7": {
    "title": "Preventing Posterior Collapse with delta-VAEs",
    "volume": "poster",
    "abstract": "Due to the phenomenon of \"posterior collapse,\" current latent variable generative models pose a challenging design choice that either weakens the capacity of the decoder or requires altering the training objective. We develop an alternative that utilizes the most powerful generative models as decoders, optimize the variational lower bound, and ensures that the latent variables preserve and encode useful information. Our proposed Œ¥-VAEs achieve this by constraining the variational family for the posterior to have a minimum distance to the prior. For sequential latent variable models, our approach resembles the classic representation learning approach of slow feature analysis. We demonstrate our method's efficacy at modeling text on LM1B and modeling images: learning representations, improving sample quality, and achieving state of the art log-likelihood on CIFAR-10 and ImageNet 32 √ó 32",
    "checked": true,
    "id": "097b7ad748e0f5fb8eeae9fec9ccb344801f87e1",
    "semantic_title": "preventing posterior collapse with delta-vaes",
    "citation_count": 172,
    "authors": []
  },
  "https://openreview.net/forum?id=HyxAfnA5tm": {
    "title": "Deep Online Learning Via Meta-Learning: Continual Adaptation for Model-Based RL",
    "volume": "poster",
    "abstract": "Humans and animals can learn complex predictive models that allow them to accurately and reliably reason about real-world phenomena, and they can adapt such models extremely quickly in the face of unexpected changes. Deep neural network models allow us to represent very complex functions, but lack this capacity for rapid online adaptation. The goal in this paper is to develop a method for continual online learning from an incoming stream of data, using deep neural network models. We formulate an online learning procedure that uses stochastic gradient descent to update model parameters, and an expectation maximization algorithm with a Chinese restaurant process prior to develop and maintain a mixture of models to handle non-stationary task distributions. This allows for all models to be adapted as necessary, with new models instantiated for task changes and old models recalled when previously seen tasks are encountered again. Furthermore, we observe that meta-learning can be used to meta-train a model such that this direct online adaptation with SGD is effective, which is otherwise not the case for large function approximators. We apply our method to model-based reinforcement learning, where adapting the predictive model is critical for control; we demonstrate that our online learning via meta-learning algorithm outperforms alternative prior methods, and enables effective continuous adaptation in non-stationary task distributions such as varying terrains, motor failures, and unexpected disturbances",
    "checked": true,
    "id": "42de54e614110c0c0a0bbbfee045e11e53eb4a7d",
    "semantic_title": "deep online learning via meta-learning: continual adaptation for model-based rl",
    "citation_count": 194,
    "authors": []
  },
  "https://openreview.net/forum?id=SJG6G2RqtX": {
    "title": "Value Propagation Networks",
    "volume": "poster",
    "abstract": "We present Value Propagation (VProp), a set of parameter-efficient differentiable planning modules built on Value Iteration which can successfully be trained using reinforcement learning to solve unseen tasks, has the capability to generalize to larger map sizes, and can learn to navigate in dynamic environments. We show that the modules enable learning to plan when the environment also includes stochastic elements, providing a cost-efficient learning system to build low-level size-invariant planners for a variety of interactive navigation problems. We evaluate on static and dynamic configurations of MazeBase grid-worlds, with randomly generated environments of several different sizes, and on a StarCraft navigation scenario, with more complex dynamics, and pixels as input",
    "checked": true,
    "id": "e2662442770457da538b90cb9a0d42782059ef8c",
    "semantic_title": "value propagation networks",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=Byxpfh0cFm": {
    "title": "Efficient Augmentation via Data Subsampling",
    "volume": "poster",
    "abstract": "Data augmentation is commonly used to encode invariances in learning methods. However, this process is often performed in an inefficient manner, as artificial examples are created by applying a number of transformations to all points in the training set. The resulting explosion of the dataset size can be an issue in terms of storage and training costs, as well as in selecting and tuning the optimal set of transformations to apply. In this work, we demonstrate that it is possible to significantly reduce the number of data points included in data augmentation while realizing the same accuracy and invariance benefits of augmenting the entire dataset. We propose a novel set of subsampling policies, based on model influence and loss, that can achieve a 90% reduction in augmentation set size while maintaining the accuracy gains of standard data augmentation",
    "checked": true,
    "id": "526ef02a3a4624f1dbd0f2ce51778eb92a893e8a",
    "semantic_title": "efficient augmentation via data subsampling",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=B1lnzn0ctQ": {
    "title": "ALISTA: Analytic Weights Are As Good As Learned Weights in LISTA",
    "volume": "poster",
    "abstract": "Deep neural networks based on unfolding an iterative algorithm, for example, LISTA (learned iterative shrinkage thresholding algorithm), have been an empirical success for sparse signal recovery. The weights of these neural networks are currently determined by data-driven \"black-box\" training. In this work, we propose Analytic LISTA (ALISTA), where the weight matrix in LISTA is computed as the solution to a data-free optimization problem, leaving only the stepsize and threshold parameters to data-driven learning. This signiÔ¨Åcantly simpliÔ¨Åes the training. SpeciÔ¨Åcally, the data-free optimization problem is based on coherence minimization. We show our ALISTA retains the optimal linear convergence proved in (Chen et al., 2018) and has a performance comparable to LISTA. Furthermore, we extend ALISTA to convolutional linear operators, again determined in a data-free manner. We also propose a feed-forward framework that combines the data-free optimization and ALISTA networks from end to end, one that can be jointly trained to gain robustness to small perturbations in the encoding model",
    "checked": true,
    "id": "e15e022cb7874912313e2601d38b944fc93bbdd9",
    "semantic_title": "alista: analytic weights are as good as learned weights in lista",
    "citation_count": 181,
    "authors": []
  },
  "https://openreview.net/forum?id=HygsfnR9Ym": {
    "title": "Recall Traces: Backtracking Models for Efficient Reinforcement Learning",
    "volume": "poster",
    "abstract": "In many environments only a tiny subset of all states yield high reward. In these cases, few of the interactions with the environment provide a relevant learning signal. Hence, we may want to preferentially train on those high-reward states and the probable trajectories leading to them. To this end, we advocate for the use of a \\textit{backtracking model} that predicts the preceding states that terminate at a given high-reward state. We can train a model which, starting from a high value state (or one that is estimated to have high value), predicts and samples which (state, action)-tuples may have led to that high value state. These traces of (state, action) pairs, which we refer to as Recall Traces, sampled from this backtracking model starting from a high value state, are informative as they terminate in good states, and hence we can use these traces to improve a policy. We provide a variational interpretation for this idea and a practical algorithm in which the backtracking model samples from an approximate posterior distribution over trajectories which lead to large rewards. Our method improves the sample efficiency of both on- and off-policy RL algorithms across several environments and tasks",
    "checked": true,
    "id": "29aab768e642588352134a03c0368e1bdc1f1e8d",
    "semantic_title": "recall traces: backtracking models for efficient reinforcement learning",
    "citation_count": 69,
    "authors": []
  },
  "https://openreview.net/forum?id=H1gsz30cKX": {
    "title": "Fixup Initialization: Residual Learning Without Normalization",
    "volume": "poster",
    "abstract": "Normalization layers are a staple in state-of-the-art deep neural network architectures. They are widely believed to stabilize training, enable higher learning rate, accelerate convergence and improve generalization, though the reason for their effectiveness is still an active research topic. In this work, we challenge the commonly-held beliefs by showing that none of the perceived benefits is unique to normalization. Specifically, we propose fixed-update initialization (Fixup), an initialization motivated by solving the exploding and vanishing gradient problem at the beginning of training via properly rescaling a standard initialization. We find training residual networks with Fixup to be as stable as training with normalization -- even for networks with 10,000 layers. Furthermore, with proper regularization, Fixup enables residual networks without normalization to achieve state-of-the-art performance in image classification and machine translation",
    "checked": true,
    "id": "96c82727dd5a80fef93007f888bb8569feb6bd85",
    "semantic_title": "fixup initialization: residual learning without normalization",
    "citation_count": 351,
    "authors": []
  },
  "https://openreview.net/forum?id=rJliMh09F7": {
    "title": "Diversity-Sensitive Conditional Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "We propose a simple yet highly effective method that addresses the mode-collapse problem in the Conditional Generative Adversarial Network (cGAN). Although conditional distributions are multi-modal (i.e., having many modes) in practice, most cGAN approaches tend to learn an overly simplified distribution where an input is always mapped to a single output regardless of variations in latent code. To address such issue, we propose to explicitly regularize the generator to produce diverse outputs depending on latent codes. The proposed regularization is simple, general, and can be easily integrated into most conditional GAN objectives. Additionally, explicit regularization on generator allows our method to control a balance between visual quality and diversity. We demonstrate the effectiveness of our method on three conditional generation tasks: image-to-image translation, image inpainting, and future video prediction. We show that simple addition of our regularization to existing models leads to surprisingly diverse generations, substantially outperforming the previous approaches for multi-modal conditional generation specifically designed in each individual task",
    "checked": true,
    "id": "cc8a7a229151d17b1088d059fc471da2cea718c3",
    "semantic_title": "diversity-sensitive conditional generative adversarial networks",
    "citation_count": 217,
    "authors": []
  },
  "https://openreview.net/forum?id=S1lqMn05Ym": {
    "title": "Information asymmetry in KL-regularized RL",
    "volume": "poster",
    "abstract": "Many real world tasks exhibit rich structure that is repeated across different parts of the state space or in time. In this work we study the possibility of leveraging such repeated structure to speed up and regularize learning. We start from the KL regularized expected reward objective which introduces an additional component, a default policy. Instead of relying on a fixed default policy, we learn it from data. But crucially, we restrict the amount of information the default policy receives, forcing it to learn reusable behaviors that help the policy learn faster. We formalize this strategy and discuss connections to information bottleneck approaches and to the variational EM algorithm. We present empirical results in both discrete and continuous action domains and demonstrate that, for certain tasks, learning a default policy alongside the policy can significantly speed up and improve learning. Please watch the video demonstrating learned experts and default policies on several continuous control tasks ( https://youtu.be/U2qA3llzus8 )",
    "checked": true,
    "id": "549c9dfb32e85d9ef5a48566767be42ad132a3c4",
    "semantic_title": "information asymmetry in kl-regularized rl",
    "citation_count": 103,
    "authors": []
  },
  "https://openreview.net/forum?id=ByftGnR9KX": {
    "title": "FlowQA: Grasping Flow in History for Conversational Machine Comprehension",
    "volume": "poster",
    "abstract": "Conversational machine comprehension requires a deep understanding of the conversation history. To enable traditional, single-turn models to encode the history comprehensively, we introduce Flow, a mechanism that can incorporate intermediate representations generated during the process of answering previous questions, through an alternating parallel processing structure. Compared to shallow approaches that concatenate previous questions/answers as input, Flow integrates the latent semantics of the conversation history more deeply. Our model, FlowQA, shows superior performance on two recently proposed conversational challenges (+7.2% F1 on CoQA and +4.0% on QuAC). The effectiveness of Flow also shows in other tasks. By reducing sequential instruction understanding to conversational machine comprehension, FlowQA outperforms the best models on all three domains in SCONE, with +1.8% to +4.4% improvement in accuracy",
    "checked": true,
    "id": "9065ace5366ef548cf81bd9f239f1d132c1ef412",
    "semantic_title": "flowqa: grasping flow in history for conversational machine comprehension",
    "citation_count": 99,
    "authors": []
  },
  "https://openreview.net/forum?id=ByetGn0cYX": {
    "title": "Probabilistic Planning with Sequential Monte Carlo methods",
    "volume": "poster",
    "abstract": "In this work, we propose a novel formulation of planning which views it as a probabilistic inference problem over future optimal trajectories. This enables us to use sampling methods, and thus, tackle planning in continuous domains using a fixed computational budget. We design a new algorithm, Sequential Monte Carlo Planning, by leveraging classical methods in Sequential Monte Carlo and Bayesian smoothing in the context of control as inference. Furthermore, we show that Sequential Monte Carlo Planning can capture multimodal policies and can quickly learn continuous control tasks",
    "checked": true,
    "id": "c6b040f3f8215f32e362ca7a483c48070585b029",
    "semantic_title": "probabilistic planning with sequential monte carlo methods",
    "citation_count": 46,
    "authors": []
  },
  "https://openreview.net/forum?id=SkGuG2R5tm": {
    "title": "Spreading vectors for similarity search",
    "volume": "poster",
    "abstract": "Discretizing floating-point vectors is a fundamental step of modern indexing methods. State-of-the-art techniques learn parameters of the quantizers on training data for optimal performance, thus adapting quantizers to the data. In this work, we propose to reverse this paradigm and adapt the data to the quantizer: we train a neural net whose last layers form a fixed parameter-free quantizer, such as pre-defined points of a sphere. As a proxy objective, we design and train a neural network that favors uniformity in the spherical latent space, while preserving the neighborhood structure after the mapping. For this purpose, we propose a new regularizer derived from the Kozachenko-Leonenko differential entropy estimator and combine it with a locality-aware triplet loss. Experiments show that our end-to-end approach outperforms most learned quantization methods, and is competitive with the state of the art on widely adopted benchmarks. Further more, we show that training without the quantization step results in almost no difference in accuracy, but yields a generic catalyser that can be applied with any subsequent quantization technique",
    "checked": true,
    "id": "6ac386b9f77c3e4d84c06ec8b66475b1a6eada67",
    "semantic_title": "spreading vectors for similarity search",
    "citation_count": 125,
    "authors": []
  },
  "https://openreview.net/forum?id=BkedznAqKQ": {
    "title": "LanczosNet: Multi-Scale Deep Graph Convolutional Networks",
    "volume": "poster",
    "abstract": "We propose Lanczos network (LanczosNet) which uses the Lanczos algorithm to construct low rank approximations of the graph Laplacian for graph convolution. Relying on the tridiagonal decomposition of the Lanczos algorithm, we not only efficiently exploit multi-scale information via fast approximated computation of matrix power but also design learnable spectral filters. Being fully differentiable, LanczosNet facilitates both graph kernel learning as well as learning node embeddings. We show the connection between our LanczosNet and graph based manifold learning, especially diffusion maps. We benchmark our model against $8$ recent deep graph networks on citation datasets and QM8 quantum chemistry dataset. Experimental results show that our model achieves the state-of-the-art performance in most tasks",
    "checked": true,
    "id": "5098df13be6d1f2a31c9fbf85703336ef77a9665",
    "semantic_title": "lanczosnet: multi-scale deep graph convolutional networks",
    "citation_count": 228,
    "authors": []
  },
  "https://openreview.net/forum?id=SylPMnR9Ym": {
    "title": "Learning what you can do before doing anything",
    "volume": "poster",
    "abstract": "Intelligent agents can learn to represent the action spaces of other agents simply by observing them act. Such representations help agents quickly learn to predict the effects of their own actions on the environment and to plan complex action sequences. In this work, we address the problem of learning an agent's action space purely from visual observation. We use stochastic video prediction to learn a latent variable that captures the scene's dynamics while being minimally sensitive to the scene's static content. We introduce a loss term that encourages the network to capture the composability of visual sequences and show that it leads to representations that disentangle the structure of actions. We call the full model with composable action representations Composable Learned Action Space Predictor (CLASP). We show the applicability of our method to synthetic settings and its potential to capture action spaces in complex, realistic visual settings. When used in a semi-supervised setting, our learned representations perform comparably to existing fully supervised methods on tasks such as action-conditioned video prediction and planning in the learned action space, while requiring orders of magnitude fewer action labels. Project website: https://daniilidis-group.github.io/learned_action_spaces",
    "checked": true,
    "id": "a53874f5c63b31468ad2fe3f5dea558a6ce35820",
    "semantic_title": "learning what you can do before doing anything",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=rylDfnCqF7": {
    "title": "Lagging Inference Networks and Posterior Collapse in Variational Autoencoders",
    "volume": "poster",
    "abstract": "The variational autoencoder (VAE) is a popular combination of deep latent variable model and accompanying variational learning technique. By using a neural inference network to approximate the model's posterior on latent variables, VAEs efficiently parameterize a lower bound on marginal data likelihood that can be optimized directly via gradient methods. In practice, however, VAE training often results in a degenerate local optimum known as \"posterior collapse\" where the model learns to ignore the latent variable and the approximate posterior mimics the prior. In this paper, we investigate posterior collapse from the perspective of training dynamics. We find that during the initial stages of training the inference network fails to approximate the model's true posterior, which is a moving target. As a result, the model is encouraged to ignore the latent encoding and posterior collapse occurs. Based on this observation, we propose an extremely simple modification to VAE training to reduce inference lag: depending on the model's current mutual information between latent variable and observation, we aggressively optimize the inference network before performing each model update. Despite introducing neither new model components nor significant complexity over basic VAE, our approach is able to avoid the problem of collapse that has plagued a large amount of previous work. Empirically, our approach outperforms strong autoregressive baselines on text and image benchmarks in terms of held-out likelihood, and is competitive with more complex techniques for avoiding collapse while being substantially faster",
    "checked": true,
    "id": "159078136930f3963e01d694faa1b6b51f93c7ec",
    "semantic_title": "lagging inference networks and posterior collapse in variational autoencoders",
    "citation_count": 275,
    "authors": []
  },
  "https://openreview.net/forum?id=rkevMnRqYQ": {
    "title": "Preferences Implicit in the State of the World",
    "volume": "poster",
    "abstract": "Reinforcement learning (RL) agents optimize only the features specified in a reward function and are indifferent to anything left out inadvertently. This means that we must not only specify what to do, but also the much larger space of what not to do. It is easy to forget these preferences, since these preferences are already satisfied in our environment. This motivates our key insight: when a robot is deployed in an environment that humans act in, the state of the environment is already optimized for what humans want. We can therefore use this implicit preference information from the state to fill in the blanks. We develop an algorithm based on Maximum Causal Entropy IRL and use it to evaluate the idea in a suite of proof-of-concept environments designed to show its properties. We find that information from the initial state can be used to infer both side effects that should be avoided as well as preferences for how the environment should be organized. Our code can be found at https://github.com/HumanCompatibleAI/rlsp",
    "checked": true,
    "id": "d35f9c78fc6d656d530aac2ed9f2aae6137b9041",
    "semantic_title": "preferences implicit in the state of the world",
    "citation_count": 55,
    "authors": []
  },
  "https://openreview.net/forum?id=S1lIMn05F7": {
    "title": "A Direct Approach to Robust Deep Learning Using Adversarial Networks",
    "volume": "poster",
    "abstract": "Deep neural networks have been shown to perform well in many classical machine learning problems, especially in image classification tasks. However, researchers have found that neural networks can be easily fooled, and they are surprisingly sensitive to small perturbations imperceptible to humans. Carefully crafted input images (adversarial examples) can force a well-trained neural network to provide arbitrary outputs. Including adversarial examples during training is a popular defense mechanism against adversarial attacks. In this paper we propose a new defensive mechanism under the generative adversarial network~(GAN) framework. We model the adversarial noise using a generative network, trained jointly with a classification discriminative network as a minimax game. We show empirically that our adversarial network approach works well against black box attacks, with performance on par with state-of-art methods such as ensemble adversarial training and adversarial training with projected gradient descent",
    "checked": true,
    "id": "04ee17ea05341aadc8643a21d21746cb67993a9d",
    "semantic_title": "a direct approach to robust deep learning using adversarial networks",
    "citation_count": 78,
    "authors": []
  },
  "https://openreview.net/forum?id=SyfIfnC5Ym": {
    "title": "Improving the Generalization of Adversarial Training with Domain Adaptation",
    "volume": "poster",
    "abstract": "By injecting adversarial examples into training data, adversarial training is promising for improving the robustness of deep learning models. However, most existing adversarial training approaches are based on a specific type of adversarial attack. It may not provide sufficiently representative samples from the adversarial domain, leading to a weak generalization ability on adversarial examples from other attacks. Moreover, during the adversarial training, adversarial perturbations on inputs are usually crafted by fast single-step adversaries so as to scale to large datasets. This work is mainly focused on the adversarial training yet efficient FGSM adversary. In this scenario, it is difficult to train a model with great generalization due to the lack of representative adversarial samples, aka the samples are unable to accurately reflect the adversarial domain. To alleviate this problem, we propose a novel Adversarial Training with Domain Adaptation (ATDA) method. Our intuition is to regard the adversarial training on FGSM adversary as a domain adaption task with limited number of target domain samples. The main idea is to learn a representation that is semantically meaningful and domain invariant on the clean domain as well as the adversarial domain. Empirical evaluations on Fashion-MNIST, SVHN, CIFAR-10 and CIFAR-100 demonstrate that ATDA can greatly improve the generalization of adversarial training and the smoothness of the learned models, and outperforms state-of-the-art methods on standard benchmark datasets. To show the transfer ability of our method, we also extend ATDA to the adversarial training on iterative attacks such as PGD-Adversial Training (PAT) and the defense performance is improved considerably",
    "checked": true,
    "id": "428c2e5992d6ed3186c087cba0fdba2ab6a468b2",
    "semantic_title": "improving the generalization of adversarial training with domain adaptation",
    "citation_count": 133,
    "authors": []
  },
  "https://openreview.net/forum?id=HklSf3CqKm": {
    "title": "Subgradient Descent Learns Orthogonal Dictionaries",
    "volume": "poster",
    "abstract": "This paper concerns dictionary learning, i.e., sparse coding, a fundamental representation learning problem. We show that a subgradient descent algorithm, with random initialization, can recover orthogonal dictionaries on a natural nonsmooth, nonconvex L1 minimization formulation of the problem, under mild statistical assumption on the data. This is in contrast to previous provable methods that require either expensive computation or delicate initialization schemes. Our analysis develops several tools for characterizing landscapes of nonsmooth functions, which might be of independent interest for provable training of deep networks with nonsmooth activations (e.g., ReLU), among other applications. Preliminary synthetic and real experiments corroborate our analysis and show that our algorithm works well empirically in recovering orthogonal dictionaries",
    "checked": true,
    "id": "15411d45ac13f432c3505c3ce825dfa00d88f8a6",
    "semantic_title": "subgradient descent learns orthogonal dictionaries",
    "citation_count": 53,
    "authors": []
  },
  "https://openreview.net/forum?id=HyGEM3C9KQ": {
    "title": "Improving Differentiable Neural Computers Through Memory Masking, De-allocation, and Link Distribution Sharpness Control",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d5535d4da15a7a8dfbeb34f61cddb4874bbc56e0",
    "semantic_title": "improving differentiable neural computers through memory masking, de-allocation, and link distribution sharpness control",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=r1eVMnA9K7": {
    "title": "Unsupervised Control Through Non-Parametric Discriminative Rewards",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b65a6be07ce9c86797e6917258cf5ba45273ee73",
    "semantic_title": "unsupervised control through non-parametric discriminative rewards",
    "citation_count": 181,
    "authors": []
  },
  "https://openreview.net/forum?id=r1eEG20qKQ": {
    "title": "Self-Tuning Networks: Bilevel Optimization of Hyperparameters using Structured Best-Response Functions",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0ef6910cb25305a447aaa8c4be89ead9879a5016",
    "semantic_title": "self-tuning networks: bilevel optimization of hyperparameters using structured best-response functions",
    "citation_count": 165,
    "authors": []
  },
  "https://openreview.net/forum?id=B1MXz20cYQ": {
    "title": "Explaining Image Classifiers by Counterfactual Generation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f1173ca43481c1b33f4e7891ce77200e51eecba2",
    "semantic_title": "explaining image classifiers by counterfactual generation",
    "citation_count": 269,
    "authors": []
  },
  "https://openreview.net/forum?id=HJlQfnCqKX": {
    "title": "Predicting the Generalization Gap in Deep Networks with Margin Distributions",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4f409f21d04347db66e3cc09a32c4ef0d5b66094",
    "semantic_title": "predicting the generalization gap in deep networks with margin distributions",
    "citation_count": 199,
    "authors": []
  },
  "https://openreview.net/forum?id=HyN-M2Rctm": {
    "title": "Mode Normalization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b578b10ef5e271087200ef6182b9806c472c223d",
    "semantic_title": "mode normalization",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=r1GbfhRqF7": {
    "title": "Kernel Change-point Detection with Auxiliary Deep Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "fb327a4cc8a7e3de79362228b6904848d007e61d",
    "semantic_title": "kernel change-point detection with auxiliary deep generative models",
    "citation_count": 71,
    "authors": []
  },
  "https://openreview.net/forum?id=BJzbG20cFQ": {
    "title": "Towards Metamerism via Foveated Style Transfer",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7f3df93baabaf66bebbf7a24257f4ada252bc959",
    "semantic_title": "towards metamerism via foveated style transfer",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=BJxgz2R9t7": {
    "title": "Learning To Solve Circuit-SAT: An Unsupervised Differentiable Approach",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2b64300879d6fcaabe932e87ecb412066359b286",
    "semantic_title": "learning to solve circuit-sat: an unsupervised differentiable approach",
    "citation_count": 101,
    "authors": []
  },
  "https://openreview.net/forum?id=Hyg1G2AqtQ": {
    "title": "Variance Reduction for Reinforcement Learning in Input-Driven Environments",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c470f1d1e7d377b4e0d01890ae418a918c0e7093",
    "semantic_title": "variance reduction for reinforcement learning in input-driven environments",
    "citation_count": 95,
    "authors": []
  },
  "https://openreview.net/forum?id=SyxAb30cY7": {
    "title": "Robustness May Be at Odds with Accuracy",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1b9c6022598085dd892f360122c0fa4c630b3f18",
    "semantic_title": "robustness may be at odds with accuracy",
    "citation_count": 1801,
    "authors": []
  },
  "https://openreview.net/forum?id=H1g0Z3A9Fm": {
    "title": "Supervised Community Detection with Line Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "11222e4f5262c774bf9c006420eb647b951624b2",
    "semantic_title": "supervised community detection with line graph neural networks",
    "citation_count": 322,
    "authors": []
  },
  "https://openreview.net/forum?id=S1M6Z2Cctm": {
    "title": "Harmonic Unpaired Image-to-image Translation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c2f32f0ea55d287ea7dee6bc214e7b96fc2c2763",
    "semantic_title": "harmonic unpaired image-to-image translation",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=rklaWn0qK7": {
    "title": "Learning Neural PDE Solvers with Convergence Guarantees",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e2be7cd38737099a741184ad62587a3fd226db55",
    "semantic_title": "learning neural pde solvers with convergence guarantees",
    "citation_count": 145,
    "authors": []
  },
  "https://openreview.net/forum?id=HyxnZh0ct7": {
    "title": "Meta-learning with differentiable closed-form solvers",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "208cd4b25768f0096fb2e80e7690473da0e2a563",
    "semantic_title": "meta-learning with differentiable closed-form solvers",
    "citation_count": 937,
    "authors": []
  },
  "https://openreview.net/forum?id=S1lhbnRqF7": {
    "title": "Building Dynamic Knowledge Graphs from Text using Machine Reading Comprehension",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e8443666cf927806576ace54b9351af72a1d2d9b",
    "semantic_title": "building dynamic knowledge graphs from text using machine reading comprehension",
    "citation_count": 79,
    "authors": []
  },
  "https://openreview.net/forum?id=BkMiWhR5K7": {
    "title": "Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "10ab21b120e305b6d3cbf81c5a906d36521152f1",
    "semantic_title": "prior convictions: black-box adversarial attacks with bandits and priors",
    "citation_count": 379,
    "authors": []
  },
  "https://openreview.net/forum?id=Byf5-30qFX": {
    "title": "DHER: Hindsight Experience Replay for Dynamic Goals",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "154a19b77f2302d6b9bab354d3160ef527d29f55",
    "semantic_title": "dher: hindsight experience replay for dynamic goals",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=HJf9ZhC9FX": {
    "title": "Stochastic Gradient/Mirror Descent: Minimax Optimality and Implicit Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1d57045bfa02ac54df0481c1a977829b63564334",
    "semantic_title": "stochastic gradient/mirror descent: minimax optimality and implicit regularization",
    "citation_count": 64,
    "authors": []
  },
  "https://openreview.net/forum?id=H1lqZhRcFm": {
    "title": "Unsupervised Learning of the Set of Local Maxima",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "46d5cf70365d48ad38ed549fb44cf76810e8f8da",
    "semantic_title": "unsupervised learning of the set of local maxima",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=B1xY-hRctX": {
    "title": "Neural Logic Machines",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3a6447361b20c249f5306ae17dee43f645430e31",
    "semantic_title": "neural logic machines",
    "citation_count": 254,
    "authors": []
  },
  "https://openreview.net/forum?id=ryetZ20ctX": {
    "title": "Defensive Quantization: When Efficiency Meets Robustness",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ed1f55fb7ba0e3196913027840c4e23155e2e80c",
    "semantic_title": "defensive quantization: when efficiency meets robustness",
    "citation_count": 204,
    "authors": []
  },
  "https://openreview.net/forum?id=SygD-hCcF7": {
    "title": "Dimensionality Reduction for Representing the Knowledge of Probabilistic Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "effc2e95fb3ca77bba896439a1d9cf49f18f4af0",
    "semantic_title": "dimensionality reduction for representing the knowledge of probabilistic models",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=SygvZ209F7": {
    "title": "Biologically-Plausible Learning Algorithms Can Scale to Large Datasets",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2c26cca611ef1403ca81df238d04d0ef4a584a86",
    "semantic_title": "biologically-plausible learning algorithms can scale to large datasets",
    "citation_count": 76,
    "authors": []
  },
  "https://openreview.net/forum?id=rkxw-hAcFQ": {
    "title": "Generating Multi-Agent Trajectories using Programmatic Weak Supervision",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1dd5e6ea1cff5644f1fb80272d25ab37b3ef64bf",
    "semantic_title": "generating multi-agent trajectories using programmatic weak supervision",
    "citation_count": 90,
    "authors": []
  },
  "https://openreview.net/forum?id=H1gL-2A9Ym": {
    "title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ac225094aab9e7b629bc5b3343e026dea0200c70",
    "semantic_title": "predict then propagate: graph neural networks meet personalized pagerank",
    "citation_count": 1721,
    "authors": []
  },
  "https://openreview.net/forum?id=ryl8-3AcFX": {
    "title": "Environment Probing Interaction Policies",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "633266814150ab66f0474d7b9a6807b729c7e0af",
    "semantic_title": "environment probing interaction policies",
    "citation_count": 67,
    "authors": []
  },
  "https://openreview.net/forum?id=SyzVb3CcFX": {
    "title": "Time-Agnostic Prediction: Predicting Predictable Video Frames",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "973e116b2b2949fdb8c533a8d84ddd811c0920cf",
    "semantic_title": "time-agnostic prediction: predicting predictable video frames",
    "citation_count": 94,
    "authors": []
  },
  "https://openreview.net/forum?id=BJg4Z3RqF7": {
    "title": "Unsupervised Adversarial Image Reconstruction",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "59ae0a50c5ebc8aa9c408e9d75f56c195ac7276e",
    "semantic_title": "unsupervised adversarial image reconstruction",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=rylV-2C9KQ": {
    "title": "Deep Decoder: Concise Image Representations from Untrained Non-convolutional Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f73802729a2fad366fecc9ed6221d2aa2a8a6f38",
    "semantic_title": "deep decoder: concise image representations from untrained non-convolutional networks",
    "citation_count": 288,
    "authors": []
  },
  "https://openreview.net/forum?id=S1xNb2A9YX": {
    "title": "Minimal Images in Deep Neural Networks: Fragile Object Recognition in Natural Images",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3b523a15853e99dafc16962827e6e6f467d41a03",
    "semantic_title": "minimal images in deep neural networks: fragile object recognition in natural images",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=Hkg4W2AcFm": {
    "title": "Overcoming the Disentanglement vs Reconstruction Trade-off via Jacobian Supervision",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "aee543a4859821b750b31cc03a0c39f9b4daacc0",
    "semantic_title": "overcoming the disentanglement vs reconstruction trade-off via jacobian supervision",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=H1l7bnR5Ym": {
    "title": "ProbGAN: Towards Probabilistic GAN with Theoretical Guarantees",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "99c870a6d94d1295a6ed1d004235cb4f8f676fbe",
    "semantic_title": "probgan: towards probabilistic gan with theoretical guarantees",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=rkxQ-nA9FX": {
    "title": "Theoretical Analysis of Auto Rate-Tuning by Batch Normalization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3dfb0a18ab5a5413c50d911e49b3c83b1a9383a3",
    "semantic_title": "theoretical analysis of auto rate-tuning by batch normalization",
    "citation_count": 132,
    "authors": []
  },
  "https://openreview.net/forum?id=B1lz-3Rct7": {
    "title": "Three Mechanisms of Weight Decay Regularization",
    "volume": "poster",
    "abstract": "Weight decay is one of the standard tricks in the neural network toolbox, but the reasons for its regularization effect are poorly understood, and recent results have cast doubt on the traditional interpretation in terms of $L_2$ regularization. Literal weight decay has been shown to outperform $L_2$ regularization for optimizers for which they differ. We empirically investigate weight decay for three optimization algorithms (SGD, Adam, and K-FAC) and a variety of network architectures. We identify three distinct mechanisms by which weight decay exerts a regularization effect, depending on the particular optimization algorithm and architecture: (1) increasing the effective learning rate, (2) approximately regularizing the input-output Jacobian norm, and (3) reducing the effective damping coefficient for second-order optimization. Our results provide insight into how to improve the regularization of neural networks",
    "checked": true,
    "id": "ba618ec05a9dbef75310c5e4bcce8a559e0270b5",
    "semantic_title": "three mechanisms of weight decay regularization",
    "citation_count": 262,
    "authors": []
  },
  "https://openreview.net/forum?id=SkfMWhAqYQ": {
    "title": "Approximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet",
    "volume": "poster",
    "abstract": "Deep Neural Networks (DNNs) excel on many complex perceptual tasks but it has proven notoriously difficult to understand how they reach their decisions. We here introduce a high-performance DNN architecture on ImageNet whose decisions are considerably easier to explain. Our model, a simple variant of the ResNet-50 architecture called BagNet, classifies an image based on the occurrences of small local image features without taking into account their spatial ordering. This strategy is closely related to the bag-of-feature (BoF) models popular before the onset of deep learning and reaches a surprisingly high accuracy on ImageNet (87.6% top-5 for 32 x 32 px features and Alexnet performance for 16 x16 px features). The constraint on local features makes it straight-forward to analyse how exactly each part of the image influences the classification. Furthermore, the BagNets behave similar to state-of-the art deep neural networks such as VGG-16, ResNet-152 or DenseNet-169 in terms of feature sensitivity, error distribution and interactions between image parts. This suggests that the improvements of DNNs over previous bag-of-feature classifiers in the last few years is mostly achieved by better fine-tuning rather than by qualitatively different decision strategies",
    "checked": true,
    "id": "810ae452a3a1f673ea241bd540f9551b2996ed5b",
    "semantic_title": "approximating cnns with bag-of-local-features models works surprisingly well on imagenet",
    "citation_count": 571,
    "authors": []
  },
  "https://openreview.net/forum?id=SyMWn05F7": {
    "title": "Learning Exploration Policies for Navigation",
    "volume": "poster",
    "abstract": "Numerous past works have tackled the problem of task-driven navigation. But, how to effectively explore a new environment to enable a variety of down-stream tasks has received much less attention. In this work, we study how agents can autonomously explore realistic and complex 3D environments without the context of task-rewards. We propose a learning-based approach and investigate different policy architectures, reward functions, and training paradigms. We find that use of policies with spatial memory that are bootstrapped with imitation learning and finally finetuned with coverage rewards derived purely from on-board sensors can be effective at exploring novel environments. We show that our learned exploration policies can explore better than classical approaches based on geometry alone and generic learning-based exploration techniques. Finally, we also show how such task-agnostic exploration can be used for down-stream tasks. Videos are available at https://sites.google.com/view/exploration-for-nav/",
    "checked": true,
    "id": "48182d7620a9278d7e9cd880a961fa14d22a0281",
    "semantic_title": "learning exploration policies for navigation",
    "citation_count": 239,
    "authors": []
  },
  "https://openreview.net/forum?id=SJggZnRcFQ": {
    "title": "Learning Programmatically Structured Representations with Perceptor Gradients",
    "volume": "poster",
    "abstract": "We present the perceptor gradients algorithm -- a novel approach to learning symbolic representations based on the idea of decomposing an agent's policy into i) a perceptor network extracting symbols from raw observation data and ii) a task encoding program which maps the input symbols to output actions. We show that the proposed algorithm is able to learn representations that can be directly fed into a Linear-Quadratic Regulator (LQR) or a general purpose A* planner. Our experimental results confirm that the perceptor gradients algorithm is able to efficiently learn transferable symbolic representations as well as generate new observations according to a semantically meaningful specification",
    "checked": true,
    "id": "7e2d24fc41325e19f175dc9dc35db76d56402a72",
    "semantic_title": "learning programmatically structured representations with perceptor gradients",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=HJxeWnCcF7": {
    "title": "Learning Mixed-Curvature Representations in Product Spaces",
    "volume": "poster",
    "abstract": "The quality of the representations achieved by embeddings is determined by how well the geometry of the embedding space matches the structure of the data. Euclidean space has been the workhorse for embeddings; recently hyperbolic and spherical spaces have gained popularity due to their ability to better embed new types of structured data---such as hierarchical data---but most data is not structured so uniformly. We address this problem by proposing learning embeddings in a product manifold combining multiple copies of these model spaces (spherical, hyperbolic, Euclidean), providing a space of heterogeneous curvature suitable for a wide variety of structures. We introduce a heuristic to estimate the sectional curvature of graph data and directly determine an appropriate signature---the number of component spaces and their dimensions---of the product manifold. Empirically, we jointly learn the curvature and the embedding in the product space via Riemannian optimization. We discuss how to define and compute intrinsic quantities such as means---a challenging notion for product manifolds---and provably learnable optimization functions. On a range of datasets and reconstruction tasks, our product space embeddings outperform single Euclidean or hyperbolic spaces used in previous works, reducing distortion by 32.55% on a Facebook social network dataset. We learn word embeddings and find that a product of hyperbolic spaces in 50 dimensions consistently improves on baseline Euclidean and hyperbolic embeddings, by 2.6 points in Spearman rank correlation on similarity tasks and 3.4 points on analogy accuracy",
    "checked": true,
    "id": "779ad52e8c27b77c10d14d536133da61c2c1f9b2",
    "semantic_title": "learning mixed-curvature representations in product spaces",
    "citation_count": 204,
    "authors": []
  },
  "https://openreview.net/forum?id=Hygxb2CqKm": {
    "title": "Stable Recurrent Models",
    "volume": "poster",
    "abstract": "Stability is a fundamental property of dynamical systems, yet to this date it has had little bearing on the practice of recurrent neural networks. In this work, we conduct a thorough investigation of stable recurrent models. Theoretically, we prove stable recurrent neural networks are well approximated by feed-forward networks for the purpose of both inference and training by gradient descent. Empirically, we demonstrate stable recurrent models often perform as well as their unstable counterparts on benchmark sequence tasks. Taken together, these findings shed light on the effective power of recurrent networks and suggest much of sequence learning happens, or can be made to happen, in the stable regime. Moreover, our results help to explain why in many cases practitioners succeed in replacing recurrent models by feed-forward models",
    "checked": true,
    "id": "e77099681374e940ea45821fd7e406394721552f",
    "semantic_title": "stable recurrent models",
    "citation_count": 119,
    "authors": []
  },
  "https://openreview.net/forum?id=HyxCxhRcY7": {
    "title": "Deep Anomaly Detection with Outlier Exposure",
    "volume": "poster",
    "abstract": "It is important to detect anomalous inputs when deploying machine learning systems. The use of larger and more complex inputs in deep learning magnifies the difficulty of distinguishing between anomalous and in-distribution examples. At the same time, diverse image and text data are available in enormous quantities. We propose leveraging these data to improve deep anomaly detection by training anomaly detectors against an auxiliary dataset of outliers, an approach we call Outlier Exposure (OE). This enables anomaly detectors to generalize and detect unseen anomalies. In extensive experiments on natural language processing and small- and large-scale vision tasks, we find that Outlier Exposure significantly improves detection performance. We also observe that cutting-edge generative models trained on CIFAR-10 may assign higher likelihoods to SVHN images than to CIFAR-10 images; we use OE to mitigate this issue. We also analyze the flexibility and robustness of Outlier Exposure, and identify characteristics of the auxiliary dataset that improve performance",
    "checked": true,
    "id": "2d8c97db4bae00ff243d122b957091a236a697a7",
    "semantic_title": "deep anomaly detection with outlier exposure",
    "citation_count": 1508,
    "authors": []
  },
  "https://openreview.net/forum?id=S1lTg3RqYQ": {
    "title": "Exemplar Guided Unsupervised Image-to-Image Translation with Semantic Consistency",
    "volume": "poster",
    "abstract": "Image-to-image translation has recently received significant attention due to advances in deep learning. Most works focus on learning either a one-to-one mapping in an unsupervised way or a many-to-many mapping in a supervised way. However, a more practical setting is many-to-many mapping in an unsupervised way, which is harder due to the lack of supervision and the complex inner- and cross-domain variations. To alleviate these issues, we propose the Exemplar Guided & Semantically Consistent Image-to-image Translation (EGSC-IT) network which conditions the translation process on an exemplar image in the target domain. We assume that an image comprises of a content component which is shared across domains, and a style component specific to each domain. Under the guidance of an exemplar from the target domain we apply Adaptive Instance Normalization to the shared content component, which allows us to transfer the style information of the target domain to the source domain. To avoid semantic inconsistencies during translation that naturally appear due to the large inner- and cross-domain variations, we introduce the concept of feature masks that provide coarse semantic guidance without requiring the use of any semantic labels. Experimental results on various datasets show that EGSC-IT does not only translate the source image to diverse instances in the target domain, but also preserves the semantic consistency during the process",
    "checked": false,
    "id": "7242666492464a5408fc072f5c4f51502e0d0e5b",
    "semantic_title": "exemplar guided unsupervised image-to-image translation",
    "citation_count": 142,
    "authors": []
  },
  "https://openreview.net/forum?id=HkG3e205K7": {
    "title": "Doubly Reparameterized Gradient Estimators for Monte Carlo Objectives",
    "volume": "poster",
    "abstract": "Deep latent variable models have become a popular model choice due to the scalable learning algorithms introduced by (Kingma & Welling 2013, Rezende et al. 2014). These approaches maximize a variational lower bound on the intractable log likelihood of the observed data. Burda et al. (2015) introduced a multi-sample variational bound, IWAE, that is at least as tight as the standard variational lower bound and becomes increasingly tight as the number of samples increases. Counterintuitively, the typical inference network gradient estimator for the IWAE bound performs poorly as the number of samples increases (Rainforth et al. 2018, Le et al. 2018). Roeder et a. (2017) propose an improved gradient estimator, however, are unable to show it is unbiased. We show that it is in fact biased and that the bias can be estimated efficiently with a second application of the reparameterization trick. The doubly reparameterized gradient (DReG) estimator does not suffer as the number of samples increases, resolving the previously raised issues. The same idea can be used to improve many recently introduced training techniques for latent variable models. In particular, we show that this estimator reduces the variance of the IWAE gradient, the reweighted wake-sleep update (RWS) (Bornschein & Bengio 2014), and the jackknife variational inference (JVI) gradient (Nowozin 2018). Finally, we show that this computationally efficient, drop-in estimator translates to improved performance for all three objectives on several modeling tasks",
    "checked": true,
    "id": "d8778f909eb027bcb7c5122bca23acb680ded647",
    "semantic_title": "doubly reparameterized gradient estimators for monte carlo objectives",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=Bylnx209YX": {
    "title": "Adversarial Attacks on Graph Neural Networks via Meta Learning",
    "volume": "poster",
    "abstract": "Deep learning models for graphs have advanced the state of the art on many tasks. Despite their recent success, little is known about their robustness. We investigate training time attacks on graph neural networks for node classification that perturb the discrete graph structure. Our core principle is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize. Our experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings. Remarkably, the perturbations created by our algorithm can misguide the graph neural networks such that they perform worse than a simple baseline that ignores all relational information. Our attacks do not assume any knowledge about or access to the target classifiers",
    "checked": true,
    "id": "6f5b1076ebacd30849d86e5f5787e3d43b65911f",
    "semantic_title": "adversarial attacks on graph neural networks via meta learning",
    "citation_count": 580,
    "authors": []
  },
  "https://openreview.net/forum?id=Bkg3g2R9FX": {
    "title": "Adaptive Gradient Methods with Dynamic Bound of Learning Rate",
    "volume": "poster",
    "abstract": "Adaptive optimization methods such as AdaGrad, RMSprop and Adam have been proposed to achieve a rapid training process with an element-wise scaling term on learning rates. Though prevailing, they are observed to generalize poorly compared with SGD or even fail to converge due to unstable and extreme learning rates. Recent work has put forward some algorithms such as AMSGrad to tackle this issue but they failed to achieve considerable improvement over existing methods. In our paper, we demonstrate that extreme learning rates can lead to poor performance. We provide new variants of Adam and AMSGrad, called AdaBound and AMSBound respectively, which employ dynamic bounds on learning rates to achieve a gradual and smooth transition from adaptive methods to SGD and give a theoretical proof of convergence. We further conduct experiments on various popular tasks and models, which is often insufficient in previous work. Experimental results show that new variants can eliminate the generalization gap between adaptive methods and SGD and maintain higher learning speed early in training at the same time. Moreover, they can bring significant improvement over their prototypes, especially on complex deep networks. The implementation of the algorithm can be found at https://github.com/Luolc/AdaBound",
    "checked": true,
    "id": "03af562fb8e69677865dbe94910e464443dd4623",
    "semantic_title": "adaptive gradient methods with dynamic bound of learning rate",
    "citation_count": 609,
    "authors": []
  },
  "https://openreview.net/forum?id=HyGcghRct7": {
    "title": "Random mesh projectors for inverse problems",
    "volume": "poster",
    "abstract": "We propose a new learning-based approach to solve ill-posed inverse problems in imaging. We address the case where ground truth training samples are rare and the problem is severely ill-posed---both because of the underlying physics and because we can only get few measurements. This setting is common in geophysical imaging and remote sensing. We show that in this case the common approach to directly learn the mapping from the measured data to the reconstruction becomes unstable. Instead, we propose to first learn an ensemble of simpler mappings from the data to projections of the unknown image into random piecewise-constant subspaces. We then combine the projections to form a final reconstruction by solving a deconvolution-like problem. We show experimentally that the proposed method is more robust to measurement noise and corruptions not seen during training than a directly learned inverse",
    "checked": true,
    "id": "e2c18e8f9c808dd5ccc8df752da6efe69619fe34",
    "semantic_title": "random mesh projectors for inverse problems",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=S1xcx3C5FX": {
    "title": "A Statistical Approach to Assessing Neural Network Robustness",
    "volume": "poster",
    "abstract": "We present a new approach to assessing the robustness of neural networks based on estimating the proportion of inputs for which a property is violated. Specifically, we estimate the probability of the event that the property is violated under an input model. Our approach critically varies from the formal verification framework in that when the property can be violated, it provides an informative notion of how robust the network is, rather than just the conventional assertion that the network is not verifiable. Furthermore, it provides an ability to scale to larger networks than formal verification approaches. Though the framework still provides a formal guarantee of satisfiability whenever it successfully finds one or more violations, these advantages do come at the cost of only providing a statistical estimate of unsatisfiability whenever no violation is found. Key to the practical success of our approach is an adaptation of multi-level splitting, a Monte Carlo approach for estimating the probability of rare events, to our statistical robustness framework. We demonstrate that our approach is able to emulate formal verification procedures on benchmark problems, while scaling to larger networks and providing reliable additional information in the form of accurate estimates of the violation probability",
    "checked": true,
    "id": "85df5b9fcd85c41ec0e1eb6c1ab15b8d7147c885",
    "semantic_title": "a statistical approach to assessing neural network robustness",
    "citation_count": 85,
    "authors": []
  },
  "https://openreview.net/forum?id=Hye9lnCct7": {
    "title": "Learning Actionable Representations with Goal Conditioned Policies",
    "volume": "poster",
    "abstract": "Representation learning is a central challenge across a range of machine learning areas. In reinforcement learning, effective and functional representations have the potential to tremendously accelerate learning progress and solve more challenging problems. Most prior work on representation learning has focused on generative approaches, learning representations that capture all the underlying factors of variation in the observation space in a more disentangled or well-ordered manner. In this paper, we instead aim to learn functionally salient representations: representations that are not necessarily complete in terms of capturing all factors of variation in the observation space, but rather aim to capture those factors of variation that are important for decision making -- that are \"actionable\". These representations are aware of the dynamics of the environment, and capture only the elements of the observation that are necessary for decision making rather than all factors of variation, eliminating the need for explicit reconstruction. We show how these learned representations can be useful to improve exploration for sparse reward problems, to enable long horizon hierarchical reinforcement learning, and as a state representation for learning policies for downstream tasks. We evaluate our method on a number of simulated environments, and compare it to prior methods for representation learning, exploration, and hierarchical reinforcement learning",
    "checked": false,
    "id": "c46d80f83813fba0e8363a0ab36a19fba062540e",
    "semantic_title": "learning actionable representations with goal-conditioned policies",
    "citation_count": 110,
    "authors": []
  },
  "https://openreview.net/forum?id=rJgYxn09Fm": {
    "title": "Learning Implicitly Recurrent CNNs Through Parameter Sharing",
    "volume": "poster",
    "abstract": "We introduce a parameter sharing scheme, in which different layers of a convolutional neural network (CNN) are defined by a learned linear combination of parameter tensors from a global bank of templates. Restricting the number of templates yields a flexible hybridization of traditional CNNs and recurrent networks. Compared to traditional CNNs, we demonstrate substantial parameter savings on standard image classification tasks, while maintaining accuracy. Our simple parameter sharing scheme, though defined via soft weights, in practice often yields trained networks with near strict recurrent structure; with negligible side effects, they convert into networks with actual loops. Training these networks thus implicitly involves discovery of suitable recurrent architectures. Though considering only the aspect of recurrent links, our trained networks achieve accuracy competitive with those built using state-of-the-art neural architecture search (NAS) procedures. Our hybridization of recurrent and convolutional networks may also represent a beneficial architectural bias. Specifically, on synthetic tasks which are algorithmic in nature, our hybrid networks both train faster and extrapolate better to test examples outside the span of the training set",
    "checked": true,
    "id": "359cdea86e4203f73de4c12356fd9139dba9a745",
    "semantic_title": "learning implicitly recurrent cnns through parameter sharing",
    "citation_count": 71,
    "authors": []
  },
  "https://openreview.net/forum?id=ByldlhAqYQ": {
    "title": "Transfer Learning for Sequences via Learning to Collocate",
    "volume": "poster",
    "abstract": "Transfer learning aims to solve the data sparsity for a specific domain by applying information of another domain. Given a sequence (e.g. a natural language sentence), the transfer learning, usually enabled by recurrent neural network (RNN), represent the sequential information transfer. RNN uses a chain of repeating cells to model the sequence data. However, previous studies of neural network based transfer learning simply transfer the information across the whole layers, which are unfeasible for seq2seq and sequence labeling. Meanwhile, such layer-wise transfer learning mechanisms also lose the fine-grained cell-level information from the source domain. In this paper, we proposed the aligned recurrent transfer, ART, to achieve cell-level information transfer. ART is in a recurrent manner that different cells share the same parameters. Besides transferring the corresponding information at the same position, ART transfers information from all collocated words in the source domain. This strategy enables ART to capture the word collocation across domains in a more flexible way. We conducted extensive experiments on both sequence labeling tasks (POS tagging, NER) and sentence classification (sentiment analysis). ART outperforms the state-of-the-arts over all experiments",
    "checked": true,
    "id": "3b81ba581733e0c1e9c76b317645d2ee973fb710",
    "semantic_title": "transfer learning for sequences via learning to collocate",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=HJGven05Y7": {
    "title": "How to train your MAML",
    "volume": "poster",
    "abstract": "The field of few-shot learning has recently seen substantial advancements. Most of these advancements came from casting few-shot learning as a meta-learning problem.Model Agnostic Meta Learning or MAML is currently one of the best approaches for few-shot learning via meta-learning. MAML is simple, elegant and very powerful, however, it has a variety of issues, such as being very sensitive to neural network architectures, often leading to instability during training, requiring arduous hyperparameter searches to stabilize training and achieve high generalization and being very computationally expensive at both training and inference times. In this paper, we propose various modifications to MAML that not only stabilize the system, but also substantially improve the generalization performance, convergence speed and computational overhead of MAML, which we call MAML++",
    "checked": true,
    "id": "80892afa04c2fe139eee1d20119978751491b47c",
    "semantic_title": "how to train your maml",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HyxPx3R9tm": {
    "title": "Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow",
    "volume": "poster",
    "abstract": "Adversarial learning methods have been proposed for a wide range of applications, but the training of adversarial models can be notoriously unstable. Effectively balancing the performance of the generator and discriminator is critical, since a discriminator that achieves very high accuracy will produce relatively uninformative gradients. In this work, we propose a simple and general technique to constrain information flow in the discriminator by means of an information bottleneck. By enforcing a constraint on the mutual information between the observations and the discriminator's internal representation, we can effectively modulate the discriminator's accuracy and maintain useful and informative gradients. We demonstrate that our proposed variational discriminator bottleneck (VDB) leads to significant improvements across three distinct application areas for adversarial learning algorithms. Our primary evaluation studies the applicability of the VDB to imitation learning of dynamic continuous control skills, such as running. We show that our method can learn such skills directly from raw video demonstrations, substantially outperforming prior adversarial imitation learning methods. The VDB can also be combined with adversarial inverse reinforcement learning to learn parsimonious reward functions that can be transferred and re-optimized in new settings. Finally, we demonstrate that VDB can train GANs more effectively for image generation, improving upon a number of prior stabilization methods",
    "checked": true,
    "id": "4b59846404c085f5c9523c69cd790537613a3df5",
    "semantic_title": "variational discriminator bottleneck: improving imitation learning, inverse rl, and gans by constraining information flow",
    "citation_count": 219,
    "authors": []
  },
  "https://openreview.net/forum?id=BJgLg3R9KQ": {
    "title": "Learning what and where to attend",
    "volume": "poster",
    "abstract": "Most recent gains in visual recognition have originated from the inclusion of attention mechanisms in deep convolutional networks (DCNs). Because these networks are optimized for object recognition, they learn where to attend using only a weak form of supervision derived from image class labels. Here, we demonstrate the benefit of using stronger supervisory signals by teaching DCNs to attend to image regions that humans deem important for object recognition. We first describe a large-scale online experiment (ClickMe) used to supplement ImageNet with nearly half a million human-derived \"top-down\" attention maps. Using human psychophysics, we confirm that the identified top-down features from ClickMe are more diagnostic than \"bottom-up\" saliency features for rapid image categorization. As a proof of concept, we extend a state-of-the-art attention network and demonstrate that adding ClickMe supervision significantly improves its accuracy and yields visual features that are more interpretable and more similar to those used by human observers",
    "checked": true,
    "id": "136c96810238657bf0c6f0d4b56b0e40e24f3c47",
    "semantic_title": "learning what and where to attend",
    "citation_count": 143,
    "authors": []
  },
  "https://openreview.net/forum?id=SygLehCqtm": {
    "title": "Learning protein sequence embeddings using information from structure",
    "volume": "poster",
    "abstract": "Inferring the structural properties of a protein from its amino acid sequence is a challenging yet important problem in biology. Structures are not known for the vast majority of protein sequences, but structure is critical for understanding function. Existing approaches for detecting structural similarity between proteins from sequence are unable to recognize and exploit structural patterns when sequences have diverged too far, limiting our ability to transfer knowledge between structurally related proteins. We newly approach this problem through the lens of representation learning. We introduce a framework that maps any protein sequence to a sequence of vector embeddings --- one per amino acid position --- that encode structural information. We train bidirectional long short-term memory (LSTM) models on protein sequences with a two-part feedback mechanism that incorporates information from (i) global structural similarity between proteins and (ii) pairwise residue contact maps for individual proteins. To enable learning from structural similarity information, we define a novel similarity measure between arbitrary-length sequences of vector embeddings based on a soft symmetric alignment (SSA) between them. Our method is able to learn useful position-specific embeddings despite lacking direct observations of position-level correspondence between sequences. We show empirically that our multi-task framework outperforms other sequence-based methods and even a top-performing structure-based alignment method when predicting structural similarity, our goal. Finally, we demonstrate that our learned embeddings can be transferred to other protein sequence problems, improving the state-of-the-art in transmembrane domain prediction",
    "checked": true,
    "id": "3637fcccc758786ae1c6529ab22fe85ea98e9c36",
    "semantic_title": "learning protein sequence embeddings using information from structure",
    "citation_count": 293,
    "authors": []
  },
  "https://openreview.net/forum?id=SJzSgnRcKX": {
    "title": "What do you learn from context? Probing for sentence structure in contextualized word representations",
    "volume": "poster",
    "abstract": "Contextualized representation models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline",
    "checked": true,
    "id": "e2587eddd57bc4ba286d91b27c185083f16f40ee",
    "semantic_title": "what do you learn from context? probing for sentence structure in contextualized word representations",
    "citation_count": 877,
    "authors": []
  },
  "https://openreview.net/forum?id=rye4g3AqFm": {
    "title": "Deep learning generalizes because the parameter-function map is biased towards simple functions",
    "volume": "poster",
    "abstract": "Deep neural networks (DNNs) generalize remarkably well without explicit regularization even in the strongly over-parametrized regime where classical learning theory would instead predict that they would severely overfit. While many proposals for some kind of implicit regularization have been made to rationalise this success, there is no consensus for the fundamental reason why DNNs do not strongly overfit. In this paper, we provide a new explanation. By applying a very general probability-complexity bound recently derived from algorithmic information theory (AIT), we argue that the parameter-function map of many DNNs should be exponentially biased towards simple functions. We then provide clear evidence for this strong simplicity bias in a model DNN for Boolean functions, as well as in much larger fully connected and convolutional networks trained on CIFAR10 and MNIST. As the target functions in many real problems are expected to be highly structured, this intrinsic simplicity bias helps explain why deep networks generalize well on real world problems. This picture also facilitates a novel PAC-Bayes approach where the prior is taken over the DNN input-output function space, rather than the more conventional prior over parameter space. If we assume that the training algorithm samples parameters close to uniformly within the zero-error region then the PAC-Bayes theorem can be used to guarantee good expected generalization for target functions producing high-likelihood training sets. By exploiting recently discovered connections between DNNs and Gaussian processes to estimate the marginal likelihood, we produce relatively tight generalization PAC-Bayes error bounds which correlate well with the true error on realistic datasets such as MNIST and CIFAR10 and for architectures including convolutional and fully connected networks",
    "checked": true,
    "id": "7a327286d3549be1607c1ce153b041b08be151f8",
    "semantic_title": "deep learning generalizes because the parameter-function map is biased towards simple functions",
    "citation_count": 233,
    "authors": []
  },
  "https://openreview.net/forum?id=SJgEl3A5tm": {
    "title": "CAMOU: Learning Physical Vehicle Camouflages to Adversarially Attack Detectors in the Wild",
    "volume": "poster",
    "abstract": "In this paper, we conduct an intriguing experimental study about the physical adversarial attack on object detectors in the wild. In particular, we learn a camouflage pattern to hide vehicles from being detected by state-of-the-art convolutional neural network based detectors. Our approach alternates between two threads. In the first, we train a neural approximation function to imitate how a simulator applies a camouflage to vehicles and how a vehicle detector performs given images of the camouflaged vehicles. In the second, we minimize the approximated detection score by searching for the optimal camouflage. Experiments show that the learned camouflage can not only hide a vehicle from the image-based detectors under many test cases but also generalizes to different environments, vehicles, and object detectors",
    "checked": true,
    "id": "2ee77a5fa75717118b62833f4cf111cfd0293aec",
    "semantic_title": "camou: learning physical vehicle camouflages to adversarially attack detectors in the wild",
    "citation_count": 108,
    "authors": []
  },
  "https://openreview.net/forum?id=Hke4l2AcKQ": {
    "title": "MAE: Mutual Posterior-Divergence Regularization for Variational AutoEncoders",
    "volume": "poster",
    "abstract": "Variational Autoencoder (VAE), a simple and effective deep generative model, has led to a number of impressive empirical successes and spawned many advanced variants and theoretical investigations. However, recent studies demonstrate that, when equipped with expressive generative distributions (aka. decoders), VAE suffers from learning uninformative latent representations with the observation called KL Varnishing, in which case VAE collapses into an unconditional generative model. In this work, we introduce mutual posterior-divergence regularization, a novel regularization that is able to control the geometry of the latent space to accomplish meaningful representation learning, while achieving comparable or superior capability of density estimation.Experiments on three image benchmark datasets demonstrate that, when equipped with powerful decoders, our model performs well both on density estimation and representation learning",
    "checked": true,
    "id": "c6d2a216fae7043e9251e5e87141482a1ffc68cd",
    "semantic_title": "mae: mutual posterior-divergence regularization for variational autoencoders",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=SkMQg3C5K7": {
    "title": "A Convergence Analysis of Gradient Descent for Deep Linear Neural Networks",
    "volume": "poster",
    "abstract": "We analyze speed of convergence to global optimum for gradient descent training a deep linear neural network by minimizing the L2 loss over whitened data. Convergence at a linear rate is guaranteed when the following hold: (i) dimensions of hidden layers are at least the minimum of the input and output dimensions; (ii) weight matrices at initialization are approximately balanced; and (iii) the initial loss is smaller than the loss of any rank-deficient solution. The assumptions on initialization (conditions (ii) and (iii)) are necessary, in the sense that violating any one of them may lead to convergence failure. Moreover, in the important case of output dimension 1, i.e. scalar regression, they are met, and thus convergence to global optimum holds, with constant probability under a random initialization scheme. Our results significantly extend previous analyses, e.g., of deep linear residual networks (Bartlett et al., 2018)",
    "checked": true,
    "id": "134c165953e23a6dc7d4f0d86989e92362ca4335",
    "semantic_title": "a convergence analysis of gradient descent for deep linear neural networks",
    "citation_count": 298,
    "authors": []
  },
  "https://openreview.net/forum?id=SkxXg2C5FX": {
    "title": "Don't Settle for Average, Go for the Max: Fuzzy Sets and Max-Pooled Word Vectors",
    "volume": "poster",
    "abstract": "Recent literature suggests that averaged word vectors followed by simple post-processing outperform many deep learning methods on semantic textual similarity tasks. Furthermore, when averaged word vectors are trained supervised on large corpora of paraphrases, they achieve state-of-the-art results on standard STS benchmarks. Inspired by these insights, we push the limits of word embeddings even further. We propose a novel fuzzy bag-of-words (FBoW) representation for text that contains all the words in the vocabulary simultaneously but with different degrees of membership, which are derived from similarities between word vectors. We show that max-pooled word vectors are only a special case of fuzzy BoW and should be compared via fuzzy Jaccard index rather than cosine similarity. Finally, we propose DynaMax, a completely unsupervised and non-parametric similarity measure that dynamically extracts and max-pools good features depending on the sentence pair. This method is both efficient and easy to implement, yet outperforms current baselines on STS tasks by a large margin and is even competitive with supervised word vectors trained to directly optimise cosine similarity",
    "checked": true,
    "id": "117b08d70961242283afdb412d9ab9004262d9d5",
    "semantic_title": "don't settle for average, go for the max: fuzzy sets and max-pooled word vectors",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=BygfghAcYX": {
    "title": "The role of over-parametrization in generalization of neural networks",
    "volume": "poster",
    "abstract": "Despite existing work on ensuring generalization of neural networks in terms of scale sensitive complexity measures, such as norms, margin and sharpness, these complexity measures do not offer an explanation of why neural networks generalize better with over-parametrization. In this work we suggest a novel complexity measure based on unit-wise capacities resulting in a tighter generalization bound for two layer ReLU networks. Our capacity bound correlates with the behavior of test error with increasing network sizes (within the range reported in the experiments), and could partly explain the improvement in generalization with over-parametrization. We further present a matching lower bound for the Rademacher complexity that improves over previous capacity lower bounds for neural networks",
    "checked": true,
    "id": "6e8001afb2e24b648ae9ceb4e00c20ded4a5ee99",
    "semantic_title": "the role of over-parametrization in generalization of neural networks",
    "citation_count": 191,
    "authors": []
  },
  "https://openreview.net/forum?id=H1x-x309tm": {
    "title": "On the Convergence of A Class of Adam-Type Algorithms for Non-Convex Optimization",
    "volume": "poster",
    "abstract": "This paper studies a class of adaptive gradient based momentum algorithms that update the search directions and learning rates simultaneously using past gradients. This class, which we refer to as the ''``Adam-type'', includes the popular algorithms such as Adam, AMSGrad, AdaGrad. Despite their popularity in training deep neural networks (DNNs), the convergence of these algorithms for solving non-convex problems remains an open question. In this paper, we develop an analysis framework and a set of mild sufficient conditions that guarantee the convergence of the Adam-type methods, with a convergence rate of order $O(\\log{T}/\\sqrt{T})$ for non-convex stochastic optimization. Our convergence analysis applies to a new algorithm called AdaFom (AdaGrad with First Order Momentum). We show that the conditions are essential, by identifying concrete examples in which violating the conditions makes an algorithm diverge. Besides providing one of the first comprehensive analysis for Adam-type methods in the non-convex setting, our results can also help the practitioners to easily monitor the progress of algorithms and determine their convergence behavior",
    "checked": true,
    "id": "c215b9ac79f07c8a43782b224f4416943837ffa8",
    "semantic_title": "on the convergence of a class of adam-type algorithms for non-convex optimization",
    "citation_count": 329,
    "authors": []
  },
  "https://openreview.net/forum?id=HJflg30qKX": {
    "title": "Gradient descent aligns the layers of deep linear networks",
    "volume": "poster",
    "abstract": "This paper establishes risk convergence and asymptotic weight matrix alignment --- a form of implicit regularization --- of gradient flow and gradient descent when applied to deep linear networks on linearly separable data. In more detail, for gradient flow applied to strictly decreasing loss functions (with similar results for gradient descent with particular decreasing step sizes): (i) the risk converges to 0; (ii) the normalized i-th weight matrix asymptotically equals its rank-1 approximation u_iv_i^T; (iii) these rank-1 matrices are aligned across layers, meaning |v_{i+1}^T u_i| -> 1. In the case of the logistic loss (binary cross entropy), more can be said: the linear function induced by the network --- the product of its weight matrices --- converges to the same direction as the maximum margin solution. This last property was identified in prior work, but only under assumptions on gradient descent which here are implied by the alignment phenomenon",
    "checked": true,
    "id": "5786917220aab2f6d0b00606eee9fe0ad0700f1b",
    "semantic_title": "gradient descent aligns the layers of deep linear networks",
    "citation_count": 257,
    "authors": []
  },
  "https://openreview.net/forum?id=SJz1x20cFQ": {
    "title": "Hierarchical RL Using an Ensemble of Proprioceptive Periodic Policies",
    "volume": "poster",
    "abstract": "In this paper we introduce a simple, robust approach to hierarchically training an agent in the setting of sparse reward tasks. The agent is split into a low-level and a high-level policy. The low-level policy only accesses internal, proprioceptive dimensions of the state observation. The low-level policies are trained with a simple reward that encourages changing the values of the non-proprioceptive dimensions. Furthermore, it is induced to be periodic with the use a ``phase function.'' The high-level policy is trained using a sparse, task-dependent reward, and operates by choosing which of the low-level policies to run at any given time. Using this approach, we solve difficult maze and navigation tasks with sparse rewards using the Mujoco Ant and Humanoid agents and show improvement over recent hierarchical methods",
    "checked": true,
    "id": "622cd0cc96d437481d8ca1de5ea5400e655efcc3",
    "semantic_title": "hierarchical rl using an ensemble of proprioceptive periodic policies",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=HJgkx2Aqt7": {
    "title": "Learning To Simulate",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "73b226ea356f14e6751f16871da2e357a7989990",
    "semantic_title": "learning to simulate",
    "citation_count": 119,
    "authors": []
  },
  "https://openreview.net/forum?id=Skeke3C5Fm": {
    "title": "Multilingual Neural Machine Translation With Soft Decoupled Encoding",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "be312e930f6739a709e60547aa0dfb9c3dc44497",
    "semantic_title": "multilingual neural machine translation with soft decoupled encoding",
    "citation_count": 62,
    "authors": []
  },
  "https://openreview.net/forum?id=BJgklhAcK7": {
    "title": "Meta-Learning with Latent Embedding Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "04f739a0c29b75877243731aeead512bf0ed1dff",
    "semantic_title": "meta-learning with latent embedding optimization",
    "citation_count": 1381,
    "authors": []
  },
  "https://openreview.net/forum?id=HJeRkh05Km": {
    "title": "Visual Semantic Navigation using Scene Priors",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c7aea4b653d4e12cb47438960f5689f5f835e073",
    "semantic_title": "visual semantic navigation using scene priors",
    "citation_count": 328,
    "authors": []
  },
  "https://openreview.net/forum?id=HkgTkhRcKQ": {
    "title": "AdaShift: Decorrelation and Convergence of Adaptive Learning Rate Methods",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9f88722cbc4107e3c3d0e1c7934cc7f1d5ae4fdb",
    "semantic_title": "adashift: decorrelation and convergence of adaptive learning rate methods",
    "citation_count": 66,
    "authors": []
  },
  "https://openreview.net/forum?id=H1xaJn05FQ": {
    "title": "Sliced Wasserstein Auto-Encoders",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "da798da448db9dd8707b55efb856b5c7ad5b6c00",
    "semantic_title": "sliced wasserstein auto-encoders",
    "citation_count": 166,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgpy3C5tX": {
    "title": "Amortized Bayesian Meta-Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2686aef553af3ebd43cb0ea98a538a9435f7dd5f",
    "semantic_title": "amortized bayesian meta-learning",
    "citation_count": 137,
    "authors": []
  },
  "https://openreview.net/forum?id=S1g2JnRcFX": {
    "title": "Local SGD Converges Fast and Communicates Little",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7cfa76a82be96c74b2eff514265b7fd271a179cd",
    "semantic_title": "local sgd converges fast and communicates little",
    "citation_count": 1081,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgoyn09KQ": {
    "title": "textTOvec: DEEP CONTEXTUALIZED NEURAL AUTOREGRESSIVE TOPIC MODELS OF LANGUAGE WITH DISTRIBUTED COMPOSITIONAL PRIOR",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "ed01746601d9cb7724568645d8b53876f6cd7c16",
    "semantic_title": "texttovec: deep contextualized neural autoregressive models of language with distributed compositional prior",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=ByloJ20qtm": {
    "title": "Neural Program Repair by Jointly Learning to Localize and Repair",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "564bce85c8ad9a50f4652a4d05e1ed0aaa22df49",
    "semantic_title": "neural program repair by jointly learning to localize and repair",
    "citation_count": 132,
    "authors": []
  },
  "https://openreview.net/forum?id=ryl5khRcKm": {
    "title": "Human-level Protein Localization with Convolutional Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6a65652fd2735d546281dd54c912d6eeaa17546c",
    "semantic_title": "human-level protein localization with convolutional neural networks",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=r1lq1hRqYQ": {
    "title": "From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "758311575a6385bb15d4f9af8c0e671cb98184b4",
    "semantic_title": "from language to goals: inverse reinforcement learning for vision-based instruction following",
    "citation_count": 125,
    "authors": []
  },
  "https://openreview.net/forum?id=HklY120cYm": {
    "title": "ClariNet: Parallel Wave Generation in End-to-End Text-to-Speech",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5e3a59695261f03aa3f09a8a5ac6166fb63e0a2e",
    "semantic_title": "clarinet: parallel wave generation in end-to-end text-to-speech",
    "citation_count": 349,
    "authors": []
  },
  "https://openreview.net/forum?id=SyxtJh0qYm": {
    "title": "Variational Autoencoder with Arbitrary Conditioning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c61b43acd00d73e58fade68b8eb7c3ae875fc60c",
    "semantic_title": "variational autoencoder with arbitrary conditioning",
    "citation_count": 148,
    "authors": []
  },
  "https://openreview.net/forum?id=BJluy2RcFm": {
    "title": "Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "de9550945b2f631c541c299114a770c4f47f9616",
    "semantic_title": "janossy pooling: learning deep permutation-invariant functions for variable-size inputs",
    "citation_count": 193,
    "authors": []
  },
  "https://openreview.net/forum?id=ByGuynAct7": {
    "title": "The Deep Weight Prior",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "cf9d672680312c086b2277dbaa04bd37cb295d6e",
    "semantic_title": "the deep weight prior",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=rkluJ2R9KQ": {
    "title": "A new dog learns old tricks: RL finds classic optimization algorithms",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0029ccaa0047d8af36e34bf555c33cd09ce4cb31",
    "semantic_title": "a new dog learns old tricks: rl finds classic optimization algorithms",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=HJgd1nAqFX": {
    "title": "DOM-Q-NET: Grounded RL on Structured Language",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d7d5406eabff0e0e7fc5f7d8706297d85446e4b2",
    "semantic_title": "dom-q-net: grounded rl on structured language",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=ryxwJhC9YX": {
    "title": "InstaGAN: Instance-aware Image-to-Image Translation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "af7994e85e6900c0eaf48a5d80fbf5c127f58644",
    "semantic_title": "instagan: instance-aware image-to-image translation",
    "citation_count": 159,
    "authors": []
  },
  "https://openreview.net/forum?id=SyNPk2R9K7": {
    "title": "Learning to Describe Scenes with Programs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "58dcc24c63ed179d9a9b458d5f1284a7a297c5d6",
    "semantic_title": "learning to describe scenes with programs",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=HJfwJ2A5KX": {
    "title": "Data-Dependent Coresets for Compressing Neural Networks with Applications to Generalization Bounds",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f61fc6c49d3b22f297b7ccb72bbfe6f7c1344e78",
    "semantic_title": "data-dependent coresets for compressing neural networks with applications to generalization bounds",
    "citation_count": 79,
    "authors": []
  },
  "https://openreview.net/forum?id=rJg8yhAqKm": {
    "title": "InfoBot: Transfer and Exploration via the Information Bottleneck",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bf7f1ada5feecc0992f71b39c1ebeccb19ae631b",
    "semantic_title": "infobot: transfer and exploration via the information bottleneck",
    "citation_count": 168,
    "authors": []
  },
  "https://openreview.net/forum?id=BylE1205Fm": {
    "title": "Emerging Disentanglement in Auto-Encoder Based Unsupervised Image Content Transfer",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "98f0f57e5c9425c81e24618b00b364bbcb61338b",
    "semantic_title": "emerging disentanglement in auto-encoder based unsupervised image content transfer",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=rJg4J3CqFm": {
    "title": "Learning Embeddings into Entropic Wasserstein Spaces",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8fec21b4553cb8eb72345aff30b35bcffd456378",
    "semantic_title": "learning embeddings into entropic wasserstein spaces",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=H1g4k309F7": {
    "title": "Wasserstein Barycenter Model Ensembling",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3c50284257e0180133d5d71da20bd1c7e089b85c",
    "semantic_title": "wasserstein barycenter model ensembling",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=Hyx4knR9Ym": {
    "title": "Generalizable Adversarial Training via Spectral Normalization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3a606480406886742572a956e221e986c65d94c1",
    "semantic_title": "generalizable adversarial training via spectral normalization",
    "citation_count": 141,
    "authors": []
  },
  "https://openreview.net/forum?id=Bylmkh05KX": {
    "title": "Unsupervised Speech Recognition via Segmental Empirical Output Distribution Matching",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ad4b7c0e1c71e26c1cf352069e9fd5cb06f70148",
    "semantic_title": "unsupervised speech recognition via segmental empirical output distribution matching",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=rye7knCqK7": {
    "title": "Learning when to Communicate at Scale in Multiagent Cooperative and Competitive Tasks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "320f80ad1a95de5824b9477c4e3cec37ee007d10",
    "semantic_title": "learning when to communicate at scale in multiagent cooperative and competitive tasks",
    "citation_count": 250,
    "authors": []
  },
  "https://openreview.net/forum?id=HyzMyhCcK7": {
    "title": "ProxQuant: Quantized Neural Networks via Proximal Operators",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7c42d7ff616efc45a42b264b0da6c74e8141a9ed",
    "semantic_title": "proxquant: quantized neural networks via proximal operators",
    "citation_count": 121,
    "authors": []
  },
  "https://openreview.net/forum?id=rkMW1hRqKX": {
    "title": "Optimal Completion Distillation for Sequence Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6ffe584f5edad5e343a30892ec54f1ef8b8e740e",
    "semantic_title": "optimal completion distillation for sequence learning",
    "citation_count": 45,
    "authors": []
  },
  "https://openreview.net/forum?id=SyxZJn05YX": {
    "title": "Feature Intertwiner for Object Detection",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d2420d9ce64d101b28641660b4641c415fc7a6c9",
    "semantic_title": "feature intertwiner for object detection",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=BkxWJnC9tX": {
    "title": "Diversity and Depth in Per-Example Routing Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7b7482c433da5e3b52224524206f36bda4c14edf",
    "semantic_title": "diversity and depth in per-example routing models",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=Hke-JhA9Y7": {
    "title": "Learning concise representations for regression by evolving networks of trees",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a7dc6bb1ffb17fa9428dc12303a9d762c959fd00",
    "semantic_title": "learning concise representations for regression by evolving networks of trees",
    "citation_count": 59,
    "authors": []
  },
  "https://openreview.net/forum?id=H1lJJnR5Ym": {
    "title": "Exploration by random network distillation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4cb3fd057949624aa4f0bbe7a6dcc8777ff04758",
    "semantic_title": "exploration by random network distillation",
    "citation_count": 1370,
    "authors": []
  },
  "https://openreview.net/forum?id=rygkk305YQ": {
    "title": "Hierarchical Generative Modeling for Controllable Speech Synthesis",
    "volume": "poster",
    "abstract": "This paper proposes a neural end-to-end text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions. The model is formulated as a conditional generative model with two levels of hierarchical latent variables. The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability. The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes. This amounts to using a Gaussian mixture model (GMM) for the latent distribution. Extensive evaluation demonstrates its ability to control the aforementioned attributes. In particular, it is capable of consistently synthesizing high-quality clean speech regardless of the quality of the training data for the target speaker",
    "checked": true,
    "id": "3321263fd0b2be6011f20d7b74b8ae801741eb21",
    "semantic_title": "hierarchical generative modeling for controllable speech synthesis",
    "citation_count": 277,
    "authors": []
  },
  "https://openreview.net/forum?id=Bkx0RjA9tX": {
    "title": "Generative Question Answering: Learning to Answer the Whole Question",
    "volume": "poster",
    "abstract": "Discriminative question answering models can overfit to superficial biases in datasets, because their loss function saturates when any clue makes the answer likely. We introduce generative models of the joint distribution of questions and answers, which are trained to explain the whole question, not just to answer it.Our question answering (QA) model is implemented by learning a prior over answers, and a conditional language model to generate the question given the answer‚Äîallowing scalable and interpretable many-hop reasoning as the question is generated word-by-word. Our model achieves competitive performance with specialised discriminative models on the SQUAD and CLEVR benchmarks, indicating that it is a more general architecture for language understanding and reasoning than previous work. The model greatly improves generalisation both from biased training data and to adversarial testing data, achieving a new state-of-the-art on ADVERSARIAL SQUAD. We will release our code",
    "checked": true,
    "id": "6182aed90596acd1573bd5ccbc2284b1e8a7291b",
    "semantic_title": "generative question answering: learning to answer the whole question",
    "citation_count": 73,
    "authors": []
  },
  "https://openreview.net/forum?id=Bkg6RiCqY7": {
    "title": "Decoupled Weight Decay Regularization",
    "volume": "poster",
    "abstract": "L$_2$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \\emph{not} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L$_2$ regularization (often calling it ``weight decay'' in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \\emph{decoupling} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at \\url{https://github.com/loshchil/AdamW-and-SGDW}",
    "checked": true,
    "id": "d07284a6811f1b2745d91bdb06b040b57f226882",
    "semantic_title": "decoupled weight decay regularization",
    "citation_count": 24116,
    "authors": []
  },
  "https://openreview.net/forum?id=rkl6As0cF7": {
    "title": "Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning",
    "volume": "poster",
    "abstract": "Humans are capable of attributing latent mental contents such as beliefs, or intentions to others. The social skill is critical in everyday life to reason about the potential consequences of their behaviors so as to plan ahead. It is known that humans use this reasoning ability recursively, i.e. considering what others believe about their own beliefs. In this paper, we start from level-$1$ recursion and introduce a probabilistic recursive reasoning (PR2) framework for multi-agent reinforcement learning. Our hypothesis is that it is beneficial for each agent to account for how the opponents would react to its future behaviors. Under the PR2 framework, we adopt variational Bayes methods to approximate the opponents' conditional policy, to which each agent finds the best response and then improve their own policy. We develop decentralized-training-decentralized-execution algorithms, PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenario when there is one Nash equilibrium. Our methods are tested on both the matrix game and the differential game, which have a non-trivial equilibrium where common gradient-based methods fail to converge. Our experiments show that it is critical to reason about how the opponents believe about what the agent believes. We expect our work to contribute a new idea of modeling the opponents to the multi-agent reinforcement learning community",
    "checked": true,
    "id": "bb905c6b8d9abe84adad1c6784e6037dc9516e95",
    "semantic_title": "probabilistic recursive reasoning for multi-agent reinforcement learning",
    "citation_count": 153,
    "authors": []
  },
  "https://openreview.net/forum?id=BJl6AjC5F7": {
    "title": "Learning to Represent Edits",
    "volume": "poster",
    "abstract": "We introduce the problem of learning distributed representations of edits. By combining a \"neural editor\" with an \"edit encoder\", our models learn to represent the salient information of an edit and can be used to apply edits to new inputs. We experiment on natural language and source code edit data. Our evaluation yields promising results that suggest that our neural network models learn to capture the structure and semantics of edits. We hope that this interesting task and data source will inspire other researchers to work further on this problem",
    "checked": true,
    "id": "f6224ed8b4969aee883c8c0a57444b79f681d499",
    "semantic_title": "learning to represent edits",
    "citation_count": 113,
    "authors": []
  },
  "https://openreview.net/forum?id=BklhAj09K7": {
    "title": "Unsupervised Domain Adaptation for Distance Metric Learning",
    "volume": "poster",
    "abstract": "Unsupervised domain adaptation is a promising avenue to enhance the performance of deep neural networks on a target domain, using labels only from a source domain. However, the two predominant methods, domain discrepancy reduction learning and semi-supervised learning, are not readily applicable when source and target domains do not share a common label space. This paper addresses the above scenario by learning a representation space that retains discriminative power on both the (labeled) source and (unlabeled) target domains while keeping representations for the two domains well-separated. Inspired by a theoretical analysis, we first reformulate the disjoint classification task, where the source and target domains correspond to non-overlapping class labels, to a verification one. To handle both within and cross domain verifications, we propose a Feature Transfer Network (FTN) to separate the target feature space from the original source space while aligned with a transformed source space. Moreover, we present a non-parametric multi-class entropy minimization loss to further boost the discriminative power of FTNs on the target domain. In experiments, we first illustrate how FTN works in a controlled setting of adapting from MNIST-M to MNIST with disjoint digit classes between the two domains and then demonstrate the effectiveness of FTNs through state-of-the-art performances on a cross-ethnicity face recognition problem",
    "checked": true,
    "id": "c9c3e58677b95a110220ad89946fd941fa1b287e",
    "semantic_title": "unsupervised domain adaptation for distance metric learning",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=B1g30j0qF7": {
    "title": "Bayesian Deep Convolutional Networks with Many Channels are Gaussian Processes",
    "volume": "poster",
    "abstract": "There is a previously identified equivalence between wide fully connected neural networks (FCNs) and Gaussian processes (GPs). This equivalence enables, for instance, test set predictions that would have resulted from a fully Bayesian, infinitely wide trained FCN to be computed without ever instantiating the FCN, but by instead evaluating the corresponding GP. In this work, we derive an analogous equivalence for multi-layer convolutional neural networks (CNNs) both with and without pooling layers, and achieve state of the art results on CIFAR10 for GPs without trainable kernels. We also introduce a Monte Carlo method to estimate the GP corresponding to a given neural network architecture, even in cases where the analytic form has too many terms to be computationally feasible. Surprisingly, in the absence of pooling layers, the GPs corresponding to CNNs with and without weight sharing are identical. As a consequence, translation equivariance, beneficial in finite channel CNNs trained with stochastic gradient descent (SGD), is guaranteed to play no role in the Bayesian treatment of the infinite channel limit - a qualitative difference between the two regimes that is not present in the FCN case. We confirm experimentally, that while in some scenarios the performance of SGD-trained finite CNNs approaches that of the corresponding GPs as the channel count increases, with careful tuning SGD-trained CNNs can significantly outperform their corresponding GPs, suggesting advantages from SGD training compared to fully Bayesian parameter estimation",
    "checked": true,
    "id": "9ff05248a26c637a4fc9a11e08608574467d6320",
    "semantic_title": "bayesian deep convolutional networks with many channels are gaussian processes",
    "citation_count": 311,
    "authors": []
  },
  "https://openreview.net/forum?id=Hke20iA9Y7": {
    "title": "Efficient Training on Very Large Corpora via Gramian Estimation",
    "volume": "poster",
    "abstract": "We study the problem of learning similarity functions over very large corpora using neural network embedding models. These models are typically trained using SGD with random sampling of unobserved pairs, with a sample size that grows quadratically with the corpus size, making it expensive to scale. We propose new efficient methods to train these models without having to sample unobserved pairs. Inspired by matrix factorization, our approach relies on adding a global quadratic penalty and expressing this term as the inner-product of two generalized Gramians. We show that the gradient of this term can be efficiently computed by maintaining estimates of the Gramians, and develop variance reduction schemes to improve the quality of the estimates. We conduct large-scale experiments that show a significant improvement both in training time and generalization performance compared to sampling methods",
    "checked": true,
    "id": "88c31abd700b40e3237472f8c296365cd9f079b1",
    "semantic_title": "efficient training on very large corpora via gramian estimation",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=BkloRs0qK7": {
    "title": "A comprehensive, application-oriented study of catastrophic forgetting in DNNs",
    "volume": "poster",
    "abstract": "We present a large-scale empirical study of catastrophic forgetting (CF) in modern Deep Neural Network (DNN) models that perform sequential (or: incremental) learning. A new experimental protocol is proposed that takes into account typical constraints encountered in application scenarios. As the investigation is empirical, we evaluate CF behavior on the hitherto largest number of visual classification datasets, from each of which we construct a representative number of Sequential Learning Tasks (SLTs) in close alignment to previous works on CF. Our results clearly indicate that there is no model that avoids CF for all investigated datasets and SLTs under application conditions. We conclude with a discussion of potential solutions and workarounds to CF, notably for the EWC and IMM models",
    "checked": true,
    "id": "eaf6a986ef90b442d6363ee54852d9b5b4788b0f",
    "semantic_title": "a comprehensive, application-oriented study of catastrophic forgetting in dnns",
    "citation_count": 90,
    "authors": []
  },
  "https://openreview.net/forum?id=SJgsCjCqt7": {
    "title": "Variational Autoencoders with Jointly Optimized Latent Dependency Structure",
    "volume": "poster",
    "abstract": "We propose a method for learning the dependency structure between latent variables in deep latent variable models. Our general modeling and inference framework combines the complementary strengths of deep generative models and probabilistic graphical models. In particular, we express the latent variable space of a variational autoencoder (VAE) in terms of a Bayesian network with a learned, flexible dependency structure. The network parameters, variational parameters as well as the latent topology are optimized simultaneously with a single objective. Inference is formulated via a sampling procedure that produces expectations over latent variable structures and incorporates top-down and bottom-up reasoning over latent variable values. We validate our framework in extensive experiments on MNIST, Omniglot, and CIFAR-10. Comparisons to state-of-the-art structured variational autoencoder baselines show improvements in terms of the expressiveness of the learned model",
    "checked": true,
    "id": "0f2d473dd476f4e8775081af57e0ef6f9d4c70bb",
    "semantic_title": "variational autoencoders with jointly optimized latent dependency structure",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=HyeFAsRctQ": {
    "title": "Verification of Non-Linear Specifications for Neural Networks",
    "volume": "poster",
    "abstract": "Prior work on neural network verification has focused on specifications that are linear functions of the output of the network, e.g., invariance of the classifier output under adversarial perturbations of the input. In this paper, we extend verification algorithms to be able to certify richer properties of neural networks. To do this we introduce the class of convex-relaxable specifications, which constitute nonlinear specifications that can be verified using a convex relaxation. We show that a number of important properties of interest can be modeled within this class, including conservation of energy in a learned dynamics model of a physical system; semantic consistency of a classifier's output labels under adversarial perturbations and bounding errors in a system that predicts the summation of handwritten digits. Our experimental evaluation shows that our method is able to effectively verify these specifications. Moreover, our evaluation exposes the failure modes in models which cannot be verified to satisfy these specifications. Thus, emphasizing the importance of training models not just to fit training data but also to be consistent with specifications",
    "checked": true,
    "id": "ad640ea420d49a969a995632aed22c21251891df",
    "semantic_title": "verification of non-linear specifications for neural networks",
    "citation_count": 45,
    "authors": []
  },
  "https://openreview.net/forum?id=S1xtAjR5tX": {
    "title": "Improving Sequence-to-Sequence Learning via Optimal Transport",
    "volume": "poster",
    "abstract": "Sequence-to-sequence models are commonly trained via maximum likelihood estimation (MLE). However, standard MLE training considers a word-level objective, predicting the next word given the previous ground-truth partial sentence. This procedure focuses on modeling local syntactic patterns, and may fail to capture long-range semantic structure. We present a novel solution to alleviate these issues. Our approach imposes global sequence-level guidance via new supervision based on optimal transport, enabling the overall characterization and preservation of semantic features. We further show that this method can be understood as a Wasserstein gradient flow trying to match our model to the ground truth sequence distribution. Extensive experiments are conducted to validate the utility of the proposed approach, showing consistent improvements over a wide variety of NLP tasks, including machine translation, abstractive text summarization, and image captioning",
    "checked": true,
    "id": "717daba98eb57b898687fc013b705f763eb2916b",
    "semantic_title": "improving sequence-to-sequence learning via optimal transport",
    "citation_count": 94,
    "authors": []
  },
  "https://openreview.net/forum?id=SyVuRiC5K7": {
    "title": "LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING",
    "volume": "poster",
    "abstract": "The goal of few-shot learning is to learn a classifier that generalizes well even when trained with a limited number of training instances per class. The recently introduced meta-learning approaches tackle this problem by learning a generic classifier across a large number of multiclass classification tasks and generalizing the model to a new task. Yet, even with such meta-learning, the low-data problem in the novel classification task still remains. In this paper, we propose Transductive Propagation Network (TPN), a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem. Specifically, we propose to learn to propagate labels from labeled instances to unlabeled test instances, by learning a graph construction module that exploits the manifold structure in the data. TPN jointly learns both the parameters of feature embedding and the graph construction in an end-to-end manner. We validate TPN on multiple benchmark datasets, on which it largely outperforms existing few-shot learning approaches and achieves the state-of-the-art results",
    "checked": true,
    "id": "a3d7257cf0ba5c501d2f6b8ddab931bcd588dfbf",
    "semantic_title": "learning to propagate labels: transductive propagation network for few-shot learning",
    "citation_count": 676,
    "authors": []
  },
  "https://openreview.net/forum?id=HyzdRiR9Y7": {
    "title": "Universal Transformers",
    "volume": "poster",
    "abstract": "Recurrent neural networks (RNNs) sequentially process data by updating their state with each new data point, and have long been the de facto choice for sequence modeling tasks. However, their inherently sequential computation makes them slow to train. Feed-forward and convolutional architectures have recently been shown to achieve superior results on some sequence modeling tasks such as machine translation, with the added advantage that they concurrently process all inputs in the sequence, leading to easy parallelization and faster training times. Despite these successes, however, popular feed-forward sequence models like the Transformer fail to generalize in many simple tasks that recurrent models handle with ease, e.g. copying strings or even simple logical inference when the string or formula lengths exceed those observed at training time. We propose the Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues. UTs combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs. We also add a dynamic per-position halting mechanism and find that it improves accuracy on several tasks. In contrast to the standard Transformer, under certain assumptions UTs can be shown to be Turing-complete. Our experiments show that UTs outperform standard Transformers on a wide range of algorithmic and language understanding tasks, including the challenging LAMBADA language modeling task where UTs achieve a new state of the art, and machine translation where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset",
    "checked": true,
    "id": "ac4dafdef1d2b685b7f28a11837414573d39ff4e",
    "semantic_title": "universal transformers",
    "citation_count": 764,
    "authors": []
  },
  "https://openreview.net/forum?id=rJfUCoR5KX": {
    "title": "An Empirical study of Binary Neural Networks' Optimisation",
    "volume": "poster",
    "abstract": "Binary neural networks using the Straight-Through-Estimator (STE) have been shown to achieve state-of-the-art results, but their training process is not well-founded. This is due to the discrepancy between the evaluated function in the forward path, and the weight updates in the back-propagation, updates which do not correspond to gradients of the forward path. Efficient convergence and accuracy of binary models often rely on careful fine-tuning and various ad-hoc techniques. In this work, we empirically identify and study the effectiveness of the various ad-hoc techniques commonly used in the literature, providing best-practices for efficient training of binary models. We show that adapting learning rates using second moment methods is crucial for the successful use of the STE, and that other optimisers can easily get stuck in local minima. We also find that many of the commonly employed tricks are only effective towards the end of the training, with these methods making early stages of the training considerably slower. Our analysis disambiguates necessary from unnecessary ad-hoc techniques for training of binary neural networks, paving the way for future development of solid theoretical foundations for these. Our newly-found insights further lead to new procedures which make training of existing binary neural networks notably faster",
    "checked": true,
    "id": "a2980c4ab69e017e72ca36db0ac3a0a427ff1aee",
    "semantic_title": "an empirical study of binary neural networks' optimisation",
    "citation_count": 89,
    "authors": []
  },
  "https://openreview.net/forum?id=rylIAsCqYm": {
    "title": "A2BCD: Asynchronous Acceleration with Optimal Complexity",
    "volume": "poster",
    "abstract": "In this paper, we propose the Asynchronous Accelerated Nonuniform Randomized Block Coordinate Descent algorithm (A2BCD). We prove A2BCD converges linearly to a solution of the convex minimization problem at the same rate as NU_ACDM, so long as the maximum delay is not too large. This is the first asynchronous Nesterov-accelerated algorithm that attains any provable speedup. Moreover, we then prove that these algorithms both have optimal complexity. Asynchronous algorithms complete much faster iterations, and A2BCD has optimal complexity. Hence we observe in experiments that A2BCD is the top-performing coordinate descent algorithm, converging up to 4-5x faster than NU_ACDM on some data sets in terms of wall-clock time. To motivate our theory and proof techniques, we also derive and analyze a continuous-time analog of our algorithm and prove it converges at the same rate",
    "checked": true,
    "id": "81f7e93492a1b3effb239d482d41b76093c44ec7",
    "semantic_title": "a2bcd: asynchronous acceleration with optimal complexity",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=r1lrAiA5Ym": {
    "title": "Backpropamine: training self-modifying neural networks with differentiable neuromodulated plasticity",
    "volume": "poster",
    "abstract": "The impressive lifelong learning in animal brains is primarily enabled by plastic changes in synaptic connectivity. Importantly, these changes are not passive, but are actively controlled by neuromodulation, which is itself under the control of the brain. The resulting self-modifying abilities of the brain play an important role in learning and adaptation, and are a major basis for biological reinforcement learning. Here we show for the first time that artificial neural networks with such neuromodulated plasticity can be trained with gradient descent. Extending previous work on differentiable Hebbian plasticity, we propose a differentiable formulation for the neuromodulation of plasticity. We show that neuromodulated plasticity improves the performance of neural networks on both reinforcement learning and supervised learning tasks. In one task, neuromodulated plastic LSTMs with millions of parameters outperform standard LSTMs on a benchmark language modeling task (controlling for the number of parameters). We conclude that differentiable neuromodulation of plasticity offers a powerful new framework for training neural networks",
    "checked": true,
    "id": "146c231532d4e38de95e63368dcd09d0f8cea291",
    "semantic_title": "backpropamine: training self-modifying neural networks with differentiable neuromodulated plasticity",
    "citation_count": 91,
    "authors": []
  },
  "https://openreview.net/forum?id=S1EERs09YQ": {
    "title": "Discovery of Natural Language Concepts in Individual Units of CNNs",
    "volume": "poster",
    "abstract": "Although deep convolutional networks have achieved improved performance in many natural language tasks, they have been treated as black boxes because they are difficult to interpret. Especially, little is known about how they represent language in their intermediate layers. In an attempt to understand the representations of deep convolutional networks trained on language tasks, we show that individual units are selectively responsive to specific morphemes, words, and phrases, rather than responding to arbitrary and uninterpretable patterns. In order to quantitatively analyze such intriguing phenomenon, we propose a concept alignment method based on how units respond to replicated text. We conduct analyses with different architectures on multiple datasets for classification and translation tasks and provide new insights into how deep models understand natural language",
    "checked": true,
    "id": "55c1222c959435867c12e87e2a5c9a7f3a5d1e4b",
    "semantic_title": "discovery of natural language concepts in individual units of cnns",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=ryzECoAcY7": {
    "title": "Learning Multi-Level Hierarchies with Hindsight",
    "volume": "poster",
    "abstract": "Hierarchical agents have the potential to solve sequential decision making tasks with greater sample efficiency than their non-hierarchical counterparts because hierarchical agents can break down tasks into sets of subtasks that only require short sequences of decisions. In order to realize this potential of faster learning, hierarchical agents need to be able to learn their multiple levels of policies in parallel so these simpler subproblems can be solved simultaneously. Yet, learning multiple levels of policies in parallel is hard because it is inherently unstable: changes in a policy at one level of the hierarchy may cause changes in the transition and reward functions at higher levels in the hierarchy, making it difficult to jointly learn multiple levels of policies. In this paper, we introduce a new Hierarchical Reinforcement Learning (HRL) framework, Hierarchical Actor-Critic (HAC), that can overcome the instability issues that arise when agents try to jointly learn multiple levels of policies. The main idea behind HAC is to train each level of the hierarchy independently of the lower levels by training each level as if the lower level policies are already optimal. We demonstrate experimentally in both grid world and simulated robotics domains that our approach can significantly accelerate learning relative to other non-hierarchical and hierarchical methods. Indeed, our framework is the first to successfully learn 3-level hierarchies in parallel in tasks with continuous state and action spaces",
    "checked": true,
    "id": "17704b148b5c20ddf92acbaf1addda134ecbb474",
    "semantic_title": "learning multi-level hierarchies with hindsight",
    "citation_count": 317,
    "authors": []
  },
  "https://openreview.net/forum?id=SkxXCi0qFX": {
    "title": "ProMP: Proximal Meta-Policy Search",
    "volume": "poster",
    "abstract": "Credit assignment in Meta-reinforcement learning (Meta-RL) is still poorly understood. Existing methods either neglect credit assignment to pre-adaptation behavior or implement it naively. This leads to poor sample-efficiency during meta-training as well as ineffective task identification strategies. This paper provides a theoretical analysis of credit assignment in gradient-based Meta-RL. Building on the gained insights we develop a novel meta-learning algorithm that overcomes both the issue of poor credit assignment and previous difficulties in estimating meta-policy gradients. By controlling the statistical distance of both pre-adaptation and adapted policies during meta-policy search, the proposed algorithm endows efficient and stable meta-learning. Our approach leads to superior pre-adaptation policy behavior and consistently outperforms previous Meta-RL algorithms in sample-efficiency, wall-clock time, and asymptotic performance",
    "checked": true,
    "id": "c456941a270cb2040eed4abfb39150508caf920c",
    "semantic_title": "promp: proximal meta-policy search",
    "citation_count": 212,
    "authors": []
  },
  "https://openreview.net/forum?id=HyM7AiA5YX": {
    "title": "Complement Objective Training",
    "volume": "poster",
    "abstract": "Learning with a primary objective, such as softmax cross entropy for classification and sequence generation, has been the norm for training deep neural networks for years. Although being a widely-adopted approach, using cross entropy as the primary objective exploits mostly the information from the ground-truth class for maximizing data likelihood, and largely ignores information from the complement (incorrect) classes. We argue that, in addition to the primary objective, training also using a complement objective that leverages information from the complement classes can be effective in improving model performance. This motivates us to study a new training paradigm that maximizes the likelihood of the ground-truth class while neutralizing the probabilities of the complement classes. We conduct extensive experiments on multiple tasks ranging from computer vision to natural language understanding. The experimental results confirm that, compared to the conventional training with just one primary objective, training also with the complement objective further improves the performance of the state-of-the-art models across all tasks. In addition to the accuracy improvement, we also show that models trained with both primary and complement objectives are more robust to single-step adversarial attacks",
    "checked": true,
    "id": "13d96d1f052254e3fe48077f12afd63b6a0c844a",
    "semantic_title": "complement objective training",
    "citation_count": 49,
    "authors": []
  },
  "https://openreview.net/forum?id=rJeXCo0cYX": {
    "title": "BabyAI: A Platform to Study the Sample Efficiency of Grounded Language Learning",
    "volume": "poster",
    "abstract": "Allowing humans to interactively train artificial agents to understand language instructions is desirable for both practical and scientific reasons. Though, given the lack of sample efficiency in current learning methods, reaching this goal may require substantial research efforts. We introduce the BabyAI research platform, with the goal of supporting investigations towards including humans in the loop for grounded language learning. The BabyAI platform comprises an extensible suite of 19 levels of increasing difficulty. Each level gradually leads the agent towards acquiring a combinatorially rich synthetic language, which is a proper subset of English. The platform also provides a hand-crafted bot agent, which simulates a human teacher. We report estimated amount of supervision required for training neural reinforcement and behavioral-cloning agents on some BabyAI levels. We put forward strong evidence that current deep learning methods are not yet sufficiently sample-efficient in the context of learning a language with compositional properties",
    "checked": true,
    "id": "1b19f433a3e8497e9d9bd67efb108521d16b5b85",
    "semantic_title": "babyai: a platform to study the sample efficiency of grounded language learning",
    "citation_count": 245,
    "authors": []
  },
  "https://openreview.net/forum?id=H1gMCsAqY7": {
    "title": "Slimmable Neural Networks",
    "volume": "poster",
    "abstract": "We present a simple and general method to train a single neural network executable at different widths (number of channels in a layer), permitting instant and adaptive accuracy-efficiency trade-offs at runtime. Instead of training individual networks with different width configurations, we train a shared network with switchable batch normalization. At runtime, the network can adjust its width on the fly according to on-device benchmarks and resource constraints, rather than downloading and offloading different models. Our trained networks, named slimmable neural networks, achieve similar (and in many cases better) ImageNet classification accuracy than individually trained models of MobileNet v1, MobileNet v2, ShuffleNet and ResNet-50 at different widths respectively. We also demonstrate better performance of slimmable models compared with individual ones across a wide range of applications including COCO bounding-box object detection, instance segmentation and person keypoint detection without tuning hyper-parameters. Lastly we visualize and discuss the learned features of slimmable networks. Code and models are available at: https://github.com/JiahuiYu/slimmable_networks",
    "checked": true,
    "id": "120ffccea4787b88f78b55b9302891ff96cb4228",
    "semantic_title": "slimmable neural networks",
    "citation_count": 565,
    "authors": []
  },
  "https://openreview.net/forum?id=HyxzRsR9Y7": {
    "title": "Learning Self-Imitating Diverse Policies",
    "volume": "poster",
    "abstract": "The success of popular algorithms for deep reinforcement learning, such as policy-gradients and Q-learning, relies heavily on the availability of an informative reward signal at each timestep of the sequential decision-making process. When rewards are only sparsely available during an episode, or a rewarding feedback is provided only after episode termination, these algorithms perform sub-optimally due to the difficultly in credit assignment. Alternatively, trajectory-based policy optimization methods, such as cross-entropy method and evolution strategies, do not require per-timestep rewards, but have been found to suffer from high sample complexity by completing forgoing the temporal nature of the problem. Improving the efficiency of RL algorithms in real-world problems with sparse or episodic rewards is therefore a pressing need. In this work, we introduce a self-imitation learning algorithm that exploits and explores well in the sparse and episodic reward settings. We view each policy as a state-action visitation distribution and formulate policy optimization as a divergence minimization problem. We show that with Jensen-Shannon divergence, this divergence minimization problem can be reduced into a policy-gradient algorithm with shaped rewards learned from experience replays. Experimental results indicate that our algorithm works comparable to existing algorithms in environments with dense rewards, and significantly better in environments with sparse and episodic rewards. We then discuss limitations of self-imitation learning, and propose to solve them by using Stein variational policy gradient descent with the Jensen-Shannon kernel to learn multiple diverse policies. We demonstrate its effectiveness on a challenging variant of continuous-control MuJoCo locomotion tasks",
    "checked": true,
    "id": "653bdfb3c35621ee04ee5d5253dc7e3a422d69e1",
    "semantic_title": "learning self-imitating diverse policies",
    "citation_count": 68,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgW0oA9FX": {
    "title": "Graph HyperNetworks for Neural Architecture Search",
    "volume": "poster",
    "abstract": "Neural architecture search (NAS) automatically finds the best task-specific neural network topology, outperforming many manual architecture designs. However, it can be prohibitively expensive as the search requires training thousands of different networks, while each training run can last for hours. In this work, we propose the Graph HyperNetwork (GHN) to amortize the search cost: given an architecture, it directly generates the weights by running inference on a graph neural network. GHNs model the topology of an architecture and therefore can predict network performance more accurately than regular hypernetworks and premature early stopping. To perform NAS, we randomly sample architectures and use the validation accuracy of networks with GHN generated weights as the surrogate search signal. GHNs are fast - they can search nearly 10√ó faster than other random search methods on CIFAR-10 and ImageNet. GHNs can be further extended to the anytime prediction setting, where they have found networks with better speed-accuracy tradeoff than the state-of-the-art manual designs",
    "checked": true,
    "id": "89c10e08902cb90abbe1276a3042b93c2f9c78b4",
    "semantic_title": "graph hypernetworks for neural architecture search",
    "citation_count": 281,
    "authors": []
  },
  "https://openreview.net/forum?id=ryxxCiRqYX": {
    "title": "Deep Layers as Stochastic Solvers",
    "volume": "poster",
    "abstract": "We provide a novel perspective on the forward pass through a block of layers in a deep network. In particular, we show that a forward pass through a standard dropout layer followed by a linear layer and a non-linear activation is equivalent to optimizing a convex objective with a single iteration of a $\\tau$-nice Proximal Stochastic Gradient method. We further show that replacing standard Bernoulli dropout with additive dropout is equivalent to optimizing the same convex objective with a variance-reduced proximal method. By expressing both fully-connected and convolutional layers as special cases of a high-order tensor product, we unify the underlying convex optimization problem in the tensor setting and derive a formula for the Lipschitz constant $L$ used to determine the optimal step size of the above proximal methods. We conduct experiments with standard convolutional networks applied to the CIFAR-10 and CIFAR-100 datasets and show that replacing a block of layers with multiple iterations of the corresponding solver, with step size set via $L$, consistently improves classification accuracy",
    "checked": true,
    "id": "b9df003fc73d4b6d3171c74cf1cd8ba35d92dda0",
    "semantic_title": "deep layers as stochastic solvers",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=S1lg0jAcYm": {
    "title": "ARM: Augment-REINFORCE-Merge Gradient for Stochastic Binary Networks",
    "volume": "poster",
    "abstract": "To backpropagate the gradients through stochastic binary layers, we propose the augment-REINFORCE-merge (ARM) estimator that is unbiased, exhibits low variance, and has low computational complexity. Exploiting variable augmentation, REINFORCE, and reparameterization, the ARM estimator achieves adaptive variance reduction for Monte Carlo integration by merging two expectations via common random numbers. The variance-reduction mechanism of the ARM estimator can also be attributed to either antithetic sampling in an augmented space, or the use of an optimal anti-symmetric \"self-control\" baseline function together with the REINFORCE estimator in that augmented space. Experimental results show the ARM estimator provides state-of-the-art performance in auto-encoding variational inference and maximum likelihood estimation, for discrete latent variable models with one or multiple stochastic binary layers. Python code for reproducible research is publicly available",
    "checked": true,
    "id": "97f0cff278f9131c22a2e26748a46511b761014b",
    "semantic_title": "arm: augment-reinforce-merge gradient for stochastic binary networks",
    "citation_count": 63,
    "authors": []
  },
  "https://openreview.net/forum?id=HyexAiA5Fm": {
    "title": "Scalable Unbalanced Optimal Transport using Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "Generative adversarial networks (GANs) are an expressive class of neural generative models with tremendous success in modeling high-dimensional continuous measures. In this paper, we present a scalable method for unbalanced optimal transport (OT) based on the generative-adversarial framework. We formulate unbalanced OT as a problem of simultaneously learning a transport map and a scaling factor that push a source measure to a target measure in a cost-optimal manner. We provide theoretical justification for this formulation, showing that it is closely related to an existing static formulation by Liero et al. (2018). We then propose an algorithm for solving this problem based on stochastic alternating gradient updates, similar in practice to GANs, and perform numerical experiments demonstrating how this methodology can be applied to population modeling",
    "checked": true,
    "id": "bdd151d4c522b924b2971b1139cd05c5b76c1513",
    "semantic_title": "scalable unbalanced optimal transport using generative adversarial networks",
    "citation_count": 77,
    "authors": []
  },
  "https://openreview.net/forum?id=rJe10iC5K7": {
    "title": "Unsupervised Discovery of Parts, Structure, and Dynamics",
    "volume": "poster",
    "abstract": "Humans easily recognize object parts and their hierarchical structure by watching how they move; they can then predict how each part moves in the future. In this paper, we propose a novel formulation that simultaneously learns a hierarchical, disentangled object representation and a dynamics model for object parts from unlabeled videos. Our Parts, Structure, and Dynamics (PSD) model learns to, first, recognize the object parts via a layered image representation; second, predict hierarchy via a structural descriptor that composes low-level concepts into a hierarchical structure; and third, model the system dynamics by predicting the future. Experiments on multiple real and synthetic datasets demonstrate that our PSD model works well on all three tasks: segmenting object parts, building their hierarchical structure, and capturing their motion distributions",
    "checked": true,
    "id": "4b64cd1fb5ee85e874a792c49271e4bf5314d6d7",
    "semantic_title": "unsupervised discovery of parts, structure, and dynamics",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=B1xJAsA5F7": {
    "title": "Learning Multimodal Graph-to-Graph Translation for Molecule Optimization",
    "volume": "poster",
    "abstract": "We view molecule optimization as a graph-to-graph translation problem. The goal is to learn to map from one molecular graph to another with better properties based on an available corpus of paired molecules. Since molecules can be optimized in different ways, there are multiple viable translations for each input graph. A key challenge is therefore to model diverse translation outputs. Our primary contributions include a junction tree encoder-decoder for learning diverse graph translations along with a novel adversarial training method for aligning distributions of molecules. Diverse output distributions in our model are explicitly realized by low-dimensional latent vectors that modulate the translation process. We evaluate our model on multiple molecule optimization tasks and show that our model outperforms previous state-of-the-art baselines by a significant margin",
    "checked": false,
    "id": "7975918889eb92138f0b06e86279b4b0ca3db303",
    "semantic_title": "learning multimodal graph-to-graph translation for molecular optimization",
    "citation_count": 232,
    "authors": []
  },
  "https://openreview.net/forum?id=HJxyAjRcFX": {
    "title": "Harmonizing Maximum Likelihood with GANs for Multimodal Conditional Generation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0dc02a63400c4f1899c952afdc620479238abedf",
    "semantic_title": "harmonizing maximum likelihood with gans for multimodal conditional generation",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=SkeRTsAcYm": {
    "title": "Phase-Aware Speech Enhancement with Deep Complex U-Net",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "101757dd6c7cb3704a54b25fd8ae0e8c0daee248",
    "semantic_title": "phase-aware speech enhancement with deep complex u-net",
    "citation_count": 334,
    "authors": []
  },
  "https://openreview.net/forum?id=rJgTTjA9tX": {
    "title": "The Comparative Power of ReLU Networks and Polynomial Kernels in the Presence of Sparse Latent Structure",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "99b4267421b63f1c018581bc87b263c0e5132910",
    "semantic_title": "the comparative power of relu networks and polynomial kernels in the presence of sparse latent structure",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=BJl6TjRcY7": {
    "title": "Neural Probabilistic Motor Primitives for Humanoid Control",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "96ae5d3ac1a1dcc365684bc92fcfa4d40d802bca",
    "semantic_title": "neural probabilistic motor primitives for humanoid control",
    "citation_count": 161,
    "authors": []
  },
  "https://openreview.net/forum?id=H1xipsA5K7": {
    "title": "Learning Two-layer Neural Networks with Symmetric Inputs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "40c36298ff685a0e1a2c83b52590250ed44664bd",
    "semantic_title": "learning two-layer neural networks with symmetric inputs",
    "citation_count": 59,
    "authors": []
  },
  "https://openreview.net/forum?id=SJzqpj09YQ": {
    "title": "Spectral Inference Networks: Unifying Deep and Spectral Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8910e3211989fdeb6123b542667ba4c717523596",
    "semantic_title": "spectral inference networks: unifying deep and spectral learning",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=Hkl5aoR5tm": {
    "title": "On Self Modulation for Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f91ff6c0aeba78f94f02b761446d5d911e6ab390",
    "semantic_title": "on self modulation for generative adversarial networks",
    "citation_count": 105,
    "authors": []
  },
  "https://openreview.net/forum?id=BJgK6iA5KX": {
    "title": "AutoLoss: Learning Discrete Schedule for Alternate Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d8e40c8af4a8777a1b85a50a6a2001b2e62e499c",
    "semantic_title": "autoloss: learning discrete schedule for alternate optimization",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=rJed6j0cKX": {
    "title": "Analyzing Inverse Problems with Invertible Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "25e433197844c239742f67fbb4171e913e0b9fe2",
    "semantic_title": "analyzing inverse problems with invertible neural networks",
    "citation_count": 497,
    "authors": []
  },
  "https://openreview.net/forum?id=S1gOpsCctm": {
    "title": "Learning Finite State Representations of Recurrent Policy Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bd858cef3d94faf6121b7f2bd6bb6c308db4aa16",
    "semantic_title": "learning finite state representations of recurrent policy networks",
    "citation_count": 88,
    "authors": []
  },
  "https://openreview.net/forum?id=SkMwpiR9Y7": {
    "title": "Measuring and regularizing networks in function space",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e3fee9244fc47aa9e80006e39352af90f64631fe",
    "semantic_title": "measuring and regularizing networks in function space",
    "citation_count": 140,
    "authors": []
  },
  "https://openreview.net/forum?id=BkgPajAcY7": {
    "title": "No Training Required: Exploring Random Encoders for Sentence Classification",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7a8f8109e65ed9a6048859681a825eb5655e5dd2",
    "semantic_title": "no training required: exploring random encoders for sentence classification",
    "citation_count": 99,
    "authors": []
  },
  "https://openreview.net/forum?id=SyVU6s05K7": {
    "title": "Deep Frank-Wolfe For Neural Network Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a97d86772364b7244eca65d48c5f9225f6fdc162",
    "semantic_title": "deep frank-wolfe for neural network optimization",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=S1fUpoR5FQ": {
    "title": "Quasi-hyperbolic momentum and Adam for deep learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8c62519ba7567b0c0016f9d49eefa2584e5df2cb",
    "semantic_title": "quasi-hyperbolic momentum and adam for deep learning",
    "citation_count": 134,
    "authors": []
  },
  "https://openreview.net/forum?id=rJNH6sAqY7": {
    "title": "On Computation and Generalization of Generative Adversarial Networks under Spectrum Control",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4292986f497324293091189f86003d47cab3f664",
    "semantic_title": "on computation and generalization of generative adversarial networks under spectrum control",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=HJMHpjC9Ym": {
    "title": "Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "425032e4fdd531cd7a5d3e7effc12d16f9c076d9",
    "semantic_title": "big-little net: an efficient multi-scale feature representation for visual and speech recognition",
    "citation_count": 99,
    "authors": []
  },
  "https://openreview.net/forum?id=BklHpjCqKm": {
    "title": "Deep Lagrangian Networks: Using Physics as Model Prior for Deep Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e129e344083b307e005c5342ba49524d9981a420",
    "semantic_title": "deep lagrangian networks: using physics as model prior for deep learning",
    "citation_count": 391,
    "authors": []
  },
  "https://openreview.net/forum?id=ByMVTsR5KQ": {
    "title": "Adversarial Audio Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0d3bbe47fe67e5a125a2547913ac0e4a30f18c8d",
    "semantic_title": "adversarial audio synthesis",
    "citation_count": 624,
    "authors": []
  },
  "https://openreview.net/forum?id=B1xVTjCqKQ": {
    "title": "A Data-Driven and Distributed Approach to Sparse Signal Representation and Recovery",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e34755989bc59b1092bd83393f21d42cbd68623e",
    "semantic_title": "a data-driven and distributed approach to sparse signal representation and recovery",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=HJlNpoA5YQ": {
    "title": "The Laplacian in RL: Learning Representations with Efficient Approximations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "af03168ecb019c4ba427b8386b9c0bc3695e917e",
    "semantic_title": "the laplacian in rl: learning representations with efficient approximations",
    "citation_count": 91,
    "authors": []
  },
  "https://openreview.net/forum?id=SkgEaj05t7": {
    "title": "On the Relation Between the Sharpest Directions of DNN Loss and the SGD Step Length",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5f9e2f6d4a844189b2e34da8fd0ba282f3f36c6f",
    "semantic_title": "on the relation between the sharpest directions of dnn loss and the sgd step length",
    "citation_count": 120,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk4fpoA5Km": {
    "title": "Discriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "876053977063ab843dd24c78425cbad1779a62ed",
    "semantic_title": "discriminator-actor-critic: addressing sample inefficiency and reward bias in adversarial imitation learning",
    "citation_count": 262,
    "authors": []
  },
  "https://openreview.net/forum?id=BkfbpsAcF7": {
    "title": "Excessive Invariance Causes Adversarial Vulnerability",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "67b72e427187b1113c787f9265926322e3d123e8",
    "semantic_title": "excessive invariance causes adversarial vulnerability",
    "citation_count": 167,
    "authors": []
  },
  "https://openreview.net/forum?id=H1ebTsActm": {
    "title": "Adaptivity of deep ReLU network for learning in Besov and mixed smooth Besov spaces: optimal rate and curse of dimensionality",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9f56bda06700297fe8a8aab4bb429451cee9a441",
    "semantic_title": "adaptivity of deep relu network for learning in besov and mixed smooth besov spaces: optimal rate and curse of dimensionality",
    "citation_count": 248,
    "authors": []
  },
  "https://openreview.net/forum?id=ryxepo0cFX": {
    "title": "AntisymmetricRNN: A Dynamical System View on Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e2c8a6b49cd999b16ac4dcfdc375563a6932b1c7",
    "semantic_title": "antisymmetricrnn: a dynamical system view on recurrent neural networks",
    "citation_count": 208,
    "authors": []
  },
  "https://openreview.net/forum?id=S1GkToR5tm": {
    "title": "Discriminator Rejection Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "866aa9bcb15cf4a23a0afed515fa2f6b93f91d11",
    "semantic_title": "discriminator rejection sampling",
    "citation_count": 131,
    "authors": []
  },
  "https://openreview.net/forum?id=r1My6sR9tX": {
    "title": "Unsupervised Learning via Meta-Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "37b3d9ab049c5671fed29f56cacee858d98c2ea8",
    "semantic_title": "unsupervised learning via meta-learning",
    "citation_count": 231,
    "authors": []
  },
  "https://openreview.net/forum?id=r1lyTjAqYX": {
    "title": "Recurrent Experience Replay in Distributed Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8ede7ddf99986d69562455bc8d69222fc3e27350",
    "semantic_title": "recurrent experience replay in distributed reinforcement learning",
    "citation_count": 500,
    "authors": []
  },
  "https://openreview.net/forum?id=rJlk6iRqKX": {
    "title": "Query-Efficient Hard-label Black-box Attack: An Optimization-based Approach",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b862efa06baea0b032214675eb3c3645d5d69d46",
    "semantic_title": "query-efficient hard-label black-box attack: an optimization-based approach",
    "citation_count": 350,
    "authors": []
  },
  "https://openreview.net/forum?id=SJzR2iRcK7": {
    "title": "Multi-class classification without multi-class labels",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5761b8de288b7622e94eb5addd6718313860f68b",
    "semantic_title": "multi-class classification without multi-class labels",
    "citation_count": 171,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgT3jRct7": {
    "title": "Large-Scale Answerer in Questioner's Mind for Visual Dialog Question Generation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0dde473b9280b115ae4405e5dc936a6e913aa573",
    "semantic_title": "large-scale answerer in questioner's mind for visual dialog question generation",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=HJe62s09tX": {
    "title": "Unsupervised Hyper-alignment for Multilingual Word Embeddings",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "115d6dffb5d6866c03ddec57a1779a1ba032613b",
    "semantic_title": "unsupervised hyperalignment for multilingual word embeddings",
    "citation_count": 71,
    "authors": []
  },
  "https://openreview.net/forum?id=SJx63jRqFm": {
    "title": "Diversity is All You Need: Learning Skills without a Reward Function",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5b01eaef54a653ba03ddd5a978690380fbc19bfc",
    "semantic_title": "diversity is all you need: learning skills without a reward function",
    "citation_count": 1089,
    "authors": []
  },
  "https://openreview.net/forum?id=Hyfn2jCcKm": {
    "title": "Solving the Rubik's Cube with Approximate Policy Iteration",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ff0dfe776b6c4b5ab2bc40ce7480736017b87127",
    "semantic_title": "solving the rubik's cube with approximate policy iteration",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=BJxh2j0qYm": {
    "title": "Dynamic Channel Pruning: Feature Boosting and Suppression",
    "volume": "poster",
    "abstract": "Making deep convolutional neural networks more accurate typically comes at the cost of increased computational and memory resources. In this paper, we reduce this cost by exploiting the fact that the importance of features computed by convolutional layers is highly input-dependent, and propose feature boosting and suppression (FBS), a new method to predictively amplify salient convolutional channels and skip unimportant ones at run-time. FBS introduces small auxiliary connections to existing convolutional layers. In contrast to channel pruning methods which permanently remove channels, it preserves the full network structures and accelerates convolution by dynamically skipping unimportant input and output channels. FBS-augmented networks are trained with conventional stochastic gradient descent, making it readily available for many state-of-the-art CNNs. We compare FBS to a range of existing channel pruning and dynamic execution schemes and demonstrate large improvements on ImageNet classification. Experiments show that FBS can respectively provide 5√ó and 2√ó savings in compute on VGG-16 and ResNet-18, both with less than 0.6% top-5 accuracy loss",
    "checked": true,
    "id": "a055b9917759dd75811edbc8500ca247b457c5b2",
    "semantic_title": "dynamic channel pruning: feature boosting and suppression",
    "citation_count": 321,
    "authors": []
  },
  "https://openreview.net/forum?id=SJl2niR9KQ": {
    "title": "Beyond Pixel Norm-Balls: Parametric Adversaries using an Analytically Differentiable Renderer",
    "volume": "poster",
    "abstract": "Many machine learning image classifiers are vulnerable to adversarial attacks, inputs with perturbations designed to intentionally trigger misclassification. Current adversarial methods directly alter pixel colors and evaluate against pixel norm-balls: pixel perturbations smaller than a specified magnitude, according to a measurement norm. This evaluation, however, has limited practical utility since perturbations in the pixel space do not correspond to underlying real-world phenomena of image formation that lead to them and has no security motivation attached. Pixels in natural images are measurements of light that has interacted with the geometry of a physical scene. As such, we propose a novel evaluation measure, parametric norm-balls, by directly perturbing physical parameters that underly image formation. One enabling contribution we present is a physically-based differentiable renderer that allows us to propagate pixel gradients to the parametric space of lighting and geometry. Our approach enables physically-based adversarial attacks, and our differentiable renderer leverages models from the interactive rendering literature to balance the performance and accuracy trade-offs necessary for a memory-efficient and scalable adversarial data augmentation workflow",
    "checked": true,
    "id": "742af5aa06ed8de12f8f77e248ff3fe5700061ff",
    "semantic_title": "beyond pixel norm-balls: parametric adversaries using an analytically differentiable renderer",
    "citation_count": 114,
    "authors": []
  },
  "https://openreview.net/forum?id=Hygn2o0qKX": {
    "title": "Deterministic PAC-Bayesian generalization bounds for deep networks via generalizing noise-resilience",
    "volume": "poster",
    "abstract": "The ability of overparameterized deep networks to generalize well has been linked to the fact that stochastic gradient descent (SGD) finds solutions that lie in flat, wide minima in the training loss -- minima where the output of the network is resilient to small random noise added to its parameters. So far this observation has been used to provide generalization guarantees only for neural networks whose parameters are either \\textit{stochastic} or \\textit{compressed}. In this work, we present a general PAC-Bayesian framework that leverages this observation to provide a bound on the original network learned -- a network that is deterministic and uncompressed. What enables us to do this is a key novelty in our approach: our framework allows us to show that if on training data, the interactions between the weight matrices satisfy certain conditions that imply a wide training loss minimum, these conditions themselves {\\em generalize} to the interactions between the matrices on test data, thereby implying a wide test loss minimum. We then apply our general framework in a setup where we assume that the pre-activation values of the network are not too small (although we assume this only on the training data). In this setup, we provide a generalization guarantee for the original (deterministic, uncompressed) network, that does not scale with product of the spectral norms of the weight matrices -- a guarantee that would not have been possible with prior approaches",
    "checked": true,
    "id": "0204871837acb118871e8d1bb59407da73142333",
    "semantic_title": "deterministic pac-bayesian generalization bounds for deep networks via generalizing noise-resilience",
    "citation_count": 101,
    "authors": []
  },
  "https://openreview.net/forum?id=r1lohoCqY7": {
    "title": "Learning-Based Frequency Estimation Algorithms",
    "volume": "poster",
    "abstract": "Estimating the frequencies of elements in a data stream is a fundamental task in data analysis and machine learning. The problem is typically addressed using streaming algorithms which can process very large data using limited storage. Today's streaming algorithms, however, cannot exploit patterns in their input to improve performance. We propose a new class of algorithms that automatically learn relevant patterns in the input data and use them to improve its frequency estimates. The proposed algorithms combine the benefits of machine learning with the formal guarantees available through algorithm theory. We prove that our learning-based algorithms have lower estimation errors than their non-learning counterparts. We also evaluate our algorithms on two real-world datasets and demonstrate empirically their performance gains",
    "checked": true,
    "id": "72c969a5dc5b236d511fbdaae88c443d14145ae8",
    "semantic_title": "learning-based frequency estimation algorithms",
    "citation_count": 162,
    "authors": []
  },
  "https://openreview.net/forum?id=Syxt2jC5FX": {
    "title": "From Hard to Soft: Understanding Deep Network Nonlinearities via Vector Quantization and Statistical Inference",
    "volume": "poster",
    "abstract": "Nonlinearity is crucial to the performance of a deep (neural) network (DN). To date there has been little progress understanding the menagerie of available nonlinearities, but recently progress has been made on understanding the r\\^{o}le played by piecewise affine and convex nonlinearities like the ReLU and absolute value activation functions and max-pooling. In particular, DN layers constructed from these operations can be interpreted as {\\em max-affine spline operators} (MASOs) that have an elegant link to vector quantization (VQ) and $K$-means. While this is good theoretical progress, the entire MASO approach is predicated on the requirement that the nonlinearities be piecewise affine and convex, which precludes important activation functions like the sigmoid, hyperbolic tangent, and softmax. {\\em This paper extends the MASO framework to these and an infinitely large class of new nonlinearities by linking deterministic MASOs with probabilistic Gaussian Mixture Models (GMMs).} We show that, under a GMM, piecewise affine, convex nonlinearities like ReLU, absolute value, and max-pooling can be interpreted as solutions to certain natural ``hard'' VQ inference problems, while sigmoid, hyperbolic tangent, and softmax can be interpreted as solutions to corresponding ``soft'' VQ inference problems. We further extend the framework by hybridizing the hard and soft VQ optimizations to create a $\\beta$-VQ inference that interpolates between hard, soft, and linear VQ inference. A prime example of a $\\beta$-VQ DN nonlinearity is the {\\em swish} nonlinearity, which offers state-of-the-art performance in a range of computer vision tasks but was developed ad hoc by experimentation. Finally, we validate with experiments an important assertion of our theory, namely that DN performance can be significantly improved by enforcing orthogonality in its linear filters",
    "checked": true,
    "id": "c630f37bf7d5ab6979216ca426e4620f104fba89",
    "semantic_title": "from hard to soft: understanding deep network nonlinearities via vector quantization and statistical inference",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=SkeK3s0qKQ": {
    "title": "Episodic Curiosity through Reachability",
    "volume": "poster",
    "abstract": "Rewards are sparse in the real world and most of today's reinforcement learning algorithms struggle with such sparsity. One solution to this problem is to allow the agent to create rewards for itself - thus making rewards dense and more suitable for learning. In particular, inspired by curious behaviour in animals, observing something novel could be rewarded with a bonus. Such bonus is summed up with the real task reward - making it possible for RL algorithms to learn from the combined reward. We propose a new curiosity method which uses episodic memory to form the novelty bonus. To determine the bonus, the current observation is compared with the observations in memory. Crucially, the comparison is done based on how many environment steps it takes to reach the current observation from those in memory - which incorporates rich information about environment dynamics. This allows us to overcome the known \"couch-potato\" issues of prior work - when the agent finds a way to instantly gratify itself by exploiting actions which lead to hardly predictable consequences. We test our approach in visually rich 3D environments in ViZDoom, DMLab and MuJoCo. In navigational tasks from ViZDoom and DMLab, our agent outperforms the state-of-the-art curiosity method ICM. In MuJoCo, an ant equipped with our curiosity module learns locomotion out of the first-person-view curiosity only. The code is available at https://github.com/google-research/episodic-curiosity/",
    "checked": true,
    "id": "fdfeeb14bbde2ab31b18e56b92d362dcd1b14f71",
    "semantic_title": "episodic curiosity through reachability",
    "citation_count": 271,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgK3oC5Fm": {
    "title": "Bayesian Prediction of Future Street Scenes using Synthetic Likelihoods",
    "volume": "poster",
    "abstract": "For autonomous agents to successfully operate in the real world, the ability to anticipate future scene states is a key competence. In real-world scenarios, future states become increasingly uncertain and multi-modal, particularly on long time horizons. Dropout based Bayesian inference provides a computationally tractable, theoretically well grounded approach to learn different hypotheses/models to deal with uncertain futures and make predictions that correspond well to observations -- are well calibrated. However, it turns out that such approaches fall short to capture complex real-world scenes, even falling behind in accuracy when compared to the plain deterministic approaches. This is because the used log-likelihood estimate discourages diversity. In this work, we propose a novel Bayesian formulation for anticipating future scene states which leverages synthetic likelihoods that encourage the learning of diverse models to accurately capture the multi-modal nature of future scene states. We show that our approach achieves accurate state-of-the-art predictions and calibrated probabilities through extensive experiments for scene anticipation on Cityscapes dataset. Moreover, we show that our approach generalizes across diverse tasks such as digit generation and precipitation forecasting",
    "checked": true,
    "id": "1c57f1d3c1191df543f81fb56c2ec5ded1bc5488",
    "semantic_title": "bayesian prediction of future street scenes using synthetic likelihoods",
    "citation_count": 46,
    "authors": []
  },
  "https://openreview.net/forum?id=S1eK3i09YQ": {
    "title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks",
    "volume": "poster",
    "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function. Our analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods",
    "checked": true,
    "id": "d6f6d1504cfedde4efb23e7ec0f42f006062c6a0",
    "semantic_title": "gradient descent provably optimizes over-parameterized neural networks",
    "citation_count": 1278,
    "authors": []
  },
  "https://openreview.net/forum?id=SJGvns0qK7": {
    "title": "Bayesian Policy Optimization for Model Uncertainty",
    "volume": "poster",
    "abstract": "Addressing uncertainty is critical for autonomous systems to robustly adapt to the real world. We formulate the problem of model uncertainty as a continuous Bayes-Adaptive Markov Decision Process (BAMDP), where an agent maintains a posterior distribution over latent model parameters given a history of observations and maximizes its expected long-term reward with respect to this belief distribution. Our algorithm, Bayesian Policy Optimization, builds on recent policy optimization algorithms to learn a universal policy that navigates the exploration-exploitation trade-off to maximize the Bayesian value function. To address challenges from discretizing the continuous latent parameter space, we propose a new policy network architecture that encodes the belief distribution independently from the observable state. Our method significantly outperforms algorithms that address model uncertainty without explicitly reasoning about belief distributions and is competitive with state-of-the-art Partially Observable Markov Decision Process solvers",
    "checked": true,
    "id": "4722d3e408c3687028e7f52ea6b7bf6b69981764",
    "semantic_title": "bayesian policy optimization for model uncertainty",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=rJlDnoA5Y7": {
    "title": "Von Mises-Fisher Loss for Training Sequence to Sequence Models with Continuous Outputs",
    "volume": "poster",
    "abstract": "The Softmax function is used in the final layer of nearly all existing sequence-to-sequence models for language generation. However, it is usually the slowest layer to compute which limits the vocabulary size to a subset of most frequent types; and it has a large memory footprint. We propose a general technique for replacing the softmax layer with a continuous embedding layer. Our primary innovations are a novel probabilistic loss, and a training and inference procedure in which we generate a probability distribution over pre-trained word embeddings, instead of a multinomial distribution over the vocabulary obtained via softmax. We evaluate this new class of sequence-to-sequence models with continuous outputs on the task of neural machine translation. We show that our models obtain upto 2.5x speed-up in training time while performing on par with the state-of-the-art models in terms of translation quality. These models are capable of handling very large vocabularies without compromising on translation quality. They also produce more meaningful errors than in the softmax-based models, as these errors typically lie in a subspace of the vector space of the reference translations",
    "checked": true,
    "id": "3f67f9c8a97dc2a0b7c40d1ced73294133e67aa1",
    "semantic_title": "von mises-fisher loss for training sequence to sequence models with continuous outputs",
    "citation_count": 72,
    "authors": []
  },
  "https://openreview.net/forum?id=Byx83s09Km": {
    "title": "Information-Directed Exploration for Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "Efficient exploration remains a major challenge for reinforcement learning. One reason is that the variability of the returns often depends on the current state and action, and is therefore heteroscedastic. Classical exploration strategies such as upper confidence bound algorithms and Thompson sampling fail to appropriately account for heteroscedasticity, even in the bandit setting. Motivated by recent findings that address this issue in bandits, we propose to use Information-Directed Sampling (IDS) for exploration in reinforcement learning. As our main contribution, we build on recent advances in distributional reinforcement learning and propose a novel, tractable approximation of IDS for deep Q-learning. The resulting exploration strategy explicitly accounts for both parametric uncertainty and heteroscedastic observation noise. We evaluate our method on Atari games and demonstrate a significant improvement over alternative approaches",
    "checked": true,
    "id": "a25b645c3d24f91164230a0ac5bb2d4ec88c1538",
    "semantic_title": "information-directed exploration for deep reinforcement learning",
    "citation_count": 73,
    "authors": []
  },
  "https://openreview.net/forum?id=r1gNni0qtm": {
    "title": "Generalized Tensor Models for Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "Recurrent Neural Networks (RNNs) are very successful at solving challenging problems with sequential data. However, this observed efficiency is not yet entirely explained by theory. It is known that a certain class of multiplicative RNNs enjoys the property of depth efficiency --- a shallow network of exponentially large width is necessary to realize the same score function as computed by such an RNN. Such networks, however, are not very often applied to real life tasks. In this work, we attempt to reduce the gap between theory and practice by extending the theoretical analysis to RNNs which employ various nonlinearities, such as Rectified Linear Unit (ReLU), and show that they also benefit from properties of universality and depth efficiency. Our theoretical results are verified by a series of extensive computational experiments",
    "checked": true,
    "id": "bfd01ae65d568801b908c6d5bb0e0704c9c5feab",
    "semantic_title": "generalized tensor models for recurrent neural networks",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=Syx72jC9tm": {
    "title": "Invariant and Equivariant Graph Networks",
    "volume": "poster",
    "abstract": "Invariant and equivariant networks have been successfully used for learning images, sets, point clouds, and graphs. A basic challenge in developing such networks is finding the maximal collection of invariant and equivariant \\emph{linear} layers. Although this question is answered for the first three examples (for popular transformations, at-least), a full characterization of invariant and equivariant linear layers for graphs is not known. In this paper we provide a characterization of all permutation invariant and equivariant linear layers for (hyper-)graph data, and show that their dimension, in case of edge-value graph data, is $2$ and $15$, respectively. More generally, for graph data defined on $k$-tuples of nodes, the dimension is the $k$-th and $2k$-th Bell numbers. Orthogonal bases for the layers are computed, including generalization to multi-graph data. The constant number of basis elements and their characteristics allow successfully applying the networks to different size graphs. From the theoretical point of view, our results generalize and unify recent advancement in equivariant deep learning. In particular, we show that our model is capable of approximating any message passing neural network. Applying these new linear layers in a simple deep neural network framework is shown to achieve comparable results to state-of-the-art and to have better expressivity than previous invariant and equivariant bases",
    "checked": true,
    "id": "6541afa4b4061a7d5c8387514bedea9dc249fd80",
    "semantic_title": "invariant and equivariant graph networks",
    "citation_count": 513,
    "authors": []
  },
  "https://openreview.net/forum?id=r1l73iRqKm": {
    "title": "Wizard of Wikipedia: Knowledge-Powered Conversational Agents",
    "volume": "poster",
    "abstract": "In open-domain dialogue intelligent agents should exhibit the use of knowledge, however there are few convincing demonstrations of this to date. The most popular sequence to sequence models typically \"generate and hope\" generic utterances that can be memorized in the weights of the model when mapping from input utterance(s) to output, rather than employing recalled knowledge as context. Use of knowledge has so far proved difficult, in part because of the lack of a supervised learning benchmark task which exhibits knowledgeable open dialogue with clear grounding. To that end we collect and release a large dataset with conversations directly grounded with knowledge retrieved from Wikipedia. We then design architectures capable of retrieving knowledge, reading and conditioning on it, and finally generating natural responses. Our best performing dialogue models are able to conduct knowledgeable discussions on open-domain topics as evaluated by automatic metrics and human evaluations, while our new benchmark allows for measuring further improvements in this important research direction",
    "checked": true,
    "id": "227458886343b86bd15adf58c769be326b4b058a",
    "semantic_title": "wizard of wikipedia: knowledge-powered conversational agents",
    "citation_count": 957,
    "authors": []
  },
  "https://openreview.net/forum?id=HkeGhoA5FX": {
    "title": "Residual Non-local Attention Networks for Image Restoration",
    "volume": "poster",
    "abstract": "In this paper, we propose a residual non-local attention network for high-quality image restoration. Without considering the uneven distribution of information in the corrupted images, previous methods are restricted by local convolutional operation and equal treatment of spatial- and channel-wise features. To address this issue, we design local and non-local attention blocks to extract features that capture the long-range dependencies between pixels and pay more attention to the challenging parts. Specifically, we design trunk branch and (non-)local mask branch in each (non-)local attention block. The trunk branch is used to extract hierarchical features. Local and non-local mask branches aim to adaptively rescale these hierarchical features with mixed attentions. The local mask branch concentrates on more local structures with convolutional operations, while non-local attention considers more about long-range dependencies in the whole feature map. Furthermore, we propose residual local and non-local attention learning to train the very deep network, which further enhance the representation ability of the network. Our proposed method can be generalized for various image restoration applications, such as image denoising, demosaicing, compression artifacts reduction, and super-resolution. Experiments demonstrate that our method obtains comparable or better results compared with recently leading methods quantitatively and visually",
    "checked": true,
    "id": "34439db81b482cd562e1cdba974c70a2b89cd6d4",
    "semantic_title": "residual non-local attention networks for image restoration",
    "citation_count": 683,
    "authors": []
  },
  "https://openreview.net/forum?id=ryGfnoC5KQ": {
    "title": "Kernel RNN Learning (KeRNL)",
    "volume": "poster",
    "abstract": "We describe Kernel RNN Learning (KeRNL), a reduced-rank, temporal eligibility trace-based approximation to backpropagation through time (BPTT) for training recurrent neural networks (RNNs) that gives competitive performance to BPTT on long time-dependence tasks. The approximation replaces a rank-4 gradient learning tensor, which describes how past hidden unit activations affect the current state, by a simple reduced-rank product of a sensitivity weight and a temporal eligibility trace. In this structured approximation motivated by node perturbation, the sensitivity weights and eligibility kernel time scales are themselves learned by applying perturbations. The rule represents another step toward biologically plausible or neurally inspired ML, with lower complexity in terms of relaxed architectural requirements (no symmetric return weights), a smaller memory demand (no unfolding and storage of states over time), and a shorter feedback time",
    "checked": true,
    "id": "c3e47eaff722e3950f5e2f9d8eb00fabf4d4d636",
    "semantic_title": "kernel rnn learning (kernl)",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=S1zz2i0cY7": {
    "title": "Integer Networks for Data Compression with Latent-Variable Models",
    "volume": "poster",
    "abstract": "We consider the problem of using variational latent-variable models for data compression. For such models to produce a compressed binary sequence, which is the universal data representation in a digital world, the latent representation needs to be subjected to entropy coding. Range coding as an entropy coding technique is optimal, but it can fail catastrophically if the computation of the prior differs even slightly between the sending and the receiving side. Unfortunately, this is a common scenario when floating point math is used and the sender and receiver operate on different hardware or software platforms, as numerical round-off is often platform dependent. We propose using integer networks as a universal solution to this problem, and demonstrate that they enable reliable cross-platform encoding and decoding of images using variational models",
    "checked": true,
    "id": "2405cb80949231747aa09af3fa8a11d5fff512d4",
    "semantic_title": "integer networks for data compression with latent-variable models",
    "citation_count": 71,
    "authors": []
  },
  "https://openreview.net/forum?id=BkgzniCqY7": {
    "title": "Structured Adversarial Attack: Towards General Implementation and Better Interpretability",
    "volume": "poster",
    "abstract": "When generating adversarial examples to attack deep neural networks (DNNs), Lp norm of the added perturbation is usually used to measure the similarity between original image and adversarial example. However, such adversarial attacks perturbing the raw input spaces may fail to capture structural information hidden in the input. This work develops a more general attack model, i.e., the structured attack (StrAttack), which explores group sparsity in adversarial perturbation by sliding a mask through images aiming for extracting key spatial structures. An ADMM (alternating direction method of multipliers)-based framework is proposed that can split the original problem into a sequence of analytically solvable subproblems and can be generalized to implement other attacking methods. Strong group sparsity is achieved in adversarial perturbations even with the same level of Lp-norm distortion (p‚àà {1,2,‚àû}) as the state-of-the-art attacks. We demonstrate the effectiveness of StrAttack by extensive experimental results on MNIST, CIFAR-10 and ImageNet. We also show that StrAttack provides better interpretability (i.e., better correspondence with discriminative image regions) through adversarial saliency map (Paper-not et al., 2016b) and class activation map (Zhou et al., 2016)",
    "checked": true,
    "id": "5bc67a8a47c796053d5ed77aaecd3cbbd4c5d4c1",
    "semantic_title": "structured adversarial attack: towards general implementation and better interpretability",
    "citation_count": 164,
    "authors": []
  },
  "https://openreview.net/forum?id=r1e13s05YX": {
    "title": "Neural network gradient-based learning of black-box function interfaces",
    "volume": "poster",
    "abstract": "Deep neural networks work well at approximating complicated functions when provided with data and trained by gradient descent methods. At the same time, there is a vast amount of existing functions that programmatically solve different tasks in a precise manner eliminating the need for training. In many cases, it is possible to decompose a task to a series of functions, of which for some we may prefer to use a neural network to learn the functionality, while for others the preferred method would be to use existing black-box functions. We propose a method for end-to-end training of a base neural network that integrates calls to existing black-box functions. We do so by approximating the black-box functionality with a differentiable neural network in a way that drives the base network to comply with the black-box function interface during the end-to-end optimization process. At inference time, we replace the differentiable estimator with its external black-box non-differentiable counterpart such that the base network output matches the input arguments of the black-box function. Using this ``Estimate and Replace'' paradigm, we train a neural network, end to end, to compute the input to black-box functionality while eliminating the need for intermediate labels. We show that by leveraging the existing precise black-box function during inference, the integrated model generalizes better than a fully differentiable model, and learns more efficiently compared to RL-based methods",
    "checked": true,
    "id": "25d5a513f7534c157ab1381d51ffe294773ffc91",
    "semantic_title": "neural network gradient-based learning of black-box function interfaces",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=BJx0sjC5FX": {
    "title": "RNNs implicitly implement tensor-product representations",
    "volume": "poster",
    "abstract": "Recurrent neural networks (RNNs) can learn continuous vector representations of symbolic structures such as sequences and sentences; these representations often exhibit linear regularities (analogies). Such regularities motivate our hypothesis that RNNs that show such regularities implicitly compile symbolic structures into tensor product representations (TPRs; Smolensky, 1990), which additively combine tensor products of vectors representing roles (e.g., sequence positions) and vectors representing fillers (e.g., particular words). To test this hypothesis, we introduce Tensor Product Decomposition Networks (TPDNs), which use TPRs to approximate existing vector representations. We demonstrate using synthetic data that TPDNs can successfully approximate linear and tree-based RNN autoencoder representations, suggesting that these representations exhibit interpretable compositional structure; we explore the settings that lead RNNs to induce such structure-sensitive representations. By contrast, further TPDN experiments show that the representations of four models trained to encode naturally-occurring sentences can be largely approximated with a bag of words, with only marginal improvements from more sophisticated structures. We conclude that TPDNs provide a powerful method for interpreting vector representations, and that standard RNNs can induce compositional sequence representations that are remarkably well approximated byTPRs; at the same time, existing training tasks for sentence representation learning may not be sufficient for inducing robust structural representations",
    "checked": false,
    "id": "d3ded34ff3378aadaa9a7c10e51cef6d04391a86",
    "semantic_title": "rnns implicitly implement tensor product representations",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=r1GAsjC5Fm": {
    "title": "Self-Monitoring Navigation Agent via Auxiliary Progress Estimation",
    "volume": "poster",
    "abstract": "The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at https://github.com/chihyaoma/selfmonitoring-agent",
    "checked": true,
    "id": "29e13746fa5aed13e51558a521a39aaeaa99c1b1",
    "semantic_title": "self-monitoring navigation agent via auxiliary progress estimation",
    "citation_count": 280,
    "authors": []
  },
  "https://openreview.net/forum?id=H1g6osRcFQ": {
    "title": "Policy Transfer with Strategy Optimization",
    "volume": "poster",
    "abstract": "Computer simulation provides an automatic and safe way for training robotic control policies to achieve complex tasks such as locomotion. However, a policy trained in simulation usually does not transfer directly to the real hardware due to the differences between the two environments. Transfer learning using domain randomization is a promising approach, but it usually assumes that the target environment is close to the distribution of the training environments, thus relying heavily on accurate system identification. In this paper, we present a different approach that leverages domain randomization for transferring control policies to unknown environments. The key idea that, instead of learning a single policy in the simulation, we simultaneously learn a family of policies that exhibit different behaviors. When tested in the target environment, we directly search for the best policy in the family based on the task performance, without the need to identify the dynamic parameters. We evaluate our method on five simulated robotic control problems with different discrepancies in the training and testing environment and demonstrate that our method can overcome larger modeling errors compared to training a robust policy or an adaptive policy",
    "checked": true,
    "id": "0e31f993257d0b25d7c18eb75a4ba39f7563bd81",
    "semantic_title": "policy transfer with strategy optimization",
    "citation_count": 81,
    "authors": []
  },
  "https://openreview.net/forum?id=rJg6ssC5Y7": {
    "title": "DeepOBS: A Deep Learning Optimizer Benchmark Suite",
    "volume": "poster",
    "abstract": "Because the choice and tuning of the optimizer affects the speed, and ultimately the performance of deep learning, there is significant past and recent research in this area. Yet, perhaps surprisingly, there is no generally agreed-upon protocol for the quantitative and reproducible evaluation of optimization strategies for deep learning. We suggest routines and benchmarks for stochastic optimization, with special focus on the unique aspects of deep learning, such as stochasticity, tunability and generalization. As the primary contribution, we present DeepOBS, a Python package of deep learning optimization benchmarks. The package addresses key challenges in the quantitative assessment of stochastic optimizers, and automates most steps of benchmarking. The library includes a wide and extensible set of ready-to-use realistic optimization problems, such as training Residual Networks for image classification on ImageNet or character-level language prediction models, as well as popular classics like MNIST and CIFAR-10. The package also provides realistic baseline results for the most popular optimizers on these test problems, ensuring a fair comparison to the competition when benchmarking new optimizers, and without having to run costly experiments. It comes with output back-ends that directly produce LaTeX code for inclusion in academic publications. It supports TensorFlow and is available open source",
    "checked": true,
    "id": "5794c7f8d703b91804c22f1ce37cb9288eaedc60",
    "semantic_title": "deepobs: a deep learning optimizer benchmark suite",
    "citation_count": 72,
    "authors": []
  },
  "https://openreview.net/forum?id=BJxhijAcY7": {
    "title": "signSGD with Majority Vote is Communication Efficient and Fault Tolerant",
    "volume": "poster",
    "abstract": "Training neural networks on large datasets can be accelerated by distributing the workload over a network of machines. As datasets grow ever larger, networks of hundreds or thousands of machines become economically viable. The time cost of communicating gradients limits the effectiveness of using such large machine counts, as may the increased chance of network faults. We explore a particularly simple algorithm for robust, communication-efficient learning---signSGD. Workers transmit only the sign of their gradient vector to a server, and the overall update is decided by a majority vote. This algorithm uses 32x less communication per iteration than full-precision, distributed SGD. Under natural conditions verified by experiment, we prove that signSGD converges in the large and mini-batch settings, establishing convergence for a parameter regime of Adam as a byproduct. Aggregating sign gradients by majority vote means that no individual worker has too much power. We prove that unlike SGD, majority vote is robust when up to 50% of workers behave adversarially. The class of adversaries we consider includes as special cases those that invert or randomise their gradient estimate. On the practical side, we built our distributed training system in Pytorch. Benchmarking against the state of the art collective communications library (NCCL), our framework---with the parameter server housed entirely on one machine---led to a 25% reduction in time for training resnet50 on Imagenet when using 15 AWS p3.2xlarge machines",
    "checked": true,
    "id": "dbca9dbe14e9933515d2005dc1163ae2c24d9afd",
    "semantic_title": "signsgd with majority vote is communication efficient and fault tolerant",
    "citation_count": 251,
    "authors": []
  },
  "https://openreview.net/forum?id=HkljioCcFQ": {
    "title": "MARGINALIZED AVERAGE ATTENTIONAL NETWORK FOR WEAKLY-SUPERVISED LEARNING",
    "volume": "poster",
    "abstract": "In weakly-supervised temporal action localization, previous works have failed to locate dense and integral regions for each entire action due to the overestimation of the most salient regions. To alleviate this issue, we propose a marginalized average attentional network (MAAN) to suppress the dominant response of the most salient regions in a principled manner. The MAAN employs a novel marginalized average aggregation (MAA) module and learns a set of latent discriminative probabilities in an end-to-end fashion. MAA samples multiple subsets from the video snippet features according to a set of latent discriminative probabilities and takes the expectation over all the averaged subset features. Theoretically, we prove that the MAA module with learned latent discriminative probabilities successfully reduces the difference in responses between the most salient regions and the others. Therefore, MAAN is able to generate better class activation sequences and identify dense and integral action regions in the videos. Moreover, we propose a fast algorithm to reduce the complexity of constructing MAA from $O(2^T)$ to $O(T^2)$. Extensive experiments on two large-scale video datasets show that our MAAN achieves a superior performance on weakly-supervised temporal action localization",
    "checked": true,
    "id": "6c97556edbc192896cc55395f8f21fe0ff148580",
    "semantic_title": "marginalized average attentional network for weakly-supervised learning",
    "citation_count": 82,
    "authors": []
  },
  "https://openreview.net/forum?id=SyGjjsC5tQ": {
    "title": "Stable Opponent Shaping in Differentiable Games",
    "volume": "poster",
    "abstract": "A growing number of learning methods are actually differentiable games whose players optimise multiple, interdependent objectives in parallel ‚Äì from GANs and intrinsic curiosity to multi-agent RL. Opponent shaping is a powerful approach to improve learning dynamics in these games, accounting for player influence on others' updates. Learning with Opponent-Learning Awareness (LOLA) is a recent algorithm that exploits this response and leads to cooperation in settings like the Iterated Prisoner's Dilemma. Although experimentally successful, we show that LOLA agents can exhibit ‚Äòarrogant' behaviour directly at odds with convergence. In fact, remarkably few algorithms have theoretical guarantees applying across all (n-player, non-convex) games. In this paper we present Stable Opponent Shaping (SOS), a new method that interpolates between LOLA and a stable variant named LookAhead. We prove that LookAhead converges locally to equilibria and avoids strict saddles in all differentiable games. SOS inherits these essential guarantees, while also shaping the learning of opponents and consistently either matching or outperforming LOLA experimentally",
    "checked": true,
    "id": "3cf906d2cc57f07244cd1f74ccb1ace8e70073cc",
    "semantic_title": "stable opponent shaping in differentiable games",
    "citation_count": 111,
    "authors": []
  },
  "https://openreview.net/forum?id=BJxssoA5KX": {
    "title": "Bounce and Learn: Modeling Scene Dynamics with Real-World Bounces",
    "volume": "poster",
    "abstract": "We introduce an approach to model surface properties governing bounces in everyday scenes. Our model learns end-to-end, starting from sensor inputs, to predict post-bounce trajectories and infer two underlying physical properties that govern bouncing - restitution and effective collision normals. Our model, Bounce and Learn, comprises two modules -- a Physics Inference Module (PIM) and a Visual Inference Module (VIM). VIM learns to infer physical parameters for locations in a scene given a single still image, while PIM learns to model physical interactions for the prediction task given physical parameters and observed pre-collision 3D trajectories. To achieve our results, we introduce the Bounce Dataset comprising 5K RGB-D videos of bouncing trajectories of a foam ball to probe surfaces of varying shapes and materials in everyday scenes including homes and offices. Our proposed model learns from our collected dataset of real-world bounces and is bootstrapped with additional information from simple physics simulations. We show on our newly collected dataset that our model out-performs baselines, including trajectory fitting with Newtonian physics, in predicting post-bounce trajectories and inferring physical properties of a scene",
    "checked": true,
    "id": "b39399b1b7c8d2950109b645552439a712913bf1",
    "semantic_title": "bounce and learn: modeling scene dynamics with real-world bounces",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=H1eqjiCctX": {
    "title": "Understanding Composition of Word Embeddings via Tensor Decomposition",
    "volume": "poster",
    "abstract": "Word embedding is a powerful tool in natural language processing. In this paper we consider the problem of word embedding composition \\--- given vector representations of two words, compute a vector for the entire phrase. We give a generative model that can capture specific syntactic relations between words. Under our model, we prove that the correlations between three words (measured by their PMI) form a tensor that has an approximate low rank Tucker decomposition. The result of the Tucker decomposition gives the word embeddings as well as a core tensor, which can be used to produce better compositions of the word embeddings. We also complement our theoretical results with experiments that verify our assumptions, and demonstrate the effectiveness of the new composition method",
    "checked": true,
    "id": "1ee096afc761526e4be9e3af11d6287d4a0a5393",
    "semantic_title": "understanding composition of word embeddings via tensor decomposition",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=rylqooRqK7": {
    "title": "SNAS: stochastic neural architecture search",
    "volume": "poster",
    "abstract": "We propose Stochastic Neural Architecture Search (SNAS), an economical end-to-end solution to Neural Architecture Search (NAS) that trains neural operation parameters and architecture distribution parameters in same round of back-propagation, while maintaining the completeness and differentiability of the NAS pipeline. In this work, NAS is reformulated as an optimization problem on parameters of a joint distribution for the search space in a cell. To leverage the gradient information in generic differentiable loss for architecture search, a novel search gradient is proposed. We prove that this search gradient optimizes the same objective as reinforcement-learning-based NAS, but assigns credits to structural decisions more efficiently. This credit assignment is further augmented with locally decomposable reward to enforce a resource-efficient constraint. In experiments on CIFAR-10, SNAS takes less epochs to find a cell architecture with state-of-the-art accuracy than non-differentiable evolution-based and reinforcement-learning-based NAS, which is also transferable to ImageNet. It is also shown that child networks of SNAS can maintain the validation accuracy in searching, with which attention-based NAS requires parameter retraining to compete, exhibiting potentials to stride towards efficient NAS on big datasets",
    "checked": true,
    "id": "3f0a2de309f21a957b4741dd68007eb08d9b12e3",
    "semantic_title": "snas: stochastic neural architecture search",
    "citation_count": 942,
    "authors": []
  },
  "https://openreview.net/forum?id=HJGciiR5Y7": {
    "title": "Latent Convolutional Models",
    "volume": "poster",
    "abstract": "We present a new latent model of natural images that can be learned on large-scale datasets. The learning process provides a latent embedding for every image in the training dataset, as well as a deep convolutional network that maps the latent space to the image space. After training, the new model provides a strong and universal image prior for a variety of image restoration tasks such as large-hole inpainting, superresolution, and colorization. To model high-resolution natural images, our approach uses latent spaces of very high dimensionality (one to two orders of magnitude higher than previous latent image models). To tackle this high dimensionality, we use latent spaces with a special manifold structure (convolutional manifolds) parameterized by a ConvNet of a certain architecture. In the experiments, we compare the learned latent models with latent models learned by autoencoders, advanced variants of generative adversarial networks, and a strong baseline system using simpler parameterization of the latent space. Our model outperforms the competing approaches over a range of restoration tasks",
    "checked": true,
    "id": "dcf4a22445f4486932fffd7014c320fa9bd91011",
    "semantic_title": "latent convolutional models",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=rkxciiC9tm": {
    "title": "NADPEx: An on-policy temporally consistent exploration method for deep reinforcement learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a88d54c168a84ed8a04d2a32be0b5939586b5792",
    "semantic_title": "nadpex: an on-policy temporally consistent exploration method for deep reinforcement learning",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=SkEYojRqtm": {
    "title": "Representation Degeneration Problem in Training Natural Language Generation Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "55e960535f643637161b2e99a8c21a92c0d13757",
    "semantic_title": "representation degeneration problem in training natural language generation models",
    "citation_count": 276,
    "authors": []
  },
  "https://openreview.net/forum?id=HyEtjoCqFX": {
    "title": "Soft Q-Learning with Mutual-Information Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b90793b029b2fa64e94d1ee7d5d2c7e1d1fcaaa9",
    "semantic_title": "soft q-learning with mutual-information regularization",
    "citation_count": 65,
    "authors": []
  },
  "https://openreview.net/forum?id=HyztsoC5Y7": {
    "title": "Learning to Adapt in Dynamic, Real-World Environments through Meta-Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "944bd3b472c8a30163bbfc1b5cbab8545693c3e0",
    "semantic_title": "learning to adapt in dynamic, real-world environments through meta-reinforcement learning",
    "citation_count": 560,
    "authors": []
  },
  "https://openreview.net/forum?id=SylKoo0cKm": {
    "title": "How Important is a Neuron",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "eb322f6f798fd1b381896d0b79f5498d89585b1f",
    "semantic_title": "how important is a neuron?",
    "citation_count": 134,
    "authors": []
  },
  "https://openreview.net/forum?id=BJeOioA9Y7": {
    "title": "Knowledge Flow: Improve Upon Your Teachers",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6dd986b2621d7f420ca80be5b71e12dece41e877",
    "semantic_title": "knowledge flow: improve upon your teachers",
    "citation_count": 63,
    "authors": []
  },
  "https://openreview.net/forum?id=BkG8sjR5Km": {
    "title": "Emergent Coordination Through Competition",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8a5c62f9c49a943b66fb1ae379442497609c8596",
    "semantic_title": "emergent coordination through competition",
    "citation_count": 153,
    "authors": []
  },
  "https://openreview.net/forum?id=Bkg8jjC9KQ": {
    "title": "Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0ffb2d5bbbc4f2cc547569337bd20cc77c1111d7",
    "semantic_title": "optimistic mirror descent in saddle-point problems: going the extra (gradient) mile",
    "citation_count": 297,
    "authors": []
  },
  "https://openreview.net/forum?id=S1gUsoR9YX": {
    "title": "Multilingual Neural Machine Translation with Knowledge Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1b24b7b4ac2427d20ab60c8451563eb8d99caf9c",
    "semantic_title": "multilingual neural machine translation with knowledge distillation",
    "citation_count": 251,
    "authors": []
  },
  "https://openreview.net/forum?id=H1ersoRqtm": {
    "title": "Structured Neural Summarization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2225b7c480dc627e68f03e5321383f27e12cb1d7",
    "semantic_title": "structured neural summarization",
    "citation_count": 214,
    "authors": []
  },
  "https://openreview.net/forum?id=rJxHsjRqFQ": {
    "title": "Hyperbolic Attention Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ebff4eb2f94dcf38171a5ca6a24ee95bc8e88c10",
    "semantic_title": "hyperbolic attention networks",
    "citation_count": 230,
    "authors": []
  },
  "https://openreview.net/forum?id=rkeSiiA5Fm": {
    "title": "Deep Learning 3D Shapes Using Alt-az Anisotropic 2-Sphere Convolution",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6410edc9a0b4c1a71696b70d7bd2853566e13738",
    "semantic_title": "deep learning 3d shapes using alt-az anisotropic 2-sphere convolution",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=SkeVsiAcYm": {
    "title": "Generative predecessor models for sample-efficient imitation learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b6eae428eed3e5f1965a14dd2c26acf5400df473",
    "semantic_title": "generative predecessor models for sample-efficient imitation learning",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=rJlEojAqFm": {
    "title": "Relational Forward Models for Multi-Agent Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6ce08c7259b1dca6dd6b1e19f1037e974b12621e",
    "semantic_title": "relational forward models for multi-agent learning",
    "citation_count": 79,
    "authors": []
  },
  "https://openreview.net/forum?id=SJVmjjR9FX": {
    "title": "Variational Bayesian Phylogenetic Inference",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c8d136aacf10bfade0630d796ca7f8716d867b51",
    "semantic_title": "variational bayesian phylogenetic inference",
    "citation_count": 55,
    "authors": []
  },
  "https://openreview.net/forum?id=rk4Qso0cKm": {
    "title": "Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c1f76891bdfa07d9a61ad11a15de13b139b20d2a",
    "semantic_title": "adv-bnn: improved adversarial defense through robust bayesian neural network",
    "citation_count": 171,
    "authors": []
  },
  "https://openreview.net/forum?id=ryf7ioRqFX": {
    "title": "h-detach: Modifying the LSTM Gradient Towards Better Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "916a119cf9365fd5d5d7d8befdc95e1e69e895de",
    "semantic_title": "h-detach: modifying the lstm gradient towards better optimization",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=HJgXsjA5tQ": {
    "title": "On the loss landscape of a class of deep neural networks with no bad local valleys",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e3300e3800fae17aa4a4e1e684359cbd272421d0",
    "semantic_title": "on the loss landscape of a class of deep neural networks with no bad local valleys",
    "citation_count": 88,
    "authors": []
  },
  "https://openreview.net/forum?id=BklMjsRqY7": {
    "title": "Accumulation Bit-Width Scaling For Ultra-Low Precision Training Of Deep Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e931f4444f634695bfab5a6e57c817da52fc512b",
    "semantic_title": "accumulation bit-width scaling for ultra-low precision training of deep networks",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=Bklfsi0cKm": {
    "title": "Deep Convolutional Networks as shallow Gaussian Processes",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "43e33e80d74205e860dd4b8e26b7c458c60e201a",
    "semantic_title": "deep convolutional networks as shallow gaussian processes",
    "citation_count": 271,
    "authors": []
  },
  "https://openreview.net/forum?id=S1VWjiRcKX": {
    "title": "Universal Successor Features Approximators",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "894536f2ac4728850bc18705daeeda6e88f3d6f1",
    "semantic_title": "universal successor features approximators",
    "citation_count": 118,
    "authors": []
  },
  "https://openreview.net/forum?id=SkeZisA5t7": {
    "title": "Adaptive Estimators Show Information Compression in Deep Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "46f41e44bfc53b7297134b2734980494c0833874",
    "semantic_title": "adaptive estimators show information compression in deep neural networks",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=H1MgjoR9tQ": {
    "title": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a35532918341dd582f1ebef56652ff7593f7d769",
    "semantic_title": "cbow is not all you need: combining cbow with the compositional matrix space model",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=SkNksoRctQ": {
    "title": "Fluctuation-dissipation relations for stochastic gradient descent",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3eb0f85ff84e4869387beffea2846c9243351aa7",
    "semantic_title": "fluctuation-dissipation relations for stochastic gradient descent",
    "citation_count": 75,
    "authors": []
  },
  "https://openreview.net/forum?id=HJGkisCcKm": {
    "title": "A Universal Music Translation Network",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "45ce9fce4a4eea9f72688885182aee0c84786fab",
    "semantic_title": "a universal music translation network",
    "citation_count": 111,
    "authors": []
  },
  "https://openreview.net/forum?id=ByxkijC5FQ": {
    "title": "Neural Persistence: A Complexity Measure for Deep Neural Networks Using Algebraic Topology",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2484ebfa2053999b4481ab2cd979eccb47cb0321",
    "semantic_title": "neural persistence: a complexity measure for deep neural networks using algebraic topology",
    "citation_count": 112,
    "authors": []
  },
  "https://openreview.net/forum?id=HyNA5iRcFQ": {
    "title": "Detecting Egregious Responses in Neural Sequence-to-sequence Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2272d5b2ebffa289bea56cf1c11efd6fef0c4c6f",
    "semantic_title": "detecting egregious responses in neural sequence-to-sequence models",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=HJz05o0qK7": {
    "title": "Measuring Compositionality in Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ba4cf6046d420af0ae86e2f4b587a8d50d219be3",
    "semantic_title": "measuring compositionality in representation learning",
    "citation_count": 150,
    "authors": []
  },
  "https://openreview.net/forum?id=HJMCcjAcYX": {
    "title": "Learning Representations of Sets through Optimized Permutations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8083cfb0e76358ab54f92eedbe13ed6a874a48e5",
    "semantic_title": "learning representations of sets through optimized permutations",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=H1gR5iR5FX": {
    "title": "Analysing Mathematical Reasoning Abilities of Neural Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "afed6dc6900d3b37e528b9086661bba583d60bf6",
    "semantic_title": "analysing mathematical reasoning abilities of neural models",
    "citation_count": 439,
    "authors": []
  },
  "https://openreview.net/forum?id=rkxacs0qY7": {
    "title": "FUNCTIONAL VARIATIONAL BAYESIAN NEURAL NETWORKS",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "69555845bf26bf930ecbfc223fa0ee454b2d58df",
    "semantic_title": "functional variational bayesian neural networks",
    "citation_count": 242,
    "authors": []
  },
  "https://openreview.net/forum?id=HygjqjR9Km": {
    "title": "Improving MMD-GAN Training with Repulsive Loss Function",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "15cc54ed7b1582b2efd71bedf28b23634d82991b",
    "semantic_title": "improving mmd-gan training with repulsive loss function",
    "citation_count": 80,
    "authors": []
  },
  "https://openreview.net/forum?id=rygjcsR9Y7": {
    "title": "SOM-VAE: Interpretable Discrete Representation Learning on Time Series",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e8c5f091420cb75013121dd02ee417ae974ecee1",
    "semantic_title": "som-vae: interpretable discrete representation learning on time series",
    "citation_count": 141,
    "authors": []
  },
  "https://openreview.net/forum?id=r1eiqi09K7": {
    "title": "Riemannian Adaptive Optimization Methods",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1f992463673ebd65840fe8d763da98dc94e04c17",
    "semantic_title": "riemannian adaptive optimization methods",
    "citation_count": 268,
    "authors": []
  },
  "https://openreview.net/forum?id=BJgqqsAct7": {
    "title": "Non-vacuous Generalization Bounds at the ImageNet Scale: a PAC-Bayesian Compression Approach",
    "volume": "poster",
    "abstract": "Modern neural networks are highly overparameterized, with capacity to substantially overfit to training data. Nevertheless, these networks often generalize well in practice. It has also been observed that trained networks can often be ``compressed to much smaller representations. The purpose of this paper is to connect these two empirical observations. Our main technical result is a generalization bound for compressed networks based on the compressed size that, combined with off-the-shelf compression algorithms, leads to state-of-the-art generalization guarantees. In particular, we provide the first non-vacuous generalization guarantees for realistic architectures applied to the ImageNet classification problem. Additionally, we show that compressibility of models that tend to overfit is limited. Empirical results show that an increase in overfitting increases the number of bits required to describe a trained network",
    "checked": true,
    "id": "74fe26e60f04055af52a88a513a1d6229ade5781",
    "semantic_title": "non-vacuous generalization bounds at the imagenet scale: a pac-bayesian compression approach",
    "citation_count": 218,
    "authors": []
  },
  "https://openreview.net/forum?id=rygqqsA9KX": {
    "title": "Learning Factorized Multimodal Representations",
    "volume": "poster",
    "abstract": "Learning multimodal representations is a fundamentally complex research problem due to the presence of multiple heterogeneous sources of information. Although the presence of multiple modalities provides additional valuable information, there are two key challenges to address when learning from multimodal data: 1) models must learn the complex intra-modal and cross-modal interactions for prediction and 2) models must be robust to unexpected missing or noisy modalities during testing. In this paper, we propose to optimize for a joint generative-discriminative objective across multimodal data and labels. We introduce a model that factorizes representations into two sets of independent factors: multimodal discriminative and modality-specific generative factors. Multimodal discriminative factors are shared across all modalities and contain joint multimodal features required for discriminative tasks such as sentiment prediction. Modality-specific generative factors are unique for each modality and contain the information required for generating data. Experimental results show that our model is able to learn meaningful multimodal representations that achieve state-of-the-art or competitive performance on six multimodal datasets. Our model demonstrates flexible generative capabilities by conditioning on independent factors and can reconstruct missing modalities without significantly impacting performance. Lastly, we interpret our factorized representations to understand the interactions that influence multimodal learning",
    "checked": true,
    "id": "034f1c5589644a6b42f50bf61b1628a1c5607fd9",
    "semantic_title": "learning factorized multimodal representations",
    "citation_count": 412,
    "authors": []
  },
  "https://openreview.net/forum?id=Syxt5oC5YQ": {
    "title": "Aggregated Momentum: Stability Through Passive Damping",
    "volume": "poster",
    "abstract": "Momentum is a simple and widely used trick which allows gradient-based optimizers to pick up speed along low curvature directions. Its performance depends crucially on a damping coefficient. Largecamping coefficients can potentially deliver much larger speedups, but are prone to oscillations and instability; hence one typically resorts to small values such as 0.5 or 0.9. We propose Aggregated Momentum (AggMo), a variant of momentum which combines multiple velocity vectors with different damping coefficients. AggMo is trivial to implement, but significantly dampens oscillations, enabling it to remain stable even for aggressive damping coefficients such as 0.999. We reinterpret Nesterov's accelerated gradient descent as a special case of AggMo and analyze rates of convergence for quadratic objectives. Empirically, we find that AggMo is a suitable drop-in replacement for other momentum methods, and frequently delivers faster convergence with little to no tuning",
    "checked": true,
    "id": "1de35bfced22b716db622d8946a66ac9c5d39690",
    "semantic_title": "aggregated momentum: stability through passive damping",
    "citation_count": 69,
    "authors": []
  },
  "https://openreview.net/forum?id=SJxu5iR9KQ": {
    "title": "Learning to Schedule Communication in Multi-agent Reinforcement Learning",
    "volume": "poster",
    "abstract": "Many real-world reinforcement learning tasks require multiple agents to make sequential decisions under the agents' interaction, where well-coordinated actions among the agents are crucial to achieve the target goal better at these tasks. One way to accelerate the coordination effect is to enable multiple agents to communicate with each other in a distributed manner and behave as a group. In this paper, we study a practical scenario when (i) the communication bandwidth is limited and (ii) the agents share the communication medium so that only a restricted number of agents are able to simultaneously use the medium, as in the state-of-the-art wireless networking standards. This calls for a certain form of communication scheduling. In that regard, we propose a multi-agent deep reinforcement learning framework, called SchedNet, in which agents learn how to schedule themselves, how to encode the messages, and how to select actions based on received messages. SchedNet is capable of deciding which agents should be entitled to broadcasting their (encoded) messages, by learning the importance of each agent's partially observed information. We evaluate SchedNet against multiple baselines under two different applications, namely, cooperative communication and navigation, and predator-prey. Our experiments show a non-negligible performance gap between SchedNet and other mechanisms such as the ones without communication and with vanilla scheduling methods, e.g., round robin, ranging from 32% to 43%",
    "checked": true,
    "id": "0732925ffaf7ee525a57786f7c9791491ef084db",
    "semantic_title": "learning to schedule communication in multi-agent reinforcement learning",
    "citation_count": 212,
    "authors": []
  },
  "https://openreview.net/forum?id=H1xD9sR5Fm": {
    "title": "Minimum Divergence vs. Maximum Margin: an Empirical Comparison on Seq2Seq Models",
    "volume": "poster",
    "abstract": "Sequence to sequence (seq2seq) models have become a popular framework for neural sequence prediction. While traditional seq2seq models are trained by Maximum Likelihood Estimation (MLE), much recent work has made various attempts to optimize evaluation scores directly to solve the mismatch between training and evaluation, since model predictions are usually evaluated by a task specific evaluation metric like BLEU or ROUGE scores instead of perplexity. This paper puts this existing work into two categories, a) minimum divergence, and b) maximum margin. We introduce a new training criterion based on the analysis of existing work, and empirically compare models in the two categories. Our experimental results show that our new training criterion can usually work better than existing methods, on both the tasks of machine translation and sentence summarization",
    "checked": true,
    "id": "ea49cef629b6e5a87a596944ad4ea9144651bfdd",
    "semantic_title": "minimum divergence vs. maximum margin: an empirical comparison on seq2seq models",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=ryGvcoA5YX": {
    "title": "Overcoming Catastrophic Forgetting for Continual Learning via Model Adaptation",
    "volume": "poster",
    "abstract": "Learning multiple tasks sequentially is important for the development of AI and lifelong learning systems. However, standard neural network architectures suffer from catastrophic forgetting which makes it difficult for them to learn a sequence of tasks. Several continual learning methods have been proposed to address the problem. In this paper, we propose a very different approach, called Parameter Generation and Model Adaptation (PGMA), to dealing with the problem. The proposed approach learns to build a model, called the solver, with two sets of parameters. The first set is shared by all tasks learned so far and the second set is dynamically generated to adapt the solver to suit each test example in order to classify it. Extensive experiments have been carried out to demonstrate the effectiveness of the proposed approach",
    "checked": true,
    "id": "48518df15244bb691b20e827aca53e407c63adef",
    "semantic_title": "overcoming catastrophic forgetting for continual learning via model adaptation",
    "citation_count": 128,
    "authors": []
  },
  "https://openreview.net/forum?id=Sklv5iRqYX": {
    "title": "Multi-Domain Adversarial Learning",
    "volume": "poster",
    "abstract": "Multi-domain learning (MDL) aims at obtaining a model with minimal average risk across multiple domains. Our empirical motivation is automated microscopy data, where cultured cells are imaged after being exposed to known and unknown chemical perturbations, and each dataset displays significant experimental bias. This paper presents a multi-domain adversarial learning approach, MuLANN, to leverage multiple datasets with overlapping but distinct class sets, in a semi-supervised setting. Our contributions include: i) a bound on the average- and worst-domain risk in MDL, obtained using the H-divergence; ii) a new loss to accommodate semi-supervised multi-domain learning and domain adaptation; iii) the experimental validation of the approach, improving on the state of the art on two standard image benchmarks, and a novel bioimage dataset, Cell",
    "checked": true,
    "id": "78e7507af6aa29d542eb4ee983a973157094b726",
    "semantic_title": "multi-domain adversarial learning",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=rJzLciCqKm": {
    "title": "Learning from Positive and Unlabeled Data with a Selection Bias",
    "volume": "poster",
    "abstract": "We consider the problem of learning a binary classifier only from positive data and unlabeled data (PU learning). Recent methods of PU learning commonly assume that the labeled positive data are identically distributed as the unlabeled positive data. However, this assumption is unrealistic in many instances of PU learning because it fails to capture the existence of a selection bias in the labeling process. When the data has a selection bias, it is difficult to learn the Bayes optimal classifier by conventional methods of PU learning. In this paper, we propose a method to partially identify the classifier. The proposed algorithm learns a scoring function that preserves the order induced by the class posterior under mild assumptions, which can be used as a classifier by setting an appropriate threshold. Through experiments, we show that the method outperforms previous methods for PU learning on various real-world datasets",
    "checked": true,
    "id": "f4b74295a2aeaff7fab34a414e1feb5e52d52cb8",
    "semantic_title": "learning from positive and unlabeled data with a selection bias",
    "citation_count": 102,
    "authors": []
  },
  "https://openreview.net/forum?id=BkeU5j0ctQ": {
    "title": "CEM-RL: Combining evolutionary and gradient-based methods for policy search",
    "volume": "poster",
    "abstract": "Deep neuroevolution and deep reinforcement learning (deep RL) algorithms are two popular approaches to policy search. The former is widely applicable and rather stable, but suffers from low sample efficiency. By contrast, the latter is more sample efficient, but the most sample efficient variants are also rather unstable and highly sensitive to hyper-parameter setting. So far, these families of methods have mostly been compared as competing tools. However, an emerging approach consists in combining them so as to get the best of both worlds. Two previously existing combinations use either an ad hoc evolutionary algorithm or a goal exploration process together with the Deep Deterministic Policy Gradient (DDPG) algorithm, a sample efficient off-policy deep RL algorithm. In this paper, we propose a different combination scheme using the simple cross-entropy method (CEM) and Twin Delayed Deep Deterministic policy gradient (TD3), another off-policy deep RL algorithm which improves over DDPG. We evaluate the resulting method, CEM-RL, on a set of benchmarks classically used in deep RL. We show that CEM-RL benefits from several advantages over its competitors and offers a satisfactory trade-off between performance and sample efficiency",
    "checked": true,
    "id": "d9e08e872c69e5acfed2a93ec6f6d623cfd0c680",
    "semantic_title": "cem-rl: combining evolutionary and gradient-based methods for policy search",
    "citation_count": 162,
    "authors": []
  },
  "https://openreview.net/forum?id=BylIciRcYQ": {
    "title": "SGD Converges to Global Minimum in Deep Learning via Star-convex Path",
    "volume": "poster",
    "abstract": "Stochastic gradient descent (SGD) has been found to be surprisingly effective in training a variety of deep neural networks. However, there is still a lack of understanding on how and why SGD can train these complex networks towards a global minimum. In this study, we establish the convergence of SGD to a global minimum for nonconvex optimization problems that are commonly encountered in neural network training. Our argument exploits the following two important properties: 1) the training loss can achieve zero value (approximately), which has been widely observed in deep learning; 2) SGD follows a star-convex path, which is verified by various experiments in this paper. In such a context, our analysis shows that SGD, although has long been considered as a randomized algorithm, converges in an intrinsically deterministic manner to a global minimum",
    "checked": true,
    "id": "b4a4d4c12e49110176847baaf6dd88b0dfeed285",
    "semantic_title": "sgd converges to global minimum in deep learning via star-convex path",
    "citation_count": 76,
    "authors": []
  },
  "https://openreview.net/forum?id=HJxB5sRcFQ": {
    "title": "LayoutGAN: Generating Graphic Layouts with Wireframe Discriminators",
    "volume": "poster",
    "abstract": "Layout is important for graphic design and scene generation. We propose a novel Generative Adversarial Network, called LayoutGAN, that synthesizes layouts by modeling geometric relations of different types of 2D elements. The generator of LayoutGAN takes as input a set of randomly-placed 2D graphic elements and uses self-attention modules to refine their labels and geometric parameters jointly to produce a realistic layout. Accurate alignment is critical for good layouts. We thus propose a novel differentiable wireframe rendering layer that maps the generated layout to a wireframe image, upon which a CNN-based discriminator is used to optimize the layouts in image space. We validate the effectiveness of LayoutGAN in various experiments including MNIST digit generation, document layout generation, clipart abstract scene generation and tangram graphic design",
    "checked": true,
    "id": "f3f4e8f1871622bd937a592b23fa7cc0a41bfbe0",
    "semantic_title": "layoutgan: generating graphic layouts with wireframe discriminators",
    "citation_count": 236,
    "authors": []
  },
  "https://openreview.net/forum?id=r1gEqiC9FX": {
    "title": "Equi-normalization of Neural Networks",
    "volume": "poster",
    "abstract": "Modern neural networks are over-parametrized. In particular, each rectified linear hidden unit can be modified by a multiplicative factor by adjusting input and out- put weights, without changing the rest of the network. Inspired by the Sinkhorn-Knopp algorithm, we introduce a fast iterative method for minimizing the l2 norm of the weights, equivalently the weight decay regularizer. It provably converges to a unique solution. Interleaving our algorithm with SGD during training improves the test accuracy. For small batches, our approach offers an alternative to batch- and group- normalization on CIFAR-10 and ImageNet with a ResNet-18",
    "checked": true,
    "id": "8dd29bd8bd4197a97d28614f3c13b18fc6c9e2b5",
    "semantic_title": "equi-normalization of neural networks",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=rkemqsC9Fm": {
    "title": "Information Theoretic lower bounds on negative log likelihood",
    "volume": "poster",
    "abstract": "In this article we use rate-distortion theory, a branch of information theory devoted to the problem of lossy compression, to shed light on an important problem in latent variable modeling of data: is there room to improve the model? One way to address this question is to find an upper bound on the probability (equivalently a lower bound on the negative log likelihood) that the model can assign to some data as one varies the prior and/or the likelihood function in a latent variable model. The core of our contribution is to formally show that the problem of optimizing priors in latent variable models is exactly an instance of the variational optimization problem that information theorists solve when computing rate-distortion functions, and then to use this to derive a lower bound on negative log likelihood. Moreover, we will show that if changing the prior can improve the log likelihood, then there is a way to change the likelihood function instead and attain the same log likelihood, and thus rate-distortion theory is of relevance to both optimizing priors as well as optimizing likelihood functions. We will experimentally argue for the usefulness of quantities derived from rate-distortion theory in latent variable modeling by applying them to a problem in image modeling",
    "checked": true,
    "id": "e0ac65da87d98a1af4a43a37a020a001ad478c3c",
    "semantic_title": "information theoretic lower bounds on negative log likelihood",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=B1xf9jAqFQ": {
    "title": "Neural Speed Reading with Structural-Jump-LSTM",
    "volume": "poster",
    "abstract": "Recurrent neural networks (RNNs) can model natural language by sequentially ''reading'' input tokens and outputting a distributed representation of each token. Due to the sequential nature of RNNs, inference time is linearly dependent on the input length, and all inputs are read regardless of their importance. Efforts to speed up this inference, known as ''neural speed reading'', either ignore or skim over part of the input. We present Structural-Jump-LSTM: the first neural speed reading model to both skip and jump text during inference. The model consists of a standard LSTM and two agents: one capable of skipping single words when reading, and one capable of exploiting punctuation structure (sub-sentence separators (,:), sentence end symbols (.!?), or end of text markers) to jump ahead after reading a word. A comprehensive experimental evaluation of our model against all five state-of-the-art neural reading models shows that Structural-Jump-LSTM achieves the best overall floating point operations (FLOP) reduction (hence is faster), while keeping the same accuracy or even improving it compared to a vanilla LSTM that reads the whole text",
    "checked": true,
    "id": "d647a64de005113f7bb5859347f5edca81bc0eec",
    "semantic_title": "neural speed reading with structural-jump-lstm",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=rklz9iAcKQ": {
    "title": "Deep Graph Infomax",
    "volume": "poster",
    "abstract": "We present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs---both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning",
    "checked": true,
    "id": "967a21a111757d6af7f7a25ca7ea2bdf6d505098",
    "semantic_title": "deep graph infomax",
    "citation_count": 2438,
    "authors": []
  },
  "https://openreview.net/forum?id=B1VZqjAcYX": {
    "title": "SNIP: SINGLE-SHOT NETWORK PRUNING BASED ON CONNECTION SENSITIVITY",
    "volume": "poster",
    "abstract": "Pruning large neural networks while maintaining their performance is often desirable due to the reduced space and time complexity. In existing methods, pruning is done within an iterative optimization procedure with either heuristically designed pruning schedules or additional hyperparameters, undermining their utility. In this work, we present a new approach that prunes a given network once at initialization prior to training. To achieve this, we introduce a saliency criterion based on connection sensitivity that identifies structurally important connections in the network for the given task. This eliminates the need for both pretraining and the complex pruning schedule while making it robust to architecture variations. After pruning, the sparse network is trained in the standard way. Our method obtains extremely sparse networks with virtually the same accuracy as the reference network on the MNIST, CIFAR-10, and Tiny-ImageNet classification tasks and is broadly applicable to various architectures including convolutional, residual and recurrent networks. Unlike existing methods, our approach enables us to demonstrate that the retained connections are indeed relevant to the given task",
    "checked": true,
    "id": "cf440ccce4a7a8681e238b4f26d5b95109add55d",
    "semantic_title": "snip: single-shot network pruning based on connection sensitivity",
    "citation_count": 1229,
    "authors": []
  },
  "https://openreview.net/forum?id=SJfb5jCqKm": {
    "title": "Bias-Reduced Uncertainty Estimation for Deep Neural Classifiers",
    "volume": "poster",
    "abstract": "We consider the problem of uncertainty estimation in the context of (non-Bayesian) deep neural classification. In this context, all known methods are based on extracting uncertainty signals from a trained network optimized to solve the classification problem at hand. We demonstrate that such techniques tend to introduce biased estimates for instances whose predictions are supposed to be highly confident. We argue that this deficiency is an artifact of the dynamics of training with SGD-like optimizers, and it has some properties similar to overfitting. Based on this observation, we develop an uncertainty estimation algorithm that selectively estimates the uncertainty of highly confident points, using earlier snapshots of the trained model, before their estimates are jittered (and way before they are ready for actual classification). We present extensive experiments indicating that the proposed algorithm provides uncertainty estimates that are consistently better than all known methods",
    "checked": true,
    "id": "dfb58733b367005945a2f2c2028145e4678efd7a",
    "semantic_title": "bias-reduced uncertainty estimation for deep neural classifiers",
    "citation_count": 142,
    "authors": []
  },
  "https://openreview.net/forum?id=rJfW5oA5KQ": {
    "title": "Approximability of Discriminators Implies Diversity in GANs",
    "volume": "poster",
    "abstract": "While Generative Adversarial Networks (GANs) have empirically produced impressive results on learning complex real-world distributions, recent works have shown that they suffer from lack of diversity or mode collapse. The theoretical work of Arora et al. (2017a) suggests a dilemma about GANs' statistical properties: powerful discriminators cause overfitting, whereas weak discriminators cannot detect mode collapse. By contrast, we show in this paper that GANs can in principle learn distributions in Wasserstein distance (or KL-divergence in many cases) with polynomial sample complexity, if the discriminator class has strong distinguishing power against the particular generator class (instead of against all possible generators). For various generator classes such as mixture of Gaussians, exponential families, and invertible and injective neural networks generators, we design corresponding discriminators (which are often neural nets of specific architectures) such that the Integral Probability Metric (IPM) induced by the discriminators can provably approximate the Wasserstein distance and/or KL-divergence. This implies that if the training is successful, then the learned distribution is close to the true distribution in Wasserstein distance or KL divergence, and thus cannot drop modes. Our preliminary experiments show that on synthetic datasets the test IPM is well correlated with KL divergence or the Wasserstein distance, indicating that the lack of diversity in GANs may be caused by the sub-optimality in optimization instead of statistical inefficiency",
    "checked": true,
    "id": "c3f03c67e119900e6f8f86740c445b46d0c5bcf3",
    "semantic_title": "approximability of discriminators implies diversity in gans",
    "citation_count": 74,
    "authors": []
  },
  "https://openreview.net/forum?id=B1xWcj0qYm": {
    "title": "On the Minimal Supervision for Training Any Binary Classifier from Only Unlabeled Data",
    "volume": "poster",
    "abstract": "Empirical risk minimization (ERM), with proper loss function and regularization, is the common practice of supervised classification. In this paper, we study training arbitrary (from linear to deep) binary classifier from only unlabeled (U) data by ERM. We prove that it is impossible to estimate the risk of an arbitrary binary classifier in an unbiased manner given a single set of U data, but it becomes possible given two sets of U data with different class priors. These two facts answer a fundamental question---what the minimal supervision is for training any binary classifier from only U data. Following these findings, we propose an ERM-based learning method from two sets of U data, and then prove it is consistent. Experiments demonstrate the proposed method could train deep models and outperform state-of-the-art methods for learning from two sets of U data",
    "checked": true,
    "id": "570f3c52e4e9608d65afd00076e784800c286524",
    "semantic_title": "on the minimal supervision for training any binary classifier from only unlabeled data",
    "citation_count": 89,
    "authors": []
  },
  "https://openreview.net/forum?id=r1NJqsRctX": {
    "title": "Auxiliary Variational MCMC",
    "volume": "poster",
    "abstract": "We introduce Auxiliary Variational MCMC, a novel framework for learning MCMC kernels that combines recent advances in variational inference with insights drawn from traditional auxiliary variable MCMC methods such as Hamiltonian Monte Carlo. Our framework exploits low dimensional structure in the target distribution in order to learn a more efficient MCMC sampler. The resulting sampler is able to suppress random walk behaviour and mix between modes efficiently, without the need to compute gradients of the target distribution. We test our sampler on a number of challenging distributions, where the underlying structure is known, and on the task of posterior sampling in Bayesian logistic regression. Code to reproduce all experiments is available at https://github.com/AVMCMC/AuxiliaryVariationalMCMC",
    "checked": true,
    "id": "ebbab9a45b241de9600f55d68025e2890333a942",
    "semantic_title": "auxiliary variational mcmc",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=S1zk9iRqF7": {
    "title": "PATE-GAN: Generating Synthetic Data with Differential Privacy Guarantees",
    "volume": "poster",
    "abstract": "Machine learning has the potential to assist many communities in using the large datasets that are becoming more and more available. Unfortunately, much of that potential is not being realized because it would require sharing data in a way that compromises privacy. In this paper, we investigate a method for ensuring (differential) privacy of the generator of the Generative Adversarial Nets (GAN) framework. The resulting model can be used for generating synthetic data on which algorithms can be trained and validated, and on which competitions can be conducted, without compromising the privacy of the original dataset. Our method modifies the Private Aggregation of Teacher Ensembles (PATE) framework and applies it to GANs. Our modified framework (which we call PATE-GAN) allows us to tightly bound the influence of any individual sample on the model, resulting in tight differential privacy guarantees and thus an improved performance over models with the same guarantees. We also look at measuring the quality of synthetic data from a new angle; we assert that for the synthetic data to be useful for machine learning researchers, the relative performance of two algorithms (trained and tested) on the synthetic dataset should be the same as their relative performance (when trained and tested) on the original dataset. Our experiments, on various datasets, demonstrate that PATE-GAN consistently outperforms the state-of-the-art method with respect to this and other notions of synthetic data quality",
    "checked": true,
    "id": "af1841e1db6579f1f1777a59c7e9e4658d2ac466",
    "semantic_title": "pate-gan: generating synthetic data with differential privacy guarantees",
    "citation_count": 669,
    "authors": []
  },
  "https://openreview.net/forum?id=r1f0YiCctm": {
    "title": "Minimal Random Code Learning: Getting Bits Back from Compressed Model Parameters",
    "volume": "poster",
    "abstract": "While deep neural networks are a highly successful model class, their large memory footprint puts considerable strain on energy consumption, communication bandwidth, and storage requirements. Consequently, model size reduction has become an utmost goal in deep learning. A typical approach is to train a set of deterministic weights, while applying certain techniques such as pruning and quantization, in order that the empirical weight distribution becomes amenable to Shannon-style coding schemes. However, as shown in this paper, relaxing weight determinism and using a full variational distribution over weights allows for more efficient coding schemes and consequently higher compression rates. In particular, following the classical bits-back argument, we encode the network weights using a random sample, requiring only a number of bits corresponding to the Kullback-Leibler divergence between the sampled variational distribution and the encoding distribution. By imposing a constraint on the Kullback-Leibler divergence, we are able to explicitly control the compression rate, while optimizing the expected loss on the training set. The employed encoding scheme can be shown to be close to the optimal information-theoretical lower bound, with respect to the employed variational family. Our method sets new state-of-the-art in neural network compression, as it strictly dominates previous approaches in a Pareto sense: On the benchmarks LeNet-5/MNIST and VGG-16/CIFAR-10, our approach yields the best test performance for a fixed memory budget, and vice versa, it achieves the highest compression rates for a fixed test performance",
    "checked": true,
    "id": "90b60322be13e63c9903b40b3407861e001eba1d",
    "semantic_title": "minimal random code learning: getting bits back from compressed model parameters",
    "citation_count": 82,
    "authors": []
  },
  "https://openreview.net/forum?id=ryf6Fs09YX": {
    "title": "GO Gradient for Expectation-Based Objectives",
    "volume": "poster",
    "abstract": "Within many machine learning algorithms, a fundamental problem concerns efficient calculation of an unbiased gradient wrt parameters $\\boldsymbol{\\gamma}$ for expectation-based objectives $\\mathbb{E}_{q_{\\boldsymbol{\\gamma}} (\\boldsymbol{y})} [f (\\boldsymbol{y}) ]$. Most existing methods either ($i$) suffer from high variance, seeking help from (often) complicated variance-reduction techniques; or ($ii$) they only apply to reparameterizable continuous random variables and employ a reparameterization trick. To address these limitations, we propose a General and One-sample (GO) gradient that ($i$) applies to many distributions associated with non-reparameterizable continuous {\\em or} discrete random variables, and ($ii$) has the same low-variance as the reparameterization trick. We find that the GO gradient often works well in practice based on only one Monte Carlo sample (although one can of course use more samples if desired). Alongside the GO gradient, we develop a means of propagating the chain rule through distributions, yielding statistical back-propagation, coupling neural networks to common random variables",
    "checked": true,
    "id": "15d0330ef349310db06e7f9babd8f09970905b51",
    "semantic_title": "go gradient for expectation-based objectives",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=HJz6tiCqYm": {
    "title": "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations",
    "volume": "poster",
    "abstract": "In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, ImageNet-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called ImageNet-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize",
    "checked": true,
    "id": "49b64383fe36268410c430352637ed23b16820c5",
    "semantic_title": "benchmarking neural network robustness to common corruptions and perturbations",
    "citation_count": 3526,
    "authors": []
  },
  "https://openreview.net/forum?id=HkxaFoC9KQ": {
    "title": "Deep reinforcement learning with relational inductive biases",
    "volume": "poster",
    "abstract": "We introduce an approach for augmenting model-free deep reinforcement learning agents with a mechanism for relational reasoning over structured representations, which improves performance, learning efficiency, generalization, and interpretability. Our architecture encodes an image as a set of vectors, and applies an iterative message-passing procedure to discover and reason about relevant entities and relations in a scene. In six of seven StarCraft II Learning Environment mini-games, our agent achieved state-of-the-art performance, and surpassed human grandmaster-level on four. In a novel navigation and planning task, our agent's performance and learning efficiency far exceeded non-relational baselines, it was able to generalize to more complex scenes than it had experienced during training. Moreover, when we examined its learned internal representations, they reflected important structure about the problem and the agent's intentions. The main contribution of this work is to introduce techniques for representing and reasoning about states in model-free deep reinforcement learning agents via relational inductive biases. Our experiments show this approach can offer advantages in efficiency, generalization, and interpretability, and can scale up to meet some of the most challenging test environments in modern artificial intelligence",
    "checked": true,
    "id": "9ea92ebeb7462f2db346cfa3281ad7497b1063d6",
    "semantic_title": "deep reinforcement learning with relational inductive biases",
    "citation_count": 209,
    "authors": []
  },
  "https://openreview.net/forum?id=S1E3Ko09F7": {
    "title": "L-Shapley and C-Shapley: Efficient Model Interpretation for Structured Data",
    "volume": "poster",
    "abstract": "Instancewise feature scoring is a method for model interpretation, which yields, for each test instance, a vector of importance scores associated with features. Methods based on the Shapley score have been proposed as a fair way of computing feature attributions, but incur an exponential complexity in the number of features. This combinatorial explosion arises from the definition of Shapley value and prevents these methods from being scalable to large data sets and complex models. We focus on settings in which the data have a graph structure, and the contribution of features to the target variable is well-approximated by a graph-structured factorization. In such settings, we develop two algorithms with linear complexity for instancewise feature importance scoring on black-box models. We establish the relationship of our methods to the Shapley value and a closely related concept known as the Myerson value from cooperative game theory. We demonstrate on both language and image data that our algorithms compare favorably with other methods using both quantitative metrics and human evaluation",
    "checked": true,
    "id": "85634f8a458f13d7cb80e946eec23880295ba1a3",
    "semantic_title": "l-shapley and c-shapley: efficient model interpretation for structured data",
    "citation_count": 218,
    "authors": []
  },
  "https://openreview.net/forum?id=S1x2Fj0qKQ": {
    "title": "Whitening and Coloring Batch Transform for GANs",
    "volume": "poster",
    "abstract": "Batch Normalization (BN) is a common technique used to speed-up and stabilize training. On the other hand, the learnable parameters of BN are commonly used in conditional Generative Adversarial Networks (cGANs) for representing class-specific information using conditional Batch Normalization (cBN). In this paper we propose to generalize both BN and cBN using a Whitening and Coloring based batch normalization. We show that our conditional Coloring can represent categorical conditioning information which largely helps the cGAN qualitative results. Moreover, we show that full-feature whitening is important in a general GAN scenario in which the training process is known to be highly unstable. We test our approach on different datasets and using different GAN networks and training protocols, showing a consistent improvement in all the tested frameworks. Our CIFAR-10 conditioned results are higher than all previous works on this dataset",
    "checked": true,
    "id": "e5cf28063f76ad1a3cda56f68b08ff264a0aa198",
    "semantic_title": "whitening and coloring batch transform for gans",
    "citation_count": 52,
    "authors": []
  },
  "https://openreview.net/forum?id=Sk4jFoA9K7": {
    "title": "PeerNets: Exploiting Peer Wisdom Against Adversarial Attacks",
    "volume": "poster",
    "abstract": "Deep learning systems have become ubiquitous in many aspects of our lives. Unfortunately, it has been shown that such systems are vulnerable to adversarial attacks, making them prone to potential unlawful uses. Designing deep neural networks that are robust to adversarial attacks is a fundamental step in making such systems safer and deployable in a broader variety of applications (e.g. autonomous driving), but more importantly is a necessary step to design novel and more advanced architectures built on new computational paradigms rather than marginally building on the existing ones. In this paper we introduce PeerNets, a novel family of convolutional networks alternating classical Euclidean convolutions with graph convolutions to harness information from a graph of peer samples. This results in a form of non-local forward propagation in the model, where latent features are conditioned on the global structure induced by the graph, that is up to 3 times more robust to a variety of white- and black-box adversarial attacks compared to conventional architectures with almost no drop in accuracy",
    "checked": true,
    "id": "bfb0d179916c000d54f27e7a9ea18b6269963e74",
    "semantic_title": "peernets: exploiting peer wisdom against adversarial attacks",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=B1gstsCqt7": {
    "title": "Sparse Dictionary Learning by Dynamical Neural Networks",
    "volume": "poster",
    "abstract": "A dynamical neural network consists of a set of interconnected neurons that interact over time continuously. It can exhibit computational properties in the sense that the dynamical system's evolution and/or limit points in the associated state space can correspond to numerical solutions to certain mathematical optimization or learning problems. Such a computational system is particularly attractive in that it can be mapped to a massively parallel computer architecture for power and throughput efficiency, especially if each neuron can rely solely on local information (i.e., local memory). Deriving gradients from the dynamical network's various states while conforming to this last constraint, however, is challenging. We show that by combining ideas of top-down feedback and contrastive learning, a dynamical network for solving the l1-minimizing dictionary learning problem can be constructed, and the true gradients for learning are provably computable by individual neurons. Using spiking neurons to construct our dynamical network, we present a learning process, its rigorous mathematical analysis, and numerical results on several dictionary learning problems",
    "checked": true,
    "id": "33cacec6bc67f52f611845b8c6a88feaa5f88d09",
    "semantic_title": "sparse dictionary learning by dynamical neural networks",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=HkxjYoCqKX": {
    "title": "Relaxed Quantization for Discretized Neural Networks",
    "volume": "poster",
    "abstract": "Neural network quantization has become an important research area due to its great impact on deployment of large models on resource constrained devices. In order to train networks that can be effectively discretized without loss of performance, we introduce a differentiable quantization procedure. Differentiability can be achieved by transforming continuous distributions over the weights and activations of the network to categorical distributions over the quantization grid. These are subsequently relaxed to continuous surrogates that can allow for efficient gradient-based optimization. We further show that stochastic rounding can be seen as a special case of the proposed approach and that under this formulation the quantization grid itself can also be optimized with gradient descent. We experimentally validate the performance of our method on MNIST, CIFAR 10 and Imagenet classification",
    "checked": false,
    "id": "cc34e129335325010c99af288a0b9812c7a7685e",
    "semantic_title": "additive powers-of-two quantization: a non-uniform discretization for neural networks",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=HkgqFiAcFm": {
    "title": "Marginal Policy Gradients: A Unified Family of Estimators for Bounded Action Spaces with Applications",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a2c7c3b8ffe918912f25cf7f476d3b6f603e8153",
    "semantic_title": "marginal policy gradients: a unified family of estimators for bounded action spaces with applications",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=H1gKYo09tX": {
    "title": "code2seq: Generating Sequences from Structured Representations of Code",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "98d1307bed619b58b4a44acd8e65ac58495776c2",
    "semantic_title": "code2seq: generating sequences from structured representations of code",
    "citation_count": 704,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk4dFjR5K7": {
    "title": "ADef: an Iterative Algorithm to Construct Adversarial Deformations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3ecda636a99ec93acc941c0217a65c9a3af9562f",
    "semantic_title": "adef: an iterative algorithm to construct adversarial deformations",
    "citation_count": 99,
    "authors": []
  },
  "https://openreview.net/forum?id=rke_YiRct7": {
    "title": "Small nonlinearities in activation functions create bad local minima in neural networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ab0813895b5a7df2bffd34808a8974d7220ac36b",
    "semantic_title": "small nonlinearities in activation functions create bad local minima in neural networks",
    "citation_count": 95,
    "authors": []
  },
  "https://openreview.net/forum?id=SyNvti09KQ": {
    "title": "Visceral Machines: Risk-Aversion in Reinforcement Learning with Intrinsic Physiological Rewards",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "56cf1351c926a3f837d0c53eda7d2f58ececf6d6",
    "semantic_title": "visceral machines: risk-aversion in reinforcement learning with intrinsic physiological rewards",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=SJfPFjA9Fm": {
    "title": "ACCELERATING NONCONVEX LEARNING VIA REPLICA EXCHANGE LANGEVIN DIFFUSION",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d2eff663ca365f3f4d05ff3819a81dabf7400a9a",
    "semantic_title": "accelerating nonconvex learning via replica exchange langevin diffusion",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=rJevYoA9Fm": {
    "title": "The Singular Values of Convolutional Layers",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "fb2edf25484c9e9e5f94b719c55dc1faf7591bfa",
    "semantic_title": "the singular values of convolutional layers",
    "citation_count": 204,
    "authors": []
  },
  "https://openreview.net/forum?id=ByxPYjC5KQ": {
    "title": "Improving Generalization and Stability of Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a7dbdcac11bc300d73e0352fc0422b57ff075f51",
    "semantic_title": "improving generalization and stability of generative adversarial networks",
    "citation_count": 116,
    "authors": []
  },
  "https://openreview.net/forum?id=r1xwKoR9Y7": {
    "title": "GamePad: A Learning Environment for Theorem Proving",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "87c425f23bcac2f082968abda64a971f91522f73",
    "semantic_title": "gamepad: a learning environment for theorem proving",
    "citation_count": 111,
    "authors": []
  },
  "https://openreview.net/forum?id=HJlLKjR9FQ": {
    "title": "Towards Understanding Regularization in Batch Normalization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9fa42ea422fe1729e04a98fa8072a7b48ce91bc8",
    "semantic_title": "towards understanding regularization in batch normalization",
    "citation_count": 180,
    "authors": []
  },
  "https://openreview.net/forum?id=SylLYsCcFm": {
    "title": "Learning to Make Analogies by Contrasting Abstract Relational Structure",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0d16b0fbe1e2bf6ef02aea2e058f2e13c3a83fa2",
    "semantic_title": "learning to make analogies by contrasting abstract relational structure",
    "citation_count": 101,
    "authors": []
  },
  "https://openreview.net/forum?id=ByxBFsRqYm": {
    "title": "Attention, Learn to Solve Routing Problems!",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ce4f001c1d8ddb9a95cf54e14240ef02c44bd329",
    "semantic_title": "attention, learn to solve routing problems!",
    "citation_count": 1264,
    "authors": []
  },
  "https://openreview.net/forum?id=BkeStsCcKQ": {
    "title": "Critical Learning Periods in Deep Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1802a7870a642d414f435273dd9e9190a0dc4fcb",
    "semantic_title": "critical learning periods in deep networks",
    "citation_count": 138,
    "authors": []
  },
  "https://openreview.net/forum?id=HkxStoC5F7": {
    "title": "Meta-Learning Probabilistic Inference for Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7b0aad12a6917b7d444ba2f87c7f8ccc5357797a",
    "semantic_title": "meta-learning probabilistic inference for prediction",
    "citation_count": 266,
    "authors": []
  },
  "https://openreview.net/forum?id=HyeVtoRqtQ": {
    "title": "Trellis Networks for Sequence Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a14af711aaa3ae83eb64d1f517b024b8c3094a8a",
    "semantic_title": "trellis networks for sequence modeling",
    "citation_count": 146,
    "authors": []
  },
  "https://openreview.net/forum?id=Bke4KsA5FX": {
    "title": "Generative Code Modeling with Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "75c0d369c7151b925155cfe1b3f01dd7d0503981",
    "semantic_title": "generative code modeling with graphs",
    "citation_count": 183,
    "authors": []
  },
  "https://openreview.net/forum?id=HkNGYjR9FX": {
    "title": "Learning Recurrent Binary/Ternary Weights",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ce98b1ec4c79235eb83539ff1379c0f79dd3e2b9",
    "semantic_title": "learning recurrent binary/ternary weights",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=SJfZKiC5FX": {
    "title": "Dynamically Unfolding Recurrent Restorer: A Moving Endpoint Control Method for Image Restoration",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f2ef09c15eeb7d197c8ec23fb85ce3bf4be6bd63",
    "semantic_title": "dynamically unfolding recurrent restorer: a moving endpoint control method for image restoration",
    "citation_count": 51,
    "authors": []
  },
  "https://openreview.net/forum?id=HJMC_iA5tm": {
    "title": "Learning a SAT Solver from Single-Bit Supervision",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "fe257027193ea4a74fdab99d7509ce4002ad7de6",
    "semantic_title": "learning a sat solver from single-bit supervision",
    "citation_count": 438,
    "authors": []
  },
  "https://openreview.net/forum?id=BklCusRct7": {
    "title": "Optimal Transport Maps For Distribution Preserving Operations on Latent Spaces of Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "fcb68849a63000c05c9b8cd592302a82b723dc32",
    "semantic_title": "optimal transport maps for distribution preserving operations on latent spaces of generative models",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=Hkf2_sC5FX": {
    "title": "Efficient Lifelong Learning with A-GEM",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4a954b3e72a61968ab235076bcc242aca3a05520",
    "semantic_title": "efficient lifelong learning with a-gem",
    "citation_count": 1487,
    "authors": []
  },
  "https://openreview.net/forum?id=HkeoOo09YX": {
    "title": "Meta-Learning For Stochastic Gradient MCMC",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "43410e1d4b8790582717013542313c994588590c",
    "semantic_title": "meta-learning for stochastic gradient mcmc",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=B1G9doA9F7": {
    "title": "Augmented Cyclic Adversarial Learning for Low Resource Domain Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "653435e90a6dc9c331e03f70ed32906eb3f6810d",
    "semantic_title": "augmented cyclic adversarial learning for low resource domain adaptation",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=HklKui0ct7": {
    "title": "Off-Policy Evaluation and Learning from Logged Bandit Feedback: Error Reduction via Surrogate Policy",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "91727b55133f77dafed05603da0d4752a7606019",
    "semantic_title": "off-policy evaluation and learning from logged bandit feedback: error reduction via surrogate policy",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=HkfYOoCcYX": {
    "title": "Double Viterbi: Weight Encoding for High Compression Ratio and Fast On-Chip Reconstruction for Deep Neural Network",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1f7fa36fe81a6447bdcd06a84f72228e5dc0524a",
    "semantic_title": "double viterbi: weight encoding for high compression ratio and fast on-chip reconstruction for deep neural network",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=H1ewdiR5tQ": {
    "title": "Graph Wavelet Neural Network",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9f91568d8ec72c417d9146a551b61e69ccf1f848",
    "semantic_title": "graph wavelet neural network",
    "citation_count": 335,
    "authors": []
  },
  "https://openreview.net/forum?id=SJgw_sRqFQ": {
    "title": "The Unusual Effectiveness of Averaging in GAN Training",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "fed5546113d945807e2f24317c0e560d1d21068c",
    "semantic_title": "the unusual effectiveness of averaging in gan training",
    "citation_count": 175,
    "authors": []
  },
  "https://openreview.net/forum?id=HyGIdiRqtm": {
    "title": "Evaluating Robustness of Neural Networks with Mixed Integer Programming",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9de69a46e6c619255eeffbfbb6c7b7163690eb48",
    "semantic_title": "evaluating robustness of neural networks with mixed integer programming",
    "citation_count": 856,
    "authors": []
  },
  "https://openreview.net/forum?id=S1EHOsC9tX": {
    "title": "Towards the first adversarially robust neural network model on MNIST",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "fd7789de401811fd8692466b8d49230e7184655f",
    "semantic_title": "towards the first adversarially robust neural network model on mnist",
    "citation_count": 370,
    "authors": []
  },
  "https://openreview.net/forum?id=HyGBdo0qFm": {
    "title": "On the Turing Completeness of Modern Neural Network Architectures",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3694381e74445a8b9f8cb8d373e39626e47191b5",
    "semantic_title": "on the turing completeness of modern neural network architectures",
    "citation_count": 150,
    "authors": []
  },
  "https://openreview.net/forum?id=ByeSdsC9Km": {
    "title": "Adaptive Posterior Learning: few-shot learning with a surprise-based memory module",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f3bcab5b23eb8999ea3c3c29140c75861255cfea",
    "semantic_title": "adaptive posterior learning: few-shot learning with a surprise-based memory module",
    "citation_count": 79,
    "authors": []
  },
  "https://openreview.net/forum?id=r14EOsCqKX": {
    "title": "A Closer Look at Deep Learning Heuristics: Learning rate restarts, Warmup and Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c4ab32dc966bff2de35723374f7410eeab85053f",
    "semantic_title": "a closer look at deep learning heuristics: learning rate restarts, warmup and distillation",
    "citation_count": 279,
    "authors": []
  },
  "https://openreview.net/forum?id=Syl7OsRqY7": {
    "title": "Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e17c13217e1fad11bae46820c4acae1745f69b43",
    "semantic_title": "coarse-grain fine-grain coattention network for multi-evidence question answering",
    "citation_count": 63,
    "authors": []
  },
  "https://openreview.net/forum?id=H1emus0qF7": {
    "title": "Near-Optimal Representation Learning for Hierarchical Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e4a89a978f747d0b548f5887b2380c5f618061f0",
    "semantic_title": "near-optimal representation learning for hierarchical reinforcement learning",
    "citation_count": 214,
    "authors": []
  },
  "https://openreview.net/forum?id=H1gfOiAqYm": {
    "title": "Execution-Guided Neural Program Synthesis",
    "volume": "poster",
    "abstract": "Neural program synthesis from input-output examples has attracted an increasing interest from both the machine learning and the programming language community. Most existing neural program synthesis approaches employ an encoder-decoder architecture, which uses an encoder to compute the embedding of the given input-output examples, as well as a decoder to generate the program from the embedding following a given syntax. Although such approaches achieve a reasonable performance on simple tasks such as FlashFill, on more complex tasks such as Karel, the state-of-the-art approach can only achieve an accuracy of around 77%. We observe that the main drawback of existing approaches is that the semantic information is greatly under-utilized. In this work, we propose two simple yet principled techniques to better leverage the semantic information, which are execution-guided synthesis and synthesizer ensemble. These techniques are general enough to be combined with any existing encoder-decoder-style neural program synthesizer. Applying our techniques to the Karel dataset, we can boost the accuracy from around 77% to more than 90%",
    "checked": true,
    "id": "6c41bedc4637f3fd504c68baa3b3d8881e056ac1",
    "semantic_title": "execution-guided neural program synthesis",
    "citation_count": 162,
    "authors": []
  },
  "https://openreview.net/forum?id=rJlWOj0qF7": {
    "title": "Imposing Category Trees Onto Word-Embeddings Using A Geometric Construction",
    "volume": "poster",
    "abstract": "We present a novel method to precisely impose tree-structured category information onto word-embeddings, resulting in ball embeddings in higher dimensional spaces (N-balls for short). Inclusion relations among N-balls implicitly encode subordinate relations among categories. The similarity measurement in terms of the cosine function is enriched by category information. Using a geometric construction method instead of back-propagation, we create large N-ball embeddings that satisfy two conditions: (1) category trees are precisely imposed onto word embeddings at zero energy cost; (2) pre-trained word embeddings are well preserved. A new benchmark data set is created for validating the category of unknown words. Experiments show that N-ball embeddings, carrying category information, significantly outperform word embeddings in the test of nearest neighborhoods, and demonstrate surprisingly good performance in validating categories of unknown words. Source codes and data-sets are free for public access \\url{https://github.com/gnodisnait/nball4tree.git} and \\url{https://github.com/gnodisnait/bp94nball.git}",
    "checked": true,
    "id": "df97d99457cac7ba4cac120018174790f1e1bc1c",
    "semantic_title": "imposing category trees onto word-embeddings using a geometric construction",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=BJgRDjR9tQ": {
    "title": "ROBUST ESTIMATION VIA GENERATIVE ADVERSARIAL NETWORKS",
    "volume": "poster",
    "abstract": "Robust estimation under Huber's $\\epsilon$-contamination model has become an important topic in statistics and theoretical computer science. Rate-optimal procedures such as Tukey's median and other estimators based on statistical depth functions are impractical because of their computational intractability. In this paper, we establish an intriguing connection between f-GANs and various depth functions through the lens of f-Learning. Similar to the derivation of f-GAN, we show that these depth functions that lead to rate-optimal robust estimators can all be viewed as variational lower bounds of the total variation distance in the framework of f-Learning. This connection opens the door of computing robust estimators using tools developed for training GANs. In particular, we show that a JS-GAN that uses a neural network discriminator with at least one hidden layer is able to achieve the minimax rate of robust mean estimation under Huber's $\\epsilon$-contamination model. Interestingly, the hidden layers of the neural net structure in the discriminator class are shown to be necessary for robust estimation",
    "checked": true,
    "id": "a78d516846e1931b251ae873d9e78f2717c2ccec",
    "semantic_title": "robust estimation via generative adversarial networks",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=BJG0voC9YQ": {
    "title": "Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search",
    "volume": "poster",
    "abstract": "Learning policies on data synthesized by models can in principle quench the thirst of reinforcement learning algorithms for large amounts of real experience, which is often costly to acquire. However, simulating plausible experience de novo is a hard problem for many complex environments, often resulting in biases for model-based policy evaluation and search. Instead of de novo synthesis of data, here we assume logged, real experience and model alternative outcomes of this experience under counterfactual actions, i.e. actions that were not actually taken. Based on this, we propose the Counterfactually-Guided Policy Search (CF-GPS) algorithm for learning policies in POMDPs from off-policy experience. It leverages structural causal models for counterfactual evaluation of arbitrary policies on individual off-policy episodes. CF-GPS can improve on vanilla model-based RL algorithms by making use of available logged data to de-bias model predictions. In contrast to off-policy algorithms based on Importance Sampling which re-weight data, CF-GPS leverages a model to explicitly consider alternative outcomes, allowing the algorithm to make better use of experience data. We find empirically that these advantages translate into improved policy evaluation and search results on a non-trivial grid-world task. Finally, we show that CF-GPS generalizes the previously proposed Guided Policy Search and that reparameterization-based algorithms such Stochastic Value Gradient can be interpreted as counterfactual methods",
    "checked": true,
    "id": "745a134eca192982e8e0c16d6f36cfe24f9bdd08",
    "semantic_title": "woulda, coulda, shoulda: counterfactually-guided policy search",
    "citation_count": 142,
    "authors": []
  },
  "https://openreview.net/forum?id=Byg0DsCqYQ": {
    "title": "Robust Conditional Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "Conditional generative adversarial networks (cGAN) have led to large improvements in the task of conditional image generation, which lies at the heart of computer vision. The major focus so far has been on performance improvement, while there has been little effort in making cGAN more robust to noise. The regression (of the generator) might lead to arbitrarily large errors in the output, which makes cGAN unreliable for real-world applications. In this work, we introduce a novel conditional GAN model, called RoCGAN, which leverages structure in the target space of the model to address the issue. Our model augments the generator with an unsupervised pathway, which promotes the outputs of the generator to span the target manifold even in the presence of intense noise. We prove that RoCGAN share similar theoretical properties as GAN and experimentally verify that our model outperforms existing state-of-the-art cGAN architectures by a large margin in a variety of domains including images from natural scenes and faces",
    "checked": true,
    "id": "0251807835a2d863c809c25b3d5899d6431dbe89",
    "semantic_title": "robust conditional generative adversarial networks",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=SkE6PjC9KX": {
    "title": "Attentive Neural Processes",
    "volume": "poster",
    "abstract": "Neural Processes (NPs) (Garnelo et al., 2018) approach regression by learning to map a context set of observed input-output pairs to a distribution over regression functions. Each function models the distribution of the output given an input, conditioned on the context. NPs have the benefit of fitting observed data efficiently with linear complexity in the number of context input-output pairs, and can learn a wide family of conditional distributions; they learn predictive distributions conditioned on context sets of arbitrary size. Nonetheless, we show that NPs suffer a fundamental drawback of underfitting, giving inaccurate predictions at the inputs of the observed data they condition on. We address this issue by incorporating attention into NPs, allowing each input location to attend to the relevant context points for the prediction. We show that this greatly improves the accuracy of predictions, results in noticeably faster training, and expands the range of functions that can be modelled",
    "checked": true,
    "id": "9b396268f367917211bbd33947325e72b7742d36",
    "semantic_title": "attentive neural processes",
    "citation_count": 444,
    "authors": []
  },
  "https://openreview.net/forum?id=B1fpDsAqt7": {
    "title": "Visual Reasoning by Progressive Module Networks",
    "volume": "poster",
    "abstract": "Humans learn to solve tasks of increasing complexity by building on top of previously acquired knowledge. Typically, there exists a natural progression in the tasks that we learn ‚Äì most do not require completely independent solutions, but can be broken down into simpler subtasks. We propose to represent a solver for each task as a neural module that calls existing modules (solvers for simpler tasks) in a functional program-like manner. Lower modules are a black box to the calling module, and communicate only via a query and an output. Thus, a module for a new task learns to query existing modules and composes their outputs in order to produce its own output. Our model effectively combines previous skill-sets, does not suffer from forgetting, and is fully differentiable. We test our model in learning a set of visual reasoning tasks, and demonstrate improved performances in all tasks by learning progressively. By evaluating the reasoning process using human judges, we show that our model is more interpretable than an attention-based baseline",
    "checked": true,
    "id": "7af4a37e6e63b5f06e7bfb6e7c8910322774efb9",
    "semantic_title": "visual reasoning by progressive module networks",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=Bkg2viA5FQ": {
    "title": "Hindsight policy gradients",
    "volume": "poster",
    "abstract": "A reinforcement learning agent that needs to pursue different goals across episodes requires a goal-conditional policy. In addition to their potential to generalize desirable behavior to unseen goals, such policies may also enable higher-level planning based on subgoals. In sparse-reward environments, the capacity to exploit information about the degree to which an arbitrary goal has been achieved while another goal was intended appears crucial to enable sample efficient learning. However, reinforcement learning agents have only recently been endowed with such capacity for hindsight. In this paper, we demonstrate how hindsight can be introduced to policy gradient methods, generalizing this idea to a broad class of successful algorithms. Our experiments on a diverse selection of sparse-reward environments show that hindsight leads to a remarkable increase in sample efficiency",
    "checked": true,
    "id": "1fa1f04b80f057e477549e6b9798fab7c7e57db5",
    "semantic_title": "hindsight policy gradients",
    "citation_count": 68,
    "authors": []
  },
  "https://openreview.net/forum?id=SkloDjAqYm": {
    "title": "LeMoNADe: Learned Motif and Neuronal Assembly Detection in calcium imaging videos",
    "volume": "poster",
    "abstract": "Neuronal assemblies, loosely defined as subsets of neurons with reoccurring spatio-temporally coordinated activation patterns, or \"motifs\", are thought to be building blocks of neural representations and information processing. We here propose LeMoNADe, a new exploratory data analysis method that facilitates hunting for motifs in calcium imaging videos, the dominant microscopic functional imaging modality in neurophysiology. Our nonparametric method extracts motifs directly from videos, bypassing the difficult intermediate step of spike extraction. Our technique augments variational autoencoders with a discrete stochastic node, and we show in detail how a differentiable reparametrization and relaxation can be used. An evaluation on simulated data, with available ground truth, reveals excellent quantitative performance. In real video data acquired from brain slices, with no ground truth available, LeMoNADe uncovers nontrivial candidate motifs that can help generate hypotheses for more focused biological investigations",
    "checked": true,
    "id": "bc652086b359a16dd8493270db22cc34f51e2ba8",
    "semantic_title": "lemonade: learned motif and neuronal assembly detection in calcium imaging videos",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=H1ziPjC5Fm": {
    "title": "Visual Explanation by Interpretation: Improving Visual Feedback Capabilities of Deep Neural Networks",
    "volume": "poster",
    "abstract": "Visual Interpretation and explanation of deep models is critical towards wide adoption of systems that rely on them. In this paper, we propose a novel scheme for both interpretation as well as explanation in which, given a pretrained model, we automatically identify internal features relevant for the set of classes considered by the model, without relying on additional annotations. We interpret the model through average visualizations of this reduced set of features. Then, at test time, we explain the network prediction by accompanying the predicted class label with supporting visualizations derived from the identified features. In addition, we propose a method to address the artifacts introduced by strided operations in deconvNet-based visualizations. Moreover, we introduce an8Flower , a dataset specifically designed for objective quantitative evaluation of methods for visual explanation. Experiments on the MNIST , ILSVRC 12, Fashion 144k and an8Flower datasets show that our method produces detailed explanations with good coverage of relevant features of the classes of interest",
    "checked": true,
    "id": "8d8bc608da14bc0ce32c3a5d1fdfbe037993626d",
    "semantic_title": "visual explanation by interpretation: improving visual feedback capabilities of deep neural networks",
    "citation_count": 62,
    "authors": []
  },
  "https://openreview.net/forum?id=BJg9DoR9t7": {
    "title": "Max-MIG: an Information Theoretic Approach for Joint Learning from Crowds",
    "volume": "poster",
    "abstract": "Eliciting labels from crowds is a potential way to obtain large labeled data. Despite a variety of methods developed for learning from crowds, a key challenge remains unsolved: \\emph{learning from crowds without knowing the information structure among the crowds a priori, when some people of the crowds make highly correlated mistakes and some of them label effortlessly (e.g. randomly)}. We propose an information theoretic approach, Max-MIG, for joint learning from crowds, with a common assumption: the crowdsourced labels and the data are independent conditioning on the ground truth. Max-MIG simultaneously aggregates the crowdsourced labels and learns an accurate data classifier. Furthermore, we devise an accurate data-crowds forecaster that employs both the data and the crowdsourced labels to forecast the ground truth. To the best of our knowledge, this is the first algorithm that solves the aforementioned challenge of learning from crowds. In addition to the theoretical validation, we also empirically show that our algorithm achieves the new state-of-the-art results in most settings, including the real-world data, and is the first algorithm that is robust to various information structures. Codes are available at https://github.com/Newbeeer/Max-MIG",
    "checked": true,
    "id": "789e1c58964f98f67471fe3552ed9934f24d10ff",
    "semantic_title": "max-mig: an information theoretic approach for joint learning from crowds",
    "citation_count": 56,
    "authors": []
  },
  "https://openreview.net/forum?id=BJfYvo09Y7": {
    "title": "Hierarchical Visuomotor Control of Humanoids",
    "volume": "poster",
    "abstract": "We aim to build complex humanoid agents that integrate perception, motor control, and memory. In this work, we partly factor this problem into low-level motor control from proprioception and high-level coordination of the low-level skills informed by vision. We develop an architecture capable of surprisingly flexible, task-directed motor control of a relatively high-DoF humanoid body by combining pre-training of low-level motor controllers with a high-level, task-focused controller that switches among low-level sub-policies. The resulting system is able to control a physically-simulated humanoid body to solve tasks that require coupling visual perception from an unstabilized egocentric RGB camera during locomotion in the environment. Supplementary video link: https://youtu.be/fBoir7PNxPk",
    "checked": true,
    "id": "a8a7219ee83cfd7ca258e20b5826a0c0786dcb73",
    "semantic_title": "hierarchical visuomotor control of humanoids",
    "citation_count": 97,
    "authors": []
  },
  "https://openreview.net/forum?id=BkgtDsCcKQ": {
    "title": "Function Space Particle Optimization for Bayesian Neural Networks",
    "volume": "poster",
    "abstract": "While Bayesian neural networks (BNNs) have drawn increasing attention, their posterior inference remains challenging, due to the high-dimensional and over-parameterized nature. To address this issue, several highly flexible and scalable variational inference procedures based on the idea of particle optimization have been proposed. These methods directly optimize a set of particles to approximate the target posterior. However, their application to BNNs often yields sub-optimal performance, as such methods have a particular failure mode on over-parameterized models. In this paper, we propose to solve this issue by performing particle optimization directly in the space of regression functions. We demonstrate through extensive experiments that our method successfully overcomes this issue, and outperforms strong baselines in a variety of tasks including prediction, defense against adversarial examples, and reinforcement learning",
    "checked": true,
    "id": "4e1e6f810375fd0a52404fc4f71eb566cbce3b99",
    "semantic_title": "function space particle optimization for bayesian neural networks",
    "citation_count": 66,
    "authors": []
  },
  "https://openreview.net/forum?id=SkMuPjRcKQ": {
    "title": "Feed-forward Propagation in Probabilistic Neural Networks with Categorical and Max Layers",
    "volume": "poster",
    "abstract": "Probabilistic Neural Networks deal with various sources of stochasticity: input noise, dropout, stochastic neurons, parameter uncertainties modeled as random variables, etc. In this paper we revisit a feed-forward propagation approach that allows one to estimate for each neuron its mean and variance w.r.t. all mentioned sources of stochasticity. In contrast, standard NNs propagate only point estimates, discarding the uncertainty. Methods propagating also the variance have been proposed by several authors in different context. The view presented here attempts to clarify the assumptions and derivation behind such methods, relate them to classical NNs and broaden their scope of applicability. The main technical contributions are new approximations for the distributions of argmax and max-related transforms, which allow for fully analytic uncertainty propagation in networks with softmax and max-pooling layers as well as leaky ReLU activations. We evaluate the accuracy of the approximation and suggest a simple calibration. Applying the method to networks with dropout allows for faster training and gives improved test likelihoods without the need of sampling",
    "checked": true,
    "id": "219ad53f77cb3a1953b8a766ec28c62ea8da49f8",
    "semantic_title": "feed-forward propagation in probabilistic neural networks with categorical and max layers",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=Hyl_vjC5KQ": {
    "title": "Hierarchical Reinforcement Learning via Advantage-Weighted Information Maximization",
    "volume": "poster",
    "abstract": "Real-world tasks are often highly structured. Hierarchical reinforcement learning (HRL) has attracted research interest as an approach for leveraging the hierarchical structure of a given task in reinforcement learning (RL). However, identifying the hierarchical policy structure that enhances the performance of RL is not a trivial task. In this paper, we propose an HRL method that learns a latent variable of a hierarchical policy using mutual information maximization. Our approach can be interpreted as a way to learn a discrete and latent representation of the state-action space. To learn option policies that correspond to modes of the advantage function, we introduce advantage-weighted importance sampling. In our HRL method, the gating policy learns to select option policies based on an option-value function, and these option policies are optimized based on the deterministic policy gradient method. This framework is derived by leveraging the analogy between a monolithic policy in standard RL and a hierarchical policy in HRL by using a deterministic option policy. Experimental results indicate that our HRL approach can learn a diversity of options and that it can enhance the performance of RL in continuous control tasks",
    "checked": true,
    "id": "1447cb195033be291674a44a07eb18ee894c23eb",
    "semantic_title": "hierarchical reinforcement learning via advantage-weighted information maximization",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=rJNwDjAqYX": {
    "title": "Large-Scale Study of Curiosity-Driven Learning",
    "volume": "poster",
    "abstract": "Reinforcement learning algorithms rely on carefully engineered rewards from the environment that are extrinsic to the agent. However, annotating each environment with hand-designed, dense rewards is difficult and not scalable, motivating the need for developing reward functions that are intrinsic to the agent. Curiosity is such intrinsic reward function which uses prediction error as a reward signal. In this paper: (a) We perform the first large-scale study of purely curiosity-driven learning, i.e. {\\em without any extrinsic rewards}, across $54$ standard benchmark environments, including the Atari game suite. Our results show surprisingly good performance as well as a high degree of alignment between the intrinsic curiosity objective and the hand-designed extrinsic rewards of many games. (b) We investigate the effect of using different feature spaces for computing prediction error and show that random features are sufficient for many popular RL game benchmarks, but learned features appear to generalize better (e.g. to novel game levels in Super Mario Bros.). (c) We demonstrate limitations of the prediction-based rewards in stochastic setups. Game-play videos and code are at https://doubleblindsupplementary.github.io/large-curiosity/",
    "checked": true,
    "id": "ca14dce53be20d3d23d4f0db844a8389ab619db3",
    "semantic_title": "large-scale study of curiosity-driven learning",
    "citation_count": 707,
    "authors": []
  },
  "https://openreview.net/forum?id=HJxwDiActX": {
    "title": "StrokeNet: A Neural Painting Environment",
    "volume": "poster",
    "abstract": "We've seen tremendous success of image generating models these years. Generating images through a neural network is usually pixel-based, which is fundamentally different from how humans create artwork using brushes. To imitate human drawing, interactions between the environment and the agent is required to allow trials. However, the environment is usually non-differentiable, leading to slow convergence and massive computation. In this paper we try to address the discrete nature of software environment with an intermediate, differentiable simulation. We present StrokeNet, a novel model where the agent is trained upon a well-crafted neural approximation of the painting environment. With this approach, our agent was able to learn to write characters such as MNIST digits faster than reinforcement learning approaches in an unsupervised manner. Our primary contribution is the neural simulation of a real-world environment. Furthermore, the agent trained with the emulated environment is able to directly transfer its skills to real-world software",
    "checked": true,
    "id": "412e4a122f5abc23ca9b8856d07d3dd962a93e5a",
    "semantic_title": "strokenet: a neural painting environment",
    "citation_count": 78,
    "authors": []
  },
  "https://openreview.net/forum?id=BkgBvsC9FQ": {
    "title": "DialogWAE: Multimodal Response Generation with Conditional Wasserstein Auto-Encoder",
    "volume": "poster",
    "abstract": "Variational autoencoders (VAEs) have shown a promise in data-driven conversation modeling. However, most VAE conversation models match the approximate posterior distribution over the latent variables to a simple prior such as standard normal distribution, thereby restricting the generated responses to a relatively simple (e.g., single-modal) scope. In this paper, we propose DialogWAE, a conditional Wasserstein autoencoder (WAE) specially designed for dialogue modeling. Unlike VAEs that impose a simple distribution over the latent variables, DialogWAE models the distribution of data by training a GAN within the latent variable space. Specifically, our model samples from the prior and posterior distributions over the latent variables by transforming context-dependent random noise using neural networks and minimizes the Wasserstein distance between the two distributions. We further develop a Gaussian mixture prior network to enrich the latent space. Experiments on two popular datasets show that DialogWAE outperforms the state-of-the-art approaches in generating more coherent, informative and diverse responses",
    "checked": true,
    "id": "77b9505e6967cfb45eaf6ec8ac7746cbaaab6e0d",
    "semantic_title": "dialogwae: multimodal response generation with conditional wasserstein auto-encoder",
    "citation_count": 131,
    "authors": []
  },
  "https://openreview.net/forum?id=ByMHvs0cFQ": {
    "title": "Quaternion Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "Recurrent neural networks (RNNs) are powerful architectures to model sequential data, due to their capability to learn short and long-term dependencies between the basic elements of a sequence. Nonetheless, popular tasks such as speech or images recognition, involve multi-dimensional input features that are characterized by strong internal dependencies between the dimensions of the input vector. We propose a novel quaternion recurrent neural network (QRNN), alongside with a quaternion long-short term memory neural network (QLSTM), that take into account both the external relations and these internal structural dependencies with the quaternion algebra. Similarly to capsules, quaternions allow the QRNN to code internal dependencies by composing and processing multidimensional features as single entities, while the recurrent operation reveals correlations between the elements composing the sequence. We show that both QRNN and QLSTM achieve better performances than RNN and LSTM in a realistic application of automatic speech recognition. Finally, we show that QRNN and QLSTM reduce by a maximum factor of 3.3x the number of free parameters needed, compared to real-valued RNNs and LSTMs to reach better results, leading to a more compact representation of the relevant information",
    "checked": true,
    "id": "31a857249f9f3bcdeb8a3b2944620fc16f128f64",
    "semantic_title": "quaternion recurrent neural networks",
    "citation_count": 132,
    "authors": []
  },
  "https://openreview.net/forum?id=SkfrvsA9FX": {
    "title": "Reward Constrained Policy Optimization",
    "volume": "poster",
    "abstract": "Solving tasks in Reinforcement Learning is no easy feat. As the goal of the agent is to maximize the accumulated reward, it often learns to exploit loopholes and misspecifications in the reward signal resulting in unwanted behavior. While constraints may solve this issue, there is no closed form solution for general constraints. In this work we present a novel multi-timescale approach for constrained policy optimization, called `Reward Constrained Policy Optimization' (RCPO), which uses an alternative penalty signal to guide the policy towards a constraint satisfying one. We prove the convergence of our approach and provide empirical evidence of its ability to train constraint satisfying policies",
    "checked": true,
    "id": "cb7c479a36520da1caeeec67db10772351a390c6",
    "semantic_title": "reward constrained policy optimization",
    "citation_count": 554,
    "authors": []
  },
  "https://openreview.net/forum?id=SJgNwi09Km": {
    "title": "Learning Latent Superstructures in Variational Autoencoders for Deep Multidimensional Clustering",
    "volume": "poster",
    "abstract": "We investigate a variant of variational autoencoders where there is a superstructure of discrete latent variables on top of the latent features. In general, our superstructure is a tree structure of multiple super latent variables and it is automatically learned from data. When there is only one latent variable in the superstructure, our model reduces to one that assumes the latent features to be generated from a Gaussian mixture model. We call our model the latent tree variational autoencoder (LTVAE). Whereas previous deep learning methods for clustering produce only one partition of data, LTVAE produces multiple partitions of data, each being given by one super latent variable. This is desirable because high dimensional data usually have many different natural facets and can be meaningfully partitioned in multiple ways",
    "checked": true,
    "id": "7b85357834e398437a291906aded59caff5151eb",
    "semantic_title": "learning latent superstructures in variational autoencoders for deep multidimensional clustering",
    "citation_count": 52,
    "authors": []
  },
  "https://openreview.net/forum?id=SygQvs0cFQ": {
    "title": "Variational Smoothing in Recurrent Neural Network Language Models",
    "volume": "poster",
    "abstract": "We present a new theoretical perspective of data noising in recurrent neural network language models (Xie et al., 2017). We show that each variant of data noising is an instance of Bayesian recurrent neural networks with a particular variational distribution (i.e., a mixture of Gaussians whose weights depend on statistics derived from the corpus such as the unigram distribution). We use this insight to propose a more principled method to apply at prediction time and propose natural extensions to data noising under the variational framework. In particular, we propose variational smoothing with tied input and output embedding matrices and an element-wise variational smoothing method. We empirically verify our analysis on two benchmark language modeling datasets and demonstrate performance improvements over existing data noising methods",
    "checked": true,
    "id": "08736d66224afbcd5514947a79a08a424a6f0576",
    "semantic_title": "variational smoothing in recurrent neural network language models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=B1GMDsR5tm": {
    "title": "Initialized Equilibrium Propagation for Backprop-Free Training",
    "volume": "poster",
    "abstract": "Deep neural networks are almost universally trained with reverse-mode automatic differentiation (a.k.a. backpropagation). Biological networks, on the other hand, appear to lack any mechanism for sending gradients back to their input neurons, and thus cannot be learning in this way. In response to this, Scellier & Bengio (2017) proposed Equilibrium Propagation - a method for gradient-based train- ing of neural networks which uses only local learning rules and, crucially, does not rely on neurons having a mechanism for back-propagating an error gradient. Equilibrium propagation, however, has a major practical limitation: inference involves doing an iterative optimization of neural activations to find a fixed-point, and the number of steps required to closely approximate this fixed point scales poorly with the depth of the network. In response to this problem, we propose Initialized Equilibrium Propagation, which trains a feedforward network to initialize the iterative inference procedure for Equilibrium propagation. This feed-forward network learns to approximate the state of the fixed-point using a local learning rule. After training, we can simply use this initializing network for inference, resulting in a learned feedforward network. Our experiments show that this network appears to work as well or better than the original version of Equilibrium propagation. This shows how we might go about training deep networks without using backpropagation",
    "checked": true,
    "id": "6811848615502f5347b3330110be6496bd2c6e4f",
    "semantic_title": "initialized equilibrium propagation for backprop-free training",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=H1z-PsR5KX": {
    "title": "Identifying and Controlling Important Neurons in Neural Machine Translation",
    "volume": "poster",
    "abstract": "Neural machine translation (NMT) models learn representations containing substantial linguistic information. However, it is not clear if such information is fully distributed or if some of it can be attributed to individual neurons. We develop unsupervised methods for discovering important neurons in NMT models. Our methods rely on the intuition that different models learn similar properties, and do not require any costly external supervision. We show experimentally that translation quality depends on the discovered neurons, and find that many of them capture common linguistic phenomena. Finally, we show how to control NMT translations in predictable ways, by modifying activations of individual neurons",
    "checked": true,
    "id": "c5489d244bfc1e9b0d8c94bf6dd774ee1aca2def",
    "semantic_title": "identifying and controlling important neurons in neural machine translation",
    "citation_count": 185,
    "authors": []
  },
  "https://openreview.net/forum?id=BJe-DsC5Fm": {
    "title": "signSGD via Zeroth-Order Oracle",
    "volume": "poster",
    "abstract": "In this paper, we design and analyze a new zeroth-order (ZO) stochastic optimization algorithm, ZO-signSGD, which enjoys dual advantages of gradient-free operations and signSGD. The latter requires only the sign information of gradient estimates but is able to achieve a comparable or even better convergence speed than SGD-type algorithms. Our study shows that ZO signSGD requires $\\sqrt{d}$ times more iterations than signSGD, leading to a convergence rate of $O(\\sqrt{d}/\\sqrt{T})$ under mild conditions, where $d$ is the number of optimization variables, and $T$ is the number of iterations. In addition, we analyze the effects of different types of gradient estimators on the convergence of ZO-signSGD, and propose two variants of ZO-signSGD that at least achieve $O(\\sqrt{d}/\\sqrt{T})$ convergence rate. On the application side we explore the connection between ZO-signSGD and black-box adversarial attacks in robust deep learning. Our empirical evaluations on image classification datasets MNIST and CIFAR-10 demonstrate the superior performance of ZO-signSGD on the generation of adversarial examples from black-box neural networks",
    "checked": true,
    "id": "3ae1544865a18d8649a5c4939f9eb17165b23bea",
    "semantic_title": "signsgd via zeroth-order oracle",
    "citation_count": 138,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgbwsAcYm": {
    "title": "DELTA: DEEP LEARNING TRANSFER USING FEATURE MAP WITH ATTENTION FOR CONVOLUTIONAL NETWORKS",
    "volume": "poster",
    "abstract": "Transfer learning through fine-tuning a pre-trained neural network with an extremely large dataset, such as ImageNet, can significantly accelerate training while the accuracy is frequently bottlenecked by the limited dataset size of the new target task. To solve the problem, some regularization methods, constraining the outer layer weights of the target network using the starting point as references (SPAR), have been studied. In this paper, we propose a novel regularized transfer learning framework DELTA, namely DEep Learning Transfer using Feature Map with Attention. Instead of constraining the weights of neural network, DELTA aims to preserve the outer layer outputs of the target network. Specifically, in addition to minimizing the empirical loss, DELTA intends to align the outer layer outputs of two networks, through constraining a subset of feature maps that are precisely selected by attention that has been learned in an supervised learning manner. We evaluate DELTA with the state-of-the-art algorithms, including L2 and L2-SP. The experiment results show that our proposed method outperforms these baselines with higher accuracy for new tasks",
    "checked": true,
    "id": "98627a34761bad5bd0582a7b03988de780b2d06b",
    "semantic_title": "delta: deep learning transfer using feature map with attention for convolutional networks",
    "citation_count": 172,
    "authors": []
  },
  "https://openreview.net/forum?id=B1GAUs0cKQ": {
    "title": "Variance Networks: When Expectation Does Not Meet Your Expectations",
    "volume": "poster",
    "abstract": "Ordinary stochastic neural networks mostly rely on the expected values of their weights to make predictions, whereas the induced noise is mostly used to capture the uncertainty, prevent overfitting and slightly boost the performance through test-time averaging. In this paper, we introduce variance layers, a different kind of stochastic layers. Each weight of a variance layer follows a zero-mean distribution and is only parameterized by its variance. It means that each object is represented by a zero-mean distribution in the space of the activations. We show that such layers can learn surprisingly well, can serve as an efficient exploration tool in reinforcement learning tasks and provide a decent defense against adversarial attacks. We also show that a number of conventional Bayesian neural networks naturally converge to such zero-mean posteriors. We observe that in these cases such zero-mean parameterization leads to a much better training objective than more flexible conventional parameterizations where the mean is being learned",
    "checked": true,
    "id": "58b8a5d5e46192c9bb0cf720cbe225f205ad1b3d",
    "semantic_title": "variance networks: when expectation does not meet your expectations",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=ryepUj0qtX": {
    "title": "Conditional Network Embeddings",
    "volume": "poster",
    "abstract": "Network Embeddings (NEs) map the nodes of a given network into $d$-dimensional Euclidean space $\\mathbb{R}^d$. Ideally, this mapping is such that 'similar' nodes are mapped onto nearby points, such that the NE can be used for purposes such as link prediction (if 'similar' means being 'more likely to be connected') or classification (if 'similar' means 'being more likely to have the same label'). In recent years various methods for NE have been introduced, all following a similar strategy: defining a notion of similarity between nodes (typically some distance measure within the network), a distance measure in the embedding space, and a loss function that penalizes large distances for similar nodes and small distances for dissimilar nodes. A difficulty faced by existing methods is that certain networks are fundamentally hard to embed due to their structural properties: (approximate) multipartiteness, certain degree distributions, assortativity, etc. To overcome this, we introduce a conceptual innovation to the NE literature and propose to create \\emph{Conditional Network Embeddings} (CNEs); embeddings that maximally add information with respect to given structural properties (e.g. node degrees, block densities, etc.). We use a simple Bayesian approach to achieve this, and propose a block stochastic gradient descent algorithm for fitting it efficiently. We demonstrate that CNEs are superior for link prediction and multi-label classification when compared to state-of-the-art methods, and this without adding significant mathematical or computational complexity. Finally, we illustrate the potential of CNE for network visualization",
    "checked": true,
    "id": "6625204c81fac0adba369d9ec924ac214e6eebc6",
    "semantic_title": "conditional network embeddings",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=SyMhLo0qKQ": {
    "title": "Distribution-Interpolation Trade off in Generative Models",
    "volume": "poster",
    "abstract": "We investigate the properties of multidimensional probability distributions in the context of latent space prior distributions of implicit generative models. Our work revolves around the phenomena arising while decoding linear interpolations between two random latent vectors -- regions of latent space in close proximity to the origin of the space are oversampled, which restricts the usability of linear interpolations as a tool to analyse the latent space. We show that the distribution mismatch can be eliminated completely by a proper choice of the latent probability distribution or using non-linear interpolations. We prove that there is a trade off between the interpolation being linear, and the latent distribution having even the most basic properties required for stable training, such as finite mean. We use the multidimensional Cauchy distribution as an example of the prior distribution, and also provide a general method of creating non-linear interpolations, that is easily applicable to a large family of commonly used latent distributions",
    "checked": true,
    "id": "3fda72294f9ca4f8afac856fbff26e83b0695e7a",
    "semantic_title": "distribution-interpolation trade off in generative models",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=rkzjUoAcFX": {
    "title": "Sample Efficient Adaptive Text-to-Speech",
    "volume": "poster",
    "abstract": "We present a meta-learning approach for adaptive text-to-speech (TTS) with few data. During training, we learn a multi-speaker model using a shared conditional WaveNet core and independent learned embeddings for each speaker. The aim of training is not to produce a neural network with fixed weights, which is then deployed as a TTS system. Instead, the aim is to produce a network that requires few data at deployment time to rapidly adapt to new speakers. We introduce and benchmark three strategies: (i) learning the speaker embedding while keeping the WaveNet core fixed, (ii) fine-tuning the entire architecture with stochastic gradient descent, and (iii) predicting the speaker embedding with a trained neural network encoder. The experiments show that these approaches are successful at adapting the multi-speaker neural network to new speakers, obtaining state-of-the-art results in both sample naturalness and voice similarity with merely a few minutes of audio data from new speakers",
    "checked": true,
    "id": "031c6baabd61f5b654ef4892f8f6ed737ec52511",
    "semantic_title": "sample efficient adaptive text-to-speech",
    "citation_count": 149,
    "authors": []
  },
  "https://openreview.net/forum?id=ByloIiCqYQ": {
    "title": "Maximal Divergence Sequential Autoencoder for Binary Software Vulnerability Detection",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "89272d09aaf902e70de0d68e5e9c15d810c8f418",
    "semantic_title": "maximal divergence sequential autoencoder for binary software vulnerability detection",
    "citation_count": 53,
    "authors": []
  },
  "https://openreview.net/forum?id=BkN5UoAqF7": {
    "title": "Sample Efficient Imitation Learning for Continuous Control",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "21946808f15dab6c73a76e89d1fa5869df515a3f",
    "semantic_title": "sample efficient imitation learning for continuous control",
    "citation_count": 58,
    "authors": []
  },
  "https://openreview.net/forum?id=ryE98iR5tm": {
    "title": "Practical lossless compression with latent variables using bits back coding",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c2ed34facd63d72e5d03ba13a6a3956ed6b2ac6c",
    "semantic_title": "practical lossless compression with latent variables using bits back coding",
    "citation_count": 143,
    "authors": []
  },
  "https://openreview.net/forum?id=HyxKIiAqYQ": {
    "title": "Context-adaptive Entropy Model for End-to-end Optimized Image Compression",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3b2409b9cc59b8f9b2d7baa200c1ac9e98b397da",
    "semantic_title": "context-adaptive entropy model for end-to-end optimized image compression",
    "citation_count": 399,
    "authors": []
  },
  "https://openreview.net/forum?id=ryM_IoAqYX": {
    "title": "Analysis of Quantized Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "02dae4e3494fe8f1b6591d4697a4e584605a3f6b",
    "semantic_title": "analysis of quantized models",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=H1edIiA9KQ": {
    "title": "Generating Multiple Objects at Spatially Distinct Locations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "15170be53e9b6d63df11ac231bdf20d9f550afdc",
    "semantic_title": "generating multiple objects at spatially distinct locations",
    "citation_count": 103,
    "authors": []
  },
  "https://openreview.net/forum?id=rkzDIiA5YQ": {
    "title": "ANYTIME MINIBATCH: EXPLOITING STRAGGLERS IN ONLINE DISTRIBUTED OPTIMIZATION",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "78991eb56de1117218a02a072eaeafd1289d40cc",
    "semantic_title": "anytime minibatch: exploiting stragglers in online distributed optimization",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=H1fU8iAqKX": {
    "title": "A rotation-equivariant convolutional neural network model of primary visual cortex",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "70c73ea3e534a46b6b2c1b91449b9e17678337ef",
    "semantic_title": "a rotation-equivariant convolutional neural network model of primary visual cortex",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=ryfMLoCqtQ": {
    "title": "An analytic theory of generalization dynamics and transfer learning in deep linear networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a77dc75ab9d3477cab828f492af638e5c27b5f4a",
    "semantic_title": "an analytic theory of generalization dynamics and transfer learning in deep linear networks",
    "citation_count": 133,
    "authors": []
  },
  "https://openreview.net/forum?id=r1lWUoA9FQ": {
    "title": "Are adversarial examples inevitable?",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "fd02c5b49bab02fb814c6999ebf161f3be377c75",
    "semantic_title": "are adversarial examples inevitable?",
    "citation_count": 283,
    "authors": []
  },
  "https://openreview.net/forum?id=BJeWUs05KQ": {
    "title": "Directed-Info GAIL: Learning Hierarchical Policies from Unsegmented Demonstrations using Directed Information",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b43d8c8b25bc65cbf3097480e9000649c79b7a51",
    "semantic_title": "directed-info gail: learning hierarchical policies from unsegmented demonstrations using directed information",
    "citation_count": 68,
    "authors": []
  },
  "https://openreview.net/forum?id=BkzeUiRcY7": {
    "title": "M^3RL: Mind-aware Multi-agent Management Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d341764a2020dcc3389f27d6508679e0c3cab486",
    "semantic_title": "m^3rl: mind-aware multi-agent management reinforcement learning",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=ryggIs0cYQ": {
    "title": "Differentiable Learning-to-Normalize via Switchable Normalization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1c42f8ab39e22225ffd3222baeba4863435220a0",
    "semantic_title": "differentiable learning-to-normalize via switchable normalization",
    "citation_count": 177,
    "authors": []
  },
  "https://openreview.net/forum?id=SJxTroR9F7": {
    "title": "Supervised Policy Update for Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c4ecab02877c91f06340877640d9274e1f85a7a1",
    "semantic_title": "supervised policy update for deep reinforcement learning",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=Hyx6Bi0qYm": {
    "title": "Adversarial Domain Adaptation for Stable Brain-Machine Interfaces",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "301201dc73384b9dbc45a02e69db95210f93e6b8",
    "semantic_title": "adversarial domain adaptation for stable brain-machine interfaces",
    "citation_count": 76,
    "authors": []
  },
  "https://openreview.net/forum?id=SJe3HiC5KX": {
    "title": "LEARNING FACTORIZED REPRESENTATIONS FOR OPEN-SET DOMAIN ADAPTATION",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "211c42a6d949d60fa7eaaf5a7b7b28ebef36da9f",
    "semantic_title": "learning factorized representations for open-set domain adaptation",
    "citation_count": 53,
    "authors": []
  },
  "https://openreview.net/forum?id=H1xsSjC9Ym": {
    "title": "Learning to Understand Goal Specifications by Modelling Reward",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4a4b71ff918ca8eeffa5dfe66be2db7fcc1291da",
    "semantic_title": "learning to understand goal specifications by modelling reward",
    "citation_count": 160,
    "authors": []
  },
  "https://openreview.net/forum?id=H1goBoR9F7": {
    "title": "Dynamic Sparse Graph for Efficient Deep Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ba6985ee462ed8b09385a924aded5a45f74e7a59",
    "semantic_title": "dynamic sparse graph for efficient deep learning",
    "citation_count": 43,
    "authors": []
  },
  "https://openreview.net/forum?id=SkEqro0ctQ": {
    "title": "Hierarchical interpretations for neural network predictions",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b3cedde36a6841b43162fc406b688e51bec68d36",
    "semantic_title": "hierarchical interpretations for neural network predictions",
    "citation_count": 147,
    "authors": []
  },
  "https://openreview.net/forum?id=BkG5SjR5YQ": {
    "title": "Post Selection Inference with Incomplete Maximum Mean Discrepancy Estimator",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0d93de3e89ea7ed04fe44be572472a244b97c678",
    "semantic_title": "post selection inference with incomplete maximum mean discrepancy estimator",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=BygqBiRcFQ": {
    "title": "Diffusion Scattering Transforms on Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "080b8de51221395038feb656f33783b65fa48434",
    "semantic_title": "diffusion scattering transforms on graphs",
    "citation_count": 103,
    "authors": []
  },
  "https://openreview.net/forum?id=Bye5SiAqKX": {
    "title": "Preconditioner on Matrix Lie Group for SGD",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "448491374325bd2c3f8cbc381b67b7742329144b",
    "semantic_title": "preconditioner on matrix lie group for sgd",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=ryeYHi0ctQ": {
    "title": "DPSNet: End-to-end Deep Plane Sweep Stereo",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "01dd15f339e8da4eb9a23e47983c1b7c480c7196",
    "semantic_title": "dpsnet: end-to-end deep plane sweep stereo",
    "citation_count": 230,
    "authors": []
  },
  "https://openreview.net/forum?id=S1eYHoC5FX": {
    "title": "DARTS: Differentiable Architecture Search",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c1f457e31b611da727f9aef76c283a18157dfa83",
    "semantic_title": "darts: differentiable architecture search",
    "citation_count": 4415,
    "authors": []
  },
  "https://openreview.net/forum?id=Syx_Ss05tm": {
    "title": "Adversarial Reprogramming of Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c8ed4a46e6aa565ea757e0ff5b7c160419699026",
    "semantic_title": "adversarial reprogramming of neural networks",
    "citation_count": 184,
    "authors": []
  },
  "https://openreview.net/forum?id=S1eOHo09KX": {
    "title": "Opportunistic Learning: Budgeted Cost-Sensitive Learning from Data Streams",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ac87ce247e9e1d5cf9c394e7cf2ad7a239542d89",
    "semantic_title": "opportunistic learning: budgeted cost-sensitive learning from data streams",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=BJg_roAcK7": {
    "title": "INVASE: Instance-wise Variable Selection using Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0e5793fc79f155a619a50aab2be4f577e87e3f99",
    "semantic_title": "invase: instance-wise variable selection using neural networks",
    "citation_count": 166,
    "authors": []
  },
  "https://openreview.net/forum?id=S1erHoR5t7": {
    "title": "The relativistic discriminator: a key element missing from standard GAN",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "dd2ebc42a1a4491b4179dec0ca8686d5c66f6bfe",
    "semantic_title": "the relativistic discriminator: a key element missing from standard gan",
    "citation_count": 986,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgBHoCqYX": {
    "title": "A Kernel Random Matrix-Based Approach for Sparse PCA",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c2b13b979556d6fa1e7c0adc7cf37911e38cebe8",
    "semantic_title": "a kernel random matrix-based approach for sparse pca",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=rke4HiAcY7": {
    "title": "Caveats for information bottleneck in deterministic scenarios",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7c789f89fd2a53d281cb67506a78a94f5c932333",
    "semantic_title": "caveats for information bottleneck in deterministic scenarios",
    "citation_count": 84,
    "authors": []
  },
  "https://openreview.net/forum?id=SJeXSo09FQ": {
    "title": "Learning Localized Generative Models for 3D Point Clouds via Graph Convolution",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "32b918246037976ba5f8363104ec042f56db42d6",
    "semantic_title": "learning localized generative models for 3d point clouds via graph convolution",
    "citation_count": 187,
    "authors": []
  },
  "https://openreview.net/forum?id=S1fQSiCcYm": {
    "title": "Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1d3885c4d8bf2451630ab7505f50948e178a64e1",
    "semantic_title": "understanding and improving interpolation in autoencoders via an adversarial regularizer",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HJlmHoR5tQ": {
    "title": "Adversarial Imitation via Variational Inverse Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c58a10a4923f59d02bd2dc2f7b9b13e7198d3383",
    "semantic_title": "adversarial imitation via variational inverse reinforcement learning",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=HyeGBj09Fm": {
    "title": "Generating Liquid Simulations with Deformation-aware Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "73a7e8165e83c5cfe2150c03b927c55102b2e815",
    "semantic_title": "generating liquid simulations with deformation-aware neural networks",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=ByxGSsR9FQ": {
    "title": "L2-Nonexpansive Neural Networks",
    "volume": "poster",
    "abstract": "This paper proposes a class of well-conditioned neural networks in which a unit amount of change in the inputs causes at most a unit amount of change in the outputs or any of the internal layers. We develop the known methodology of controlling Lipschitz constants to realize its full potential in maximizing robustness, with a new regularization scheme for linear layers, new ways to adapt nonlinearities and a new loss function. With MNIST and CIFAR-10 classifiers, we demonstrate a number of advantages. Without needing any adversarial training, the proposed classifiers exceed the state of the art in robustness against white-box L2-bounded adversarial attacks. They generalize better than ordinary networks from noisy data with partially random labels. Their outputs are quantitatively meaningful and indicate levels of confidence and generalization, among other desirable properties",
    "checked": true,
    "id": "ef2ec69e7c94b4194ba01719ac76d4595e6b4bdf",
    "semantic_title": "l2-nonexpansive neural networks",
    "citation_count": 74,
    "authors": []
  },
  "https://openreview.net/forum?id=ryGgSsAcFQ": {
    "title": "Deep, Skinny Neural Networks are not Universal Approximators",
    "volume": "poster",
    "abstract": "In order to choose a neural network architecture that will be effective for a particular modeling problem, one must understand the limitations imposed by each of the potential options. These limitations are typically described in terms of information theoretic bounds, or by comparing the relative complexity needed to approximate example functions between different architectures. In this paper, we examine the topological constraints that the architecture of a neural network imposes on the level sets of all the functions that it is able to approximate. This approach is novel for both the nature of the limitations and the fact that they are independent of network depth for a broad family of activation functions",
    "checked": true,
    "id": "6b7c16120ef3324bdca4b8433fb1f89c761e4dfc",
    "semantic_title": "deep, skinny neural networks are not universal approximators",
    "citation_count": 67,
    "authors": []
  },
  "https://openreview.net/forum?id=ryGkSo0qYm": {
    "title": "Large Scale Graph Learning From Smooth Signals",
    "volume": "poster",
    "abstract": "Graphs are a prevalent tool in data science, as they model the inherent structure of the data. Typically they are constructed either by connecting nearest samples, or by learning them from data, solving an optimization problem. While graph learning does achieve a better quality, it also comes with a higher computational cost. In particular, the current state-of-the-art model cost is O(n^2) for n samples. In this paper, we show how to scale it, obtaining an approximation with leading cost of O(n log(n)), with quality that approaches the exact graph learning model. Our algorithm uses known approximate nearest neighbor techniques to reduce the number of variables, and automatically selects the correct parameters of the model, requiring a single intuitive input: the desired edge density",
    "checked": true,
    "id": "6a432ad86a4c3c37005f8e46212ec4e0abea5482",
    "semantic_title": "large scale graph learning from smooth signals",
    "citation_count": 84,
    "authors": []
  },
  "https://openreview.net/forum?id=H1gTEj09FX": {
    "title": "RotDCF: Decomposition of Convolutional Filters for Rotation-Equivariant Deep Networks",
    "volume": "poster",
    "abstract": "Explicit encoding of group actions in deep features makes it possible for convolutional neural networks (CNNs) to handle global deformations of images, which is critical to success in many vision tasks. This paper proposes to decompose the convolutional filters over joint steerable bases across the space and the group geometry simultaneously, namely a rotation-equivariant CNN with decomposed convolutional filters (RotDCF). This decomposition facilitates computing the joint convolution, which is proved to be necessary for the group equivariance. It significantly reduces the model size and computational complexity while preserving performance, and truncation of the bases expansion serves implicitly to regularize the filters. On datasets involving in-plane and out-of-plane object rotations, RotDCF deep features demonstrate greater robustness and interpretability than regular CNNs. The stability of the equivariant representation to input variations is also proved theoretically. The RotDCF framework can be extended to groups other than rotations, providing a general approach which achieves both group equivariance and representation stability at a reduced model size",
    "checked": true,
    "id": "8ccde9d80706a59e606f6e6d48d4260b60ccc736",
    "semantic_title": "rotdcf: decomposition of convolutional filters for rotation-equivariant deep networks",
    "citation_count": 43,
    "authors": []
  },
  "https://openreview.net/forum?id=rkxaNjA9Ym": {
    "title": "Per-Tensor Fixed-Point Quantization of the Back-Propagation Algorithm",
    "volume": "poster",
    "abstract": "The high computational and parameter complexity of neural networks makes their training very slow and difficult to deploy on energy and storage-constrained comput- ing systems. Many network complexity reduction techniques have been proposed including fixed-point implementation. However, a systematic approach for design- ing full fixed-point training and inference of deep neural networks remains elusive. We describe a precision assignment methodology for neural network training in which all network parameters, i.e., activations and weights in the feedforward path, gradients and weight accumulators in the feedback path, are assigned close to minimal precision. The precision assignment is derived analytically and enables tracking the convergence behavior of the full precision training, known to converge a priori. Thus, our work leads to a systematic methodology of determining suit- able precision for fixed-point training. The near optimality (minimality) of the resulting precision assignment is validated empirically for four networks on the CIFAR-10, CIFAR-100, and SVHN datasets. The complexity reduction arising from our approach is compared with other fixed-point neural network designs",
    "checked": true,
    "id": "24c338254c60e9a53443af4d4d99b8333a2a0e81",
    "semantic_title": "per-tensor fixed-point quantization of the back-propagation algorithm",
    "citation_count": 43,
    "authors": []
  },
  "https://openreview.net/forum?id=Skh4jRcKQ": {
    "title": "Understanding Straight-Through Estimator in Training Activation Quantized Neural Nets",
    "volume": "poster",
    "abstract": "Training activation quantized neural networks involves minimizing a piecewise constant training loss whose gradient vanishes almost everywhere, which is undesirable for the standard back-propagation or chain rule. An empirical way around this issue is to use a straight-through estimator (STE) (Bengio et al., 2013) in the backward pass only, so that the \"gradient\" through the modified chain rule becomes non-trivial. Since this unusual \"gradient\" is certainly not the gradient of loss function, the following question arises: why searching in its negative direction minimizes the training loss? In this paper, we provide the theoretical justification of the concept of STE by answering this question. We consider the problem of learning a two-linear-layer network with binarized ReLU activation and Gaussian input data. We shall refer to the unusual \"gradient\" given by the STE-modifed chain rule as coarse gradient. The choice of STE is not unique. We prove that if the STE is properly chosen, the expected coarse gradient correlates positively with the population gradient (not available for the training), and its negation is a descent direction for minimizing the population loss. We further show the associated coarse gradient descent algorithm converges to a critical point of the population loss minimization problem. Moreover, we show that a poor choice of STE leads to instability of the training algorithm near certain local minima, which is verified with CIFAR-10 experiments",
    "checked": true,
    "id": "cf0671ec7da36af49699de81bee05e9549140db2",
    "semantic_title": "understanding straight-through estimator in training activation quantized neural nets",
    "citation_count": 322,
    "authors": []
  },
  "https://openreview.net/forum?id=B1G5ViAqFm": {
    "title": "Convolutional Neural Networks on Non-uniform Geometrical Signals Using Euclidean Spectral Transformation",
    "volume": "poster",
    "abstract": "Convolutional Neural Networks (CNN) have been successful in processing data signals that are uniformly sampled in the spatial domain (e.g., images). However, most data signals do not natively exist on a grid, and in the process of being sampled onto a uniform physical grid suffer significant aliasing error and information loss. Moreover, signals can exist in different topological structures as, for example, points, lines, surfaces and volumes. It has been challenging to analyze signals with mixed topologies (for example, point cloud with surface mesh). To this end, we develop mathematical formulations for Non-Uniform Fourier Transforms (NUFT) to directly, and optimally, sample nonuniform data signals of different topologies defined on a simplex mesh into the spectral domain with no spatial sampling error. The spectral transform is performed in the Euclidean space, which removes the translation ambiguity from works on the graph spectrum. Our representation has four distinct advantages: (1) the process causes no spatial sampling error during initial sampling, (2) the generality of this approach provides a unified framework for using CNNs to analyze signals of mixed topologies, (3) it allows us to leverage state-of-the-art backbone CNN architectures for effective learning without having to design a particular architecture for a particular data structure in an ad-hoc fashion, and (4) the representation allows weighted meshes where each element has a different weight (i.e., texture) indicating local properties. We achieve good results on-par with state-of-the-art for 3D shape retrieval task, and new state-of-the-art for point cloud to surface reconstruction task",
    "checked": true,
    "id": "c7ae7cb97e954b33878c893ed237886c2bfc9e7d",
    "semantic_title": "convolutional neural networks on non-uniform geometrical signals using euclidean spectral transformation",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=BJfIVjAcKm": {
    "title": "Training for Faster Adversarial Robustness Verification via Inducing ReLU Stability",
    "volume": "poster",
    "abstract": "We explore the concept of co-design in the context of neural network verification. Specifically, we aim to train deep neural networks that not only are robust to adversarial perturbations but also whose robustness can be verified more easily. To this end, we identify two properties of network models - weight sparsity and so-called ReLU stability - that turn out to significantly impact the complexity of the corresponding verification task. We demonstrate that improving weight sparsity alone already enables us to turn computationally intractable verification problems into tractable ones. Then, improving ReLU stability leads to an additional 4-13x speedup in verification times. An important feature of our methodology is its \"universality,\" in the sense that it can be used with a broad range of training procedures and verification approaches",
    "checked": true,
    "id": "de49430578bb3f8de3e610423255662c45f17610",
    "semantic_title": "training for faster adversarial robustness verification via inducing relu stability",
    "citation_count": 202,
    "authors": []
  }
}