{
  "https://openreview.net/forum?id=B1gabhRcYX": {
    "title": "BA-Net: Dense Bundle Adjustment Networks",
    "volume": "oral",
    "abstract": "",
    "checked": false,
    "id": "4ba326228728c7ea03a4de6778d0b428b035d5cd",
    "semantic_title": "ba-net: dense bundle adjustment network",
    "citation_count": 289,
    "authors": []
  },
  "https://openreview.net/forum?id=HygBZnRctX": {
    "title": "Transferring Knowledge across Learning Processes",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "0c37a1feac9a09a2fa5554cb7ff735d4bcd6ed5b",
    "semantic_title": "transferring knowledge across learning processes",
    "citation_count": 64,
    "authors": []
  },
  "https://openreview.net/forum?id=rJl-b3RcF7": {
    "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "21937ecd9d66567184b83eca3d3e09eb4e6fbd60",
    "semantic_title": "the lottery ticket hypothesis: finding sparse, trainable neural networks",
    "citation_count": 3489,
    "authors": []
  },
  "https://openreview.net/forum?id=SkVhlh09tX": {
    "title": "Pay Less Attention with Lightweight and Dynamic Convolutions",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "fea820b7d953d32069e189af2961c28fd213470b",
    "semantic_title": "pay less attention with lightweight and dynamic convolutions",
    "citation_count": 610,
    "authors": []
  },
  "https://openreview.net/forum?id=S1x4ghC9tQ": {
    "title": "Temporal Difference Variational Auto-Encoder",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "9d671a4de50b98c3f00623ee597e37c9f00ba0cc",
    "semantic_title": "temporal difference variational auto-encoder",
    "citation_count": 127,
    "authors": []
  },
  "https://openreview.net/forum?id=rJgMlhRctm": {
    "title": "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "50f76736c3090c6effac25400e5e40cc0b7b5ad9",
    "semantic_title": "the neuro-symbolic concept learner: interpreting scenes, words, and sentences from natural supervision",
    "citation_count": 704,
    "authors": []
  },
  "https://openreview.net/forum?id=Byg3y3C9Km": {
    "title": "Learning Protein Structure with a Differentiable Simulator",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "5909909bd3f6d6d15d5c8d9a64c8031cddf604db",
    "semantic_title": "learning protein structure with a differentiable simulator",
    "citation_count": 148,
    "authors": []
  },
  "https://openreview.net/forum?id=rJxgknCcK7": {
    "title": "FFJORD: Free-Form Continuous Dynamics for Scalable Reversible Generative Models",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "8afa6dd9f9ac46462a1fb70a757c4ae1cd45bbf6",
    "semantic_title": "ffjord: free-form continuous dynamics for scalable reversible generative models",
    "citation_count": 882,
    "authors": []
  },
  "https://openreview.net/forum?id=r1lYRjC9F7": {
    "title": "Enabling Factorized Piano Music Modeling and Generation with the MAESTRO Dataset",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "2603a68b4503ba949c91c7e00cd342624b4aae2f",
    "semantic_title": "enabling factorized piano music modeling and generation with the maestro dataset",
    "citation_count": 452,
    "authors": []
  },
  "https://openreview.net/forum?id=ryGs6iA5Km": {
    "title": "How Powerful are Graph Neural Networks?",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "62ed9bf1d83c8db1f9cbf92ea2f57ea90ef683d9",
    "semantic_title": "how powerful are graph neural networks?",
    "citation_count": 7711,
    "authors": []
  },
  "https://openreview.net/forum?id=HylzTiC5Km": {
    "title": "GENERATING HIGH FIDELITY IMAGES WITH SUBSCALE PIXEL NETWORKS AND MULTIDIMENSIONAL UPSCALING",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "e8fd01d3a2ae47827cc8e008d658bca830d99415",
    "semantic_title": "generating high fidelity images with subscale pixel networks and multidimensional upscaling",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S1xq3oR5tQ": {
    "title": "A Unified Theory of Early Visual Representations from Retina to Cortex through Anatomically Constrained Deep CNNs",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "ad0466d3f1c128691416bab7244ace11a6716c7d",
    "semantic_title": "a unified theory of early visual representations from retina to cortex through anatomically constrained deep cnns",
    "citation_count": 72,
    "authors": []
  },
  "https://openreview.net/forum?id=Bklr3j0cKX": {
    "title": "Learning deep representations by mutual information estimation and maximization",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "af3825437b627db1a99f946f7aa773ba8b03befd",
    "semantic_title": "learning deep representations by mutual information estimation and maximization",
    "citation_count": 2675,
    "authors": []
  },
  "https://openreview.net/forum?id=rJEjjoR9K7": {
    "title": "Learning Robust Representations by Projecting Superficial Statistics Out",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "96b32b204a62777bef66eea595de2c47b4e9d6e9",
    "semantic_title": "learning robust representations by projecting superficial statistics out",
    "citation_count": 238,
    "authors": []
  },
  "https://openreview.net/forum?id=HkNDsiC9KQ": {
    "title": "Meta-Learning Update Rules for Unsupervised Representation Learning",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "c6509a450bdda7ca5b8567103dfe9671dbf3b567",
    "semantic_title": "meta-learning update rules for unsupervised representation learning",
    "citation_count": 123,
    "authors": []
  },
  "https://openreview.net/forum?id=B1l6qiR5F7": {
    "title": "Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "15d6f3d815d0ff176fafb14a3f46e5723ebac723",
    "semantic_title": "ordered neurons: integrating tree structures into recurrent neural networks",
    "citation_count": 325,
    "authors": []
  },
  "https://openreview.net/forum?id=Bygh9j09KX": {
    "title": "ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "0f50b7483f1b200ebf88c4dd7698de986399a0f3",
    "semantic_title": "imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness",
    "citation_count": 2678,
    "authors": []
  },
  "https://openreview.net/forum?id=B1xsqj09Fm": {
    "title": "Large Scale GAN Training for High Fidelity Natural Image Synthesis",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "22aab110058ebbd198edb1f1e7b4f69fb13c0613",
    "semantic_title": "large scale gan training for high fidelity natural image synthesis",
    "citation_count": 5412,
    "authors": []
  },
  "https://openreview.net/forum?id=ByeZ5jC5YQ": {
    "title": "KnockoffGAN: Generating Knockoffs for Feature Selection using Generative Adversarial Networks",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "7b767e7cb5b67589646f0c1162423ceeb2daa3a7",
    "semantic_title": "knockoffgan: generating knockoffs for feature selection using generative adversarial networks",
    "citation_count": 58,
    "authors": []
  },
  "https://openreview.net/forum?id=r1xlvi0qYm": {
    "title": "Learning to Remember More with Less Memorization",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "dc836eda6649fa9e0534a7ba4ddafd97a579926b",
    "semantic_title": "learning to remember more with less memorization",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=B1l08oAct7": {
    "title": "Deterministic Variational Inference for Robust Bayesian Neural Networks",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "51bb7f2681605651a4df725ac6c9e18377d36ac5",
    "semantic_title": "deterministic variational inference for robust bayesian neural networks",
    "citation_count": 165,
    "authors": []
  },
  "https://openreview.net/forum?id=rJVorjCcKQ": {
    "title": "Slalom: Fast, Verifiable and Private Execution of Neural Networks in Trusted Hardware",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "662f3ca1f074e24803d33fcd6c7d19564de107f2",
    "semantic_title": "slalom: fast, verifiable and private execution of neural networks in trusted hardware",
    "citation_count": 401,
    "authors": []
  },
  "https://openreview.net/forum?id=HJx54i05tX": {
    "title": "On Random Deep Weight-Tied Autoencoders: Exact Asymptotic Analysis, Phase Transitions, and Implications to Training",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "5a0772885ed113e9da7aa643c877feea387d160e",
    "semantic_title": "on random deep weight-tied autoencoders: exact asymptotic analysis, phase transitions, and implications to training",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=H1xSNiRcF7": {
    "title": "Smoothing the Geometry of Probabilistic Box Embeddings",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "63bcdc392c7135ccc2ded4cd53ef4e2f65ba80a0",
    "semantic_title": "smoothing the geometry of probabilistic box embeddings",
    "citation_count": 85,
    "authors": []
  },
  "https://openreview.net/forum?id=rJl0r3R9KX": {
    "title": "Regularized Learning for Domain Adaptation under Label Shifts",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6d64363e52cd7ecad99d7ce6ae849f245dfbbf92",
    "semantic_title": "regularized learning for domain adaptation under label shifts",
    "citation_count": 208,
    "authors": []
  },
  "https://openreview.net/forum?id=SylCrnCcFX": {
    "title": "Towards Robust, Locally Linear Deep Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a403099192180450edc594d2fed7a19296e9ff48",
    "semantic_title": "towards robust, locally linear deep networks",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=HylTBhA5tQ": {
    "title": "The Limitations of Adversarial Training and the Blind-Spot Attack",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e749e8c947550485eddf864f8efeb870b894e4ce",
    "semantic_title": "the limitations of adversarial training and the blind-spot attack",
    "citation_count": 145,
    "authors": []
  },
  "https://openreview.net/forum?id=B1gTShAct7": {
    "title": "Learning to Learn without Forgetting by Maximizing Transfer and Minimizing Interference",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2b877889ac31b73d1ede70b00eb4c7118ef8eca2",
    "semantic_title": "learning to learn without forgetting by maximizing transfer and minimizing interference",
    "citation_count": 790,
    "authors": []
  },
  "https://openreview.net/forum?id=ryxnHhRqFm": {
    "title": "Global-to-local Memory Pointer Networks for Task-Oriented Dialogue",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "924acc8b76779a5211cc04bbe6cf13bd0bc6e7f8",
    "semantic_title": "global-to-local memory pointer networks for task-oriented dialogue",
    "citation_count": 168,
    "authors": []
  },
  "https://openreview.net/forum?id=rJlnB3C5Ym": {
    "title": "Rethinking the Value of Network Pruning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4a1004ecd34118116344633c7cdcc34493c423ee",
    "semantic_title": "rethinking the value of network pruning",
    "citation_count": 1480,
    "authors": []
  },
  "https://openreview.net/forum?id=ByzcS3AcYX": {
    "title": "Neural TTS Stylization with Adversarial and Collaborative Games",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c978ee204c0f6ab6e1a8c635a93d16612927e24b",
    "semantic_title": "neural tts stylization with adversarial and collaborative games",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=SJe9rh0cFX": {
    "title": "On the Universal Approximability and Complexity Bounds of Quantized ReLU Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "169b173ed1a397e8b47d9dbac7eefb770cb30ce9",
    "semantic_title": "on the universal approximability and complexity bounds of quantized relu neural networks",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=Ske5r3AqK7": {
    "title": "Poincare Glove: Hyperbolic Word Embeddings",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "e8fa823c17aeb8d08fe9aa5fc2bc0eaacb9edcdf",
    "semantic_title": "poincaré glove: hyperbolic word embeddings",
    "citation_count": 288,
    "authors": []
  },
  "https://openreview.net/forum?id=B1lKS2AqtX": {
    "title": "Eidetic 3D LSTM: A Model for Video Prediction and Beyond",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "82a646e1cb33124f672beba451f5039e9e32fb6d",
    "semantic_title": "eidetic 3d lstm: a model for video prediction and beyond",
    "citation_count": 361,
    "authors": []
  },
  "https://openreview.net/forum?id=HkxKH2AcFm": {
    "title": "Towards GAN Benchmarks Which Require Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c7dbce9513f4fc902da181d6630995627ba83192",
    "semantic_title": "towards gan benchmarks which require generalization",
    "citation_count": 59,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgKBhA5Y7": {
    "title": "There Are Many Consistent Explanations of Unlabeled Data: Why You Should Average",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "98286df6d923d787f26e034bbaf3a5a64ac29cb1",
    "semantic_title": "there are many consistent explanations of unlabeled data: why you should average",
    "citation_count": 244,
    "authors": []
  },
  "https://openreview.net/forum?id=ryeOSnAqYm": {
    "title": "Synthetic Datasets for Neural Program Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a15763582df784b43548c6d53edfd55568c35168",
    "semantic_title": "synthetic datasets for neural program synthesis",
    "citation_count": 46,
    "authors": []
  },
  "https://openreview.net/forum?id=r1xdH3CcKX": {
    "title": "Stochastic Prediction of Multi-Agent Interactions from Partial Observations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "906c03e6e188d301b85ca6521955a8584f9babe7",
    "semantic_title": "stochastic prediction of multi-agent interactions from partial observations",
    "citation_count": 89,
    "authors": []
  },
  "https://openreview.net/forum?id=HyePrhR5KX": {
    "title": "DyRep: Learning Representations over Dynamic Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5593001e49474a475dbcae99be350a8d527c05a3",
    "semantic_title": "dyrep: learning representations over dynamic graphs",
    "citation_count": 520,
    "authors": []
  },
  "https://openreview.net/forum?id=rkxwShA9Ym": {
    "title": "Label super-resolution networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "938bd120c083170c785be0dbb3a67edbb6e5356a",
    "semantic_title": "label super-resolution networks",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=HkfPSh05K7": {
    "title": "Multi-step Retriever-Reader Interaction for Scalable Open-domain Question Answering",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e7512b84e923372ae410d7614e71224d573ed2ef",
    "semantic_title": "multi-step retriever-reader interaction for scalable open-domain question answering",
    "citation_count": 165,
    "authors": []
  },
  "https://openreview.net/forum?id=Byl8BnRcYm": {
    "title": "Capsule Graph Neural Network",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "67de0bc5d1dfb458efc9aa192c879bf00ee667b3",
    "semantic_title": "capsule graph neural network",
    "citation_count": 227,
    "authors": []
  },
  "https://openreview.net/forum?id=Syl8Sn0cK7": {
    "title": "Learning a Meta-Solver for Syntax-Guided Program Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f262ad9e3a84bcea08acd1c97ee9740cd78ee256",
    "semantic_title": "learning a meta-solver for syntax-guided program synthesis",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=H1eSS3CcKX": {
    "title": "Stochastic Optimization of Sorting Networks via Continuous Relaxations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "38e3a9620f575dc16cf26957c3cafa035b8f1895",
    "semantic_title": "stochastic optimization of sorting networks via continuous relaxations",
    "citation_count": 174,
    "authors": []
  },
  "https://openreview.net/forum?id=BylBr3C9K7": {
    "title": "Energy-Constrained Compression for Deep Neural Networks via Weighted Sparse Projection and Layer Input Masking",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0986f2ac6755df5d196ceb09b5bdf19593cbbaef",
    "semantic_title": "energy-constrained compression for deep neural networks via weighted sparse projection and layer input masking",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=rygrBhC5tQ": {
    "title": "Composing Complex Skills by Learning Transition Policies",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "74e12851de2d542aa2aef7b8a39ef021a5802689",
    "semantic_title": "composing complex skills by learning transition policies",
    "citation_count": 92,
    "authors": []
  },
  "https://openreview.net/forum?id=ryxSrhC9KX": {
    "title": "Revealing interpretable object representations from human behavior",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bfd9f718c8e7ec5e3ef47e7cc388fa9918a86525",
    "semantic_title": "revealing interpretable object representations from human behavior",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=HylVB3AqYm": {
    "title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f323407464c4cd492d3fc1afd7170eab08f44d9b",
    "semantic_title": "proxylessnas: direct neural architecture search on target task and hardware",
    "citation_count": 1877,
    "authors": []
  },
  "https://openreview.net/forum?id=r1x4BnCqKX": {
    "title": "A Generative Model For Electron Paths",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6e3e17ac9a5a25ca718bd2da025a7117c4f03634",
    "semantic_title": "a generative model for electron paths",
    "citation_count": 56,
    "authors": []
  },
  "https://openreview.net/forum?id=rylNH20qFQ": {
    "title": "Learning to Infer and Execute 3D Shape Programs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7e2f5eca9465cf114043ed6c95ea59d9dbea45a1",
    "semantic_title": "learning to infer and execute 3d shape programs",
    "citation_count": 147,
    "authors": []
  },
  "https://openreview.net/forum?id=rJe4ShAcF7": {
    "title": "Music Transformer: Generating Music with Long-Term Structure",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "fb507ada871d1e8c29e376dbf7b7879689aa89f9",
    "semantic_title": "music transformer: generating music with long-term structure",
    "citation_count": 486,
    "authors": []
  },
  "https://openreview.net/forum?id=SkgQBn0cF7": {
    "title": "Modeling the Long Term Future in Model-Based Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c4e02b75e525c4e8c4616d2e9dfeb4a638142c51",
    "semantic_title": "modeling the long term future in model-based reinforcement learning",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=HygQBn0cYm": {
    "title": "Model-Predictive Policy Learning with Uncertainty Regularization for Driving in Dense Traffic",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2eeace98cf3c105a8d37884dc8d33c50ae4b7ddb",
    "semantic_title": "model-predictive policy learning with uncertainty regularization for driving in dense traffic",
    "citation_count": 123,
    "authors": []
  },
  "https://openreview.net/forum?id=ByeMB3Act7": {
    "title": "Learning to Screen for Fast Softmax Inference on Large Vocabulary Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "dccf55bad7c065fba1f25e56699584392895b05a",
    "semantic_title": "learning to screen for fast softmax inference on large vocabulary neural networks",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=r1efr3C9Ym": {
    "title": "Interpolation-Prediction Networks for Irregularly Sampled Time Series",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "07334ab67ee57811f32da13588f900e8dc3211c6",
    "semantic_title": "interpolation-prediction networks for irregularly sampled time series",
    "citation_count": 148,
    "authors": []
  },
  "https://openreview.net/forum?id=HyxGB2AcY7": {
    "title": "Contingency-Aware Exploration in Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bf604ae3ddd5adec55554921b37f04035b7350a7",
    "semantic_title": "contingency-aware exploration in reinforcement learning",
    "citation_count": 73,
    "authors": []
  },
  "https://openreview.net/forum?id=BkgWHnR5tm": {
    "title": "Neural Graph Evolution: Towards Efficient Automatic Robot Design",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5b3a45b76e144fed2478419a7229930b28151f46",
    "semantic_title": "neural graph evolution: towards efficient automatic robot design",
    "citation_count": 63,
    "authors": []
  },
  "https://openreview.net/forum?id=Bkxbrn0cYX": {
    "title": "Selfless Sequential Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "974e22699cb165b0285a7c72b5cc6c3d42010143",
    "semantic_title": "selfless sequential learning",
    "citation_count": 114,
    "authors": []
  },
  "https://openreview.net/forum?id=rJgbSn09Ym": {
    "title": "Learning Particle Dynamics for Manipulating Rigid Bodies, Deformable Objects, and Fluids",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d40531ed81859ce40f119c1bbc1d1cb50af498fd",
    "semantic_title": "learning particle dynamics for manipulating rigid bodies, deformable objects, and fluids",
    "citation_count": 399,
    "authors": []
  },
  "https://openreview.net/forum?id=H1zeHnA9KX": {
    "title": "Representing Formal Languages: A Comparison Between Finite Automata and Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9fac39c702724732188ff61090d2e2b498a34eb2",
    "semantic_title": "representing formal languages: a comparison between finite automata and recurrent neural networks",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=B1exrnCcF7": {
    "title": "Disjoint Mapping Network for Cross-modal Matching of Voices and Faces",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4188f289eb85bb047dbbc15acbd79fae6abe25f5",
    "semantic_title": "disjoint mapping network for cross-modal matching of voices and faces",
    "citation_count": 71,
    "authors": []
  },
  "https://openreview.net/forum?id=ByleB2CcKm": {
    "title": "Learning Procedural Abstractions and Evaluating Discrete Latent Temporal Structure",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a92a616aa46f5d06cab7a02c337370ec0e19b4f4",
    "semantic_title": "learning procedural abstractions and evaluating discrete latent temporal structure",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=ByfyHh05tQ": {
    "title": "Learning to Design RNA",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "60b115f0c1ea9565928ebced8d0606f4c31d6640",
    "semantic_title": "learning to design rna",
    "citation_count": 72,
    "authors": []
  },
  "https://openreview.net/forum?id=BygANhA9tQ": {
    "title": "Cost-Sensitive Robustness against Adversarial Examples",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "75d59ae0ed3ce51e37b383985cfff310251f591a",
    "semantic_title": "cost-sensitive robustness against adversarial examples",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=S1lTEh09FQ": {
    "title": "Combinatorial Attacks on Binarized Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e51b580d595a74616272ec3a1a45ed05989a3453",
    "semantic_title": "combinatorial attacks on binarized neural networks",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=r1laEnA5Ym": {
    "title": "A Variational Inequality Perspective on Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "641f0891e63ea033332be6428fa815beaecb61e2",
    "semantic_title": "a variational inequality perspective on generative adversarial networks",
    "citation_count": 353,
    "authors": []
  },
  "https://openreview.net/forum?id=H1g2NhC5KQ": {
    "title": "Multiple-Attribute Text Rewriting",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "49c34076336a5de8e195a9af933b925b5e16d46a",
    "semantic_title": "multiple-attribute text rewriting",
    "citation_count": 236,
    "authors": []
  },
  "https://openreview.net/forum?id=HyGhN2A5tm": {
    "title": "Multi-Agent Dual Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4f00db92bd7c1fd7b41d361c13797b9ff45a5b91",
    "semantic_title": "multi-agent dual learning",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=SJxsV2R5FQ": {
    "title": "Learning sparse relational transition models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b445e8f3d812f59b68f6ed70890d14a882f99b89",
    "semantic_title": "learning sparse relational transition models",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=rkxoNnC5FQ": {
    "title": "SPIGAN: Privileged Adversarial Learning from Simulation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1962791d77a5f7555e8921ab9e92f7cd1fd95736",
    "semantic_title": "spigan: privileged adversarial learning from simulation",
    "citation_count": 105,
    "authors": []
  },
  "https://openreview.net/forum?id=Syx5V2CcFm": {
    "title": "Universal Stagewise Learning for Non-Convex Problems with Convergence on Averaged Solutions",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a048662aab04b4f683cb6778100356892527fb1b",
    "semantic_title": "universal stagewise learning for non-convex problems with convergence on averaged solutions",
    "citation_count": 58,
    "authors": []
  },
  "https://openreview.net/forum?id=HJx9EhC9tQ": {
    "title": "Reasoning About Physical Interactions with Object-Oriented Prediction and Planning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "91cb47c26dffa7e3fcc339abdaab9fa229b37d95",
    "semantic_title": "reasoning about physical interactions with object-oriented prediction and planning",
    "citation_count": 129,
    "authors": []
  },
  "https://openreview.net/forum?id=BkltNhC9FX": {
    "title": "Posterior Attention Models for Sequence to Sequence Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "086cef88cbac0b733e1d0b7d4756600df953851f",
    "semantic_title": "posterior attention models for sequence to sequence learning",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=HJeu43ActQ": {
    "title": "NOODL: Provable Online Dictionary Learning and Sparse Coding",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "23e4f9ee3c7f6d201081cc895ddb5f33476b9058",
    "semantic_title": "noodl: provable online dictionary learning and sparse coding",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=rJedV3R5tm": {
    "title": "RelGAN: Relational Generative Adversarial Networks for Text Generation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0e8b9dc48e4c8be508a7797fd7742506fe59875c",
    "semantic_title": "relgan: relational generative adversarial networks for text generation",
    "citation_count": 176,
    "authors": []
  },
  "https://openreview.net/forum?id=H1xwNhCcYm": {
    "title": "Do Deep Generative Models Know What They Don't Know?",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6507909a8f77c88144c3a67b9336bd1c85e84cac",
    "semantic_title": "do deep generative models know what they don't know?",
    "citation_count": 759,
    "authors": []
  },
  "https://openreview.net/forum?id=BJxvEh0cFQ": {
    "title": "K for the Price of 1: Parameter-efficient Multi-task and Transfer Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "e685efa9ab0672bd83b01cc61e5cc753aa33db68",
    "semantic_title": "k for the price of 1: parameter efficient multi-task and transfer learning",
    "citation_count": 69,
    "authors": []
  },
  "https://openreview.net/forum?id=S1lDV3RcKm": {
    "title": "MisGAN: Learning from Incomplete Data with Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0a869336c65185f078ba473d7ca5b86a371ab929",
    "semantic_title": "misgan: learning from incomplete data with generative adversarial networks",
    "citation_count": 171,
    "authors": []
  },
  "https://openreview.net/forum?id=S1xLN3C9YX": {
    "title": "Learnable Embedding Space for Efficient Neural Architecture Compression",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8111960524288d0e33079be9249d73aab7667c85",
    "semantic_title": "learnable embedding space for efficient neural architecture compression",
    "citation_count": 43,
    "authors": []
  },
  "https://openreview.net/forum?id=HkgSEnA5KQ": {
    "title": "Guiding Policies with Language via Meta-Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "34f5d2f039558ba0b6a5103553ad68321fa6eabd",
    "semantic_title": "guiding policies with language via meta-learning",
    "citation_count": 64,
    "authors": []
  },
  "https://openreview.net/forum?id=HJfSEnRqKQ": {
    "title": "Active Learning with Partial Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "58c04126a5196deb57ae31d6174cd4aae154f138",
    "semantic_title": "active learning with partial feedback",
    "citation_count": 67,
    "authors": []
  },
  "https://openreview.net/forum?id=S1xNEhR9KX": {
    "title": "On the Sensitivity of Adversarial Robustness to Input Data Distributions",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "508dad538d5c63eb6c07fd10794510357a951a58",
    "semantic_title": "on the sensitivity of adversarial robustness to input data distributions",
    "citation_count": 60,
    "authors": []
  },
  "https://openreview.net/forum?id=ByME42AqK7": {
    "title": "Efficient Multi-Objective Neural Architecture Search via Lamarckian Evolution",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "77e5aa8c33a9cb9dd3f0874b09fd84389360e88e",
    "semantic_title": "efficient multi-objective neural architecture search via lamarckian evolution",
    "citation_count": 503,
    "authors": []
  },
  "https://openreview.net/forum?id=SklEEnC5tQ": {
    "title": "DISTRIBUTIONAL CONCAVITY REGULARIZATION FOR GANS",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "81473ba213bae1aa188e03e54f91319914159c9c",
    "semantic_title": "distributional concavity regularization for gans",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=r1g4E3C9t7": {
    "title": "Characterizing Audio Adversarial Examples Using Temporal Dependency",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d5c92fafb030f76d27c749e36550c390c8b3d63b",
    "semantic_title": "characterizing audio adversarial examples using temporal dependency",
    "citation_count": 165,
    "authors": []
  },
  "https://openreview.net/forum?id=H1xQVn09FX": {
    "title": "GANSynth: Adversarial Neural Audio Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "725c650ae8db8d9a57e4a7b15a555dbe69b67054",
    "semantic_title": "gansynth: adversarial neural audio synthesis",
    "citation_count": 392,
    "authors": []
  },
  "https://openreview.net/forum?id=BylQV305YQ": {
    "title": "Toward Understanding the Impact of Staleness in Distributed Machine Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "39f593a6ba742751f46011d97661d20d7437d016",
    "semantic_title": "toward understanding the impact of staleness in distributed machine learning",
    "citation_count": 81,
    "authors": []
  },
  "https://openreview.net/forum?id=r1xX42R5Fm": {
    "title": "Beyond Greedy Ranking: Slate Optimization via List-CVAE",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7fb66090fe4b7e9a56a91bf977e6f40b49d569ac",
    "semantic_title": "beyond greedy ranking: slate optimization via list-cvae",
    "citation_count": 50,
    "authors": []
  },
  "https://openreview.net/forum?id=SyxfEn09Y7": {
    "title": "G-SGD: Optimizing ReLU Neural Networks in its Positively Scale-Invariant Space",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "15cfd11abb010eb2d84dbbe9b80dd80f2e98922d",
    "semantic_title": "g-sgd: optimizing relu neural networks in its positively scale-invariant space",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=Bkl-43C9FQ": {
    "title": "Spherical CNNs on Unstructured Grids",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4aa2c7ab9646b9f988d2dd2e2375060b3158dc2a",
    "semantic_title": "spherical cnns on unstructured grids",
    "citation_count": 184,
    "authors": []
  },
  "https://openreview.net/forum?id=HJgeEh09KQ": {
    "title": "Boosting Robustness Certification of Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4963fe1027aebe63217bd2904decf24f59379e1f",
    "semantic_title": "boosting robustness certification of neural networks",
    "citation_count": 170,
    "authors": []
  },
  "https://openreview.net/forum?id=rJleN20qK7": {
    "title": "Two-Timescale Networks for Nonlinear Value Function Approximation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e4eed0e966c134bbff6a5a2a38ac2b3e44900906",
    "semantic_title": "two-timescale networks for nonlinear value function approximation",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=BJlgNh0qKQ": {
    "title": "Differentiable Perturb-and-Parse: Semi-Supervised Parsing with a Structured Variational Autoencoder",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "29927b4e90810e05c3f329a3a9324d4f6859a93c",
    "semantic_title": "differentiable perturb-and-parse: semi-supervised parsing with a structured variational autoencoder",
    "citation_count": 56,
    "authors": []
  },
  "https://openreview.net/forum?id=BJe1E2R5KX": {
    "title": "Algorithmic Framework for Model-based Deep Reinforcement Learning with Theoretical Guarantees",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d333f99881b09426283a9c7a1d25f7ac30d63062",
    "semantic_title": "algorithmic framework for model-based deep reinforcement learning with theoretical guarantees",
    "citation_count": 227,
    "authors": []
  },
  "https://openreview.net/forum?id=HkzRQhR9YX": {
    "title": "Tree-Structured Recurrent Switching Linear Dynamical Systems for Multi-Scale Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "349f1028e6587604108fd720025cf4b190da2dec",
    "semantic_title": "tree-structured recurrent switching linear dynamical systems for multi-scale modeling",
    "citation_count": 73,
    "authors": []
  },
  "https://openreview.net/forum?id=B1e0X3C9tQ": {
    "title": "Diagnosing and Enhancing VAE Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f6a201eed70e8b48e2f60d97c98cfc8fe3b7b175",
    "semantic_title": "diagnosing and enhancing vae models",
    "citation_count": 381,
    "authors": []
  },
  "https://openreview.net/forum?id=HylTXn0qYX": {
    "title": "Efficiently testing local optimality and escaping saddles for ReLU networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "04a98e1fd56e0d66eeb5f66e5b6a4d310759b3eb",
    "semantic_title": "efficiently testing local optimality and escaping saddles for relu networks",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=HJE6X305Fm": {
    "title": "Don't let your Discriminator be fooled",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1cbbaa7c718cb9abae10955a0ad6dcd3b1e123bb",
    "semantic_title": "don't let your discriminator be fooled",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=B1xhQhRcK7": {
    "title": "Rigorous Agent Evaluation: An Adversarial Approach to Uncover Catastrophic Failures",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5ad2c21ad1c3267e4ce484543bb699d558066d9a",
    "semantic_title": "rigorous agent evaluation: an adversarial approach to uncover catastrophic failures",
    "citation_count": 81,
    "authors": []
  },
  "https://openreview.net/forum?id=Sklsm20ctX": {
    "title": "Competitive experience replay",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ee248883b124a80bdbbc686e906145ccdeadc834",
    "semantic_title": "competitive experience replay",
    "citation_count": 53,
    "authors": []
  },
  "https://openreview.net/forum?id=BJej72AqF7": {
    "title": "A Max-Affine Spline Perspective of Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1bd7d872d54a650affb8b0e1101c0c52e2a912da",
    "semantic_title": "a max-affine spline perspective of recurrent neural networks",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=Byg5QhR5FQ": {
    "title": "Top-Down Neural Model For Formulae",
    "volume": "poster",
    "abstract": "We present a simple neural model that given a formula and a property tries to answer the question whether the formula has the given property, for example whether a propositional formula is always true. The structure of the formula is captured by a feedforward neural network recursively built for the given formula in a top-down manner. The results of this network are then processed by two recurrent neural networks. One of the interesting aspects of our model is how propositional atoms are treated. For example, the model is insensitive to their names, it only matters whether they are the same or distinct",
    "checked": true,
    "id": "f6b394f714ef1b9ace8879356c1db2badd21ba82",
    "semantic_title": "top-down neural model for formulae",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=S1ecm2C9K7": {
    "title": "Feature-Wise Bias Amplification",
    "volume": "poster",
    "abstract": "We study the phenomenon of bias amplification in classifiers, wherein a machine learning model learns to predict classes with a greater disparity than the underlying ground truth. We demonstrate that bias amplification can arise via inductive bias in gradient descent methods resulting in overestimation of importance of moderately-predictive ``weak'' features if insufficient training data is available. This overestimation gives rise to feature-wise bias amplification -- a previously unreported form of bias that can be traced back to the features of a trained model. Through analysis and experiments, we show that the while some bias cannot be mitigated without sacrificing accuracy, feature-wise bias amplification can be mitigated through targeted feature selection. We present two new feature selection algorithms for mitigating bias amplification in linear models, and show how they can be adapted to convolutional neural networks efficiently. Our experiments on synthetic and real data demonstrate that these algorithms consistently lead to reduced bias without harming accuracy, in some cases eliminating predictive bias altogether while providing modest gains in accuracy",
    "checked": true,
    "id": "941ac3f9c1402bbbc1c61f328eb242e99cde5a43",
    "semantic_title": "feature-wise bias amplification",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=HkgYmhR9KX": {
    "title": "AD-VAT: An Asymmetric Dueling mechanism for learning Visual Active Tracking",
    "volume": "poster",
    "abstract": "Visual Active Tracking (VAT) aims at following a target object by autonomously controlling the motion system of a tracker given visual observations. Previous work has shown that the tracker can be trained in a simulator via reinforcement learning and deployed in real-world scenarios. However, during training, such a method requires manually specifying the moving path of the target object to be tracked, which cannot ensure the tracker's generalization on the unseen object moving patterns. To learn a robust tracker for VAT, in this paper, we propose a novel adversarial RL method which adopts an Asymmetric Dueling mechanism, referred to as AD-VAT. In AD-VAT, both the tracker and the target are approximated by end-to-end neural networks, and are trained via RL in a dueling/competitive manner: i.e., the tracker intends to lockup the target, while the target tries to escape from the tracker. They are asymmetric in that the target is aware of the tracker, but not vice versa. Specifically, besides its own observation, the target is fed with the tracker's observation and action, and learns to predict the tracker's reward as an auxiliary task. We show that such an asymmetric dueling mechanism produces a stronger target, which in turn induces a more robust tracker. To stabilize the training, we also propose a novel partial zero-sum reward for the tracker/target. The experimental results, in both 2D and 3D environments, demonstrate that the proposed method leads to a faster convergence in training and yields more robust tracking behaviors in different testing scenarios. For supplementary videos, see: https://www.youtube.com/playlist?list=PL9rZj4Mea7wOZkdajK1TsprRg8iUf51BS The code is available at https://github.com/zfw1226/active_tracking_rl",
    "checked": true,
    "id": "1433d442139fd876001093f5a02fdf9f4f07425a",
    "semantic_title": "ad-vat: an asymmetric dueling mechanism for learning visual active tracking",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=BJfOXnActQ": {
    "title": "Learning to Learn with Conditional Class Dependencies",
    "volume": "poster",
    "abstract": "Neural networks can learn to extract statistical properties from data, but they seldom make use of structured information from the label space to help representation learning. Although some label structure can implicitly be obtained when training on huge amounts of data, in a few-shot learning context where little data is available, making explicit use of the label structure can inform the model to reshape the representation space to reflect a global sense of class dependencies. We propose a meta-learning framework, Conditional class-Aware Meta-Learning (CAML), that conditionally transforms feature representations based on a metric space that is trained to capture inter-class dependencies. This enables a conditional modulation of the feature representations of the base-learner to impose regularities informed by the label space. Experiments show that the conditional transformation in CAML leads to more disentangled representations and achieves competitive results on the miniImageNet benchmark",
    "checked": true,
    "id": "cb80484b34f62d9d7db0bb9214c5374f6bcb861d",
    "semantic_title": "learning to learn with conditional class dependencies",
    "citation_count": 81,
    "authors": []
  },
  "https://openreview.net/forum?id=Hyg_X2C5FX": {
    "title": "GAN Dissection: Visualizing and Understanding Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "Generative Adversarial Networks (GANs) have recently achieved impressive results for many real-world applications, and many GAN variants have emerged with improvements in sample quality and training stability. However, visualization and understanding of GANs is largely missing. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models. In this work, we present an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. We first identify a group of interpretable units that are closely related to object concepts with a segmentation-based network dissection method. Then, we quantify the causal effect of interpretable units by measuring the ability of interventions to control objects in the output. Finally, we examine the contextual relationship between these units and their surrounding by inserting the discovered object concepts into new images. We show several practical applications enabled by our framework, from comparing internal representations across different layers, models, and datasets, to improving GANs by locating and removing artifact-causing units, to interactively manipulating objects in the scene. We provide open source interpretation tools to help peer researchers and practitioners better understand their GAN models",
    "checked": true,
    "id": "df7ad8eeb595da5f7774e91dae06075be952acff",
    "semantic_title": "gan dissection: visualizing and understanding generative adversarial networks",
    "citation_count": 469,
    "authors": []
  },
  "https://openreview.net/forum?id=S1lvm305YQ": {
    "title": "TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre Transfer",
    "volume": "poster",
    "abstract": "In this work, we address the problem of musical timbre transfer, where the goal is to manipulate the timbre of a sound sample from one instrument to match another instrument while preserving other musical content, such as pitch, rhythm, and loudness. In principle, one could apply image-based style transfer techniques to a time-frequency representation of an audio signal, but this depends on having a representation that allows independent manipulation of timbre as well as high-quality waveform generation. We introduce TimbreTron, a method for musical timbre transfer which applies \"image\" domain style transfer to a time-frequency representation of the audio signal, and then produces a high-quality waveform using a conditional WaveNet synthesizer. We show that the Constant Q Transform (CQT) representation is particularly well-suited to convolutional architectures due to its approximate pitch equivariance. Based on human perceptual evaluations, we confirmed that TimbreTron recognizably transferred the timbre while otherwise preserving the musical content, for both monophonic and polyphonic samples. We made an accompanying demo video here: https://www.cs.toronto.edu/~huang/TimbreTron/index.html which we strongly encourage you to watch before reading the paper",
    "checked": true,
    "id": "f1fc78d559a4b6cf02fd2bbe579b9473ae4ae213",
    "semantic_title": "timbretron: a wavenet(cyclegan(cqt(audio))) pipeline for musical timbre transfer",
    "citation_count": 98,
    "authors": []
  },
  "https://openreview.net/forum?id=SyMDXnCcF7": {
    "title": "A Mean Field Theory of Batch Normalization",
    "volume": "poster",
    "abstract": "We develop a mean field theory for batch normalization in fully-connected feedforward neural networks. In so doing, we provide a precise characterization of signal propagation and gradient backpropagation in wide batch-normalized networks at initialization. Our theory shows that gradient signals grow exponentially in depth and that these exploding gradients cannot be eliminated by tuning the initial weight variances or by adjusting the nonlinear activation function. Indeed, batch normalization itself is the cause of gradient explosion. As a result, vanilla batch-normalized networks without skip connections are not trainable at large depths for common initialization schemes, a prediction that we verify with a variety of empirical simulations. While gradient explosion cannot be eliminated, it can be reduced by tuning the network close to the linear regime, which improves the trainability of deep batch-normalized networks without residual connections. Finally, we investigate the learning dynamics of batch-normalized networks and observe that after a single step of optimization the networks achieve a relatively stable equilibrium in which gradients have dramatically smaller dynamic range. Our theory leverages Laplace, Fourier, and Gegenbauer transforms and we derive new identities that may be of independent interest",
    "checked": true,
    "id": "e5b7c1ce5a46e059fce96249c0c034afdd3c287a",
    "semantic_title": "a mean field theory of batch normalization",
    "citation_count": 180,
    "authors": []
  },
  "https://openreview.net/forum?id=HkxLXnAcFQ": {
    "title": "A Closer Look at Few-shot Classification",
    "volume": "poster",
    "abstract": "Few-shot classiﬁcation aims to learn a classiﬁer to recognize unseen classes during training with limited labeled examples. While signiﬁcant progress has been made, the growing complexity of network designs, meta-learning algorithms, and differences in implementation details make a fair comparison difﬁcult. In this paper, we present 1) a consistent comparative analysis of several representative few-shot classiﬁcation algorithms, with results showing that deeper backbones signiﬁcantly reduce the gap across methods including the baseline, 2) a slightly modiﬁed baseline method that surprisingly achieves competitive performance when compared with the state-of-the-art on both the mini-ImageNet and the CUB datasets, and 3) a new experimental setting for evaluating the cross-domain generalization ability for few-shot classiﬁcation algorithms. Our results reveal that reducing intra-class variation is an important factor when the feature backbone is shallow, but not as critical when using deeper backbones. In a realistic, cross-domain evaluation setting, we show that a baseline method with a standard ﬁne-tuning practice compares favorably against other state-of-the-art few-shot learning algorithms",
    "checked": true,
    "id": "9d5ec23154fb278a765f47ba5ee5150bd441d0de",
    "semantic_title": "a closer look at few-shot classification",
    "citation_count": 1770,
    "authors": []
  },
  "https://openreview.net/forum?id=HkzSQhCcK7": {
    "title": "STCN: Stochastic Temporal Convolutional Networks",
    "volume": "poster",
    "abstract": "Convolutional architectures have recently been shown to be competitive on many sequence modelling tasks when compared to the de-facto standard of recurrent neural networks (RNNs) while providing computational and modelling advantages due to inherent parallelism. However, currently, there remains a performance gap to more expressive stochastic RNN variants, especially those with several layers of dependent random variables. In this work, we propose stochastic temporal convolutional networks (STCNs), a novel architecture that combines the computational advantages of temporal convolutional networks (TCN) with the representational power and robustness of stochastic latent spaces. In particular, we propose a hierarchy of stochastic latent variables that captures temporal dependencies at different time-scales. The architecture is modular and flexible due to the decoupling of the deterministic and stochastic layers. We show that the proposed architecture achieves state of the art log-likelihoods across several tasks. Finally, the model is capable of predicting high-quality synthetic samples over a long-range temporal horizon in modelling of handwritten text",
    "checked": true,
    "id": "b9357ef8753b07253b144617e0ef798569192bed",
    "semantic_title": "stcn: stochastic temporal convolutional networks",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=HkgEQnRqYQ": {
    "title": "RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space",
    "volume": "poster",
    "abstract": "We study the problem of learning representations of entities and relations in knowledge graphs for predicting missing links. The success of such a task heavily relies on the ability of modeling and inferring the patterns of (or between) the relations. In this paper, we present a new approach for knowledge graph embedding called RotatE, which is able to model and infer various relation patterns including: symmetry/antisymmetry, inversion, and composition. Specifically, the RotatE model defines each relation as a rotation from the source entity to the target entity in the complex vector space. In addition, we propose a novel self-adversarial negative sampling technique for efficiently and effectively training the RotatE model. Experimental results on multiple benchmark knowledge graphs show that the proposed RotatE model is not only scalable, but also able to infer and model various relation patterns and significantly outperform existing state-of-the-art models for link prediction",
    "checked": true,
    "id": "8f096071a09701012c9c279aee2a88143a295935",
    "semantic_title": "rotate: knowledge graph embedding by relational rotation in complex space",
    "citation_count": 2147,
    "authors": []
  },
  "https://openreview.net/forum?id=BJemQ209FQ": {
    "title": "Learning to Navigate the Web",
    "volume": "poster",
    "abstract": "Learning in environments with large state and action spaces, and sparse rewards, can hinder a Reinforcement Learning (RL) agent's learning through trial-and-error. For instance, following natural language instructions on the Web (such as booking a flight ticket) leads to RL settings where input vocabulary and number of actionable elements on a page can grow very large. Even though recent approaches improve the success rate on relatively simple environments with the help of human demonstrations to guide the exploration, they still fail in environments where the set of possible instructions can reach millions. We approach the aforementioned problems from a different perspective and propose guided RL approaches that can generate unbounded amount of experience for an agent to learn from. Instead of learning from a complicated instruction with a large vocabulary, we decompose it into multiple sub-instructions and schedule a curriculum in which an agent is tasked with a gradually increasing subset of these relatively easier sub-instructions. In addition, when the expert demonstrations are not available, we propose a novel meta-learning framework that generates new instruction following tasks and trains the agent more effectively. We train DQN, deep reinforcement learning agent, with Q-value function approximated with a novel QWeb neural network architecture on these smaller, synthetic instructions. We evaluate the ability of our agent to generalize to new instructions onWorld of Bits benchmark, on forms with up to 100 elements, supporting 14 million possible instructions. The QWeb agent outperforms the baseline without using any human demonstration achieving 100% success rate on several difficult environments",
    "checked": true,
    "id": "a7038473320c50df76fa950aca486015c5659503",
    "semantic_title": "learning to navigate the web",
    "citation_count": 65,
    "authors": []
  },
  "https://openreview.net/forum?id=r1xQQhAqKX": {
    "title": "Modeling Uncertainty with Hedged Instance Embeddings",
    "volume": "poster",
    "abstract": "Instance embeddings are an efficient and versatile image representation that facilitates applications like recognition, verification, retrieval, and clustering. Many metric learning methods represent the input as a single point in the embedding space. Often the distance between points is used as a proxy for match confidence. However, this can fail to represent uncertainty which can arise when the input is ambiguous, e.g., due to occlusion or blurriness. This work addresses this issue and explicitly models the uncertainty by \"hedging\" the location of each input in the embedding space. We introduce the hedged instance embedding (HIB) in which embeddings are modeled as random variables and the model is trained under the variational information bottleneck principle (Alemi et al., 2016; Achille & Soatto, 2018). Empirical results on our new N-digit MNIST dataset show that our method leads to the desired behavior of \"hedging its bets\" across the embedding space upon encountering ambiguous inputs. This results in improved performance for image matching and classification tasks, more structure in the learned embedding space, and an ability to compute a per-exemplar uncertainty measure which is correlated with downstream performance",
    "checked": true,
    "id": "99e702dead5fa8e7edbb95d8f27925000bd9b145",
    "semantic_title": "modeling uncertainty with hedged instance embeddings",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=B1ffQnRcKX": {
    "title": "Automatically Composing Representation Transformations as a Means for Generalization",
    "volume": "poster",
    "abstract": "A generally intelligent learner should generalize to more complex tasks than it has previously encountered, but the two common paradigms in machine learning -- either training a separate learner per task or training a single learner for all tasks -- both have difficulty with such generalization because they do not leverage the compositional structure of the task distribution. This paper introduces the compositional problem graph as a broadly applicable formalism to relate tasks of different complexity in terms of problems with shared subproblems. We propose the compositional generalization problem for measuring how readily old knowledge can be reused and hence built upon. As a first step for tackling compositional generalization, we introduce the compositional recursive learner, a domain-general framework for learning algorithmic procedures for composing representation transformations, producing a learner that reasons about what computation to execute by making analogies to previously seen problems. We show on a symbolic and a high-dimensional domain that our compositional approach can generalize to more complex problems than the learner has previously encountered, whereas baselines that are not explicitly compositional do not",
    "checked": true,
    "id": "1766648967f6206a944a4bd18bbbd92a74c164bd",
    "semantic_title": "automatically composing representation transformations as a means for generalization",
    "citation_count": 70,
    "authors": []
  },
  "https://openreview.net/forum?id=HkezXnA9YX": {
    "title": "Systematic Generalization: What Is Required and Can It Be Learned?",
    "volume": "poster",
    "abstract": "Numerous models for grounded language understanding have been recently proposed, including (i) generic models that can be easily adapted to any given task and (ii) intuitively appealing modular models that require background knowledge to be instantiated. We compare both types of models in how much they lend themselves to a particular form of systematic generalization. Using a synthetic VQA test, we evaluate which models are capable of reasoning about all possible object pairs after training on only a small subset of them. Our findings show that the generalization of modular models is much more systematic and that it is highly sensitive to the module layout, i.e. to how exactly the modules are connected. We furthermore investigate if modular models that generalize well could be made more end-to-end by learning their layout and parametrization. We find that end-to-end methods from prior work often learn inappropriate layouts or parametrizations that do not facilitate systematic generalization. Our results suggest that, in addition to modularity, systematic generalization in language understanding may require explicit regularizers or priors",
    "checked": null,
    "id": "6c7494a47cc5421a7b636c244e13586dc2dab007",
    "semantic_title": "systematic generalization: what is required and can it be learned?",
    "citation_count": 162,
    "authors": []
  },
  "https://openreview.net/forum?id=ByxZX20qFQ": {
    "title": "Adaptive Input Representations for Neural Language Modeling",
    "volume": "poster",
    "abstract": "We introduce adaptive input representations for neural language modeling which extend the adaptive softmax of Grave et al. (2017) to input representations of variable capacity. There are several choices on how to factorize the input and output layers, and whether to model words, characters or sub-word units. We perform a systematic comparison of popular choices for a self-attentional architecture. Our experiments show that models equipped with adaptive embeddings are more than twice as fast to train than the popular character input CNN while having a lower number of parameters. On the WikiText-103 benchmark we achieve 18.7 perplexity, an improvement of 10.5 perplexity compared to the previously best published result and on the Billion Word benchmark, we achieve 23.02 perplexity",
    "checked": true,
    "id": "d170bd486e4c0fe82601e322b0e9e0dde63ab299",
    "semantic_title": "adaptive input representations for neural language modeling",
    "citation_count": 390,
    "authors": []
  },
  "https://openreview.net/forum?id=H1MW72AcK7": {
    "title": "Optimal Control Via Neural Networks: A Convex Approach",
    "volume": "poster",
    "abstract": "Control of complex systems involves both system identification and controller design. Deep neural networks have proven to be successful in many identification tasks, however, from model-based control perspective, these networks are difficult to work with because they are typically nonlinear and nonconvex. Therefore many systems are still identified and controlled based on simple linear models despite their poor representation capability. In this paper we bridge the gap between model accuracy and control tractability faced by neural networks, by explicitly constructing networks that are convex with respect to their inputs. We show that these input convex networks can be trained to obtain accurate models of complex physical systems. In particular, we design input convex recurrent neural networks to capture temporal behavior of dynamical systems. Then optimal controllers can be achieved via solving a convex model predictive control problem. Experiment results demonstrate the good potential of the proposed input convex neural network based approach in a variety of control applications. In particular we show that in the MuJoCo locomotion tasks, we could achieve over 10% higher performance using 5 times less time compared with state-of-the-art model-based reinforcement learning method; and in the building HVAC control example, our method achieved up to 20% energy reduction compared with classic linear models",
    "checked": true,
    "id": "e20b0a274d62a94fb1259ce3f2166ecae4673a7e",
    "semantic_title": "optimal control via neural networks: a convex approach",
    "citation_count": 189,
    "authors": []
  },
  "https://openreview.net/forum?id=BJlxm30cKm": {
    "title": "An Empirical Study of Example Forgetting during Deep Neural Network Learning",
    "volume": "poster",
    "abstract": "Inspired by the phenomenon of catastrophic forgetting, we investigate the learning dynamics of neural networks as they train on single classification tasks. Our goal is to understand whether a related phenomenon occurs when data does not undergo a clear distributional shift. We define a ``forgetting event'' to have occurred when an individual training example transitions from being classified correctly to incorrectly over the course of learning. Across several benchmark data sets, we find that: (i) certain examples are forgotten with high frequency, and some not at all; (ii) a data set's (un)forgettable examples generalize across neural architectures; and (iii) based on forgetting dynamics, a significant fraction of examples can be omitted from the training data set while still maintaining state-of-the-art generalization performance",
    "checked": true,
    "id": "a2b5d224895d96bfe2e384e2dcf1ebd136ac3782",
    "semantic_title": "an empirical study of example forgetting during deep neural network learning",
    "citation_count": 743,
    "authors": []
  },
  "https://openreview.net/forum?id=rJ4km2R5t7": {
    "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    "volume": "poster",
    "abstract": "For natural language understanding (NLU) technology to be maximally useful, it must be able to process language in a way that is not exclusive to a single task, genre, or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation (GLUE) benchmark, a collection of tools for evaluating the performance of models across a diverse set of existing NLU tasks. By including tasks with limited training data, GLUE is designed to favor and encourage models that share general linguistic knowledge across tasks. GLUE also includes a hand-crafted diagnostic test suite that enables detailed linguistic analysis of models. We evaluate baselines based on current methods for transfer and representation learning and find that multi-task training on all tasks performs better than training a separate model per task. However, the low absolute performance of our best model indicates the need for improved general NLU systems",
    "checked": true,
    "id": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c",
    "semantic_title": "glue: a multi-task benchmark and analysis platform for natural language understanding",
    "citation_count": 7208,
    "authors": []
  },
  "https://openreview.net/forum?id=Byey7n05FQ": {
    "title": "Plan Online, Learn Offline: Efficient Learning and Exploration via Model-Based Control",
    "volume": "poster",
    "abstract": "We propose a \"plan online and learn offline\" framework for the setting where an agent, with an internal model, needs to continually act and learn in the world. Our work builds on the synergistic relationship between local model-based control, global value function learning, and exploration. We study how local trajectory optimization can cope with approximation errors in the value function, and can stabilize and accelerate value function learning. Conversely, we also study how approximate value functions can help reduce the planning horizon and allow for better policies beyond local solutions. Finally, we also demonstrate how trajectory optimization can be used to perform temporally coordinated exploration in conjunction with estimating uncertainty in value function approximation. This exploration is critical for fast and stable learning of the value function. Combining these components enable solutions to complex control tasks, like humanoid locomotion and dexterous in-hand manipulation, in the equivalent of a few minutes of experience in the real world",
    "checked": true,
    "id": "6a9013a8cdd84e423223f76a903028011c84c4ab",
    "semantic_title": "plan online, learn offline: efficient learning and exploration via model-based control",
    "citation_count": 229,
    "authors": []
  },
  "https://openreview.net/forum?id=Syx0Mh05YQ": {
    "title": "Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion",
    "volume": "poster",
    "abstract": "This paper proposes a representational model for grid cells. In this model, the 2D self-position of the agent is represented by a high-dimensional vector, and the 2D self-motion or displacement of the agent is represented by a matrix that transforms the vector. Each component of the vector is a unit or a cell. The model consists of the following three sub-models. (1) Vector-matrix multiplication. The movement from the current position to the next position is modeled by matrix-vector multi- plication, i.e., the vector of the next position is obtained by multiplying the matrix of the motion to the vector of the current position. (2) Magnified local isometry. The angle between two nearby vectors equals the Euclidean distance between the two corresponding positions multiplied by a magnifying factor. (3) Global adjacency kernel. The inner product between two vectors measures the adjacency between the two corresponding positions, which is defined by a kernel function of the Euclidean distance between the two positions. Our representational model has explicit algebra and geometry. It can learn hexagon patterns of grid cells, and it is capable of error correction, path integral and path planning",
    "checked": true,
    "id": "2abc9d603a87b300a251a3796a12b8a2d21746df",
    "semantic_title": "learning grid cells as vector representation of self-position coupled with matrix representation of self-motion",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=BJe0Gn0cY7": {
    "title": "Preventing Posterior Collapse with delta-VAEs",
    "volume": "poster",
    "abstract": "Due to the phenomenon of \"posterior collapse,\" current latent variable generative models pose a challenging design choice that either weakens the capacity of the decoder or requires altering the training objective. We develop an alternative that utilizes the most powerful generative models as decoders, optimize the variational lower bound, and ensures that the latent variables preserve and encode useful information. Our proposed δ-VAEs achieve this by constraining the variational family for the posterior to have a minimum distance to the prior. For sequential latent variable models, our approach resembles the classic representation learning approach of slow feature analysis. We demonstrate our method's efficacy at modeling text on LM1B and modeling images: learning representations, improving sample quality, and achieving state of the art log-likelihood on CIFAR-10 and ImageNet 32 × 32",
    "checked": true,
    "id": "097b7ad748e0f5fb8eeae9fec9ccb344801f87e1",
    "semantic_title": "preventing posterior collapse with delta-vaes",
    "citation_count": 171,
    "authors": []
  },
  "https://openreview.net/forum?id=HyxAfnA5tm": {
    "title": "Deep Online Learning Via Meta-Learning: Continual Adaptation for Model-Based RL",
    "volume": "poster",
    "abstract": "Humans and animals can learn complex predictive models that allow them to accurately and reliably reason about real-world phenomena, and they can adapt such models extremely quickly in the face of unexpected changes. Deep neural network models allow us to represent very complex functions, but lack this capacity for rapid online adaptation. The goal in this paper is to develop a method for continual online learning from an incoming stream of data, using deep neural network models. We formulate an online learning procedure that uses stochastic gradient descent to update model parameters, and an expectation maximization algorithm with a Chinese restaurant process prior to develop and maintain a mixture of models to handle non-stationary task distributions. This allows for all models to be adapted as necessary, with new models instantiated for task changes and old models recalled when previously seen tasks are encountered again. Furthermore, we observe that meta-learning can be used to meta-train a model such that this direct online adaptation with SGD is effective, which is otherwise not the case for large function approximators. We apply our method to model-based reinforcement learning, where adapting the predictive model is critical for control; we demonstrate that our online learning via meta-learning algorithm outperforms alternative prior methods, and enables effective continuous adaptation in non-stationary task distributions such as varying terrains, motor failures, and unexpected disturbances",
    "checked": true,
    "id": "42de54e614110c0c0a0bbbfee045e11e53eb4a7d",
    "semantic_title": "deep online learning via meta-learning: continual adaptation for model-based rl",
    "citation_count": 191,
    "authors": []
  },
  "https://openreview.net/forum?id=SJG6G2RqtX": {
    "title": "Value Propagation Networks",
    "volume": "poster",
    "abstract": "We present Value Propagation (VProp), a set of parameter-efficient differentiable planning modules built on Value Iteration which can successfully be trained using reinforcement learning to solve unseen tasks, has the capability to generalize to larger map sizes, and can learn to navigate in dynamic environments. We show that the modules enable learning to plan when the environment also includes stochastic elements, providing a cost-efficient learning system to build low-level size-invariant planners for a variety of interactive navigation problems. We evaluate on static and dynamic configurations of MazeBase grid-worlds, with randomly generated environments of several different sizes, and on a StarCraft navigation scenario, with more complex dynamics, and pixels as input",
    "checked": true,
    "id": "e2662442770457da538b90cb9a0d42782059ef8c",
    "semantic_title": "value propagation networks",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=Byxpfh0cFm": {
    "title": "Efficient Augmentation via Data Subsampling",
    "volume": "poster",
    "abstract": "Data augmentation is commonly used to encode invariances in learning methods. However, this process is often performed in an inefficient manner, as artificial examples are created by applying a number of transformations to all points in the training set. The resulting explosion of the dataset size can be an issue in terms of storage and training costs, as well as in selecting and tuning the optimal set of transformations to apply. In this work, we demonstrate that it is possible to significantly reduce the number of data points included in data augmentation while realizing the same accuracy and invariance benefits of augmenting the entire dataset. We propose a novel set of subsampling policies, based on model influence and loss, that can achieve a 90% reduction in augmentation set size while maintaining the accuracy gains of standard data augmentation",
    "checked": true,
    "id": "526ef02a3a4624f1dbd0f2ce51778eb92a893e8a",
    "semantic_title": "efficient augmentation via data subsampling",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=B1lnzn0ctQ": {
    "title": "ALISTA: Analytic Weights Are As Good As Learned Weights in LISTA",
    "volume": "poster",
    "abstract": "Deep neural networks based on unfolding an iterative algorithm, for example, LISTA (learned iterative shrinkage thresholding algorithm), have been an empirical success for sparse signal recovery. The weights of these neural networks are currently determined by data-driven \"black-box\" training. In this work, we propose Analytic LISTA (ALISTA), where the weight matrix in LISTA is computed as the solution to a data-free optimization problem, leaving only the stepsize and threshold parameters to data-driven learning. This signiﬁcantly simpliﬁes the training. Speciﬁcally, the data-free optimization problem is based on coherence minimization. We show our ALISTA retains the optimal linear convergence proved in (Chen et al., 2018) and has a performance comparable to LISTA. Furthermore, we extend ALISTA to convolutional linear operators, again determined in a data-free manner. We also propose a feed-forward framework that combines the data-free optimization and ALISTA networks from end to end, one that can be jointly trained to gain robustness to small perturbations in the encoding model",
    "checked": true,
    "id": "e15e022cb7874912313e2601d38b944fc93bbdd9",
    "semantic_title": "alista: analytic weights are as good as learned weights in lista",
    "citation_count": 178,
    "authors": []
  },
  "https://openreview.net/forum?id=HygsfnR9Ym": {
    "title": "Recall Traces: Backtracking Models for Efficient Reinforcement Learning",
    "volume": "poster",
    "abstract": "In many environments only a tiny subset of all states yield high reward. In these cases, few of the interactions with the environment provide a relevant learning signal. Hence, we may want to preferentially train on those high-reward states and the probable trajectories leading to them. To this end, we advocate for the use of a \\textit{backtracking model} that predicts the preceding states that terminate at a given high-reward state. We can train a model which, starting from a high value state (or one that is estimated to have high value), predicts and samples which (state, action)-tuples may have led to that high value state. These traces of (state, action) pairs, which we refer to as Recall Traces, sampled from this backtracking model starting from a high value state, are informative as they terminate in good states, and hence we can use these traces to improve a policy. We provide a variational interpretation for this idea and a practical algorithm in which the backtracking model samples from an approximate posterior distribution over trajectories which lead to large rewards. Our method improves the sample efficiency of both on- and off-policy RL algorithms across several environments and tasks",
    "checked": true,
    "id": "29aab768e642588352134a03c0368e1bdc1f1e8d",
    "semantic_title": "recall traces: backtracking models for efficient reinforcement learning",
    "citation_count": 68,
    "authors": []
  },
  "https://openreview.net/forum?id=H1gsz30cKX": {
    "title": "Fixup Initialization: Residual Learning Without Normalization",
    "volume": "poster",
    "abstract": "Normalization layers are a staple in state-of-the-art deep neural network architectures. They are widely believed to stabilize training, enable higher learning rate, accelerate convergence and improve generalization, though the reason for their effectiveness is still an active research topic. In this work, we challenge the commonly-held beliefs by showing that none of the perceived benefits is unique to normalization. Specifically, we propose fixed-update initialization (Fixup), an initialization motivated by solving the exploding and vanishing gradient problem at the beginning of training via properly rescaling a standard initialization. We find training residual networks with Fixup to be as stable as training with normalization -- even for networks with 10,000 layers. Furthermore, with proper regularization, Fixup enables residual networks without normalization to achieve state-of-the-art performance in image classification and machine translation",
    "checked": true,
    "id": "96c82727dd5a80fef93007f888bb8569feb6bd85",
    "semantic_title": "fixup initialization: residual learning without normalization",
    "citation_count": 351,
    "authors": []
  },
  "https://openreview.net/forum?id=rJliMh09F7": {
    "title": "Diversity-Sensitive Conditional Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "We propose a simple yet highly effective method that addresses the mode-collapse problem in the Conditional Generative Adversarial Network (cGAN). Although conditional distributions are multi-modal (i.e., having many modes) in practice, most cGAN approaches tend to learn an overly simplified distribution where an input is always mapped to a single output regardless of variations in latent code. To address such issue, we propose to explicitly regularize the generator to produce diverse outputs depending on latent codes. The proposed regularization is simple, general, and can be easily integrated into most conditional GAN objectives. Additionally, explicit regularization on generator allows our method to control a balance between visual quality and diversity. We demonstrate the effectiveness of our method on three conditional generation tasks: image-to-image translation, image inpainting, and future video prediction. We show that simple addition of our regularization to existing models leads to surprisingly diverse generations, substantially outperforming the previous approaches for multi-modal conditional generation specifically designed in each individual task",
    "checked": true,
    "id": "cc8a7a229151d17b1088d059fc471da2cea718c3",
    "semantic_title": "diversity-sensitive conditional generative adversarial networks",
    "citation_count": 216,
    "authors": []
  },
  "https://openreview.net/forum?id=S1lqMn05Ym": {
    "title": "Information asymmetry in KL-regularized RL",
    "volume": "poster",
    "abstract": "Many real world tasks exhibit rich structure that is repeated across different parts of the state space or in time. In this work we study the possibility of leveraging such repeated structure to speed up and regularize learning. We start from the KL regularized expected reward objective which introduces an additional component, a default policy. Instead of relying on a fixed default policy, we learn it from data. But crucially, we restrict the amount of information the default policy receives, forcing it to learn reusable behaviors that help the policy learn faster. We formalize this strategy and discuss connections to information bottleneck approaches and to the variational EM algorithm. We present empirical results in both discrete and continuous action domains and demonstrate that, for certain tasks, learning a default policy alongside the policy can significantly speed up and improve learning. Please watch the video demonstrating learned experts and default policies on several continuous control tasks ( https://youtu.be/U2qA3llzus8 )",
    "checked": true,
    "id": "549c9dfb32e85d9ef5a48566767be42ad132a3c4",
    "semantic_title": "information asymmetry in kl-regularized rl",
    "citation_count": 104,
    "authors": []
  },
  "https://openreview.net/forum?id=ByftGnR9KX": {
    "title": "FlowQA: Grasping Flow in History for Conversational Machine Comprehension",
    "volume": "poster",
    "abstract": "Conversational machine comprehension requires a deep understanding of the conversation history. To enable traditional, single-turn models to encode the history comprehensively, we introduce Flow, a mechanism that can incorporate intermediate representations generated during the process of answering previous questions, through an alternating parallel processing structure. Compared to shallow approaches that concatenate previous questions/answers as input, Flow integrates the latent semantics of the conversation history more deeply. Our model, FlowQA, shows superior performance on two recently proposed conversational challenges (+7.2% F1 on CoQA and +4.0% on QuAC). The effectiveness of Flow also shows in other tasks. By reducing sequential instruction understanding to conversational machine comprehension, FlowQA outperforms the best models on all three domains in SCONE, with +1.8% to +4.4% improvement in accuracy",
    "checked": true,
    "id": "9065ace5366ef548cf81bd9f239f1d132c1ef412",
    "semantic_title": "flowqa: grasping flow in history for conversational machine comprehension",
    "citation_count": 98,
    "authors": []
  },
  "https://openreview.net/forum?id=ByetGn0cYX": {
    "title": "Probabilistic Planning with Sequential Monte Carlo methods",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "c6b040f3f8215f32e362ca7a483c48070585b029",
    "semantic_title": "probabilistic planning with sequential monte carlo methods",
    "citation_count": 45,
    "authors": []
  },
  "https://openreview.net/forum?id=SkGuG2R5tm": {
    "title": "Spreading vectors for similarity search",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6ac386b9f77c3e4d84c06ec8b66475b1a6eada67",
    "semantic_title": "spreading vectors for similarity search",
    "citation_count": 121,
    "authors": []
  },
  "https://openreview.net/forum?id=BkedznAqKQ": {
    "title": "LanczosNet: Multi-Scale Deep Graph Convolutional Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5098df13be6d1f2a31c9fbf85703336ef77a9665",
    "semantic_title": "lanczosnet: multi-scale deep graph convolutional networks",
    "citation_count": 228,
    "authors": []
  },
  "https://openreview.net/forum?id=SylPMnR9Ym": {
    "title": "Learning what you can do before doing anything",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a53874f5c63b31468ad2fe3f5dea558a6ce35820",
    "semantic_title": "learning what you can do before doing anything",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=rylDfnCqF7": {
    "title": "Lagging Inference Networks and Posterior Collapse in Variational Autoencoders",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "159078136930f3963e01d694faa1b6b51f93c7ec",
    "semantic_title": "lagging inference networks and posterior collapse in variational autoencoders",
    "citation_count": 273,
    "authors": []
  },
  "https://openreview.net/forum?id=rkevMnRqYQ": {
    "title": "Preferences Implicit in the State of the World",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d35f9c78fc6d656d530aac2ed9f2aae6137b9041",
    "semantic_title": "preferences implicit in the state of the world",
    "citation_count": 55,
    "authors": []
  },
  "https://openreview.net/forum?id=S1lIMn05F7": {
    "title": "A Direct Approach to Robust Deep Learning Using Adversarial Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "04ee17ea05341aadc8643a21d21746cb67993a9d",
    "semantic_title": "a direct approach to robust deep learning using adversarial networks",
    "citation_count": 77,
    "authors": []
  },
  "https://openreview.net/forum?id=SyfIfnC5Ym": {
    "title": "Improving the Generalization of Adversarial Training with Domain Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "428c2e5992d6ed3186c087cba0fdba2ab6a468b2",
    "semantic_title": "improving the generalization of adversarial training with domain adaptation",
    "citation_count": 132,
    "authors": []
  },
  "https://openreview.net/forum?id=HklSf3CqKm": {
    "title": "Subgradient Descent Learns Orthogonal Dictionaries",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "15411d45ac13f432c3505c3ce825dfa00d88f8a6",
    "semantic_title": "subgradient descent learns orthogonal dictionaries",
    "citation_count": 51,
    "authors": []
  },
  "https://openreview.net/forum?id=HyGEM3C9KQ": {
    "title": "Improving Differentiable Neural Computers Through Memory Masking, De-allocation, and Link Distribution Sharpness Control",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d5535d4da15a7a8dfbeb34f61cddb4874bbc56e0",
    "semantic_title": "improving differentiable neural computers through memory masking, de-allocation, and link distribution sharpness control",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=r1eVMnA9K7": {
    "title": "Unsupervised Control Through Non-Parametric Discriminative Rewards",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "b65a6be07ce9c86797e6917258cf5ba45273ee73",
    "semantic_title": "unsupervised control through non-parametric discriminative rewards",
    "citation_count": 177,
    "authors": []
  },
  "https://openreview.net/forum?id=r1eEG20qKQ": {
    "title": "Self-Tuning Networks: Bilevel Optimization of Hyperparameters using Structured Best-Response Functions",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0ef6910cb25305a447aaa8c4be89ead9879a5016",
    "semantic_title": "self-tuning networks: bilevel optimization of hyperparameters using structured best-response functions",
    "citation_count": 164,
    "authors": []
  },
  "https://openreview.net/forum?id=B1MXz20cYQ": {
    "title": "Explaining Image Classifiers by Counterfactual Generation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f1173ca43481c1b33f4e7891ce77200e51eecba2",
    "semantic_title": "explaining image classifiers by counterfactual generation",
    "citation_count": 265,
    "authors": []
  },
  "https://openreview.net/forum?id=HJlQfnCqKX": {
    "title": "Predicting the Generalization Gap in Deep Networks with Margin Distributions",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4f409f21d04347db66e3cc09a32c4ef0d5b66094",
    "semantic_title": "predicting the generalization gap in deep networks with margin distributions",
    "citation_count": 199,
    "authors": []
  },
  "https://openreview.net/forum?id=HyN-M2Rctm": {
    "title": "Mode Normalization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b578b10ef5e271087200ef6182b9806c472c223d",
    "semantic_title": "mode normalization",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=r1GbfhRqF7": {
    "title": "Kernel Change-point Detection with Auxiliary Deep Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "fb327a4cc8a7e3de79362228b6904848d007e61d",
    "semantic_title": "kernel change-point detection with auxiliary deep generative models",
    "citation_count": 71,
    "authors": []
  },
  "https://openreview.net/forum?id=BJzbG20cFQ": {
    "title": "Towards Metamerism via Foveated Style Transfer",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7f3df93baabaf66bebbf7a24257f4ada252bc959",
    "semantic_title": "towards metamerism via foveated style transfer",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=BJxgz2R9t7": {
    "title": "Learning To Solve Circuit-SAT: An Unsupervised Differentiable Approach",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2b64300879d6fcaabe932e87ecb412066359b286",
    "semantic_title": "learning to solve circuit-sat: an unsupervised differentiable approach",
    "citation_count": 99,
    "authors": []
  },
  "https://openreview.net/forum?id=Hyg1G2AqtQ": {
    "title": "Variance Reduction for Reinforcement Learning in Input-Driven Environments",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c470f1d1e7d377b4e0d01890ae418a918c0e7093",
    "semantic_title": "variance reduction for reinforcement learning in input-driven environments",
    "citation_count": 95,
    "authors": []
  },
  "https://openreview.net/forum?id=SyxAb30cY7": {
    "title": "Robustness May Be at Odds with Accuracy",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1b9c6022598085dd892f360122c0fa4c630b3f18",
    "semantic_title": "robustness may be at odds with accuracy",
    "citation_count": 1786,
    "authors": []
  },
  "https://openreview.net/forum?id=H1g0Z3A9Fm": {
    "title": "Supervised Community Detection with Line Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "11222e4f5262c774bf9c006420eb647b951624b2",
    "semantic_title": "supervised community detection with line graph neural networks",
    "citation_count": 321,
    "authors": []
  },
  "https://openreview.net/forum?id=S1M6Z2Cctm": {
    "title": "Harmonic Unpaired Image-to-image Translation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c2f32f0ea55d287ea7dee6bc214e7b96fc2c2763",
    "semantic_title": "harmonic unpaired image-to-image translation",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=rklaWn0qK7": {
    "title": "Learning Neural PDE Solvers with Convergence Guarantees",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e2be7cd38737099a741184ad62587a3fd226db55",
    "semantic_title": "learning neural pde solvers with convergence guarantees",
    "citation_count": 145,
    "authors": []
  },
  "https://openreview.net/forum?id=HyxnZh0ct7": {
    "title": "Meta-learning with differentiable closed-form solvers",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "208cd4b25768f0096fb2e80e7690473da0e2a563",
    "semantic_title": "meta-learning with differentiable closed-form solvers",
    "citation_count": 931,
    "authors": []
  },
  "https://openreview.net/forum?id=S1lhbnRqF7": {
    "title": "Building Dynamic Knowledge Graphs from Text using Machine Reading Comprehension",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e8443666cf927806576ace54b9351af72a1d2d9b",
    "semantic_title": "building dynamic knowledge graphs from text using machine reading comprehension",
    "citation_count": 79,
    "authors": []
  },
  "https://openreview.net/forum?id=BkMiWhR5K7": {
    "title": "Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "10ab21b120e305b6d3cbf81c5a906d36521152f1",
    "semantic_title": "prior convictions: black-box adversarial attacks with bandits and priors",
    "citation_count": 375,
    "authors": []
  },
  "https://openreview.net/forum?id=Byf5-30qFX": {
    "title": "DHER: Hindsight Experience Replay for Dynamic Goals",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "154a19b77f2302d6b9bab354d3160ef527d29f55",
    "semantic_title": "dher: hindsight experience replay for dynamic goals",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=HJf9ZhC9FX": {
    "title": "Stochastic Gradient/Mirror Descent: Minimax Optimality and Implicit Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1d57045bfa02ac54df0481c1a977829b63564334",
    "semantic_title": "stochastic gradient/mirror descent: minimax optimality and implicit regularization",
    "citation_count": 64,
    "authors": []
  },
  "https://openreview.net/forum?id=H1lqZhRcFm": {
    "title": "Unsupervised Learning of the Set of Local Maxima",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "46d5cf70365d48ad38ed549fb44cf76810e8f8da",
    "semantic_title": "unsupervised learning of the set of local maxima",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=B1xY-hRctX": {
    "title": "Neural Logic Machines",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3a6447361b20c249f5306ae17dee43f645430e31",
    "semantic_title": "neural logic machines",
    "citation_count": 250,
    "authors": []
  },
  "https://openreview.net/forum?id=ryetZ20ctX": {
    "title": "Defensive Quantization: When Efficiency Meets Robustness",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ed1f55fb7ba0e3196913027840c4e23155e2e80c",
    "semantic_title": "defensive quantization: when efficiency meets robustness",
    "citation_count": 204,
    "authors": []
  },
  "https://openreview.net/forum?id=SygD-hCcF7": {
    "title": "Dimensionality Reduction for Representing the Knowledge of Probabilistic Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "effc2e95fb3ca77bba896439a1d9cf49f18f4af0",
    "semantic_title": "dimensionality reduction for representing the knowledge of probabilistic models",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=SygvZ209F7": {
    "title": "Biologically-Plausible Learning Algorithms Can Scale to Large Datasets",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2c26cca611ef1403ca81df238d04d0ef4a584a86",
    "semantic_title": "biologically-plausible learning algorithms can scale to large datasets",
    "citation_count": 76,
    "authors": []
  },
  "https://openreview.net/forum?id=rkxw-hAcFQ": {
    "title": "Generating Multi-Agent Trajectories using Programmatic Weak Supervision",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1dd5e6ea1cff5644f1fb80272d25ab37b3ef64bf",
    "semantic_title": "generating multi-agent trajectories using programmatic weak supervision",
    "citation_count": 90,
    "authors": []
  },
  "https://openreview.net/forum?id=H1gL-2A9Ym": {
    "title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank",
    "volume": "poster",
    "abstract": "Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online",
    "checked": true,
    "id": "ac225094aab9e7b629bc5b3343e026dea0200c70",
    "semantic_title": "predict then propagate: graph neural networks meet personalized pagerank",
    "citation_count": 1695,
    "authors": []
  },
  "https://openreview.net/forum?id=ryl8-3AcFX": {
    "title": "Environment Probing Interaction Policies",
    "volume": "poster",
    "abstract": "A key challenge in reinforcement learning (RL) is environment generalization: a policy trained to solve a task in one environment often fails to solve the same task in a slightly different test environment. A common approach to improve inter-environment transfer is to learn policies that are invariant to the distribution of testing environments. However, we argue that instead of being invariant, the policy should identify the specific nuances of an environment and exploit them to achieve better performance. In this work, we propose the \"Environment-Probing\" Interaction (EPI) policy, a policy that probes a new environment to extract an implicit understanding of that environment's behavior. Once this environment-specific information is obtained, it is used as an additional input to a task-specific policy that can now perform environment-conditioned actions to solve a task. To learn these EPI-policies, we present a reward function based on transition predictability. Specifically, a higher reward is given if the trajectory generated by the EPI-policy can be used to better predict transitions. We experimentally show that EPI-conditioned task-specific policies significantly outperform commonly used policy generalization methods on novel testing environments",
    "checked": true,
    "id": "633266814150ab66f0474d7b9a6807b729c7e0af",
    "semantic_title": "environment probing interaction policies",
    "citation_count": 67,
    "authors": []
  },
  "https://openreview.net/forum?id=SyzVb3CcFX": {
    "title": "Time-Agnostic Prediction: Predicting Predictable Video Frames",
    "volume": "poster",
    "abstract": "Prediction is arguably one of the most basic functions of an intelligent system. In general, the problem of predicting events in the future or between two waypoints is exceedingly difficult. However, most phenomena naturally pass through relatively predictable bottlenecks---while we cannot predict the precise trajectory of a robot arm between being at rest and holding an object up, we can be certain that it must have picked the object up. To exploit this, we decouple visual prediction from a rigid notion of time. While conventional approaches predict frames at regularly spaced temporal intervals, our time-agnostic predictors (TAP) are not tied to specific times so that they may instead discover predictable \"bottleneck\" frames no matter when they occur. We evaluate our approach for future and intermediate frame prediction across three robotic manipulation tasks. Our predictions are not only of higher visual quality, but also correspond to coherent semantic subgoals in temporally extended tasks",
    "checked": true,
    "id": "973e116b2b2949fdb8c533a8d84ddd811c0920cf",
    "semantic_title": "time-agnostic prediction: predicting predictable video frames",
    "citation_count": 94,
    "authors": []
  },
  "https://openreview.net/forum?id=BJg4Z3RqF7": {
    "title": "Unsupervised Adversarial Image Reconstruction",
    "volume": "poster",
    "abstract": "We address the problem of recovering an underlying signal from lossy, inaccurate observations in an unsupervised setting. Typically, we consider situations where there is little to no background knowledge on the structure of the underlying signal, no access to signal-measurement pairs, nor even unpaired signal-measurement data. The only available information is provided by the observations and the measurement process statistics. We cast the problem as finding the \\textit{maximum a posteriori} estimate of the signal given each measurement, and propose a general framework for the reconstruction problem. We use a formulation of generative adversarial networks, where the generator takes as input a corrupted observation in order to produce realistic reconstructions, and add a penalty term tying the reconstruction to the associated observation. We evaluate our reconstructions on several image datasets with different types of corruptions. The proposed approach yields better results than alternative baselines, and comparable performance with model variants trained with additional supervision",
    "checked": true,
    "id": "59ae0a50c5ebc8aa9c408e9d75f56c195ac7276e",
    "semantic_title": "unsupervised adversarial image reconstruction",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=rylV-2C9KQ": {
    "title": "Deep Decoder: Concise Image Representations from Untrained Non-convolutional Networks",
    "volume": "poster",
    "abstract": "Deep neural networks, in particular convolutional neural networks, have become highly effective tools for compressing images and solving inverse problems including denoising, inpainting, and reconstruction from few and noisy measurements. This success can be attributed in part to their ability to represent and generate natural images well. Contrary to classical tools such as wavelets, image-generating deep neural networks have a large number of parameters---typically a multiple of their output dimension---and need to be trained on large datasets. In this paper, we propose an untrained simple image model, called the deep decoder, which is a deep neural network that can generate natural images from very few weight parameters. The deep decoder has a simple architecture with no convolutions and fewer weight parameters than the output dimensionality. This underparameterization enables the deep decoder to compress images into a concise set of network weights, which we show is on par with wavelet-based thresholding. Further, underparameterization provides a barrier to overfitting, allowing the deep decoder to have state-of-the-art performance for denoising. The deep decoder is simple in the sense that each layer has an identical structure that consists of only one upsampling unit, pixel-wise linear combination of channels, ReLU activation, and channelwise normalization. This simplicity makes the network amenable to theoretical analysis, and it sheds light on the aspects of neural networks that enable them to form effective signal representations",
    "checked": true,
    "id": "f73802729a2fad366fecc9ed6221d2aa2a8a6f38",
    "semantic_title": "deep decoder: concise image representations from untrained non-convolutional networks",
    "citation_count": 287,
    "authors": []
  },
  "https://openreview.net/forum?id=S1xNb2A9YX": {
    "title": "Minimal Images in Deep Neural Networks: Fragile Object Recognition in Natural Images",
    "volume": "poster",
    "abstract": "The human ability to recognize objects is impaired when the object is not shown in full. \"Minimal images\" are the smallest regions of an image that remain recognizable for humans. Ullman et al. (2016) show that a slight modification of the location and size of the visible region of the minimal image produces a sharp drop in human recognition accuracy. In this paper, we demonstrate that such drops in accuracy due to changes of the visible region are a common phenomenon between humans and existing state-of-the-art deep neural networks (DNNs), and are much more prominent in DNNs. We found many cases where DNNs classified one region correctly and the other incorrectly, though they only differed by one row or column of pixels, and were often bigger than the average human minimal image size. We show that this phenomenon is independent from previous works that have reported lack of invariance to minor modifications in object location in DNNs. Our results thus reveal a new failure mode of DNNs that also affects humans to a much lesser degree. They expose how fragile DNN recognition ability is in natural images even without adversarial patterns being introduced. Bringing the robustness of DNNs in natural images to the human level remains an open challenge for the community",
    "checked": true,
    "id": "3b523a15853e99dafc16962827e6e6f467d41a03",
    "semantic_title": "minimal images in deep neural networks: fragile object recognition in natural images",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=Hkg4W2AcFm": {
    "title": "Overcoming the Disentanglement vs Reconstruction Trade-off via Jacobian Supervision",
    "volume": "poster",
    "abstract": "A major challenge in learning image representations is the disentangling of the factors of variation underlying the image formation. This is typically achieved with an autoencoder architecture where a subset of the latent variables is constrained to correspond to specific factors, and the rest of them are considered nuisance variables. This approach has an important drawback: as the dimension of the nuisance variables is increased, image reconstruction is improved, but the decoder has the flexibility to ignore the specified factors, thus losing the ability to condition the output on them. In this work, we propose to overcome this trade-off by progressively growing the dimension of the latent code, while constraining the Jacobian of the output image with respect to the disentangled variables to remain the same. As a result, the obtained models are effective at both disentangling and reconstruction. We demonstrate the applicability of this method in both unsupervised and supervised scenarios for learning disentangled representations. In a facial attribute manipulation task, we obtain high quality image generation while smoothly controlling dozens of attributes with a single model. This is an order of magnitude more disentangled factors than state-of-the-art methods, while obtaining visually similar or superior results, and avoiding adversarial training",
    "checked": true,
    "id": "aee543a4859821b750b31cc03a0c39f9b4daacc0",
    "semantic_title": "overcoming the disentanglement vs reconstruction trade-off via jacobian supervision",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=H1l7bnR5Ym": {
    "title": "ProbGAN: Towards Probabilistic GAN with Theoretical Guarantees",
    "volume": "poster",
    "abstract": "Probabilistic modelling is a principled framework to perform model aggregation, which has been a primary mechanism to combat mode collapse in the context of Generative Adversarial Networks (GAN). In this paper, we propose a novel probabilistic framework for GANs, ProbGAN, which iteratively learns a distribution over generators with a carefully crafted prior. Learning is efficiently triggered by a tailored stochastic gradient Hamiltonian Monte Carlo with a novel gradient approximation to perform Bayesian inference. Our theoretical analysis further reveals that our treatment is the first probabilistic framework that yields an equilibrium where generator distributions are faithful to the data distribution. Empirical evidence on synthetic high-dimensional multi-modal data and image databases (CIFAR-10, STL-10, and ImageNet) demonstrates the superiority of our method over both start-of-the-art multi-generator GANs and other probabilistic treatment for GANs",
    "checked": true,
    "id": "99c870a6d94d1295a6ed1d004235cb4f8f676fbe",
    "semantic_title": "probgan: towards probabilistic gan with theoretical guarantees",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=rkxQ-nA9FX": {
    "title": "Theoretical Analysis of Auto Rate-Tuning by Batch Normalization",
    "volume": "poster",
    "abstract": "Batch Normalization (BN) has become a cornerstone of deep learning across diverse architectures, appearing to help optimization as well as generalization. While the idea makes intuitive sense, theoretical analysis of its effectiveness has been lacking. Here theoretical support is provided for one of its conjectured properties, namely, the ability to allow gradient descent to succeed with less tuning of learning rates. It is shown that even if we fix the learning rate of scale-invariant parameters (e.g., weights of each layer with BN) to a constant (say, 0.3), gradient descent still approaches a stationary point (i.e., a solution where gradient is zero) in the rate of T^{−1/2} in T iterations, asymptotically matching the best bound for gradient descent with well-tuned learning rates. A similar result with convergence rate T^{−1/4} is also shown for stochastic gradient descent",
    "checked": true,
    "id": "3dfb0a18ab5a5413c50d911e49b3c83b1a9383a3",
    "semantic_title": "theoretical analysis of auto rate-tuning by batch normalization",
    "citation_count": 131,
    "authors": []
  },
  "https://openreview.net/forum?id=B1lz-3Rct7": {
    "title": "Three Mechanisms of Weight Decay Regularization",
    "volume": "poster",
    "abstract": "Weight decay is one of the standard tricks in the neural network toolbox, but the reasons for its regularization effect are poorly understood, and recent results have cast doubt on the traditional interpretation in terms of $L_2$ regularization. Literal weight decay has been shown to outperform $L_2$ regularization for optimizers for which they differ. We empirically investigate weight decay for three optimization algorithms (SGD, Adam, and K-FAC) and a variety of network architectures. We identify three distinct mechanisms by which weight decay exerts a regularization effect, depending on the particular optimization algorithm and architecture: (1) increasing the effective learning rate, (2) approximately regularizing the input-output Jacobian norm, and (3) reducing the effective damping coefficient for second-order optimization. Our results provide insight into how to improve the regularization of neural networks",
    "checked": true,
    "id": "ba618ec05a9dbef75310c5e4bcce8a559e0270b5",
    "semantic_title": "three mechanisms of weight decay regularization",
    "citation_count": 259,
    "authors": []
  },
  "https://openreview.net/forum?id=SkfMWhAqYQ": {
    "title": "Approximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet",
    "volume": "poster",
    "abstract": "Deep Neural Networks (DNNs) excel on many complex perceptual tasks but it has proven notoriously difficult to understand how they reach their decisions. We here introduce a high-performance DNN architecture on ImageNet whose decisions are considerably easier to explain. Our model, a simple variant of the ResNet-50 architecture called BagNet, classifies an image based on the occurrences of small local image features without taking into account their spatial ordering. This strategy is closely related to the bag-of-feature (BoF) models popular before the onset of deep learning and reaches a surprisingly high accuracy on ImageNet (87.6% top-5 for 32 x 32 px features and Alexnet performance for 16 x16 px features). The constraint on local features makes it straight-forward to analyse how exactly each part of the image influences the classification. Furthermore, the BagNets behave similar to state-of-the art deep neural networks such as VGG-16, ResNet-152 or DenseNet-169 in terms of feature sensitivity, error distribution and interactions between image parts. This suggests that the improvements of DNNs over previous bag-of-feature classifiers in the last few years is mostly achieved by better fine-tuning rather than by qualitatively different decision strategies",
    "checked": true,
    "id": "810ae452a3a1f673ea241bd540f9551b2996ed5b",
    "semantic_title": "approximating cnns with bag-of-local-features models works surprisingly well on imagenet",
    "citation_count": 561,
    "authors": []
  },
  "https://openreview.net/forum?id=SyMWn05F7": {
    "title": "Learning Exploration Policies for Navigation",
    "volume": "poster",
    "abstract": "Numerous past works have tackled the problem of task-driven navigation. But, how to effectively explore a new environment to enable a variety of down-stream tasks has received much less attention. In this work, we study how agents can autonomously explore realistic and complex 3D environments without the context of task-rewards. We propose a learning-based approach and investigate different policy architectures, reward functions, and training paradigms. We find that use of policies with spatial memory that are bootstrapped with imitation learning and finally finetuned with coverage rewards derived purely from on-board sensors can be effective at exploring novel environments. We show that our learned exploration policies can explore better than classical approaches based on geometry alone and generic learning-based exploration techniques. Finally, we also show how such task-agnostic exploration can be used for down-stream tasks. Videos are available at https://sites.google.com/view/exploration-for-nav/",
    "checked": true,
    "id": "48182d7620a9278d7e9cd880a961fa14d22a0281",
    "semantic_title": "learning exploration policies for navigation",
    "citation_count": 239,
    "authors": []
  },
  "https://openreview.net/forum?id=SJggZnRcFQ": {
    "title": "Learning Programmatically Structured Representations with Perceptor Gradients",
    "volume": "poster",
    "abstract": "We present the perceptor gradients algorithm -- a novel approach to learning symbolic representations based on the idea of decomposing an agent's policy into i) a perceptor network extracting symbols from raw observation data and ii) a task encoding program which maps the input symbols to output actions. We show that the proposed algorithm is able to learn representations that can be directly fed into a Linear-Quadratic Regulator (LQR) or a general purpose A* planner. Our experimental results confirm that the perceptor gradients algorithm is able to efficiently learn transferable symbolic representations as well as generate new observations according to a semantically meaningful specification",
    "checked": true,
    "id": "7e2d24fc41325e19f175dc9dc35db76d56402a72",
    "semantic_title": "learning programmatically structured representations with perceptor gradients",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=HJxeWnCcF7": {
    "title": "Learning Mixed-Curvature Representations in Product Spaces",
    "volume": "poster",
    "abstract": "The quality of the representations achieved by embeddings is determined by how well the geometry of the embedding space matches the structure of the data. Euclidean space has been the workhorse for embeddings; recently hyperbolic and spherical spaces have gained popularity due to their ability to better embed new types of structured data---such as hierarchical data---but most data is not structured so uniformly. We address this problem by proposing learning embeddings in a product manifold combining multiple copies of these model spaces (spherical, hyperbolic, Euclidean), providing a space of heterogeneous curvature suitable for a wide variety of structures. We introduce a heuristic to estimate the sectional curvature of graph data and directly determine an appropriate signature---the number of component spaces and their dimensions---of the product manifold. Empirically, we jointly learn the curvature and the embedding in the product space via Riemannian optimization. We discuss how to define and compute intrinsic quantities such as means---a challenging notion for product manifolds---and provably learnable optimization functions. On a range of datasets and reconstruction tasks, our product space embeddings outperform single Euclidean or hyperbolic spaces used in previous works, reducing distortion by 32.55% on a Facebook social network dataset. We learn word embeddings and find that a product of hyperbolic spaces in 50 dimensions consistently improves on baseline Euclidean and hyperbolic embeddings, by 2.6 points in Spearman rank correlation on similarity tasks and 3.4 points on analogy accuracy",
    "checked": true,
    "id": "779ad52e8c27b77c10d14d536133da61c2c1f9b2",
    "semantic_title": "learning mixed-curvature representations in product spaces",
    "citation_count": 201,
    "authors": []
  },
  "https://openreview.net/forum?id=Hygxb2CqKm": {
    "title": "Stable Recurrent Models",
    "volume": "poster",
    "abstract": "Stability is a fundamental property of dynamical systems, yet to this date it has had little bearing on the practice of recurrent neural networks. In this work, we conduct a thorough investigation of stable recurrent models. Theoretically, we prove stable recurrent neural networks are well approximated by feed-forward networks for the purpose of both inference and training by gradient descent. Empirically, we demonstrate stable recurrent models often perform as well as their unstable counterparts on benchmark sequence tasks. Taken together, these findings shed light on the effective power of recurrent networks and suggest much of sequence learning happens, or can be made to happen, in the stable regime. Moreover, our results help to explain why in many cases practitioners succeed in replacing recurrent models by feed-forward models",
    "checked": true,
    "id": "e77099681374e940ea45821fd7e406394721552f",
    "semantic_title": "stable recurrent models",
    "citation_count": 119,
    "authors": []
  },
  "https://openreview.net/forum?id=HyxCxhRcY7": {
    "title": "Deep Anomaly Detection with Outlier Exposure",
    "volume": "poster",
    "abstract": "It is important to detect anomalous inputs when deploying machine learning systems. The use of larger and more complex inputs in deep learning magnifies the difficulty of distinguishing between anomalous and in-distribution examples. At the same time, diverse image and text data are available in enormous quantities. We propose leveraging these data to improve deep anomaly detection by training anomaly detectors against an auxiliary dataset of outliers, an approach we call Outlier Exposure (OE). This enables anomaly detectors to generalize and detect unseen anomalies. In extensive experiments on natural language processing and small- and large-scale vision tasks, we find that Outlier Exposure significantly improves detection performance. We also observe that cutting-edge generative models trained on CIFAR-10 may assign higher likelihoods to SVHN images than to CIFAR-10 images; we use OE to mitigate this issue. We also analyze the flexibility and robustness of Outlier Exposure, and identify characteristics of the auxiliary dataset that improve performance",
    "checked": null,
    "id": "2d8c97db4bae00ff243d122b957091a236a697a7",
    "semantic_title": "deep anomaly detection with outlier exposure",
    "citation_count": 1475,
    "authors": []
  },
  "https://openreview.net/forum?id=S1lTg3RqYQ": {
    "title": "Exemplar Guided Unsupervised Image-to-Image Translation with Semantic Consistency",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "7242666492464a5408fc072f5c4f51502e0d0e5b",
    "semantic_title": "exemplar guided unsupervised image-to-image translation",
    "citation_count": 141,
    "authors": []
  },
  "https://openreview.net/forum?id=HkG3e205K7": {
    "title": "Doubly Reparameterized Gradient Estimators for Monte Carlo Objectives",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d8778f909eb027bcb7c5122bca23acb680ded647",
    "semantic_title": "doubly reparameterized gradient estimators for monte carlo objectives",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=Bylnx209YX": {
    "title": "Adversarial Attacks on Graph Neural Networks via Meta Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6f5b1076ebacd30849d86e5f5787e3d43b65911f",
    "semantic_title": "adversarial attacks on graph neural networks via meta learning",
    "citation_count": 574,
    "authors": []
  },
  "https://openreview.net/forum?id=Bkg3g2R9FX": {
    "title": "Adaptive Gradient Methods with Dynamic Bound of Learning Rate",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "03af562fb8e69677865dbe94910e464443dd4623",
    "semantic_title": "adaptive gradient methods with dynamic bound of learning rate",
    "citation_count": 602,
    "authors": []
  },
  "https://openreview.net/forum?id=HyGcghRct7": {
    "title": "Random mesh projectors for inverse problems",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e2c18e8f9c808dd5ccc8df752da6efe69619fe34",
    "semantic_title": "random mesh projectors for inverse problems",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=S1xcx3C5FX": {
    "title": "A Statistical Approach to Assessing Neural Network Robustness",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "85df5b9fcd85c41ec0e1eb6c1ab15b8d7147c885",
    "semantic_title": "a statistical approach to assessing neural network robustness",
    "citation_count": 83,
    "authors": []
  },
  "https://openreview.net/forum?id=Hye9lnCct7": {
    "title": "Learning Actionable Representations with Goal Conditioned Policies",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "c46d80f83813fba0e8363a0ab36a19fba062540e",
    "semantic_title": "learning actionable representations with goal-conditioned policies",
    "citation_count": 110,
    "authors": []
  },
  "https://openreview.net/forum?id=rJgYxn09Fm": {
    "title": "Learning Implicitly Recurrent CNNs Through Parameter Sharing",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "359cdea86e4203f73de4c12356fd9139dba9a745",
    "semantic_title": "learning implicitly recurrent cnns through parameter sharing",
    "citation_count": 70,
    "authors": []
  },
  "https://openreview.net/forum?id=ByldlhAqYQ": {
    "title": "Transfer Learning for Sequences via Learning to Collocate",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3b81ba581733e0c1e9c76b317645d2ee973fb710",
    "semantic_title": "transfer learning for sequences via learning to collocate",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=HJGven05Y7": {
    "title": "How to train your MAML",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3e2e315e62f2d113a200a0d97b1b11d8f4e235b6",
    "semantic_title": "how to train your maml",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HyxPx3R9tm": {
    "title": "Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4b59846404c085f5c9523c69cd790537613a3df5",
    "semantic_title": "variational discriminator bottleneck: improving imitation learning, inverse rl, and gans by constraining information flow",
    "citation_count": 216,
    "authors": []
  },
  "https://openreview.net/forum?id=BJgLg3R9KQ": {
    "title": "Learning what and where to attend",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "136c96810238657bf0c6f0d4b56b0e40e24f3c47",
    "semantic_title": "learning what and where to attend",
    "citation_count": 141,
    "authors": []
  },
  "https://openreview.net/forum?id=SygLehCqtm": {
    "title": "Learning protein sequence embeddings using information from structure",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3637fcccc758786ae1c6529ab22fe85ea98e9c36",
    "semantic_title": "learning protein sequence embeddings using information from structure",
    "citation_count": 290,
    "authors": []
  },
  "https://openreview.net/forum?id=SJzSgnRcKX": {
    "title": "What do you learn from context? Probing for sentence structure in contextualized word representations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e2587eddd57bc4ba286d91b27c185083f16f40ee",
    "semantic_title": "what do you learn from context? probing for sentence structure in contextualized word representations",
    "citation_count": 866,
    "authors": []
  },
  "https://openreview.net/forum?id=rye4g3AqFm": {
    "title": "Deep learning generalizes because the parameter-function map is biased towards simple functions",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7a327286d3549be1607c1ce153b041b08be151f8",
    "semantic_title": "deep learning generalizes because the parameter-function map is biased towards simple functions",
    "citation_count": 232,
    "authors": []
  },
  "https://openreview.net/forum?id=SJgEl3A5tm": {
    "title": "CAMOU: Learning Physical Vehicle Camouflages to Adversarially Attack Detectors in the Wild",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2ee77a5fa75717118b62833f4cf111cfd0293aec",
    "semantic_title": "camou: learning physical vehicle camouflages to adversarially attack detectors in the wild",
    "citation_count": 104,
    "authors": []
  },
  "https://openreview.net/forum?id=Hke4l2AcKQ": {
    "title": "MAE: Mutual Posterior-Divergence Regularization for Variational AutoEncoders",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c6d2a216fae7043e9251e5e87141482a1ffc68cd",
    "semantic_title": "mae: mutual posterior-divergence regularization for variational autoencoders",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=SkMQg3C5K7": {
    "title": "A Convergence Analysis of Gradient Descent for Deep Linear Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "134c165953e23a6dc7d4f0d86989e92362ca4335",
    "semantic_title": "a convergence analysis of gradient descent for deep linear neural networks",
    "citation_count": 293,
    "authors": []
  },
  "https://openreview.net/forum?id=SkxXg2C5FX": {
    "title": "Don't Settle for Average, Go for the Max: Fuzzy Sets and Max-Pooled Word Vectors",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "117b08d70961242283afdb412d9ab9004262d9d5",
    "semantic_title": "don't settle for average, go for the max: fuzzy sets and max-pooled word vectors",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=BygfghAcYX": {
    "title": "The role of over-parametrization in generalization of neural networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6e8001afb2e24b648ae9ceb4e00c20ded4a5ee99",
    "semantic_title": "the role of over-parametrization in generalization of neural networks",
    "citation_count": 190,
    "authors": []
  },
  "https://openreview.net/forum?id=H1x-x309tm": {
    "title": "On the Convergence of A Class of Adam-Type Algorithms for Non-Convex Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c215b9ac79f07c8a43782b224f4416943837ffa8",
    "semantic_title": "on the convergence of a class of adam-type algorithms for non-convex optimization",
    "citation_count": 324,
    "authors": []
  },
  "https://openreview.net/forum?id=HJflg30qKX": {
    "title": "Gradient descent aligns the layers of deep linear networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5786917220aab2f6d0b00606eee9fe0ad0700f1b",
    "semantic_title": "gradient descent aligns the layers of deep linear networks",
    "citation_count": 257,
    "authors": []
  },
  "https://openreview.net/forum?id=SJz1x20cFQ": {
    "title": "Hierarchical RL Using an Ensemble of Proprioceptive Periodic Policies",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "622cd0cc96d437481d8ca1de5ea5400e655efcc3",
    "semantic_title": "hierarchical rl using an ensemble of proprioceptive periodic policies",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=HJgkx2Aqt7": {
    "title": "Learning To Simulate",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "73b226ea356f14e6751f16871da2e357a7989990",
    "semantic_title": "learning to simulate",
    "citation_count": 119,
    "authors": []
  },
  "https://openreview.net/forum?id=Skeke3C5Fm": {
    "title": "Multilingual Neural Machine Translation With Soft Decoupled Encoding",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "be312e930f6739a709e60547aa0dfb9c3dc44497",
    "semantic_title": "multilingual neural machine translation with soft decoupled encoding",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=BJgklhAcK7": {
    "title": "Meta-Learning with Latent Embedding Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "04f739a0c29b75877243731aeead512bf0ed1dff",
    "semantic_title": "meta-learning with latent embedding optimization",
    "citation_count": 1374,
    "authors": []
  },
  "https://openreview.net/forum?id=HJeRkh05Km": {
    "title": "Visual Semantic Navigation using Scene Priors",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c7aea4b653d4e12cb47438960f5689f5f835e073",
    "semantic_title": "visual semantic navigation using scene priors",
    "citation_count": 325,
    "authors": []
  },
  "https://openreview.net/forum?id=HkgTkhRcKQ": {
    "title": "AdaShift: Decorrelation and Convergence of Adaptive Learning Rate Methods",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9f88722cbc4107e3c3d0e1c7934cc7f1d5ae4fdb",
    "semantic_title": "adashift: decorrelation and convergence of adaptive learning rate methods",
    "citation_count": 66,
    "authors": []
  },
  "https://openreview.net/forum?id=H1xaJn05FQ": {
    "title": "Sliced Wasserstein Auto-Encoders",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "da798da448db9dd8707b55efb856b5c7ad5b6c00",
    "semantic_title": "sliced wasserstein auto-encoders",
    "citation_count": 164,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgpy3C5tX": {
    "title": "Amortized Bayesian Meta-Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2686aef553af3ebd43cb0ea98a538a9435f7dd5f",
    "semantic_title": "amortized bayesian meta-learning",
    "citation_count": 137,
    "authors": []
  },
  "https://openreview.net/forum?id=S1g2JnRcFX": {
    "title": "Local SGD Converges Fast and Communicates Little",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7cfa76a82be96c74b2eff514265b7fd271a179cd",
    "semantic_title": "local sgd converges fast and communicates little",
    "citation_count": 1070,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgoyn09KQ": {
    "title": "textTOvec: DEEP CONTEXTUALIZED NEURAL AUTOREGRESSIVE TOPIC MODELS OF LANGUAGE WITH DISTRIBUTED COMPOSITIONAL PRIOR",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "ed01746601d9cb7724568645d8b53876f6cd7c16",
    "semantic_title": "texttovec: deep contextualized neural autoregressive models of language with distributed compositional prior",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=ByloJ20qtm": {
    "title": "Neural Program Repair by Jointly Learning to Localize and Repair",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "564bce85c8ad9a50f4652a4d05e1ed0aaa22df49",
    "semantic_title": "neural program repair by jointly learning to localize and repair",
    "citation_count": 132,
    "authors": []
  },
  "https://openreview.net/forum?id=ryl5khRcKm": {
    "title": "Human-level Protein Localization with Convolutional Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6a65652fd2735d546281dd54c912d6eeaa17546c",
    "semantic_title": "human-level protein localization with convolutional neural networks",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=r1lq1hRqYQ": {
    "title": "From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "758311575a6385bb15d4f9af8c0e671cb98184b4",
    "semantic_title": "from language to goals: inverse reinforcement learning for vision-based instruction following",
    "citation_count": 125,
    "authors": []
  },
  "https://openreview.net/forum?id=HklY120cYm": {
    "title": "ClariNet: Parallel Wave Generation in End-to-End Text-to-Speech",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5e3a59695261f03aa3f09a8a5ac6166fb63e0a2e",
    "semantic_title": "clarinet: parallel wave generation in end-to-end text-to-speech",
    "citation_count": 347,
    "authors": []
  },
  "https://openreview.net/forum?id=SyxtJh0qYm": {
    "title": "Variational Autoencoder with Arbitrary Conditioning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c61b43acd00d73e58fade68b8eb7c3ae875fc60c",
    "semantic_title": "variational autoencoder with arbitrary conditioning",
    "citation_count": 147,
    "authors": []
  },
  "https://openreview.net/forum?id=BJluy2RcFm": {
    "title": "Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "de9550945b2f631c541c299114a770c4f47f9616",
    "semantic_title": "janossy pooling: learning deep permutation-invariant functions for variable-size inputs",
    "citation_count": 193,
    "authors": []
  },
  "https://openreview.net/forum?id=ByGuynAct7": {
    "title": "The Deep Weight Prior",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "cf9d672680312c086b2277dbaa04bd37cb295d6e",
    "semantic_title": "the deep weight prior",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=rkluJ2R9KQ": {
    "title": "A new dog learns old tricks: RL finds classic optimization algorithms",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0029ccaa0047d8af36e34bf555c33cd09ce4cb31",
    "semantic_title": "a new dog learns old tricks: rl finds classic optimization algorithms",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=HJgd1nAqFX": {
    "title": "DOM-Q-NET: Grounded RL on Structured Language",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d7d5406eabff0e0e7fc5f7d8706297d85446e4b2",
    "semantic_title": "dom-q-net: grounded rl on structured language",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=ryxwJhC9YX": {
    "title": "InstaGAN: Instance-aware Image-to-Image Translation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "af7994e85e6900c0eaf48a5d80fbf5c127f58644",
    "semantic_title": "instagan: instance-aware image-to-image translation",
    "citation_count": 157,
    "authors": []
  },
  "https://openreview.net/forum?id=SyNPk2R9K7": {
    "title": "Learning to Describe Scenes with Programs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "58dcc24c63ed179d9a9b458d5f1284a7a297c5d6",
    "semantic_title": "learning to describe scenes with programs",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=HJfwJ2A5KX": {
    "title": "Data-Dependent Coresets for Compressing Neural Networks with Applications to Generalization Bounds",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f61fc6c49d3b22f297b7ccb72bbfe6f7c1344e78",
    "semantic_title": "data-dependent coresets for compressing neural networks with applications to generalization bounds",
    "citation_count": 79,
    "authors": []
  },
  "https://openreview.net/forum?id=rJg8yhAqKm": {
    "title": "InfoBot: Transfer and Exploration via the Information Bottleneck",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bf7f1ada5feecc0992f71b39c1ebeccb19ae631b",
    "semantic_title": "infobot: transfer and exploration via the information bottleneck",
    "citation_count": 167,
    "authors": []
  },
  "https://openreview.net/forum?id=BylE1205Fm": {
    "title": "Emerging Disentanglement in Auto-Encoder Based Unsupervised Image Content Transfer",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "98f0f57e5c9425c81e24618b00b364bbcb61338b",
    "semantic_title": "emerging disentanglement in auto-encoder based unsupervised image content transfer",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=rJg4J3CqFm": {
    "title": "Learning Embeddings into Entropic Wasserstein Spaces",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8fec21b4553cb8eb72345aff30b35bcffd456378",
    "semantic_title": "learning embeddings into entropic wasserstein spaces",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=H1g4k309F7": {
    "title": "Wasserstein Barycenter Model Ensembling",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3c50284257e0180133d5d71da20bd1c7e089b85c",
    "semantic_title": "wasserstein barycenter model ensembling",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=Hyx4knR9Ym": {
    "title": "Generalizable Adversarial Training via Spectral Normalization",
    "volume": "poster",
    "abstract": "Deep neural networks (DNNs) have set benchmarks on a wide array of supervised learning tasks. Trained DNNs, however, often lack robustness to minor adversarial perturbations to the input, which undermines their true practicality. Recent works have increased the robustness of DNNs by fitting networks using adversarially-perturbed training samples, but the improved performance can still be far below the performance seen in non-adversarial settings. A significant portion of this gap can be attributed to the decrease in generalization performance due to adversarial training. In this work, we extend the notion of margin loss to adversarial settings and bound the generalization error for DNNs trained under several well-known gradient-based attack schemes, motivating an effective regularization scheme based on spectral normalization of the DNN's weight matrices. We also provide a computationally-efficient method for normalizing the spectral norm of convolutional layers with arbitrary stride and padding schemes in deep convolutional networks. We evaluate the power of spectral normalization extensively on combinations of datasets, network architectures, and adversarial training schemes",
    "checked": true,
    "id": "3a606480406886742572a956e221e986c65d94c1",
    "semantic_title": "generalizable adversarial training via spectral normalization",
    "citation_count": 140,
    "authors": []
  },
  "https://openreview.net/forum?id=Bylmkh05KX": {
    "title": "Unsupervised Speech Recognition via Segmental Empirical Output Distribution Matching",
    "volume": "poster",
    "abstract": "We consider the problem of training speech recognition systems without using any labeled data, under the assumption that the learner can only access to the input utterances and a phoneme language model estimated from a non-overlapping corpus. We propose a fully unsupervised learning algorithm that alternates between solving two sub-problems: (i) learn a phoneme classifier for a given set of phoneme segmentation boundaries, and (ii) refining the phoneme boundaries based on a given classifier. To solve the first sub-problem, we introduce a novel unsupervised cost function named Segmental Empirical Output Distribution Matching, which generalizes the work in (Liu et al., 2017) to segmental structures. For the second sub-problem, we develop an approximate MAP approach to refining the boundaries obtained from Wang et al. (2017). Experimental results on TIMIT dataset demonstrate the success of this fully unsupervised phoneme recognition system, which achieves a phone error rate (PER) of 41.6%. Although it is still far away from the state-of-the-art supervised systems, we show that with oracle boundaries and matching language model, the PER could be improved to 32.5%. This performance approaches the supervised system of the same model architecture, demonstrating the great potential of the proposed method",
    "checked": true,
    "id": "ad4b7c0e1c71e26c1cf352069e9fd5cb06f70148",
    "semantic_title": "unsupervised speech recognition via segmental empirical output distribution matching",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=rye7knCqK7": {
    "title": "Learning when to Communicate at Scale in Multiagent Cooperative and Competitive Tasks",
    "volume": "poster",
    "abstract": "Learning when to communicate and doing that effectively is essential in multi-agent tasks. Recent works show that continuous communication allows efficient training with back-propagation in multi-agent scenarios, but have been restricted to fully-cooperative tasks. In this paper, we present Individualized Controlled Continuous Communication Model (IC3Net) which has better training efficiency than simple continuous communication model, and can be applied to semi-cooperative and competitive settings along with the cooperative settings. IC3Net controls continuous communication with a gating mechanism and uses individualized rewards foreach agent to gain better performance and scalability while fixing credit assignment issues. Using variety of tasks including StarCraft BroodWars explore and combat scenarios, we show that our network yields improved performance and convergence rates than the baselines as the scale increases. Our results convey that IC3Net agents learn when to communicate based on the scenario and profitability",
    "checked": true,
    "id": "320f80ad1a95de5824b9477c4e3cec37ee007d10",
    "semantic_title": "learning when to communicate at scale in multiagent cooperative and competitive tasks",
    "citation_count": 244,
    "authors": []
  },
  "https://openreview.net/forum?id=HyzMyhCcK7": {
    "title": "ProxQuant: Quantized Neural Networks via Proximal Operators",
    "volume": "poster",
    "abstract": "To make deep neural networks feasible in resource-constrained environments (such as mobile devices), it is beneficial to quantize models by using low-precision weights. One common technique for quantizing neural networks is the straight-through gradient method, which enables back-propagation through the quantization mapping. Despite its empirical success, little is understood about why the straight-through gradient method works. Building upon a novel observation that the straight-through gradient method is in fact identical to the well-known Nesterov's dual-averaging algorithm on a quantization constrained optimization problem, we propose a more principled alternative approach, called ProxQuant , that formulates quantized network training as a regularized learning problem instead and optimizes it via the prox-gradient method. ProxQuant does back-propagation on the underlying full-precision vector and applies an efficient prox-operator in between stochastic gradient steps to encourage quantizedness. For quantizing ResNets and LSTMs, ProxQuant outperforms state-of-the-art results on binary quantization and is on par with state-of-the-art on multi-bit quantization. We further perform theoretical analyses showing that ProxQuant converges to stationary points under mild smoothness assumptions, whereas variants such as lazy prox-gradient method can fail to converge in the same setting",
    "checked": true,
    "id": "7c42d7ff616efc45a42b264b0da6c74e8141a9ed",
    "semantic_title": "proxquant: quantized neural networks via proximal operators",
    "citation_count": 117,
    "authors": []
  },
  "https://openreview.net/forum?id=rkMW1hRqKX": {
    "title": "Optimal Completion Distillation for Sequence Learning",
    "volume": "poster",
    "abstract": "We present Optimal Completion Distillation (OCD), a training procedure for optimizing sequence to sequence models based on edit distance. OCD is efficient, has no hyper-parameters of its own, and does not require pre-training or joint optimization with conditional log-likelihood. Given a partial sequence generated by the model, we first identify the set of optimal suffixes that minimize the total edit distance, using an efficient dynamic programming algorithm. Then, for each position of the generated sequence, we use a target distribution which puts equal probability on the first token of all the optimal suffixes. OCD achieves the state-of-the-art performance on end-to-end speech recognition, on both Wall Street Journal and Librispeech datasets, achieving $9.3\\%$ WER and $4.5\\%$ WER, respectively",
    "checked": true,
    "id": "6ffe584f5edad5e343a30892ec54f1ef8b8e740e",
    "semantic_title": "optimal completion distillation for sequence learning",
    "citation_count": 45,
    "authors": []
  },
  "https://openreview.net/forum?id=SyxZJn05YX": {
    "title": "Feature Intertwiner for Object Detection",
    "volume": "poster",
    "abstract": "A well-trained model should classify objects with unanimous score for every category. This requires the high-level semantic features should be alike among samples, despite a wide span in resolution, texture, deformation, etc. Previous works focus on re-designing the loss function or proposing new regularization constraints on the loss. In this paper, we address this problem via a new perspective. For each category, it is assumed that there are two sets in the feature space: one with more reliable information and the other with less reliable source. We argue that the reliable set could guide the feature learning of the less reliable set during training - in spirit of student mimicking teacher's behavior and thus pushing towards a more compact class centroid in the high-dimensional space. Such a scheme also benefits the reliable set since samples become more closer within the same category - implying that it is easilier for the classifier to identify. We refer to this mutual learning process as feature intertwiner and embed the spirit into object detection. It is well-known that objects of low resolution are more difficult to detect due to the loss of detailed information during network forward pass. We thus regard objects of high resolution as the reliable set and objects of low resolution as the less reliable set. Specifically, an intertwiner is achieved by minimizing the distribution divergence between two sets. We design a historical buffer to represent all previous samples in the reliable set and utilize them to guide the feature learning of the less reliable set. The design of obtaining an effective feature representation for the reliable set is further investigated, where we introduce the optimal transport (OT) algorithm into the framework. Samples in the less reliable set are better aligned with the reliable set with aid of OT metric. Incorporated with such a plug-and-play intertwiner, we achieve an evident improvement over previous state-of-the-arts on the COCO object detection benchmark",
    "checked": true,
    "id": "d2420d9ce64d101b28641660b4641c415fc7a6c9",
    "semantic_title": "feature intertwiner for object detection",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=BkxWJnC9tX": {
    "title": "Diversity and Depth in Per-Example Routing Models",
    "volume": "poster",
    "abstract": "Routing models, a form of conditional computation where examples are routed through a subset of components in a larger network, have shown promising results in recent works. Surprisingly, routing models to date have lacked important properties, such as architectural diversity and large numbers of routing decisions. Both architectural diversity and routing depth can increase the representational power of a routing network. In this work, we address both of these deficiencies. We discuss the significance of architectural diversity in routing models, and explain the tradeoffs between capacity and optimization when increasing routing depth. In our experiments, we find that adding architectural diversity to routing models significantly improves performance, cutting the error rates of a strong baseline by 35% on an Omniglot setup. However, when scaling up routing depth, we find that modern routing techniques struggle with optimization. We conclude by discussing both the positive and negative results, and suggest directions for future research",
    "checked": true,
    "id": "7b7482c433da5e3b52224524206f36bda4c14edf",
    "semantic_title": "diversity and depth in per-example routing models",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=Hke-JhA9Y7": {
    "title": "Learning concise representations for regression by evolving networks of trees",
    "volume": "poster",
    "abstract": "We propose and study a method for learning interpretable representations for the task of regression. Features are represented as networks of multi-type expression trees comprised of activation functions common in neural networks in addition to other elementary functions. Differentiable features are trained via gradient descent, and the performance of features in a linear model is used to weight the rate of change among subcomponents of each representation. The search process maintains an archive of representations with accuracy-complexity trade-offs to assist in generalization and interpretation. We compare several stochastic optimization approaches within this framework. We benchmark these variants on 100 open-source regression problems in comparison to state-of-the-art machine learning approaches. Our main finding is that this approach produces the highest average test scores across problems while producing representations that are orders of magnitude smaller than the next best performing method (gradient boosting). We also report a negative result in which attempts to directly optimize the disentanglement of the representation result in more highly correlated features",
    "checked": true,
    "id": "a7dc6bb1ffb17fa9428dc12303a9d762c959fd00",
    "semantic_title": "learning concise representations for regression by evolving networks of trees",
    "citation_count": 58,
    "authors": []
  },
  "https://openreview.net/forum?id=H1lJJnR5Ym": {
    "title": "Exploration by random network distillation",
    "volume": "poster",
    "abstract": "We introduce an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed. The bonus is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network. We also introduce a method to flexibly combine intrinsic and extrinsic rewards. We find that the random network distillation (RND) bonus combined with this increased flexibility enables significant progress on several hard exploration Atari games. In particular we establish state of the art performance on Montezuma's Revenge, a game famously difficult for deep reinforcement learning methods. To the best of our knowledge, this is the first method that achieves better than average human performance on this game without using demonstrations or having access the underlying state of the game, and occasionally completes the first level. This suggests that relatively simple methods that scale well can be sufficient to tackle challenging exploration problems",
    "checked": true,
    "id": "4cb3fd057949624aa4f0bbe7a6dcc8777ff04758",
    "semantic_title": "exploration by random network distillation",
    "citation_count": 1345,
    "authors": []
  },
  "https://openreview.net/forum?id=rygkk305YQ": {
    "title": "Hierarchical Generative Modeling for Controllable Speech Synthesis",
    "volume": "poster",
    "abstract": "This paper proposes a neural end-to-end text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions. The model is formulated as a conditional generative model with two levels of hierarchical latent variables. The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability. The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes. This amounts to using a Gaussian mixture model (GMM) for the latent distribution. Extensive evaluation demonstrates its ability to control the aforementioned attributes. In particular, it is capable of consistently synthesizing high-quality clean speech regardless of the quality of the training data for the target speaker",
    "checked": true,
    "id": "3321263fd0b2be6011f20d7b74b8ae801741eb21",
    "semantic_title": "hierarchical generative modeling for controllable speech synthesis",
    "citation_count": 276,
    "authors": []
  },
  "https://openreview.net/forum?id=Bkx0RjA9tX": {
    "title": "Generative Question Answering: Learning to Answer the Whole Question",
    "volume": "poster",
    "abstract": "Discriminative question answering models can overfit to superficial biases in datasets, because their loss function saturates when any clue makes the answer likely. We introduce generative models of the joint distribution of questions and answers, which are trained to explain the whole question, not just to answer it.Our question answering (QA) model is implemented by learning a prior over answers, and a conditional language model to generate the question given the answer—allowing scalable and interpretable many-hop reasoning as the question is generated word-by-word. Our model achieves competitive performance with specialised discriminative models on the SQUAD and CLEVR benchmarks, indicating that it is a more general architecture for language understanding and reasoning than previous work. The model greatly improves generalisation both from biased training data and to adversarial testing data, achieving a new state-of-the-art on ADVERSARIAL SQUAD. We will release our code",
    "checked": true,
    "id": "6182aed90596acd1573bd5ccbc2284b1e8a7291b",
    "semantic_title": "generative question answering: learning to answer the whole question",
    "citation_count": 73,
    "authors": []
  },
  "https://openreview.net/forum?id=Bkg6RiCqY7": {
    "title": "Decoupled Weight Decay Regularization",
    "volume": "poster",
    "abstract": "L$_2$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \\emph{not} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L$_2$ regularization (often calling it ``weight decay'' in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \\emph{decoupling} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at \\url{https://github.com/loshchil/AdamW-and-SGDW}",
    "checked": true,
    "id": "d07284a6811f1b2745d91bdb06b040b57f226882",
    "semantic_title": "decoupled weight decay regularization",
    "citation_count": 23307,
    "authors": []
  },
  "https://openreview.net/forum?id=rkl6As0cF7": {
    "title": "Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning",
    "volume": "poster",
    "abstract": "Humans are capable of attributing latent mental contents such as beliefs, or intentions to others. The social skill is critical in everyday life to reason about the potential consequences of their behaviors so as to plan ahead. It is known that humans use this reasoning ability recursively, i.e. considering what others believe about their own beliefs. In this paper, we start from level-$1$ recursion and introduce a probabilistic recursive reasoning (PR2) framework for multi-agent reinforcement learning. Our hypothesis is that it is beneficial for each agent to account for how the opponents would react to its future behaviors. Under the PR2 framework, we adopt variational Bayes methods to approximate the opponents' conditional policy, to which each agent finds the best response and then improve their own policy. We develop decentralized-training-decentralized-execution algorithms, PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenario when there is one Nash equilibrium. Our methods are tested on both the matrix game and the differential game, which have a non-trivial equilibrium where common gradient-based methods fail to converge. Our experiments show that it is critical to reason about how the opponents believe about what the agent believes. We expect our work to contribute a new idea of modeling the opponents to the multi-agent reinforcement learning community",
    "checked": true,
    "id": "bb905c6b8d9abe84adad1c6784e6037dc9516e95",
    "semantic_title": "probabilistic recursive reasoning for multi-agent reinforcement learning",
    "citation_count": 152,
    "authors": []
  },
  "https://openreview.net/forum?id=BJl6AjC5F7": {
    "title": "Learning to Represent Edits",
    "volume": "poster",
    "abstract": "We introduce the problem of learning distributed representations of edits. By combining a \"neural editor\" with an \"edit encoder\", our models learn to represent the salient information of an edit and can be used to apply edits to new inputs. We experiment on natural language and source code edit data. Our evaluation yields promising results that suggest that our neural network models learn to capture the structure and semantics of edits. We hope that this interesting task and data source will inspire other researchers to work further on this problem",
    "checked": true,
    "id": "f6224ed8b4969aee883c8c0a57444b79f681d499",
    "semantic_title": "learning to represent edits",
    "citation_count": 112,
    "authors": []
  },
  "https://openreview.net/forum?id=BklhAj09K7": {
    "title": "Unsupervised Domain Adaptation for Distance Metric Learning",
    "volume": "poster",
    "abstract": "Unsupervised domain adaptation is a promising avenue to enhance the performance of deep neural networks on a target domain, using labels only from a source domain. However, the two predominant methods, domain discrepancy reduction learning and semi-supervised learning, are not readily applicable when source and target domains do not share a common label space. This paper addresses the above scenario by learning a representation space that retains discriminative power on both the (labeled) source and (unlabeled) target domains while keeping representations for the two domains well-separated. Inspired by a theoretical analysis, we first reformulate the disjoint classification task, where the source and target domains correspond to non-overlapping class labels, to a verification one. To handle both within and cross domain verifications, we propose a Feature Transfer Network (FTN) to separate the target feature space from the original source space while aligned with a transformed source space. Moreover, we present a non-parametric multi-class entropy minimization loss to further boost the discriminative power of FTNs on the target domain. In experiments, we first illustrate how FTN works in a controlled setting of adapting from MNIST-M to MNIST with disjoint digit classes between the two domains and then demonstrate the effectiveness of FTNs through state-of-the-art performances on a cross-ethnicity face recognition problem",
    "checked": true,
    "id": "c9c3e58677b95a110220ad89946fd941fa1b287e",
    "semantic_title": "unsupervised domain adaptation for distance metric learning",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=B1g30j0qF7": {
    "title": "Bayesian Deep Convolutional Networks with Many Channels are Gaussian Processes",
    "volume": "poster",
    "abstract": "There is a previously identified equivalence between wide fully connected neural networks (FCNs) and Gaussian processes (GPs). This equivalence enables, for instance, test set predictions that would have resulted from a fully Bayesian, infinitely wide trained FCN to be computed without ever instantiating the FCN, but by instead evaluating the corresponding GP. In this work, we derive an analogous equivalence for multi-layer convolutional neural networks (CNNs) both with and without pooling layers, and achieve state of the art results on CIFAR10 for GPs without trainable kernels. We also introduce a Monte Carlo method to estimate the GP corresponding to a given neural network architecture, even in cases where the analytic form has too many terms to be computationally feasible. Surprisingly, in the absence of pooling layers, the GPs corresponding to CNNs with and without weight sharing are identical. As a consequence, translation equivariance, beneficial in finite channel CNNs trained with stochastic gradient descent (SGD), is guaranteed to play no role in the Bayesian treatment of the infinite channel limit - a qualitative difference between the two regimes that is not present in the FCN case. We confirm experimentally, that while in some scenarios the performance of SGD-trained finite CNNs approaches that of the corresponding GPs as the channel count increases, with careful tuning SGD-trained CNNs can significantly outperform their corresponding GPs, suggesting advantages from SGD training compared to fully Bayesian parameter estimation",
    "checked": true,
    "id": "9ff05248a26c637a4fc9a11e08608574467d6320",
    "semantic_title": "bayesian deep convolutional networks with many channels are gaussian processes",
    "citation_count": 310,
    "authors": []
  },
  "https://openreview.net/forum?id=Hke20iA9Y7": {
    "title": "Efficient Training on Very Large Corpora via Gramian Estimation",
    "volume": "poster",
    "abstract": "We study the problem of learning similarity functions over very large corpora using neural network embedding models. These models are typically trained using SGD with random sampling of unobserved pairs, with a sample size that grows quadratically with the corpus size, making it expensive to scale. We propose new efficient methods to train these models without having to sample unobserved pairs. Inspired by matrix factorization, our approach relies on adding a global quadratic penalty and expressing this term as the inner-product of two generalized Gramians. We show that the gradient of this term can be efficiently computed by maintaining estimates of the Gramians, and develop variance reduction schemes to improve the quality of the estimates. We conduct large-scale experiments that show a significant improvement both in training time and generalization performance compared to sampling methods",
    "checked": true,
    "id": "88c31abd700b40e3237472f8c296365cd9f079b1",
    "semantic_title": "efficient training on very large corpora via gramian estimation",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=BkloRs0qK7": {
    "title": "A comprehensive, application-oriented study of catastrophic forgetting in DNNs",
    "volume": "poster",
    "abstract": "We present a large-scale empirical study of catastrophic forgetting (CF) in modern Deep Neural Network (DNN) models that perform sequential (or: incremental) learning. A new experimental protocol is proposed that takes into account typical constraints encountered in application scenarios. As the investigation is empirical, we evaluate CF behavior on the hitherto largest number of visual classification datasets, from each of which we construct a representative number of Sequential Learning Tasks (SLTs) in close alignment to previous works on CF. Our results clearly indicate that there is no model that avoids CF for all investigated datasets and SLTs under application conditions. We conclude with a discussion of potential solutions and workarounds to CF, notably for the EWC and IMM models",
    "checked": true,
    "id": "eaf6a986ef90b442d6363ee54852d9b5b4788b0f",
    "semantic_title": "a comprehensive, application-oriented study of catastrophic forgetting in dnns",
    "citation_count": 90,
    "authors": []
  },
  "https://openreview.net/forum?id=SJgsCjCqt7": {
    "title": "Variational Autoencoders with Jointly Optimized Latent Dependency Structure",
    "volume": "poster",
    "abstract": "We propose a method for learning the dependency structure between latent variables in deep latent variable models. Our general modeling and inference framework combines the complementary strengths of deep generative models and probabilistic graphical models. In particular, we express the latent variable space of a variational autoencoder (VAE) in terms of a Bayesian network with a learned, flexible dependency structure. The network parameters, variational parameters as well as the latent topology are optimized simultaneously with a single objective. Inference is formulated via a sampling procedure that produces expectations over latent variable structures and incorporates top-down and bottom-up reasoning over latent variable values. We validate our framework in extensive experiments on MNIST, Omniglot, and CIFAR-10. Comparisons to state-of-the-art structured variational autoencoder baselines show improvements in terms of the expressiveness of the learned model",
    "checked": true,
    "id": "0f2d473dd476f4e8775081af57e0ef6f9d4c70bb",
    "semantic_title": "variational autoencoders with jointly optimized latent dependency structure",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=HyeFAsRctQ": {
    "title": "Verification of Non-Linear Specifications for Neural Networks",
    "volume": "poster",
    "abstract": "Prior work on neural network verification has focused on specifications that are linear functions of the output of the network, e.g., invariance of the classifier output under adversarial perturbations of the input. In this paper, we extend verification algorithms to be able to certify richer properties of neural networks. To do this we introduce the class of convex-relaxable specifications, which constitute nonlinear specifications that can be verified using a convex relaxation. We show that a number of important properties of interest can be modeled within this class, including conservation of energy in a learned dynamics model of a physical system; semantic consistency of a classifier's output labels under adversarial perturbations and bounding errors in a system that predicts the summation of handwritten digits. Our experimental evaluation shows that our method is able to effectively verify these specifications. Moreover, our evaluation exposes the failure modes in models which cannot be verified to satisfy these specifications. Thus, emphasizing the importance of training models not just to fit training data but also to be consistent with specifications",
    "checked": true,
    "id": "ad640ea420d49a969a995632aed22c21251891df",
    "semantic_title": "verification of non-linear specifications for neural networks",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=S1xtAjR5tX": {
    "title": "Improving Sequence-to-Sequence Learning via Optimal Transport",
    "volume": "poster",
    "abstract": "Sequence-to-sequence models are commonly trained via maximum likelihood estimation (MLE). However, standard MLE training considers a word-level objective, predicting the next word given the previous ground-truth partial sentence. This procedure focuses on modeling local syntactic patterns, and may fail to capture long-range semantic structure. We present a novel solution to alleviate these issues. Our approach imposes global sequence-level guidance via new supervision based on optimal transport, enabling the overall characterization and preservation of semantic features. We further show that this method can be understood as a Wasserstein gradient flow trying to match our model to the ground truth sequence distribution. Extensive experiments are conducted to validate the utility of the proposed approach, showing consistent improvements over a wide variety of NLP tasks, including machine translation, abstractive text summarization, and image captioning",
    "checked": true,
    "id": "717daba98eb57b898687fc013b705f763eb2916b",
    "semantic_title": "improving sequence-to-sequence learning via optimal transport",
    "citation_count": 94,
    "authors": []
  },
  "https://openreview.net/forum?id=SyVuRiC5K7": {
    "title": "LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING",
    "volume": "poster",
    "abstract": "The goal of few-shot learning is to learn a classifier that generalizes well even when trained with a limited number of training instances per class. The recently introduced meta-learning approaches tackle this problem by learning a generic classifier across a large number of multiclass classification tasks and generalizing the model to a new task. Yet, even with such meta-learning, the low-data problem in the novel classification task still remains. In this paper, we propose Transductive Propagation Network (TPN), a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem. Specifically, we propose to learn to propagate labels from labeled instances to unlabeled test instances, by learning a graph construction module that exploits the manifold structure in the data. TPN jointly learns both the parameters of feature embedding and the graph construction in an end-to-end manner. We validate TPN on multiple benchmark datasets, on which it largely outperforms existing few-shot learning approaches and achieves the state-of-the-art results",
    "checked": true,
    "id": "a3d7257cf0ba5c501d2f6b8ddab931bcd588dfbf",
    "semantic_title": "learning to propagate labels: transductive propagation network for few-shot learning",
    "citation_count": 670,
    "authors": []
  },
  "https://openreview.net/forum?id=HyzdRiR9Y7": {
    "title": "Universal Transformers",
    "volume": "poster",
    "abstract": "Recurrent neural networks (RNNs) sequentially process data by updating their state with each new data point, and have long been the de facto choice for sequence modeling tasks. However, their inherently sequential computation makes them slow to train. Feed-forward and convolutional architectures have recently been shown to achieve superior results on some sequence modeling tasks such as machine translation, with the added advantage that they concurrently process all inputs in the sequence, leading to easy parallelization and faster training times. Despite these successes, however, popular feed-forward sequence models like the Transformer fail to generalize in many simple tasks that recurrent models handle with ease, e.g. copying strings or even simple logical inference when the string or formula lengths exceed those observed at training time. We propose the Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues. UTs combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs. We also add a dynamic per-position halting mechanism and find that it improves accuracy on several tasks. In contrast to the standard Transformer, under certain assumptions UTs can be shown to be Turing-complete. Our experiments show that UTs outperform standard Transformers on a wide range of algorithmic and language understanding tasks, including the challenging LAMBADA language modeling task where UTs achieve a new state of the art, and machine translation where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset",
    "checked": true,
    "id": "ac4dafdef1d2b685b7f28a11837414573d39ff4e",
    "semantic_title": "universal transformers",
    "citation_count": 756,
    "authors": []
  },
  "https://openreview.net/forum?id=rJfUCoR5KX": {
    "title": "An Empirical study of Binary Neural Networks' Optimisation",
    "volume": "poster",
    "abstract": "Binary neural networks using the Straight-Through-Estimator (STE) have been shown to achieve state-of-the-art results, but their training process is not well-founded. This is due to the discrepancy between the evaluated function in the forward path, and the weight updates in the back-propagation, updates which do not correspond to gradients of the forward path. Efficient convergence and accuracy of binary models often rely on careful fine-tuning and various ad-hoc techniques. In this work, we empirically identify and study the effectiveness of the various ad-hoc techniques commonly used in the literature, providing best-practices for efficient training of binary models. We show that adapting learning rates using second moment methods is crucial for the successful use of the STE, and that other optimisers can easily get stuck in local minima. We also find that many of the commonly employed tricks are only effective towards the end of the training, with these methods making early stages of the training considerably slower. Our analysis disambiguates necessary from unnecessary ad-hoc techniques for training of binary neural networks, paving the way for future development of solid theoretical foundations for these. Our newly-found insights further lead to new procedures which make training of existing binary neural networks notably faster",
    "checked": true,
    "id": "a2980c4ab69e017e72ca36db0ac3a0a427ff1aee",
    "semantic_title": "an empirical study of binary neural networks' optimisation",
    "citation_count": 90,
    "authors": []
  },
  "https://openreview.net/forum?id=rylIAsCqYm": {
    "title": "A2BCD: Asynchronous Acceleration with Optimal Complexity",
    "volume": "poster",
    "abstract": "In this paper, we propose the Asynchronous Accelerated Nonuniform Randomized Block Coordinate Descent algorithm (A2BCD). We prove A2BCD converges linearly to a solution of the convex minimization problem at the same rate as NU_ACDM, so long as the maximum delay is not too large. This is the first asynchronous Nesterov-accelerated algorithm that attains any provable speedup. Moreover, we then prove that these algorithms both have optimal complexity. Asynchronous algorithms complete much faster iterations, and A2BCD has optimal complexity. Hence we observe in experiments that A2BCD is the top-performing coordinate descent algorithm, converging up to 4-5x faster than NU_ACDM on some data sets in terms of wall-clock time. To motivate our theory and proof techniques, we also derive and analyze a continuous-time analog of our algorithm and prove it converges at the same rate",
    "checked": true,
    "id": "81f7e93492a1b3effb239d482d41b76093c44ec7",
    "semantic_title": "a2bcd: asynchronous acceleration with optimal complexity",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=r1lrAiA5Ym": {
    "title": "Backpropamine: training self-modifying neural networks with differentiable neuromodulated plasticity",
    "volume": "poster",
    "abstract": "The impressive lifelong learning in animal brains is primarily enabled by plastic changes in synaptic connectivity. Importantly, these changes are not passive, but are actively controlled by neuromodulation, which is itself under the control of the brain. The resulting self-modifying abilities of the brain play an important role in learning and adaptation, and are a major basis for biological reinforcement learning. Here we show for the first time that artificial neural networks with such neuromodulated plasticity can be trained with gradient descent. Extending previous work on differentiable Hebbian plasticity, we propose a differentiable formulation for the neuromodulation of plasticity. We show that neuromodulated plasticity improves the performance of neural networks on both reinforcement learning and supervised learning tasks. In one task, neuromodulated plastic LSTMs with millions of parameters outperform standard LSTMs on a benchmark language modeling task (controlling for the number of parameters). We conclude that differentiable neuromodulation of plasticity offers a powerful new framework for training neural networks",
    "checked": true,
    "id": "146c231532d4e38de95e63368dcd09d0f8cea291",
    "semantic_title": "backpropamine: training self-modifying neural networks with differentiable neuromodulated plasticity",
    "citation_count": 90,
    "authors": []
  },
  "https://openreview.net/forum?id=S1EERs09YQ": {
    "title": "Discovery of Natural Language Concepts in Individual Units of CNNs",
    "volume": "poster",
    "abstract": "Although deep convolutional networks have achieved improved performance in many natural language tasks, they have been treated as black boxes because they are difficult to interpret. Especially, little is known about how they represent language in their intermediate layers. In an attempt to understand the representations of deep convolutional networks trained on language tasks, we show that individual units are selectively responsive to specific morphemes, words, and phrases, rather than responding to arbitrary and uninterpretable patterns. In order to quantitatively analyze such intriguing phenomenon, we propose a concept alignment method based on how units respond to replicated text. We conduct analyses with different architectures on multiple datasets for classification and translation tasks and provide new insights into how deep models understand natural language",
    "checked": true,
    "id": "55c1222c959435867c12e87e2a5c9a7f3a5d1e4b",
    "semantic_title": "discovery of natural language concepts in individual units of cnns",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=ryzECoAcY7": {
    "title": "Learning Multi-Level Hierarchies with Hindsight",
    "volume": "poster",
    "abstract": "Hierarchical agents have the potential to solve sequential decision making tasks with greater sample efficiency than their non-hierarchical counterparts because hierarchical agents can break down tasks into sets of subtasks that only require short sequences of decisions. In order to realize this potential of faster learning, hierarchical agents need to be able to learn their multiple levels of policies in parallel so these simpler subproblems can be solved simultaneously. Yet, learning multiple levels of policies in parallel is hard because it is inherently unstable: changes in a policy at one level of the hierarchy may cause changes in the transition and reward functions at higher levels in the hierarchy, making it difficult to jointly learn multiple levels of policies. In this paper, we introduce a new Hierarchical Reinforcement Learning (HRL) framework, Hierarchical Actor-Critic (HAC), that can overcome the instability issues that arise when agents try to jointly learn multiple levels of policies. The main idea behind HAC is to train each level of the hierarchy independently of the lower levels by training each level as if the lower level policies are already optimal. We demonstrate experimentally in both grid world and simulated robotics domains that our approach can significantly accelerate learning relative to other non-hierarchical and hierarchical methods. Indeed, our framework is the first to successfully learn 3-level hierarchies in parallel in tasks with continuous state and action spaces",
    "checked": true,
    "id": "17704b148b5c20ddf92acbaf1addda134ecbb474",
    "semantic_title": "learning multi-level hierarchies with hindsight",
    "citation_count": 314,
    "authors": []
  },
  "https://openreview.net/forum?id=SkxXCi0qFX": {
    "title": "ProMP: Proximal Meta-Policy Search",
    "volume": "poster",
    "abstract": "Credit assignment in Meta-reinforcement learning (Meta-RL) is still poorly understood. Existing methods either neglect credit assignment to pre-adaptation behavior or implement it naively. This leads to poor sample-efficiency during meta-training as well as ineffective task identification strategies. This paper provides a theoretical analysis of credit assignment in gradient-based Meta-RL. Building on the gained insights we develop a novel meta-learning algorithm that overcomes both the issue of poor credit assignment and previous difficulties in estimating meta-policy gradients. By controlling the statistical distance of both pre-adaptation and adapted policies during meta-policy search, the proposed algorithm endows efficient and stable meta-learning. Our approach leads to superior pre-adaptation policy behavior and consistently outperforms previous Meta-RL algorithms in sample-efficiency, wall-clock time, and asymptotic performance",
    "checked": true,
    "id": "c456941a270cb2040eed4abfb39150508caf920c",
    "semantic_title": "promp: proximal meta-policy search",
    "citation_count": 211,
    "authors": []
  },
  "https://openreview.net/forum?id=HyM7AiA5YX": {
    "title": "Complement Objective Training",
    "volume": "poster",
    "abstract": "Learning with a primary objective, such as softmax cross entropy for classification and sequence generation, has been the norm for training deep neural networks for years. Although being a widely-adopted approach, using cross entropy as the primary objective exploits mostly the information from the ground-truth class for maximizing data likelihood, and largely ignores information from the complement (incorrect) classes. We argue that, in addition to the primary objective, training also using a complement objective that leverages information from the complement classes can be effective in improving model performance. This motivates us to study a new training paradigm that maximizes the likelihood of the ground-truth class while neutralizing the probabilities of the complement classes. We conduct extensive experiments on multiple tasks ranging from computer vision to natural language understanding. The experimental results confirm that, compared to the conventional training with just one primary objective, training also with the complement objective further improves the performance of the state-of-the-art models across all tasks. In addition to the accuracy improvement, we also show that models trained with both primary and complement objectives are more robust to single-step adversarial attacks",
    "checked": true,
    "id": "13d96d1f052254e3fe48077f12afd63b6a0c844a",
    "semantic_title": "complement objective training",
    "citation_count": 49,
    "authors": []
  },
  "https://openreview.net/forum?id=rJeXCo0cYX": {
    "title": "BabyAI: A Platform to Study the Sample Efficiency of Grounded Language Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1b19f433a3e8497e9d9bd67efb108521d16b5b85",
    "semantic_title": "babyai: a platform to study the sample efficiency of grounded language learning",
    "citation_count": 241,
    "authors": []
  },
  "https://openreview.net/forum?id=H1gMCsAqY7": {
    "title": "Slimmable Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "120ffccea4787b88f78b55b9302891ff96cb4228",
    "semantic_title": "slimmable neural networks",
    "citation_count": 557,
    "authors": []
  },
  "https://openreview.net/forum?id=HyxzRsR9Y7": {
    "title": "Learning Self-Imitating Diverse Policies",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "653bdfb3c35621ee04ee5d5253dc7e3a422d69e1",
    "semantic_title": "learning self-imitating diverse policies",
    "citation_count": 68,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgW0oA9FX": {
    "title": "Graph HyperNetworks for Neural Architecture Search",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "89c10e08902cb90abbe1276a3042b93c2f9c78b4",
    "semantic_title": "graph hypernetworks for neural architecture search",
    "citation_count": 280,
    "authors": []
  },
  "https://openreview.net/forum?id=ryxxCiRqYX": {
    "title": "Deep Layers as Stochastic Solvers",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "b9df003fc73d4b6d3171c74cf1cd8ba35d92dda0",
    "semantic_title": "deep layers as stochastic solvers",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=S1lg0jAcYm": {
    "title": "ARM: Augment-REINFORCE-Merge Gradient for Stochastic Binary Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "97f0cff278f9131c22a2e26748a46511b761014b",
    "semantic_title": "arm: augment-reinforce-merge gradient for stochastic binary networks",
    "citation_count": 63,
    "authors": []
  },
  "https://openreview.net/forum?id=HyexAiA5Fm": {
    "title": "Scalable Unbalanced Optimal Transport using Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bdd151d4c522b924b2971b1139cd05c5b76c1513",
    "semantic_title": "scalable unbalanced optimal transport using generative adversarial networks",
    "citation_count": 76,
    "authors": []
  },
  "https://openreview.net/forum?id=rJe10iC5K7": {
    "title": "Unsupervised Discovery of Parts, Structure, and Dynamics",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4b64cd1fb5ee85e874a792c49271e4bf5314d6d7",
    "semantic_title": "unsupervised discovery of parts, structure, and dynamics",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=B1xJAsA5F7": {
    "title": "Learning Multimodal Graph-to-Graph Translation for Molecule Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "7975918889eb92138f0b06e86279b4b0ca3db303",
    "semantic_title": "learning multimodal graph-to-graph translation for molecular optimization",
    "citation_count": 231,
    "authors": []
  },
  "https://openreview.net/forum?id=HJxyAjRcFX": {
    "title": "Harmonizing Maximum Likelihood with GANs for Multimodal Conditional Generation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0dc02a63400c4f1899c952afdc620479238abedf",
    "semantic_title": "harmonizing maximum likelihood with gans for multimodal conditional generation",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=SkeRTsAcYm": {
    "title": "Phase-Aware Speech Enhancement with Deep Complex U-Net",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "101757dd6c7cb3704a54b25fd8ae0e8c0daee248",
    "semantic_title": "phase-aware speech enhancement with deep complex u-net",
    "citation_count": 334,
    "authors": []
  },
  "https://openreview.net/forum?id=rJgTTjA9tX": {
    "title": "The Comparative Power of ReLU Networks and Polynomial Kernels in the Presence of Sparse Latent Structure",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "99b4267421b63f1c018581bc87b263c0e5132910",
    "semantic_title": "the comparative power of relu networks and polynomial kernels in the presence of sparse latent structure",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=BJl6TjRcY7": {
    "title": "Neural Probabilistic Motor Primitives for Humanoid Control",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "96ae5d3ac1a1dcc365684bc92fcfa4d40d802bca",
    "semantic_title": "neural probabilistic motor primitives for humanoid control",
    "citation_count": 160,
    "authors": []
  },
  "https://openreview.net/forum?id=H1xipsA5K7": {
    "title": "Learning Two-layer Neural Networks with Symmetric Inputs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "40c36298ff685a0e1a2c83b52590250ed44664bd",
    "semantic_title": "learning two-layer neural networks with symmetric inputs",
    "citation_count": 59,
    "authors": []
  },
  "https://openreview.net/forum?id=SJzqpj09YQ": {
    "title": "Spectral Inference Networks: Unifying Deep and Spectral Learning",
    "volume": "poster",
    "abstract": "We present Spectral Inference Networks, a framework for learning eigenfunctions of linear operators by stochastic optimization. Spectral Inference Networks generalize Slow Feature Analysis to generic symmetric operators, and are closely related to Variational Monte Carlo methods from computational physics. As such, they can be a powerful tool for unsupervised representation learning from video or graph-structured data. We cast training Spectral Inference Networks as a bilevel optimization problem, which allows for online learning of multiple eigenfunctions. We show results of training Spectral Inference Networks on problems in quantum mechanics and feature learning for videos on synthetic datasets. Our results demonstrate that Spectral Inference Networks accurately recover eigenfunctions of linear operators and can discover interpretable representations from video in a fully unsupervised manner",
    "checked": true,
    "id": "8910e3211989fdeb6123b542667ba4c717523596",
    "semantic_title": "spectral inference networks: unifying deep and spectral learning",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=Hkl5aoR5tm": {
    "title": "On Self Modulation for Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "Training Generative Adversarial Networks (GANs) is notoriously challenging. We propose and study an architectural modification, self-modulation, which improves GAN performance across different data sets, architectures, losses, regularizers, and hyperparameter settings. Intuitively, self-modulation allows the intermediate feature maps of a generator to change as a function of the input noise vector. While reminiscent of other conditioning techniques, it requires no labeled data. In a large-scale empirical study we observe a relative decrease of 5%-35% in FID. Furthermore, all else being equal, adding this modification to the generator leads to improved performance in 124/144 (86%) of the studied settings. Self-modulation is a simple architectural change that requires no additional parameter tuning, which suggests that it can be applied readily to any GAN",
    "checked": true,
    "id": "f91ff6c0aeba78f94f02b761446d5d911e6ab390",
    "semantic_title": "on self modulation for generative adversarial networks",
    "citation_count": 105,
    "authors": []
  },
  "https://openreview.net/forum?id=BJgK6iA5KX": {
    "title": "AutoLoss: Learning Discrete Schedule for Alternate Optimization",
    "volume": "poster",
    "abstract": "Many machine learning problems involve iteratively and alternately optimizing different task objectives with respect to different sets of parameters. Appropriately scheduling the optimization of a task objective or a set of parameters is usually crucial to the quality of convergence. In this paper, we present AutoLoss, a meta-learning framework that automatically learns and determines the optimization schedule. AutoLoss provides a generic way to represent and learn the discrete optimization schedule from metadata, allows for a dynamic and data-driven schedule in ML problems that involve alternating updates of different parameters or from different loss objectives. We apply AutoLoss on four ML tasks: d-ary quadratic regression, classification using a multi-layer perceptron (MLP), image generation using GANs, and multi-task neural machine translation (NMT). We show that the AutoLoss controller is able to capture the distribution of better optimization schedules that result in higher quality of convergence on all four tasks. The trained AutoLoss controller is generalizable -- it can guide and improve the learning of a new task model with different specifications, or on different datasets",
    "checked": true,
    "id": "d8e40c8af4a8777a1b85a50a6a2001b2e62e499c",
    "semantic_title": "autoloss: learning discrete schedule for alternate optimization",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=rJed6j0cKX": {
    "title": "Analyzing Inverse Problems with Invertible Neural Networks",
    "volume": "poster",
    "abstract": "For many applications, in particular in natural science, the task is to determine hidden system parameters from a set of measurements. Often, the forward process from parameter- to measurement-space is well-defined, whereas the inverse problem is ambiguous: multiple parameter sets can result in the same measurement. To fully characterize this ambiguity, the full posterior parameter distribution, conditioned on an observed measurement, has to be determined. We argue that a particular class of neural networks is well suited for this task – so-called Invertible Neural Networks (INNs). Unlike classical neural networks, which attempt to solve the ambiguous inverse problem directly, INNs focus on learning the forward process, using additional latent output variables to capture the information otherwise lost. Due to invertibility, a model of the corresponding inverse process is learned implicitly. Given a specific measurement and the distribution of the latent variables, the inverse pass of the INN provides the full posterior over parameter space. We prove theoretically and verify experimentally, on artificial data and real-world problems from medicine and astrophysics, that INNs are a powerful analysis tool to find multi-modalities in parameter space, uncover parameter correlations, and identify unrecoverable parameters",
    "checked": true,
    "id": "25e433197844c239742f67fbb4171e913e0b9fe2",
    "semantic_title": "analyzing inverse problems with invertible neural networks",
    "citation_count": 494,
    "authors": []
  },
  "https://openreview.net/forum?id=S1gOpsCctm": {
    "title": "Learning Finite State Representations of Recurrent Policy Networks",
    "volume": "poster",
    "abstract": "Recurrent neural networks (RNNs) are an effective representation of control policies for a wide range of reinforcement and imitation learning problems. RNN policies, however, are particularly difficult to explain, understand, and analyze due to their use of continuous-valued memory vectors and observation features. In this paper, we introduce a new technique, Quantized Bottleneck Insertion, to learn finite representations of these vectors and features. The result is a quantized representation of the RNN that can be analyzed to improve our understanding of memory use and general behavior. We present results of this approach on synthetic environments and six Atari games. The resulting finite representations are surprisingly small in some cases, using as few as 3 discrete memory states and 10 observations for a perfect Pong policy. We also show that these finite policy representations lead to improved interpretability",
    "checked": true,
    "id": "bd858cef3d94faf6121b7f2bd6bb6c308db4aa16",
    "semantic_title": "learning finite state representations of recurrent policy networks",
    "citation_count": 88,
    "authors": []
  },
  "https://openreview.net/forum?id=SkMwpiR9Y7": {
    "title": "Measuring and regularizing networks in function space",
    "volume": "poster",
    "abstract": "To optimize a neural network one often thinks of optimizing its parameters, but it is ultimately a matter of optimizing the function that maps inputs to outputs. Since a change in the parameters might serve as a poor proxy for the change in the function, it is of some concern that primacy is given to parameters but that the correspondence has not been tested. Here, we show that it is simple and computationally feasible to calculate distances between functions in a $L^2$ Hilbert space. We examine how typical networks behave in this space, and compare how parameter $\\ell^2$ distances compare to function $L^2$ distances between various points of an optimization trajectory. We find that the two distances are nontrivially related. In particular, the $L^2/\\ell^2$ ratio decreases throughout optimization, reaching a steady value around when test error plateaus. We then investigate how the $L^2$ distance could be applied directly to optimization. We first propose that in multitask learning, one can avoid catastrophic forgetting by directly limiting how much the input/output function changes between tasks. Secondly, we propose a new learning rule that constrains the distance a network can travel through $L^2$-space in any one update. This allows new examples to be learned in a way that minimally interferes with what has previously been learned. These applications demonstrate how one can measure and regularize function distances directly, without relying on parameters or local approximations like loss curvature",
    "checked": true,
    "id": "e3fee9244fc47aa9e80006e39352af90f64631fe",
    "semantic_title": "measuring and regularizing networks in function space",
    "citation_count": 140,
    "authors": []
  },
  "https://openreview.net/forum?id=BkgPajAcY7": {
    "title": "No Training Required: Exploring Random Encoders for Sentence Classification",
    "volume": "poster",
    "abstract": "We explore various methods for computing sentence representations from pre-trained word embeddings without any training, i.e., using nothing but random parameterizations. Our aim is to put sentence embeddings on more solid footing by 1) looking at how much modern sentence embeddings gain over random methods---as it turns out, surprisingly little; and by 2) providing the field with more appropriate baselines going forward---which are, as it turns out, quite strong. We also make important observations about proper experimental protocol for sentence classification evaluation, together with recommendations for future research",
    "checked": true,
    "id": "7a8f8109e65ed9a6048859681a825eb5655e5dd2",
    "semantic_title": "no training required: exploring random encoders for sentence classification",
    "citation_count": 99,
    "authors": []
  },
  "https://openreview.net/forum?id=SyVU6s05K7": {
    "title": "Deep Frank-Wolfe For Neural Network Optimization",
    "volume": "poster",
    "abstract": "Learning a deep neural network requires solving a challenging optimization problem: it is a high-dimensional, non-convex and non-smooth minimization problem with a large number of terms. The current practice in neural network optimization is to rely on the stochastic gradient descent (SGD) algorithm or its adaptive variants. However, SGD requires a hand-designed schedule for the learning rate. In addition, its adaptive variants tend to produce solutions that generalize less well on unseen data than SGD with a hand-designed schedule. We present an optimization method that offers empirically the best of both worlds: our algorithm yields good generalization performance while requiring only one hyper-parameter. Our approach is based on a composite proximal framework, which exploits the compositional nature of deep neural networks and can leverage powerful convex optimization algorithms by design. Specifically, we employ the Frank-Wolfe (FW) algorithm for SVM, which computes an optimal step-size in closed-form at each time-step. We further show that the descent direction is given by a simple backward pass in the network, yielding the same computational cost per iteration as SGD. We present experiments on the CIFAR and SNLI data sets, where we demonstrate the significant superiority of our method over Adam, Adagrad, as well as the recently proposed BPGrad and AMSGrad. Furthermore, we compare our algorithm to SGD with a hand-designed learning rate schedule, and show that it provides similar generalization while often converging faster. The code is publicly available at https://github.com/oval-group/dfw",
    "checked": true,
    "id": "a97d86772364b7244eca65d48c5f9225f6fdc162",
    "semantic_title": "deep frank-wolfe for neural network optimization",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=S1fUpoR5FQ": {
    "title": "Quasi-hyperbolic momentum and Adam for deep learning",
    "volume": "poster",
    "abstract": "Momentum-based acceleration of stochastic gradient descent (SGD) is widely used in deep learning. We propose the quasi-hyperbolic momentum algorithm (QHM) as an extremely simple alteration of momentum SGD, averaging a plain SGD step with a momentum step. We describe numerous connections to and identities with other algorithms, and we characterize the set of two-state optimization algorithms that QHM can recover. Finally, we propose a QH variant of Adam called QHAdam, and we empirically demonstrate that our algorithms lead to significantly improved training in a variety of settings, including a new state-of-the-art result on WMT16 EN-DE. We hope that these empirical results, combined with the conceptual and practical simplicity of QHM and QHAdam, will spur interest from both practitioners and researchers. Code is immediately available",
    "checked": true,
    "id": "8c62519ba7567b0c0016f9d49eefa2584e5df2cb",
    "semantic_title": "quasi-hyperbolic momentum and adam for deep learning",
    "citation_count": 130,
    "authors": []
  },
  "https://openreview.net/forum?id=rJNH6sAqY7": {
    "title": "On Computation and Generalization of Generative Adversarial Networks under Spectrum Control",
    "volume": "poster",
    "abstract": "Generative Adversarial Networks (GANs), though powerful, is hard to train. Several recent works (Brock et al., 2016; Miyato et al., 2018) suggest that controlling the spectra of weight matrices in the discriminator can significantly improve the training of GANs. Motivated by their discovery, we propose a new framework for training GANs, which allows more flexible spectrum control (e.g., making the weight matrices of the discriminator have slow singular value decays). Specifically, we propose a new reparameterization approach for the weight matrices of the discriminator in GANs, which allows us to directly manipulate the spectra of the weight matrices through various regularizers and constraints, without intensively computing singular value decompositions. Theoretically, we further show that the spectrum control improves the generalization ability of GANs. Our experiments on CIFAR-10, STL-10, and ImgaeNet datasets confirm that compared to other competitors, our proposed method is capable of generating images with better or equal quality by utilizing spectral normalization and encouraging the slow singular value decay",
    "checked": true,
    "id": "4292986f497324293091189f86003d47cab3f664",
    "semantic_title": "on computation and generalization of generative adversarial networks under spectrum control",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=HJMHpjC9Ym": {
    "title": "Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition",
    "volume": "poster",
    "abstract": "In this paper, we propose a novel Convolutional Neural Network (CNN) architecture for learning multi-scale feature representations with good tradeoffs between speed and accuracy. This is achieved by using a multi-branch network, which has different computational complexity at different branches with different resolutions. Through frequent merging of features from branches at distinct scales, our model obtains multi-scale features while using less computation. The proposed approach demonstrates improvement of model efficiency and performance on both object recognition and speech recognition tasks, using popular architectures including ResNet, ResNeXt and SEResNeXt. For object recognition, our approach reduces computation by 1/3 while improving accuracy significantly over 1% point than the baselines, and the computational savings can be higher up to 1/2 without compromising the accuracy. Our model also surpasses state-of-the-art CNN acceleration approaches by a large margin in terms of accuracy and FLOPs. On the task of speech recognition, our proposed multi-scale CNNs save 30% FLOPs with slightly better word error rates, showing good generalization across domains",
    "checked": true,
    "id": "425032e4fdd531cd7a5d3e7effc12d16f9c076d9",
    "semantic_title": "big-little net: an efficient multi-scale feature representation for visual and speech recognition",
    "citation_count": 96,
    "authors": []
  },
  "https://openreview.net/forum?id=BklHpjCqKm": {
    "title": "Deep Lagrangian Networks: Using Physics as Model Prior for Deep Learning",
    "volume": "poster",
    "abstract": "Deep learning has achieved astonishing results on many tasks with large amounts of data and generalization within the proximity of training data. For many important real-world applications, these requirements are unfeasible and additional prior knowledge on the task domain is required to overcome the resulting problems. In particular, learning physics models for model-based control requires robust extrapolation from fewer samples – often collected online in real-time – and model errors may lead to drastic damages of the system. Directly incorporating physical insight has enabled us to obtain a novel deep model learning approach that extrapolates well while requiring fewer samples. As a first example, we propose Deep Lagrangian Networks (DeLaN) as a deep network structure upon which Lagrangian Mechanics have been imposed. DeLaN can learn the equations of motion of a mechanical system (i.e., system dynamics) with a deep network efficiently while ensuring physical plausibility. The resulting DeLaN network performs very well at robot tracking control. The proposed method did not only outperform previous model learning approaches at learning speed but exhibits substantially improved and more robust extrapolation to novel trajectories and learns online in real-time",
    "checked": true,
    "id": "e129e344083b307e005c5342ba49524d9981a420",
    "semantic_title": "deep lagrangian networks: using physics as model prior for deep learning",
    "citation_count": 381,
    "authors": []
  },
  "https://openreview.net/forum?id=ByMVTsR5KQ": {
    "title": "Adversarial Audio Synthesis",
    "volume": "poster",
    "abstract": "Audio signals are sampled at high temporal resolutions, and learning to synthesize audio requires capturing structure across a range of timescales. Generative adversarial networks (GANs) have seen wide success at generating images that are both locally and globally coherent, but they have seen little application to audio generation. In this paper we introduce WaveGAN, a first attempt at applying GANs to unsupervised synthesis of raw-waveform audio. WaveGAN is capable of synthesizing one second slices of audio waveforms with global coherence, suitable for sound effect generation. Our experiments demonstrate that—without labels—WaveGAN learns to produce intelligible words when trained on a small-vocabulary speech dataset, and can also synthesize audio from other domains such as drums, bird vocalizations, and piano. We compare WaveGAN to a method which applies GANs designed for image generation on image-like audio feature representations, finding both approaches to be promising",
    "checked": true,
    "id": "0d3bbe47fe67e5a125a2547913ac0e4a30f18c8d",
    "semantic_title": "adversarial audio synthesis",
    "citation_count": 614,
    "authors": []
  },
  "https://openreview.net/forum?id=B1xVTjCqKQ": {
    "title": "A Data-Driven and Distributed Approach to Sparse Signal Representation and Recovery",
    "volume": "poster",
    "abstract": "In this paper, we focus on two challenges which offset the promise of sparse signal representation, sensing, and recovery. First, real-world signals can seldom be described as perfectly sparse vectors in a known basis, and traditionally used random measurement schemes are seldom optimal for sensing them. Second, existing signal recovery algorithms are usually not fast enough to make them applicable to real-time problems. In this paper, we address these two challenges by presenting a novel framework based on deep learning. For the first challenge, we cast the problem of finding informative measurements by using a maximum likelihood (ML) formulation and show how we can build a data-driven dimensionality reduction protocol for sensing signals using convolutional architectures. For the second challenge, we discuss and analyze a novel parallelization scheme and show it significantly speeds-up the signal recovery process. We demonstrate the significant improvement our method obtains over competing methods through a series of experiments",
    "checked": true,
    "id": "e34755989bc59b1092bd83393f21d42cbd68623e",
    "semantic_title": "a data-driven and distributed approach to sparse signal representation and recovery",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=HJlNpoA5YQ": {
    "title": "The Laplacian in RL: Learning Representations with Efficient Approximations",
    "volume": "poster",
    "abstract": "The smallest eigenvectors of the graph Laplacian are well-known to provide a succinct representation of the geometry of a weighted graph. In reinforcement learning (RL), where the weighted graph may be interpreted as the state transition process induced by a behavior policy acting on the environment, approximating the eigenvectors of the Laplacian provides a promising approach to state representation learning. However, existing methods for performing this approximation are ill-suited in general RL settings for two main reasons: First, they are computationally expensive, often requiring operations on large matrices. Second, these methods lack adequate justification beyond simple, tabular, finite-state settings. In this paper, we present a fully general and scalable method for approximating the eigenvectors of the Laplacian in a model-free RL context. We systematically evaluate our approach and empirically show that it generalizes beyond the tabular, finite-state setting. Even in tabular, finite-state settings, its ability to approximate the eigenvectors outperforms previous proposals. Finally, we show the potential benefits of using a Laplacian representation learned using our method in goal-achieving RL tasks, providing evidence that our technique can be used to significantly improve the performance of an RL agent",
    "checked": true,
    "id": "af03168ecb019c4ba427b8386b9c0bc3695e917e",
    "semantic_title": "the laplacian in rl: learning representations with efficient approximations",
    "citation_count": 87,
    "authors": []
  },
  "https://openreview.net/forum?id=SkgEaj05t7": {
    "title": "On the Relation Between the Sharpest Directions of DNN Loss and the SGD Step Length",
    "volume": "poster",
    "abstract": "The training of deep neural networks with Stochastic Gradient Descent (SGD) with a large learning rate or a small batch-size typically ends in flat regions of the weight space, as indicated by small eigenvalues of the Hessian of the training loss. This was found to correlate with a good final generalization performance. In this paper we extend previous work by investigating the curvature of the loss surface along the whole training trajectory, rather than only at the endpoint. We find that initially SGD visits increasingly sharp regions, reaching a maximum sharpness determined by both the learning rate and the batch-size of SGD. At this peak value SGD starts to fail to minimize the loss along directions in the loss surface corresponding to the largest curvature (sharpest directions). To further investigate the effect of these dynamics in the training process, we study a variant of SGD using a reduced learning rate along the sharpest directions which we show can improve training speed while finding both sharper and better generalizing solution, compared to vanilla SGD. Overall, our results show that the SGD dynamics in the subspace of the sharpest directions influence the regions that SGD steers to (where larger learning rate or smaller batch size result in wider regions visited), the overall training speed, and the generalization ability of the final model",
    "checked": true,
    "id": "5f9e2f6d4a844189b2e34da8fd0ba282f3f36c6f",
    "semantic_title": "on the relation between the sharpest directions of dnn loss and the sgd step length",
    "citation_count": 118,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk4fpoA5Km": {
    "title": "Discriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning",
    "volume": "poster",
    "abstract": "We identify two issues with the family of algorithms based on the Adversarial Imitation Learning framework. The first problem is implicit bias present in the reward functions used in these algorithms. While these biases might work well for some environments, they can also lead to sub-optimal behavior in others. Secondly, even though these algorithms can learn from few expert demonstrations, they require a prohibitively large number of interactions with the environment in order to imitate the expert for many real-world applications. In order to address these issues, we propose a new algorithm called Discriminator-Actor-Critic that uses off-policy Reinforcement Learning to reduce policy-environment interaction sample complexity by an average factor of 10. Furthermore, since our reward function is designed to be unbiased, we can apply our algorithm to many problems without making any task-specific adjustments",
    "checked": true,
    "id": "876053977063ab843dd24c78425cbad1779a62ed",
    "semantic_title": "discriminator-actor-critic: addressing sample inefficiency and reward bias in adversarial imitation learning",
    "citation_count": 259,
    "authors": []
  },
  "https://openreview.net/forum?id=BkfbpsAcF7": {
    "title": "Excessive Invariance Causes Adversarial Vulnerability",
    "volume": "poster",
    "abstract": "Despite their impressive performance, deep neural networks exhibit striking failures on out-of-distribution inputs. One core idea of adversarial example research is to reveal neural network errors under such distribution shifts. We decompose these errors into two complementary sources: sensitivity and invariance. We show deep networks are not only too sensitive to task-irrelevant changes of their input, as is well-known from epsilon-adversarial examples, but are also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks. We show such excessive invariance occurs across various tasks and architecture types. On MNIST and ImageNet one can manipulate the class-specific content of almost any image without changing the hidden activations. We identify an insufficiency of the standard cross-entropy loss as a reason for these failures. Further, we extend this objective based on an information-theoretic analysis so it encourages the model to consider all task-dependent features in its decision. This provides the first approach tailored explicitly to overcome excessive invariance and resulting vulnerabilities",
    "checked": true,
    "id": "67b72e427187b1113c787f9265926322e3d123e8",
    "semantic_title": "excessive invariance causes adversarial vulnerability",
    "citation_count": 167,
    "authors": []
  },
  "https://openreview.net/forum?id=H1ebTsActm": {
    "title": "Adaptivity of deep ReLU network for learning in Besov and mixed smooth Besov spaces: optimal rate and curse of dimensionality",
    "volume": "poster",
    "abstract": "Deep learning has shown high performances in various types of tasks from visual recognition to natural language processing, which indicates superior flexibility and adaptivity of deep learning. To understand this phenomenon theoretically, we develop a new approximation and estimation error analysis of deep learning with the ReLU activation for functions in a Besov space and its variant with mixed smoothness. The Besov space is a considerably general function space including the Holder space and Sobolev space, and especially can capture spatial inhomogeneity of smoothness. Through the analysis in the Besov space, it is shown that deep learning can achieve the minimax optimal rate and outperform any non-adaptive (linear) estimator such as kernel ridge regression, which shows that deep learning has higher adaptivity to the spatial inhomogeneity of the target function than other estimators such as linear ones. In addition to this, it is shown that deep learning can avoid the curse of dimensionality if the target function is in a mixed smooth Besov space. We also show that the dependency of the convergence rate on the dimensionality is tight due to its minimax optimality. These results support high adaptivity of deep learning and its superior ability as a feature extractor",
    "checked": true,
    "id": "9f56bda06700297fe8a8aab4bb429451cee9a441",
    "semantic_title": "adaptivity of deep relu network for learning in besov and mixed smooth besov spaces: optimal rate and curse of dimensionality",
    "citation_count": 246,
    "authors": []
  },
  "https://openreview.net/forum?id=ryxepo0cFX": {
    "title": "AntisymmetricRNN: A Dynamical System View on Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "Recurrent neural networks have gained widespread use in modeling sequential data. Learning long-term dependencies using these models remains difficult though, due to exploding or vanishing gradients. In this paper, we draw connections between recurrent networks and ordinary differential equations. A special form of recurrent networks called the AntisymmetricRNN is proposed under this theoretical framework, which is able to capture long-term dependencies thanks to the stability property of its underlying differential equation. Existing approaches to improving RNN trainability often incur significant computation overhead. In comparison, AntisymmetricRNN achieves the same goal by design. We showcase the advantage of this new architecture through extensive simulations and experiments. AntisymmetricRNN exhibits much more predictable dynamics. It outperforms regular LSTM models on tasks requiring long-term memory and matches the performance on tasks where short-term dependencies dominate despite being much simpler",
    "checked": true,
    "id": "e2c8a6b49cd999b16ac4dcfdc375563a6932b1c7",
    "semantic_title": "antisymmetricrnn: a dynamical system view on recurrent neural networks",
    "citation_count": 207,
    "authors": []
  },
  "https://openreview.net/forum?id=S1GkToR5tm": {
    "title": "Discriminator Rejection Sampling",
    "volume": "poster",
    "abstract": "We propose a rejection sampling scheme using the discriminator of a GAN to approximately correct errors in the GAN generator distribution. We show that under quite strict assumptions, this will allow us to recover the data distribution exactly. We then examine where those strict assumptions break down and design a practical algorithm—called Discriminator Rejection Sampling (DRS)—that can be used on real data-sets. Finally, we demonstrate the efficacy of DRS on a mixture of Gaussians and on the state of the art SAGAN model. On ImageNet, we train an improved baseline that increases the best published Inception Score from 52.52 to 62.36 and reduces the Frechet Inception Distance from 18.65 to 14.79. We then use DRS to further improve on this baseline, improving the Inception Score to 76.08 and the FID to 13.75",
    "checked": true,
    "id": "866aa9bcb15cf4a23a0afed515fa2f6b93f91d11",
    "semantic_title": "discriminator rejection sampling",
    "citation_count": 131,
    "authors": []
  },
  "https://openreview.net/forum?id=r1My6sR9tX": {
    "title": "Unsupervised Learning via Meta-Learning",
    "volume": "poster",
    "abstract": "A central goal of unsupervised learning is to acquire representations from unlabeled data or experience that can be used for more effective learning of downstream tasks from modest amounts of labeled data. Many prior unsupervised learning works aim to do so by developing proxy objectives based on reconstruction, disentanglement, prediction, and other metrics. Instead, we develop an unsupervised meta-learning method that explicitly optimizes for the ability to learn a variety of tasks from small amounts of data. To do so, we construct tasks from unlabeled data in an automatic way and run meta-learning over the constructed tasks. Surprisingly, we find that, when integrated with meta-learning, relatively simple task construction mechanisms, such as clustering embeddings, lead to good performance on a variety of downstream, human-specified tasks. Our experiments across four image datasets indicate that our unsupervised meta-learning approach acquires a learning algorithm without any labeled data that is applicable to a wide range of downstream classification tasks, improving upon the embedding learned by four prior unsupervised learning methods",
    "checked": true,
    "id": "37b3d9ab049c5671fed29f56cacee858d98c2ea8",
    "semantic_title": "unsupervised learning via meta-learning",
    "citation_count": 230,
    "authors": []
  },
  "https://openreview.net/forum?id=r1lyTjAqYX": {
    "title": "Recurrent Experience Replay in Distributed Reinforcement Learning",
    "volume": "poster",
    "abstract": "Building on the recent successes of distributed training of RL agents, in this paper we investigate the training of RNN-based RL agents from distributed prioritized experience replay. We study the effects of parameter lag resulting in representational drift and recurrent state staleness and empirically derive an improved training strategy. Using a single network architecture and fixed set of hyper-parameters, the resulting agent, Recurrent Replay Distributed DQN, quadruples the previous state of the art on Atari-57, and matches the state of the art on DMLab-30. It is the first agent to exceed human-level performance in 52 of the 57 Atari games",
    "checked": true,
    "id": "8ede7ddf99986d69562455bc8d69222fc3e27350",
    "semantic_title": "recurrent experience replay in distributed reinforcement learning",
    "citation_count": 491,
    "authors": []
  },
  "https://openreview.net/forum?id=rJlk6iRqKX": {
    "title": "Query-Efficient Hard-label Black-box Attack: An Optimization-based Approach",
    "volume": "poster",
    "abstract": "We study the problem of attacking machine learning models in the hard-label black-box setting, where no model information is revealed except that the attacker can make queries to probe the corresponding hard-label decisions. This is a very challenging problem since the direct extension of state-of-the-art white-box attacks (e.g., C&W or PGD) to the hard-label black-box setting will require minimizing a non-continuous step function, which is combinatorial and cannot be solved by a gradient-based optimizer. The only two current approaches are based on random walk on the boundary (Brendel et al., 2017) and random trials to evaluate the loss function (Ilyas et al., 2018), which require lots of queries and lacks convergence guarantees. We propose a novel way to formulate the hard-label black-box attack as a real-valued optimization problem which is usually continuous and can be solved by any zeroth order optimization algorithm. For example, using the Randomized Gradient-Free method (Nesterov & Spokoiny, 2017), we are able to bound the number of iterations needed for our algorithm to achieve stationary points under mild assumptions. We demonstrate that our proposed method outperforms the previous stochastic approaches to attacking convolutional neural networks on MNIST, CIFAR, and ImageNet datasets. More interestingly, we show that the proposed algorithm can also be used to attack other discrete and non-continuous machine learning models, such as Gradient Boosting Decision Trees (GBDT)",
    "checked": true,
    "id": "b862efa06baea0b032214675eb3c3645d5d69d46",
    "semantic_title": "query-efficient hard-label black-box attack: an optimization-based approach",
    "citation_count": 348,
    "authors": []
  },
  "https://openreview.net/forum?id=SJzR2iRcK7": {
    "title": "Multi-class classification without multi-class labels",
    "volume": "poster",
    "abstract": "This work presents a new strategy for multi-class classification that requires no class-specific labels, but instead leverages pairwise similarity between examples, which is a weaker form of annotation. The proposed method, meta classification learning, optimizes a binary classifier for pairwise similarity prediction and through this process learns a multi-class classifier as a submodule. We formulate this approach, present a probabilistic graphical model for it, and derive a surprisingly simple loss function that can be used to learn neural network-based models. We then demonstrate that this same framework generalizes to the supervised, unsupervised cross-task, and semi-supervised settings. Our method is evaluated against state of the art in all three learning paradigms and shows a superior or comparable accuracy, providing evidence that learning multi-class classification without multi-class labels is a viable learning option",
    "checked": true,
    "id": "5761b8de288b7622e94eb5addd6718313860f68b",
    "semantic_title": "multi-class classification without multi-class labels",
    "citation_count": 168,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgT3jRct7": {
    "title": "Large-Scale Answerer in Questioner's Mind for Visual Dialog Question Generation",
    "volume": "poster",
    "abstract": "Answerer in Questioner's Mind (AQM) is an information-theoretic framework that has been recently proposed for task-oriented dialog systems. AQM benefits from asking a question that would maximize the information gain when it is asked. However, due to its intrinsic nature of explicitly calculating the information gain, AQM has a limitation when the solution space is very large. To address this, we propose AQM+ that can deal with a large-scale problem and ask a question that is more coherent to the current context of the dialog. We evaluate our method on GuessWhich, a challenging task-oriented visual dialog problem, where the number of candidate classes is near 10K. Our experimental results and ablation studies show that AQM+ outperforms the state-of-the-art models by a remarkable margin with a reasonable approximation. In particular, the proposed AQM+ reduces more than 60% of error as the dialog proceeds, while the comparative algorithms diminish the error by less than 6%. Based on our results, we argue that AQM+ is a general task-oriented dialog algorithm that can be applied for non-yes-or-no responses",
    "checked": true,
    "id": "0dde473b9280b115ae4405e5dc936a6e913aa573",
    "semantic_title": "large-scale answerer in questioner's mind for visual dialog question generation",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=HJe62s09tX": {
    "title": "Unsupervised Hyper-alignment for Multilingual Word Embeddings",
    "volume": "poster",
    "abstract": "We consider the problem of aligning continuous word representations, learned in multiple languages, to a common space. It was recently shown that, in the case of two languages, it is possible to learn such a mapping without supervision. This paper extends this line of work to the problem of aligning multiple languages to a common space. A solution is to independently map all languages to a pivot language. Unfortunately, this degrades the quality of indirect word translation. We thus propose a novel formulation that ensures composable mappings, leading to better alignments. We evaluate our method by jointly aligning word vectors in eleven languages, showing consistent improvement with indirect mappings while maintaining competitive performance on direct word translation",
    "checked": false,
    "id": "115d6dffb5d6866c03ddec57a1779a1ba032613b",
    "semantic_title": "unsupervised hyperalignment for multilingual word embeddings",
    "citation_count": 71,
    "authors": []
  },
  "https://openreview.net/forum?id=SJx63jRqFm": {
    "title": "Diversity is All You Need: Learning Skills without a Reward Function",
    "volume": "poster",
    "abstract": "Intelligent creatures can explore their environments and learn useful skills without supervision. In this paper, we propose ``Diversity is All You Need''(DIAYN), a method for learning useful skills without a reward function. Our proposed method learns skills by maximizing an information theoretic objective using a maximum entropy policy. On a variety of simulated robotic tasks, we show that this simple objective results in the unsupervised emergence of diverse skills, such as walking and jumping. In a number of reinforcement learning benchmark environments, our method is able to learn a skill that solves the benchmark task despite never receiving the true task reward. We show how pretrained skills can provide a good parameter initialization for downstream tasks, and can be composed hierarchically to solve complex, sparse reward tasks. Our results suggest that unsupervised discovery of skills can serve as an effective pretraining mechanism for overcoming challenges of exploration and data efficiency in reinforcement learning",
    "checked": true,
    "id": "5b01eaef54a653ba03ddd5a978690380fbc19bfc",
    "semantic_title": "diversity is all you need: learning skills without a reward function",
    "citation_count": 1089,
    "authors": []
  },
  "https://openreview.net/forum?id=Hyfn2jCcKm": {
    "title": "Solving the Rubik's Cube with Approximate Policy Iteration",
    "volume": "poster",
    "abstract": "Recently, Approximate Policy Iteration (API) algorithms have achieved super-human proficiency in two-player zero-sum games such as Go, Chess, and Shogi without human data. These API algorithms iterate between two policies: a slow policy (tree search), and a fast policy (a neural network). In these two-player games, a reward is always received at the end of the game. However, the Rubik's Cube has only a single solved state, and episodes are not guaranteed to terminate. This poses a major problem for these API algorithms since they rely on the reward received at the end of the game. We introduce Autodidactic Iteration: an API algorithm that overcomes the problem of sparse rewards by training on a distribution of states that allows the reward to propagate from the goal state to states farther away. Autodidactic Iteration is able to learn how to solve the Rubik's Cube and the 15-puzzle without relying on human data. Our algorithm is able to solve 100% of randomly scrambled cubes while achieving a median solve length of 30 moves — less than or equal to solvers that employ human domain knowledge",
    "checked": true,
    "id": "ff0dfe776b6c4b5ab2bc40ce7480736017b87127",
    "semantic_title": "solving the rubik's cube with approximate policy iteration",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=BJxh2j0qYm": {
    "title": "Dynamic Channel Pruning: Feature Boosting and Suppression",
    "volume": "poster",
    "abstract": "Making deep convolutional neural networks more accurate typically comes at the cost of increased computational and memory resources. In this paper, we reduce this cost by exploiting the fact that the importance of features computed by convolutional layers is highly input-dependent, and propose feature boosting and suppression (FBS), a new method to predictively amplify salient convolutional channels and skip unimportant ones at run-time. FBS introduces small auxiliary connections to existing convolutional layers. In contrast to channel pruning methods which permanently remove channels, it preserves the full network structures and accelerates convolution by dynamically skipping unimportant input and output channels. FBS-augmented networks are trained with conventional stochastic gradient descent, making it readily available for many state-of-the-art CNNs. We compare FBS to a range of existing channel pruning and dynamic execution schemes and demonstrate large improvements on ImageNet classification. Experiments show that FBS can respectively provide 5× and 2× savings in compute on VGG-16 and ResNet-18, both with less than 0.6% top-5 accuracy loss",
    "checked": true,
    "id": "a055b9917759dd75811edbc8500ca247b457c5b2",
    "semantic_title": "dynamic channel pruning: feature boosting and suppression",
    "citation_count": 318,
    "authors": []
  },
  "https://openreview.net/forum?id=SJl2niR9KQ": {
    "title": "Beyond Pixel Norm-Balls: Parametric Adversaries using an Analytically Differentiable Renderer",
    "volume": "poster",
    "abstract": "Many machine learning image classifiers are vulnerable to adversarial attacks, inputs with perturbations designed to intentionally trigger misclassification. Current adversarial methods directly alter pixel colors and evaluate against pixel norm-balls: pixel perturbations smaller than a specified magnitude, according to a measurement norm. This evaluation, however, has limited practical utility since perturbations in the pixel space do not correspond to underlying real-world phenomena of image formation that lead to them and has no security motivation attached. Pixels in natural images are measurements of light that has interacted with the geometry of a physical scene. As such, we propose a novel evaluation measure, parametric norm-balls, by directly perturbing physical parameters that underly image formation. One enabling contribution we present is a physically-based differentiable renderer that allows us to propagate pixel gradients to the parametric space of lighting and geometry. Our approach enables physically-based adversarial attacks, and our differentiable renderer leverages models from the interactive rendering literature to balance the performance and accuracy trade-offs necessary for a memory-efficient and scalable adversarial data augmentation workflow",
    "checked": true,
    "id": "742af5aa06ed8de12f8f77e248ff3fe5700061ff",
    "semantic_title": "beyond pixel norm-balls: parametric adversaries using an analytically differentiable renderer",
    "citation_count": 114,
    "authors": []
  },
  "https://openreview.net/forum?id=Hygn2o0qKX": {
    "title": "Deterministic PAC-Bayesian generalization bounds for deep networks via generalizing noise-resilience",
    "volume": "poster",
    "abstract": "The ability of overparameterized deep networks to generalize well has been linked to the fact that stochastic gradient descent (SGD) finds solutions that lie in flat, wide minima in the training loss -- minima where the output of the network is resilient to small random noise added to its parameters. So far this observation has been used to provide generalization guarantees only for neural networks whose parameters are either \\textit{stochastic} or \\textit{compressed}. In this work, we present a general PAC-Bayesian framework that leverages this observation to provide a bound on the original network learned -- a network that is deterministic and uncompressed. What enables us to do this is a key novelty in our approach: our framework allows us to show that if on training data, the interactions between the weight matrices satisfy certain conditions that imply a wide training loss minimum, these conditions themselves {\\em generalize} to the interactions between the matrices on test data, thereby implying a wide test loss minimum. We then apply our general framework in a setup where we assume that the pre-activation values of the network are not too small (although we assume this only on the training data). In this setup, we provide a generalization guarantee for the original (deterministic, uncompressed) network, that does not scale with product of the spectral norms of the weight matrices -- a guarantee that would not have been possible with prior approaches",
    "checked": true,
    "id": "0204871837acb118871e8d1bb59407da73142333",
    "semantic_title": "deterministic pac-bayesian generalization bounds for deep networks via generalizing noise-resilience",
    "citation_count": 101,
    "authors": []
  },
  "https://openreview.net/forum?id=r1lohoCqY7": {
    "title": "Learning-Based Frequency Estimation Algorithms",
    "volume": "poster",
    "abstract": "Estimating the frequencies of elements in a data stream is a fundamental task in data analysis and machine learning. The problem is typically addressed using streaming algorithms which can process very large data using limited storage. Today's streaming algorithms, however, cannot exploit patterns in their input to improve performance. We propose a new class of algorithms that automatically learn relevant patterns in the input data and use them to improve its frequency estimates. The proposed algorithms combine the benefits of machine learning with the formal guarantees available through algorithm theory. We prove that our learning-based algorithms have lower estimation errors than their non-learning counterparts. We also evaluate our algorithms on two real-world datasets and demonstrate empirically their performance gains",
    "checked": true,
    "id": "72c969a5dc5b236d511fbdaae88c443d14145ae8",
    "semantic_title": "learning-based frequency estimation algorithms",
    "citation_count": 160,
    "authors": []
  },
  "https://openreview.net/forum?id=Syxt2jC5FX": {
    "title": "From Hard to Soft: Understanding Deep Network Nonlinearities via Vector Quantization and Statistical Inference",
    "volume": "poster",
    "abstract": "Nonlinearity is crucial to the performance of a deep (neural) network (DN). To date there has been little progress understanding the menagerie of available nonlinearities, but recently progress has been made on understanding the r\\^{o}le played by piecewise affine and convex nonlinearities like the ReLU and absolute value activation functions and max-pooling. In particular, DN layers constructed from these operations can be interpreted as {\\em max-affine spline operators} (MASOs) that have an elegant link to vector quantization (VQ) and $K$-means. While this is good theoretical progress, the entire MASO approach is predicated on the requirement that the nonlinearities be piecewise affine and convex, which precludes important activation functions like the sigmoid, hyperbolic tangent, and softmax. {\\em This paper extends the MASO framework to these and an infinitely large class of new nonlinearities by linking deterministic MASOs with probabilistic Gaussian Mixture Models (GMMs).} We show that, under a GMM, piecewise affine, convex nonlinearities like ReLU, absolute value, and max-pooling can be interpreted as solutions to certain natural ``hard'' VQ inference problems, while sigmoid, hyperbolic tangent, and softmax can be interpreted as solutions to corresponding ``soft'' VQ inference problems. We further extend the framework by hybridizing the hard and soft VQ optimizations to create a $\\beta$-VQ inference that interpolates between hard, soft, and linear VQ inference. A prime example of a $\\beta$-VQ DN nonlinearity is the {\\em swish} nonlinearity, which offers state-of-the-art performance in a range of computer vision tasks but was developed ad hoc by experimentation. Finally, we validate with experiments an important assertion of our theory, namely that DN performance can be significantly improved by enforcing orthogonality in its linear filters",
    "checked": true,
    "id": "c630f37bf7d5ab6979216ca426e4620f104fba89",
    "semantic_title": "from hard to soft: understanding deep network nonlinearities via vector quantization and statistical inference",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=SkeK3s0qKQ": {
    "title": "Episodic Curiosity through Reachability",
    "volume": "poster",
    "abstract": "Rewards are sparse in the real world and most of today's reinforcement learning algorithms struggle with such sparsity. One solution to this problem is to allow the agent to create rewards for itself - thus making rewards dense and more suitable for learning. In particular, inspired by curious behaviour in animals, observing something novel could be rewarded with a bonus. Such bonus is summed up with the real task reward - making it possible for RL algorithms to learn from the combined reward. We propose a new curiosity method which uses episodic memory to form the novelty bonus. To determine the bonus, the current observation is compared with the observations in memory. Crucially, the comparison is done based on how many environment steps it takes to reach the current observation from those in memory - which incorporates rich information about environment dynamics. This allows us to overcome the known \"couch-potato\" issues of prior work - when the agent finds a way to instantly gratify itself by exploiting actions which lead to hardly predictable consequences. We test our approach in visually rich 3D environments in ViZDoom, DMLab and MuJoCo. In navigational tasks from ViZDoom and DMLab, our agent outperforms the state-of-the-art curiosity method ICM. In MuJoCo, an ant equipped with our curiosity module learns locomotion out of the first-person-view curiosity only. The code is available at https://github.com/google-research/episodic-curiosity/",
    "checked": true,
    "id": "fdfeeb14bbde2ab31b18e56b92d362dcd1b14f71",
    "semantic_title": "episodic curiosity through reachability",
    "citation_count": 271,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgK3oC5Fm": {
    "title": "Bayesian Prediction of Future Street Scenes using Synthetic Likelihoods",
    "volume": "poster",
    "abstract": "For autonomous agents to successfully operate in the real world, the ability to anticipate future scene states is a key competence. In real-world scenarios, future states become increasingly uncertain and multi-modal, particularly on long time horizons. Dropout based Bayesian inference provides a computationally tractable, theoretically well grounded approach to learn different hypotheses/models to deal with uncertain futures and make predictions that correspond well to observations -- are well calibrated. However, it turns out that such approaches fall short to capture complex real-world scenes, even falling behind in accuracy when compared to the plain deterministic approaches. This is because the used log-likelihood estimate discourages diversity. In this work, we propose a novel Bayesian formulation for anticipating future scene states which leverages synthetic likelihoods that encourage the learning of diverse models to accurately capture the multi-modal nature of future scene states. We show that our approach achieves accurate state-of-the-art predictions and calibrated probabilities through extensive experiments for scene anticipation on Cityscapes dataset. Moreover, we show that our approach generalizes across diverse tasks such as digit generation and precipitation forecasting",
    "checked": true,
    "id": "1c57f1d3c1191df543f81fb56c2ec5ded1bc5488",
    "semantic_title": "bayesian prediction of future street scenes using synthetic likelihoods",
    "citation_count": 46,
    "authors": []
  },
  "https://openreview.net/forum?id=S1eK3i09YQ": {
    "title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks",
    "volume": "poster",
    "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function. Our analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods",
    "checked": true,
    "id": "d6f6d1504cfedde4efb23e7ec0f42f006062c6a0",
    "semantic_title": "gradient descent provably optimizes over-parameterized neural networks",
    "citation_count": 1276,
    "authors": []
  },
  "https://openreview.net/forum?id=SJGvns0qK7": {
    "title": "Bayesian Policy Optimization for Model Uncertainty",
    "volume": "poster",
    "abstract": "Addressing uncertainty is critical for autonomous systems to robustly adapt to the real world. We formulate the problem of model uncertainty as a continuous Bayes-Adaptive Markov Decision Process (BAMDP), where an agent maintains a posterior distribution over latent model parameters given a history of observations and maximizes its expected long-term reward with respect to this belief distribution. Our algorithm, Bayesian Policy Optimization, builds on recent policy optimization algorithms to learn a universal policy that navigates the exploration-exploitation trade-off to maximize the Bayesian value function. To address challenges from discretizing the continuous latent parameter space, we propose a new policy network architecture that encodes the belief distribution independently from the observable state. Our method significantly outperforms algorithms that address model uncertainty without explicitly reasoning about belief distributions and is competitive with state-of-the-art Partially Observable Markov Decision Process solvers",
    "checked": true,
    "id": "4722d3e408c3687028e7f52ea6b7bf6b69981764",
    "semantic_title": "bayesian policy optimization for model uncertainty",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=rJlDnoA5Y7": {
    "title": "Von Mises-Fisher Loss for Training Sequence to Sequence Models with Continuous Outputs",
    "volume": "poster",
    "abstract": "The Softmax function is used in the final layer of nearly all existing sequence-to-sequence models for language generation. However, it is usually the slowest layer to compute which limits the vocabulary size to a subset of most frequent types; and it has a large memory footprint. We propose a general technique for replacing the softmax layer with a continuous embedding layer. Our primary innovations are a novel probabilistic loss, and a training and inference procedure in which we generate a probability distribution over pre-trained word embeddings, instead of a multinomial distribution over the vocabulary obtained via softmax. We evaluate this new class of sequence-to-sequence models with continuous outputs on the task of neural machine translation. We show that our models obtain upto 2.5x speed-up in training time while performing on par with the state-of-the-art models in terms of translation quality. These models are capable of handling very large vocabularies without compromising on translation quality. They also produce more meaningful errors than in the softmax-based models, as these errors typically lie in a subspace of the vector space of the reference translations",
    "checked": true,
    "id": "3f67f9c8a97dc2a0b7c40d1ced73294133e67aa1",
    "semantic_title": "von mises-fisher loss for training sequence to sequence models with continuous outputs",
    "citation_count": 71,
    "authors": []
  },
  "https://openreview.net/forum?id=Byx83s09Km": {
    "title": "Information-Directed Exploration for Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "Efficient exploration remains a major challenge for reinforcement learning. One reason is that the variability of the returns often depends on the current state and action, and is therefore heteroscedastic. Classical exploration strategies such as upper confidence bound algorithms and Thompson sampling fail to appropriately account for heteroscedasticity, even in the bandit setting. Motivated by recent findings that address this issue in bandits, we propose to use Information-Directed Sampling (IDS) for exploration in reinforcement learning. As our main contribution, we build on recent advances in distributional reinforcement learning and propose a novel, tractable approximation of IDS for deep Q-learning. The resulting exploration strategy explicitly accounts for both parametric uncertainty and heteroscedastic observation noise. We evaluate our method on Atari games and demonstrate a significant improvement over alternative approaches",
    "checked": true,
    "id": "a25b645c3d24f91164230a0ac5bb2d4ec88c1538",
    "semantic_title": "information-directed exploration for deep reinforcement learning",
    "citation_count": 72,
    "authors": []
  },
  "https://openreview.net/forum?id=r1gNni0qtm": {
    "title": "Generalized Tensor Models for Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "Recurrent Neural Networks (RNNs) are very successful at solving challenging problems with sequential data. However, this observed efficiency is not yet entirely explained by theory. It is known that a certain class of multiplicative RNNs enjoys the property of depth efficiency --- a shallow network of exponentially large width is necessary to realize the same score function as computed by such an RNN. Such networks, however, are not very often applied to real life tasks. In this work, we attempt to reduce the gap between theory and practice by extending the theoretical analysis to RNNs which employ various nonlinearities, such as Rectified Linear Unit (ReLU), and show that they also benefit from properties of universality and depth efficiency. Our theoretical results are verified by a series of extensive computational experiments",
    "checked": true,
    "id": "bfd01ae65d568801b908c6d5bb0e0704c9c5feab",
    "semantic_title": "generalized tensor models for recurrent neural networks",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=Syx72jC9tm": {
    "title": "Invariant and Equivariant Graph Networks",
    "volume": "poster",
    "abstract": "Invariant and equivariant networks have been successfully used for learning images, sets, point clouds, and graphs. A basic challenge in developing such networks is finding the maximal collection of invariant and equivariant \\emph{linear} layers. Although this question is answered for the first three examples (for popular transformations, at-least), a full characterization of invariant and equivariant linear layers for graphs is not known. In this paper we provide a characterization of all permutation invariant and equivariant linear layers for (hyper-)graph data, and show that their dimension, in case of edge-value graph data, is $2$ and $15$, respectively. More generally, for graph data defined on $k$-tuples of nodes, the dimension is the $k$-th and $2k$-th Bell numbers. Orthogonal bases for the layers are computed, including generalization to multi-graph data. The constant number of basis elements and their characteristics allow successfully applying the networks to different size graphs. From the theoretical point of view, our results generalize and unify recent advancement in equivariant deep learning. In particular, we show that our model is capable of approximating any message passing neural network. Applying these new linear layers in a simple deep neural network framework is shown to achieve comparable results to state-of-the-art and to have better expressivity than previous invariant and equivariant bases",
    "checked": true,
    "id": "6541afa4b4061a7d5c8387514bedea9dc249fd80",
    "semantic_title": "invariant and equivariant graph networks",
    "citation_count": 508,
    "authors": []
  },
  "https://openreview.net/forum?id=r1l73iRqKm": {
    "title": "Wizard of Wikipedia: Knowledge-Powered Conversational Agents",
    "volume": "poster",
    "abstract": "In open-domain dialogue intelligent agents should exhibit the use of knowledge, however there are few convincing demonstrations of this to date. The most popular sequence to sequence models typically \"generate and hope\" generic utterances that can be memorized in the weights of the model when mapping from input utterance(s) to output, rather than employing recalled knowledge as context. Use of knowledge has so far proved difficult, in part because of the lack of a supervised learning benchmark task which exhibits knowledgeable open dialogue with clear grounding. To that end we collect and release a large dataset with conversations directly grounded with knowledge retrieved from Wikipedia. We then design architectures capable of retrieving knowledge, reading and conditioning on it, and finally generating natural responses. Our best performing dialogue models are able to conduct knowledgeable discussions on open-domain topics as evaluated by automatic metrics and human evaluations, while our new benchmark allows for measuring further improvements in this important research direction",
    "checked": true,
    "id": "227458886343b86bd15adf58c769be326b4b058a",
    "semantic_title": "wizard of wikipedia: knowledge-powered conversational agents",
    "citation_count": 951,
    "authors": []
  },
  "https://openreview.net/forum?id=HkeGhoA5FX": {
    "title": "Residual Non-local Attention Networks for Image Restoration",
    "volume": "poster",
    "abstract": "In this paper, we propose a residual non-local attention network for high-quality image restoration. Without considering the uneven distribution of information in the corrupted images, previous methods are restricted by local convolutional operation and equal treatment of spatial- and channel-wise features. To address this issue, we design local and non-local attention blocks to extract features that capture the long-range dependencies between pixels and pay more attention to the challenging parts. Specifically, we design trunk branch and (non-)local mask branch in each (non-)local attention block. The trunk branch is used to extract hierarchical features. Local and non-local mask branches aim to adaptively rescale these hierarchical features with mixed attentions. The local mask branch concentrates on more local structures with convolutional operations, while non-local attention considers more about long-range dependencies in the whole feature map. Furthermore, we propose residual local and non-local attention learning to train the very deep network, which further enhance the representation ability of the network. Our proposed method can be generalized for various image restoration applications, such as image denoising, demosaicing, compression artifacts reduction, and super-resolution. Experiments demonstrate that our method obtains comparable or better results compared with recently leading methods quantitatively and visually",
    "checked": true,
    "id": "34439db81b482cd562e1cdba974c70a2b89cd6d4",
    "semantic_title": "residual non-local attention networks for image restoration",
    "citation_count": 678,
    "authors": []
  },
  "https://openreview.net/forum?id=ryGfnoC5KQ": {
    "title": "Kernel RNN Learning (KeRNL)",
    "volume": "poster",
    "abstract": "We describe Kernel RNN Learning (KeRNL), a reduced-rank, temporal eligibility trace-based approximation to backpropagation through time (BPTT) for training recurrent neural networks (RNNs) that gives competitive performance to BPTT on long time-dependence tasks. The approximation replaces a rank-4 gradient learning tensor, which describes how past hidden unit activations affect the current state, by a simple reduced-rank product of a sensitivity weight and a temporal eligibility trace. In this structured approximation motivated by node perturbation, the sensitivity weights and eligibility kernel time scales are themselves learned by applying perturbations. The rule represents another step toward biologically plausible or neurally inspired ML, with lower complexity in terms of relaxed architectural requirements (no symmetric return weights), a smaller memory demand (no unfolding and storage of states over time), and a shorter feedback time",
    "checked": true,
    "id": "c3e47eaff722e3950f5e2f9d8eb00fabf4d4d636",
    "semantic_title": "kernel rnn learning (kernl)",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=S1zz2i0cY7": {
    "title": "Integer Networks for Data Compression with Latent-Variable Models",
    "volume": "poster",
    "abstract": "We consider the problem of using variational latent-variable models for data compression. For such models to produce a compressed binary sequence, which is the universal data representation in a digital world, the latent representation needs to be subjected to entropy coding. Range coding as an entropy coding technique is optimal, but it can fail catastrophically if the computation of the prior differs even slightly between the sending and the receiving side. Unfortunately, this is a common scenario when floating point math is used and the sender and receiver operate on different hardware or software platforms, as numerical round-off is often platform dependent. We propose using integer networks as a universal solution to this problem, and demonstrate that they enable reliable cross-platform encoding and decoding of images using variational models",
    "checked": true,
    "id": "2405cb80949231747aa09af3fa8a11d5fff512d4",
    "semantic_title": "integer networks for data compression with latent-variable models",
    "citation_count": 71,
    "authors": []
  },
  "https://openreview.net/forum?id=BkgzniCqY7": {
    "title": "Structured Adversarial Attack: Towards General Implementation and Better Interpretability",
    "volume": "poster",
    "abstract": "When generating adversarial examples to attack deep neural networks (DNNs), Lp norm of the added perturbation is usually used to measure the similarity between original image and adversarial example. However, such adversarial attacks perturbing the raw input spaces may fail to capture structural information hidden in the input. This work develops a more general attack model, i.e., the structured attack (StrAttack), which explores group sparsity in adversarial perturbation by sliding a mask through images aiming for extracting key spatial structures. An ADMM (alternating direction method of multipliers)-based framework is proposed that can split the original problem into a sequence of analytically solvable subproblems and can be generalized to implement other attacking methods. Strong group sparsity is achieved in adversarial perturbations even with the same level of Lp-norm distortion (p∈ {1,2,∞}) as the state-of-the-art attacks. We demonstrate the effectiveness of StrAttack by extensive experimental results on MNIST, CIFAR-10 and ImageNet. We also show that StrAttack provides better interpretability (i.e., better correspondence with discriminative image regions) through adversarial saliency map (Paper-not et al., 2016b) and class activation map (Zhou et al., 2016)",
    "checked": true,
    "id": "5bc67a8a47c796053d5ed77aaecd3cbbd4c5d4c1",
    "semantic_title": "structured adversarial attack: towards general implementation and better interpretability",
    "citation_count": 162,
    "authors": []
  },
  "https://openreview.net/forum?id=r1e13s05YX": {
    "title": "Neural network gradient-based learning of black-box function interfaces",
    "volume": "poster",
    "abstract": "Deep neural networks work well at approximating complicated functions when provided with data and trained by gradient descent methods. At the same time, there is a vast amount of existing functions that programmatically solve different tasks in a precise manner eliminating the need for training. In many cases, it is possible to decompose a task to a series of functions, of which for some we may prefer to use a neural network to learn the functionality, while for others the preferred method would be to use existing black-box functions. We propose a method for end-to-end training of a base neural network that integrates calls to existing black-box functions. We do so by approximating the black-box functionality with a differentiable neural network in a way that drives the base network to comply with the black-box function interface during the end-to-end optimization process. At inference time, we replace the differentiable estimator with its external black-box non-differentiable counterpart such that the base network output matches the input arguments of the black-box function. Using this ``Estimate and Replace'' paradigm, we train a neural network, end to end, to compute the input to black-box functionality while eliminating the need for intermediate labels. We show that by leveraging the existing precise black-box function during inference, the integrated model generalizes better than a fully differentiable model, and learns more efficiently compared to RL-based methods",
    "checked": true,
    "id": "25d5a513f7534c157ab1381d51ffe294773ffc91",
    "semantic_title": "neural network gradient-based learning of black-box function interfaces",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=BJx0sjC5FX": {
    "title": "RNNs implicitly implement tensor-product representations",
    "volume": "poster",
    "abstract": "Recurrent neural networks (RNNs) can learn continuous vector representations of symbolic structures such as sequences and sentences; these representations often exhibit linear regularities (analogies). Such regularities motivate our hypothesis that RNNs that show such regularities implicitly compile symbolic structures into tensor product representations (TPRs; Smolensky, 1990), which additively combine tensor products of vectors representing roles (e.g., sequence positions) and vectors representing fillers (e.g., particular words). To test this hypothesis, we introduce Tensor Product Decomposition Networks (TPDNs), which use TPRs to approximate existing vector representations. We demonstrate using synthetic data that TPDNs can successfully approximate linear and tree-based RNN autoencoder representations, suggesting that these representations exhibit interpretable compositional structure; we explore the settings that lead RNNs to induce such structure-sensitive representations. By contrast, further TPDN experiments show that the representations of four models trained to encode naturally-occurring sentences can be largely approximated with a bag of words, with only marginal improvements from more sophisticated structures. We conclude that TPDNs provide a powerful method for interpreting vector representations, and that standard RNNs can induce compositional sequence representations that are remarkably well approximated byTPRs; at the same time, existing training tasks for sentence representation learning may not be sufficient for inducing robust structural representations",
    "checked": false,
    "id": "d3ded34ff3378aadaa9a7c10e51cef6d04391a86",
    "semantic_title": "rnns implicitly implement tensor product representations",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=r1GAsjC5Fm": {
    "title": "Self-Monitoring Navigation Agent via Auxiliary Progress Estimation",
    "volume": "poster",
    "abstract": "The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at https://github.com/chihyaoma/selfmonitoring-agent",
    "checked": true,
    "id": "29e13746fa5aed13e51558a521a39aaeaa99c1b1",
    "semantic_title": "self-monitoring navigation agent via auxiliary progress estimation",
    "citation_count": 278,
    "authors": []
  },
  "https://openreview.net/forum?id=H1g6osRcFQ": {
    "title": "Policy Transfer with Strategy Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0e31f993257d0b25d7c18eb75a4ba39f7563bd81",
    "semantic_title": "policy transfer with strategy optimization",
    "citation_count": 81,
    "authors": []
  },
  "https://openreview.net/forum?id=rJg6ssC5Y7": {
    "title": "DeepOBS: A Deep Learning Optimizer Benchmark Suite",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5794c7f8d703b91804c22f1ce37cb9288eaedc60",
    "semantic_title": "deepobs: a deep learning optimizer benchmark suite",
    "citation_count": 71,
    "authors": []
  },
  "https://openreview.net/forum?id=BJxhijAcY7": {
    "title": "signSGD with Majority Vote is Communication Efficient and Fault Tolerant",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "dbca9dbe14e9933515d2005dc1163ae2c24d9afd",
    "semantic_title": "signsgd with majority vote is communication efficient and fault tolerant",
    "citation_count": 244,
    "authors": []
  },
  "https://openreview.net/forum?id=HkljioCcFQ": {
    "title": "MARGINALIZED AVERAGE ATTENTIONAL NETWORK FOR WEAKLY-SUPERVISED LEARNING",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6c97556edbc192896cc55395f8f21fe0ff148580",
    "semantic_title": "marginalized average attentional network for weakly-supervised learning",
    "citation_count": 82,
    "authors": []
  },
  "https://openreview.net/forum?id=SyGjjsC5tQ": {
    "title": "Stable Opponent Shaping in Differentiable Games",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3cf906d2cc57f07244cd1f74ccb1ace8e70073cc",
    "semantic_title": "stable opponent shaping in differentiable games",
    "citation_count": 110,
    "authors": []
  },
  "https://openreview.net/forum?id=BJxssoA5KX": {
    "title": "Bounce and Learn: Modeling Scene Dynamics with Real-World Bounces",
    "volume": "poster",
    "abstract": "We introduce an approach to model surface properties governing bounces in everyday scenes. Our model learns end-to-end, starting from sensor inputs, to predict post-bounce trajectories and infer two underlying physical properties that govern bouncing - restitution and effective collision normals. Our model, Bounce and Learn, comprises two modules -- a Physics Inference Module (PIM) and a Visual Inference Module (VIM). VIM learns to infer physical parameters for locations in a scene given a single still image, while PIM learns to model physical interactions for the prediction task given physical parameters and observed pre-collision 3D trajectories. To achieve our results, we introduce the Bounce Dataset comprising 5K RGB-D videos of bouncing trajectories of a foam ball to probe surfaces of varying shapes and materials in everyday scenes including homes and offices. Our proposed model learns from our collected dataset of real-world bounces and is bootstrapped with additional information from simple physics simulations. We show on our newly collected dataset that our model out-performs baselines, including trajectory fitting with Newtonian physics, in predicting post-bounce trajectories and inferring physical properties of a scene",
    "checked": true,
    "id": "b39399b1b7c8d2950109b645552439a712913bf1",
    "semantic_title": "bounce and learn: modeling scene dynamics with real-world bounces",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=H1eqjiCctX": {
    "title": "Understanding Composition of Word Embeddings via Tensor Decomposition",
    "volume": "poster",
    "abstract": "Word embedding is a powerful tool in natural language processing. In this paper we consider the problem of word embedding composition \\--- given vector representations of two words, compute a vector for the entire phrase. We give a generative model that can capture specific syntactic relations between words. Under our model, we prove that the correlations between three words (measured by their PMI) form a tensor that has an approximate low rank Tucker decomposition. The result of the Tucker decomposition gives the word embeddings as well as a core tensor, which can be used to produce better compositions of the word embeddings. We also complement our theoretical results with experiments that verify our assumptions, and demonstrate the effectiveness of the new composition method",
    "checked": true,
    "id": "1ee096afc761526e4be9e3af11d6287d4a0a5393",
    "semantic_title": "understanding composition of word embeddings via tensor decomposition",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=rylqooRqK7": {
    "title": "SNAS: stochastic neural architecture search",
    "volume": "poster",
    "abstract": "We propose Stochastic Neural Architecture Search (SNAS), an economical end-to-end solution to Neural Architecture Search (NAS) that trains neural operation parameters and architecture distribution parameters in same round of back-propagation, while maintaining the completeness and differentiability of the NAS pipeline. In this work, NAS is reformulated as an optimization problem on parameters of a joint distribution for the search space in a cell. To leverage the gradient information in generic differentiable loss for architecture search, a novel search gradient is proposed. We prove that this search gradient optimizes the same objective as reinforcement-learning-based NAS, but assigns credits to structural decisions more efficiently. This credit assignment is further augmented with locally decomposable reward to enforce a resource-efficient constraint. In experiments on CIFAR-10, SNAS takes less epochs to find a cell architecture with state-of-the-art accuracy than non-differentiable evolution-based and reinforcement-learning-based NAS, which is also transferable to ImageNet. It is also shown that child networks of SNAS can maintain the validation accuracy in searching, with which attention-based NAS requires parameter retraining to compete, exhibiting potentials to stride towards efficient NAS on big datasets",
    "checked": true,
    "id": "3f0a2de309f21a957b4741dd68007eb08d9b12e3",
    "semantic_title": "snas: stochastic neural architecture search",
    "citation_count": 940,
    "authors": []
  },
  "https://openreview.net/forum?id=HJGciiR5Y7": {
    "title": "Latent Convolutional Models",
    "volume": "poster",
    "abstract": "We present a new latent model of natural images that can be learned on large-scale datasets. The learning process provides a latent embedding for every image in the training dataset, as well as a deep convolutional network that maps the latent space to the image space. After training, the new model provides a strong and universal image prior for a variety of image restoration tasks such as large-hole inpainting, superresolution, and colorization. To model high-resolution natural images, our approach uses latent spaces of very high dimensionality (one to two orders of magnitude higher than previous latent image models). To tackle this high dimensionality, we use latent spaces with a special manifold structure (convolutional manifolds) parameterized by a ConvNet of a certain architecture. In the experiments, we compare the learned latent models with latent models learned by autoencoders, advanced variants of generative adversarial networks, and a strong baseline system using simpler parameterization of the latent space. Our model outperforms the competing approaches over a range of restoration tasks",
    "checked": true,
    "id": "dcf4a22445f4486932fffd7014c320fa9bd91011",
    "semantic_title": "latent convolutional models",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=rkxciiC9tm": {
    "title": "NADPEx: An on-policy temporally consistent exploration method for deep reinforcement learning",
    "volume": "poster",
    "abstract": "Reinforcement learning agents need exploratory behaviors to escape from local optima. These behaviors may include both immediate dithering perturbation and temporally consistent exploration. To achieve these, a stochastic policy model that is inherently consistent through a period of time is in desire, especially for tasks with either sparse rewards or long term information. In this work, we introduce a novel on-policy temporally consistent exploration strategy - Neural Adaptive Dropout Policy Exploration (NADPEx) - for deep reinforcement learning agents. Modeled as a global random variable for conditional distribution, dropout is incorporated to reinforcement learning policies, equipping them with inherent temporal consistency, even when the reward signals are sparse. Two factors, gradients' alignment with the objective and KL constraint in policy space, are discussed to guarantee NADPEx policy's stable improvement. Our experiments demonstrate that NADPEx solves tasks with sparse reward while naive exploration and parameter noise fail. It yields as well or even faster convergence in the standard mujoco benchmark for continuous control",
    "checked": true,
    "id": "a88d54c168a84ed8a04d2a32be0b5939586b5792",
    "semantic_title": "nadpex: an on-policy temporally consistent exploration method for deep reinforcement learning",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=SkEYojRqtm": {
    "title": "Representation Degeneration Problem in Training Natural Language Generation Models",
    "volume": "poster",
    "abstract": "We study an interesting problem in training neural network-based models for natural language generation tasks, which we call the \\emph{representation degeneration problem}. We observe that when training a model for natural language generation tasks through likelihood maximization with the weight tying trick, especially with big training datasets, most of the learnt word embeddings tend to degenerate and be distributed into a narrow cone, which largely limits the representation power of word embeddings. We analyze the conditions and causes of this problem and propose a novel regularization method to address it. Experiments on language modeling and machine translation show that our method can largely mitigate the representation degeneration problem and achieve better performance than baseline algorithms",
    "checked": true,
    "id": "55e960535f643637161b2e99a8c21a92c0d13757",
    "semantic_title": "representation degeneration problem in training natural language generation models",
    "citation_count": 270,
    "authors": []
  },
  "https://openreview.net/forum?id=HyEtjoCqFX": {
    "title": "Soft Q-Learning with Mutual-Information Regularization",
    "volume": "poster",
    "abstract": "We propose a reinforcement learning (RL) algorithm that uses mutual-information regularization to optimize a prior action distribution for better performance and exploration. Entropy-based regularization has previously been shown to improve both exploration and robustness in challenging sequential decision-making tasks. It does so by encouraging policies to put probability mass on all actions. However, entropy regularization might be undesirable when actions have significantly different importance. In this paper, we propose a theoretically motivated framework that dynamically weights the importance of actions by using the mutual-information. In particular, we express the RL problem as an inference problem where the prior probability distribution over actions is subject to optimization. We show that the prior optimization introduces a mutual-information regularizer in the RL objective. This regularizer encourages the policy to be close to a non-uniform distribution that assigns higher probability mass to more important actions. We empirically demonstrate that our method significantly improves over entropy regularization methods and unregularized methods",
    "checked": true,
    "id": "b90793b029b2fa64e94d1ee7d5d2c7e1d1fcaaa9",
    "semantic_title": "soft q-learning with mutual-information regularization",
    "citation_count": 64,
    "authors": []
  },
  "https://openreview.net/forum?id=HyztsoC5Y7": {
    "title": "Learning to Adapt in Dynamic, Real-World Environments through Meta-Reinforcement Learning",
    "volume": "poster",
    "abstract": "Although reinforcement learning methods can achieve impressive results in simulation, the real world presents two major challenges: generating samples is exceedingly expensive, and unexpected perturbations or unseen situations cause proficient but specialized policies to fail at test time. Given that it is impractical to train separate policies to accommodate all situations the agent may see in the real world, this work proposes to learn how to quickly and effectively adapt online to new tasks. To enable sample-efficient learning, we consider learning online adaptation in the context of model-based reinforcement learning. Our approach uses meta-learning to train a dynamics model prior such that, when combined with recent data, this prior can be rapidly adapted to the local context. Our experiments demonstrate online adaptation for continuous control tasks on both simulated and real-world agents. We first show simulated agents adapting their behavior online to novel terrains, crippled body parts, and highly-dynamic environments. We also illustrate the importance of incorporating online adaptation into autonomous agents that operate in the real world by applying our method to a real dynamic legged millirobot: We demonstrate the agent's learned ability to quickly adapt online to a missing leg, adjust to novel terrains and slopes, account for miscalibration or errors in pose estimation, and compensate for pulling payloads",
    "checked": true,
    "id": "944bd3b472c8a30163bbfc1b5cbab8545693c3e0",
    "semantic_title": "learning to adapt in dynamic, real-world environments through meta-reinforcement learning",
    "citation_count": 554,
    "authors": []
  },
  "https://openreview.net/forum?id=SylKoo0cKm": {
    "title": "How Important is a Neuron",
    "volume": "poster",
    "abstract": "The problem of attributing a deep network's prediction to its input/base features is well-studied (cf. Simonyan et al. (2013)). We introduce the notion of conductance to extend the notion of attribution to understanding the importance of hidden units. Informally, the conductance of a hidden unit of a deep network is the flow of attribution via this hidden unit. We can use conductance to understand the importance of a hidden unit to the prediction for a specific input, or over a set of inputs. We justify conductance in multiple ways via a qualitative comparison with other methods, via some axiomatic results, and via an empirical evaluation based on a feature selection task. The empirical evaluations are done using the Inception network over ImageNet data, and a convolutinal network over text data. In both cases, we demonstrate the effectiveness of conductance in identifying interesting insights about the internal workings of these networks",
    "checked": false,
    "id": "eb322f6f798fd1b381896d0b79f5498d89585b1f",
    "semantic_title": "how important is a neuron?",
    "citation_count": 131,
    "authors": []
  },
  "https://openreview.net/forum?id=BJeOioA9Y7": {
    "title": "Knowledge Flow: Improve Upon Your Teachers",
    "volume": "poster",
    "abstract": "A zoo of deep nets is available these days for almost any given task, and it is increasingly unclear which net to start with when addressing a new task, or which net to use as an initialization for fine-tuning a new model. To address this issue, in this paper, we develop knowledge flow which moves ‘knowledge' from multiple deep nets, referred to as teachers, to a new deep net model, called the student. The structure of the teachers and the student can differ arbitrarily and they can be trained on entirely different tasks with different output spaces too. Upon training with knowledge flow the student is independent of the teachers. We demonstrate our approach on a variety of supervised and reinforcement learning tasks, outperforming fine-tuning and other ‘knowledge exchange' methods",
    "checked": true,
    "id": "6dd986b2621d7f420ca80be5b71e12dece41e877",
    "semantic_title": "knowledge flow: improve upon your teachers",
    "citation_count": 62,
    "authors": []
  },
  "https://openreview.net/forum?id=BkG8sjR5Km": {
    "title": "Emergent Coordination Through Competition",
    "volume": "poster",
    "abstract": "We study the emergence of cooperative behaviors in reinforcement learning agents by introducing a challenging competitive multi-agent soccer environment with continuous simulated physics. We demonstrate that decentralized, population-based training with co-play can lead to a progression in agents' behaviors: from random, to simple ball chasing, and finally showing evidence of cooperation. Our study highlights several of the challenges encountered in large scale multi-agent training in continuous control. In particular, we demonstrate that the automatic optimization of simple shaping rewards, not themselves conducive to co-operative behavior, can lead to long-horizon team behavior. We further apply an evaluation scheme, grounded by game theoretic principals, that can assess agent performance in the absence of pre-defined evaluation tasks or human baselines",
    "checked": true,
    "id": "8a5c62f9c49a943b66fb1ae379442497609c8596",
    "semantic_title": "emergent coordination through competition",
    "citation_count": 151,
    "authors": []
  },
  "https://openreview.net/forum?id=Bkg8jjC9KQ": {
    "title": "Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile",
    "volume": "poster",
    "abstract": "Owing to their connection with generative adversarial networks (GANs), saddle-point problems have recently attracted considerable interest in machine learning and beyond. By necessity, most theoretical guarantees revolve around convex-concave (or even linear) problems; however, making theoretical inroads towards efficient GAN training depends crucially on moving beyond this classic framework. To make piecemeal progress along these lines, we analyze the behavior of mirror descent (MD) in a class of non-monotone problems whose solutions coincide with those of a naturally associated variational inequality – a property which we call coherence. We first show that ordinary, \"vanilla\" MD converges under a strict version of this condition, but not otherwise; in particular, it may fail to converge even in bilinear models with a unique solution. We then show that this deficiency is mitigated by optimism: by taking an \"extra-gradient\" step, optimistic mirror descent (OMD) converges in all coherent problems. Our analysis generalizes and extends the results of Daskalakis et al. [2018] for optimistic gradient descent (OGD) in bilinear problems, and makes concrete headway for provable convergence beyond convex-concave games. We also provide stochastic analogues of these results, and we validate our analysis by numerical experiments in a wide array of GAN models (including Gaussian mixture models, and the CelebA and CIFAR-10 datasets)",
    "checked": true,
    "id": "0ffb2d5bbbc4f2cc547569337bd20cc77c1111d7",
    "semantic_title": "optimistic mirror descent in saddle-point problems: going the extra (gradient) mile",
    "citation_count": 297,
    "authors": []
  },
  "https://openreview.net/forum?id=S1gUsoR9YX": {
    "title": "Multilingual Neural Machine Translation with Knowledge Distillation",
    "volume": "poster",
    "abstract": "Multilingual machine translation, which translates multiple languages with a single model, has attracted much attention due to its efficiency of offline training and online serving. However, traditional multilingual translation usually yields inferior accuracy compared with the counterpart using individual models for each language pair, due to language diversity and model capacity limitations. In this paper, we propose a distillation-based approach to boost the accuracy of multilingual machine translation. Specifically, individual models are first trained and regarded as teachers, and then the multilingual model is trained to fit the training data and match the outputs of individual models simultaneously through knowledge distillation. Experiments on IWSLT, WMT and Ted talk translation datasets demonstrate the effectiveness of our method. Particularly, we show that one model is enough to handle multiple languages (up to 44 languages in our experiment), with comparable or even better accuracy than individual models",
    "checked": true,
    "id": "1b24b7b4ac2427d20ab60c8451563eb8d99caf9c",
    "semantic_title": "multilingual neural machine translation with knowledge distillation",
    "citation_count": 250,
    "authors": []
  },
  "https://openreview.net/forum?id=H1ersoRqtm": {
    "title": "Structured Neural Summarization",
    "volume": "poster",
    "abstract": "Summarization of long sequences into a concise statement is a core problem in natural language processing, requiring non-trivial understanding of the input. Based on the promising results of graph neural networks on highly structured data, we develop a framework to extend existing sequence encoders with a graph component that can reason about long-distance relationships in weakly structured data such as text. In an extensive evaluation, we show that the resulting hybrid sequence-graph models outperform both pure sequence models as well as pure graph models on a range of summarization tasks",
    "checked": true,
    "id": "2225b7c480dc627e68f03e5321383f27e12cb1d7",
    "semantic_title": "structured neural summarization",
    "citation_count": 212,
    "authors": []
  },
  "https://openreview.net/forum?id=rJxHsjRqFQ": {
    "title": "Hyperbolic Attention Networks",
    "volume": "poster",
    "abstract": "Recent approaches have successfully demonstrated the benefits of learning the parameters of shallow networks in hyperbolic space. We extend this line of work by imposing hyperbolic geometry on the embeddings used to compute the ubiquitous attention mechanisms for different neural networks architectures. By only changing the geometry of embedding of object representations, we can use the embedding space more efficiently without increasing the number of parameters of the model. Mainly as the number of objects grows exponentially for any semantic distance from the query, hyperbolic geometry --as opposed to Euclidean geometry-- can encode those objects without having any interference. Our method shows improvements in generalization on neural machine translation on WMT'14 (English to German), learning on graphs (both on synthetic and real-world graph tasks) and visual question answering (CLEVR) tasks while keeping the neural representations compact",
    "checked": true,
    "id": "ebff4eb2f94dcf38171a5ca6a24ee95bc8e88c10",
    "semantic_title": "hyperbolic attention networks",
    "citation_count": 224,
    "authors": []
  },
  "https://openreview.net/forum?id=rkeSiiA5Fm": {
    "title": "Deep Learning 3D Shapes Using Alt-az Anisotropic 2-Sphere Convolution",
    "volume": "poster",
    "abstract": "The ground-breaking performance obtained by deep convolutional neural networks (CNNs) for image processing tasks is inspiring research efforts attempting to extend it for 3D geometric tasks. One of the main challenge in applying CNNs to 3D shape analysis is how to define a natural convolution operator on non-euclidean surfaces. In this paper, we present a method for applying deep learning to 3D surfaces using their spherical descriptors and alt-az anisotropic convolution on 2-sphere. A cascade set of geodesic disk filters rotate on the 2-sphere and collect spherical patterns and so to extract geometric features for various 3D shape analysis tasks. We demonstrate theoretically and experimentally that our proposed method has the possibility to bridge the gap between 2D images and 3D shapes with the desired rotation equivariance/invariance, and its effectiveness is evaluated in applications of non-rigid/ rigid shape classification and shape retrieval",
    "checked": true,
    "id": "6410edc9a0b4c1a71696b70d7bd2853566e13738",
    "semantic_title": "deep learning 3d shapes using alt-az anisotropic 2-sphere convolution",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=SkeVsiAcYm": {
    "title": "Generative predecessor models for sample-efficient imitation learning",
    "volume": "poster",
    "abstract": "We propose Generative Predecessor Models for Imitation Learning (GPRIL), a novel imitation learning algorithm that matches the state-action distribution to the distribution observed in expert demonstrations, using generative models to reason probabilistically about alternative histories of demonstrated states. We show that this approach allows an agent to learn robust policies using only a small number of expert demonstrations and self-supervised interactions with the environment. We derive this approach from first principles and compare it empirically to a state-of-the-art imitation learning method, showing that it outperforms or matches its performance on two simulated robot manipulation tasks and demonstrate significantly higher sample efficiency by applying the algorithm on a real robot",
    "checked": true,
    "id": "b6eae428eed3e5f1965a14dd2c26acf5400df473",
    "semantic_title": "generative predecessor models for sample-efficient imitation learning",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=rJlEojAqFm": {
    "title": "Relational Forward Models for Multi-Agent Learning",
    "volume": "poster",
    "abstract": "The behavioral dynamics of multi-agent systems have a rich and orderly structure, which can be leveraged to understand these systems, and to improve how artificial agents learn to operate in them. Here we introduce Relational Forward Models (RFM) for multi-agent learning, networks that can learn to make accurate predictions of agents' future behavior in multi-agent environments. Because these models operate on the discrete entities and relations present in the environment, they produce interpretable intermediate representations which offer insights into what drives agents' behavior, and what events mediate the intensity and valence of social interactions. Furthermore, we show that embedding RFM modules inside agents results in faster learning systems compared to non-augmented baselines. As more and more of the autonomous systems we develop and interact with become multi-agent in nature, developing richer analysis tools for characterizing how and why agents make decisions is increasingly necessary. Moreover, developing artificial agents that quickly and safely learn to coordinate with one another, and with humans in shared environments, is crucial",
    "checked": true,
    "id": "6ce08c7259b1dca6dd6b1e19f1037e974b12621e",
    "semantic_title": "relational forward models for multi-agent learning",
    "citation_count": 79,
    "authors": []
  },
  "https://openreview.net/forum?id=SJVmjjR9FX": {
    "title": "Variational Bayesian Phylogenetic Inference",
    "volume": "poster",
    "abstract": "Bayesian phylogenetic inference is currently done via Markov chain Monte Carlo with simple mechanisms for proposing new states, which hinders exploration efficiency and often requires long runs to deliver accurate posterior estimates. In this paper we present an alternative approach: a variational framework for Bayesian phylogenetic analysis. We approximate the true posterior using an expressive graphical model for tree distributions, called a subsplit Bayesian network, together with appropriate branch length distributions. We train the variational approximation via stochastic gradient ascent and adopt multi-sample based gradient estimators for different latent variables separately to handle the composite latent space of phylogenetic models. We show that our structured variational approximations are flexible enough to provide comparable posterior estimation to MCMC, while requiring less computation due to a more efficient tree exploration mechanism enabled by variational inference. Moreover, the variational approximations can be readily used for further statistical analysis such as marginal likelihood estimation for model comparison via importance sampling. Experiments on both synthetic data and real data Bayesian phylogenetic inference problems demonstrate the effectiveness and efficiency of our methods",
    "checked": true,
    "id": "c8d136aacf10bfade0630d796ca7f8716d867b51",
    "semantic_title": "variational bayesian phylogenetic inference",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=rk4Qso0cKm": {
    "title": "Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network",
    "volume": "poster",
    "abstract": "We present a new algorithm to train a robust neural network against adversarial attacks. Our algorithm is motivated by the following two ideas. First, although recent work has demonstrated that fusing randomness can improve the robustness of neural networks (Liu 2017), we noticed that adding noise blindly to all the layers is not the optimal way to incorporate randomness. Instead, we model randomness under the framework of Bayesian Neural Network (BNN) to formally learn the posterior distribution of models in a scalable way. Second, we formulate the mini-max problem in BNN to learn the best model distribution under adversarial attacks, leading to an adversarial-trained Bayesian neural net. Experiment results demonstrate that the proposed algorithm achieves state-of-the-art performance under strong attacks. On CIFAR-10 with VGG network, our model leads to 14% accuracy improvement compared with adversarial training (Madry 2017) and random self-ensemble (Liu, 2017) under PGD attack with 0.035 distortion, and the gap becomes even larger on a subset of ImageNet",
    "checked": true,
    "id": "c1f76891bdfa07d9a61ad11a15de13b139b20d2a",
    "semantic_title": "adv-bnn: improved adversarial defense through robust bayesian neural network",
    "citation_count": 171,
    "authors": []
  },
  "https://openreview.net/forum?id=ryf7ioRqFX": {
    "title": "h-detach: Modifying the LSTM Gradient Towards Better Optimization",
    "volume": "poster",
    "abstract": "Recurrent neural networks are known for their notorious exploding and vanishing gradient problem (EVGP). This problem becomes more evident in tasks where the information needed to correctly solve them exist over long time scales, because EVGP prevents important gradient components from being back-propagated adequately over a large number of steps. We introduce a simple stochastic algorithm (\\textit{h}-detach) that is specific to LSTM optimization and targeted towards addressing this problem. Specifically, we show that when the LSTM weights are large, the gradient components through the linear path (cell state) in the LSTM computational graph get suppressed. Based on the hypothesis that these components carry information about long term dependencies (which we show empirically), their suppression can prevent LSTMs from capturing them. Our algorithm\\footnote{Our code is available at https://github.com/bhargav104/h-detach.} prevents gradients flowing through this path from getting suppressed, thus allowing the LSTM to capture such dependencies better. We show significant improvements over vanilla LSTM gradient based training in terms of convergence speed, robustness to seed and learning rate, and generalization using our modification of LSTM gradient on various benchmark datasets",
    "checked": true,
    "id": "916a119cf9365fd5d5d7d8befdc95e1e69e895de",
    "semantic_title": "h-detach: modifying the lstm gradient towards better optimization",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=HJgXsjA5tQ": {
    "title": "On the loss landscape of a class of deep neural networks with no bad local valleys",
    "volume": "poster",
    "abstract": "We identify a class of over-parameterized deep neural networks with standard activation functions and cross-entropy loss which provably have no bad local valley, in the sense that from any point in parameter space there exists a continuous path on which the cross-entropy loss is non-increasing and gets arbitrarily close to zero. This implies that these networks have no sub-optimal strict local minima",
    "checked": true,
    "id": "e3300e3800fae17aa4a4e1e684359cbd272421d0",
    "semantic_title": "on the loss landscape of a class of deep neural networks with no bad local valleys",
    "citation_count": 87,
    "authors": []
  },
  "https://openreview.net/forum?id=BklMjsRqY7": {
    "title": "Accumulation Bit-Width Scaling For Ultra-Low Precision Training Of Deep Networks",
    "volume": "poster",
    "abstract": "Efforts to reduce the numerical precision of computations in deep learning training have yielded systems that aggressively quantize weights and activations, yet employ wide high-precision accumulators for partial sums in inner-product operations to preserve the quality of convergence. The absence of any framework to analyze the precision requirements of partial sum accumulations results in conservative design choices. This imposes an upper-bound on the reduction of complexity of multiply-accumulate units. We present a statistical approach to analyze the impact of reduced accumulation precision on deep learning training. Observing that a bad choice for accumulation precision results in loss of information that manifests itself as a reduction in variance in an ensemble of partial sums, we derive a set of equations that relate this variance to the length of accumulation and the minimum number of bits needed for accumulation. We apply our analysis to three benchmark networks: CIFAR-10 ResNet 32, ImageNet ResNet 18 and ImageNet AlexNet. In each case, with accumulation precision set in accordance with our proposed equations, the networks successfully converge to the single precision floating-point baseline. We also show that reducing accumulation precision further degrades the quality of the trained network, proving that our equations produce tight bounds. Overall this analysis enables precise tailoring of computation hardware to the application, yielding area- and power-optimal systems",
    "checked": true,
    "id": "e931f4444f634695bfab5a6e57c817da52fc512b",
    "semantic_title": "accumulation bit-width scaling for ultra-low precision training of deep networks",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=Bklfsi0cKm": {
    "title": "Deep Convolutional Networks as shallow Gaussian Processes",
    "volume": "poster",
    "abstract": "We show that the output of a (residual) CNN with an appropriate prior over the weights and biases is a GP in the limit of infinitely many convolutional filters, extending similar results for dense networks. For a CNN, the equivalent kernel can be computed exactly and, unlike \"deep kernels\", has very few parameters: only the hyperparameters of the original CNN. Further, we show that this kernel has two properties that allow it to be computed efficiently; the cost of evaluating the kernel for a pair of images is similar to a single forward pass through the original CNN with only one filter per layer. The kernel equivalent to a 32-layer ResNet obtains 0.84% classification error on MNIST, a new record for GP with a comparable number of parameters",
    "checked": true,
    "id": "43e33e80d74205e860dd4b8e26b7c458c60e201a",
    "semantic_title": "deep convolutional networks as shallow gaussian processes",
    "citation_count": 271,
    "authors": []
  },
  "https://openreview.net/forum?id=S1VWjiRcKX": {
    "title": "Universal Successor Features Approximators",
    "volume": "poster",
    "abstract": "The ability of a reinforcement learning (RL) agent to learn about many reward functions at the same time has many potential benefits, such as the decomposition of complex tasks into simpler ones, the exchange of information between tasks, and the reuse of skills. We focus on one aspect in particular, namely the ability to generalise to unseen tasks. Parametric generalisation relies on the interpolation power of a function approximator that is given the task description as input; one of its most common form are universal value function approximators (UVFAs). Another way to generalise to new tasks is to exploit structure in the RL problem itself. Generalised policy improvement (GPI) combines solutions of previous tasks into a policy for the unseen task; this relies on instantaneous policy evaluation of old policies under the new reward function, which is made possible through successor features (SFs). Our proposed \\emph{universal successor features approximators} (USFAs) combine the advantages of all of these, namely the scalability of UVFAs, the instant inference of SFs, and the strong generalisation of GPI. We discuss the challenges involved in training a USFA, its generalisation properties and demonstrate its practical benefits and transfer abilities on a large-scale domain in which the agent has to navigate in a first-person perspective three-dimensional environment",
    "checked": true,
    "id": "894536f2ac4728850bc18705daeeda6e88f3d6f1",
    "semantic_title": "universal successor features approximators",
    "citation_count": 117,
    "authors": []
  },
  "https://openreview.net/forum?id=SkeZisA5t7": {
    "title": "Adaptive Estimators Show Information Compression in Deep Neural Networks",
    "volume": "poster",
    "abstract": "To improve how neural networks function it is crucial to understand their learning process. The information bottleneck theory of deep learning proposes that neural networks achieve good generalization by compressing their representations to disregard information that is not relevant to the task. However, empirical evidence for this theory is conflicting, as compression was only observed when networks used saturating activation functions. In contrast, networks with non-saturating activation functions achieved comparable levels of task performance but did not show compression. In this paper we developed more robust mutual information estimation techniques, that adapt to hidden activity of neural networks and produce more sensitive measurements of activations from all functions, especially unbounded functions. Using these adaptive estimation techniques, we explored compression in networks with a range of different activation functions. With two improved methods of estimation, firstly, we show that saturation of the activation function is not required for compression, and the amount of compression varies between different activation functions. We also find that there is a large amount of variation in compression between different network initializations. Secondary, we see that L2 regularization leads to significantly increased compression, while preventing overfitting. Finally, we show that only compression of the last layer is positively correlated with generalization",
    "checked": true,
    "id": "46f41e44bfc53b7297134b2734980494c0833874",
    "semantic_title": "adaptive estimators show information compression in deep neural networks",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=H1MgjoR9tQ": {
    "title": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model",
    "volume": "poster",
    "abstract": "Continuous Bag of Words (CBOW) is a powerful text embedding method. Due to its strong capabilities to encode word content, CBOW embeddings perform well on a wide range of downstream tasks while being efficient to compute. However, CBOW is not capable of capturing the word order. The reason is that the computation of CBOW's word embeddings is commutative, i.e., embeddings of XYZ and ZYX are the same. In order to address this shortcoming, we propose a learning algorithm for the Continuous Matrix Space Model, which we call Continual Multiplication of Words (CMOW). Our algorithm is an adaptation of word2vec, so that it can be trained on large quantities of unlabeled text. We empirically show that CMOW better captures linguistic properties, but it is inferior to CBOW in memorizing word content. Motivated by these findings, we propose a hybrid model that combines the strengths of CBOW and CMOW. Our results show that the hybrid CBOW-CMOW-model retains CBOW's strong ability to memorize word content while at the same time substantially improving its ability to encode other linguistic information by 8%. As a result, the hybrid also performs better on 8 out of 11 supervised downstream tasks with an average improvement of 1.2%",
    "checked": true,
    "id": "a35532918341dd582f1ebef56652ff7593f7d769",
    "semantic_title": "cbow is not all you need: combining cbow with the compositional matrix space model",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=SkNksoRctQ": {
    "title": "Fluctuation-dissipation relations for stochastic gradient descent",
    "volume": "poster",
    "abstract": "The notion of the stationary equilibrium ensemble has played a central role in statistical mechanics. In machine learning as well, training serves as generalized equilibration that drives the probability distribution of model parameters toward stationarity. Here, we derive stationary fluctuation-dissipation relations that link measurable quantities and hyperparameters in the stochastic gradient descent algorithm. These relations hold exactly for any stationary state and can in particular be used to adaptively set training schedule. We can further use the relations to efficiently extract information pertaining to a loss-function landscape such as the magnitudes of its Hessian and anharmonicity. Our claims are empirically verified",
    "checked": true,
    "id": "3eb0f85ff84e4869387beffea2846c9243351aa7",
    "semantic_title": "fluctuation-dissipation relations for stochastic gradient descent",
    "citation_count": 75,
    "authors": []
  },
  "https://openreview.net/forum?id=HJGkisCcKm": {
    "title": "A Universal Music Translation Network",
    "volume": "poster",
    "abstract": "We present a method for translating music across musical instruments and styles. This method is based on unsupervised training of a multi-domain wavenet autoencoder, with a shared encoder and a domain-independent latent space that is trained end-to-end on waveforms. Employing a diverse training dataset and large net capacity, the single encoder allows us to translate also from musical domains that were not seen during training. We evaluate our method on a dataset collected from professional musicians, and achieve convincing translations. We also study the properties of the obtained translation and demonstrate translating even from a whistle, potentially enabling the creation of instrumental music by untrained humans",
    "checked": true,
    "id": "45ce9fce4a4eea9f72688885182aee0c84786fab",
    "semantic_title": "a universal music translation network",
    "citation_count": 110,
    "authors": []
  },
  "https://openreview.net/forum?id=ByxkijC5FQ": {
    "title": "Neural Persistence: A Complexity Measure for Deep Neural Networks Using Algebraic Topology",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2484ebfa2053999b4481ab2cd979eccb47cb0321",
    "semantic_title": "neural persistence: a complexity measure for deep neural networks using algebraic topology",
    "citation_count": 111,
    "authors": []
  },
  "https://openreview.net/forum?id=HyNA5iRcFQ": {
    "title": "Detecting Egregious Responses in Neural Sequence-to-sequence Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2272d5b2ebffa289bea56cf1c11efd6fef0c4c6f",
    "semantic_title": "detecting egregious responses in neural sequence-to-sequence models",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=HJz05o0qK7": {
    "title": "Measuring Compositionality in Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ba4cf6046d420af0ae86e2f4b587a8d50d219be3",
    "semantic_title": "measuring compositionality in representation learning",
    "citation_count": 150,
    "authors": []
  },
  "https://openreview.net/forum?id=HJMCcjAcYX": {
    "title": "Learning Representations of Sets through Optimized Permutations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8083cfb0e76358ab54f92eedbe13ed6a874a48e5",
    "semantic_title": "learning representations of sets through optimized permutations",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=H1gR5iR5FX": {
    "title": "Analysing Mathematical Reasoning Abilities of Neural Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "afed6dc6900d3b37e528b9086661bba583d60bf6",
    "semantic_title": "analysing mathematical reasoning abilities of neural models",
    "citation_count": 431,
    "authors": []
  },
  "https://openreview.net/forum?id=rkxacs0qY7": {
    "title": "FUNCTIONAL VARIATIONAL BAYESIAN NEURAL NETWORKS",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "69555845bf26bf930ecbfc223fa0ee454b2d58df",
    "semantic_title": "functional variational bayesian neural networks",
    "citation_count": 241,
    "authors": []
  },
  "https://openreview.net/forum?id=HygjqjR9Km": {
    "title": "Improving MMD-GAN Training with Repulsive Loss Function",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "15cc54ed7b1582b2efd71bedf28b23634d82991b",
    "semantic_title": "improving mmd-gan training with repulsive loss function",
    "citation_count": 80,
    "authors": []
  },
  "https://openreview.net/forum?id=rygjcsR9Y7": {
    "title": "SOM-VAE: Interpretable Discrete Representation Learning on Time Series",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e8c5f091420cb75013121dd02ee417ae974ecee1",
    "semantic_title": "som-vae: interpretable discrete representation learning on time series",
    "citation_count": 140,
    "authors": []
  },
  "https://openreview.net/forum?id=r1eiqi09K7": {
    "title": "Riemannian Adaptive Optimization Methods",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1f992463673ebd65840fe8d763da98dc94e04c17",
    "semantic_title": "riemannian adaptive optimization methods",
    "citation_count": 259,
    "authors": []
  },
  "https://openreview.net/forum?id=BJgqqsAct7": {
    "title": "Non-vacuous Generalization Bounds at the ImageNet Scale: a PAC-Bayesian Compression Approach",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "74fe26e60f04055af52a88a513a1d6229ade5781",
    "semantic_title": "non-vacuous generalization bounds at the imagenet scale: a pac-bayesian compression approach",
    "citation_count": 215,
    "authors": []
  },
  "https://openreview.net/forum?id=rygqqsA9KX": {
    "title": "Learning Factorized Multimodal Representations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "034f1c5589644a6b42f50bf61b1628a1c5607fd9",
    "semantic_title": "learning factorized multimodal representations",
    "citation_count": 409,
    "authors": []
  },
  "https://openreview.net/forum?id=Syxt5oC5YQ": {
    "title": "Aggregated Momentum: Stability Through Passive Damping",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1de35bfced22b716db622d8946a66ac9c5d39690",
    "semantic_title": "aggregated momentum: stability through passive damping",
    "citation_count": 68,
    "authors": []
  },
  "https://openreview.net/forum?id=SJxu5iR9KQ": {
    "title": "Learning to Schedule Communication in Multi-agent Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0732925ffaf7ee525a57786f7c9791491ef084db",
    "semantic_title": "learning to schedule communication in multi-agent reinforcement learning",
    "citation_count": 208,
    "authors": []
  },
  "https://openreview.net/forum?id=H1xD9sR5Fm": {
    "title": "Minimum Divergence vs. Maximum Margin: an Empirical Comparison on Seq2Seq Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ea49cef629b6e5a87a596944ad4ea9144651bfdd",
    "semantic_title": "minimum divergence vs. maximum margin: an empirical comparison on seq2seq models",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=ryGvcoA5YX": {
    "title": "Overcoming Catastrophic Forgetting for Continual Learning via Model Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "48518df15244bb691b20e827aca53e407c63adef",
    "semantic_title": "overcoming catastrophic forgetting for continual learning via model adaptation",
    "citation_count": 127,
    "authors": []
  },
  "https://openreview.net/forum?id=Sklv5iRqYX": {
    "title": "Multi-Domain Adversarial Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "78e7507af6aa29d542eb4ee983a973157094b726",
    "semantic_title": "multi-domain adversarial learning",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=rJzLciCqKm": {
    "title": "Learning from Positive and Unlabeled Data with a Selection Bias",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f4b74295a2aeaff7fab34a414e1feb5e52d52cb8",
    "semantic_title": "learning from positive and unlabeled data with a selection bias",
    "citation_count": 102,
    "authors": []
  },
  "https://openreview.net/forum?id=BkeU5j0ctQ": {
    "title": "CEM-RL: Combining evolutionary and gradient-based methods for policy search",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d9e08e872c69e5acfed2a93ec6f6d623cfd0c680",
    "semantic_title": "cem-rl: combining evolutionary and gradient-based methods for policy search",
    "citation_count": 161,
    "authors": []
  },
  "https://openreview.net/forum?id=BylIciRcYQ": {
    "title": "SGD Converges to Global Minimum in Deep Learning via Star-convex Path",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b4a4d4c12e49110176847baaf6dd88b0dfeed285",
    "semantic_title": "sgd converges to global minimum in deep learning via star-convex path",
    "citation_count": 74,
    "authors": []
  },
  "https://openreview.net/forum?id=HJxB5sRcFQ": {
    "title": "LayoutGAN: Generating Graphic Layouts with Wireframe Discriminators",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f3f4e8f1871622bd937a592b23fa7cc0a41bfbe0",
    "semantic_title": "layoutgan: generating graphic layouts with wireframe discriminators",
    "citation_count": 231,
    "authors": []
  },
  "https://openreview.net/forum?id=r1gEqiC9FX": {
    "title": "Equi-normalization of Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8dd29bd8bd4197a97d28614f3c13b18fc6c9e2b5",
    "semantic_title": "equi-normalization of neural networks",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=rkemqsC9Fm": {
    "title": "Information Theoretic lower bounds on negative log likelihood",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e0ac65da87d98a1af4a43a37a020a001ad478c3c",
    "semantic_title": "information theoretic lower bounds on negative log likelihood",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=B1xf9jAqFQ": {
    "title": "Neural Speed Reading with Structural-Jump-LSTM",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d647a64de005113f7bb5859347f5edca81bc0eec",
    "semantic_title": "neural speed reading with structural-jump-lstm",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=rklz9iAcKQ": {
    "title": "Deep Graph Infomax",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "967a21a111757d6af7f7a25ca7ea2bdf6d505098",
    "semantic_title": "deep graph infomax",
    "citation_count": 2401,
    "authors": []
  },
  "https://openreview.net/forum?id=B1VZqjAcYX": {
    "title": "SNIP: SINGLE-SHOT NETWORK PRUNING BASED ON CONNECTION SENSITIVITY",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "cf440ccce4a7a8681e238b4f26d5b95109add55d",
    "semantic_title": "snip: single-shot network pruning based on connection sensitivity",
    "citation_count": 1212,
    "authors": []
  },
  "https://openreview.net/forum?id=SJfb5jCqKm": {
    "title": "Bias-Reduced Uncertainty Estimation for Deep Neural Classifiers",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "dfb58733b367005945a2f2c2028145e4678efd7a",
    "semantic_title": "bias-reduced uncertainty estimation for deep neural classifiers",
    "citation_count": 140,
    "authors": []
  },
  "https://openreview.net/forum?id=rJfW5oA5KQ": {
    "title": "Approximability of Discriminators Implies Diversity in GANs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c3f03c67e119900e6f8f86740c445b46d0c5bcf3",
    "semantic_title": "approximability of discriminators implies diversity in gans",
    "citation_count": 74,
    "authors": []
  },
  "https://openreview.net/forum?id=B1xWcj0qYm": {
    "title": "On the Minimal Supervision for Training Any Binary Classifier from Only Unlabeled Data",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "570f3c52e4e9608d65afd00076e784800c286524",
    "semantic_title": "on the minimal supervision for training any binary classifier from only unlabeled data",
    "citation_count": 88,
    "authors": []
  },
  "https://openreview.net/forum?id=r1NJqsRctX": {
    "title": "Auxiliary Variational MCMC",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ebbab9a45b241de9600f55d68025e2890333a942",
    "semantic_title": "auxiliary variational mcmc",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=S1zk9iRqF7": {
    "title": "PATE-GAN: Generating Synthetic Data with Differential Privacy Guarantees",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "af1841e1db6579f1f1777a59c7e9e4658d2ac466",
    "semantic_title": "pate-gan: generating synthetic data with differential privacy guarantees",
    "citation_count": 662,
    "authors": []
  },
  "https://openreview.net/forum?id=r1f0YiCctm": {
    "title": "Minimal Random Code Learning: Getting Bits Back from Compressed Model Parameters",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "90b60322be13e63c9903b40b3407861e001eba1d",
    "semantic_title": "minimal random code learning: getting bits back from compressed model parameters",
    "citation_count": 82,
    "authors": []
  },
  "https://openreview.net/forum?id=ryf6Fs09YX": {
    "title": "GO Gradient for Expectation-Based Objectives",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "15d0330ef349310db06e7f9babd8f09970905b51",
    "semantic_title": "go gradient for expectation-based objectives",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=HJz6tiCqYm": {
    "title": "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "49b64383fe36268410c430352637ed23b16820c5",
    "semantic_title": "benchmarking neural network robustness to common corruptions and perturbations",
    "citation_count": 3459,
    "authors": []
  },
  "https://openreview.net/forum?id=HkxaFoC9KQ": {
    "title": "Deep reinforcement learning with relational inductive biases",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9ea92ebeb7462f2db346cfa3281ad7497b1063d6",
    "semantic_title": "deep reinforcement learning with relational inductive biases",
    "citation_count": 209,
    "authors": []
  },
  "https://openreview.net/forum?id=S1E3Ko09F7": {
    "title": "L-Shapley and C-Shapley: Efficient Model Interpretation for Structured Data",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "85634f8a458f13d7cb80e946eec23880295ba1a3",
    "semantic_title": "l-shapley and c-shapley: efficient model interpretation for structured data",
    "citation_count": 217,
    "authors": []
  },
  "https://openreview.net/forum?id=S1x2Fj0qKQ": {
    "title": "Whitening and Coloring Batch Transform for GANs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e5cf28063f76ad1a3cda56f68b08ff264a0aa198",
    "semantic_title": "whitening and coloring batch transform for gans",
    "citation_count": 50,
    "authors": []
  },
  "https://openreview.net/forum?id=Sk4jFoA9K7": {
    "title": "PeerNets: Exploiting Peer Wisdom Against Adversarial Attacks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bfb0d179916c000d54f27e7a9ea18b6269963e74",
    "semantic_title": "peernets: exploiting peer wisdom against adversarial attacks",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=B1gstsCqt7": {
    "title": "Sparse Dictionary Learning by Dynamical Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "33cacec6bc67f52f611845b8c6a88feaa5f88d09",
    "semantic_title": "sparse dictionary learning by dynamical neural networks",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=HkxjYoCqKX": {
    "title": "Relaxed Quantization for Discretized Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "cc34e129335325010c99af288a0b9812c7a7685e",
    "semantic_title": "additive powers-of-two quantization: a non-uniform discretization for neural networks",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=HkgqFiAcFm": {
    "title": "Marginal Policy Gradients: A Unified Family of Estimators for Bounded Action Spaces with Applications",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a2c7c3b8ffe918912f25cf7f476d3b6f603e8153",
    "semantic_title": "marginal policy gradients: a unified family of estimators for bounded action spaces with applications",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=H1gKYo09tX": {
    "title": "code2seq: Generating Sequences from Structured Representations of Code",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "98d1307bed619b58b4a44acd8e65ac58495776c2",
    "semantic_title": "code2seq: generating sequences from structured representations of code",
    "citation_count": 700,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk4dFjR5K7": {
    "title": "ADef: an Iterative Algorithm to Construct Adversarial Deformations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3ecda636a99ec93acc941c0217a65c9a3af9562f",
    "semantic_title": "adef: an iterative algorithm to construct adversarial deformations",
    "citation_count": 97,
    "authors": []
  },
  "https://openreview.net/forum?id=rke_YiRct7": {
    "title": "Small nonlinearities in activation functions create bad local minima in neural networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ab0813895b5a7df2bffd34808a8974d7220ac36b",
    "semantic_title": "small nonlinearities in activation functions create bad local minima in neural networks",
    "citation_count": 95,
    "authors": []
  },
  "https://openreview.net/forum?id=SyNvti09KQ": {
    "title": "Visceral Machines: Risk-Aversion in Reinforcement Learning with Intrinsic Physiological Rewards",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "56cf1351c926a3f837d0c53eda7d2f58ececf6d6",
    "semantic_title": "visceral machines: risk-aversion in reinforcement learning with intrinsic physiological rewards",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=SJfPFjA9Fm": {
    "title": "ACCELERATING NONCONVEX LEARNING VIA REPLICA EXCHANGE LANGEVIN DIFFUSION",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d2eff663ca365f3f4d05ff3819a81dabf7400a9a",
    "semantic_title": "accelerating nonconvex learning via replica exchange langevin diffusion",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=rJevYoA9Fm": {
    "title": "The Singular Values of Convolutional Layers",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "fb2edf25484c9e9e5f94b719c55dc1faf7591bfa",
    "semantic_title": "the singular values of convolutional layers",
    "citation_count": 203,
    "authors": []
  },
  "https://openreview.net/forum?id=ByxPYjC5KQ": {
    "title": "Improving Generalization and Stability of Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a7dbdcac11bc300d73e0352fc0422b57ff075f51",
    "semantic_title": "improving generalization and stability of generative adversarial networks",
    "citation_count": 122,
    "authors": []
  },
  "https://openreview.net/forum?id=r1xwKoR9Y7": {
    "title": "GamePad: A Learning Environment for Theorem Proving",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "87c425f23bcac2f082968abda64a971f91522f73",
    "semantic_title": "gamepad: a learning environment for theorem proving",
    "citation_count": 110,
    "authors": []
  },
  "https://openreview.net/forum?id=HJlLKjR9FQ": {
    "title": "Towards Understanding Regularization in Batch Normalization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9fa42ea422fe1729e04a98fa8072a7b48ce91bc8",
    "semantic_title": "towards understanding regularization in batch normalization",
    "citation_count": 180,
    "authors": []
  },
  "https://openreview.net/forum?id=SylLYsCcFm": {
    "title": "Learning to Make Analogies by Contrasting Abstract Relational Structure",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0d16b0fbe1e2bf6ef02aea2e058f2e13c3a83fa2",
    "semantic_title": "learning to make analogies by contrasting abstract relational structure",
    "citation_count": 100,
    "authors": []
  },
  "https://openreview.net/forum?id=ByxBFsRqYm": {
    "title": "Attention, Learn to Solve Routing Problems!",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ce4f001c1d8ddb9a95cf54e14240ef02c44bd329",
    "semantic_title": "attention, learn to solve routing problems!",
    "citation_count": 1232,
    "authors": []
  },
  "https://openreview.net/forum?id=BkeStsCcKQ": {
    "title": "Critical Learning Periods in Deep Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1802a7870a642d414f435273dd9e9190a0dc4fcb",
    "semantic_title": "critical learning periods in deep networks",
    "citation_count": 137,
    "authors": []
  },
  "https://openreview.net/forum?id=HkxStoC5F7": {
    "title": "Meta-Learning Probabilistic Inference for Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7b0aad12a6917b7d444ba2f87c7f8ccc5357797a",
    "semantic_title": "meta-learning probabilistic inference for prediction",
    "citation_count": 265,
    "authors": []
  },
  "https://openreview.net/forum?id=HyeVtoRqtQ": {
    "title": "Trellis Networks for Sequence Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a14af711aaa3ae83eb64d1f517b024b8c3094a8a",
    "semantic_title": "trellis networks for sequence modeling",
    "citation_count": 146,
    "authors": []
  },
  "https://openreview.net/forum?id=Bke4KsA5FX": {
    "title": "Generative Code Modeling with Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "75c0d369c7151b925155cfe1b3f01dd7d0503981",
    "semantic_title": "generative code modeling with graphs",
    "citation_count": 178,
    "authors": []
  },
  "https://openreview.net/forum?id=HkNGYjR9FX": {
    "title": "Learning Recurrent Binary/Ternary Weights",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ce98b1ec4c79235eb83539ff1379c0f79dd3e2b9",
    "semantic_title": "learning recurrent binary/ternary weights",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=SJfZKiC5FX": {
    "title": "Dynamically Unfolding Recurrent Restorer: A Moving Endpoint Control Method for Image Restoration",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f2ef09c15eeb7d197c8ec23fb85ce3bf4be6bd63",
    "semantic_title": "dynamically unfolding recurrent restorer: a moving endpoint control method for image restoration",
    "citation_count": 51,
    "authors": []
  },
  "https://openreview.net/forum?id=HJMC_iA5tm": {
    "title": "Learning a SAT Solver from Single-Bit Supervision",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "fe257027193ea4a74fdab99d7509ce4002ad7de6",
    "semantic_title": "learning a sat solver from single-bit supervision",
    "citation_count": 426,
    "authors": []
  },
  "https://openreview.net/forum?id=BklCusRct7": {
    "title": "Optimal Transport Maps For Distribution Preserving Operations on Latent Spaces of Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "fcb68849a63000c05c9b8cd592302a82b723dc32",
    "semantic_title": "optimal transport maps for distribution preserving operations on latent spaces of generative models",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=Hkf2_sC5FX": {
    "title": "Efficient Lifelong Learning with A-GEM",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4a954b3e72a61968ab235076bcc242aca3a05520",
    "semantic_title": "efficient lifelong learning with a-gem",
    "citation_count": 1463,
    "authors": []
  },
  "https://openreview.net/forum?id=HkeoOo09YX": {
    "title": "Meta-Learning For Stochastic Gradient MCMC",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "43410e1d4b8790582717013542313c994588590c",
    "semantic_title": "meta-learning for stochastic gradient mcmc",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=B1G9doA9F7": {
    "title": "Augmented Cyclic Adversarial Learning for Low Resource Domain Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "653435e90a6dc9c331e03f70ed32906eb3f6810d",
    "semantic_title": "augmented cyclic adversarial learning for low resource domain adaptation",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=HklKui0ct7": {
    "title": "Off-Policy Evaluation and Learning from Logged Bandit Feedback: Error Reduction via Surrogate Policy",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "91727b55133f77dafed05603da0d4752a7606019",
    "semantic_title": "off-policy evaluation and learning from logged bandit feedback: error reduction via surrogate policy",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=HkfYOoCcYX": {
    "title": "Double Viterbi: Weight Encoding for High Compression Ratio and Fast On-Chip Reconstruction for Deep Neural Network",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1f7fa36fe81a6447bdcd06a84f72228e5dc0524a",
    "semantic_title": "double viterbi: weight encoding for high compression ratio and fast on-chip reconstruction for deep neural network",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=H1ewdiR5tQ": {
    "title": "Graph Wavelet Neural Network",
    "volume": "poster",
    "abstract": "We present graph wavelet neural network (GWNN), a novel graph convolutional neural network (CNN), leveraging graph wavelet transform to address the shortcomings of previous spectral graph CNN methods that depend on graph Fourier transform. Different from graph Fourier transform, graph wavelet transform can be obtained via a fast algorithm without requiring matrix eigendecomposition with high computational cost. Moreover, graph wavelets are sparse and localized in vertex domain, offering high efficiency and good interpretability for graph convolution. The proposed GWNN significantly outperforms previous spectral graph CNNs in the task of graph-based semi-supervised classification on three benchmark datasets: Cora, Citeseer and Pubmed",
    "checked": true,
    "id": "9f91568d8ec72c417d9146a551b61e69ccf1f848",
    "semantic_title": "graph wavelet neural network",
    "citation_count": 333,
    "authors": []
  },
  "https://openreview.net/forum?id=SJgw_sRqFQ": {
    "title": "The Unusual Effectiveness of Averaging in GAN Training",
    "volume": "poster",
    "abstract": "We examine two different techniques for parameter averaging in GAN training. Moving Average (MA) computes the time-average of parameters, whereas Exponential Moving Average (EMA) computes an exponentially discounted sum. Whilst MA is known to lead to convergence in bilinear settings, we provide the -- to our knowledge -- first theoretical arguments in support of EMA. We show that EMA converges to limit cycles around the equilibrium with vanishing amplitude as the discount parameter approaches one for simple bilinear games and also enhances the stability of general GAN training. We establish experimentally that both techniques are strikingly effective in the non-convex-concave GAN setting as well. Both improve inception and FID scores on different architectures and for different GAN objectives. We provide comprehensive experimental results across a range of datasets -- mixture of Gaussians, CIFAR-10, STL-10, CelebA and ImageNet -- to demonstrate its effectiveness. We achieve state-of-the-art results on CIFAR-10 and produce clean CelebA face images.\\footnote{~The code is available at \\url{https://github.com/yasinyazici/EMA_GAN}}",
    "checked": true,
    "id": "fed5546113d945807e2f24317c0e560d1d21068c",
    "semantic_title": "the unusual effectiveness of averaging in gan training",
    "citation_count": 175,
    "authors": []
  },
  "https://openreview.net/forum?id=HyGIdiRqtm": {
    "title": "Evaluating Robustness of Neural Networks with Mixed Integer Programming",
    "volume": "poster",
    "abstract": "Neural networks trained only to optimize for training accuracy can often be fooled by adversarial examples --- slightly perturbed inputs misclassified with high confidence. Verification of networks enables us to gauge their vulnerability to such adversarial examples. We formulate verification of piecewise-linear neural networks as a mixed integer program. On a representative task of finding minimum adversarial distortions, our verifier is two to three orders of magnitude quicker than the state-of-the-art. We achieve this computational speedup via tight formulations for non-linearities, as well as a novel presolve algorithm that makes full use of all information available. The computational speedup allows us to verify properties on convolutional and residual networks with over 100,000 ReLUs --- several orders of magnitude more than networks previously verified by any complete verifier. In particular, we determine for the first time the exact adversarial accuracy of an MNIST classifier to perturbations with bounded l-∞ norm ε=0.1: for this classifier, we find an adversarial example for 4.38% of samples, and a certificate of robustness to norm-bounded perturbations for the remainder. Across all robust training procedures and network architectures considered, and for both the MNIST and CIFAR-10 datasets, we are able to certify more samples than the state-of-the-art and find more adversarial examples than a strong first-order attack",
    "checked": true,
    "id": "9de69a46e6c619255eeffbfbb6c7b7163690eb48",
    "semantic_title": "evaluating robustness of neural networks with mixed integer programming",
    "citation_count": 845,
    "authors": []
  },
  "https://openreview.net/forum?id=S1EHOsC9tX": {
    "title": "Towards the first adversarially robust neural network model on MNIST",
    "volume": "poster",
    "abstract": "Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful L-inf defense by Madry et~al. (1) has lower L0 robustness than undefended networks and still highly susceptible to L2 perturbations, (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L-inf perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class",
    "checked": true,
    "id": "fd7789de401811fd8692466b8d49230e7184655f",
    "semantic_title": "towards the first adversarially robust neural network model on mnist",
    "citation_count": 370,
    "authors": []
  },
  "https://openreview.net/forum?id=HyGBdo0qFm": {
    "title": "On the Turing Completeness of Modern Neural Network Architectures",
    "volume": "poster",
    "abstract": "Alternatives to recurrent neural networks, in particular, architectures based on attention or convolutions, have been gaining momentum for processing input sequences. In spite of their relevance, the computational properties of these alternatives have not yet been fully explored. We study the computational power of two of the most paradigmatic architectures exemplifying these mechanisms: the Transformer (Vaswani et al., 2017) and the Neural GPU (Kaiser & Sutskever, 2016). We show both models to be Turing complete exclusively based on their capacity to compute and access internal dense representations of the data. In particular, neither the Transformer nor the Neural GPU requires access to an external memory to become Turing complete. Our study also reveals some minimal sets of elements needed to obtain these completeness results",
    "checked": true,
    "id": "3694381e74445a8b9f8cb8d373e39626e47191b5",
    "semantic_title": "on the turing completeness of modern neural network architectures",
    "citation_count": 146,
    "authors": []
  },
  "https://openreview.net/forum?id=ByeSdsC9Km": {
    "title": "Adaptive Posterior Learning: few-shot learning with a surprise-based memory module",
    "volume": "poster",
    "abstract": "The ability to generalize quickly from few observations is crucial for intelligent systems. In this paper we introduce APL, an algorithm that approximates probability distributions by remembering the most surprising observations it has encountered. These past observations are recalled from an external memory module and processed by a decoder network that can combine information from different memory slots to generalize beyond direct recall. We show this algorithm can perform as well as state of the art baselines on few-shot classification benchmarks with a smaller memory footprint. In addition, its memory compression allows it to scale to thousands of unknown labels. Finally, we introduce a meta-learning reasoning task which is more challenging than direct classification. In this setting, APL is able to generalize with fewer than one example per class via deductive reasoning",
    "checked": true,
    "id": "f3bcab5b23eb8999ea3c3c29140c75861255cfea",
    "semantic_title": "adaptive posterior learning: few-shot learning with a surprise-based memory module",
    "citation_count": 77,
    "authors": []
  },
  "https://openreview.net/forum?id=r14EOsCqKX": {
    "title": "A Closer Look at Deep Learning Heuristics: Learning rate restarts, Warmup and Distillation",
    "volume": "poster",
    "abstract": "The convergence rate and final performance of common deep learning models have significantly benefited from recently proposed heuristics such as learning rate schedules, knowledge distillation, skip connections and normalization layers. In the absence of theoretical underpinnings, controlled experiments aimed at explaining the efficacy of these strategies can aid our understanding of deep learning landscapes and the training dynamics. Existing approaches for empirical analysis rely on tools of linear interpolation and visualizations with dimensionality reduction, each with their limitations. Instead, we revisit the empirical analysis of heuristics through the lens of recently proposed methods for loss surface and representation analysis, viz. mode connectivity and canonical correlation analysis (CCA), and hypothesize reasons why the heuristics succeed. In particular, we explore knowledge distillation and learning rate heuristics of (cosine) restarts and warmup using mode connectivity and CCA. Our empirical analysis suggests that: (a) the reasons often quoted for the success of cosine annealing are not evidenced in practice; (b) that the effect of learning rate warmup is to prevent the deeper layers from creating training instability; and (c) that the latent knowledge shared by the teacher is primarily disbursed in the deeper layers",
    "checked": true,
    "id": "c4ab32dc966bff2de35723374f7410eeab85053f",
    "semantic_title": "a closer look at deep learning heuristics: learning rate restarts, warmup and distillation",
    "citation_count": 277,
    "authors": []
  },
  "https://openreview.net/forum?id=Syl7OsRqY7": {
    "title": "Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering",
    "volume": "poster",
    "abstract": "End-to-end neural models have made significant progress in question answering, however recent studies show that these models implicitly assume that the answer and evidence appear close together in a single document. In this work, we propose the Coarse-grain Fine-grain Coattention Network (CFC), a new question answering model that combines information from evidence across multiple documents. The CFC consists of a coarse-grain module that interprets documents with respect to the query then finds a relevant answer, and a fine-grain module which scores each candidate answer by comparing its occurrences across all of the documents with the query. We design these modules using hierarchies of coattention and self-attention, which learn to emphasize different parts of the input. On the Qangaroo WikiHop multi-evidence question answering task, the CFC obtains a new state-of-the-art result of 70.6% on the blind test set, outperforming the previous best by 3% accuracy despite not using pretrained contextual encoders",
    "checked": true,
    "id": "e17c13217e1fad11bae46820c4acae1745f69b43",
    "semantic_title": "coarse-grain fine-grain coattention network for multi-evidence question answering",
    "citation_count": 63,
    "authors": []
  },
  "https://openreview.net/forum?id=H1emus0qF7": {
    "title": "Near-Optimal Representation Learning for Hierarchical Reinforcement Learning",
    "volume": "poster",
    "abstract": "We study the problem of representation learning in goal-conditioned hierarchical reinforcement learning. In such hierarchical structures, a higher-level controller solves tasks by iteratively communicating goals which a lower-level policy is trained to reach. Accordingly, the choice of representation -- the mapping of observation space to goal space -- is crucial. To study this problem, we develop a notion of sub-optimality of a representation, defined in terms of expected reward of the optimal hierarchical policy using this representation. We derive expressions which bound the sub-optimality and show how these expressions can be translated to representation learning objectives which may be optimized in practice. Results on a number of difficult continuous-control tasks show that our approach to representation learning yields qualitatively better representations as well as quantitatively better hierarchical policies, compared to existing methods",
    "checked": true,
    "id": "e4a89a978f747d0b548f5887b2380c5f618061f0",
    "semantic_title": "near-optimal representation learning for hierarchical reinforcement learning",
    "citation_count": 211,
    "authors": []
  },
  "https://openreview.net/forum?id=H1gfOiAqYm": {
    "title": "Execution-Guided Neural Program Synthesis",
    "volume": "poster",
    "abstract": "Neural program synthesis from input-output examples has attracted an increasing interest from both the machine learning and the programming language community. Most existing neural program synthesis approaches employ an encoder-decoder architecture, which uses an encoder to compute the embedding of the given input-output examples, as well as a decoder to generate the program from the embedding following a given syntax. Although such approaches achieve a reasonable performance on simple tasks such as FlashFill, on more complex tasks such as Karel, the state-of-the-art approach can only achieve an accuracy of around 77%. We observe that the main drawback of existing approaches is that the semantic information is greatly under-utilized. In this work, we propose two simple yet principled techniques to better leverage the semantic information, which are execution-guided synthesis and synthesizer ensemble. These techniques are general enough to be combined with any existing encoder-decoder-style neural program synthesizer. Applying our techniques to the Karel dataset, we can boost the accuracy from around 77% to more than 90%",
    "checked": true,
    "id": "6c41bedc4637f3fd504c68baa3b3d8881e056ac1",
    "semantic_title": "execution-guided neural program synthesis",
    "citation_count": 162,
    "authors": []
  },
  "https://openreview.net/forum?id=rJlWOj0qF7": {
    "title": "Imposing Category Trees Onto Word-Embeddings Using A Geometric Construction",
    "volume": "poster",
    "abstract": "We present a novel method to precisely impose tree-structured category information onto word-embeddings, resulting in ball embeddings in higher dimensional spaces (N-balls for short). Inclusion relations among N-balls implicitly encode subordinate relations among categories. The similarity measurement in terms of the cosine function is enriched by category information. Using a geometric construction method instead of back-propagation, we create large N-ball embeddings that satisfy two conditions: (1) category trees are precisely imposed onto word embeddings at zero energy cost; (2) pre-trained word embeddings are well preserved. A new benchmark data set is created for validating the category of unknown words. Experiments show that N-ball embeddings, carrying category information, significantly outperform word embeddings in the test of nearest neighborhoods, and demonstrate surprisingly good performance in validating categories of unknown words. Source codes and data-sets are free for public access \\url{https://github.com/gnodisnait/nball4tree.git} and \\url{https://github.com/gnodisnait/bp94nball.git}",
    "checked": true,
    "id": "df97d99457cac7ba4cac120018174790f1e1bc1c",
    "semantic_title": "imposing category trees onto word-embeddings using a geometric construction",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=BJgRDjR9tQ": {
    "title": "ROBUST ESTIMATION VIA GENERATIVE ADVERSARIAL NETWORKS",
    "volume": "poster",
    "abstract": "Robust estimation under Huber's $\\epsilon$-contamination model has become an important topic in statistics and theoretical computer science. Rate-optimal procedures such as Tukey's median and other estimators based on statistical depth functions are impractical because of their computational intractability. In this paper, we establish an intriguing connection between f-GANs and various depth functions through the lens of f-Learning. Similar to the derivation of f-GAN, we show that these depth functions that lead to rate-optimal robust estimators can all be viewed as variational lower bounds of the total variation distance in the framework of f-Learning. This connection opens the door of computing robust estimators using tools developed for training GANs. In particular, we show that a JS-GAN that uses a neural network discriminator with at least one hidden layer is able to achieve the minimax rate of robust mean estimation under Huber's $\\epsilon$-contamination model. Interestingly, the hidden layers of the neural net structure in the discriminator class are shown to be necessary for robust estimation",
    "checked": true,
    "id": "a78d516846e1931b251ae873d9e78f2717c2ccec",
    "semantic_title": "robust estimation via generative adversarial networks",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=BJG0voC9YQ": {
    "title": "Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search",
    "volume": "poster",
    "abstract": "Learning policies on data synthesized by models can in principle quench the thirst of reinforcement learning algorithms for large amounts of real experience, which is often costly to acquire. However, simulating plausible experience de novo is a hard problem for many complex environments, often resulting in biases for model-based policy evaluation and search. Instead of de novo synthesis of data, here we assume logged, real experience and model alternative outcomes of this experience under counterfactual actions, i.e. actions that were not actually taken. Based on this, we propose the Counterfactually-Guided Policy Search (CF-GPS) algorithm for learning policies in POMDPs from off-policy experience. It leverages structural causal models for counterfactual evaluation of arbitrary policies on individual off-policy episodes. CF-GPS can improve on vanilla model-based RL algorithms by making use of available logged data to de-bias model predictions. In contrast to off-policy algorithms based on Importance Sampling which re-weight data, CF-GPS leverages a model to explicitly consider alternative outcomes, allowing the algorithm to make better use of experience data. We find empirically that these advantages translate into improved policy evaluation and search results on a non-trivial grid-world task. Finally, we show that CF-GPS generalizes the previously proposed Guided Policy Search and that reparameterization-based algorithms such Stochastic Value Gradient can be interpreted as counterfactual methods",
    "checked": true,
    "id": "745a134eca192982e8e0c16d6f36cfe24f9bdd08",
    "semantic_title": "woulda, coulda, shoulda: counterfactually-guided policy search",
    "citation_count": 138,
    "authors": []
  },
  "https://openreview.net/forum?id=Byg0DsCqYQ": {
    "title": "Robust Conditional Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "Conditional generative adversarial networks (cGAN) have led to large improvements in the task of conditional image generation, which lies at the heart of computer vision. The major focus so far has been on performance improvement, while there has been little effort in making cGAN more robust to noise. The regression (of the generator) might lead to arbitrarily large errors in the output, which makes cGAN unreliable for real-world applications. In this work, we introduce a novel conditional GAN model, called RoCGAN, which leverages structure in the target space of the model to address the issue. Our model augments the generator with an unsupervised pathway, which promotes the outputs of the generator to span the target manifold even in the presence of intense noise. We prove that RoCGAN share similar theoretical properties as GAN and experimentally verify that our model outperforms existing state-of-the-art cGAN architectures by a large margin in a variety of domains including images from natural scenes and faces",
    "checked": true,
    "id": "0251807835a2d863c809c25b3d5899d6431dbe89",
    "semantic_title": "robust conditional generative adversarial networks",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=SkE6PjC9KX": {
    "title": "Attentive Neural Processes",
    "volume": "poster",
    "abstract": "Neural Processes (NPs) (Garnelo et al., 2018) approach regression by learning to map a context set of observed input-output pairs to a distribution over regression functions. Each function models the distribution of the output given an input, conditioned on the context. NPs have the benefit of fitting observed data efficiently with linear complexity in the number of context input-output pairs, and can learn a wide family of conditional distributions; they learn predictive distributions conditioned on context sets of arbitrary size. Nonetheless, we show that NPs suffer a fundamental drawback of underfitting, giving inaccurate predictions at the inputs of the observed data they condition on. We address this issue by incorporating attention into NPs, allowing each input location to attend to the relevant context points for the prediction. We show that this greatly improves the accuracy of predictions, results in noticeably faster training, and expands the range of functions that can be modelled",
    "checked": true,
    "id": "9b396268f367917211bbd33947325e72b7742d36",
    "semantic_title": "attentive neural processes",
    "citation_count": 442,
    "authors": []
  },
  "https://openreview.net/forum?id=B1fpDsAqt7": {
    "title": "Visual Reasoning by Progressive Module Networks",
    "volume": "poster",
    "abstract": "Humans learn to solve tasks of increasing complexity by building on top of previously acquired knowledge. Typically, there exists a natural progression in the tasks that we learn – most do not require completely independent solutions, but can be broken down into simpler subtasks. We propose to represent a solver for each task as a neural module that calls existing modules (solvers for simpler tasks) in a functional program-like manner. Lower modules are a black box to the calling module, and communicate only via a query and an output. Thus, a module for a new task learns to query existing modules and composes their outputs in order to produce its own output. Our model effectively combines previous skill-sets, does not suffer from forgetting, and is fully differentiable. We test our model in learning a set of visual reasoning tasks, and demonstrate improved performances in all tasks by learning progressively. By evaluating the reasoning process using human judges, we show that our model is more interpretable than an attention-based baseline",
    "checked": true,
    "id": "7af4a37e6e63b5f06e7bfb6e7c8910322774efb9",
    "semantic_title": "visual reasoning by progressive module networks",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=Bkg2viA5FQ": {
    "title": "Hindsight policy gradients",
    "volume": "poster",
    "abstract": "A reinforcement learning agent that needs to pursue different goals across episodes requires a goal-conditional policy. In addition to their potential to generalize desirable behavior to unseen goals, such policies may also enable higher-level planning based on subgoals. In sparse-reward environments, the capacity to exploit information about the degree to which an arbitrary goal has been achieved while another goal was intended appears crucial to enable sample efficient learning. However, reinforcement learning agents have only recently been endowed with such capacity for hindsight. In this paper, we demonstrate how hindsight can be introduced to policy gradient methods, generalizing this idea to a broad class of successful algorithms. Our experiments on a diverse selection of sparse-reward environments show that hindsight leads to a remarkable increase in sample efficiency",
    "checked": true,
    "id": "1fa1f04b80f057e477549e6b9798fab7c7e57db5",
    "semantic_title": "hindsight policy gradients",
    "citation_count": 68,
    "authors": []
  },
  "https://openreview.net/forum?id=SkloDjAqYm": {
    "title": "LeMoNADe: Learned Motif and Neuronal Assembly Detection in calcium imaging videos",
    "volume": "poster",
    "abstract": "Neuronal assemblies, loosely defined as subsets of neurons with reoccurring spatio-temporally coordinated activation patterns, or \"motifs\", are thought to be building blocks of neural representations and information processing. We here propose LeMoNADe, a new exploratory data analysis method that facilitates hunting for motifs in calcium imaging videos, the dominant microscopic functional imaging modality in neurophysiology. Our nonparametric method extracts motifs directly from videos, bypassing the difficult intermediate step of spike extraction. Our technique augments variational autoencoders with a discrete stochastic node, and we show in detail how a differentiable reparametrization and relaxation can be used. An evaluation on simulated data, with available ground truth, reveals excellent quantitative performance. In real video data acquired from brain slices, with no ground truth available, LeMoNADe uncovers nontrivial candidate motifs that can help generate hypotheses for more focused biological investigations",
    "checked": true,
    "id": "bc652086b359a16dd8493270db22cc34f51e2ba8",
    "semantic_title": "lemonade: learned motif and neuronal assembly detection in calcium imaging videos",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=H1ziPjC5Fm": {
    "title": "Visual Explanation by Interpretation: Improving Visual Feedback Capabilities of Deep Neural Networks",
    "volume": "poster",
    "abstract": "Visual Interpretation and explanation of deep models is critical towards wide adoption of systems that rely on them. In this paper, we propose a novel scheme for both interpretation as well as explanation in which, given a pretrained model, we automatically identify internal features relevant for the set of classes considered by the model, without relying on additional annotations. We interpret the model through average visualizations of this reduced set of features. Then, at test time, we explain the network prediction by accompanying the predicted class label with supporting visualizations derived from the identified features. In addition, we propose a method to address the artifacts introduced by strided operations in deconvNet-based visualizations. Moreover, we introduce an8Flower , a dataset specifically designed for objective quantitative evaluation of methods for visual explanation. Experiments on the MNIST , ILSVRC 12, Fashion 144k and an8Flower datasets show that our method produces detailed explanations with good coverage of relevant features of the classes of interest",
    "checked": true,
    "id": "8d8bc608da14bc0ce32c3a5d1fdfbe037993626d",
    "semantic_title": "visual explanation by interpretation: improving visual feedback capabilities of deep neural networks",
    "citation_count": 62,
    "authors": []
  },
  "https://openreview.net/forum?id=BJg9DoR9t7": {
    "title": "Max-MIG: an Information Theoretic Approach for Joint Learning from Crowds",
    "volume": "poster",
    "abstract": "Eliciting labels from crowds is a potential way to obtain large labeled data. Despite a variety of methods developed for learning from crowds, a key challenge remains unsolved: \\emph{learning from crowds without knowing the information structure among the crowds a priori, when some people of the crowds make highly correlated mistakes and some of them label effortlessly (e.g. randomly)}. We propose an information theoretic approach, Max-MIG, for joint learning from crowds, with a common assumption: the crowdsourced labels and the data are independent conditioning on the ground truth. Max-MIG simultaneously aggregates the crowdsourced labels and learns an accurate data classifier. Furthermore, we devise an accurate data-crowds forecaster that employs both the data and the crowdsourced labels to forecast the ground truth. To the best of our knowledge, this is the first algorithm that solves the aforementioned challenge of learning from crowds. In addition to the theoretical validation, we also empirically show that our algorithm achieves the new state-of-the-art results in most settings, including the real-world data, and is the first algorithm that is robust to various information structures. Codes are available at https://github.com/Newbeeer/Max-MIG",
    "checked": true,
    "id": "789e1c58964f98f67471fe3552ed9934f24d10ff",
    "semantic_title": "max-mig: an information theoretic approach for joint learning from crowds",
    "citation_count": 55,
    "authors": []
  },
  "https://openreview.net/forum?id=BJfYvo09Y7": {
    "title": "Hierarchical Visuomotor Control of Humanoids",
    "volume": "poster",
    "abstract": "We aim to build complex humanoid agents that integrate perception, motor control, and memory. In this work, we partly factor this problem into low-level motor control from proprioception and high-level coordination of the low-level skills informed by vision. We develop an architecture capable of surprisingly flexible, task-directed motor control of a relatively high-DoF humanoid body by combining pre-training of low-level motor controllers with a high-level, task-focused controller that switches among low-level sub-policies. The resulting system is able to control a physically-simulated humanoid body to solve tasks that require coupling visual perception from an unstabilized egocentric RGB camera during locomotion in the environment. Supplementary video link: https://youtu.be/fBoir7PNxPk",
    "checked": true,
    "id": "a8a7219ee83cfd7ca258e20b5826a0c0786dcb73",
    "semantic_title": "hierarchical visuomotor control of humanoids",
    "citation_count": 97,
    "authors": []
  },
  "https://openreview.net/forum?id=BkgtDsCcKQ": {
    "title": "Function Space Particle Optimization for Bayesian Neural Networks",
    "volume": "poster",
    "abstract": "While Bayesian neural networks (BNNs) have drawn increasing attention, their posterior inference remains challenging, due to the high-dimensional and over-parameterized nature. To address this issue, several highly flexible and scalable variational inference procedures based on the idea of particle optimization have been proposed. These methods directly optimize a set of particles to approximate the target posterior. However, their application to BNNs often yields sub-optimal performance, as such methods have a particular failure mode on over-parameterized models. In this paper, we propose to solve this issue by performing particle optimization directly in the space of regression functions. We demonstrate through extensive experiments that our method successfully overcomes this issue, and outperforms strong baselines in a variety of tasks including prediction, defense against adversarial examples, and reinforcement learning",
    "checked": true,
    "id": "4e1e6f810375fd0a52404fc4f71eb566cbce3b99",
    "semantic_title": "function space particle optimization for bayesian neural networks",
    "citation_count": 65,
    "authors": []
  },
  "https://openreview.net/forum?id=SkMuPjRcKQ": {
    "title": "Feed-forward Propagation in Probabilistic Neural Networks with Categorical and Max Layers",
    "volume": "poster",
    "abstract": "Probabilistic Neural Networks deal with various sources of stochasticity: input noise, dropout, stochastic neurons, parameter uncertainties modeled as random variables, etc. In this paper we revisit a feed-forward propagation approach that allows one to estimate for each neuron its mean and variance w.r.t. all mentioned sources of stochasticity. In contrast, standard NNs propagate only point estimates, discarding the uncertainty. Methods propagating also the variance have been proposed by several authors in different context. The view presented here attempts to clarify the assumptions and derivation behind such methods, relate them to classical NNs and broaden their scope of applicability. The main technical contributions are new approximations for the distributions of argmax and max-related transforms, which allow for fully analytic uncertainty propagation in networks with softmax and max-pooling layers as well as leaky ReLU activations. We evaluate the accuracy of the approximation and suggest a simple calibration. Applying the method to networks with dropout allows for faster training and gives improved test likelihoods without the need of sampling",
    "checked": true,
    "id": "219ad53f77cb3a1953b8a766ec28c62ea8da49f8",
    "semantic_title": "feed-forward propagation in probabilistic neural networks with categorical and max layers",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=Hyl_vjC5KQ": {
    "title": "Hierarchical Reinforcement Learning via Advantage-Weighted Information Maximization",
    "volume": "poster",
    "abstract": "Real-world tasks are often highly structured. Hierarchical reinforcement learning (HRL) has attracted research interest as an approach for leveraging the hierarchical structure of a given task in reinforcement learning (RL). However, identifying the hierarchical policy structure that enhances the performance of RL is not a trivial task. In this paper, we propose an HRL method that learns a latent variable of a hierarchical policy using mutual information maximization. Our approach can be interpreted as a way to learn a discrete and latent representation of the state-action space. To learn option policies that correspond to modes of the advantage function, we introduce advantage-weighted importance sampling. In our HRL method, the gating policy learns to select option policies based on an option-value function, and these option policies are optimized based on the deterministic policy gradient method. This framework is derived by leveraging the analogy between a monolithic policy in standard RL and a hierarchical policy in HRL by using a deterministic option policy. Experimental results indicate that our HRL approach can learn a diversity of options and that it can enhance the performance of RL in continuous control tasks",
    "checked": true,
    "id": "1447cb195033be291674a44a07eb18ee894c23eb",
    "semantic_title": "hierarchical reinforcement learning via advantage-weighted information maximization",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=rJNwDjAqYX": {
    "title": "Large-Scale Study of Curiosity-Driven Learning",
    "volume": "poster",
    "abstract": "Reinforcement learning algorithms rely on carefully engineered rewards from the environment that are extrinsic to the agent. However, annotating each environment with hand-designed, dense rewards is difficult and not scalable, motivating the need for developing reward functions that are intrinsic to the agent. Curiosity is such intrinsic reward function which uses prediction error as a reward signal. In this paper: (a) We perform the first large-scale study of purely curiosity-driven learning, i.e. {\\em without any extrinsic rewards}, across $54$ standard benchmark environments, including the Atari game suite. Our results show surprisingly good performance as well as a high degree of alignment between the intrinsic curiosity objective and the hand-designed extrinsic rewards of many games. (b) We investigate the effect of using different feature spaces for computing prediction error and show that random features are sufficient for many popular RL game benchmarks, but learned features appear to generalize better (e.g. to novel game levels in Super Mario Bros.). (c) We demonstrate limitations of the prediction-based rewards in stochastic setups. Game-play videos and code are at https://doubleblindsupplementary.github.io/large-curiosity/",
    "checked": true,
    "id": "ca14dce53be20d3d23d4f0db844a8389ab619db3",
    "semantic_title": "large-scale study of curiosity-driven learning",
    "citation_count": 707,
    "authors": []
  },
  "https://openreview.net/forum?id=HJxwDiActX": {
    "title": "StrokeNet: A Neural Painting Environment",
    "volume": "poster",
    "abstract": "We've seen tremendous success of image generating models these years. Generating images through a neural network is usually pixel-based, which is fundamentally different from how humans create artwork using brushes. To imitate human drawing, interactions between the environment and the agent is required to allow trials. However, the environment is usually non-differentiable, leading to slow convergence and massive computation. In this paper we try to address the discrete nature of software environment with an intermediate, differentiable simulation. We present StrokeNet, a novel model where the agent is trained upon a well-crafted neural approximation of the painting environment. With this approach, our agent was able to learn to write characters such as MNIST digits faster than reinforcement learning approaches in an unsupervised manner. Our primary contribution is the neural simulation of a real-world environment. Furthermore, the agent trained with the emulated environment is able to directly transfer its skills to real-world software",
    "checked": true,
    "id": "412e4a122f5abc23ca9b8856d07d3dd962a93e5a",
    "semantic_title": "strokenet: a neural painting environment",
    "citation_count": 78,
    "authors": []
  },
  "https://openreview.net/forum?id=BkgBvsC9FQ": {
    "title": "DialogWAE: Multimodal Response Generation with Conditional Wasserstein Auto-Encoder",
    "volume": "poster",
    "abstract": "Variational autoencoders (VAEs) have shown a promise in data-driven conversation modeling. However, most VAE conversation models match the approximate posterior distribution over the latent variables to a simple prior such as standard normal distribution, thereby restricting the generated responses to a relatively simple (e.g., single-modal) scope. In this paper, we propose DialogWAE, a conditional Wasserstein autoencoder (WAE) specially designed for dialogue modeling. Unlike VAEs that impose a simple distribution over the latent variables, DialogWAE models the distribution of data by training a GAN within the latent variable space. Specifically, our model samples from the prior and posterior distributions over the latent variables by transforming context-dependent random noise using neural networks and minimizes the Wasserstein distance between the two distributions. We further develop a Gaussian mixture prior network to enrich the latent space. Experiments on two popular datasets show that DialogWAE outperforms the state-of-the-art approaches in generating more coherent, informative and diverse responses",
    "checked": true,
    "id": "77b9505e6967cfb45eaf6ec8ac7746cbaaab6e0d",
    "semantic_title": "dialogwae: multimodal response generation with conditional wasserstein auto-encoder",
    "citation_count": 131,
    "authors": []
  },
  "https://openreview.net/forum?id=ByMHvs0cFQ": {
    "title": "Quaternion Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "Recurrent neural networks (RNNs) are powerful architectures to model sequential data, due to their capability to learn short and long-term dependencies between the basic elements of a sequence. Nonetheless, popular tasks such as speech or images recognition, involve multi-dimensional input features that are characterized by strong internal dependencies between the dimensions of the input vector. We propose a novel quaternion recurrent neural network (QRNN), alongside with a quaternion long-short term memory neural network (QLSTM), that take into account both the external relations and these internal structural dependencies with the quaternion algebra. Similarly to capsules, quaternions allow the QRNN to code internal dependencies by composing and processing multidimensional features as single entities, while the recurrent operation reveals correlations between the elements composing the sequence. We show that both QRNN and QLSTM achieve better performances than RNN and LSTM in a realistic application of automatic speech recognition. Finally, we show that QRNN and QLSTM reduce by a maximum factor of 3.3x the number of free parameters needed, compared to real-valued RNNs and LSTMs to reach better results, leading to a more compact representation of the relevant information",
    "checked": true,
    "id": "31a857249f9f3bcdeb8a3b2944620fc16f128f64",
    "semantic_title": "quaternion recurrent neural networks",
    "citation_count": 132,
    "authors": []
  },
  "https://openreview.net/forum?id=SkfrvsA9FX": {
    "title": "Reward Constrained Policy Optimization",
    "volume": "poster",
    "abstract": "Solving tasks in Reinforcement Learning is no easy feat. As the goal of the agent is to maximize the accumulated reward, it often learns to exploit loopholes and misspecifications in the reward signal resulting in unwanted behavior. While constraints may solve this issue, there is no closed form solution for general constraints. In this work we present a novel multi-timescale approach for constrained policy optimization, called `Reward Constrained Policy Optimization' (RCPO), which uses an alternative penalty signal to guide the policy towards a constraint satisfying one. We prove the convergence of our approach and provide empirical evidence of its ability to train constraint satisfying policies",
    "checked": true,
    "id": "cb7c479a36520da1caeeec67db10772351a390c6",
    "semantic_title": "reward constrained policy optimization",
    "citation_count": 545,
    "authors": []
  },
  "https://openreview.net/forum?id=SJgNwi09Km": {
    "title": "Learning Latent Superstructures in Variational Autoencoders for Deep Multidimensional Clustering",
    "volume": "poster",
    "abstract": "We investigate a variant of variational autoencoders where there is a superstructure of discrete latent variables on top of the latent features. In general, our superstructure is a tree structure of multiple super latent variables and it is automatically learned from data. When there is only one latent variable in the superstructure, our model reduces to one that assumes the latent features to be generated from a Gaussian mixture model. We call our model the latent tree variational autoencoder (LTVAE). Whereas previous deep learning methods for clustering produce only one partition of data, LTVAE produces multiple partitions of data, each being given by one super latent variable. This is desirable because high dimensional data usually have many different natural facets and can be meaningfully partitioned in multiple ways",
    "checked": true,
    "id": "7b85357834e398437a291906aded59caff5151eb",
    "semantic_title": "learning latent superstructures in variational autoencoders for deep multidimensional clustering",
    "citation_count": 51,
    "authors": []
  },
  "https://openreview.net/forum?id=SygQvs0cFQ": {
    "title": "Variational Smoothing in Recurrent Neural Network Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "08736d66224afbcd5514947a79a08a424a6f0576",
    "semantic_title": "variational smoothing in recurrent neural network language models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=B1GMDsR5tm": {
    "title": "Initialized Equilibrium Propagation for Backprop-Free Training",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6811848615502f5347b3330110be6496bd2c6e4f",
    "semantic_title": "initialized equilibrium propagation for backprop-free training",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=H1z-PsR5KX": {
    "title": "Identifying and Controlling Important Neurons in Neural Machine Translation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c5489d244bfc1e9b0d8c94bf6dd774ee1aca2def",
    "semantic_title": "identifying and controlling important neurons in neural machine translation",
    "citation_count": 184,
    "authors": []
  },
  "https://openreview.net/forum?id=BJe-DsC5Fm": {
    "title": "signSGD via Zeroth-Order Oracle",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3ae1544865a18d8649a5c4939f9eb17165b23bea",
    "semantic_title": "signsgd via zeroth-order oracle",
    "citation_count": 136,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgbwsAcYm": {
    "title": "DELTA: DEEP LEARNING TRANSFER USING FEATURE MAP WITH ATTENTION FOR CONVOLUTIONAL NETWORKS",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "98627a34761bad5bd0582a7b03988de780b2d06b",
    "semantic_title": "delta: deep learning transfer using feature map with attention for convolutional networks",
    "citation_count": 172,
    "authors": []
  },
  "https://openreview.net/forum?id=B1GAUs0cKQ": {
    "title": "Variance Networks: When Expectation Does Not Meet Your Expectations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "58b8a5d5e46192c9bb0cf720cbe225f205ad1b3d",
    "semantic_title": "variance networks: when expectation does not meet your expectations",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=ryepUj0qtX": {
    "title": "Conditional Network Embeddings",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6625204c81fac0adba369d9ec924ac214e6eebc6",
    "semantic_title": "conditional network embeddings",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=SyMhLo0qKQ": {
    "title": "Distribution-Interpolation Trade off in Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3fda72294f9ca4f8afac856fbff26e83b0695e7a",
    "semantic_title": "distribution-interpolation trade off in generative models",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=rkzjUoAcFX": {
    "title": "Sample Efficient Adaptive Text-to-Speech",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "031c6baabd61f5b654ef4892f8f6ed737ec52511",
    "semantic_title": "sample efficient adaptive text-to-speech",
    "citation_count": 149,
    "authors": []
  },
  "https://openreview.net/forum?id=ByloIiCqYQ": {
    "title": "Maximal Divergence Sequential Autoencoder for Binary Software Vulnerability Detection",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "89272d09aaf902e70de0d68e5e9c15d810c8f418",
    "semantic_title": "maximal divergence sequential autoencoder for binary software vulnerability detection",
    "citation_count": 53,
    "authors": []
  },
  "https://openreview.net/forum?id=BkN5UoAqF7": {
    "title": "Sample Efficient Imitation Learning for Continuous Control",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "21946808f15dab6c73a76e89d1fa5869df515a3f",
    "semantic_title": "sample efficient imitation learning for continuous control",
    "citation_count": 58,
    "authors": []
  },
  "https://openreview.net/forum?id=ryE98iR5tm": {
    "title": "Practical lossless compression with latent variables using bits back coding",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c2ed34facd63d72e5d03ba13a6a3956ed6b2ac6c",
    "semantic_title": "practical lossless compression with latent variables using bits back coding",
    "citation_count": 142,
    "authors": []
  },
  "https://openreview.net/forum?id=HyxKIiAqYQ": {
    "title": "Context-adaptive Entropy Model for End-to-end Optimized Image Compression",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3b2409b9cc59b8f9b2d7baa200c1ac9e98b397da",
    "semantic_title": "context-adaptive entropy model for end-to-end optimized image compression",
    "citation_count": 398,
    "authors": []
  },
  "https://openreview.net/forum?id=ryM_IoAqYX": {
    "title": "Analysis of Quantized Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "02dae4e3494fe8f1b6591d4697a4e584605a3f6b",
    "semantic_title": "analysis of quantized models",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=H1edIiA9KQ": {
    "title": "Generating Multiple Objects at Spatially Distinct Locations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "15170be53e9b6d63df11ac231bdf20d9f550afdc",
    "semantic_title": "generating multiple objects at spatially distinct locations",
    "citation_count": 103,
    "authors": []
  },
  "https://openreview.net/forum?id=rkzDIiA5YQ": {
    "title": "ANYTIME MINIBATCH: EXPLOITING STRAGGLERS IN ONLINE DISTRIBUTED OPTIMIZATION",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "93a94c2c91da45bafe767a5d742011bb262f81b3",
    "semantic_title": "anytime minibatch: exploiting stragglers in online distributed optimization",
    "citation_count": 43,
    "authors": []
  },
  "https://openreview.net/forum?id=H1fU8iAqKX": {
    "title": "A rotation-equivariant convolutional neural network model of primary visual cortex",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "70c73ea3e534a46b6b2c1b91449b9e17678337ef",
    "semantic_title": "a rotation-equivariant convolutional neural network model of primary visual cortex",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=ryfMLoCqtQ": {
    "title": "An analytic theory of generalization dynamics and transfer learning in deep linear networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a77dc75ab9d3477cab828f492af638e5c27b5f4a",
    "semantic_title": "an analytic theory of generalization dynamics and transfer learning in deep linear networks",
    "citation_count": 131,
    "authors": []
  },
  "https://openreview.net/forum?id=r1lWUoA9FQ": {
    "title": "Are adversarial examples inevitable?",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "fd02c5b49bab02fb814c6999ebf161f3be377c75",
    "semantic_title": "are adversarial examples inevitable?",
    "citation_count": 283,
    "authors": []
  },
  "https://openreview.net/forum?id=BJeWUs05KQ": {
    "title": "Directed-Info GAIL: Learning Hierarchical Policies from Unsegmented Demonstrations using Directed Information",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b43d8c8b25bc65cbf3097480e9000649c79b7a51",
    "semantic_title": "directed-info gail: learning hierarchical policies from unsegmented demonstrations using directed information",
    "citation_count": 68,
    "authors": []
  },
  "https://openreview.net/forum?id=BkzeUiRcY7": {
    "title": "M^3RL: Mind-aware Multi-agent Management Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d341764a2020dcc3389f27d6508679e0c3cab486",
    "semantic_title": "m^3rl: mind-aware multi-agent management reinforcement learning",
    "citation_count": 53,
    "authors": []
  },
  "https://openreview.net/forum?id=ryggIs0cYQ": {
    "title": "Differentiable Learning-to-Normalize via Switchable Normalization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1c42f8ab39e22225ffd3222baeba4863435220a0",
    "semantic_title": "differentiable learning-to-normalize via switchable normalization",
    "citation_count": 177,
    "authors": []
  },
  "https://openreview.net/forum?id=SJxTroR9F7": {
    "title": "Supervised Policy Update for Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c4ecab02877c91f06340877640d9274e1f85a7a1",
    "semantic_title": "supervised policy update for deep reinforcement learning",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=Hyx6Bi0qYm": {
    "title": "Adversarial Domain Adaptation for Stable Brain-Machine Interfaces",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "301201dc73384b9dbc45a02e69db95210f93e6b8",
    "semantic_title": "adversarial domain adaptation for stable brain-machine interfaces",
    "citation_count": 75,
    "authors": []
  },
  "https://openreview.net/forum?id=SJe3HiC5KX": {
    "title": "LEARNING FACTORIZED REPRESENTATIONS FOR OPEN-SET DOMAIN ADAPTATION",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "211c42a6d949d60fa7eaaf5a7b7b28ebef36da9f",
    "semantic_title": "learning factorized representations for open-set domain adaptation",
    "citation_count": 53,
    "authors": []
  },
  "https://openreview.net/forum?id=H1xsSjC9Ym": {
    "title": "Learning to Understand Goal Specifications by Modelling Reward",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4a4b71ff918ca8eeffa5dfe66be2db7fcc1291da",
    "semantic_title": "learning to understand goal specifications by modelling reward",
    "citation_count": 159,
    "authors": []
  },
  "https://openreview.net/forum?id=H1goBoR9F7": {
    "title": "Dynamic Sparse Graph for Efficient Deep Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ba6985ee462ed8b09385a924aded5a45f74e7a59",
    "semantic_title": "dynamic sparse graph for efficient deep learning",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=SkEqro0ctQ": {
    "title": "Hierarchical interpretations for neural network predictions",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b3cedde36a6841b43162fc406b688e51bec68d36",
    "semantic_title": "hierarchical interpretations for neural network predictions",
    "citation_count": 146,
    "authors": []
  },
  "https://openreview.net/forum?id=BkG5SjR5YQ": {
    "title": "Post Selection Inference with Incomplete Maximum Mean Discrepancy Estimator",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0d93de3e89ea7ed04fe44be572472a244b97c678",
    "semantic_title": "post selection inference with incomplete maximum mean discrepancy estimator",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=BygqBiRcFQ": {
    "title": "Diffusion Scattering Transforms on Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "080b8de51221395038feb656f33783b65fa48434",
    "semantic_title": "diffusion scattering transforms on graphs",
    "citation_count": 103,
    "authors": []
  },
  "https://openreview.net/forum?id=Bye5SiAqKX": {
    "title": "Preconditioner on Matrix Lie Group for SGD",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "448491374325bd2c3f8cbc381b67b7742329144b",
    "semantic_title": "preconditioner on matrix lie group for sgd",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=ryeYHi0ctQ": {
    "title": "DPSNet: End-to-end Deep Plane Sweep Stereo",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "01dd15f339e8da4eb9a23e47983c1b7c480c7196",
    "semantic_title": "dpsnet: end-to-end deep plane sweep stereo",
    "citation_count": 229,
    "authors": []
  },
  "https://openreview.net/forum?id=S1eYHoC5FX": {
    "title": "DARTS: Differentiable Architecture Search",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c1f457e31b611da727f9aef76c283a18157dfa83",
    "semantic_title": "darts: differentiable architecture search",
    "citation_count": 4375,
    "authors": []
  },
  "https://openreview.net/forum?id=Syx_Ss05tm": {
    "title": "Adversarial Reprogramming of Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c8ed4a46e6aa565ea757e0ff5b7c160419699026",
    "semantic_title": "adversarial reprogramming of neural networks",
    "citation_count": 183,
    "authors": []
  },
  "https://openreview.net/forum?id=S1eOHo09KX": {
    "title": "Opportunistic Learning: Budgeted Cost-Sensitive Learning from Data Streams",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "ac87ce247e9e1d5cf9c394e7cf2ad7a239542d89",
    "semantic_title": "opportunistic learning: budgeted cost-sensitive learning from data streams",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=BJg_roAcK7": {
    "title": "INVASE: Instance-wise Variable Selection using Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "0e5793fc79f155a619a50aab2be4f577e87e3f99",
    "semantic_title": "invase: instance-wise variable selection using neural networks",
    "citation_count": 166,
    "authors": []
  },
  "https://openreview.net/forum?id=S1erHoR5t7": {
    "title": "The relativistic discriminator: a key element missing from standard GAN",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "dd2ebc42a1a4491b4179dec0ca8686d5c66f6bfe",
    "semantic_title": "the relativistic discriminator: a key element missing from standard gan",
    "citation_count": 974,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgBHoCqYX": {
    "title": "A Kernel Random Matrix-Based Approach for Sparse PCA",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "c2b13b979556d6fa1e7c0adc7cf37911e38cebe8",
    "semantic_title": "a kernel random matrix-based approach for sparse pca",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=rke4HiAcY7": {
    "title": "Caveats for information bottleneck in deterministic scenarios",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "7c789f89fd2a53d281cb67506a78a94f5c932333",
    "semantic_title": "caveats for information bottleneck in deterministic scenarios",
    "citation_count": 83,
    "authors": []
  },
  "https://openreview.net/forum?id=SJeXSo09FQ": {
    "title": "Learning Localized Generative Models for 3D Point Clouds via Graph Convolution",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "32b918246037976ba5f8363104ec042f56db42d6",
    "semantic_title": "learning localized generative models for 3d point clouds via graph convolution",
    "citation_count": 184,
    "authors": []
  },
  "https://openreview.net/forum?id=S1fQSiCcYm": {
    "title": "Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "3517b9824def42f3c723c6c63eda7ade12d25538",
    "semantic_title": "understanding and improving interpolation in autoencoders via an adversarial regularizer",
    "citation_count": 264,
    "authors": []
  },
  "https://openreview.net/forum?id=HJlmHoR5tQ": {
    "title": "Adversarial Imitation via Variational Inverse Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "c58a10a4923f59d02bd2dc2f7b9b13e7198d3383",
    "semantic_title": "adversarial imitation via variational inverse reinforcement learning",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=HyeGBj09Fm": {
    "title": "Generating Liquid Simulations with Deformation-aware Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "73a7e8165e83c5cfe2150c03b927c55102b2e815",
    "semantic_title": "generating liquid simulations with deformation-aware neural networks",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=ByxGSsR9FQ": {
    "title": "L2-Nonexpansive Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "ef2ec69e7c94b4194ba01719ac76d4595e6b4bdf",
    "semantic_title": "l2-nonexpansive neural networks",
    "citation_count": 74,
    "authors": []
  },
  "https://openreview.net/forum?id=ryGgSsAcFQ": {
    "title": "Deep, Skinny Neural Networks are not Universal Approximators",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "6b7c16120ef3324bdca4b8433fb1f89c761e4dfc",
    "semantic_title": "deep, skinny neural networks are not universal approximators",
    "citation_count": 66,
    "authors": []
  },
  "https://openreview.net/forum?id=ryGkSo0qYm": {
    "title": "Large Scale Graph Learning From Smooth Signals",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "6a432ad86a4c3c37005f8e46212ec4e0abea5482",
    "semantic_title": "large scale graph learning from smooth signals",
    "citation_count": 82,
    "authors": []
  },
  "https://openreview.net/forum?id=H1gTEj09FX": {
    "title": "RotDCF: Decomposition of Convolutional Filters for Rotation-Equivariant Deep Networks",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "8ccde9d80706a59e606f6e6d48d4260b60ccc736",
    "semantic_title": "rotdcf: decomposition of convolutional filters for rotation-equivariant deep networks",
    "citation_count": 43,
    "authors": []
  },
  "https://openreview.net/forum?id=rkxaNjA9Ym": {
    "title": "Per-Tensor Fixed-Point Quantization of the Back-Propagation Algorithm",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "24c338254c60e9a53443af4d4d99b8333a2a0e81",
    "semantic_title": "per-tensor fixed-point quantization of the back-propagation algorithm",
    "citation_count": 43,
    "authors": []
  },
  "https://openreview.net/forum?id=Skh4jRcKQ": {
    "title": "Understanding Straight-Through Estimator in Training Activation Quantized Neural Nets",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "cf0671ec7da36af49699de81bee05e9549140db2",
    "semantic_title": "understanding straight-through estimator in training activation quantized neural nets",
    "citation_count": 314,
    "authors": []
  },
  "https://openreview.net/forum?id=B1G5ViAqFm": {
    "title": "Convolutional Neural Networks on Non-uniform Geometrical Signals Using Euclidean Spectral Transformation",
    "volume": "poster",
    "abstract": "Convolutional Neural Networks (CNN) have been successful in processing data signals that are uniformly sampled in the spatial domain (e.g., images). However, most data signals do not natively exist on a grid, and in the process of being sampled onto a uniform physical grid suffer significant aliasing error and information loss. Moreover, signals can exist in different topological structures as, for example, points, lines, surfaces and volumes. It has been challenging to analyze signals with mixed topologies (for example, point cloud with surface mesh). To this end, we develop mathematical formulations for Non-Uniform Fourier Transforms (NUFT) to directly, and optimally, sample nonuniform data signals of different topologies defined on a simplex mesh into the spectral domain with no spatial sampling error. The spectral transform is performed in the Euclidean space, which removes the translation ambiguity from works on the graph spectrum. Our representation has four distinct advantages: (1) the process causes no spatial sampling error during initial sampling, (2) the generality of this approach provides a unified framework for using CNNs to analyze signals of mixed topologies, (3) it allows us to leverage state-of-the-art backbone CNN architectures for effective learning without having to design a particular architecture for a particular data structure in an ad-hoc fashion, and (4) the representation allows weighted meshes where each element has a different weight (i.e., texture) indicating local properties. We achieve good results on-par with state-of-the-art for 3D shape retrieval task, and new state-of-the-art for point cloud to surface reconstruction task",
    "checked": null,
    "id": "c7ae7cb97e954b33878c893ed237886c2bfc9e7d",
    "semantic_title": "convolutional neural networks on non-uniform geometrical signals using euclidean spectral transformation",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=BJfIVjAcKm": {
    "title": "Training for Faster Adversarial Robustness Verification via Inducing ReLU Stability",
    "volume": "poster",
    "abstract": "We explore the concept of co-design in the context of neural network verification. Specifically, we aim to train deep neural networks that not only are robust to adversarial perturbations but also whose robustness can be verified more easily. To this end, we identify two properties of network models - weight sparsity and so-called ReLU stability - that turn out to significantly impact the complexity of the corresponding verification task. We demonstrate that improving weight sparsity alone already enables us to turn computationally intractable verification problems into tractable ones. Then, improving ReLU stability leads to an additional 4-13x speedup in verification times. An important feature of our methodology is its \"universality,\" in the sense that it can be used with a broad range of training procedures and verification approaches",
    "checked": null,
    "id": "de49430578bb3f8de3e610423255662c45f17610",
    "semantic_title": "training for faster adversarial robustness verification via inducing relu stability",
    "citation_count": 200,
    "authors": []
  }
}