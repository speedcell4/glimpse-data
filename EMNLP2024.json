{
  "https://aclanthology.org/2024.emnlp-main.1": {
    "title": "UniGen: Universal Domain Generalization for Sentiment Classification via Zero-shot Dataset Generation",
    "volume": "main",
    "abstract": "Although pre-trained language models have exhibited great flexibility and versatility with prompt-based few-shot learning, they suffer from the extensive parameter size and limited applicability for inference. Recent studies have suggested that PLMs be used as dataset generators and a tiny task-specific model be trained to achieve efficient inference. However, their applicability to various domains is limited because they tend to generate domain-specific datasets. In this work, we propose a novel approach to universal domain generalization that generates a dataset regardless of the target domain. This allows for generalization of the tiny task model to any domain that shares the label space, thus enhancing the real-world applicability of the dataset generation paradigm. Our experiments indicate that the proposed method accomplishes generalizability across various domains while using a parameter set that is orders of magnitude smaller than PLMs",
    "checked": true,
    "id": "b2e32806ea51b93bf5fb734540072edeab314f8a",
    "semantic_title": "unigen: universal domain generalization for sentiment classification via zero-shot dataset generation",
    "citation_count": 0,
    "authors": [
      "Juhwan Choi",
      "Yeonghwa Kim",
      "Seunguk Yu",
      "JungMin Yun",
      "YoungBin Kim"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.2": {
    "title": "Multi-News+: Cost-efficient Dataset Cleansing via LLM-based Data Annotation",
    "volume": "main",
    "abstract": "The quality of the dataset is crucial for ensuring optimal performance and reliability of downstream task models. However, datasets often contain noisy data inadvertently included during the construction process. Numerous attempts have been made to correct this issue through human annotators. However, hiring and managing human annotators is expensive and time-consuming. As an alternative, recent studies are exploring the use of large language models (LLMs) for data annotation.In this study, we present a case study that extends the application of LLM-based data annotation to enhance the quality of existing datasets through a cleansing strategy. Specifically, we leverage approaches such as chain-of-thought and majority voting to imitate human annotation and classify unrelated documents from the Multi-News dataset, which is widely used for the multi-document summarization task. Through our proposed cleansing method, we introduce an enhanced Multi-News+. By employing LLMs for data cleansing, we demonstrate an efficient and effective approach to improving dataset quality without relying on expensive human annotation efforts",
    "checked": true,
    "id": "123871c749f6eeafc151cc6ab476f104aa32ffa0",
    "semantic_title": "multi-news+: cost-efficient dataset cleansing via llm-based data annotation",
    "citation_count": 1,
    "authors": [
      "Juhwan Choi",
      "JungMin Yun",
      "Kyohoon Jin",
      "YoungBin Kim"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.3": {
    "title": "FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document",
    "volume": "main",
    "abstract": "Through the advent of pre-trained language models, there have been notable advancements in abstractive summarization systems. Simultaneously, a considerable number of novel methods for evaluating factual consistency in abstractive summarization systems has been developed. But these evaluation approaches incorporate substantial limitations, especially on refinement and interpretability. In this work, we propose highly effective and interpretable factual inconsistency detection method FIZZ (Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document) for abstractive summarization systems that is based on fine-grained atomic facts decomposition. Moreover, we align atomic facts decomposed from the summary with the source document through adaptive granularity expansion. These atomic facts represent a more fine-grained unit of information, facilitating detailed understanding and interpretability of the summary's factual inconsistency. Experimental results demonstrate that our proposed factual consistency checking system significantly outperforms existing systems. We release the code at https://github.com/plm3332/FIZZ",
    "checked": true,
    "id": "f7451a0ae5919157aa26f4dd35acd896e070f7d8",
    "semantic_title": "fizz: factual inconsistency detection by zoom-in summary and zoom-out document",
    "citation_count": 0,
    "authors": [
      "Joonho Yang",
      "Seunghyun Yoon",
      "ByeongJeong Kim",
      "Hwanhee Lee"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.4": {
    "title": "Prompts have evil twins",
    "volume": "main",
    "abstract": "We discover that many natural-language prompts can be replaced by corresponding prompts that are unintelligible to humans but that provably elicit similar behavior in language models. We call these prompts \"evil twins\" because they are obfuscated and uninterpretable (evil), but at the same time mimic the functionality of the original natural-language prompts (twins). Remarkably, evil twins transfer between models. We find these prompts by solving a maximum-likelihood problem which has applications of independent interest",
    "checked": true,
    "id": "73fa74194e301ec164223fa017300cee4fc62b9c",
    "semantic_title": "prompts have evil twins",
    "citation_count": 2,
    "authors": [
      "Rimon Melamed",
      "Lucas McCabe",
      "Tanay Wakhare",
      "Yejin Kim",
      "H. Howie Huang",
      "Enric Boix-Adserà"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.5": {
    "title": "Table Question Answering for Low-resourced Indic Languages",
    "volume": "main",
    "abstract": "TableQA is the task of answering questions over tables of structured information, returning individual cells or tables as output. TableQA research has focused primarily on high-resource languages, leaving medium- and low-resource languages with little progress due to scarcity of annotated data and neural models. We address this gap by introducing a fully automatic large-scale tableQA data generation process for low-resource languages with limited budget. We incorporate our data generation method on two Indic languages, Bengali and Hindi, which have no tableQA datasets or models. TableQA models trained on our large-scale datasets outperform state-of-the-art LLMs. We further study the trained models on different aspects, including mathematical reasoning capabilities and zero-shot cross-lingual transfer. Our work is the first on low-resource tableQA focusing on scalable data generation and evaluation procedures. Our proposed data generation method can be applied to any low-resource language with a web presence. We release datasets, models, and code (https://github.com/kolk/Low-Resource-TableQA-Indic-languages)",
    "checked": true,
    "id": "261c76cbd1cc6adaf27589091a8720b5ee1cf9d8",
    "semantic_title": "table question answering for low-resourced indic languages",
    "citation_count": 0,
    "authors": [
      "Vaishali Pal",
      "Evangelos Kanoulas",
      "Andrew Yates",
      "Maarten Rijke"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.6": {
    "title": "ImageInWords: Unlocking Hyper-Detailed Image Descriptions",
    "volume": "main",
    "abstract": "Despite the longstanding adage \"an image is worth a thousand words,\" generating accurate hyper-detailed image descriptions remains unsolved. Trained on short web-scraped image-text, vision-language models often generate incomplete descriptions with visual inconsistencies. We address this via a novel data-centric approach with ImageInWords (IIW), a carefully designed human-in-the-loop framework for curating hyper-detailed image descriptions. Human evaluations on IIW data show major gains compared to recent datasets (+66%) and GPT-4V (+48%) across comprehensiveness, specificity, hallucinations, and more. We also show that fine-tuning with IIW data improves these metrics by +31% against models trained with prior work, even with only 9k samples. Lastly, we evaluate IIW models with text-to-image generation and vision-language reasoning tasks. Our generated descriptions result in the highest fidelity images, and boost compositional reasoning by up to 6% on ARO, SVO-Probes, and Winoground datasets. We release the IIW-Eval benchmark with human judgement labels, object and image-level annotations from our framework, and existing image caption datasets enriched via IIW-model",
    "checked": true,
    "id": "fbf1a2c841ae085216d3dee46e2ea545a7aeeb96",
    "semantic_title": "imageinwords: unlocking hyper-detailed image descriptions",
    "citation_count": 10,
    "authors": [
      "Roopal Garg",
      "Andrea Burns",
      "Burcu Karagol Ayan",
      "Yonatan Bitton",
      "Ceslee Montgomery",
      "Yasumasa Onoe",
      "Andrew Bunner",
      "Ranjay Krishna",
      "Jason Baldridge",
      "Radu Soricut"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.7": {
    "title": "LLM-Based Agent Society Investigation: Collaboration and Confrontation in Avalon Gameplay",
    "volume": "main",
    "abstract": "This paper explores the open research problem of understanding the social behaviors of LLM-based agents. Using Avalon as a testbed, we employ system prompts to guide LLM agents in gameplay. While previous studies have touched on gameplay with LLM agents, research on their social behaviors is lacking. We propose a novel framework, tailored for Avalon, features a multi-agent system facilitating efficient communication and interaction. We evaluate its performance based on game success and analyze LLM agents' social behaviors. Results affirm the framework's effectiveness in creating adaptive agents and suggest LLM-based agents' potential in navigating dynamic social interactions. By examining collaboration and confrontation behaviors, we offer insights into this field's research and applications",
    "checked": true,
    "id": "ff406e2ab8fdcce6b051cad1ead794c928440f77",
    "semantic_title": "llm-based agent society investigation: collaboration and confrontation in avalon gameplay",
    "citation_count": 21,
    "authors": [
      "Yihuai Lan",
      "Zhiqiang Hu",
      "Lei Wang",
      "Yang Wang",
      "Deheng Ye",
      "Peilin Zhao",
      "Ee-Peng Lim",
      "Hui Xiong",
      "Hao Wang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.8": {
    "title": "When LLMs Meets Acoustic Landmarks: An Efficient Approach to Integrate Speech into Large Language Models for Depression Detection",
    "volume": "main",
    "abstract": "Depression is a critical concern in global mental health, prompting extensive research into AI-based detection methods. Among various AI technologies, Large Language Models (LLMs) stand out for their versatility in healthcare applications. However, the application of LLMs in the identification and analysis of depressive states remains relatively unexplored, presenting an intriguing avenue for future research. In this paper, we present an innovative approach to employ an LLM in the realm of depression detection, integrating acoustic speech information into the LLM framework for this specific application. We investigate an efficient method for automatic depression detection by integrating speech signals into LLMs utilizing Acoustic Landmarks. This approach is not only valuable for the detection of depression but also represents a new perspective in enhancing the ability of LLMs to comprehend and process speech signals. By incorporating acoustic landmarks, which are specific to the pronunciation of spoken words, our method adds critical dimensions to text transcripts. This integration also provides insights into the unique speech patterns of individuals, revealing the potential mental states of individuals. By encoding acoustic landmarks information into LLMs, evaluations of the proposed approach on the DAIC-WOZ dataset reveal state-of-the-art results when compared with existing Audio-Text baselines",
    "checked": true,
    "id": "7b75b3d9f08aea9498baef8426f954106c3b5802",
    "semantic_title": "when llms meets acoustic landmarks: an efficient approach to integrate speech into large language models for depression detection",
    "citation_count": 3,
    "authors": [
      "Xiangyu Zhang",
      "Hexin Liu",
      "Kaishuai Xu",
      "Qiquan Zhang",
      "Daijiao Liu",
      "Beena Ahmed",
      "Julien Epps"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.9": {
    "title": "Speaking in Wavelet Domain: A Simple and Efficient Approach to Speed up Speech Diffusion Model",
    "volume": "main",
    "abstract": "Recently, Denoising Diffusion Probabilistic Models (DDPMs) have attained leading performances across a diverse range of generative tasks. However, in the field of speech synthesis, although DDPMs exhibit impressive performance, their prolonged training duration and substantial inference costs hinder practical deployment. Existing approaches primarily focus on enhancing inference speed, while approaches to accelerate training—a key factor in the costs associated with adding or customizing voices—often necessitate complex modifications to the model, compromising their universal applicability. To address the aforementioned challenges, we propose an inquiry: is it possible to enhance the training/inference speed and performance of DDPMs by modifying the speech signal itself? In this paper, we double the training and inference speed of Speech DDPMs by simply redirecting the generative target to the wavelet domain. This method not only achieves comparable or superior performance to the original model in speech synthesis tasks but also demonstrates its versatility. By investigating and utilizing different wavelet bases, our approach proves effective not just in speech synthesis, but also in speech enhancement",
    "checked": true,
    "id": "510278b81907bcc0f247eda5f69e99fbf8b3ff82",
    "semantic_title": "speaking in wavelet domain: a simple and efficient approach to speed up speech diffusion model",
    "citation_count": 0,
    "authors": [
      "Xiangyu Zhang",
      "Daijiao Liu",
      "Hexin Liu",
      "Qiquan Zhang",
      "Hanyu Meng",
      "Leibny Paola Garcia Perera",
      "EngSiong Chng",
      "Lina Yao"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.10": {
    "title": "Hateful Word in Context Classification",
    "volume": "main",
    "abstract": "Hate speech detection is a prevalent research field, yet it remains underexplored at the level of word meaning. This is significant, as terms used to convey hate often involve non-standard or novel usages which might be overlooked by commonly leveraged LMs trained on general language use. In this paper, we introduce the Hateful Word in Context Classification (HateWiC) task and present a dataset of ~4000 WiC-instances, each labeled by three annotators. Our analyses and computational exploration focus on the interplay between the subjective nature (context-dependent connotations) and the descriptive nature (as described in dictionary definitions) of hateful word senses. HateWiC annotations confirm that hatefulness of a word in context does not always derive from the sense definition alone. We explore the prediction of both majority and individual annotator labels, and we experiment with modeling context- and sense-based inputs. Our findings indicate that including definitions proves effective overall, yet not in cases where hateful connotations vary. Conversely, including annotator demographics becomes more important for mitigating performance drop in subjective hate prediction",
    "checked": false,
    "id": "63de8bfac59e406fbf449bd7d7692de0edc1b507",
    "semantic_title": "an automated toxicity classification on social media using lstm and word embedding",
    "citation_count": 16,
    "authors": [
      "Sanne Hoeken",
      "Sina Zarrieß",
      "Özge Alacam"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.11": {
    "title": "Eyes Don't Lie: Subjective Hate Annotation and Detection with Gaze",
    "volume": "main",
    "abstract": "Hate speech is a complex and subjective phenomenon. In this paper, we present a dataset (GAZE4HATE) that provides gaze data collected in a hate speech annotation experiment. We study whether the gaze of an annotator provides predictors of their subjective hatefulness rating, and how gaze features can improve Hate Speech Detection (HSD). We conduct experiments on statistical modeling of subjective hate ratings and gaze and analyze to what extent rationales derived from hate speech models correspond to human gaze and explanations in our data. Finally, we introduce MEANION, a first gaze-integrated HSD model. Our experiments show that particular gaze features like dwell time or fixation counts systematically correlate with annotators' subjective hate ratings and improve predictions of text-only hate speech models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Özge Alacam",
      "Sanne Hoeken",
      "Sina Zarrieß"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.12": {
    "title": "NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning",
    "volume": "main",
    "abstract": "Language models struggle with handling numerical data and performing arithmetic operations. We hypothesize that this limitation can be partially attributed to non-intuitive textual numbers representation. When a digit is read or generated by a causal language model it does not know its place value (e.g. thousands vs. hundreds) until the entire number is processed. To address this issue, we propose a simple adjustment to how numbers are represented by including the count of digits before each number. For instance, instead of \"42\", we suggest using \"2:42\" as the new format. This approach, which we term NumeroLogic, offers an added advantage in number generation by serving as a Chain of Thought (CoT). By requiring the model to consider the number of digits first, it enhances the reasoning process before generating the actual number. We use arithmetic tasks to demonstrate the effectiveness of the NumeroLogic formatting. We further demonstrate NumeroLogic applicability to general natural language modeling, improving language understanding performance in the MMLU benchmark",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eli Schwartz",
      "Leshem Choshen",
      "Joseph Shtok",
      "Sivan Doveh",
      "Leonid Karlinsky",
      "Assaf Arbelle"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.13": {
    "title": "Thinking\" Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models",
    "volume": "main",
    "abstract": "Existing debiasing techniques are typically training-based or require access to the model's internals and output distributions, so they are inaccessible to end-users looking to adapt LLM outputs for their particular needs. In this study, we examine whether structured prompting techniques can offer opportunities for fair text generation. We evaluate a comprehensive end-user-focused iterative framework of debiasing that applies System 2 thinking processes for prompts to induce logical, reflective, and critical text generation, with single, multi-step, instruction, and role-based variants. By systematically evaluating many LLMs across many datasets and different prompting strategies, we show that the more complex System 2-based Implicative Prompts significantly improve over other techniques demonstrating lower mean bias in the outputs with competitive performance on the downstream tasks. Our work offers research directions for the design and the potential of end-user-focused evaluative frameworks for LLM use",
    "checked": false,
    "id": "b868d93e76ad6a7434e0d70d4f08d2d52c7cbaf3",
    "semantic_title": "thinking fair and slow: on the efficacy of structured prompts for debiasing language models",
    "citation_count": 2,
    "authors": [
      "Shaz Furniturewala",
      "Surgan Jandial",
      "Abhinav Java",
      "Pragyan Banerjee",
      "Simra Shahid",
      "Sumit Bhatia",
      "Kokil Jaidka"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.14": {
    "title": "A Usage-centric Take on Intent Understanding in E-Commerce",
    "volume": "main",
    "abstract": "Identifying and understanding user intents is a pivotal task for E-Commerce. Despite its essential role in product recommendation and business user profiling analysis, intent understanding has not been consistently defined or accurately benchmarked. In this paper, we focus on predicative user intents as \"how a customer uses a product\", and pose intent understanding as a natural language reasoning task, independent of product ontologies. We identify two weaknesses of FolkScope, the SOTA E-Commerce Intent Knowledge Graph: category-rigidity and property-ambiguity. They limit its ability to strongly align user intents with products having the most desirable property, and to recommend useful products across diverse categories. Following these observations, we introduce a Product Recovery Benchmark featuring a novel evaluation framework and an example dataset. We further validate the above FolkScope weaknesses on this benchmark. Our code and dataset are available at https://github.com/stayones/Usgae-Centric-Intent-Understanding",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wendi Zhou",
      "Tianyi Li",
      "Pavlos Vougiouklis",
      "Mark Steedman",
      "Jeff Pan"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.15": {
    "title": "Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs",
    "volume": "main",
    "abstract": "Large language models (LLMs) encapsulate a vast amount of factual information within their pre-trained weights, as evidenced by their ability to answer diverse questions across different domains. However, this knowledge is inherently limited, relying heavily on the characteristics of the training data. Consequently, using external datasets to incorporate new information or refine the capabilities of LLMs on previously seen information poses a significant challenge. In this study, we compare two common approaches: unsupervised fine-tuning and retrieval-augmented generation (RAG). We evaluate both approaches on a variety of knowledge-intensive tasks across different topics. Our findings reveal that while unsupervised fine-tuning offers some improvement, RAG consistently outperforms it, both for existing knowledge encountered during training and entirely new knowledge. Moreover, we find that LLMs struggle to learn new factual information through unsupervised fine-tuning, and that exposing them to numerous variations of the same fact during training could alleviate this problem",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oded Ovadia",
      "Menachem Brief",
      "Moshik Mishaeli",
      "Oren Elisha"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.16": {
    "title": "Systematic Biases in LLM Simulations of Debates",
    "volume": "main",
    "abstract": "The emergence of Large Language Models (LLMs), has opened exciting possibilities for constructing computational simulations designed to replicate human behavior accurately. Current research suggests that LLM-based agents become increasingly human-like in their performance, sparking interest in using these AI agents as substitutes for human participants in behavioral studies. However, LLMs are complex statistical learners without straightforward deductive rules, making them prone to unexpected behaviors. Hence, it is crucial to study and pinpoint the key behavioral distinctions between humans and LLM-based agents. In this study, we highlight the limitations of LLMs in simulating human interactions, particularly focusing on LLMs' ability to simulate political debates on topics that are important aspects of people's day-to-day lives and decision-making processes. Our findings indicate a tendency for LLM agents to conform to the model's inherent social biases despite being directed to debate from certain political perspectives. This tendency results in behavioral patterns that seem to deviate from well-established social dynamics among humans. We reinforce these observations using an automatic self-fine-tuning method, which enables us to manipulate the biases within the LLM and demonstrate that agents subsequently align with the altered biases. These results underscore the need for further research to develop methods that help agents overcome these biases, a critical step toward creating more realistic simulations",
    "checked": true,
    "id": "f503b95c0a64f6a84eb1d90e5ea1e094b1e1892b",
    "semantic_title": "systematic biases in llm simulations of debates",
    "citation_count": 25,
    "authors": [
      "Amir Taubenfeld",
      "Yaniv Dover",
      "Roi Reichart",
      "Ariel Goldstein"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.17": {
    "title": "Studying and Mitigating Biases in Sign Language Understanding Models",
    "volume": "main",
    "abstract": "Ensuring that the benefits of sign language technologies are distributed equitably among all community members is crucial. Thus, it is important to address potential biases and inequities that may arise from the design or use of these resources. Crowd-sourced sign language datasets, such as the ASL Citizen dataset, are great resources for improving accessibility and preserving linguistic diversity, but they must be used thoughtfully to avoid reinforcing existing biases.In this work, we utilize the rich information about participant demographics and lexical features present in the ASL Citizen dataset to study and document the biases that may result from models trained on crowd-sourced sign datasets. Further, we apply several bias mitigation techniques during model training, and find that these techniques reduce performance disparities without decreasing accuracy. With the publication of this work, we release the demographic information about the participants in the ASL Citizen dataset to encourage future bias mitigation work in this space",
    "checked": true,
    "id": "4ac4a5831ea0db86c5236a115e9f78c0e40e1b4e",
    "semantic_title": "studying and mitigating biases in sign language understanding models",
    "citation_count": 0,
    "authors": [
      "Katherine Atwell",
      "Danielle Bragg",
      "Malihe Alikhani"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.18": {
    "title": "Uncertainty in Language Models: Assessment through Rank-Calibration",
    "volume": "main",
    "abstract": "Language Models (LMs) have shown promising performance in natural language generation. However, as LMs often generate incorrect or hallucinated responses, it is crucial to correctly quantify their uncertainty in responding to given inputs. In addition to verbalized confidence elicited via prompting, many uncertainty measures (e.g., semantic entropy and affinity-graph-based measures) have been proposed. However, these measures can differ greatly, and it is unclear how to compare them, partly because they take values over different ranges (e.g., [0,∞) or [0,1]). In this work, we address this issue by developing a novel and practical framework, termed *Rank-Calibration*, to assess uncertainty and confidence measures for LMs. Our key tenet is that higher uncertainty (or lower confidence) should imply lower generation quality, on average. Rank-calibration quantifies deviations from this ideal relationship in a principled manner, without requiring ad hoc binary thresholding of the correctness score (e.g., ROUGE or METEOR). The broad applicability and the granular interpretability of our methods are demonstrated empirically",
    "checked": true,
    "id": "ba63e1ab5b6e9d849982ae293ac0483053badaff",
    "semantic_title": "uncertainty in language models: assessment through rank-calibration",
    "citation_count": 11,
    "authors": [
      "Xinmeng Huang",
      "Shuo Li",
      "Mengxin Yu",
      "Matteo Sesia",
      "Hamed Hassani",
      "Insup Lee",
      "Osbert Bastani",
      "Edgar Dobriban"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.19": {
    "title": "RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning",
    "volume": "main",
    "abstract": "Tool learning has generated widespread interest as a vital means of interaction between Large Language Models (LLMs) and the physical world. Current research predominantly emphasizes LLMs' capacity to utilize tools in well-structured environments while overlooking their stability when confronted with the inevitable noise of the real world. To bridge this gap, we introduce *RoTBench*, a multi-level benchmark for evaluating the robustness of LLMs in tool learning. Specifically, we establish five external environments, each featuring varying levels of noise (i.e., Clean, Slight, Medium, Heavy, and Union), providing an in-depth analysis of the model's resilience across three critical phases: tool selection, parameter identification, and content filling. Experiments involving six widely-used models underscore the urgent necessity for enhancing the robustness of LLMs in tool learning. For instance, the performance of GPT-4 even drops significantly from 80.00 to 58.10 when there is no substantial change in manual accuracy. More surprisingly, the noise correction capability inherent in the GPT family paradoxically impedes its adaptability in the face of mild noise. In light of these findings, we propose RoTTuning, a strategy that enriches the diversity of training environments to bolster the robustness of LLMs in tool learning. The code and data are available at https://github.com/Junjie-Ye/RoTBench",
    "checked": true,
    "id": "d6beb9cc394f1e2046371678737346f05270ca91",
    "semantic_title": "rotbench: a multi-level benchmark for evaluating the robustness of large language models in tool learning",
    "citation_count": 7,
    "authors": [
      "Junjie Ye",
      "Yilong Wu",
      "Songyang Gao",
      "Caishuang Huang",
      "Sixian Li",
      "Guanyu Li",
      "Xiaoran Fan",
      "Qi Zhang",
      "Tao Gui",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.20": {
    "title": "Learning Planning-based Reasoning by Trajectories Collection and Process Reward Synthesizing",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have demonstrated significant potential in handling complex reasoning tasks through step-by-step rationale generation. However, recent studies have raised concerns regarding the hallucination and flaws in their reasoning process. Substantial efforts are being made to improve the reliability and faithfulness of the generated rationales. Some approaches model reasoning as planning, while others focus on annotating for process supervision. Nevertheless, the planning-based search process often results in high latency due to the frequent assessment of intermediate reasoning states and the extensive exploration space. Additionally, supervising the reasoning process with human annotation is costly and challenging to scale for LLM training. To address these issues, in this paper, we propose a framework to learn planning-based reasoning through Direct Preference Optimization (DPO) on collected trajectories, which are ranked according to synthesized process rewards. Our results on challenging logical reasoning benchmarks demonstrate the effectiveness of our learning framework, showing that our 7B model can surpass the strong counterparts like GPT-3.5-Turbo",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangkai Jiao",
      "Chengwei Qin",
      "Zhengyuan Liu",
      "Nancy Chen",
      "Shafiq Joty"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.21": {
    "title": "Scaling Properties of Speech Language Models",
    "volume": "main",
    "abstract": "Speech Language Models (SLMs) aim to learn language from raw audio, without textual resources. Despite significant advances, our current models exhibit weak syntax and semantic abilities. However, if the scaling properties of neural language models hold for the speech modality, these abilities will improve as the amount of compute used for training increases. In this paper, we use models of this scaling behavior to estimate the scale at which our current methods will yield a SLM with the English proficiency of text-based Large Language Models (LLMs). We establish a strong correlation between pre-training loss and downstream syntactic and semantic performance in SLMs and LLMs, which results in predictable scaling of linguistic performance. We show that the linguistic performance of SLMs scales up to three orders of magnitude more slowly than that of text-based LLMs. Additionally, we study the benefits of synthetic data designed to boost semantic understanding and the effects of coarser speech tokenization",
    "checked": true,
    "id": "a7eeee0df79da5662e7329d9710e97da032c3108",
    "semantic_title": "scaling properties of speech language models",
    "citation_count": 3,
    "authors": [
      "Santiago Cuervo",
      "Ricard Marxer"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.22": {
    "title": "We Demand Justice!\": Towards Social Context Grounding of Political Texts",
    "volume": "main",
    "abstract": "Political discourse on social media often contains similar language with opposing intended meanings. For example, the phrase thoughts and prayers, is used to express sympathy for mass shooting victims, as well as satirically criticize the lack of legislative action on gun control. Understanding such discourse fully by reading only the text is difficult. However, knowledge of the social context information makes it easier. We characterize the social context required to fully understand such ambiguous discourse, by grounding the text in real-world entities, actions, and attitudes. We propose two datasets that require understanding social context and benchmark them using large pre-trained language models and several novel structured models. We show that structured models, explicitly modeling social context, outperform larger models on both tasks, but still lag significantly behind human performance. Finally, we perform an extensive analysis, to obtain further insights into the language understanding challenges posed by our social grounding tasks",
    "checked": true,
    "id": "f6424db408c9bbc949d529d65e7f0ee535515414",
    "semantic_title": "we demand justice!\": towards social context grounding of political texts",
    "citation_count": 1,
    "authors": [
      "Rajkumar Pujari",
      "Chengfei Wu",
      "Dan Goldwasser"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.23": {
    "title": "An Experimental Analysis on Evaluating Patent Citations",
    "volume": "main",
    "abstract": "The patent citation count is a good indicator of patent quality. This often generates monetary value for the inventors and organizations. However, the factors that influence a patent receiving high citations over the year are still not well understood. With the patents over the past two decades, we study the problem of patent citation prediction and formulate this as a binary classification problem. We create a semantic graph of patents based on their semantic similarities, enabling the use of Graph Neural Network (GNN)-based approaches for predicting citations. Our experimental results demonstrate the effectiveness of our GNN-based methods when applied to the semantic graph, showing that they can accurately predict patent citations using only patent text. More specifically, these methods produce up to 94% recall for patents with high citations and outperform existing baselines. Furthermore, we leverage this constructed graph to gain insights and explanations for the predictions made by the GNNs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rabindra Nath Nandi",
      "Suman Maity",
      "Brian Uzzi",
      "Sourav Medya"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.24": {
    "title": "Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice?",
    "volume": "main",
    "abstract": "Traditionally, success in multilingual machine translation can be attributed to three key factors in training data: large volume, diverse translation directions, and high quality. In the current practice of fine-tuning large language models (LLMs) for translation, we revisit the importance of these factors. We find that LLMs display strong translation capability after being fine-tuned on as few as 32 parallel sentences and that fine-tuning on a single translation direction enables translation in multiple directions. However, the choice of direction is critical: fine-tuning LLMs with only English on the target side can lead to task misinterpretation, which hinders translation into non-English languages. Problems also arise when noisy synthetic data is placed on the target side, especially when the target language is well-represented in LLM pre-training. Yet interestingly, synthesized data in an under-represented language has a less pronounced effect. Our findings suggest that when adapting LLMs to translation, the requirement on data quantity can be eased but careful considerations are still crucial to prevent an LLM from exploiting unintended data biases",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dawei Zhu",
      "Pinzhen Chen",
      "Miaoran Zhang",
      "Barry Haddow",
      "Xiaoyu Shen",
      "Dietrich Klakow"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.25": {
    "title": "Consolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing",
    "volume": "main",
    "abstract": "The powerful generative abilities of large language models (LLMs) show potential in generating relevance labels for search applications. Previous work has found that directly asking about relevancy, such as \"*How relevant is document A to query Q?*\", results in suboptimal ranking. Instead, the pairwise-ranking prompting (PRP) approach produces promising ranking performance through asking about pairwise comparisons, e.g., \"*Is document A more relevant than document B to query Q?*\". Thus, while LLMs are effective at their ranking ability, this is not reflected in their relevance label generation.In this work, we propose a post-processing method to consolidate the relevance labels generated by an LLM with its powerful ranking abilities. Our method takes both LLM generated relevance labels and pairwise preferences. The labels are then altered to satisfy the pairwise preferences of the LLM, while staying as close to the original values as possible. Our experimental results indicate that our approach effectively balances label accuracy and ranking performance. Thereby, our work shows it is possible to combine both the ranking and labeling abilities of LLMs through post-processing",
    "checked": true,
    "id": "477d98492acd671c08b964b6d1e25b1161748d72",
    "semantic_title": "consolidating ranking and relevance predictions of large language models through post-processing",
    "citation_count": 2,
    "authors": [
      "Le Yan",
      "Zhen Qin",
      "Honglei Zhuang",
      "Rolf Jagerman",
      "Xuanhui Wang",
      "Michael Bendersky",
      "Harrie Oosterhuis"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.26": {
    "title": "Strength Lies in Differences! Improving Strategy Planning for Non-collaborative Dialogues via Diversified User Simulation",
    "volume": "main",
    "abstract": "We investigate non-collaborative dialogue agents, which are expected to engage in strategic conversations with diverse users, for securing a mutual agreement that leans favorably towards the system's objectives. This poses two main challenges for existing dialogue agents: 1) The inability to integrate user-specific characteristics into the strategic planning, and 2) The difficulty of training strategic planners that can be generalized to diverse users. To address these challenges, we propose TRIP to enhance the capability in tailored strategic planning, incorporating a user-aware strategic planning module and a population-based training paradigm. Through experiments on benchmark non-collaborative dialogue tasks, we demonstrate the effectiveness of TRIP in catering to diverse users",
    "checked": true,
    "id": "84d5d280f726ddf94ba85621581c4fe5f243a04f",
    "semantic_title": "strength lies in differences! improving strategy planning for non-collaborative dialogues via diversified user simulation",
    "citation_count": 2,
    "authors": [
      "Tong Zhang",
      "Chen Huang",
      "Yang Deng",
      "Hongru Liang",
      "Jia Liu",
      "Zujie Wen",
      "Wenqiang Lei",
      "Tat-Seng Chua"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.27": {
    "title": "Impeding LLM-assisted Cheating in Introductory Programming Assignments via Adversarial Perturbation",
    "volume": "main",
    "abstract": "While Large language model (LLM)-based programming assistants such as CoPilot and ChatGPT can help improve the productivity of professional software developers, they can also facilitate cheating in introductory computer programming courses. Assuming instructors have limited control over the industrial-strength models, this paper investigates the baseline performance of 5 widely used LLMs on a collection of introductory programming problems, examines adversarial perturbations to degrade their performance, and describes the results of a user study aimed at measuring the efficacy of such perturbations in hindering actual code generation for introductory programming assignments. The user study suggests that i) perturbations combinedly reduced the average correctness score by 77%, ii) the drop in correctness caused by these perturbations was affected based on their detectability",
    "checked": true,
    "id": "bb03245c2867c4252a4087a4e9a95794e7ed9025",
    "semantic_title": "impeding llm-assisted cheating in introductory programming assignments via adversarial perturbation",
    "citation_count": 0,
    "authors": [
      "Saiful Salim",
      "Rubin Yang",
      "Alexander Cooper",
      "Suryashree Ray",
      "Saumya Debray",
      "Sazzadur Rahaman"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.28": {
    "title": "Clustering and Ranking: Diversity-preserved Instruction Selection through Expert-aligned Quality Estimation",
    "volume": "main",
    "abstract": "With contributions from the open-source community, a vast amount of instruction tuning (IT) data has emerged. Given the significant resource allocation required by training and evaluating models, it is advantageous to have an efficient method for selecting high-quality IT data. However, existing methods for instruction data selection have limitations such as relying on fragile external APIs, being affected by biases in GPT models, or reducing the diversity of the selected instruction dataset. In this paper, we propose an industrial-friendly, expert-aligned and diversity-preserved instruction data selection method: Clustering and Ranking (CaR). CaR consists of two steps. The first step involves ranking instruction pairs using a scoring model that is well aligned with expert preferences (achieving an accuracy of 84.25%). The second step involves preserving dataset diversity through a clustering process. In our experiment, CaR selected a subset containing only 1.96% of Alpaca's IT data, yet the underlying AlpaCaR model trained on this subset outperforms Alpaca by an average of 32.1% in GPT-4 evaluations. Furthermore, our method utilizes small models (550M parameters) and requires only 11.2% of the monetary cost compared to existing methods, making it easily deployable in industrial scenarios",
    "checked": true,
    "id": "2403ca4ff39727bb1c922891d0320c07004bc17e",
    "semantic_title": "clustering and ranking: diversity-preserved instruction selection through expert-aligned quality estimation",
    "citation_count": 8,
    "authors": [
      "Yuan Ge",
      "Yilun Liu",
      "Chi Hu",
      "Weibin Meng",
      "Shimin Tao",
      "Xiaofeng Zhao",
      "Mahong Xia",
      "Zhang Li",
      "Boxing Chen",
      "Hao Yang",
      "Bei Li",
      "Tong Xiao",
      "JingBo Zhu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.29": {
    "title": "On the Influence of Gender and Race in Romantic Relationship Prediction from Large Language Models",
    "volume": "main",
    "abstract": "We study the presence of heteronormative biases and prejudice against interracial romantic relationships in large language models by performing controlled name-replacement experiments for the task of relationship prediction. We show that models are less likely to predict romantic relationships for (a) same-gender character pairs than different-gender pairs; and (b) intra/inter-racial character pairs involving Asian names as compared to Black, Hispanic, or White names. We examine the contextualized embeddings of first names and find that gender for Asian names is less discernible than non-Asian names. We discuss the social implications of our findings, underlining the need to prioritize the development of inclusive and equitable technology",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhilasha Sancheti",
      "Haozhe An",
      "Rachel Rudinger"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.30": {
    "title": "EmphAssess : a Prosodic Benchmark on Assessing Emphasis Transfer in Speech-to-Speech Models",
    "volume": "main",
    "abstract": "We introduce EmphAssess, a prosodic benchmark designed to evaluate the capability of speech-to-speech models to encode and reproduce prosodic emphasis. We apply this to two tasks: speech resynthesis and speech-to-speech translation. In both cases, the benchmark evaluates the ability of the model to encode emphasis in the speech input and accurately reproduce it in the output, potentially across a change of speaker and language. As part of the evaluation pipeline, we introduce EmphaClass, a new model that classifies emphasis at the frame or word level",
    "checked": true,
    "id": "436dfb4dc626e064232e2da3e8cb4469f0c585aa",
    "semantic_title": "emphassess : a prosodic benchmark on assessing emphasis transfer in speech-to-speech models",
    "citation_count": 3,
    "authors": [
      "Maureen Seyssel",
      "Antony D’Avirro",
      "Adina Williams",
      "Emmanuel Dupoux"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.31": {
    "title": "On Fake News Detection with LLM Enhanced Semantics Mining",
    "volume": "main",
    "abstract": "Large language models (LLMs) have emerged as valuable tools for enhancing textual features in various text-related tasks. Despite their superiority in capturing the lexical semantics between tokens for text analysis, our preliminary study on two popular LLMs, i.e., ChatGPT and Llama2, showcases that simply applying the news embeddings from LLMs is ineffective for fake news detection. Such embeddings only encapsulate the language styles between tokens. Meanwhile, the high-level semantics among named entities and topics, which reveal the deviating patterns of fake news, have been ignored. Therefore, we propose a topic model together with a set of specially designed prompts to extract topics and real entities from LLMs and model the relations among news, entities, and topics as a heterogeneous graph to facilitate investigating news semantics. We then propose a Generalized Page-Rank model and a consistent learning criteria for mining the local and global semantics centered on each news piece through the adaptive propagation of features across the graph. Our model shows superior performance on five benchmark datasets over seven baseline methods and the efficacy of the key ingredients has been thoroughly validated",
    "checked": true,
    "id": "26a3de5b063719a50a8bfd578811387b7929539c",
    "semantic_title": "on fake news detection with llm enhanced semantics mining",
    "citation_count": 0,
    "authors": [
      "Xiaoxiao Ma",
      "Yuchen Zhang",
      "Kaize Ding",
      "Jian Yang",
      "Jia Wu",
      "Hao Fan"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.32": {
    "title": "On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices",
    "volume": "main",
    "abstract": "While learning with limited labelled data can effectively deal with a lack of labels, it is also sensitive to the effects of uncontrolled randomness introduced by so-called randomness factors (i.e., non-deterministic decisions such as choice or order of samples). We propose and formalise a method to systematically investigate the effects of individual randomness factors while taking the interactions (dependence) between them into consideration. To this end, our method mitigates the effects of other factors while observing how the performance varies across multiple runs. Applying our method to multiple randomness factors across in-context learning and fine-tuning approaches on 7 representative text classification tasks and meta-learning on 3 tasks, we show that: 1) disregarding interactions between randomness factors in existing works led to inconsistent findings due to incorrect attribution of the effects of randomness factors, such as disproving the consistent sensitivity of in-context learning to sample order even with random sample selection; and 2) besides mutual interactions, the effects of randomness factors, especially sample order, are also dependent on more systematic choices unexplored in existing works, such as number of classes, samples per class or choice of prompt format",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Branislav Pecher",
      "Ivan Srba",
      "Maria Bielikova"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.33": {
    "title": "Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have demonstrated exceptional proficiency in instruction-following, making them increasingly integral to various applications. However, this capability introduces the risk of prompt injection attacks, where malicious instructions are embedded in the input to trigger unintended actions or content. Understanding the robustness of LLMs against such attacks is critical for ensuring their safe deployment. In this work, we establish a benchmark to evaluate the robustness of instruction-following LLMs against prompt injection attacks, assessing their ability to discern which instructions to follow and which to disregard. Through extensive experiments with leading instruction-following LLMs, we reveal significant vulnerabilities, particularly in models that mis-follow injected instructions. Our results show that certain models are excessively inclined to prioritize embedded instructions in prompts, often focusing on the latter parts of the prompt without fully understanding the overall context. Conversely, models that exhibit stronger contextual understanding and instruction-following capabilities tend to be more easily compromised by injected instructions. These findings highlight the need to balance improving LLMs' instruction-following abilities with enhancing their overall comprehension of prompts, to prevent mis-following inappropriate instructions. We hope our analysis provides valuable insights into these vulnerabilities, contributing to the development of more robust solutions in the future",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zekun Li",
      "Baolin Peng",
      "Pengcheng He",
      "Xifeng Yan"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.34": {
    "title": "A Study of Nationality Bias in Names and Perplexity using Off-the-Shelf Affect-related Tweet Classifiers",
    "volume": "main",
    "abstract": "In this paper, we apply a method to quantify biases associated with named entities from various countries. We create counterfactual examples with small perturbations on target-domain data instead of relying on templates or specific datasets for bias detection. On widely used classifiers for subjectivity analysis, including sentiment, emotion, hate speech, and offensive text using Twitter data, our results demonstrate positive biases related to the language spoken in a country across all classifiers studied. Notably, the presence of certain country names in a sentence can strongly influence predictions, up to a 23% change in hate speech detection and up to a 60% change in the prediction of negative emotions such as anger. We hypothesize that these biases stem from the training data of pre-trained language models (PLMs) and find correlations between affect predictions and PLMs likelihood in English and unknown languages like Basque and Maori, revealing distinct patterns with exacerbate correlations. Further, we followed these correlations in-between counterfactual examples from a same sentence to remove the syntactical component, uncovering interesting results suggesting the impact of the pre-training data was more important for English-speaking-country names",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Valentin Barriere",
      "Sebastian Cifuentes"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.35": {
    "title": "Mitigating the Alignment Tax of RLHF",
    "volume": "main",
    "abstract": "LLMs acquire a wide range of abilities during pre-training, but aligning LLMs under Reinforcement Learning with Human Feedback (RLHF) can lead to forgetting pretrained abilities, which is also known as the alignment tax. To investigate alignment tax, we conducted experiments with existing RLHF algorithms using OpenLLaMA-3B, which revealed a pronounced alignment tax in NLP tasks. Whereas, despite various techniques to mitigate forgetting, they are often at odds with the RLHF performance, leading to a trade-off between alignment performance and forgetting mitigation, leading to an alignment-forgetting trade-off. In this paper we show that model averaging, which simply interpolates between pre and post RLHF model weights, surprisingly achieves the most strongest alignment-forgetting Pareto front among a wide range of competing methods. To understand its effectiveness, we offer theoretical insights into model averaging, revealing that it enhances performance Pareto front by increasing feature diversity on the layers where tasks share overlapped feature spaces. Empirical evidence corroborates our analysis by showing the benefits of averaging low-level transformer layers. Building on the analysis and the observation that averaging different layers of the transformer leads to significantly different alignment-forgetting trade-offs, we propose Heterogeneous Model Averaging (HMA) to Heterogeneously find various combination ratios of model layers. HMA seeks to maximize the alignment performance while incurring minimal alignment tax. Moreover, we validate HMA's performance across a range of RLHF algorithms over OpenLLaMA-3B and further extend our findings to Mistral-7B which is evaluated by open-sourced preference model and GPT4. Code available here",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yong Lin",
      "Hangyu Lin",
      "Wei Xiong",
      "Shizhe Diao",
      "Jianmeng Liu",
      "Jipeng Zhang",
      "Rui Pan",
      "Haoxiang Wang",
      "Wenbin Hu",
      "Hanning Zhang",
      "Hanze Dong",
      "Renjie Pi",
      "Han Zhao",
      "Nan Jiang",
      "Heng Ji",
      "Yuan Yao",
      "Tong Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.36": {
    "title": "Evaluating Readability and Faithfulness of Concept-based Explanations",
    "volume": "main",
    "abstract": "With the growing popularity of general-purpose Large Language Models (LLMs), comes a need for more global explanations of model behaviors. Concept-based explanations arise as a promising avenue for explaining high-level patterns learned by LLMs. Yet their evaluation poses unique challenges, especially due to their non-local nature and high dimensional representation in a model's hidden space. Current methods approach concepts from different perspectives, lacking a unified formalization. This makes evaluating the core measures of concepts, namely faithfulness or readability, challenging. To bridge the gap, we introduce a formal definition of concepts generalizing to diverse concept-based explanations' settings. Based on this, we quantify the faithfulness of a concept explanation via perturbation. We ensure adequate perturbation in the high-dimensional space for different concepts via an optimization problem. Readability is approximated via an automatic and deterministic measure, quantifying the coherence of patterns that maximally activate a concept while aligning with human understanding. Finally, based on measurement theory, we apply a meta-evaluation method for evaluating these measures, generalizable to other types of explanations or tasks as well. Extensive experimental analysis has been conducted to inform the selection of explanation evaluation measures",
    "checked": true,
    "id": "3e46e2a679d5220169ac4f3ae1111e8a7052aee2",
    "semantic_title": "evaluating readability and faithfulness of concept-based explanations",
    "citation_count": 1,
    "authors": [
      "Meng Li",
      "Haoran Jin",
      "Ruixuan Huang",
      "Zhihao Xu",
      "Defu Lian",
      "Zijia Lin",
      "Di Zhang",
      "Xiting Wang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.37": {
    "title": "Personality-aware Student Simulation for Conversational Intelligent Tutoring Systems",
    "volume": "main",
    "abstract": "Intelligent Tutoring Systems (ITSs) can provide personalized and self-paced learning experience. The emergence of large language models (LLMs) further enables better human-machine interaction, and facilitates the development of conversational ITSs in various disciplines such as math and language learning. In dialogic teaching, recognizing and adapting to individual characteristics can significantly enhance student engagement and learning efficiency. However, characterizing and simulating student's persona remain challenging in training and evaluating conversational ITSs. In this work, we propose a framework to construct profiles of different student groups by refining and integrating both cognitive and noncognitive aspects, and leverage LLMs for personality-aware student simulation in a language learning scenario. We further enhance the framework with multi-aspect validation, and conduct extensive analysis from both teacher and student perspectives. Our experimental results show that state-of-the-art LLMs can produce diverse student responses according to the given language ability and personality traits, and trigger teacher's adaptive scaffolding strategies",
    "checked": true,
    "id": "6ae28b27d5e81aa7ad2dfdc7e3d712870159a7bc",
    "semantic_title": "personality-aware student simulation for conversational intelligent tutoring systems",
    "citation_count": 1,
    "authors": [
      "Zhengyuan Liu",
      "Stella Yin",
      "Geyu Lin",
      "Nancy Chen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.38": {
    "title": "MSI-Agent: Incorporating Multi-Scale Insight into Embodied Agents for Superior Planning and Decision-Making",
    "volume": "main",
    "abstract": "Insight gradually becomes a crucial form of long-term memory for an agent. However, the emergence of irrelevant insight and the lack of general insight can greatly undermine the effectiveness of insight. To solve this problem, in this paper, we introduce **M**ulti-**S**cale **I**nsight Agent (MSI-Agent), an embodied agent designed to improve LLMs' planning and decision-making ability by summarizing and utilizing insight effectively across different scales. MSI achieves this through the experience selector, insight generator, and insight selector. Leveraging a three-part pipeline, MSI can generate task-specific and high-level insight, store it in a database, and then use relevant insight from it to aid in decision-making. Our experiments show that MSI outperforms another insight strategy when planning by GPT3.5. Moreover, We delve into the strategies for selecting seed experience and insight, aiming to provide LLM with more useful and relevant insight for better decision-making. Our observations also indicate that MSI exhibits better robustness when facing domain-shifting scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dayuan Fu",
      "Biqing Qi",
      "Yihuai Gao",
      "Che Jiang",
      "Guanting Dong",
      "Bowen Zhou"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.39": {
    "title": "CoCoLoFa: A Dataset of News Comments with Common Logical Fallacies Written by LLM-Assisted Crowds",
    "volume": "main",
    "abstract": "Detecting logical fallacies in texts can help users spot argument flaws, but automating this detection is not easy. Manually annotating fallacies in large-scale, real-world text data to create datasets for developing and validating detection models is costly. This paper introduces CoCoLoFa, the largest known logical fallacy dataset, containing 7,706 comments for 648 news articles, with each comment labeled for fallacy presence and type. We recruited 143 crowd workers to write comments embodying specific fallacy types (e.g., slippery slope) in response to news articles. Recognizing the complexity of this writing task, we built an LLM-powered assistant into the workers' interface to aid in drafting and refining their comments. Experts rated the writing quality and labeling validity of CoCoLoFa as high and reliable. BERT-based models fine-tuned using CoCoLoFa achieved the highest fallacy detection (F1=0.86) and classification (F1=0.87) performance on its test set, outperforming the state-of-the-art LLMs. Our work shows that combining crowdsourcing and LLMs enables us to more effectively construct datasets for complex linguistic phenomena that crowd workers find challenging to produce on their own",
    "checked": true,
    "id": "277d0bd64fd9ef929c66d3d94904d6a9e6840cf8",
    "semantic_title": "cocolofa: a dataset of news comments with common logical fallacies written by llm-assisted crowds",
    "citation_count": 0,
    "authors": [
      "Min-Hsuan Yeh",
      "Ruyuan Wan",
      "Ting-Hao Huang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.40": {
    "title": "Tokenization Is More Than Compression",
    "volume": "main",
    "abstract": "Tokenization is a foundational step in natural language processing (NLP) tasks, bridging raw text and language models. Existing tokenization approaches like Byte-Pair Encoding (BPE) originate from the field of data compression, and it has been suggested that the effectiveness of BPE stems from its ability to condense text into a relatively small number of tokens. We test the hypothesis that fewer tokens lead to better downstream performance by introducing PathPiece, a new tokenizer that segments a document's text into the minimum number of tokens for a given vocabulary. Through extensive experimentation we find this hypothesis not to be the case, casting doubt on the understanding of the reasons for effective tokenization. To examine which other factors play a role, we evaluate design decisions across all three phases of tokenization: pre-tokenization, vocabulary construction, and segmentation, offering new insights into the design of effective tokenizers. Specifically, we illustrate the importance of pre-tokenization and the benefits of using BPE to initialize vocabulary construction. We train 64 language models with varying tokenization, ranging in size from 350M to 2.4B parameters, all of which are made publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Craig Schmidt",
      "Varshini Reddy",
      "Haoran Zhang",
      "Alec Alameddine",
      "Omri Uzan",
      "Yuval Pinter",
      "Chris Tanner"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.41": {
    "title": "FLIRT: Feedback Loop In-context Red Teaming",
    "volume": "main",
    "abstract": "Warning: this paper contains content that may be inappropriate or offensive.As generative models become available for public use in various applications, testing and analyzing vulnerabilities of these models has become a priority. In this work, we propose an automatic red teaming framework that evaluates a given black-box model and exposes its vulnerabilities against unsafe and inappropriate content generation. Our framework uses in-context learning in a feedback loop to red team models and trigger them into unsafe content generation. In particular, taking text-to-image models as target models, we explore different feedback mechanisms to automatically learn effective and diverse adversarial prompts. Our experiments demonstrate that even with enhanced safety features, Stable Diffusion (SD) models are vulnerable to our adversarial prompts, raising concerns on their robustness in practical uses. Furthermore, we demonstrate that the proposed framework is effective for red teaming text-to-text models",
    "checked": true,
    "id": "19443d48399d4fe89a4b0a96917c50c6fd9c5af1",
    "semantic_title": "flirt: feedback loop in-context red teaming",
    "citation_count": 42,
    "authors": [
      "Ninareh Mehrabi",
      "Palash Goyal",
      "Christophe Dupuy",
      "Qian Hu",
      "Shalini Ghosh",
      "Richard Zemel",
      "Kai-Wei Chang",
      "Aram Galstyan",
      "Rahul Gupta"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.42": {
    "title": "Successfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections",
    "volume": "main",
    "abstract": "Language models will inevitably err in situations with which they are unfamiliar. However, by effectively communicating uncertainties, they can still guide humans toward making sound decisions in those contexts. We demonstrate this idea by developing HEAR, a system that can successfully guide humans in simulated residential environments despite generating potentially inaccurate instructions. Diverging from systems that provide users with only the instructions they generate, HEAR warns users of potential errors in its instructions and suggests corrections. This rich uncertainty information effectively prevents misguidance and reduces the search space for users. Evaluation with 80 users shows that HEAR achieves a 13% increase in success rate and a 29% reduction in final location error distance compared to only presenting instructions to users. Interestingly, we find that offering users possibilities to explore, HEAR motivates them to make more attempts at the task, ultimately leading to a higher success rate. To our best knowledge, this work is the first to show the practical benefits of uncertainty communication in a long-horizon sequential decision-making problem",
    "checked": true,
    "id": "62d4a78f34315e8fb0b670988bd28a935cb2a60f",
    "semantic_title": "successfully guiding humans with imperfect instructions by highlighting potential errors and suggesting corrections",
    "citation_count": 0,
    "authors": [
      "Lingjun Zhao",
      "Khanh Nguyen",
      "Hal Daumé Iii"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.43": {
    "title": "Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks",
    "volume": "main",
    "abstract": "Large language models (LLMs) have demonstrated considerable proficiency in general natural language processing (NLP) tasks. Instruction tuning, a successful paradigm, enhances the ability of LLMs to follow natural language instructions and exhibit robust generalization across general tasks. However, these models often encounter performance limitations across multiple tasks due to constrained model capacity. Expanding this capacity during the instruction tuning phase poses significant challenges. To address this issue, we introduce parameter-efficient sparsity crafting (PESC), which crafts dense models into sparse models using the mixture-of-experts (MoE) architecture. PESC integrates adapters into the MoE layers of sparse models, differentiating experts without altering the individual weights within these layers. This method significantly reduces computational costs and GPU memory requirements, facilitating model capacity expansion through a minimal parameter increase when guaranteeing the quality of approximation in function space compared to original sparse upcycling. Our empirical evaluation demonstrates the effectiveness of the PESC method. Using PESC during instruction tuning, our best sparse model outperforms other sparse and dense models and exhibits superior general capabilities compared to GPT-3.5.Our code is available at https://github.com/wuhy68/Parameter-Efficient-MoE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyuan Wu",
      "Haisheng Zheng",
      "Zhuolun He",
      "Bei Yu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.44": {
    "title": "GeoGPT4V: Towards Geometric Multi-modal Large Language Models with Geometric Image Generation",
    "volume": "main",
    "abstract": "Large language models have seen widespread adoption in math problem-solving, yet for geometry problems, which often necessitate visual aids even for humans, the most advanced multi-modal models still struggle to effectively utilize image information. High-quality data is crucial for enhancing the geometric capabilities of multi-modal models, yet existing open-source datasets and related efforts are either too challenging for direct model learning or suffer from misalignment between text and images. To overcome this issue, we introduce a novel pipeline that leverages GPT-4 and GPT-4V to generate relatively basic geometry problems with aligned text and images, facilitating model learning. We have produced a dataset of 4.9K geometry problems and combined it with 19K open-source data to form our GeoGPT4V dataset. Experimental results demonstrate that the GeoGPT4V dataset significantly improves the geometry performance of various models on the MathVista and MathVision benchmarks. The code is available at https://anonymous.4open.science/r/GeoGPT4V-08B2",
    "checked": true,
    "id": "415e25e8e3c8c10255abbb704d545f9d02c54c76",
    "semantic_title": "geogpt4v: towards geometric multi-modal large language models with geometric image generation",
    "citation_count": 3,
    "authors": [
      "Shihao Cai",
      "Keqin Bao",
      "Hangyu Guo",
      "Jizhi Zhang",
      "Jun Song",
      "Bo Zheng"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.45": {
    "title": "DyVo: Dynamic Vocabularies for Learned Sparse Retrieval with Entities",
    "volume": "main",
    "abstract": "Learned Sparse Retrieval (LSR) models use vocabularies from pre-trained transformers, which often split entities into nonsensical fragments. Splitting entities diminishes retrieval accuracy and limits the model's ability to incorporate up-to-date world knowledge not included in the training data. In this work, we enhance the LSR vocabulary with Wikipedia concepts and entities, enabling the model to resolve ambiguities more effectively and stay current with evolving knowledge. Central to our approach is a Dynamic Vocabulary (DyVo) head, which leverages existing entity embeddings and an entity retrieval component that identifies entities relevant to a query or document. We use the DyVo head to generate entity weights, which are then merged with word piece weights to create joint representations for efficient indexing and retrieval using an inverted index. In experiments across three entity-rich document ranking datasets, the resulting DyVo model substantially outperforms several state-of-the-art baselines",
    "checked": true,
    "id": "c4a63893842f8278009e44cb34b864b25aadff3b",
    "semantic_title": "dyvo: dynamic vocabularies for learned sparse retrieval with entities",
    "citation_count": 0,
    "authors": [
      "Thong Nguyen",
      "Shubham Chatterjee",
      "Sean MacAvaney",
      "Iain Mackie",
      "Jeff Dalton",
      "Andrew Yates"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.46": {
    "title": "Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models",
    "volume": "main",
    "abstract": "Parameter-efficient fine-tuning (PEFT) is crucial for customizing Large Language Models (LLMs) with constrained resource. Although there have been various PEFT methods for dense-architecture LLMs, PEFT for sparse-architecture LLMs is still underexplored. In this work, we study the PEFT method for LLMs with the Mixture-of-Experts (MoE) architecture and the contents of this work are mainly threefold: (1) We investigate the dispersion degree of the activated experts in customized tasks, and found that the routing distribution for specific task tend to be highly concentrated, while the distribution of activated experts varies significantly across different tasks. (2) We propose the expert-specialized fine-tuning method, which tunes the experts most relevant to downstream tasks while freezing the other experts; experimental results demonstrate that our method not only improves the tuning efficiency, but also matches or even surpasses the performance of full-parameter fine-tuning. (3) We further analyze the impact of the MoE architecture on expert-specialized fine-tuning. We find that MoE models with finer-grained experts are more advantageous in selecting the combination of experts that are most relevant to downstream tasks, thereby enhancing the both the training efficiency and effectiveness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihan Wang",
      "Deli Chen",
      "Damai Dai",
      "Runxin Xu",
      "Zhuoshu Li",
      "Yu Wu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.47": {
    "title": "LongEmbed: Extending Embedding Models for Long Context Retrieval",
    "volume": "main",
    "abstract": "Embedding models play a pivotal role in modern NLP applications such as document retrieval. However, existing embedding models are limited to encoding short documents of typically 512 tokens, restrained from application scenarios requiring long inputs. This paper explores context window extension of existing embedding models, pushing their input length to a maximum of 32,768. We begin by evaluating the performance of existing embedding models using our newly constructed LongEmbed benchmark, which includes two synthetic and four real-world tasks, featuring documents of varying lengths and dispersed target information. The benchmarking results highlight huge opportunities for enhancement in current models. Via comprehensive experiments, we demonstrate that training-free context window extension strategies can effectively increase the input length of these models by several folds. Moreover, comparison of models using Absolute Position Encoding (APE) and Rotary Position Encoding (RoPE) reveals the superiority of RoPE-based embedding models in context window extension, offering empirical guidance for future models. Our benchmark, code and trained models will be released to advance the research in long context embedding models",
    "checked": true,
    "id": "bdd07688083de2fc792a48ba935cd33256066827",
    "semantic_title": "longembed: extending embedding models for long context retrieval",
    "citation_count": 9,
    "authors": [
      "Dawei Zhu",
      "Liang Wang",
      "Nan Yang",
      "Yifan Song",
      "Wenhao Wu",
      "Furu Wei",
      "Sujian Li"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.48": {
    "title": "Making Large Language Models Better Reasoners with Orchestrated Streaming Experiences",
    "volume": "main",
    "abstract": "Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps using chain-of-thought prompting under zero-shot or few-shot settings. However, zero-shot prompting always encounters low performance, and the superior performance of few-shot prompting hinges on the manual-crafting of task-specific demonstrations one by one. In this paper, we present **RoSE** (**R**easoning with **O**rchestrated **S**treaming **E**xperiences), a general framework for solving reasoning tasks that can self-improve as it answers various reasoning questions. To enable RoSE, we describe an architecture that extends an LLM to store all answered reasoning questions and their reasoning steps in a streaming experience pool and orchestrate helpful questions from the pool to assist itself in answering new questions. To set up a question-aware orchestration mechanism, RoSE first calculates the similarity of each question in the pool with the question to be answered. Since the solution to each question in the experience pool is not always correct, RoSE will sort the questions according to their similarity with the question to be answered, and then uniformly divide them into multiple buckets. It finally extracts one question from each bucket to make the extracted questions more diverse. To make the extracted questions help RoSE answer new questions as much as possible, we introduce two other attributes of uncertainty and complexity for each question. RoSE will preferentially select the questions with low uncertainty and high complexity from each bucket. We evaluate the versatility of RoSE in various complex reasoning tasks and LLMs, such as arithmetic and commonsense reasoning, and find that it can achieve excellent performance without any labeled data and pre-set unlabeled data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangyang Liu",
      "Junliang He",
      "Xipeng Qiu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.49": {
    "title": "Overcome Noise and Bias: Segmentation-Aided Multi-Granularity Denoising and Debiasing for Enhanced Quarduples Extraction in Dialogue",
    "volume": "main",
    "abstract": "Dialogue Aspect-based Sentiment Quadruple analysis (DiaASQ) extends ABSA to more complex real-world scenarios (i.e., dialogues), which makes existing generation methods encounter heightened noise and order bias challenges, leading to decreased robustness and accuracy.To address these, we propose the Segmentation-Aided multi-grained Denoising and Debiasing (SADD) method. For noise, we propose the Multi-Granularity Denoising Generation model (MGDG), achieving word-level denoising via sequence labeling and utterance-level denoising via topic-aware dialogue segmentation. Denoised Attention in MGDG integrates multi-grained denoising information to help generate denoised output.For order bias, we first theoretically analyze its direct cause as the gap between ideal and actual training objectives and propose a distribution-based solution. Since this solution introduces a one-to-many learning challenge, our proposed Segmentation-aided Order Bias Mitigation (SOBM) method utilizes dialogue segmentation to supplement order diversity, concurrently mitigating this challenge and order bias.Experiments demonstrate SADD's effectiveness, achieving state-of-the-art results with a 6.52% F1 improvement",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xianlong Luo",
      "Meng Yang",
      "Yihao Wang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.50": {
    "title": "Integrating Plutchik's Theory with Mixture of Experts for Enhancing Emotion Classification",
    "volume": "main",
    "abstract": "Emotion significantly influences human behavior and decision-making processes. We propose a labeling methodology grounded in Plutchik's Wheel of Emotions theory for emotion classification. Furthermore, we employ a Mixture of Experts (MoE) architecture to evaluate the efficacy of this labeling approach, by identifying the specific emotions that each expert learns to classify. Experimental results reveal that our methodology improves the performance of emotion classification",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongjun Lim",
      "Yun-Gyung Cheong"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.51": {
    "title": "In-context Contrastive Learning for Event Causality Identification",
    "volume": "main",
    "abstract": "Event Causality Identification (ECI) aims at determining the existence of a causal relation between two events. Although recent prompt learning-based approaches have shown promising improvements on the ECI task, their performance are often subject to the delicate design of multiple prompts and the positive correlations between the main task and derivate tasks. The in-context learning paradigm provides explicit guidance for label prediction in the prompt learning paradigm, alleviating its reliance on complex prompts and derivative tasks. However, it does not distinguish between positive and negative demonstrations for analogy learning. Motivated from such considerations, this paper proposes an **I**n-**C**ontext **C**ontrastive **L**earning (ICCL) model that utilizes contrastive learning to enhance the effectiveness of both positive and negative demonstrations. Additionally, we apply contrastive learning to event pairs to better facilitate event causality identification. Our ICCL is evaluated on the widely used corpora, including the EventStoryLine and Causal-TimeBank, and results show significant performance improvements over the state-of-the-art algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liang Chao",
      "Wei Xiang",
      "Bang Wang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.52": {
    "title": "What's Mine becomes Yours: Defining, Annotating and Detecting Context-Dependent Paraphrases in News Interview Dialogs",
    "volume": "main",
    "abstract": "Best practices for high conflict conversations like counseling or customer support almost always include recommendations to paraphrase the previous speaker. Although paraphrase classification has received widespread attention in NLP, paraphrases are usually considered independent from context, and common models and datasets are not applicable to dialog settings. In this work, we investigate paraphrases across turns in dialog (e.g., Speaker 1: \"That book is mine.\" becomes Speaker 2: \"That book is yours.\"). We provide an operationalization of context-dependent paraphrases, and develop a training for crowd-workers to classify paraphrases in dialog. We introduce ContextDeP, a dataset with utterance pairs from NPR and CNN news interviews annotated for context-dependent paraphrases. To enable analyses on label variation, the dataset contains 5,581 annotations on 600 utterance pairs. We present promising results with in-context learning and with token classification models for automatic paraphrase detection in dialog",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anna Wegmann",
      "Tijs Broek",
      "Dong Nguyen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.53": {
    "title": "Language Models Learn Rare Phenomena from Less Rare Phenomena: The Case of the Missing AANNs",
    "volume": "main",
    "abstract": "Language models learn rare syntactic phenomena, but the extent to which this is attributable to generalization vs. memorization is a major open question. To that end, we iteratively trained transformer language models on systematically manipulated corpora which were human-scale in size, and then evaluated their learning of a rare grammatical phenomenon: the English Article+Adjective+Numeral+Noun (AANN) construction (\"a beautiful five days\"). We compared how well this construction was learned on the default corpus relative to a counterfactual corpus in which AANN sentences were removed. We found that AANNs were still learned better than systematically perturbed variants of the construction. Using additional counterfactual corpora, we suggest that this learning occurs through generalization from related constructions (e.g., \"a few days\"). An additional experiment showed that this learning is enhanced when there is more variability in the input. Taken together, our results provide an existence proof that LMs can learn rare grammatical phenomena by generalization from less rare phenomena. Data and code: https://github.com/kanishkamisra/aannalysis",
    "checked": true,
    "id": "5df3397ff6007dc37e524272f42e54279982b54d",
    "semantic_title": "language models learn rare phenomena from less rare phenomena: the case of the missing aanns",
    "citation_count": 9,
    "authors": [
      "Kanishka Misra",
      "Kyle Mahowald"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.54": {
    "title": "Large Language Models for Data Annotation and Synthesis: A Survey",
    "volume": "main",
    "abstract": "Data annotation and synthesis generally refers to the labeling or generating of raw data with relevant information, which could be used for improving the efficacy of machine learning models. The process, however, is labor-intensive and costly. The emergence of advanced Large Language Models (LLMs), exemplified by GPT-4, presents an unprecedented opportunity to automate the complicated process of data annotation and synthesis. While existing surveys have extensively covered LLM architecture, training, and general applications, we uniquely focus on their specific utility for data annotation. This survey contributes to three core aspects: LLM-Based Annotation Generation, LLM-Generated Annotations Assessment, and LLM-Generated Annotations Utilization. Furthermore, this survey includes an in-depth taxonomy of data types that LLMs can annotate, a comprehensive review of learning strategies for models utilizing LLM-generated annotations, and a detailed discussion of the primary challenges and limitations associated with using LLMs for data annotation and synthesis. Serving as a key guide, this survey aims to assist researchers and practitioners in exploring the potential of the latest LLMs for data annotation, thereby fostering future advancements in this critical field",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Tan",
      "Dawei Li",
      "Song Wang",
      "Alimohammad Beigi",
      "Bohan Jiang",
      "Amrita Bhattacharjee",
      "Mansooreh Karami",
      "Jundong Li",
      "Lu Cheng",
      "Huan Liu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.55": {
    "title": "Chain-of-Dictionary Prompting Elicits Translation in Large Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) have shown surprisingly good performance in multilingual neural machine translation (MNMT) even if not being trained explicitly for translation. Yet, they still struggle with translating low-resource languages. As supported by our experiments, a bilingual dictionary between the source and the target language could help. Motivated by the fact that multilingual training effectively improves cross-lingual performance, we show that a chained multilingual dictionary with words expressed in more languages can provide more information to better enhance the LLM translation. To this end, we present a novel framework, CoD, Chain-of-Dictionary Prompting, which augments LLMs with prior knowledge with the chains of multilingual dictionaries for a subset of input words to elicit translation abilities for LLMs. Experiments indicate that ChatGPT and InstructGPT still have room for improvement in translating many language pairs. And CoD elicits large gains by up to 13x chrF++ points for MNMT (3.08 to 42.63 for English to Serbian written in Cyrillic script) on FLORES-200 full devtest set. We demonstrate the importance of chaining the multilingual dictionaries, as well as the superiority of CoD to few-shot in-context learning for low-resource languages. Using CoD helps ChatGPT to obviously surpass the SOTA translator NLLB 3.3B",
    "checked": true,
    "id": "97992c13baa6185c03d9e672f53185bc59822596",
    "semantic_title": "chain-of-dictionary prompting elicits translation in large language models",
    "citation_count": 10,
    "authors": [
      "Hongyuan Lu",
      "Haoran Yang",
      "Haoyang Huang",
      "Dongdong Zhang",
      "Wai Lam",
      "Furu Wei"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.56": {
    "title": "AdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for Memory-Efficient Large Language Models Fine-Tuning",
    "volume": "main",
    "abstract": "Fine-tuning large language models (LLMs) has achieved remarkable performance across various natural language processing tasks, yet it demands more and more memory as model sizes keep growing. To address this issue, the recently proposed Memory-efficient Zeroth-order (MeZO) methods attempt to fine-tune LLMs using only forward passes, thereby avoiding the need for a backpropagation graph. However, significant performance drops and a high risk of divergence have limited their widespread adoption. In this paper, we propose the Adaptive Zeroth-order Tensor-Train Adaption (AdaZeta) framework, specifically designed to improve the performance and convergence of the ZO methods. To enhance dimension-dependent ZO estimation accuracy, we introduce a fast-forward, low-parameter tensorized adapter. To tackle the frequently observed divergence issue in large-scale ZO fine-tuning tasks, we propose an adaptive query number schedule that guarantees convergence. Detailed theoretical analysis and extensive experimental results on Roberta-Large and Llama-2-7B models substantiate the efficacy of our AdaZeta framework in terms of accuracy, memory efficiency, and convergence speed",
    "checked": true,
    "id": "7a8a69d1ee140274dab4e63109cfd8513be127af",
    "semantic_title": "adazeta: adaptive zeroth-order tensor-train adaption for memory-efficient large language models fine-tuning",
    "citation_count": 2,
    "authors": [
      "Yifan Yang",
      "Kai Zhen",
      "Ershad Banijamali",
      "Athanasios Mouchtaris",
      "Zheng Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.57": {
    "title": "RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning",
    "volume": "main",
    "abstract": "Pre-trained language models, trained on large-scale corpora, demonstrate strong generalizability across various NLP tasks. Fine-tuning these models for specific tasks typically involves updating all parameters, which is resource-intensive. Parameter-efficient fine-tuning (PEFT) methods, such as the popular LoRA family, introduce low-rank matrices to learn only a few parameters efficiently. However, during inference, the product of these matrices updates all pre-trained parameters, complicating tasks like knowledge editing that require selective updates. We propose a novel PEFT method, which conducts row and column-wise sparse low-rank adaptation (RoseLoRA), to address this challenge. RoseLoRA identifies and updates only the most important parameters for a specific task, maintaining efficiency while preserving other model knowledge. By adding a sparsity constraint on the product of low-rank matrices and converting it to row and column-wise sparsity, we ensure efficient and precise model updates. Our theoretical analysis guarantees the lower bound of the sparsity with respective to the matrix product. Extensive experiments on five benchmarks across twenty datasets demonstrate that RoseLoRA outperforms baselines in both general fine-tuning and knowledge editing tasks",
    "checked": true,
    "id": "e483314241e09be61e1000b9522a2ea35643b2ef",
    "semantic_title": "roselora: row and column-wise sparse low-rank adaptation of pre-trained language model for knowledge editing and fine-tuning",
    "citation_count": 4,
    "authors": [
      "Haoyu Wang",
      "Tianci Liu",
      "Ruirui Li",
      "Monica Cheng",
      "Tuo Zhao",
      "Jing Gao"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.58": {
    "title": "BlendFilter: Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowledge Filtering",
    "volume": "main",
    "abstract": "Retrieval-augmented Large Language Models (LLMs) offer substantial benefits in enhancing performance across knowledge-intensive scenarios. However, these methods often struggle with complex inputs and encounter difficulties due to noisy knowledge retrieval, notably hindering model effectiveness. To address this issue, we introduce BlendFilter, a novel approach that elevates retrieval-augmented LLMs by integrating query generation blending with knowledge filtering. BlendFilter proposes the blending process through its query generation method, which integrates both external and internal knowledge augmentation with the original query, ensuring comprehensive information gathering. Additionally, our distinctive knowledge filtering module capitalizes on the intrinsic capabilities of the LLM, effectively eliminating extraneous data. We conduct extensive experiments on three open-domain question answering benchmarks, and the findings clearly indicate that our innovative BlendFilter surpasses state-of-the-art baselines significantly",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Wang",
      "Ruirui Li",
      "Haoming Jiang",
      "Jinjin Tian",
      "Zhengyang Wang",
      "Chen Luo",
      "Xianfeng Tang",
      "Monica Cheng",
      "Tuo Zhao",
      "Jing Gao"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.59": {
    "title": "HEART-felt Narratives: Tracing Empathy and Narrative Style in Personal Stories with LLMs",
    "volume": "main",
    "abstract": "Empathy serves as a cornerstone in enabling prosocial behaviors, and can be evoked through sharing of personal experiences in stories. While empathy is influenced by narrative content, intuitively, people respond to the way a story is told as well, through narrative style. Yet the relationship between empathy and narrative style is not fully understood. In this work, we empirically examine and quantify this relationship between style and empathy using LLMs and large-scale crowdsourcing studies. We introduce a novel, theory-based taxonomy, HEART (Human Empathy and Narrative Taxonomy) that delineates elements of narrative style that can lead to empathy with the narrator of a story. We establish the performance of LLMs in extracting narrative elements from HEART, showing that prompting with our taxonomy leads to reasonable, human-level annotations beyond what prior lexicon-based methods can do. To show empirical use of our taxonomy, we collect a dataset of empathy judgments of stories via a large-scale crowdsourcing study with N=2,624 participants. We show that narrative elements extracted via LLMs, in particular, vividness of emotions and plot volume, can elucidate the pathways by which narrative style cultivates empathy towards personal stories. Our work suggests that such models can be used for narrative analyses that lead to human-centered social and behavioral insights",
    "checked": true,
    "id": "6f38dc421b3f42eb737905262c867c6bcf6a77a4",
    "semantic_title": "heart-felt narratives: tracing empathy and narrative style in personal stories with llms",
    "citation_count": 0,
    "authors": [
      "Jocelyn Shen",
      "Joel Mire",
      "Hae Park",
      "Cynthia Breazeal",
      "Maarten Sap"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.60": {
    "title": "Eliminating Biased Length Reliance of Direct Preference Optimization via Down-Sampled KL Divergence",
    "volume": "main",
    "abstract": "Direct Preference Optimization (DPO) has emerged as a prominent algorithm for the direct and robust alignment of Large Language Models (LLMs) with human preferences, offering a more straightforward alternative to the complex Reinforcement Learning from Human Feedback (RLHF). Despite its promising efficacy, DPO faces a notable drawback: \"verbosity\", a common over-optimization phenomenon also observed in RLHF. While previous studies mainly attributed verbosity to biased labels within the data, we propose that the issue also stems from an inherent algorithmic length reliance in DPO. Specifically, we suggest that the discrepancy between sequence-level Kullback–Leibler (KL) divergences between chosen and rejected sequences, used in DPO, results in overestimated or underestimated rewards due to varying token lengths. Empirically, we utilize datasets with different label lengths to demonstrate the presence of biased rewards. We then introduce an effective downsampling approach, named SamPO, to eliminate potential length reliance. Our experimental evaluations, conducted across three LLMs of varying scales and a diverse array of conditional and open-ended benchmarks, highlight the efficacy of SamPO in mitigating verbosity, achieving improvements of 5% to 12% over DPO through debaised rewards. Our code can be accessed at: https://github.com/LuJunru/SamPO/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junru Lu",
      "Jiazheng Li",
      "Siyu An",
      "Meng Zhao",
      "Yulan He",
      "Di Yin",
      "Xing Sun"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.61": {
    "title": "Bridging Cultures in the Kitchen: A Framework and Benchmark for Cross-Cultural Recipe Retrieval",
    "volume": "main",
    "abstract": "The cross-cultural adaptation of recipes is an important application of identifying and bridging cultural differences in language. The challenge lies in retaining the essence of the original recipe while also aligning with the writing and dietary habits of the target culture. Information Retrieval (IR) offers a way to address the challenge because it retrieves results from the culinary practices of the target culture while maintaining relevance to the original recipe. We introduce a novel task about cross-cultural recipe retrieval and present a unique Chinese-English cross-cultural recipe retrieval benchmark. Our benchmark is manually annotated under limited resource, utilizing various retrieval models to generate a pool of candidate results for manual annotation. The dataset provides retrieval samples that are culturally adapted but textually diverse, presenting greater challenges. We propose CARROT, a plug-and-play cultural-aware recipe information retrieval framework that incorporates cultural-aware query rewriting and re-ranking methods and evaluate it both on our benchmark and intuitive human judgments. The results show that our framework significantly enhances the preservation of the original recipe and its cultural appropriateness for the target culture. We believe these insights will significantly contribute to future research on cultural adaptation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyi Hu",
      "Maria Maistro",
      "Daniel Hershcovich"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.62": {
    "title": "RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models",
    "volume": "main",
    "abstract": "The recent emergence of Medical Large Vision Language Models (Med-LVLMs) has enhanced medical diagnosis. However, current Med-LVLMs frequently encounter factual issues, often generating responses that do not align with established medical facts. Retrieval-Augmented Generation (RAG), which utilizes external knowledge, can improve the factual accuracy of these models but introduces two major challenges. First, limited retrieved contexts might not cover all necessary information, while excessive retrieval can introduce irrelevant and inaccurate references, interfering with the model's generation. Second, in cases where the model originally responds correctly, applying RAG can lead to an over-reliance on retrieved contexts, resulting in incorrect answers. To address these issues, we propose RULE, which consists of two components. First, we introduce a provably effective strategy for controlling factuality risk through the calibrated selection of the number of retrieved contexts. Second, based on samples where over-reliance on retrieved contexts led to errors, we curate a preference dataset to fine-tune the model, balancing its dependence on inherent knowledge and retrieved contexts for generation. We demonstrate the effectiveness of RAFE on three medical VQA datasets, achieving an average improvement of 20.8% in factual accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peng Xia",
      "Kangyu Zhu",
      "Haoran Li",
      "Hongtu Zhu",
      "Yun Li",
      "Gang Li",
      "Linjun Zhang",
      "Huaxiu Yao"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.63": {
    "title": "CryptoTrade: A Reflective LLM-based Agent to Guide Zero-shot Cryptocurrency Trading",
    "volume": "main",
    "abstract": "The utilization of Large Language Models (LLMs) in financial trading has primarily been concentrated within the stock market, aiding in economic and financial decisions. Yet, the unique opportunities presented by the cryptocurrency market, noted for its on-chain data's transparency and the critical influence of off-chain signals like news, remain largely untapped by LLMs. This work aims to bridge the gap by developing an LLM-based trading agent, CryptoTrade, which uniquely combines the analysis of on-chain and off-chain data. This approach leverages the transparency and immutability of on-chain data, as well as the timeliness and influence of off-chain signals, providing a comprehensive overview of the cryptocurrency market. CryptoTrade incorporates a reflective mechanism specifically engineered to refine its daily trading decisions by analyzing the outcomes of prior trading decisions. This research makes two significant contributions. Firstly, it broadens the applicability of LLMs to the domain of cryptocurrency trading. Secondly, it establishes a benchmark for cryptocurrency trading strategies. Through extensive experiments, CryptoTrade has demonstrated superior performance in maximizing returns compared to time-series baselines, but not compared to traditional trading signals, across various cryptocurrencies and market conditions. Our code and data are available at https://github.com/Xtra-Computing/CryptoTrade",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Li",
      "Bingqiao Luo",
      "Qian Wang",
      "Nuo Chen",
      "Xu Liu",
      "Bingsheng He"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.64": {
    "title": "A Survey on In-context Learning",
    "volume": "main",
    "abstract": "With the increasing capabilities of large language models (LLMs), in-context learning (ICL) has emerged as a new paradigm for natural language processing (NLP), where LLMs make predictions based on contexts augmented with a few examples. It has been a significant trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, prompt designing strategies, and related analysis. Additionally, we explore various ICL application scenarios, such as data engineering and knowledge updating. Finally, we address the challenges of ICL and suggest potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingxiu Dong",
      "Lei Li",
      "Damai Dai",
      "Ce Zheng",
      "Jingyuan Ma",
      "Rui Li",
      "Heming Xia",
      "Jingjing Xu",
      "Zhiyong Wu",
      "Baobao Chang",
      "Xu Sun",
      "Lei Li",
      "Zhifang Sui"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.65": {
    "title": "DocHieNet: A Large and Diverse Dataset for Document Hierarchy Parsing",
    "volume": "main",
    "abstract": "Parsing documents from pixels, such as pictures and scanned PDFs, into hierarchical structures is extensively demanded in the daily routines of data storage, retrieval and understanding. However, previously the research on this topic has been largely hindered since most existing datasets are small-scale, or contain documents of only a single type, which are characterized by a lack of document diversity. Moreover, there is a significant discrepancy in the annotation standards across datasets. In this paper, we introduce a large and diverse document hierarchy parsing (DHP) dataset to compensate for the data scarcity and inconsistency problem. We aim to set a new standard as a more practical, long-standing benchmark. Meanwhile, we present a new DHP framework designed to grasp both fine-grained text content and coarse-grained pattern at layout element level, enhancing the capacity of pre-trained text-layout models in handling the multi-page and multi-level challenges in DHP. Through exhaustive experiments, we validate the effectiveness of our proposed dataset and method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hangdi Xing",
      "Changxu Cheng",
      "Feiyu Gao",
      "Zirui Shao",
      "Zhi Yu",
      "Jiajun Bu",
      "Qi Zheng",
      "Cong Yao"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.66": {
    "title": "AMR-Evol: Adaptive Modular Response Evolution Elicits Better Knowledge Distillation for Large Language Models in Code Generation",
    "volume": "main",
    "abstract": "The impressive performance of proprietary LLMs like GPT4 in code generation has led to a trend to replicate these capabilities in open-source models through knowledge distillation (e.g. Code Evol-Instruct). However, these efforts often neglect the crucial aspect of response quality, relying heavily on teacher models for direct response distillation. This paradigm, especially for complex instructions, can degrade the quality of synthesized data, compromising the knowledge distillation process. To this end, our study introduces the Adaptive Modular Response Evolution (AMR-Evol) framework, which employs a two-stage process to refine response distillation. The first stage, modular decomposition, breaks down the direct response into more manageable sub-modules. The second stage, adaptive response evolution, automatically evolves the response with the related function modules. Our experiments with three popular code benchmarks—HumanEval, MBPP, and EvalPlus—attests to the superiority of the AMR-Evol framework over baseline response distillation methods. By comparing with the open-source Code LLMs trained on a similar scale of data, we observed performance enhancements: more than +3.0 points on HumanEval-Plus and +1.0 points on MBPP-Plus, which underscores the effectiveness of our framework. Our codes are available at https://github.com/ChiYeungLaw/AMR-Evol",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyang Luo",
      "Xin Li",
      "Hongzhan Lin",
      "Jing Ma",
      "Lidong Bing"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.67": {
    "title": "EFUF: Efficient Fine-Grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models",
    "volume": "main",
    "abstract": "Multimodal large language models (MLLMs) have attracted increasing attention in the past few years, but they may still generate descriptions that include objects not present in the corresponding images, a phenomenon known as object hallucination. To eliminate hallucinations, existing methods manually annotate paired responses with and without hallucinations, and then employ various alignment algorithms to improve the alignment capability between images and text. However, they not only demand considerable computation resources during the finetuning stage but also require expensive human annotation to construct paired data needed by the alignment algorithms. To address these issues, we propose an efficient fine-grained unlearning framework (EFUF), which performs gradient ascent utilizing three tailored losses to eliminate hallucinations without paired data. Extensive experiments show that our method consistently reduces hallucinations while preserving the generation quality with modest computational overhead. Our code and datasets will be publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shangyu Xing",
      "Fei Zhao",
      "Zhen Wu",
      "Tuo An",
      "Weihao Chen",
      "Chunhui Li",
      "Jianbing Zhang",
      "Xinyu Dai"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.68": {
    "title": "Rethinking Pruning Large Language Models: Benefits and Pitfalls of Reconstruction Error Minimization",
    "volume": "main",
    "abstract": "This work suggests fundamentally rethinking the current practice of pruning large language models (LLMs). The way it is done is by divide and conquer: split the model into submodels, sequentially prune them, and reconstruct predictions of the dense counterparts on small calibration data one at a time; the final model is obtained simply by putting the resulting sparse submodels together. While this approach enables pruning under memory constraints, it generates high reconstruction errors. In this work, we first present an array of reconstruction techniques that can significantly reduce this error by more than 90%. Unwittingly, however, we discover that minimizing reconstruction error is not always ideal and can overfit the given calibration data, resulting in rather increased language perplexity and poor performance at downstream tasks. We find out that a strategy of self-generating calibration data can mitigate this trade-off between reconstruction and generalization, suggesting new directions in the presence of both benefits and pitfalls of reconstruction for pruning LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sungbin Shin",
      "Wonpyo Park",
      "Jaeho Lee",
      "Namhoon Lee"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.69": {
    "title": "LLMs Are Zero-Shot Context-Aware Simultaneous Translators",
    "volume": "main",
    "abstract": "The advent of transformers has fueled progress in machine translation. More recently large language models (LLMs) have come to the spotlight thanks to their generality and strong performance in a wide range of language tasks, including translation. Here we show that open-source LLMs perform on par with or better than some state-of-the-art baselines in simultaneous machine translation (SiMT) tasks, zero-shot. We also demonstrate that injection of minimal background information, which is easy with an LLM, brings further performance gains, especially on challenging technical subject-matter. This highlights LLMs' potential for building next generation of massively multilingual, context-aware and terminologically accurate SiMT systems that require no resource-intensive training or fine-tuning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roman Koshkin",
      "Katsuhito Sudoh",
      "Satoshi Nakamura"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.70": {
    "title": "AgentReview: Exploring Peer Review Dynamics with LLM Agents",
    "volume": "main",
    "abstract": "Peer review is fundamental to the integrity and advancement of scientific publication. Traditional methods of peer review analyses often rely on exploration and statistics of existing peer review data, which do not adequately address the multivariate nature of the process, account for the latent variables, and are further constrained by privacy concerns due to the sensitive nature of the data. We introduce AgentReview, the first large language model (LLM) based peer review simulation framework, which effectively disentangles the impacts of multiple latent factors and addresses the privacy issue. Our study reveals significant insights, including a notable 37.1% variation in paper decisions due to reviewers' biases, supported by sociological theories such as the social influence theory, altruism fatigue, and authority bias. We believe that this study could offer valuable insights to improve the design of peer review mechanisms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiqiao Jin",
      "Qinlin Zhao",
      "Yiyang Wang",
      "Hao Chen",
      "Kaijie Zhu",
      "Yijia Xiao",
      "Jindong Wang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.71": {
    "title": "ChatRetriever: Adapting Large Language Models for Generalized and Robust Conversational Dense Retrieval",
    "volume": "main",
    "abstract": "Conversational search requires accurate interpretation of user intent from complex multi-turn contexts. This paper presents ChatRetriever, which inherits the strong generalization capability of large language models to robustly represent complex conversational sessions for dense retrieval. To achieve this, we propose a simple and effective dual-learning approach that adapts LLM for retrieval via contrastive learning while enhancing the complex session understanding through masked instruction tuning on high-quality conversational instruction tuning data. Extensive experiments on five conversational search benchmarks demonstrate that ChatRetriever significantly outperforms existing conversational dense retrievers, achieving state-of-the-art performance on par with LLM-based rewriting approaches. Furthermore, ChatRetriever exhibits superior robustness in handling diverse conversational contexts. Our work highlights the potential of adapting LLMs for retrieval with complex inputs like conversational search sessions and proposes an effective approach to advance this research direction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kelong Mao",
      "Chenlong Deng",
      "Haonan Chen",
      "Fengran Mo",
      "Zheng Liu",
      "Tetsuya Sakai",
      "Zhicheng Dou"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.72": {
    "title": "Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments",
    "volume": "main",
    "abstract": "Large language models (LLMs) have shown promising abilities as cost-effective and reference-free evaluators for assessing language generation quality. In particular, pairwise LLM evaluators, which compare two generated texts and determine the preferred one, have been employed in a wide range of applications. However, LLMs exhibit preference biases and worrying sensitivity to prompt designs. In this work, we first reveal that the predictive preference of LLMs can be highly brittle and skewed, even with semantically equivalent instructions. We find that fairer predictive preferences from LLMs consistently lead to judgments that are better aligned with humans. Motivated by this phenomenon, we propose an automatic Zero-shot Evaluation-oriented Prompt Optimization framework, ZEPO, which aims to produce fairer preference decisions and improve the alignment of LLM evaluators with human judgments. To this end, we propose a zero-shot learning objective based on the preference decision fairness. ZEPO demonstrates substantial performance improvements over state-of-the-art LLM evaluators, without requiring labeled data, on representative meta-evaluation benchmarks. Our findings underscore the critical correlation between preference fairness and human alignment, positioning ZEPO as an efficient prompt optimizer for bridging the gap between LLM evaluators and human judgments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Zhou",
      "Xingchen Wan",
      "Yinhong Liu",
      "Nigel Collier",
      "Ivan Vulić",
      "Anna Korhonen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.73": {
    "title": "Learning Interpretable Legal Case Retrieval via Knowledge-Guided Case Reformulation",
    "volume": "main",
    "abstract": "Legal case retrieval for sourcing similar cases is critical in upholding judicial fairness. Different from general web search, legal case retrieval involves processing lengthy, complex, and highly specialized legal documents. Existing methods in this domain often overlook the incorporation of legal expert knowledge, which is crucial for accurately understanding and modeling legal cases, leading to unsatisfactory retrieval performance. This paper introduces KELLER, a legal knowledge-guided case reformulation approach based on large language models (LLMs) for effective and interpretable legal case retrieval. By incorporating professional legal knowledge about crimes and law articles, we enable large language models to accurately reformulate the original legal case into concise sub-facts of crimes, which contain the essential information of the case. Extensive experiments on two legal case retrieval benchmarks demonstrate superior retrieval performance and robustness on complex legal case queries of KELLER over existing methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenlong Deng",
      "Kelong Mao",
      "Zhicheng Dou"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.74": {
    "title": "Effective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process",
    "volume": "main",
    "abstract": "In-context learning (ICL) is a few-shot learning paradigm that involves learning mappings through input-output pairs and appropriately applying them to new instances. Despite the remarkable ICL capabilities demonstrated by Large Language Models (LLMs), existing works are highly dependent on large-scale labeled support sets, not always feasible in practical scenarios. To refine this approach, we focus primarily on an innovative selective annotation mechanism, which precedes the standard demonstration retrieval. We introduce the Language Model-based Determinant Point Process (LM-DPP) that simultaneously considers the uncertainty and diversity of unlabeled instances for optimal selection. Consequently, this yields a subset for annotation that strikes a trade-off between the two factors. We apply LM-DPP to various language models, including GPT-J, LlaMA, and GPT-3. Experimental results on 9 NLU and 2 Generation datasets demonstrate that LM-DPP can effectively select canonical examples. Further analysis reveals that LLMs benefit most significantly from subsets that are both low uncertainty and high diversity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peng Wang",
      "Xiaobin Wang",
      "Chao Lou",
      "Shengyu Mao",
      "Pengjun Xie",
      "Yong Jiang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.75": {
    "title": "Pre-trained Language Models Do Not Help Auto-regressive Text-to-Image Generation",
    "volume": "main",
    "abstract": "Recent advances in image tokenizers, such as VQ-VAE, have enabled text-to-image generation using auto-regressive methods, similar to language modeling. However, these methods have yet to leverage pre-trained language models, despite their adaptability to various downstream tasks. In this work, we explore this gap by adapting a pre-trained language model for auto-regressive text-to-image generation, and find that pre-trained language models offer limited help. We provide a two-fold explanation by analyzing tokens from each modality. First, we demonstrate that image tokens possess significantly different semantics compared to text tokens, rendering pre-trained language models no more effective in modeling them than randomly initialized ones. Second, the text tokens in the image-text datasets are too simple compared to normal language model pre-training data, which causes the catastrophic degradation of language models' capability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhui Zhang",
      "Brandon McKinzie",
      "Zhe Gan",
      "Vaishaal Shankar",
      "Alexander Toshev"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.76": {
    "title": "QUDSELECT: Selective Decoding for Questions Under Discussion Parsing",
    "volume": "main",
    "abstract": "Question Under Discussion (QUD) is a discourse framework that uses implicit questions to reveal discourse relationships between sentences. In QUD parsing, each sentence is viewed as an answer to a question triggered by an anchor sentence in prior context. The resulting QUD structure is required to conform to several theoretical criteria like answer compatibility(how well the question is answered), making QUD parsing a challenging task. Previous works construct QUD parsers in a pipelined manner (i.e. detect the trigger sentence in context and then generate the question). However, these parsers lack a holistic view of the task and can hardly satisfy all the criteria. In this work, we introduce QUDSELECT, a joint-training framework that selectively decodes the QUD dependency structures considering the QUD criteria criteria. Using instruction-tuning, we train models to simultaneously predict the anchor sentence and generate the associated question. To explicitly incorporate the criteria, we adopt a selective decoding strategy of sampling multiple QUD candidates during inference, followed by selecting the best one with criteria scorers. Our method outperforms the state-of-the-art baseline models by 9% in human evaluation and 4% in automatic evaluation, demonstrating the effectiveness of our framework. Code and data are in https://github.com/asuvarna31/qudselect",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashima Suvarna",
      "Xiao Liu",
      "Tanmay Parekh",
      "Kai-Wei Chang",
      "Nanyun Peng"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.77": {
    "title": "Mitigating Language Bias of LMMs in Social Intelligence Understanding with Virtual Counterfactual Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peng Chen",
      "Xiao-Yu Guo",
      "Yuan-Fang Li",
      "Xiaowang Zhang",
      "Zhiyong Feng"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.78": {
    "title": "Model Balancing Helps Low-data Training and Fine-tuning",
    "volume": "main",
    "abstract": "Recent advances in foundation models have emphasized the need to align pre-trained models with specialized domains using small, curated datasets. Studies on these foundation models underscore the importance of low-data training and fine-tuning. This topic, well-known in natural language processing (NLP), has also gained increasing attention in the emerging field of scientific machine learning (SciML). To address the limitations of low-data training and fine-tuning, we draw inspiration from Heavy-Tailed Self-Regularization (HT-SR) theory, analyzing the shape of empirical spectral densities (ESDs) and revealing an imbalance in training quality across different model layers. To mitigate this issue, we adapt a recently proposed layer-wise learning rate scheduler, TempBalance, which effectively balances training quality across layers and enhances low-data training and fine-tuning for both NLP and SciML tasks. Notably, TempBalance demonstrates increasing performance gains as the amount of available tuning data decreases. Comparative analyses further highlight the effectiveness of TempBalance and its adaptability as an \"add-on\" method for improving model performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihang Liu",
      "Yuanzhe Hu",
      "Tianyu Pang",
      "Yefan Zhou",
      "Pu Ren",
      "Yaoqing Yang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.79": {
    "title": "Reuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment",
    "volume": "main",
    "abstract": "Aligning language models (LMs) based on human-annotated preference data is a crucial step in obtaining practical and performant LM-based systems. However, multilingual human preference data are difficult to obtain at scale, making it challenging to extend this framework to diverse languages. In this work, we evaluate a simple approach for zero-shot cross-lingual alignment, where a reward model is trained on preference data in one source language and directly applied to other target languages. On summarization and open-ended dialog generation, we show that this method is consistently successful under comprehensive evaluation settings, including human evaluation: cross-lingually aligned models are preferred by humans over unaligned models on up to >70% of evaluation instances. We moreover find that a different-language reward model sometimes yields better aligned models than a same-language reward model. We also identify best practices when there is no language-specific data for even supervised finetuning, another component in alignment",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaofeng Wu",
      "Ananth Balashankar",
      "Yoon Kim",
      "Jacob Eisenstein",
      "Ahmad Beirami"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.80": {
    "title": "Large Language Models as Foundations for Next-Gen Dense Retrieval: A Comprehensive Empirical Assessment",
    "volume": "main",
    "abstract": "Pre-trained language models like BERT and T5 serve as crucial backbone encoders for dense retrieval. However, these models often exhibit limited generalization capabilities and face challenges in improving in-domain accuracy. Recent research has explored using large language models (LLMs) as retrievers, achieving state-of-the-art performance across various tasks. Despite these advancements, the specific benefits of LLMs over traditional retrievers and the impact of different LLM configurations—such as parameter sizes, pre-training duration, and alignment processes—on retrieval tasks remain unclear. In this work, we conduct a comprehensive empirical study on a wide range of retrieval tasks, including in-domain accuracy, data efficiency, zero-shot generalization, lengthy retrieval, instruction-based retrieval, and multi-task learning. We evaluate over 15 different backbone LLMs and non-LLMs. Our findings reveal that larger models and extensive pre-training consistently enhance in-domain accuracy and data efficiency. Additionally, larger models demonstrate significant potential in zero-shot generalization, lengthy retrieval, instruction-based retrieval, and multi-task learning. These results underscore the advantages of LLMs as versatile and effective backbone encoders in dense retrieval, providing valuable insights for future research and development in this field",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kun Luo",
      "Minghao Qin",
      "Zheng Liu",
      "Shitao Xiao",
      "Jun Zhao",
      "Kang Liu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.81": {
    "title": "A New Pipeline for Knowledge Graph Reasoning Enhanced by Large Language Models Without Fine-Tuning",
    "volume": "main",
    "abstract": "Conventional Knowledge Graph Reasoning (KGR) models learn the embeddings of KG components over the structure of KGs, but their performances are limited when the KGs are severely incomplete. Recent LLM-enhanced KGR models input KG structural information into LLMs. However, they require fine-tuning on open-source LLMs and are not applicable to closed-source LLMs. Therefore, in this paper, to leverage the knowledge in LLMs without fine-tuning to assist and enhance conventional KGR models, we propose a new three-stage pipeline, including knowledge alignment, KG reasoning and entity reranking. Specifically, in the alignment stage, we propose three strategies to align the knowledge in LLMs to the KG schema by explicitly associating unconnected nodes with semantic relations. Based on the enriched KGs, we train structure-aware KGR models to integrate aligned knowledge to original knowledge existing in KGs. In the reranking stage, after obtaining the results of KGR models, we rerank the top-scored entities with LLMs to recall correct answers further. Experiments show our pipeline can enhance the KGR performance in both incomplete and general situations. Code and datasets are available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongwu Chen",
      "Long Bai",
      "Zixuan Li",
      "Zhen Huang",
      "Xiaolong Jin",
      "Yong Dou"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.82": {
    "title": "Towards Tool Use Alignment of Large Language Models",
    "volume": "main",
    "abstract": "Recently, tool use with LLMs has become one of the primary research topics as it can help LLM generate truthful and helpful responses. Existing studies on tool use with LLMs primarily focus on enhancing the tool-calling ability of LLMs. In practice, like chat assistants, LLMs are also required to align with human values in the context of tool use. Specifically, LLMs should refuse to answer unsafe tool use relevant instructions and insecure tool responses to ensure their reliability and harmlessness. At the same time, LLMs should demonstrate autonomy in tool use to reduce the costs associated with tool calling. To tackle this issue, we first introduce the principle that LLMs should follow in tool use scenarios: H2A. The goal of H2A is to align LLMs with **helpfulness**, **harmlessness**, and **autonomy**. In addition, we propose ToolAlign, a dataset comprising instruction-tuning data and preference data to align LLMs with the H2A principle for tool use. Based on ToolAlign, we develop LLMs by supervised fine-tuning and preference learning, and experimental results demonstrate that the LLMs exhibit remarkable tool-calling capabilities, while also refusing to engage with harmful content, and displaying a high degree of autonomy in tool utilization. The code and datasets are available at: https://github.com/zhiyuanc2001/ToolAlign",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhi-Yuan Chen",
      "Shiqi Shen",
      "Guangyao Shen",
      "Gong Zhi",
      "Xu Chen",
      "Yankai Lin"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.83": {
    "title": "DecorateLM: Data Engineering through Corpus Rating, Tagging, and Editing with Language Models",
    "volume": "main",
    "abstract": "The performance of Large Language Models (LLMs) is substantially influenced by the pretraining corpus, which consists of vast quantities of unsupervised data processed by the models. Despite its critical role in model performance, ensuring the quality of this data is challenging due to its sheer volume and the absence of sample-level quality annotations and enhancements. In this paper, we introduce DecorateLM, a data engineering method designed to refine the pretraining corpus through data rating, tagging and editing. Specifically, DecorateLM rates texts against quality criteria, tags texts with hierarchical labels, and edits texts into a more formalized format. Due to the massive size of the pretraining corpus, adopting an LLM for decorating the entire corpus is less efficient. Therefore, to balance performance with efficiency, we curate a meticulously annotated training corpus for DecorateLM using a large language model and distill data engineering expertise into a compact 1.2 billion parameter small language model (SLM). We then apply DecorateLM to enhance 100 billion tokens of the training corpus, selecting 45 billion tokens that exemplify high quality and diversity for the further training of another 1.2 billion parameter LLM. Our results demonstrate that employing such high-quality data can significantly boost model performance, showcasing a powerful approach to enhance the quality of the pretraining corpus",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ranchi Zhao",
      "Zhen Thai",
      "Yifan Zhang",
      "Shengding Hu",
      "Jie Zhou",
      "Yunqi Ba",
      "Jie Cai",
      "Zhiyuan Liu",
      "Maosong Sun"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.84": {
    "title": "Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps",
    "volume": "main",
    "abstract": "When asked to summarize articles or answer questions given a passage, large language models (LLMs) can hallucinate details and respond with unsubstantiated answers that are inaccurate with respect to the input context. This paper describes a simple approach for detecting such **contextual hallucinations**. We hypothesize that contextual hallucinations are related to the extent to which an LLM attends to information in the provided context versus its own generations. Based on this intuition, we propose a simple hallucination detection model whose input features are given by the ratio of attention weights on the context versus newly generated tokens (for each attention head). We find that a linear classifier based on these _lookback ratio_ features is as effective as a richer detector that utilizes the entire hidden states of an LLM or a text-based entailment model. The lookback ratio-based detector—**Lookback Lens**—is found to transfer across tasks and even models, allowing a detector that is trained on a 7B model to be applied (without retraining) to a larger 13B model. We further apply this detector to mitigate contextual hallucinations, and find that a simple classifier-guided decoding approach is able to reduce the amount of hallucination, for example by 9.6% in the XSum summarization task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yung-Sung Chuang",
      "Linlu Qiu",
      "Cheng-Yu Hsieh",
      "Ranjay Krishna",
      "Yoon Kim",
      "James Glass"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.85": {
    "title": "Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment",
    "volume": "main",
    "abstract": "Alignment in artificial intelligence pursues the consistency between model responses and human preferences as well as values. In practice, the multifaceted nature of human preferences inadvertently introduces what is known as the \"alignment tax\"–a compromise where enhancements in alignment within one objective (e.g., harmlessness) can diminish performance in others (e.g., helpfulness). However, existing alignment techniques are mostly unidirectional, leading to suboptimal trade-offs and poor flexibility over various objectives. To navigate this challenge, we argue the prominence of grounding LLMs with evident preferences. We introduce controllable preference optimization (CPO), which explicitly specifies preference scores for different objectives, thereby guiding the model to generate responses that meet the requirements. Our experimental analysis reveals that the aligned models can provide responses that match various preferences among the \"3H\" (helpfulness, honesty, harmlessness) desiderata. Furthermore, by introducing diverse data and alignment goals, we surpass baseline methods in aligning with single objectives, hence mitigating the impact of the alignment tax and achieving improvements in multi-objective alignment",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiju Guo",
      "Ganqu Cui",
      "Lifan Yuan",
      "Ning Ding",
      "Zexu Sun",
      "Bowen Sun",
      "Huimin Chen",
      "Ruobing Xie",
      "Jie Zhou",
      "Yankai Lin",
      "Zhiyuan Liu",
      "Maosong Sun"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.86": {
    "title": "Mitigating Matthew Effect: Multi-Hypergraph Boosted Multi-Interest Self-Supervised Learning for Conversational Recommendation",
    "volume": "main",
    "abstract": "The Matthew effect is a big challenge in Recommender Systems (RSs), where popular items tend to receive increasing attention, while less popular ones are often overlooked, perpetuating existing disparities. Although many existing methods attempt to mitigate Matthew effect in the static or quasi-static recommendation scenarios, such issue will be more pronounced as users engage with the system over time. To this end, we propose a novel framework, Multi-Hypergraph Boosted Multi-Interest Self-Supervised Learning for Conversational Recommendation (HiCore), aiming to address Matthew effect in the Conversational Recommender System (CRS) involving the dynamic user-system feedback loop. It devotes to learn multi-level user interests by building a set of hypergraphs (i.e., item-, entity-, word-oriented multiple-channel hypergraphs) to alleviate the Matthew effec. Extensive experiments on four CRS-based datasets showcase that HiCore attains a new state-of-the-art performance, underscoring its superiority in mitigating the Matthew effect effectively. Our code is available at https://github.com/zysensmile/HiCore",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongsen Zheng",
      "Ruilin Xu",
      "Guohua Wang",
      "Liang Lin",
      "Kwok-Yan Lam"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.87": {
    "title": "Advancing Event Causality Identification via Heuristic Semantic Dependency Inquiry Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoran Li",
      "Qiang Gao",
      "Hongmei Wu",
      "Li Huang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.88": {
    "title": "Exploring Union and Intersection of Visual Regions for Generating Questions, Answers, and Distractors",
    "volume": "main",
    "abstract": "Multiple-choice visual question answering (VQA) is to automatically choose a correct answer from a set of choices after reading an image. Existing efforts have been devoted to a separate generation of an image-related question, a correct answer, or challenge distractors. By contrast, we turn to a holistic generation and optimization of questions, answers, and distractors (QADs) in this study. This integrated generation strategy eliminates the need for human curation and guarantees information consistency. Furthermore, we first propose to put the spotlight on different image regions to diversify QADs. Accordingly, a novel framework ReBo is formulated in this paper. ReBo cyclically generates each QAD based on a recurrent multimodal encoder, and each generation is focusing on a different area of the image compared to those already concerned by the previously generated QADs. In addition to traditional VQA comparisons with state-of-the-art approaches, we also validate the capability of ReBo in generating augmented data to benefit VQA models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjian Ding",
      "Yao Zhang",
      "Jun Wang",
      "Adam Jatowt",
      "Zhenglu Yang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.89": {
    "title": "UniFashion: A Unified Vision-Language Model for Multimodal Fashion Retrieval and Generation",
    "volume": "main",
    "abstract": "The fashion domain encompasses a variety of real-world multimodal tasks, including multimodal retrieval and multimodal generation. The rapid advancements in artificial intelligence generated content, particularly in technologies like large language models for text generation and diffusion models for visual generation, have sparked widespread research interest in applying these multimodal models in the fashion domain. However, tasks that use embeddings, such as image-to-text or text-to-image retrieval, have been largely ignored from this perspective due to the diverse nature of the multimodal fashion domain. And current research on multi-task single models lack focus on image generation. In this work, we present UniFashion, a unified framework that simultaneously tackles the challenges of multimodal generation and retrieval tasks within the fashion domain, integrating image generation with retrieval tasks and text generation tasks. UniFashion unifies embedding and generative tasks by integrating a diffusion model and LLM, enabling controllable and high-fidelity generation. Our model significantly outperforms previous single-task state-of-the-art models across diverse fashion tasks, and can be readily adapted to manage complex vision-language tasks. This work demonstrates the potential learning synergy between multimodal generation and retrieval, offering a promising direction for future research in the fashion domain",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangyu Zhao",
      "Yuehan Zhang",
      "Wenlong Zhang",
      "Xiao-Ming Wu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.90": {
    "title": "Tracking the perspectives of interacting language models",
    "volume": "main",
    "abstract": "Large language models (LLMs) are capable of producing high quality information at unprecedented rates. As these models continue to entrench themselves in society, the content they produce will become increasingly pervasive in databases that are, in turn, incorporated into the pre-training data, fine-tuning data, retrieval data, etc. of other language models. In this paper we formalize the idea of a communication network of LLMs and introduce a method for representing the perspective of individual models within a collection of LLMs. Given these tools we systematically study information diffusion in the communication network of LLMs in various simulated settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hayden Helm",
      "Brandon Duderstadt",
      "Youngser Park",
      "Carey Priebe"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.91": {
    "title": "MAR: Matching-Augmented Reasoning for Enhancing Visual-based Entity Question Answering",
    "volume": "main",
    "abstract": "A multimodal large language model MLLMs may struggle with answering visual-based (personal) entity questions (VEQA), such as \"who is A?\" or \"who is A that B is talking to?\" for various reasons, e.g., the absence of the name of A in the caption or the inability of MLLMs to recognize A, particularly for less common entities. Furthermore, even if the MLLMs can identify A, it may refrain from answering due to privacy concerns. In this paper, we introduce a novel method called Matching-Augmented Reasoning (MAR) to enhance VEQA. Given a collection of visual objects with captions, MAR preprocesses each object individually, identifying faces, names, and their alignments within the object. It encodes this information and stores their vector representations in vector databases. When handling VEQA, MAR retrieves matching faces and names and organizes these entities into a matching graph. MAR then derives the answer to the query by reasoning over this matching graph. Extensive experiments show that MAR significantly improves VEQA compared with the state-of-the-art methods using MLLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengxuan Zhang",
      "Yin Wu",
      "Yuyu Luo",
      "Nan Tang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.92": {
    "title": "Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?",
    "volume": "main",
    "abstract": "Large language models (LLMs) have demonstrated impressive capabilities, but still suffer from inconsistency issues (e.g. LLMs can react differently to disturbances like rephrasing or inconsequential order change). In addition to these inconsistencies, we also observe that LLMs, while capable of solving hard problems, can paradoxically fail at easier ones. To evaluate this hard-to-easy inconsistency, we develop the ConsisEval benchmark, where each entry comprises a pair of questions with a strict order of difficulty. Furthermore, we introduce the concept of consistency score to quantitatively measure this inconsistency and analyze the potential for improvement in consistency by relative consistency score. Based on comprehensive experiments across a variety of existing models, we find: (1) GPT-4 achieves the highest consistency score of 92.2% but is still inconsistent to specific questions due to distraction by redundant information, misinterpretation of questions, etc.; (2) models with stronger capabilities typically exhibit higher consistency, but exceptions also exist; (3) hard data enhances consistency for both fine-tuning and in-context learning. Our data and code will be publicly available on GitHub",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhe Yang",
      "Yichang Zhang",
      "Tianyu Liu",
      "Jian Yang",
      "Junyang Lin",
      "Chang Zhou",
      "Zhifang Sui"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.93": {
    "title": "Watch Every Step! LLM Agent Learning via Iterative Step-level Process Refinement",
    "volume": "main",
    "abstract": "Large language model agents have exhibited exceptional performance across a range of complex interactive tasks. Recent approaches have utilized tuning with expert trajectories to enhance agent performance, yet they primarily concentrate on outcome rewards, which may lead to errors or suboptimal actions due to the absence of process supervision signals. In this paper, we introduce the **I**terative step-level **P**rocess **R**efinement **(IPR)** framework, which provides detailed step-by-step guidance to enhance agent training. Specifically, we adopt the Monte Carlo method to estimate step-level rewards. During each iteration, the agent explores along the expert trajectory and generates new actions. These actions are then evaluated against the corresponding step of expert trajectory using step-level rewards. Such comparison helps identify discrepancies, yielding contrastive action pairs that serve as training data for the agent. Our experiments on three complex agent tasks demonstrate that our framework outperforms a variety of strong baselines. Moreover, our analytical finds highlight the effectiveness of IPR in augmenting action efficiency and its applicability to diverse models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weimin Xiong",
      "Yifan Song",
      "Xiutian Zhao",
      "Wenhao Wu",
      "Xun Wang",
      "Ke Wang",
      "Cheng Li",
      "Wei Peng",
      "Sujian Li"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.94": {
    "title": "Standardize: Aligning Language Models with Expert-Defined Standards for Content Generation",
    "volume": "main",
    "abstract": "Domain experts across engineering, healthcare, and education follow strict standards for producing quality content such as technical manuals, medication instructions, and children's reading materials. However, current works in controllable text generation have yet to explore using these standards as references for control. Towards this end, we introduce Standardize, a retrieval-style in-context learning-based framework to guide large language models to align with expert-defined standards. Focusing on English language standards in the education domain as a use case, we consider the Common European Framework of Reference for Languages (CEFR) and Common Core Standards (CCS) for the task of open-ended content generation. Our findings show that models can gain 45% to 100% increase in precise accuracy across open and commercial LLMs evaluated, demonstrating that the use of knowledge artifacts extracted from standards and integrating them in the generation process can effectively guide models to produce better standard-aligned content",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joseph Marvin Imperial",
      "Gail Forey",
      "Harish Tayyar Madabushi"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.95": {
    "title": "Cross-domain NER with Generated Task-Oriented Knowledge: An Empirical Study from Information Density Perspective",
    "volume": "main",
    "abstract": "Cross-domain Named Entity Recognition (CDNER) is crucial for Knowledge Graph (KG) construction and natural language processing (NLP), enabling learning from source to target domains with limited data. Previous studies often rely on manually collected entity-relevant sentences from the web or attempt to bridge the gap between tokens and entity labels across domains. These approaches are time-consuming and inefficient, as these data are often weakly correlated with the target task and require extensive pre-training.To address these issues, we propose automatically generating task-oriented knowledge (GTOK) using large language models (LLMs), focusing on the reasoning process of entity extraction. Then, we employ task-oriented pre-training (TOPT) to facilitate domain adaptation. Additionally, current cross-domain NER methods often lack explicit explanations for their effectiveness. Therefore, we introduce the concept of information density to better evaluate the model's effectiveness before performing entity recognition.We conduct systematic experiments and analyses to demonstrate the effectiveness of our proposed approach and the validity of using information density for model evaluation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihao Zhang",
      "Sophia Lee",
      "Junshuang Wu",
      "Dong Zhang",
      "Shoushan Li",
      "Erik Cambria",
      "Guodong Zhou"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.96": {
    "title": "Glue pizza and eat rocks - Exploiting Vulnerabilities in Retrieval-Augmented Generative Models",
    "volume": "main",
    "abstract": "Retrieval-Augmented Generative (RAG) models enhance Large Language Models (LLMs) by integrating external knowledge bases, improving their performance in applications like fact-checking and information searching. In this paper, we demonstrate a security threat where adversaries can exploit the openness of these knowledge bases by injecting deceptive content into the retrieval database, intentionally changing the model's behavior. This threat is critical as it mirrors real-world usage scenarios where RAG systems interact with publicly accessible knowledge bases, such as web scrapings and user-contributed data pools. To be more realistic, we target a realistic setting where the adversary has no knowledge of users' queries, knowledge base data, and the LLM parameters. We demonstrate that it is possible to exploit the model successfully through crafted content uploads with access to the retriever. Our findings emphasize an urgent need for security measures in the design and deployment of RAG systems to prevent potential manipulation and ensure the integrity of machine-generated content",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Tan",
      "Chengshuai Zhao",
      "Raha Moraffah",
      "Yifan Li",
      "Song Wang",
      "Jundong Li",
      "Tianlong Chen",
      "Huan Liu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.97": {
    "title": "Predicate Debiasing in Vision-Language Models Integration for Scene Graph Generation Enhancement",
    "volume": "main",
    "abstract": "Scene Graph Generation (SGG) provides basic language representation of visual scenes, requiring models to grasp complex and diverse semantics between objects. This complexity and diversity in SGG leads to underrepresentation, where parts of triplet labels are rare or even unseen during training, resulting in imprecise predictions. To tackle this, we propose integrating the pretrained Vision-language Models to enhance representation. However, due to the gap between pretraining and SGG, direct inference of pretrained VLMs on SGG leads to severe bias, which stems from the imbalanced predicates distribution in the pretraining language set. To alleviate the bias, we introduce a novel LM Estimation to approximate the unattainable predicates distribution. Finally, we ensemble the debiased VLMs with SGG models to enhance the representation, where we design a certainty-aware indicator to score each sample and dynamically adjust the ensemble weights. Our training-free method effectively addresses the predicates bias in pretrained VLMs, enhances SGG's representation, and significantly improve the performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Wang",
      "Xiaoyuan Liu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.98": {
    "title": "SHIELD: Evaluation and Defense Strategies for Copyright Compliance in LLM Text Generation",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have transformed machine learning but raised significant legal concerns due to their potential to produce text that infringes on copyrights, resulting in several high-profile lawsuits. The legal landscape is struggling to keep pace with these rapid advancements, with ongoing debates about whether generated text might plagiarize copyrighted materials. Current LLMs may infringe on copyrights or overly restrict non-copyrighted texts, leading to these challenges: (i) the need for a comprehensive evaluation benchmark to assess copyright compliance from multiple aspects; (ii) evaluating robustness against safeguard bypassing attacks; and (iii) developing effective defenses targeted against the generation of copyrighted text.To tackle these challenges, we introduce a curated dataset to evaluate methods, test attack strategies, and propose a lightweight, real-time defense mechanism to prevent the generation of copyrighted text, ensuring the safe and lawful use of LLMs. Our experiments demonstrate that current LLMs frequently output copyrighted text, and that jailbreaking attacks can significantly increase the volume of copyrighted output. Our proposed defense mechanism substantially reduces the volume of copyrighted text generated by LLMs by effectively refusing malicious requests",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoze Liu",
      "Ting Sun",
      "Tianyang Xu",
      "Feijie Wu",
      "Cunxiang Wang",
      "Xiaoqian Wang",
      "Jing Gao"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.99": {
    "title": "MatchTime: Towards Automatic Soccer Game Commentary Generation",
    "volume": "main",
    "abstract": "Soccer is a globally popular sport with a vast audience, in this paper, we consider constructing an automatic soccer game commentary model to improve the audiences' viewing experience. In general, we make the following contributions: *First*, observing the prevalent video-text misalignment in existing datasets, we manually annotate timestamps for 49 matches, establishing a more robust benchmark for soccer game commentary generation, termed as *SN-Caption-test-align*; *Second*, we propose a multi-modal temporal alignment pipeline to automatically correct and filter the existing dataset at scale, creating a higher-quality soccer game commentary dataset for training, denoted as *MatchTime*; *Third*, based on our curated dataset, we train an automatic commentary generation model, named **MatchVoice**. Extensive experiments and ablation studies have demonstrated the effectiveness of our alignment pipeline, and training model on the curated datasets achieves state-of-the-art performance for commentary generation, showcasing that better alignment can lead to significant performance improvements in downstream tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayuan Rao",
      "Haoning Wu",
      "Chang Liu",
      "Yanfeng Wang",
      "Weidi Xie"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.100": {
    "title": "Rethinking Token Reduction for State Space Models",
    "volume": "main",
    "abstract": "Recent advancements in State Space Models (SSMs) have attracted significant interest, particularly in models optimized for parallel training and handling long-range dependencies. Architectures like Mamba have scaled to billions of parameters with selective SSM. To facilitate broader applications using Mamba, exploring its efficiency is crucial. While token reduction techniques offer a straightforward post-training strategy, we find that applying existing methods directly to SSMs leads to substantial performance drops. Through insightful analysis, we identify the reasons for this failure and the limitations of current techniques. In response, we propose a tailored, unified post-training token reduction method for SSMs. Our approach integrates token importance and similarity, thus taking advantage of both pruning and merging, to devise a fine-grained intra-layer token reduction strategy. Extensive experiments show that our method improves the average accuracy by 5.7% to 13.1% on six benchmarks with Mamba-2 compared to existing methods, while significantly reducing computational demands and memory requirements",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheng Zhan",
      "Yushu Wu",
      "Zhenglun Kong",
      "Changdi Yang",
      "Yifan Gong",
      "Xuan Shen",
      "Xue Lin",
      "Pu Zhao",
      "Yanzhi Wang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.101": {
    "title": "Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering",
    "volume": "main",
    "abstract": "Recent progress with LLM-based agents has shown promising results across various tasks. However, their use in answering questions from knowledge bases remains largely unexplored. Implementing a KBQA system using traditional methods is challenging due to the shortage of task-specific training data and the complexity of creating task-focused model structures. In this paper, we present Triad, a unified framework that utilizes an LLM-based agent with multiple roles for KBQA tasks. The agent is assigned three roles to tackle different KBQA subtasks: agent as a generalist for mastering various subtasks, as a decision maker for the selection of candidates, and as an advisor for answering questions with knowledge. Our KBQA framework is executed in four phases, involving the collaboration of the agent's multiple roles. We evaluated the performance of our framework using three benchmark datasets, and the results show that our framework outperforms state-of-the-art systems on the LC-QuAD and YAGO-QA benchmarks, yielding F1 scores of 11.8% and 20.7%, respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chang Zong",
      "Yuchen Yan",
      "Weiming Lu",
      "Jian Shao",
      "Yongfeng Huang",
      "Heng Chang",
      "Yueting Zhuang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.102": {
    "title": "MetaGPT: Merging Large Language Models Using Model Exclusive Task Arithmetic",
    "volume": "main",
    "abstract": "The advent of large language models (LLMs) like GPT-4 has catalyzed the exploration of multi-task learning (MTL), in which a single model demonstrates proficiency across diverse tasks. Task arithmetic has emerged as a cost-effective approach for MTL. It enables performance enhancement across multiple tasks by adding their corresponding task vectors to a pre-trained model. However, the current lack of a method that can simultaneously achieve optimal performance, computational efficiency, and data privacy limits their application to LLMs. In this paper, we propose Model Exclusive Task Arithmetic for merging GPT-scale models (MetaGPT) which formalizes the objective of model merging into a multi-task learning framework, aiming to minimize the average loss difference between the merged model and each individual task model. Since data privacy limits the use of multi-task training data, we leverage LLMs' local linearity and task vectors' orthogonality to separate the data term and scaling coefficients term and derive a model-exclusive task arithmetic method. Our proposed MetaGPT is data-agnostic and bypasses the heavy search process, making it cost-effective and easy to implement for LLMs. Extensive experiments demonstrate that MetaGPT leads to improvement of task arithmetic and achieves state-of-the-art performance on multiple tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuyan Zhou",
      "Liang Song",
      "Bingning Wang",
      "Weipeng Chen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.103": {
    "title": "Event Causality Identification with Synthetic Control",
    "volume": "main",
    "abstract": "Event causality identification (ECI), a process that extracts causal relations between events from text, is crucial for distinguishing causation from correlation. Traditional approaches to ECI have primarily utilized linguistic patterns and multi-hop relational inference, risking false causality identification due to informal usage of causality and specious graphical inference. In this paper, we adopt the Rubin Causal Model to identify event causality: given two temporally ordered events, we see the first event as the treatment and the second one as the observed outcome. Determining their causality involves manipulating the treatment and estimating the resultant change in the likelihood of the outcome. Given that it is only possible to implement manipulation conceptually in the text domain, as a work-around, we try to find a twin for the protagonist from existing corpora. This twin should have identical life experiences with the protagonist before the treatment but undergoes an intervention of treatment. However, the practical difficulty of locating such a match limits its feasibility. Addressing this issue, we use the synthetic control method to generate such a twin' from relevant historical data, leveraging text embedding synthesis and inversion techniques. This approach allows us to identify causal relations more robustly than previous methods, including GPT-4, which is demonstrated on a causality benchmark, COPES-hard",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Wang",
      "Fengze Liu",
      "Jiayao Zhang",
      "Dan Roth",
      "Kyle Richardson"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.104": {
    "title": "Retrieved Sequence Augmentation for Protein Representation Learning",
    "volume": "main",
    "abstract": "Protein Language Models traditionally depend on Multiple Sequence Alignments (MSA) to incorporate evolutionary knowledge. However, MSA-based approaches suffer from substantial computational overhead and generally underperform in generalizing to de novo proteins. This study reevaluates the role of MSA, proposing it as a retrieval augmentation method and questioning the necessity of sequence alignment. We show that a simple alternative, Retrieved Sequence Augmentation (RSA), can enhance protein representation learning without the need for alignment and cumbersome preprocessing. RSA surpasses MSA Transformer by an average of 5% in both structural and property prediction tasks while being 373 times faster. Additionally, RSA demonstrates enhanced transferability for predicting de novo proteins. This methodology addresses a critical need for efficiency in protein prediction and can be rapidly employed to identify homologous sequences, improve representation learning, and enhance the capacity of Large Language Models to interpret protein structures",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chang Ma",
      "Haiteng Zhao",
      "Lin Zheng",
      "Jiayi Xin",
      "Qintong Li",
      "Lijun Wu",
      "Zhihong Deng",
      "Yang Lu",
      "Qi Liu",
      "Sheng Wang",
      "Lingpeng Kong"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.105": {
    "title": "HELPD: Mitigating Hallucination of LVLMs by Hierarchical Feedback Learning with Vision-enhanced Penalty Decoding",
    "volume": "main",
    "abstract": "Large Vision-Language Models (LVLMs) have shown remarkable performance on many visual-language tasks. However, these models still suffer from multimodal hallucination, which means the generation of objects or content that violates the images. Many existing work detects hallucination by directly judging whether an object exists in an image, overlooking the association between the object and semantics. To address this issue, we propose Hierarchical Feedback Learning with Vision-enhanced Penalty Decoding (HELPD). This framework incorporates hallucination feedback at both object and sentence semantic levels. Remarkably, even with a marginal degree of training, this approach can alleviate over 15% of hallucination. Simultaneously, HELPD penalizes the output logits according to the image attention window to avoid being overly affected by generated text. HELPD can be seamlessly integrated with any LVLMs. Our experiments demonstrate that the proposed framework yields favorable results across multiple hallucination benchmarks. It effectively mitigates hallucination for different LVLMs and concurrently improves their text generation quality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Yuan",
      "Chi Qin",
      "Xiaogang Xu",
      "Piji Li"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.106": {
    "title": "TopViewRS: Vision-Language Models as Top-View Spatial Reasoners",
    "volume": "main",
    "abstract": "Top-view perspective denotes a typical way in which humans read and reason over different types of maps, and it is vital for localization and navigation of humans as well as of ‘non-human' agents, such as the ones backed by large Vision-Language Models (VLMs). Nonetheless, spatial reasoning capabilities of modern VLMs in this setup remain unattested and underexplored. In this work, we study their capability to understand and reason over spatial relations from the top view. The focus on top view also enables controlled evaluations at different granularity of spatial reasoning; we clearly disentangle different abilities (e.g., recognizing particular objects versus understanding their relative positions). We introduce the TopViewRS (Top-View Reasoning in Space) dataset, consisting of 11,384 multiple-choice questions with either realistic or semantic top-view map as visual input. We then use it to study and evaluate VLMs across 4 perception and reasoning tasks with different levels of complexity. Evaluation of 10 representative open- and closed-source VLMs reveals the gap of more than 50% compared to average human performance, and it is even lower than the random baseline in some cases. Although additional experiments show that Chain-of-Thought reasoning can boost model capabilities by 5.82% on average, the overall performance of VLMs remains limited. Our findings underscore the critical need for enhanced model capability in top-view spatial reasoning and set a foundation for further research towards human-level proficiency of VLMs in real-world multimodal tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengzu Li",
      "Caiqi Zhang",
      "Han Zhou",
      "Nigel Collier",
      "Anna Korhonen",
      "Ivan Vulić"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.107": {
    "title": "DA3: A Distribution-Aware Adversarial Attack against Language Models",
    "volume": "main",
    "abstract": "Language models can be manipulated by adversarial attacks, which introduce subtle perturbations to input data. While recent attack methods can achieve a relatively high attack success rate (ASR), we've observed that the generated adversarial examples have a different data distribution compared with the original examples. Specifically, these adversarial examples exhibit reduced confidence levels and greater divergence from the training data distribution. Consequently, they are easy to detect using straightforward detection methods, diminishing the efficacy of such attacks. To address this issue, we propose a Distribution-Aware Adversarial Attack (DA3) method. DA3 considers the distribution shifts of adversarial examples to improve attacks' effectiveness under detection methods. We further design a novel evaluation metric, the Non-detectable Attack Success Rate (NASR), which integrates both ASR and detectability for the attack task. We conduct experiments on four widely used datasets to validate the attack effectiveness and transferability of adversarial examples generated by DA3 against both the white-box BERT-base and RoBERTa-base models and the black-box LLaMA2-7b model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yibo Wang",
      "Xiangjue Dong",
      "James Caverlee",
      "Philip Yu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.108": {
    "title": "Evaluating Psychological Safety of Large Language Models",
    "volume": "main",
    "abstract": "In this work, we designed unbiased prompts to systematically evaluate the psychological safety of large language models (LLMs). First, we tested five different LLMs by using two personality tests: Short Dark Triad (SD-3) and Big Five Inventory (BFI). All models scored higher than the human average on SD-3, suggesting a relatively darker personality pattern. Despite being instruction fine-tuned with safety metrics to reduce toxicity, InstructGPT, GPT-3.5, and GPT-4 still showed dark personality patterns; these models scored higher than self-supervised GPT-3 on the Machiavellianism and narcissism traits on SD-3. Then, we evaluated the LLMs in the GPT series by using well-being tests to study the impact of fine-tuning with more training data. We observed a continuous increase in the well-being scores of GPT models. Following these observations, we showed that fine-tuning Llama-2-chat-7B with responses from BFI using direct preference optimization could effectively reduce the psychological toxicity of the model. Based on the findings, we recommended the application of systematic and comprehensive psychological metrics to further evaluate and improve the safety of LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingxuan Li",
      "Yutong Li",
      "Lin Qiu",
      "Shafiq Joty",
      "Lidong Bing"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.109": {
    "title": "An Effective Deployment of Diffusion LM for Data Augmentation in Low-Resource Sentiment Classification",
    "volume": "main",
    "abstract": "Sentiment classification (SC) often suffers from low-resource challenges such as domain-specific contexts, imbalanced label distributions, and few-shot scenarios. The potential of the diffusion language model (LM) for textual data augmentation (DA) remains unexplored, moreover, textual DA methods struggle to balance the diversity and consistency of new samples. Most DA methods either perform logical modifications or rephrase less important tokens in the original sequence with the language model. In the context of SC, strong emotional tokens could act critically on the sentiment of the whole sequence. Therefore, contrary to rephrasing less important context, we propose DiffusionCLS to leverage a diffusion LM to capture in-domain knowledge and generate pseudo samples by reconstructing strong label-related tokens. This approach ensures a balance between consistency and diversity, avoiding the introduction of noise and augmenting crucial features of datasets. DiffusionCLS also comprises a Noise-Resistant Training objective to help the model generalize. Experiments demonstrate the effectiveness of our method in various low-resource scenarios including domain-specific and domain-general problems. Ablation studies confirm the effectiveness of our framework's modules, and visualization studies highlight optimal deployment conditions, reinforcing our conclusions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuowei Chen",
      "Lianxi Wang",
      "Yuben Wu",
      "Xinfeng Liao",
      "Yujia Tian",
      "Junyang Zhong"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.110": {
    "title": "Self-Bootstrapped Visual-Language Model for Knowledge Selection and Question Answering",
    "volume": "main",
    "abstract": "While large pre-trained visual-language models have shown promising results on traditional visual question answering benchmarks, it is still challenging for them to answer complex VQA problems which requires diverse world knowledge. Motivated by the research of retrieval-augmented generation in the field of natural language processing, we use Dense Passage Retrieval (DPR) to retrieve related knowledge to help the model answer questions. However, DPR conduct retrieving in natural language space, which may not ensure comprehensive acquisition of image information. Thus, the retrieved knowledge is not truly conducive to helping answer the question, affecting the performance of the overall system. To address this issue, we propose a novel framework that leverages the visual-language model to select the key knowledge retrieved by DPR and answer questions. The framework consists of two modules: Selector and Answerer, where both are initialized by the MLLM and parameter-efficiently finetuned by self-bootstrapping: find key knowledge in the retrieved knowledge documents using the Selector, and then use them to finetune the Answerer to predict answers; obtain the pseudo-labels of key knowledge documents based on the predictions of the Answerer and weak supervision labels, and then finetune the Selector to select key knowledge; repeat. Our framework significantly enhances the performance of the baseline on the challenging open-domain Knowledge-based VQA benchmark, OK-VQA, achieving a state-of-the-art accuracy of 62.83%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongze Hao",
      "Qunbo Wang",
      "Longteng Guo",
      "Jie Jiang",
      "Jing Liu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.111": {
    "title": "PsFuture: A Pseudo-Future-based Zero-Shot Adaptive Policy for Simultaneous Machine Translation",
    "volume": "main",
    "abstract": "Simultaneous Machine Translation (SiMT) requires target tokens to be generated in real-time as streaming source tokens are consumed. Traditional approaches to SiMT typically require sophisticated architectures and extensive parameter configurations for training adaptive read/write policies, which in turn demand considerable computational power and memory. We propose PsFuture, the first zero-shot adaptive read/write policy for SiMT, enabling the translation model to independently determine read/write actions without the necessity for additional training. Furthermore, we introduce a novel training strategy, Prefix-to-Full (P2F), specifically tailored to adjust offline translation models for SiMT applications, exploiting the advantages of the bidirectional attention mechanism inherent in offline models. Experiments across multiple benchmarks demonstrate that our zero-shot policy attains performance on par with strong baselines and the P2F method can further enhance performance, achieving an outstanding trade-off between translation quality and latency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Libo Zhao",
      "Jing Li",
      "Ziqian Zeng"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.112": {
    "title": "TinyChart: Efficient Chart Understanding with Program-of-Thoughts Learning and Visual Token Merging",
    "volume": "main",
    "abstract": "Charts are important for presenting and explaining complex data relationships. Recently, multimodal large language models (MLLMs) have shown remarkable capabilities in chart understanding. However, the sheer size of these models limits their use in resource-constrained environments. In this paper, we present TinyChart, an efficient MLLM for chart understanding with only 3B parameters. TinyChart overcomes two key challenges in efficient chart understanding: (1) reduce the burden of learning numerical computations through Program-of-Thoughts (PoT) learning, which trains the model to generate Python programs for numerical calculations, and (2) reduce lengthy vision feature sequences through Vision Token Merging, which gradually merges most similar vision tokens. Extensive experiments demonstrate that our 3B TinyChart achieves SOTA performance on various chart understanding benchmarks including ChartQA, Chart-to-Text, Chart-to-Table, OpenCQA, and ChartX. It outperforms several chart-understanding MLLMs with up to 13B parameters, and close-sourced MLLM GPT-4V on ChartQA, with higher throughput during inference due to a smaller model scale and more efficient vision encoding",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liang Zhang",
      "Anwen Hu",
      "Haiyang Xu",
      "Ming Yan",
      "Yichen Xu",
      "Qin Jin",
      "Ji Zhang",
      "Fei Huang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.113": {
    "title": "Do We Need Language-Specific Fact-Checking Models? The Case of Chinese",
    "volume": "main",
    "abstract": "This paper investigates the potential benefits of language-specific fact-checking models, focusing on the case of Chinese using CHEF dataset. To better reflect real-world fact-checking, we first develop a novel Chinese document-level evidence retriever, achieving state-of-the-art performance. We then demonstrate the limitations of translation-based methods and multilingual language models, highlighting the need for language-specific systems. To better analyze token-level biases in different systems, we construct an adversarial dataset based on the CHEF dataset, where each instance has a large word overlap with the original one but holds the opposite veracity label. Experimental results on the CHEF dataset and our adversarial dataset show that our proposed method outperforms translation-based methods and multilingual language models and is more robust toward biases, emphasizing the importance of language-specific fact-checking systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Caiqi Zhang",
      "Zhijiang Guo",
      "Andreas Vlachos"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.114": {
    "title": "Enhancing Advanced Visual Reasoning Ability of Large Language Models",
    "volume": "main",
    "abstract": "Recent advancements in Vision-Language (VL) research have sparked new benchmarks for complex visual reasoning, challenging models' advanced reasoning ability. Traditional Vision-Language models (VLMs) perform well in visual perception tasks while struggling with complex reasoning scenarios. Conversely, Large Language Models (LLMs) demonstrate robust text reasoning capabilities; however, they lack visual acuity. To bridge this gap, we propose **C**omplex **V**isual **R**easoning **L**arge **L**anguage **M**odels (**CVR-LLM**), capitalizing on VLMs' visual perception proficiency and LLMs' extensive reasoning capability. Unlike recent multimodal large language models (MLLMs) that require a projection layer, our approach transforms images into detailed, context-aware descriptions using an iterative self-refinement loop and leverages LLMs' text knowledge for accurate predictions without extra training. We also introduce a novel multi-modal in-context learning (ICL) methodology to enhance LLMs' contextual understanding and reasoning. Additionally, we introduce Chain-of-Comparison (CoC), a step-by-step comparison technique enabling contrasting various aspects of predictions. Our CVR-LLM presents the first comprehensive study across a wide array of complex visual reasoning tasks and achieves SOTA performance among all",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan Li",
      "Dongnan Liu",
      "Chaoyi Zhang",
      "Heng Wang",
      "Tengfei Xue",
      "Weidong Cai"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.115": {
    "title": "CMD: a framework for Context-aware Model self-Detoxification",
    "volume": "main",
    "abstract": "Text detoxification aims to minimize the risk of language models producing toxic content. Existing detoxification methods of directly constraining the model output or further training the model on the non-toxic corpus fail to achieve a decent balance between detoxification effectiveness and generation quality. This issue stems from the neglect of constrain imposed by the context since language models are designed to generate output that closely matches the context while detoxification methods endeavor to ensure the safety of the output even if it semantically deviates from the context. In view of this, we introduce a Context-aware Model self-Detoxification (CMD) framework that pays attention to both the context and the detoxification process, i.e., first detoxifying the context and then making the language model generate along the safe context. Specifically, CMD framework involves two phases: utilizing language models to synthesize data and applying these data for training. We also introduce a toxic contrastive loss that encourages the model generation away from the negative toxic samples. Experiments on various LLMs have verified the effectiveness of our MSD framework, which can yield the best performance compared to baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zecheng Tang",
      "Keyan Zhou",
      "Juntao Li",
      "Yuyang Ding",
      "Pinzheng Wang",
      "Yan Bowen",
      "Renjie Hua",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.116": {
    "title": "Embedding and Gradient Say Wrong: A White-Box Method for Hallucination Detection",
    "volume": "main",
    "abstract": "In recent years, large language models (LLMs) have achieved remarkable success in the field of natural language generation. Compared to previous small-scale models, they are capable of generating fluent output based on the provided prefix or prompt. However, one critical challenge — the *hallucination* problem — remains to be resolved. Generally, the community refers to the undetected hallucination scenario where the LLMs generate text unrelated to the input text or facts. In this study, we intend to model the distributional distance between the regular conditional output and the unconditional output, which is generated without a given input text. Based upon Taylor Expansion for this distance at the output probability space, our approach manages to leverage the embedding and first-order gradient information. The resulting approach is plug-and-play that can be easily adapted to any autoregressive LLM. On the hallucination benchmarks HADES and other datasets, our approach achieves state-of-the-art performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaomeng Hu",
      "Yiming Zhang",
      "Ru Peng",
      "Haozhe Zhang",
      "Chenwei Wu",
      "Gang Chen",
      "Junbo Zhao"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.117": {
    "title": "TCSinger: Zero-Shot Singing Voice Synthesis with Style Transfer and Multi-Level Style Control",
    "volume": "main",
    "abstract": "Zero-shot singing voice synthesis (SVS) with style transfer and style control aims to generate high-quality singing voices with unseen timbres and styles (including singing method, emotion, rhythm, technique, and pronunciation) from audio and text prompts. However, the multifaceted nature of singing styles poses a significant challenge for effective modeling, transfer, and control. Furthermore, current SVS models often fail to generate singing voices rich in stylistic nuances for unseen singers. To address these challenges, we introduce TCSinger, the first zero-shot SVS model for style transfer across cross-lingual speech and singing styles, along with multi-level style control. Specifically, TCSinger proposes three primary modules: 1) the clustering style encoder employs a clustering vector quantization model to stably condense style information into a compact latent space; 2) the Style and Duration Language Model (S&D-LM) concurrently predicts style information and phoneme duration, which benefits both; 3) the style adaptive decoder uses a novel mel-style adaptive normalization method to generate singing voices with enhanced details. Experimental results show that TCSinger outperforms all baseline models in synthesis quality, singer similarity, and style controllability across various tasks, including zero-shot style transfer, multi-level style control, cross-lingual style transfer, and speech-to-singing style transfer",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Zhang",
      "Ziyue Jiang",
      "Ruiqi Li",
      "Changhao Pan",
      "Jinzheng He",
      "Rongjie Huang",
      "Chuxin Wang",
      "Zhou Zhao"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.118": {
    "title": "Be Helpful but Don't Talk too Much - Enhancing Helpfulness in Conversations through Relevance in Multi-Turn Emotional Support",
    "volume": "main",
    "abstract": "For a conversation to help and support, speakers should maintain an \"effect-effort\" tradeoff. As outlined in the gist of \"Cognitive Relevance Principle\", helpful speakers should optimize the \"cognitive relevance\" through maximizing the \"cognitive effects\" and minimizing the \"processing effort\" imposed on listeners. Although preference learning methods have given rise a boon of studies in pursuit of\"effect-optimization\", none have delved into the critical \"effort-optimiazation\" to fully cultivate the awareness of \"optimal relevance\" into thecognition of conversation agents. To address this gap, we integrate the \"Cognitive Relevance Principle\" into emotional support agents in the environment of multi-turn conversation. The results demonstrate a significant and robust improvement against the baseline systems with respect to response quality, human-likedness and supportivenss. This study offers compelling evidence for the effectiveness of the \"Relevance Principle\" in generating human-like, helpful, and harmless emotional support conversations. The source code will be available at https://github.com/CN-Eyetk/VLESA-ORL.git",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junlin Li",
      "Bo Peng",
      "Yu-Yin Hsu",
      "Chu-Ren Huang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.119": {
    "title": "Aligning Language Models to Explicitly Handle Ambiguity",
    "volume": "main",
    "abstract": "In interactions between users and language model agents, user utterances frequently exhibit ellipsis (omission of words or phrases) or imprecision (lack of exactness) to prioritize efficiency. This can lead to varying interpretations of the same input based on different assumptions or background knowledge. It is thus crucial for agents to adeptly handle the inherent ambiguity in queries to ensure reliability. However, even state-of-the-art large language models (LLMs) still face challenges in such scenarios, primarily due to the following hurdles: (1) LLMs are not explicitly trained to deal with ambiguous utterances; (2) the degree of ambiguity perceived by the LLMs may vary depending on the possessed knowledge. To address these issues, we propose Alignment with Perceived Ambiguity (APA), a novel pipeline that aligns LLMs to manage ambiguous queries by leveraging their own assessment of ambiguity (i.e., perceived ambiguity). Experimental results on question-answering datasets demonstrate that APA empowers LLMs to explicitly detect and manage ambiguous queries while retaining the ability to answer clear questions. Furthermore, our finding proves that APA excels beyond training with gold-standard labels, especially in out-of-distribution scenarios. The data and code are available at https://github.com/heyjoonkim/APA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyuhng Joon Kim",
      "Youna Kim",
      "Cheonbok Park",
      "Junyeob Kim",
      "Choonghyun Park",
      "Kang Min Yoo",
      "Sang-goo Lee",
      "Taeuk Kim"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.120": {
    "title": "Tag-grounded Visual Instruction Tuning with Retrieval Augmentation",
    "volume": "main",
    "abstract": "Despite recent advances in the general visual instruction-following ability of Multimodal Large Language Models (MLLMs), they still struggle with critical problems when required to provide a precise and detailed response to a visual instruction: (1) failure to identify novel objects or entities, (2) mention of non-existent objects, and (3) neglect of object's attributed details. Intuitive solutions include improving the size and quality of data or using larger foundation models. They show effectiveness in mitigating these issues, but at an expensive cost of collecting a vast amount of new data and introducing a significantly larger model. Standing at the intersection of these approaches, we examine the three object-oriented problems from the perspective of the image-to-text mapping process by the multimodal connector. In this paper, we first identify the limitations of multimodal connectors stemming from insufficient training data. Driven by this, we propose to enhance the mapping with retrieval-augmented tag tokens, which contain rich object-aware information such as object names and attributes. With our Tag-grounded visual instruction tuning with retrieval Augmentation (TUNA), we outperform baselines that share the same language model and training data on 12 benchmarks. Furthermore, we show the zero-shot capability of TUNA when provided with specific datastores",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daiqing Qi",
      "Handong Zhao",
      "Zijun Wei",
      "Sheng Li"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.121": {
    "title": "GLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models",
    "volume": "main",
    "abstract": "Despite the rapid progress of large language models (LLMs), their task performance remains sensitive to prompt design. Recent studies have explored leveraging the LLM itself as an optimizer to identify optimal prompts that maximize task accuracy. However, when evaluating prompts, such approaches heavily rely on elusive manually annotated gold labels to calculate task accuracy for each candidate prompt, which hinders its generality. To overcome the limitation, this work proposes GLaPE, a gold label-agnostic prompt evaluation method to alleviate dependence on gold labels. GLaPE is composed of two critical aspects: self-consistency evaluation of a single prompt and mutual-consistency refinement across multiple prompts. Experimental results on 8 widely-recognized reasoning tasks demonstrate that GLaPE can produce more effective prompts, achieving performance comparable to those derived from manually annotated gold labels. Analysis shows that GLaPE provides reliable evaluations aligned with accuracy, even in the absence of gold labels. Code is publicly available at **Anonymous**",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuanchang Zhang",
      "Zhuosheng Zhang",
      "Hai Zhao"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.122": {
    "title": "Decoding the Echoes of Vision from fMRI: Memory Disentangling for Past Semantic Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runze Xia",
      "Congchi Yin",
      "Piji Li"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.123": {
    "title": "Optimizing Code Retrieval: High-Quality and Scalable Dataset Annotation through Large Language Models",
    "volume": "main",
    "abstract": "Code retrieval aims to identify code from extensive codebases that semantically aligns with a given query code snippet. Collecting a broad and high-quality set of query and code pairs is crucial to the success of this task. However, existing data collection methods struggle to effectively balance scalability and annotation quality. In this paper, we first analyze the factors influencing the quality of function annotations generated by Large Language Models (LLMs). We find that the invocation of intra-repository functions and third-party APIs plays a significant role. Building on this insight, we propose a novel annotation method that enhances the annotation context by incorporating the content of functions called within the repository and information on third-party API functionalities. Additionally, we integrate LLMs with a novel sorting method to address the multi-level function call relationships within repositories. Furthermore, by applying our proposed method across a range of repositories, we have developed the Query4Code dataset. The quality of this synthesized dataset is validated through both model training and human evaluation, demonstrating high-quality annotations. Moreover, cost analysis confirms the scalability of our annotation method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Li",
      "Qi Liu",
      "Liyang He",
      "Zheng Zhang",
      "Hao Zhang",
      "Shengyu Ye",
      "Junyu Lu",
      "Zhenya Huang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.124": {
    "title": "Towards Difficulty-Agnostic Efficient Transfer Learning for Vision-Language Models",
    "volume": "main",
    "abstract": "Vision-language models (VLMs) like CLIP have demonstrated remarkable applicability across a variety of downstream tasks, including zero-shot image classification. Recently, the use of prompts or adapters for efficient transfer learning (ETL) has gained significant attention for effectively adapting to downstream tasks. However, previous studies have overlooked the challenge of varying transfer difficulty of downstream tasks. In this paper, we empirically analyze how each ETL method behaves with respect to transfer difficulty. Our observations indicate that utilizing vision prompts and text adapters is crucial for adaptability and generalizability in domains with high difficulty. Also, by applying an adaptive ensemble approach that integrates task-adapted VLMs with pre-trained VLMs and strategically leverages more general knowledge in low-difficulty and less in high-difficulty domains, we consistently enhance performance across both types of domains. Based on these observations, we propose an adaptive ensemble method that combines visual prompts and text adapters with pre-trained VLMs, tailored by transfer difficulty, to achieve optimal performance for any target domain. Upon experimenting with extensive benchmarks, our method consistently outperforms all baselines, particularly on unseen tasks, demonstrating its effectiveness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongjin Yang",
      "Jongwoo Ko",
      "Se-Young Yun"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.125": {
    "title": "Advancing Process Verification for Large Language Models via Tree-Based Preference Learning",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable potential in handling complex reasoning tasks by generating step-by-step rationales. Some methods have proven effective in boosting accuracy by introducing extra verifiers to assess these paths. However, existing verifiers, typically trained on binary-labeled reasoning paths, fail to fully utilize the relative merits of intermediate steps, thereby limiting the effectiveness of the feedback provided. To overcome this limitation, we propose Tree-based Preference Learning Verifier (Tree-PLV), a novel approach that constructs reasoning trees via a best-first search algorithm and collects step-level paired data for preference training. Compared to traditional binary classification, step-level preferences more finely capture the nuances between reasoning steps, allowing for a more precise evaluation of the complete reasoning path. We empirically evaluate Tree-PLV across a range of arithmetic and commonsense reasoning tasks, where it significantly outperforms existing benchmarks. For instance, Tree-PLV achieved substantial performance gains over the Mistral-7B self-consistency baseline on GSM8K (67.55% → 82.79%), MATH (17.00% → 26.80%), CSQA (68.14% → 72.97%), and StrategyQA (82.86% → 83.25%). Additionally, our study explores the appropriate granularity for applying preference learning, revealing that step-level guidance provides feedback that better aligns with the evaluation of the reasoning process",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingqian He",
      "Yongliang Shen",
      "Wenqi Zhang",
      "Zeqi Tan",
      "Weiming Lu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.126": {
    "title": "An Inversion Attack Against Obfuscated Embedding Matrix in Language Model Inference",
    "volume": "main",
    "abstract": "With the rapidly-growing deployment of large language model (LLM) inference services, privacy concerns have arisen regarding to the user input data. Recent studies are exploring transforming user inputs to obfuscated embedded vectors, so that the data will not be eavesdropped by service provides. However, in this paper we show that again, without a solid and deliberate security design and analysis, such embedded vector obfuscation failed to protect users' privacy. We demonstrate the conclusion via conducting a novel inversion attack called Element-wise Differential Nearest Neighbor (EDNN) on the glide-reflection proposed in (CITATION), and the result showed that the original user input text can be 100% recovered from the obfuscated embedded vectors. We further analyze security requirements on embedding obfuscation and present several remedies to our proposed attack",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Lin",
      "Qizhi Zhang",
      "Quanwei Cai",
      "Jue Hong",
      "Wu Ye",
      "Huiqi Liu",
      "Bing Duan"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.127": {
    "title": "VideoScore: Building Automatic Metrics to Simulate Fine-grained Human Feedback for Video Generation",
    "volume": "main",
    "abstract": "The recent years have witnessed great advances in video generation. However, the development of automatic video metrics is lagging significantly behind. None of the existing metric is able to provide reliable scores over generated videos. The main barrier is the lack of large-scale human-annotated dataset. In this paper, we release VideoFeedback, the first large-scale dataset containing human-provided multi-aspect score over 37.6K synthesized videos from 11 existing video generative models. We train VideoScore (initialized from Mantis)based on VideoFeedback to enable automatic video quality assessment. Experiments show that the Spearman's correlation betweenVideoScore and humans can reach 77.1 on VideoFeedback-test, beating the prior best metrics by about 50 points. Further result onother held-out EvalCrafter, GenAI-Bench, and VBench show that VideoScore has consistently much higher correlation with humanjudges than other metrics. Due to these results, we believe VideoScore can serve as a great proxy for human raters to (1) rate different video models to track progress (2) simulate fine-grained human feedback in Reinforcement Learning with Human Feedback (RLHF) to improve current video generation models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan He",
      "Dongfu Jiang",
      "Ge Zhang",
      "Max Ku",
      "Achint Soni",
      "Sherman Siu",
      "Haonan Chen",
      "Abhranil Chandra",
      "Ziyan Jiang",
      "Aaran Arulraj",
      "Kai Wang",
      "Quy Do",
      "Yuansheng Ni",
      "Bohan Lyu",
      "Yaswanth Narsupalli",
      "Rongqi Fan",
      "Zhiheng Lyu",
      "Bill Yuchen Lin",
      "Wenhu Chen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.128": {
    "title": "LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models",
    "volume": "main",
    "abstract": "We introduce LogicAsker, a novel approach for evaluating and enhancing the logical reasoning capabilities of large language models (LLMs) such as ChatGPT and GPT-4. Despite LLMs' prowess in tasks like writing assistance, code generation, and machine translation, assessing their ability to reason has been challenging. Traditional evaluations often prioritize accuracy on downstream tasks over direct assessments of reasoning processes. LogicAsker addresses this gap by employing a set of atomic reasoning skills grounded in propositional and predicate logic to systematically examine and improve the reasoning prowess of LLMs. Our methodology reveals significant gaps in LLMs' learning of logical rules, with identified reasoning failures ranging from 29% to 90% across different models. Moreover, we leverage these findings to construct targeted demonstration examples and fine-tune data, notably enhancing logical reasoning in models like GPT-4o by up to 5%. To our knowledge, this is the first effort to utilize test case outcomes to effectively refine LLMs' formal reasoning capabilities. We make our code, data, and results publicly available(https://github.com/yxwan123/LogicAsker) to facilitate further research and replication of our findings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Wan",
      "Wenxuan Wang",
      "Yiliu Yang",
      "Youliang Yuan",
      "Jen-tse Huang",
      "Pinjia He",
      "Wenxiang Jiao",
      "Michael Lyu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.129": {
    "title": "Integrating Structural Semantic Knowledge for Enhanced Information Extraction Pre-training",
    "volume": "main",
    "abstract": "Information Extraction (IE), aiming to extract structured information from unstructured natural language texts, can significantly benefit from pre-trained language models. However, existing pre-training methods solely focus on exploiting the textual knowledge, relying extensively on annotated large-scale datasets, which is labor-intensive and thus limits the scalability and versatility of the resulting models. To address these issues, we propose SKIE, a novel pre-training framework tailored for IE that integrates structural semantic knowledge via contrastive learning, effectively alleviating the annotation burden. Specifically, SKIE utilizes Abstract Meaning Representation (AMR) as a low-cost supervision source to boost model performance without human intervention. By enhancing the topology of AMR graphs, SKIE derives high-quality cohesive subgraphs as additional training samples, providing diverse multi-level structural semantic knowledge. Furthermore, SKIE refines the graph encoder to better capture cohesive information and edge relation information, thereby improving the pre-training efficacy. Extensive experimental results demonstrate that SKIE outperforms state-of-the-art baselines across multiple IE tasks and showcases exceptional performance in few-shot and zero-shot settings",
    "checked": false,
    "id": "0496f01a428e96c1e65843215964138806226225",
    "semantic_title": "graph structure enhanced pre-training language model for knowledge graph completion",
    "citation_count": 8,
    "authors": [
      "Xiaoyang Yi",
      "Yuru Bao",
      "Jian Zhang",
      "Yifang Qin",
      "Faxin Lin"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.130": {
    "title": "FuseGen: PLM Fusion for Data-generation based Zero-shot Learning",
    "volume": "main",
    "abstract": "Data-generation based zero-shot learning, although effective in training Small Task-specific Models (STMs) via synthetic datasets generated by Pre-trained Language Models (PLMs), is often limited by the low quality of such synthetic datasets. Previous solutions have primarily focused on single PLM settings, where synthetic datasets are typically restricted to specific sub-spaces and often deviate from real-world distributions, leading to severe distribution bias. To mitigate such bias, we propose FuseGen, a novel data-generation based zero-shot learning framework that introduces a new criteria for subset selection from synthetic datasets via utilizing multiple PLMs and trained STMs. The chosen subset provides in-context feedback to each PLM, enhancing dataset quality through iterative data generation. Trained STMs are then used for sample re-weighting as well, further improving data quality. Extensive experiments across diverse tasks demonstrate that FuseGen substantially outperforms existing methods, highly effective in boosting STM performance in a PLM-agnostic way. The code is available at https://github.com/LindaLydia/FuseGen",
    "checked": true,
    "id": "88e7506a39f99412722e5e3751be3eebd35edbf5",
    "semantic_title": "fusegen: plm fusion for data-generation based zero-shot learning",
    "citation_count": 0,
    "authors": [
      "Tianyuan Zou",
      "Yang Liu",
      "Peng Li",
      "Jianqing Zhang",
      "Jingjing Liu",
      "Ya-Qin Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.131": {
    "title": "I Need Help! Evaluating LLM's Ability to Ask for Users' Support: A Case Study on Text-to-SQL Generation",
    "volume": "main",
    "abstract": "This study explores the proactive ability of LLMs to seek user support. We propose metrics to evaluate the trade-off between performance improvements and user burden, and investigate whether LLMs can determine when to request help under varying information availability. Our experiments show that without external feedback, many LLMs struggle to recognize their need for user support. The findings highlight the importance of external signals and provide insights for future research on improving support-seeking strategies. Source code: https://github.com/appier-research/i-need-help",
    "checked": true,
    "id": "4043201b9d7d232608b95d899e1a1e59bc2f0e56",
    "semantic_title": "i need help! evaluating llm's ability to ask for users' support: a case study on text-to-sql generation",
    "citation_count": 0,
    "authors": [
      "Cheng-Kuang Wu",
      "Zhi Rui Tam",
      "Chao-Chung Wu",
      "Chieh-Yen Lin",
      "Hung-yi Lee",
      "Yun-Nung Chen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.132": {
    "title": "Oddballs and Misfits: Detecting Implicit Abuse in Which Identity Groups are Depicted as Deviating from the Norm",
    "volume": "main",
    "abstract": "We address the task of detecting abusive sentences in which identity groups are depicted as deviating from the norm (e.g. Gays sprinkle flour over their gardens for good luck). These abusive utterances need not be stereotypes or negative in sentiment. We introduce the first dataset for this task. It is created via crowdsourcing and includes 7 identity groups. We also report on classification experiments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Wiegand",
      "Josef Ruppenhofer"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.133": {
    "title": "By My Eyes: Grounding Multimodal Large Language Models with Sensor Data via Visual Prompting",
    "volume": "main",
    "abstract": "Large language models (LLMs) have demonstrated exceptional abilities across various domains. However, utilizing LLMs for ubiquitous sensing applications remains challenging as existing text-prompt methods show significant performance degradation when handling long sensor data sequences. In this paper, we propose a visual prompting approach for sensor data using multimodal LLMs (MLLMs). Specifically, we design a visual prompt that directs MLLMs to utilize visualized sensor data alongside descriptions of the target sensory task. Additionally, we introduce a visualization generator that automates the creation of optimal visualizations tailored to a given sensory task, eliminating the need for prior task-specific knowledge. We evaluated our approach on nine sensory tasks involving four sensing modalities, achieving an average of 10% higher accuracy compared to text-based prompts and reducing token costs by 15.8 times. Our findings highlight the effectiveness and cost-efficiency of using visual prompts with MLLMs for various sensory tasks. The source code is available at https://github.com/diamond264/ByMyEyes",
    "checked": true,
    "id": "d1a14ef92761331303db78ff2dd0b41cef4c6d8a",
    "semantic_title": "by my eyes: grounding multimodal large language models with sensor data via visual prompting",
    "citation_count": 1,
    "authors": [
      "Hyungjun Yoon",
      "Biniyam Tolera",
      "Taesik Gong",
      "Kimin Lee",
      "Sung-Ju Lee"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.134": {
    "title": "Prefixing Attention Sinks can Mitigate Activation Outliers for Large Language Model Quantization",
    "volume": "main",
    "abstract": "Despite recent advances in LLM quantization, activation quantization remains to be challenging due to the activation outliers. Conventional remedies, e.g., mixing precisions for different channels, introduce extra overhead and reduce the speedup. In this work, we develop a simple yet effective strategy to facilitate per-tensor activation quantization by preventing the generation of problematic tokens. Precisely, we propose a method to find a set of key-value cache, coined _CushionCache_, which mitigates outliers in subsequent tokens when inserted as a prefix. CushionCache works in two steps: First, we greedily search for a prompt token sequence that minimizes the maximum activation values in subsequent tokens. Then, we further tune the token cache to regularize the activations of subsequent tokens to be more quantization-friendly. The proposed method successfully addresses activation outliers of LLMs, providing a substantial performance boost for per-tensor activation quantization methods. We thoroughly evaluate our method over a wide range of models and benchmarks and find that it significantly surpasses the established baseline of per-tensor W8A8 quantization and can be seamlessly integrated with the recent activation quantization method",
    "checked": true,
    "id": "1601ad7616681ed9d7e1b9a04b64c1ad9c7196c7",
    "semantic_title": "prefixing attention sinks can mitigate activation outliers for large language model quantization",
    "citation_count": 3,
    "authors": [
      "Seungwoo Son",
      "Wonpyo Park",
      "Woohyun Han",
      "Kyuyeun Kim",
      "Jaeho Lee"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.135": {
    "title": "CHIQ: Contextual History Enhancement for Improving Query Rewriting in Conversational Search",
    "volume": "main",
    "abstract": "In this paper, we study how open-source large language models (LLMs) can be effectively deployed for improving query rewriting in conversational search, especially for ambiguous queries. We introduce CHIQ, a two-step method that leverages the capabilities of LLMs to resolve ambiguities in the conversation history before query rewriting. This approach contrasts with prior studies that predominantly use closed-source LLMs to directly generate search queries from conversation history. We demonstrate on five well-established benchmarks that CHIQ leads to state-of-the-art results across most settings, showing highly competitive performances with systems leveraging closed-source LLMs. Our study provides a first step towards leveraging open-source LLMs in conversational search, as a competitive alternative to the prevailing reliance on commercial LLMs. Data, models, and source code will be publicly available upon acceptance at https://github.com/fengranMark/CHIQ",
    "checked": true,
    "id": "38ca8bad77cb133576710acaf62be23da7f67258",
    "semantic_title": "chiq: contextual history enhancement for improving query rewriting in conversational search",
    "citation_count": 5,
    "authors": [
      "Fengran Mo",
      "Abbas Ghaddar",
      "Kelong Mao",
      "Mehdi Rezagholizadeh",
      "Boxing Chen",
      "Qun Liu",
      "Jian-Yun Nie"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.136": {
    "title": "Towards Low-Resource Harmful Meme Detection with LMM Agents",
    "volume": "main",
    "abstract": "The proliferation of Internet memes in the age of social media necessitates effective identification of harmful ones. Due to the dynamic nature of memes, existing data-driven models may struggle in low-resource scenarios where only a few labeled examples are available. In this paper, we propose an agency-driven framework for low-resource harmful meme detection, employing both outward and inward analysis with few-shot annotated samples. Inspired by the powerful capacity of Large Multimodal Models (LMMs) on multimodal reasoning, we first retrieve relative memes with annotations to leverage label information as auxiliary signals for the LMM agent. Then, we elicit knowledge-revising behavior within the LMM agent to derive well-generalized insights into meme harmfulness. By combining these strategies, our approach enables dialectical reasoning over intricate and implicit harm-indicative patterns. Extensive experiments conducted on three meme datasets demonstrate that our proposed approach achieves superior performance than state-of-the-art methods on the low-resource harmful meme detection task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianzhao Huang",
      "Hongzhan Lin",
      "Liu Ziyan",
      "Ziyang Luo",
      "Guang Chen",
      "Jing Ma"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.137": {
    "title": "VIVA: A Benchmark for Vision-Grounded Decision-Making with Human Values",
    "volume": "main",
    "abstract": "This paper introduces VIVA, a benchmark for VIsion-grounded decision-making driven by human VA. While most large vision-language models (VLMs) focus on physical-level skills, our work is the first to examine their multimodal capabilities in leveraging human values to make decisions under a vision-depicted situation. VIVA contains 1,062 images depicting diverse real-world situations and the manually annotated decisions grounded in them. Given an image there, the model should select the most appropriate action to address the situation and provide the relevant human values and reason underlying the decision. Extensive experiments based on VIVA show the limitation of VLMs in using human values to make multimodal decisions. Further analyses indicate the potential benefits of exploiting action consequences and predicted human values",
    "checked": true,
    "id": "0f5582605876d3dab492b4db6e961353f0695063",
    "semantic_title": "viva: a benchmark for vision-grounded decision-making with human values",
    "citation_count": 0,
    "authors": [
      "Zhe Hu",
      "Yixiao Ren",
      "Jing Li",
      "Yu Yin"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.138": {
    "title": "Direct Multi-Turn Preference Optimization for Language Agents",
    "volume": "main",
    "abstract": "Adapting Large Language Models (LLMs) for agent tasks is critical in developing language agents. Direct Preference Optimization (DPO) is a promising technique for this adaptation with the alleviation of compounding errors, offering a means to directly optimize Reinforcement Learning (RL) objectives. However, applying DPO to multi-turn tasks presents challenges due to the inability to cancel the partition function. Overcoming this obstacle involves making the partition function independent of the current state and addressing length disparities between preferred and dis-preferred trajectories. In this light, we replace the policy constraint with the state-action occupancy measure constraint in the RL objective and add length normalization to the Bradley-Terry model, yielding a novel loss function named DMPO for multi-turn agent tasks with theoretical explanations. Extensive experiments on three multi-turn agent task datasets confirm the effectiveness and superiority of the DMPO loss",
    "checked": true,
    "id": "3b5778b0aec88c1921fee7538400e49bed42a5fa",
    "semantic_title": "direct multi-turn preference optimization for language agents",
    "citation_count": 0,
    "authors": [
      "Wentao Shi",
      "Mengqi Yuan",
      "Junkang Wu",
      "Qifan Wang",
      "Fuli Feng"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.139": {
    "title": "Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models",
    "volume": "main",
    "abstract": "The alignment of reasoning abilities between smaller and larger Language Models are largely conducted via supervised fine-tuning using demonstrations generated from robust Large Language Models (LLMs). Although these approaches deliver more performant models, they do not show sufficiently strong generalization ability as the training only relies on the provided demonstrations.In this paper, we propose the Self-refine Instruction-tuning method that elicits Smaller Language Models to self-improve their abilities.Our approach is based on a two-stage process, where reasoning abilities are first transferred between LLMs and Small Language Models (SLMs) via Instruction-tuning on synthetic demonstrations provided by LLMs, and then the instructed models self-improve their abilities through preference optimization strategies.In particular, the second phase operates refinement heuristics based on Direct Preference Optimization, where the SLMs are elicited to deliver a series of reasoning paths by automatically sampling the generated responses and providing rewards using ground truths from the LLMs.Results obtained on commonsense and math reasoning tasks show that this approach consistently outperforms Instruction-tuning in both in-domain and out-domain scenarios, aligning the reasoning abilities of Smaller and Larger language models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leonardo Ranaldi",
      "Andre Freitas"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.140": {
    "title": "In Search of the Long-Tail: Systematic Generation of Long-Tail Inferential Knowledge via Logical Rule Guided Search",
    "volume": "main",
    "abstract": "To effectively use large language models (LLMs) for real-world queries, it is imperative that they generalize to the long-tail distribution, i.e. rare examples where models exhibit low confidence. In this work, we take the first step towards evaluating LLMs in the long-tail distribution of inferential knowledge. We exemplify long-tail evaluation on the Natural Language Inference task. First, we introduce Logic-Induced-Knowledge-Search (LINK), a systematic long-tail data generation framework, to obtain factually-correct yet long-tail inferential statements. LINK uses variable-wise prompting grounded on symbolic rules to seek low-confidence statements while ensuring factual correctness. We then use LINK to curate Logic-Induced-Long-Tail (LINT), a large-scale long-tail inferential knowledge dataset that contains 108K statements spanning four domains. We evaluate popular LLMs on LINT; we find that state-of-the-art LLMs show significant performance drop (21% relative drop for GPT4) on long-tail data as compared to on head distribution data, and smaller models show even more generalization weakness. These results further underscore the necessity of long-tail evaluation in developing generalizable LLMs",
    "checked": true,
    "id": "28685ab4bb673cd7aac4f5711b0882d08d2fa7c7",
    "semantic_title": "in search of the long-tail: systematic generation of long-tail inferential knowledge via logical rule guided search",
    "citation_count": 0,
    "authors": [
      "Huihan Li",
      "Yuting Ning",
      "Zeyi Liao",
      "Siyuan Wang",
      "Xiang Li",
      "Ximing Lu",
      "Wenting Zhao",
      "Faeze Brahman",
      "Yejin Choi",
      "Xiang Ren"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.141": {
    "title": "AutoScraper: A Progressive Understanding Web Agent for Web Scraper Generation",
    "volume": "main",
    "abstract": "Web scraping is a powerful technique that extracts data from websites, enabling automated data collection, enhancing data analysis capabilities, and minimizing manual data entry efforts. Existing methods, wrappers-based methods suffer from limited adaptability and scalability when faced with a new website, while language agents, empowered by large language models (LLMs), exhibit poor reusability in diverse web environments. In this work, we introduce the paradigm of generating web scrapers with LLMs and propose AutoScraper, a two-stage framework that can handle diverse and changing web environments more efficiently. AutoScraper leverages the hierarchical structure of HTML and similarity across different web pages for generating web scrapers. Besides, we propose a new executability metric for better measuring the performance of web scraper generation tasks. We conduct comprehensive experiments with multiple LLMs and demonstrate the effectiveness of our framework. Our work is now open-source",
    "checked": true,
    "id": "6c076122ea53e18180255bb96c9ad547bb88d283",
    "semantic_title": "autoscraper: a progressive understanding web agent for web scraper generation",
    "citation_count": 1,
    "authors": [
      "Wenhao Huang",
      "Zhouhong Gu",
      "Chenghao Peng",
      "Jiaqing Liang",
      "Zhixu Li",
      "Yanghua Xiao",
      "Liqian Wen",
      "Zulong Chen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.142": {
    "title": "Backward Lens: Projecting Language Model Gradients into the Vocabulary Space",
    "volume": "main",
    "abstract": "Understanding how Transformer-based Language Models (LMs) learn and recall information is a key goal of the deep learning community. Recent interpretability methods project weights and hidden states obtained from the forward pass to the models' vocabularies, helping to uncover how information flows within LMs. In this work, we extend this methodology to LMs' backward pass and gradients. We first prove that a gradient matrix can be cast as a low-rank linear combination of its forward and backward passes' inputs. We then develop methods to project these gradients into vocabulary items and explore the mechanics of how new information is stored in the LMs' neurons",
    "checked": true,
    "id": "a893c8d9eac0b562161278ef5b135f4b15fc6738",
    "semantic_title": "backward lens: projecting language model gradients into the vocabulary space",
    "citation_count": 4,
    "authors": [
      "Shahar Katz",
      "Yonatan Belinkov",
      "Mor Geva",
      "Lior Wolf"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.143": {
    "title": "Selective Vision is the Challenge for Visual Reasoning: A Benchmark for Visual Argument Understanding",
    "volume": "main",
    "abstract": "Visual arguments, often used in advertising or social causes, rely on images to persuade viewers to do or believe something. Understanding these arguments requires selective vision: only specific visual stimuli within an image are relevant to the argument, and relevance can only be understood within the context of a broader argumentative structure. While visual arguments are readily appreciated by human audiences, we ask: are today's AI capable of similar understanding?We present VisArgs, a dataset of 1,611 images annotated with 5,112 visual premises (with regions), 5,574 commonsense premises, and reasoning trees connecting them into structured arguments. We propose three tasks for evaluating visual argument understanding: premise localization, premise identification, and conclusion deduction.Experiments show that 1) machines struggle to capture visual cues: GPT-4-O achieved 78.5% accuracy, while humans reached 98.0%. Models also performed 19.5% worse when distinguishing between irrelevant objects within the image compared to external objects. 2) Providing relevant visual premises improved model performance significantly",
    "checked": true,
    "id": "318069651dada8a413fcf9441589fafefecbcad3",
    "semantic_title": "selective vision is the challenge for visual reasoning: a benchmark for visual argument understanding",
    "citation_count": 0,
    "authors": [
      "Jiwan Chung",
      "Sungjae Lee",
      "Minseo Kim",
      "Seungju Han",
      "Ashkan Yousefpour",
      "Jack Hessel",
      "Youngjae Yu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.144": {
    "title": "Can visual language models resolve textual ambiguity with visual cues? Let visual puns tell you!",
    "volume": "main",
    "abstract": "Humans possess multimodal literacy, allowing them to actively integrate information from various modalities to form reasoning. Faced with challenges like lexical ambiguity in text, we supplement this with other modalities, such as thumbnail images or textbook illustrations. Is it possible for machines to achieve a similar multimodal understanding capability?In response, we present Understanding Pun with Image Explanations (UNPIE), a novel benchmark designed to assess the impact of multimodal inputs in resolving lexical ambiguities. Puns serve as the ideal subject for this evaluation due to their intrinsic ambiguity. Our dataset includes 1,000 puns, each accompanied by an image that explains both meanings. We pose three multimodal challenges with the annotations to assess different aspects of multimodal literacy; Pun Grounding, Disambiguation, and Reconstruction. The results indicate that various Socratic Models and Visual-Language Models improve over the text-only models when given visual context, particularly as the complexity of the tasks increases",
    "checked": true,
    "id": "dcc58cfd3bacaa35f38a6476af61a66b3ef9f82e",
    "semantic_title": "can visual language models resolve textual ambiguity with visual cues? let visual puns tell you!",
    "citation_count": 0,
    "authors": [
      "Jiwan Chung",
      "Seungwon Lim",
      "Jaehyun Jeon",
      "Seungbeen Lee",
      "Youngjae Yu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.145": {
    "title": "Reusing Transferable Weight Increments for Low-resource Style Generation",
    "volume": "main",
    "abstract": "Text style transfer (TST) is crucial in natural language processing, aiming to endow text with a new style without altering its meaning. In real-world scenarios, not all styles have abundant resources. This work introduces TWIST (reusing Transferable Weight Increments for Style Text generation), a novel framework to mitigate data scarcity by utilizing style features in weight increments to transfer low-resource styles effectively. During target style learning, we derive knowledge via a specially designed weight pool and initialize the parameters for the unseen style. To enhance the effectiveness of merging, the target style weight increments are often merged from multiple source style weight increments through singular vectors. Considering the diversity of styles, we also designed a multi-key memory network that simultaneously focuses on task- and instance-level information to derive the most relevant weight increments. Results from multiple style transfer datasets show that TWIST demonstrates remarkable performance across different backbones, achieving particularly effective results in low-resource scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chunzhen Jin",
      "Eliot Huang",
      "Heng Chang",
      "Yaqi Wang",
      "Peng Cao",
      "Osmar Zaiane"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.146": {
    "title": "Large Language Model as an Assignment Evaluator: Insights, Feedback, and Challenges in a 1000+ Student Course",
    "volume": "main",
    "abstract": "Using large language models (LLMs) for automatic evaluation has become an important evaluation method in NLP research. However, it is unclear whether these LLM-based evaluators can be effectively applied in real-world classrooms to assess student assignments. This empirical report shares how we use GPT-4 as an automatic assignment evaluator in a university course with over 1000 students. Based on student responses, we found that LLM-based assignment evaluators are generally acceptable to students when they have free access to these tools. However, students also noted that the LLM sometimes fails to adhere to the evaluation instructions, resulting in unreasonable assessments. Additionally, we observed that students can easily manipulate the LLM to output specific strings, allowing them to achieve high scores without meeting the assignment rubric. Based on student feedback and our experience, we offer several recommendations for effectively integrating LLMs into future classroom evaluations. Our observation also highlights potential directions for improving LLM-based evaluators, including their instruction-following ability and vulnerability to prompt hacking",
    "checked": true,
    "id": "472251f534ddfae14d163874c7114a1894156efd",
    "semantic_title": "large language model as an assignment evaluator: insights, feedback, and challenges in a 1000+ student course",
    "citation_count": 0,
    "authors": [
      "Cheng-Han Chiang",
      "Wei-Chih Chen",
      "Chun-Yi Kuan",
      "Chienchou Yang",
      "Hung-yi Lee"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.147": {
    "title": "Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?",
    "volume": "main",
    "abstract": "State-of-the-art Large Language Models (LLMs) are accredited with an increasing number of different capabilities, ranging from reading comprehension over advanced mathematical and reasoning skills to possessing scientific knowledge. In this paper we focus on multi-hop reasoning—the ability to identify and integrate information from multiple textual sources.Given the concerns with the presence of simplifying cues in existing multi-hop reasoning benchmarks, which allow models to circumvent the reasoning requirement, we set out to investigate whether LLMs are prone to exploiting such simplifying cues. We find evidence that they indeed circumvent the requirement to perform multi-hop reasoning, but they do so in more subtle ways than what was reported about their fine-tuned pre-trained language model (PLM) predecessors. We propose a challenging multi-hop reasoning benchmark by generating seemingly plausible multi-hop reasoning chains that ultimately lead to incorrect answers. We evaluate multiple open and proprietary state-of-the-art LLMs and show that their multi-hop reasoning performance is affected, as indicated by up to 45% relative decrease in F1 score when presented with such seemingly plausible alternatives. We also find that—while LLMs tend to ignore misleading lexical cues—misleading reasoning paths indeed present a significant challenge. The code and data are made available at https://github.com/zawedcvg/Are-Large-Language-Models-Attentive-Readers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Neeladri Bhuiya",
      "Viktor Schlegel",
      "Stefan Winkler"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.148": {
    "title": "Instruction Pre-Training: Language Models are Supervised Multitask Learners",
    "volume": "main",
    "abstract": "Unsupervised multitask pre-training has been the critical method behind the recent success of language models (LMs). However, supervised multitask learning still holds significant promise, as scaling it in the post-training stage trends towards better generalization. In this paper, we explore supervised multitask pre-training by proposing Instruction Pre-training, a framework that scalably augments massive raw corpora with instruction-response pairs to pre-train LMs. The instruction-response pairs are generated by an efficient instruction synthesizer built on open-source models. In our experiments, we synthesize 200M instruction response pairs covering 40+ task categories to verify the effectiveness of Instruction Pre-training. In pre-training from scratch, Instruction Pre-training not only consistently enhances pre-trained base models but also benefits more from further instruction tuning. In continual pre-training, Instruction Pre-training enables Llama3-8B to be comparable to or even outperform Llama3-70B. Our model, code, and data are available at https://github.com/microsoft/LMOps",
    "checked": true,
    "id": "5b7cc3c440147f13f032570213eab40eca8d70c1",
    "semantic_title": "instruction pre-training: language models are supervised multitask learners",
    "citation_count": 11,
    "authors": [
      "Daixuan Cheng",
      "Yuxian Gu",
      "Shaohan Huang",
      "Junyu Bi",
      "Minlie Huang",
      "Furu Wei"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.149": {
    "title": "LEMoE: Advanced Mixture of Experts Adaptor for Lifelong Model Editing of Large Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) require continual knowledge updates to stay abreast of the ever-changing world facts, prompting the formulation of lifelong model editing task. While recent years have witnessed the development of various techniques for single and batch editing, these methods either fail to apply or perform sub-optimally when faced with lifelong editing. In this paper, we introduce LEMoE, an advanced Mixture of Experts (MoE) adaptor for lifelong model editing. We first analyze the factors influencing the effectiveness of conventional MoE adaptor in lifelong editing, including catastrophic forgetting, inconsistent routing and order sensitivity. Based on these insights, we propose a tailored module insertion method to achieve lifelong editing, incorporating a novel KV anchor routing to enhance routing consistency between training and inference stage, along with a concise yet effective clustering-based editing order planning. Experimental results demonstrate the effectiveness of our method in lifelong editing, surpassing previous model editing techniques while maintaining outstanding performance in batch editing task. Our code will be available",
    "checked": true,
    "id": "6202c20b152b6b9f379aa1714bbd01594dcc990a",
    "semantic_title": "lemoe: advanced mixture of experts adaptor for lifelong model editing of large language models",
    "citation_count": 3,
    "authors": [
      "Renzhi Wang",
      "Piji Li"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.150": {
    "title": "Collaborative Performance Prediction for Large Language Models",
    "volume": "main",
    "abstract": "Comprehensively understanding and accurately predicting the performance of large language models across diverse downstream tasks has emerged as a pivotal challenge in NLP research. The pioneering scaling law on downstream works demonstrated intrinsic similarities within model families and utilized such similarities for performance prediction. However, they tend to overlook the similarities between model families and only consider design factors listed in the original scaling law. To overcome these limitations, we introduce a novel framework, Collaborative Performance Prediction (CPP), which significantly enhances prediction accuracy by leveraging the historical performance of various models on downstream tasks and other design factors for both model and task. We also collect a collaborative data sourced from online platforms containing both historical performance and additional design factors. With the support of the collaborative data, CPP not only surpasses traditional scaling laws in predicting the performance of scaled LLMs but also facilitates a detailed analysis of factor importance, an area previously overlooked",
    "checked": true,
    "id": "4c51430adfe11ff9f56399b710178d8b12411430",
    "semantic_title": "collaborative performance prediction for large language models",
    "citation_count": 0,
    "authors": [
      "Qiyuan Zhang",
      "Fuyuan Lyu",
      "Xue Liu",
      "Chen Ma"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.151": {
    "title": "Surveying the Dead Minds: Historical-Psychological Text Analysis with Contextualized Construct Representation (CCR) for Classical Chinese",
    "volume": "main",
    "abstract": "In this work, we develop a pipeline for historical-psychological text analysis in classical Chinese. Humans have produced texts in various languages for thousands of years; however, most of the computational literature is focused on contemporary languages and corpora. The emerging field of historical psychology relies on computational techniques to extract aspects of psychology from historical corpora using new methods developed in natural language processing (NLP). The present pipeline, called Contextualized Construct Representations (CCR), combines expert knowledge in psychometrics (i.e., psychological surveys) with text representations generated via Transformer-based language models to measure psychological constructs such as traditionalism, norm strength, and collectivism in classical Chinese corpora. Considering the scarcity of available data, we propose an indirect supervised contrastive learning approach and build the first Chinese historical psychology corpus (C-HI-PSY) to fine-tune pre-trained models. We evaluate the pipeline to demonstrate its superior performance compared with other approaches. The CCR method outperforms word-embedding-based approaches across all of our tasks and exceeds prompting with GPT-4 in most tasks. Finally, we benchmark the pipeline against objective, external data to further verify its validity",
    "checked": true,
    "id": "b4ed361980a8557081e3a2139a016a86a5e04a68",
    "semantic_title": "surveying the dead minds: historical-psychological text analysis with contextualized construct representation (ccr) for classical chinese",
    "citation_count": 1,
    "authors": [
      "Yuqi Chen",
      "Sixuan Li",
      "Ying Li",
      "Mohammad Atari"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.152": {
    "title": "Knowledge Verification to Nip Hallucination in the Bud",
    "volume": "main",
    "abstract": "While large language models (LLMs) have demonstrated exceptional performance across various tasks following human alignment, they may still generate responses that sound plausible but contradict factual knowledge, a phenomenon known as hallucination. In this paper, we demonstrate the feasibility of mitigating hallucinations by verifying and minimizing the inconsistency between external knowledge present in the alignment data and the intrinsic knowledge embedded within foundation LLMs. Specifically, we propose a novel approach called Knowledge Consistent Alignment (KCA), which employs a well-aligned LLM to automatically formulate assessments based on external knowledge to evaluate the knowledge boundaries of foundation LLMs. To address knowledge inconsistencies in the alignment data, KCA implements several specific strategies to deal with these data instances. We demonstrate the superior efficacy of KCA in reducing hallucinations across six benchmarks, utilizing foundation LLMs of varying backbones and scales. This confirms the effectiveness of mitigating hallucinations by reducing knowledge inconsistency. Our code, model weights, and data are openly accessible at https://github.com/fanqiwan/KCA",
    "checked": true,
    "id": "fcee1c19e12f3b7e3595aeba702416d055bdbc3f",
    "semantic_title": "knowledge verification to nip hallucination in the bud",
    "citation_count": 1,
    "authors": [
      "Fanqi Wan",
      "Xinting Huang",
      "Leyang Cui",
      "Xiaojun Quan",
      "Wei Bi",
      "Shuming Shi"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.153": {
    "title": "QUITE: Quantifying Uncertainty in Natural Language Text in Bayesian Reasoning Scenarios",
    "volume": "main",
    "abstract": "Reasoning is key to many decision making processes. It requires consolidating a set of rule-like premises that are often associated with degrees of uncertainty and observations to draw conclusions. In this work, we address both the case where premises are specified as numeric probabilistic rules and situations in which humans state their estimates using words expressing degrees of certainty. Existing probabilistic reasoning datasets simplify the task, e.g., by requiring the model to only rank textual alternatives, by including only binary random variables, or by making use of a limited set of templates that result in less varied text.In this work, we present QUITE, a question answering dataset of real-world Bayesian reasoning scenarios with categorical random variables and complex relationships. QUITE provides high-quality natural language verbalizations of premises together with evidence statements and expects the answer to a question in the form of an estimated probability. We conduct an extensive set of experiments, finding that logic-based models outperform out-of-the-box large language models on all reasoning types (causal, evidential, and explaining-away). Our results provide evidence that neuro-symbolic models are a promising direction for improving complex reasoning. We release QUITE and code for training and experiments on Github",
    "checked": true,
    "id": "483997ca3e9b18b9dc1a1c8edf7defd984d60c51",
    "semantic_title": "quite: quantifying uncertainty in natural language text in bayesian reasoning scenarios",
    "citation_count": 0,
    "authors": [
      "Timo Schrader",
      "Lukas Lange",
      "Simon Razniewski",
      "Annemarie Friedrich"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.154": {
    "title": "African or European Swallow? Benchmarking Large Vision-Language Models for Fine-Grained Object Classification",
    "volume": "main",
    "abstract": "Recent Large Vision-Language Models (LVLMs) demonstrate impressive abilities on numerous image understanding and reasoning tasks. The task of fine-grained object classification (e.g., distinction between animal species), however, has been probed insufficiently, despite its downstream importance. We fill this evaluation gap by creating FOCI (Fine-grained Object ClassIfication), a difficult multiple-choice benchmark for fine-grained object classification, from existing object classification datasets: (1) multiple-choice avoids ambiguous answers associated with casting classification as open-ended QA task; (2) we retain classification difficulty by mining negative labels with a CLIP model. FOCI complements five popular classification datasets with four domain-specific subsets from ImageNet-21k. We benchmark 12 public LVLMs on and show that it tests for a complementary skill to established image understanding and reasoning benchmarks. Crucially, CLIP models exhibit dramatically better performance than LVLMs. Since the image encoders of LVLMs come from these CLIP models, this points to inadequate alignment for fine-grained object distinction between the encoder and the LLM and warrants (pre)training data with more fine-grained annotation. We release our code at ANONYMIZED",
    "checked": true,
    "id": "10b685acdf642c8c1534515941c5242e514ed270",
    "semantic_title": "african or european swallow? benchmarking large vision-language models for fine-grained object classification",
    "citation_count": 3,
    "authors": [
      "Gregor Geigle",
      "Radu Timofte",
      "Goran Glavaš"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.155": {
    "title": "Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have shown impressive capabilities but still suffer from the issue of hallucinations. A significant type of this issue is the false premise hallucination, which we define as the phenomenon when LLMs generate hallucinated text when confronted with false premise questions. In this paper, we perform a comprehensive analysis of the false premise hallucination and elucidate its internal working mechanism: a small subset of attention heads (which we designate as false premise heads) disturb the knowledge extraction process, leading to the occurrence of false premise hallucination. Based on our analysis, we propose FAITH (False premise Attention head constraIining for miTigating Hallucinations), a novel and effective method to mitigate false premise hallucinations. It constrains the false premise attention heads during the model inference process. Impressively, extensive experiments demonstrate that constraining only approximately 1% of the attention heads in the model yields a notable increase of nearly 20% of model performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongbang Yuan",
      "Pengfei Cao",
      "Zhuoran Jin",
      "Yubo Chen",
      "Daojian Zeng",
      "Kang Liu",
      "Jun Zhao"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.156": {
    "title": "To Word Senses and Beyond: Inducing Concepts with Contextualized Language Models",
    "volume": "main",
    "abstract": "Polysemy and synonymy are two crucial interrelated facets of lexicalambiguity. While both phenomena are widely documented in lexical resources and have been studied extensively in NLP,leading to dedicated systems, they are often being consideredindependently in practictal problems. While many tasks dealing with polysemy (e.g. Word SenseDisambiguiation or Induction) highlight the role of word's senses,the study of synonymy is rooted in the study of concepts, i.e. meaningsshared across the lexicon. In this paper, we introduce ConceptInduction, the unsupervised task of learning a soft clustering amongwords that defines a set of concepts directly from data. This taskgeneralizes Word Sense Induction. We propose a bi-levelapproach to Concept Induction that leverages both a locallemma-centric view and a global cross-lexicon view to induceconcepts. We evaluate the obtained clustering on SemCor's annotateddata and obtain good performance (BCubed F1 above0.60). We find that the local and the global levels are mutuallybeneficial to induce concepts and also senses in our setting. Finally,we create static embeddings representing our induced concepts and usethem on the Word-in-Context task, obtaining competitive performancewith the State-of-the-Art",
    "checked": true,
    "id": "602dbe8dcf866e17dbd52d8640d09cd03f5182bf",
    "semantic_title": "to word senses and beyond: inducing concepts with contextualized language models",
    "citation_count": 1,
    "authors": [
      "Bastien Liétard",
      "Pascal Denis",
      "Mikaela Keller"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.157": {
    "title": "ASETF: A Novel Method for Jailbreak Attack on LLMs through Translate Suffix Embeddings",
    "volume": "main",
    "abstract": "The safety defense methods of Large language models (LLMs) stays limited because the dangerous prompts are manually curated to just few known attack types, which fails to keep pace with emerging varieties. Recent studies found that attaching suffixes to harmful instructions can hack the defense of LLMs and lead to dangerous outputs. However, similar to traditional text adversarial attacks, this approach, while effective, is limited by the challenge of the discrete tokens. This gradient based discrete optimization attack requires over 100,000 LLM calls, and due to the unreadable of adversarial suffixes, it can be relatively easily penetrated by common defense methods such as perplexity filters.To cope with this challenge, in this paper, we propose an Adversarial Suffix Embedding Translation Framework (ASETF), aimed at transforming continuous adversarial suffix embeddings into coherent and understandable text. This method greatly reduces the computational overhead during the attack process and helps to automatically generate multiple adversarial samples, which can be used as data to strengthen LLM's security defense. Experimental evaluations were conducted on Llama2, Vicuna, and other prominent LLMs, employing harmful directives sourced from the Advbench dataset.The results indicate that our method significantly reduces the computation time of adversarial suffixes and achieves a much better attack success rate than existing techniques, while significantly enhancing the textual fluency of the prompts. In addition, our approach can be generalized into a broader method for generating transferable adversarial suffixes that can successfully attack multiple LLMs, even black-box LLMs, such as ChatGPT and Gemini",
    "checked": true,
    "id": "854dd4e53498bf201e9cfaeb5e977a1f7612ef4c",
    "semantic_title": "asetf: a novel method for jailbreak attack on llms through translate suffix embeddings",
    "citation_count": 4,
    "authors": [
      "Hao Wang",
      "Hao Li",
      "Minlie Huang",
      "Lei Sha"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.158": {
    "title": "An Electoral Approach to Diversify LLM-based Multi-Agent Collective Decision-Making",
    "volume": "main",
    "abstract": "Modern large language models (LLMs) have exhibited cooperative synergy on complex task-solving, and collective decision-making (CDM) is a pivotal component in LLM-based multi-agent collaboration frameworks. Our survey on 52 recent such systems uncovers a severe lack of diversity, with a heavy reliance on dictatorial and plurality voting for CDM. Through the lens of social choice theory, we scrutinize widely-adopted CDM methods and identify their limitations. To enrich current landscape of LLM-based CDM, we present GEDI, an electoral CDM module that incorporates various ordinal preferential voting mechanisms. Our empirical case study across three benchmarks shows that the integration of certain CDM methods can markedly improve the reasoning capabilities and robustness of some leading LLMs, all without requiring intricate system designs. Additionally, we find that some CDM mechanisms generate positive synergies even with as few as three agents. The voting-based methods also demonstrate robustness against single points of failure, as well as diversity in terms of hit-rate@k and subject-wise impacts",
    "checked": true,
    "id": "ca01cb09c81af738b8108615115e7bfe96f44ec9",
    "semantic_title": "an electoral approach to diversify llm-based multi-agent collective decision-making",
    "citation_count": 0,
    "authors": [
      "Xiutian Zhao",
      "Ke Wang",
      "Wei Peng"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.159": {
    "title": "Does Object Grounding Really Reduce Hallucination of Large Vision-Language Models?",
    "volume": "main",
    "abstract": "Large vision-language models (LVLMs) have recently dramatically pushed the state of the art in image captioning and many image understanding tasks (e.g., visual question answering). LVLMs, however, often hallucinate and produce captions that mention concepts that cannot be found in the image. These hallucinations erode the trustworthiness of LVLMs and are arguably among the main obstacles to their ubiquitous adoption. Recent work suggests that addition of grounding objectives—those that explicitly align image regions or objects to text spans—reduces the amount of LVLM hallucination. Although intuitive, this claim is not empirically justified as the reduction effects have been established, we argue, with flawed evaluation protocols that (i) rely on data (i.e., MSCOCO) that has been extensively used in LVLM training and (ii) measure hallucination via question answering rather than open-ended caption generation.In this work, in contrast, we offer the first systematic analysis of the effect of fine-grained object grounding on LVLM hallucination under an evaluation protocol that more realistically captures LVLM hallucination in open generation. Our extensive experiments over three backbone LLMs reveal that grounding objectives have little to no effect on object hallucination in open caption generation",
    "checked": true,
    "id": "fa5a8e7cbbbb8ee47610733c363bb96bf31e049b",
    "semantic_title": "does object grounding really reduce hallucination of large vision-language models?",
    "citation_count": 0,
    "authors": [
      "Gregor Geigle",
      "Radu Timofte",
      "Goran Glavaš"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.160": {
    "title": "Take Off the Training Wheels! Progressive In-Context Learning for Effective Alignment",
    "volume": "main",
    "abstract": "Recent studies have explored the working mechanisms of In-Context Learning (ICL). However, they mainly focus on classification and simple generation tasks, limiting their broader application to more complex generation tasks in practice. To address this gap, we investigate the impact of demonstrations on token representations within the practical alignment tasks. We find that the transformer embeds the task function learned from demonstrations into the separator token representation, which plays an important role in the generation of prior response tokens. Once the prior response tokens are determined, the demonstrations become redundant. Motivated by this finding, we propose an efficient Progressive In-Context Alignment (PICA) method consisting of two stages. In the first few-shot stage, the model generates several prior response tokens via standard ICL while concurrently extracting the ICL vector that stores the task function from the separator token representation. In the following zero-shot stage, this ICL vector guides the model to generate responses without further demonstrations. Extensive experiments demonstrate that our PICA not only surpasses vanilla ICL but also achieves comparable performance to other alignment tuning methods. The proposed training-free method reduces the time cost (e.g., 5.45×) with improved alignment performance (e.g., 6.57+). Consequently, our work highlights the application of ICL for alignment and calls for a deeper understanding of ICL for complex generations. The code will be available at https://github.com/HITsz-TMG/PICA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenyu Liu",
      "Dongfang Li",
      "Xinshuo Hu",
      "Xinping Zhao",
      "Yibin Chen",
      "Baotian Hu",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.161": {
    "title": "MoDULA: Mixture of Domain-Specific and Universal LoRA for Multi-Task Learning",
    "volume": "main",
    "abstract": "The growing demand for larger-scale models in the development of Large Language Models (LLMs) poses challenges for efficient training within limited computational resources. Traditional fine-tuning methods often exhibit instability in multi-task learning and rely heavily on extensive training resources. Here, we propose MoDULA (Mixture of Domain-Specific and Universal LoRA), a novel Parameter Efficient Fine-Tuning (PEFT) Mixture-of-Expert (MoE) paradigm for improved fine-tuning and parameter efficiency in multi-task learning. The paradigm effectively improves the multi-task capability of the model by training universal experts, domain-specific experts, and routers separately. MoDULA-Res is a new method within the MoDULA paradigm, which maintains the model's general capability by connecting universal and task-specific experts through residual connections. The experimental results demonstrate that the overall performance of the MoDULA-Flan and MoDULA-Res methods surpasses that of existing fine-tuning methods on various LLMs. Notably, MoDULA-Res achieves more significant performance improvements in multiple tasks while reducing training costs by over 80% without losing general capability. Moreover, MoDULA displays flexible pluggability, allowing for the efficient addition of new tasks without retraining existing experts from scratch. This progressive training paradigm circumvents data balancing issues, enhancing training efficiency and model stability. Overall, MoDULA provides a scalable, cost-effective solution for fine-tuning LLMs with enhanced parameter efficiency and generalization capability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufei Ma",
      "Zihan Liang",
      "Huangyu Dai",
      "Ben Chen",
      "Dehong Gao",
      "Zhuoran Ran",
      "Wang Zihan",
      "Linbo Jin",
      "Wen Jiang",
      "Guannan Zhang",
      "Xiaoyan Cai",
      "Libin Yang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.162": {
    "title": "Message Passing on Semantic-Anchor-Graphs for Fine-grained Emotion Representation Learning and Classification",
    "volume": "main",
    "abstract": "Emotion classification has wide applications in education, robotics, virtual reality, etc. However, identifying subtle differences between fine-grained emotion categories remains challenging. Current methods typically aggregate numerous token embeddings of a sentence into a single vector, which, while being an efficient compressor, may not fully capture complex semantic and temporal distributions. To solve this problem, we propose SEmantic ANchor Graph Neural Networks (SEAN-GNN) for fine-grained emotion classification. It learns a group of representative, multi-faceted semantic anchors in the token embedding space: using these anchors as a global reference, any sentence can be projected onto them to form a \"semantic-anchor graph\", with node attributes and edge weights quantifying the semantic and temporal information respectively. The graph structure is well aligned across sentences and, importantly, allows for generating comprehensive emotion representations regarding K different anchors. Message passing on this graph can further integrate and refine the learned features. Empirically, SEAN-GNN can generate meaningful semantic anchors and discriminative graph patterns for different emotion, with promising classification results on 6 popular benchmark datasets against state-of-the-arts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pinyi Zhang",
      "Jingyang Chen",
      "Junchen Shen",
      "Zijie Zhai",
      "Ping Li",
      "Jie Zhang",
      "Kai Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.163": {
    "title": "PhiloGPT: A Philology-Oriented Large Language Model for Ancient Chinese Manuscripts with Dunhuang as Case Study",
    "volume": "main",
    "abstract": "Philology, the study of ancient manuscripts, demands years of professional training in ex-tensive knowledge memorization and manual textual retrieval. Despite these requirements align closely with strengths of recent successful Large Language Models (LLMs), the scarcity of high-quality, specialized training data has hindered direct applications. To bridge this gap, we curated the PhiloCorpus-ZH, a rich collec-tion of ancient Chinese texts spanning a millen-nium with 30 diverse topics, including firsthand folk copies. This corpus facilitated the develop-ment of PhiloGPT, the first LLM tailored for discovering ancient Chinese manuscripts. To effectively tackle complex philological tasks like restoration, attribution, and linguistic anal-ysis, we introduced the PhiloCoP framework. Modeled on the analytical patterns of philol-ogists, PhiloCoP enhances LLM's handling of historical linguistic peculiarities such as phonetic loans, polysemy, and syntactic inver-sions. We further integrated these tasks into the PhiloBenchmark, establishing a new standard for evaluating ancient Chinese LLMs address-ing philology tasks. Deploying PhiloGPT in practical scenarios has enabled Dunhuang spe-cialists to resolve philology tasks, such as iden-tifying duplication of copied text and assisting archaeologists with text completion, demon-strating its potential in real-world applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuqing Zhang",
      "Baoyi He",
      "Yihan Chen",
      "Hangqi Li",
      "Han Yue",
      "Shengyu Zhang",
      "Huaiyong Dou",
      "Junchi Yan",
      "Zemin Liu",
      "Yongquan Zhang",
      "Fei Wu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.164": {
    "title": "Alignment-Enhanced Decoding: Defending Jailbreaks via Token-Level Adaptive Refining of Probability Distributions",
    "volume": "main",
    "abstract": "Large language models are susceptible to jailbreak attacks, which can result in the generation of harmful content. While prior defenses mitigate these risks by perturbing or inspecting inputs, they ignore competing objectives, the underlying cause of alignment failures. In this paper, we propose Alignment-Enhanced Decoding (AED), a novel defense that employs adaptive decoding to address the root causes of jailbreak issues. We first define the Competitive Index to quantify alignment failures and utilize feedback from self-evaluation to compute post-alignment logits. Then, AED adaptively combines Competitive Index and post-alignment logits with the original logits to obtain harmless and helpful distributions. Consequently, our method enhances safety alignment while maintaining helpfulness. We conduct experiments across five models and four common jailbreaks, with the results validating the effectiveness of our approach",
    "checked": false,
    "id": "1de5b3e45331d0902fe6496051121598df28d78f",
    "semantic_title": "alignment-enhanced decoding:defending via token-level adaptive refining of probability distributions",
    "citation_count": 0,
    "authors": [
      "Quan Liu",
      "Zhenhong Zhou",
      "Longzhu He",
      "Yi Liu",
      "Wei Zhang",
      "Sen Su"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.165": {
    "title": "MiniConGTS: A Near Ultimate Minimalist Contrastive Grid Tagging Scheme for Aspect Sentiment Triplet Extraction",
    "volume": "main",
    "abstract": "Aspect Sentiment Triplet Extraction (ASTE) aims to co-extract the sentiment triplets in a given corpus. Existing approaches within the pretraining-finetuning paradigm tend to either meticulously craft complex tagging schemes and classification heads, or incorporate external semantic augmentation to enhance performance. In this study, we, for the first time, re-evaluate the redundancy in tagging schemes and the internal enhancement in pretrained representations. We propose a method to improve and utilize pretrained representations by integrating a minimalist tagging scheme and a novel token-level contrastive learning strategy. The proposed approach demonstrates comparable or superior performance compared to state-of-the-art techniques while featuring a more compact design and reduced computational overhead. Additionally, we are the first to formally evaluate GPT-4's performance in few-shot learning and Chain-of-Thought scenarios for this task. The results demonstrate that the pretraining-finetuning paradigm remains highly effective even in the era of large language models",
    "checked": true,
    "id": "eea73ab4eb59dc1fdebeab5805b530b50bcd35e3",
    "semantic_title": "minicongts: a near ultimate minimalist contrastive grid tagging scheme for aspect sentiment triplet extraction",
    "citation_count": 0,
    "authors": [
      "Qiao Sun",
      "Liujia Yang",
      "Minghao Ma",
      "Nanyang Ye",
      "Qinying Gu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.166": {
    "title": "Evaluating Large Language Models via Linguistic Profiling",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) undergo extensive evaluation against various benchmarks collected in established leaderboards to assess their performance across multiple tasks. However, to the best of our knowledge, there is a lack of comprehensive studies evaluating these models' linguistic abilities independent of specific tasks. In this paper, we introduce a novel evaluation methodology designed to test LLMs' sentence generation abilities under specific linguistic constraints. Drawing on the ‘linguistic profiling' approach, we rigorously investigate the extent to which five LLMs of varying sizes, tested in both zero- and few-shot scenarios, effectively adhere to (morpho)syntactic constraints. Our findings shed light on the linguistic proficiency of LLMs, revealing both their capabilities and limitations in generating linguistically-constrained sentences",
    "checked": false,
    "id": "5da06eb3a746932acfe36b81c7c640c3d969ae70",
    "semantic_title": "evaluating gender bias in large language models via chain-of-thought prompting",
    "citation_count": 22,
    "authors": [
      "Alessio Miaschi",
      "Felice Dell’Orletta",
      "Giulia Venturi"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.167": {
    "title": "With Ears to See and Eyes to Hear: Sound Symbolism Experiments with Multimodal Large Language Models",
    "volume": "main",
    "abstract": "Recently, Large Language Models (LLMs) and Vision Language Models (VLMs) have demonstrated aptitude as potential substitutes for human participants in experiments testing psycholinguistic phenomena. However, an understudied question is to what extent models that only have access to vision and text modalities are able to implicitly understand sound-based phenomena via abstract reasoning from orthography and imagery alone. To investigate this, we analyse the ability of VLMs and LLMs to demonstrate sound symbolism (i.e., to recognise a non-arbitrary link between sounds and concepts) as well as their ability to \"hear\" via the interplay of the language and vision modules of open and closed-source multimodal models. We perform multiple experiments, including replicating the classic Kiki-Bouba and Mil-Mal shape and magnitude symbolism tasks and comparing human judgements of linguistic iconicity with that of LLMs. Our results show that VLMs demonstrate varying levels of agreement with human labels, and more task information may be required for VLMs versus their human counterparts for in silico experimentation. We additionally see through higher maximum agreement levels that Magnitude Symbolism is an easier pattern for VLMs to identify than Shape Symbolism, and that an understanding of linguistic iconicity is highly dependent on model size",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tyler Loakman",
      "Yucheng Li",
      "Chenghua Lin"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.168": {
    "title": "KB-Plugin: A Plug-and-play Framework for Large Language Models to Induce Programs over Low-resourced Knowledge Bases",
    "volume": "main",
    "abstract": "Program induction (PI) has become a promising paradigm for using knowledge bases (KBs) to help large language models (LLMs) answer complex knowledge-intensive questions. Nonetheless, PI typically relies on a large number of parallel question-program pairs to make the LLM aware of the schema of a given KB, and is thus challenging for many low-resourced KBs that lack annotated data. To this end, we propose KB-Plugin, a plug-and-play framework that enables LLMs to induce programs over any low-resourced KB. Firstly, KB-Plugin adopts self-supervised learning to encode the detailed schema information of a given KB into a pluggable module, namely schema plugin. Secondly, KB-Plugin utilizes abundant annotated data from a rich-resourced KB to train another pluggable module, namely PI plugin, which can help the LLM extract question-relevant schema information from the schema plugin of any KB and utilize the information to induce programs over this KB. Experiments show that KB-Plugin outperforms SoTA low-resourced PI methods with 25x smaller backbone LLM on both large-scale and domain-specific KBs, and even approaches the performance of supervised methods",
    "checked": true,
    "id": "db62b76906ab2663c79a149aab58b2b08473ab16",
    "semantic_title": "kb-plugin: a plug-and-play framework for large language models to induce programs over low-resourced knowledge bases",
    "citation_count": 1,
    "authors": [
      "Jiajie Zhang",
      "Shulin Cao",
      "Linmei Hu",
      "Ling Feng",
      "Lei Hou",
      "Juanzi Li"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.169": {
    "title": "Understanding Higher-Order Correlations Among Semantic Components in Embeddings",
    "volume": "main",
    "abstract": "Independent Component Analysis (ICA) offers interpretable semantic components of embeddings.While ICA theory assumes that embeddings can be linearly decomposed into independent components, real-world data often do not satisfy this assumption. Consequently, non-independencies remain between the estimated components, which ICA cannot eliminate. We quantified these non-independencies using higher-order correlations and demonstrated that when the higher-order correlation between two components is large, it indicates a strong semantic association between them, along with many words sharing common meanings with both components. The entire structure of non-independencies was visualized using a maximum spanning tree of semantic components. These findings provide deeper insights into embeddings through ICA",
    "checked": true,
    "id": "64f97659d1348843956f24c5aa39e264aa891f28",
    "semantic_title": "understanding higher-order correlations among semantic components in embeddings",
    "citation_count": 0,
    "authors": [
      "Momose Oyama",
      "Hiroaki Yamagiwa",
      "Hidetoshi Shimodaira"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.170": {
    "title": "DGLF: A Dual Graph-based Learning Framework for Multi-modal Sarcasm Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihong Zhu",
      "Kefan Shen",
      "Zhaorun Chen",
      "Yunyan Zhang",
      "Yuyan Chen",
      "Xiaoqi Jiao",
      "Zhongwei Wan",
      "Shaorong Xie",
      "Wei Liu",
      "Xian Wu",
      "Yefeng Zheng"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.171": {
    "title": "Evaluating D-MERIT of Partial-annotation on Information Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "365f9c71eac3da18f42e6361de6746d17b30c081",
    "semantic_title": "evaluating d-merit of partial-annotation on information retrieval",
    "citation_count": 0,
    "authors": [
      "Royi Rassin",
      "Yaron Fairstein",
      "Oren Kalinsky",
      "Guy Kushilevitz",
      "Nachshon Cohen",
      "Alexander Libov",
      "Yoav Goldberg"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.172": {
    "title": "Verification and Refinement of Natural Language Explanations through LLM-Symbolic Theorem Proving",
    "volume": "main",
    "abstract": "Natural language explanations represent a proxy for evaluating explanation-based and multi-step Natural Language Inference (NLI) models. However, assessing the validity of explanations for NLI is challenging as it typically involves the crowd-sourcing of apposite datasets, a process that is time-consuming and prone to logical errors. To address existing limitations, this paper investigates the verification and refinement of natural language explanations through the integration of Large Language Models (LLMs) and Theorem Provers (TPs). Specifically, we present a neuro-symbolic framework, named Explanation-Refiner, that integrates TPs with LLMs to generate and formalise explanatory sentences and suggest potential inference strategies for NLI. In turn, the TP is employed to provide formal guarantees on the logical validity of the explanations and to generate feedback for subsequent improvements. We demonstrate how Explanation-Refiner can be jointly used to evaluate explanatory reasoning, autoformalisation, and error correction mechanisms of state-of-the-art LLMs as well as to automatically enhance the quality of explanations of variable complexity in different domains",
    "checked": true,
    "id": "6fede93e6b6388a0f4d766a3adf22367e751de93",
    "semantic_title": "verification and refinement of natural language explanations through llm-symbolic theorem proving",
    "citation_count": 7,
    "authors": [
      "Xin Quan",
      "Marco Valentino",
      "Louise Dennis",
      "Andre Freitas"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.173": {
    "title": "Calibrating the Confidence of Large Language Models by Eliciting Fidelity",
    "volume": "main",
    "abstract": "Large language models optimized with techniques like RLHF have achieved good alignment in being helpful and harmless. However, post-alignment, these language models often exhibit overconfidence, where the expressed confidence does not accurately calibrate with their correctness rate. In this paper, we decompose the language model confidence into the Uncertainty about the question and the Fidelity to the answer generated by language models. Then, we propose a plug-and-play method, UF Calibration, to estimate the confidence of language models. Our method has shown good calibration performance by conducting experiments with 6 RLHF-LMs on four MCQA datasets. Moreover, we propose two novel metrics, IPR and CE, to evaluate the calibration of the model, and we have conducted a detailed discussion on Truly Well-Calibrated Confidence for large language models. Our method could serve as a strong baseline, and we hope that this work will provide some insights into the model confidence calibration",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mozhi Zhang",
      "Mianqiu Huang",
      "Rundong Shi",
      "Linsen Guo",
      "Chong Peng",
      "Peng Yan",
      "Yaqian Zhou",
      "Xipeng Qiu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.174": {
    "title": "The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models",
    "volume": "main",
    "abstract": "Reinforcement Learning from Human Feedback significantly enhances Natural Language Processing by aligning language models with human expectations. A critical factor in this alignment is the strength of reward models used during training. This study explores whether stronger reward models invariably lead to better language models. In this paper, through experiments on relevance, factuality, and completeness tasks using the QA-FEEDBACK dataset and reward models based on Longformer, we uncover a surprising paradox: language models trained with moderately accurate reward models outperform those guided by highly accurate ones. This challenges the widely held belief that stronger reward models always lead to better language models, and opens up new avenues for future research into the key factors driving model performance and how to choose the most suitable reward models",
    "checked": true,
    "id": "244d4b1f79b2e7abd5c0ea17273a53a91f0a6d33",
    "semantic_title": "the accuracy paradox in rlhf: when better reward models don't yield better language models",
    "citation_count": 0,
    "authors": [
      "Yanjun Chen",
      "Dawei Zhu",
      "Yirong Sun",
      "Xinghao Chen",
      "Wei Zhang",
      "Xiaoyu Shen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.175": {
    "title": "How Hard is this Test Set? NLI Characterization by Exploiting Training Dynamics",
    "volume": "main",
    "abstract": "Natural Language Inference (NLI) evaluation is crucial for assessing language understanding models; however, popular datasets suffer from systematic spurious correlations that artificially inflate actual model performance. To address this, we propose a method for the automated creation of a challenging test set without relying on the manual construction of artificial and unrealistic examples. We categorize the test set of popular NLI datasets into three difficulty levels by leveraging methods that exploit training dynamics. This categorization significantly reduces spurious correlation measures, with examples labeled as having the highest difficulty showing markedly decreased performance and encompassing more realistic and diverse linguistic phenomena. When our characterization method is applied to the training set, models trained with only a fraction of the data achieve comparable performance to those trained on the full dataset, surpassing other dataset characterization techniques. Our research addresses limitations in NLI dataset construction, providing a more authentic evaluation of model performance with implications for diverse NLU applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adrian Cosma",
      "Stefan Ruseti",
      "Mihai Dascalu",
      "Cornelia Caragea"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.176": {
    "title": "Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection",
    "volume": "main",
    "abstract": "Grammatical Error Detection (GED) methods rely heavily on human annotated error corpora. However, these annotations are unavailable in many low-resource languages. In this paper, we investigate GED in this context. Leveraging the zero-shot cross-lingual transfer capabilities of multilingual pre-trained language models, we train a model using data from a diverse set of languages to generate synthetic errors in other languages. These synthetic error corpora are then used to train a GED model. Specifically we propose a two-stage fine-tuning pipeline where the GED model is first fine-tuned on multilingual synthetic data from target languages followed by fine-tuning on human-annotated GED corpora from source languages. This approach outperforms current state-of-the-art annotation-free GED methods. We also analyse the errors produced by our method and other strong baselines, finding that our approach produces errors that are more diverse and more similar to human errors",
    "checked": true,
    "id": "ce5496b880a15d05c2d046b475c20b6e26a68a9f",
    "semantic_title": "zero-shot cross-lingual transfer for synthetic data generation in grammatical error detection",
    "citation_count": 0,
    "authors": [
      "Gaetan Latouche",
      "Marc-André Carbonneau",
      "Benjamin Swanson"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.177": {
    "title": "CUTE: Measuring LLMs' Understanding of Their Tokens",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) show remarkable performance on a wide variety of tasks. Most LLMs split text into multi-character tokens and process them as atomic units without direct access to individual characters. This raises the question: To what extent can LLMs learn orthographic information? To answer this, we propose a new benchmark, CUTE, which features a collection of tasks designed to test the orthographic knowledge of LLMs. We evaluate popular LLMs on CUTE, finding that most of them seem to know the spelling of their tokens, yet fail to use this information effectively to manipulate text, calling into question how much of this knowledge is generalizable",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lukas Edman",
      "Helmut Schmid",
      "Alexander Fraser"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.178": {
    "title": "SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation",
    "volume": "main",
    "abstract": "Recent studies in Retrieval-Augmented Generation (RAG) have investigated extracting evidence from retrieved passages to reduce computational costs and enhance the final RAG performance, yet it remains challenging. Existing methods heavily rely on heuristic-based augmentation, encountering several issues: (1) Poor generalization due to hand-crafted context filtering; (2) Semantics deficiency due to rule-based context chunking; (3) Skewed length due to sentence-wise filter learning. To address these issues, we propose a model-based evidence extraction learning framework, SEER, optimizing a vanilla model as an evidence extractor with desired properties through self-aligned learning. Extensive experiments show that our method largely improves the final RAG performance, enhances the faithfulness, helpfulness, and conciseness of the extracted evidence, and reduces the evidence length by 9.25 times. The code will be available at https://github.com/HITsz-TMG/SEER",
    "checked": true,
    "id": "a41cd77492af3140caccb7f21a351cc6dc87343f",
    "semantic_title": "seer: self-aligned evidence extraction for retrieval-augmented generation",
    "citation_count": 0,
    "authors": [
      "Xinping Zhao",
      "Dongfang Li",
      "Yan Zhong",
      "Boren Hu",
      "Yibin Chen",
      "Baotian Hu",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.179": {
    "title": "On the Role of Context in Reading Time Prediction",
    "volume": "main",
    "abstract": "We present a new perspective on how readers integrate context during real-time language comprehension. Our proposals build on surprisal theory, which posits that the processing effort of a linguistic unit (e.g., a word) is an affine function of its in-context information content. We first observe that surprisal is only one out of many potential ways that a contextual predictor can be derived from a language model. Another one is the pointwise mutual information (PMI) between a unit and its context, which turns out to yield the same predictive power as surprisal when controlling for unigram frequency. Moreover, both PMI and surprisal are correlated with frequency. This means that neither PMI nor surprisal contains information about context alone. In response to this, we propose a technique where we project surprisal onto the orthogonal complement of frequency, yielding a new contextual predictor that is uncorrelated with frequency. Our experiments show that the proportion of variance in reading times explained by context is a lot smaller when context is represented by the orthogonalized predictor. From an interpretability standpoint, this indicates that previous studies may have overstated the role that context has in predicting reading times",
    "checked": true,
    "id": "e30f0277e3e67b092e99a97d90f67144fde700c3",
    "semantic_title": "on the role of context in reading time prediction",
    "citation_count": 3,
    "authors": [
      "Andreas Opedal",
      "Eleanor Chodroff",
      "Ryan Cotterell",
      "Ethan Wilcox"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.180": {
    "title": "BC-Prover: Backward Chaining Prover for Formal Theorem Proving",
    "volume": "main",
    "abstract": "Despite the remarkable progress made by large language models in mathematical reasoning, interactive theorem proving in formal logic still remains a prominent challenge. Previous methods resort to neural models for proofstep generation and search. However, they suffer from exploring possible proofsteps empirically in a large search space. Moreover, they directly use a less rigorous informal proof for proofstep generation, neglecting the incomplete reasoning within. In this paper, we propose BC-Prover, a backward chaining framework guided by pseudo steps. Specifically, BC-Prover prioritizes pseudo steps to proofstep generation. The pseudo steps boost the proof construction in two aspects: (1) Backward Chaining that decomposes the proof into sub-goals for goal-oriented exploration. (2) Step Planning that makes a fine-grained planning to bridge the gap between informal and formal proofs. Experiments on the miniF2F benchmark show significant performance gains by our framework over the state-of-the-art approaches. Our framework is also compatible with existing provers and further improves their performance with the backward chaining technique",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhang He",
      "Jihai Zhang",
      "Jianzhu Bao",
      "Fangquan Lin",
      "Cheng Yang",
      "Bing Qin",
      "Ruifeng Xu",
      "Wotao Yin"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.181": {
    "title": "From Insights to Actions: The Impact of Interpretability and Analysis Research on NLP",
    "volume": "main",
    "abstract": "Interpretability and analysis (IA) research is a growing subfield within NLP with the goal of developing a deeper understanding of the behavior or inner workings of NLP systems and methods. Despite growing interest in the subfield, a criticism of this work is that it lacks actionable insights and therefore has little impact on NLP. In this paper, we seek to quantify the impact of IA research on the broader field of NLP. We approach this with a mixed-methods analysis of: (1) a citation graph of 185K+ papers built from all papers published at ACL and EMNLP conferences from 2018 to 2023, and their references and citations, and (2) a survey of 138 members of the NLP community. Our quantitative results show that IA work is well-cited outside of IA, and central in the NLP citation graph. Through qualitative analysis of survey responses and manual annotation of 556 papers, we find that NLP researchers build on findings from IA work and perceive it as important for progress in NLP, multiple subfields, and rely on its findings and terminology for their own work. Many novel methods are proposed based on IA findings and highly influenced by them, but highly influential non-IA work cites IA findings without being driven by them. We end by summarizing what is missing in IA work today and provide a call to action, to pave the way for a more impactful future of IA research",
    "checked": true,
    "id": "9ea1493cae4219a822738b8814467a283ebe9b1f",
    "semantic_title": "from insights to actions: the impact of interpretability and analysis research on nlp",
    "citation_count": 1,
    "authors": [
      "Marius Mosbach",
      "Vagrant Gautam",
      "Tomás Vergara Browne",
      "Dietrich Klakow",
      "Mor Geva"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.182": {
    "title": "Autoregressive Pre-Training on Pixels and Texts",
    "volume": "main",
    "abstract": "The integration of visual and textual information represents a promising direction in the advancement of language models. In this paper, we explore the dual modality of language—both visual and textual—within an autoregressive framework, pre-trained on both document images and texts. Our method employs a multimodal training strategy, utilizing visual data through next patch prediction with a regression head and/or textual data through next token prediction with a classification head. We focus on understanding the interaction between these two modalities and their combined impact on model performance. Our extensive evaluation across a wide range of benchmarks shows that incorporating both visual and textual data significantly improves the performance of pixel-based language models. Remarkably, we find that a unidirectional pixel-based model trained solely on visual data can achieve comparable results to state-of-the-art bidirectional models on several language understanding tasks. This work uncovers the untapped potential of integrating visual and textual modalities for more effective language modeling. We release our code, data, and model checkpoints at https://github.com/ernie-research/pixelgpt",
    "checked": true,
    "id": "e403ef4bed1dc184f8e92f3eb4c3ac7163a1fd2b",
    "semantic_title": "autoregressive pre-training on pixels and texts",
    "citation_count": 1,
    "authors": [
      "Yekun Chai",
      "Qingyi Liu",
      "Jingwu Xiao",
      "Shuohuan Wang",
      "Yu Sun",
      "Hua Wu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.183": {
    "title": "On Training Data Influence of GPT Models",
    "volume": "main",
    "abstract": "Amidst the rapid advancements in generative language models, the investigation of how training data shapes the performance of GPT models is still emerging. This paper presents GPTfluence, a novel approach that leverages a featurized simulation to assess the impact of training examples on the training dynamics of GPT models. Our approach not only traces the influence of individual training instances on performance trajectories, such as loss and other key metrics, on targeted test points but also enables a comprehensive comparison with existing methods across various training scenarios in GPT models, ranging from 14 million to 2.8 billion parameters, across a range of downstream tasks. Contrary to earlier methods that struggle with generalization to new data, GPTfluence introduces a parameterized simulation of training dynamics, demonstrating robust generalization capabilities to unseen training data. This adaptability is evident across both fine-tuning and instruction-tuning scenarios, spanning tasks in natural language understanding and generation. We make our code and data publicly available at https://github.com/ernie-research/gptfluence",
    "checked": true,
    "id": "02ffeb8cb7a1d1336e0e135c7c1000610ae7b942",
    "semantic_title": "on training data influence of gpt models",
    "citation_count": 2,
    "authors": [
      "Yekun Chai",
      "Qingyi Liu",
      "Shuohuan Wang",
      "Yu Sun",
      "Qiwei Peng",
      "Hua Wu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.184": {
    "title": "Understanding \"Democratization\" in NLP and ML Research",
    "volume": "main",
    "abstract": "Recent improvements in natural language processing (NLP) and machine learning (ML) and increased mainstream adoption have led to researchers frequently discussing the \"democratization\" of artificial intelligence. In this paper, we seek to clarify how democratization is understood in NLP and ML publications, through large-scale mixed-methods analyses of papers using the keyword \"democra*\" published in NLP and adjacent venues. We find that democratization is most frequently used to convey (ease of) access to or use of technologies, without meaningfully engaging with theories of democratization, while research using other invocations of \"democra*\" tends to be grounded in theories of deliberation and debate. Based on our findings, we call for researchers to enrich their use of the term democratization with appropriate theory, towards democratic technologies beyond superficial access",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arjun Subramonian",
      "Vagrant Gautam",
      "Dietrich Klakow",
      "Zeerak Talat"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.185": {
    "title": "DocKD: Knowledge Distillation from LLMs for Open-World Document Understanding Models",
    "volume": "main",
    "abstract": "Visual document understanding (VDU) is a challenging task that involves understanding documents across various modalities (text and image) and layouts (forms, tables, etc.). This study aims to enhance generalizability of small VDU models by distilling knowledge from LLMs. We identify that directly prompting LLMs often fails to generate informative and useful data. In response, we present a new framework (called DocKD) that enriches the data generation process by integrating external document knowledge. Specifically, we provide an LLM with various document elements like key-value pairs, layouts, and descriptions, to elicit open-ended answers. Our experiments show that DocKD produces high-quality document annotations and surpasses the direct knowledge distillation approach that does not leverage external document knowledge. Moreover, student VDU models trained with solely DocKD-generated data is not only comparable to those trained with human-annotated data on in-domain tasks but also significantly excel them on out-of-domain tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sungnyun Kim",
      "Haofu Liao",
      "Srikar Appalaraju",
      "Peng Tang",
      "Zhuowen Tu",
      "Ravi Satzoda",
      "R. Manmatha",
      "Vijay Mahadevan",
      "Stefano Soatto"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.186": {
    "title": "Cross-lingual Transfer for Automatic Question Generation by Learning Interrogative Structures in Target Languages",
    "volume": "main",
    "abstract": "Automatic question generation (QG) serves a wide range of purposes, such as augmenting question-answering (QA) corpora, enhancing chatbot systems, and developing educational materials. Despite its importance, most existing datasets predominantly focus on English, resulting in a considerable gap in data availability for other languages. Cross-lingual transfer for QG (XLT-QG) addresses this limitation by allowing models trained on high-resource language datasets to generate questions in low-resource languages. In this paper, we propose a simple and efficient XLT-QG method that operates without the need for monolingual, parallel, or labeled data in the target language, utilizing a small language model. Our model, trained solely on English QA datasets, learns interrogative structures from a limited set of question exemplars, which are then applied to generate questions in the target language. Experimental results show that our method outperforms several XLT-QG baselines and achieves performance comparable to GPT-3.5-turbo across different languages. Additionally, the synthetic data generated by our model proves beneficial for training multilingual QA models. With significantly fewer parameters than large language models and without requiring additional training for target languages, our approach offers an effective solution for QG and QA tasks across various languages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seonjeong Hwang",
      "Yunsu Kim",
      "Gary Lee"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.187": {
    "title": "ScalingFilter: Assessing Data Quality through Inverse Utilization of Scaling Laws",
    "volume": "main",
    "abstract": "High-quality data is crucial for the pre-training performance of large language models. Unfortunately, existing quality filtering methods rely on a known high-quality dataset as reference, which can introduce potential bias and compromise diversity. In this paper, we propose ScalingFilter, a novel approach that evaluates text quality based on the perplexity difference between two language models trained on the same data, thereby eliminating the influence of the reference dataset in the filtering process. An theoretical analysis shows that ScalingFilter is equivalent to an inverse utilization of scaling laws. Through training models with 1.3B parameters on the same data source processed by various quality filters, we find ScalingFilter can improve zero-shot performance of pre-trained models in downstream tasks. To assess the bias introduced by quality filtering, we introduce semantic diversity, a metric of utilizing text embedding models for semantic representations. Extensive experiments reveal that semantic diversity is a reliable indicator of dataset diversity, and ScalingFilter achieves an optimal balance between downstream performance and semantic diversity",
    "checked": true,
    "id": "66521063549cc84da1eab51f9f0b42108dd3efc7",
    "semantic_title": "scalingfilter: assessing data quality through inverse utilization of scaling laws",
    "citation_count": 0,
    "authors": [
      "Ruihang Li",
      "Yixuan Wei",
      "Miaosen Zhang",
      "Nenghai Yu",
      "Han Hu",
      "Houwen Peng"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.188": {
    "title": "Word Alignment as Preference for Machine Translation",
    "volume": "main",
    "abstract": "The problem of hallucination and omission, a long-standing problem in machine translation (MT), is more pronounced when a large language model (LLM) is used in MT because an LLM itself is susceptible to these phenomena. In this work, we mitigate the problem in an LLM-based MT model by guiding it to better word alignment. We first study the correlation between word alignment and the phenomena of hallucination and omission in MT. Then we propose to utilize word alignment as preference to optimize the LLM-based MT model. The preference data are constructed by selecting chosen and rejected translations from multiple MT tools. Subsequently, direct preference optimization is used to optimize the LLM-based model towards the preference signal. Given the absence of evaluators specifically designed for hallucination and omission in MT, we further propose selecting hard instances and utilizing GPT-4 to directly evaluate the performance of the models in mitigating these issues. We verify the rationality of these designed evaluation methods by experiments, followed by extensive results demonstrating the effectiveness of word alignment-based preference optimization to mitigate hallucination and omission. On the other hand, although it shows promise in mitigating hallucination and omission, the overall performance of MT in different language directions remains mixed, with slight increases in BLEU and decreases in COMET",
    "checked": true,
    "id": "5f39f57b175a27930dfe26aafc224dab8f34f8fc",
    "semantic_title": "word alignment as preference for machine translation",
    "citation_count": 1,
    "authors": [
      "Qiyu Wu",
      "Masaaki Nagata",
      "Zhongtao Miao",
      "Yoshimasa Tsuruoka"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.189": {
    "title": "Improving Multi-party Dialogue Generation via Topic and Rhetorical Coherence",
    "volume": "main",
    "abstract": "Previous studies on multi-party dialogue generation predominantly concentrated on modeling the reply-to structure of dialogue histories, always overlooking the coherence between generated responses and target utterances. To address this issue, we propose a Reinforcement Learning approach emphasizing both Topic and Rhetorical Coherence (RL-TRC). In particular, the topic- and rhetorical-coherence tasks are designed to enhance the model's perception of coherence with the target utterance. Subsequently, an agent is employed to learn a coherence policy, which guides the generation of responses that are topically and rhetorically aligned with the target utterance. Furthermore, three discourse-aware rewards are developed to assess the coherence between the generated response and the target utterance, with the objective of optimizing the policy. The experimental results and in-depth analyses on two popular datasets demonstrate that our RL-TRC significantly outperforms the state-of-the-art baselines, particularly in generating responses that are more coherent with the target utterances",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaxin Fan",
      "Peifeng Li",
      "Qiaoming Zhu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.190": {
    "title": "SEEKR: Selective Attention-Guided Knowledge Retention for Continual Learning of Large Language Models",
    "volume": "main",
    "abstract": "Continual learning (CL) is crucial for language models to dynamically adapt to the evolving real-world demands. To mitigate the catastrophic forgetting problem in CL, data replay has been proven a simple and effective strategy, and the subsequent data-replay-based distillation can further enhance the performance. However, existing methods fail to fully exploit the knowledge embedded in models from previous tasks, resulting in the need for a relatively large number of replay samples to achieve good results. In this work, we first explore and emphasize the importance of attention weights in knowledge retention, and then propose a SElective attEntion-guided Knowledge Retention method (SEEKR) for data-efficient replay-based continual learning of large language models (LLMs). Specifically, SEEKR performs attention distillation on the selected attention heads for finer-grained knowledge retention, where the proposed forgettability-based and task-sensitivity-based measures are used to identify the most valuable attention heads. Experimental results on two continual learning benchmarks for LLMs demonstrate the superiority of SEEKR over the existing methods on both performance and efficiency. Explicitly, SEEKR achieves comparable or even better performance with only 1/10 of the replayed data used by other methods, and reduces the proportion of replayed data to 1%. The code is available at https://github.com/jinghan1he/SEEKR",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinghan He",
      "Haiyun Guo",
      "Kuan Zhu",
      "Zihan Zhao",
      "Ming Tang",
      "Jinqiao Wang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.191": {
    "title": "Neuron-Level Knowledge Attribution in Large Language Models",
    "volume": "main",
    "abstract": "Identifying important neurons for final predictions is essential for understanding the mechanisms of large language models. Due to computational constraints, current attribution techniques struggle to operate at neuron level. In this paper, we propose a static method for pinpointing significant neurons. Compared to seven other methods, our approach demonstrates superior performance across three metrics. Additionally, since most static methods typically only identify \"value neurons\" directly contributing to the final prediction, we propose a method for identifying \"query neurons\" which activate these \"value neurons\". Finally, we apply our methods to analyze six types of knowledge across both attention and feed-forward network (FFN) layers. Our method and analysis are helpful for understanding the mechanisms of knowledge storage and set the stage for future research in knowledge editing. The code is available on https://github.com/zepingyu0512/neuron-attribution",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeping Yu",
      "Sophia Ananiadou"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.192": {
    "title": "How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning",
    "volume": "main",
    "abstract": "We investigate the mechanism of in-context learning (ICL) on sentence classification tasks with semantically-unrelated labels (\"foo\"/\"bar\"). We find intervening in only 1% heads (named \"in-context heads\") significantly affects ICL accuracy from 87.6% to 24.4%. To understand this phenomenon, we analyze the value-output vectors in these heads and discover that the vectors at each label position contain substantial information about the corresponding labels. Furthermore, we observe that the prediction shift from \"foo\" to \"bar\" is due to the respective reduction and increase in these heads' attention scores at \"foo\" and \"bar\" positions. Therefore, we propose a hypothesis for ICL: in in-context heads, the value-output matrices extract label features, while the query-key matrices compute the similarity between the features at the last position and those at each label position. The query and key matrices can be considered as two towers that learn the similarity metric between the last position's features and each demonstration at label positions. Using this hypothesis, we explain the majority label bias and recency bias in ICL and propose two methods to reduce these biases by 22% and 17%, respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeping Yu",
      "Sophia Ananiadou"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.193": {
    "title": "Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis",
    "volume": "main",
    "abstract": "We find arithmetic ability resides within a limited number of attention heads, with each head specializing in distinct operations. To delve into the reason, we introduce the Comparative Neuron Analysis (CNA) method, which identifies an internal logic chain consisting of four distinct stages from input to prediction: feature enhancing with shallow FFN neurons, feature transferring by shallow attention layers, feature predicting by arithmetic heads, and prediction enhancing among deep FFN neurons. Moreover, we identify the human-interpretable FFN neurons within both feature-enhancing and feature-predicting stages. These findings lead us to investigate the mechanism of LoRA, revealing that it enhances prediction probabilities by amplifying the coefficient scores of FFN neurons related to predictions. Finally, we apply our method in model pruning for arithmetic tasks and model editing for reducing gender bias. Code is on https://github.com/zepingyu0512/arithmetic-mechanism",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeping Yu",
      "Sophia Ananiadou"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.194": {
    "title": "Pixology: Probing the Linguistic and Visual Capabilities of Pixel-based Language Models",
    "volume": "main",
    "abstract": "Pixel-based language models have emerged as a compelling alternative to subword-based language modelling, particularly because they can represent virtually any script. PIXEL, a canonical example of such a model, is a vision transformer that has been pre-trained on rendered text. While PIXEL has shown promising cross-script transfer abilities and robustness to orthographic perturbations, it falls short of outperforming monolingual subword counterparts like BERT in most other contexts. This discrepancy raises questions about the amount of linguistic knowledge learnt by these models and whether their performance in language tasks stems more from their visual capabilities than their linguistic ones. To explore this, we probe PIXEL using a variety of linguistic and visual tasks to assess its position on the vision-to-language spectrum. Our findings reveal a substantial gap between the model's visual and linguistic understanding. The lower layers of PIXEL predominantly capture superficial visual features, whereas the higher layers gradually learn more syntactic and semantic abstractions. Additionally, we examine variants of PIXEL trained with different text rendering strategies, discovering that introducing certain orthographic constraints at the input level can facilitate earlier learning of surface-level features. With this study, we hope to provide insights that aid the further development of pixel-based language models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kushal Tatariya",
      "Vladimir Araujo",
      "Thomas Bauwens",
      "Miryam De Lhoneux"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.195": {
    "title": "GoldCoin: Grounding Large Language Models in Privacy Laws via Contextual Integrity Theory",
    "volume": "main",
    "abstract": "Privacy issues arise prominently during the inappropriate transmission of information between entities. Existing research primarily studies privacy by exploring various privacy attacks, defenses, and evaluations within narrowly predefined patterns, while neglecting that privacy is not an isolated, context-free concept limited to traditionally sensitive data (e.g., social security numbers), but intertwined with intricate social contexts that complicate the identification and analysis of potential privacy violations. The advent of Large Language Models (LLMs) offers unprecedented opportunities for incorporating the nuanced scenarios outlined in privacy laws to tackle these complex privacy issues. However, the scarcity of open-source relevant case studies restricts the efficiency of LLMs in aligning with specific legal statutes. To address this challenge, we introduce a novel framework, GoldCoin, designed to efficiently ground LLMs in privacy laws for judicial assessing privacy violations. Our framework leverages the theory of contextual integrity as a bridge, creating numerous synthetic scenarios grounded in relevant privacy statutes (e.g., HIPAA), to assist LLMs in comprehending the complex contexts for identifying privacy risks in the real world. Extensive experimental results demonstrate that GoldCoin markedly enhances LLMs' capabilities in recognizing privacy risks across real court cases, surpassing the baselines on different judicial tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Fan",
      "Haoran Li",
      "Zheye Deng",
      "Weiqi Wang",
      "Yangqiu Song"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.196": {
    "title": "Noise, Novels, Numbers. A Framework for Detecting and Categorizing Noise in Danish and Norwegian Literature",
    "volume": "main",
    "abstract": "We present a framework for detecting and categorizing noise in literary texts, demonstrated through its application to Danish and Norwegian literature from the late 19-th century. Noise, understood as \"aberrant sonic behaviour,\" is not only an auditory phenomenon but also a cultural construct tied to the processes of civilization and urbanization.We begin by utilizing topic modeling techniques to identify noise-related documents, followed by fine-tuning BERT-based language models trained on Danish and Norwegian texts to analyze a corpus of over 800 novels.We identify and track the prevalence of noise in these texts, offering insights into the literary perceptions of noise during the Scandinavian \"Modern Breakthrough\" period (1870-1899). Our contributions include the development of a comprehensive dataset annotated for noise-related segments and their categorization into human-made, non-human-made, and musical noises. This study illustrates the framework's potential for enhancing the understanding of the relationship between noise and its literary representations, providing a deeper appreciation of the auditory elements in literary works, including as sources for cultural history",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Al-Laith",
      "Daniel Hershcovich",
      "Jens Bjerring-Hansen",
      "Jakob Parby",
      "Alexander Conroy",
      "Timothy Tangherlini"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.197": {
    "title": "QUIK: Towards End-to-end 4-Bit Inference on Generative Large Language Models",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) from the GPT family have become extremely popular, leading to a race towards reducing their inference costs to allow for efficient local computation. However, the vast majority of existing work focuses on weight-only quantization, which can reduce runtime costs in the memory-bound one-token-at-a-time generative setting, but does not address costs in compute-bound scenarios, such as batched inference or prompt processing.In this paper, we address the general quantization problem, where both weights and activations should be quantized, which leads to computational improvements in general. We show that the majority of inference computations for large generative models can be performed with both weights and activations being cast to 4 bits, while at the same time maintaining good accuracy. We achieve this via a hybrid quantization strategy called QUIK that compresses most of the weights and activations to 4-bit, while keeping a small fraction of \"outlier\" weights and activations in higher-precision. QUIK is that it is designed with computational efficiency in mind: we provide GPU kernels matching the QUIK format with highly-efficient layer-wise runtimes, which lead to practical end-to-end throughput improvements of up to 3.4x relative to FP16 execution. We provide detailed studies for models from the OPT, LLaMA-2 and Falcon families, as well as a first instance of accurate inference using quantization plus 2:4 sparsity.Anonymized code is available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saleh Ashkboos",
      "Ilia Markov",
      "Elias Frantar",
      "Tingxuan Zhong",
      "Xincheng Wang",
      "Jie Ren",
      "Torsten Hoefler",
      "Dan Alistarh"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.198": {
    "title": "Fine-Grained Prediction of Reading Comprehension from Eye Movements",
    "volume": "main",
    "abstract": "Can human reading comprehension be assessed from eye movements in reading? In this work, we address this longstanding question using large-scale eyetracking data. We focus on a cardinal and largely unaddressed variant of this question: predicting reading comprehension of a single participant for a single question from their eye movements over a single paragraph. We tackle this task using a battery of recent models from the literature, and three new multimodal language models. We evaluate the models in two different reading regimes: ordinary reading and information seeking, and examine their generalization to new textual items, new participants, and the combination of both. The evaluations suggest that the task is highly challenging, and highlight the importance of benchmarking against a strong text-only baseline. While in some cases eye movements provide improvements over such a baseline, they tend to be small. This could be due to limitations of current modelling approaches, limitations of the data, or because eye movement behavior does not sufficiently pertain to fine-grained aspects of reading comprehension processes. Our study provides an infrastructure for making further progress on this question",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Omer Shubi",
      "Yoav Meiri",
      "Cfir Hadar",
      "Yevgeni Berzak"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.199": {
    "title": "EfficientRAG: Efficient Retriever for Multi-Hop Question Answering",
    "volume": "main",
    "abstract": "Retrieval-augmented generation (RAG) methods encounter difficulties when addressing complex questions like multi-hop queries.While iterative retrieval methods improve performance by gathering additional information, current approaches often rely on multiple calls of large language models (LLMs).In this paper, we introduce EfficientRAG, an efficient retriever for multi-hop question answering.EfficientRAG iteratively generates new queries without the need for LLM calls at each iteration and filters out irrelevant information.Experimental results demonstrate that EfficientRAG surpasses existing RAG methods on three open-domain multi-hop question-answering datasets.The code is available in [aka.ms/efficientrag](https://github.com/NIL-zhuang/EfficientRAG-official)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyuan Zhuang",
      "Zhiyang Zhang",
      "Sitao Cheng",
      "Fangkai Yang",
      "Jia Liu",
      "Shujian Huang",
      "Qingwei Lin",
      "Saravan Rajmohan",
      "Dongmei Zhang",
      "Qi Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.200": {
    "title": "Unsupervised Human Preference Learning",
    "volume": "main",
    "abstract": "Large language models demonstrate impressive reasoning abilities but struggle to provide personalized content due to their lack of individual user preference information. Existing methods, such as in-context learning and parameter-efficient fine-tuning, fall short in capturing the complexity of human preferences, especially given the small, personal datasets individuals possess. In this paper, we propose a novel approach utilizing small parameter models as preference agents to generate natural language rules that guide a larger, pre-trained model, enabling efficient personalization. Our method involves a small, local \"steering wheel\" model that directs the outputs of a much larger foundation model, producing content tailored to an individual's preferences while leveraging the extensive knowledge and capabilities of the large model. Importantly, this personalization is achieved without the need to fine-tune the large model. Experimental results on email and article datasets, demonstrate that our technique significantly outperforms baseline personalization methods. By allowing foundation models to adapt to individual preferences in a data and compute-efficient manner, our approach paves the way for highly personalized language model applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sumuk Shashidhar",
      "Abhinav Chinta",
      "Vaibhav Sahai",
      "Dilek Tur"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.201": {
    "title": "Is Safer Better? The Impact of Guardrails on the Argumentative Strength of LLMs in Hate Speech Countering",
    "volume": "main",
    "abstract": "The potential effectiveness of counterspeech as a hate speech mitigation strategy is attracting increasing interest in the NLG research community, particularly towards the task of automatically producing it. However, automatically generated responses often lack the argumentative richness which characterises expert-produced counterspeech. In this work, we focus on two aspects of counterspeech generation to produce more cogent responses. First, by investigating the tension between helpfulness and harmlessness of LLMs, we test whether the presence of safety guardrails hinders the quality of the generations. Secondly, we assess whether attacking a specific component of the hate speech results in a more effective argumentative strategy to fight online hate. By conducting an extensive human and automatic evaluation, we show how the presence of safety guardrails can be detrimental also to a task that inherently aims at fostering positive social interactions. Moreover, our results show that attacking a specific component of the hate speech, and in particular its implicit negative stereotype and its hateful parts, leads to higher-quality generations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Helena Bonaldi",
      "Greta Damo",
      "Nicolás Ocampo",
      "Elena Cabrio",
      "Serena Villata",
      "Marco Guerini"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.202": {
    "title": "Leading Whitespaces of Language Models' Subword Vocabulary Pose a Confound for Calculating Word Probabilities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Byung-Doh Oh",
      "William Schuler"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.203": {
    "title": "LLM4Decompile: Decompiling Binary Code with Large Language Models",
    "volume": "main",
    "abstract": "Decompilation aims to convert binary code to high-level source code, but traditional tools like Ghidra often produce results that are difficult to read and execute. Motivated by the advancements in Large Language Models (LLMs), we propose LLM4Decompile, the first and largest open-source LLM series (1.3B to 33B) trained to decompile binary code. We optimize the LLM training process and introduce the LLM4Decompile-End models to decompile binary directly. The resulting models significantly outperform GPT-4o and Ghidra on the HumanEval and ExeBench benchmarks by over 100% in terms of re-executability rate. Additionally, we improve the standard refinement approach to fine-tune the LLM4Decompile-Ref models, enabling them to effectively refine the decompiled code from Ghidra and achieve a further 16.2% improvement over the LLM4Decompile-End. LLM4Decompile demonstrates the potential of LLMs to revolutionize binary code decompilation, delivering remarkable improvements in readability and executability while complementing conventional tools for optimal results",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanzhuo Tan",
      "Qi Luo",
      "Jing Li",
      "Yuqun Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.204": {
    "title": "From Bottom to Top: Extending the Potential of Parameter Efficient Fine-Tuning",
    "volume": "main",
    "abstract": "With the proliferation of large language models, Parameter Efficient Fine-Tuning (PEFT) method, which freeze pre-trained parameters and only fine-tune a few task-specific parameters, are playing an increasingly important role. However, previous work primarily applied uniform operations across all layers of the model, overlooking the fact that different layers in a transformer store different information. In the process of exploration, We find that there is a significant differences in fine-tuning strategies between different layers, and fine-tuning only a subset of layers can even achieve comparable performance. Based on this, we propose the Hybrid LoRA-Prefix Tuning(HLPT) method, which uses enhanced LoRA and Prefix-tuning methods with learnable adaptive mechanism separately for the bottom and top layers, and the Half Hybrid LoRA-Prefix Tuning(H2LPT) method, which goes a step further, reducing the parameter count to nearly half by omitting fine-tuning in the middle layers. Extensive experiments with large language models on various downstream tasks provide strong evidence for the potential of PEFT focusing on different layers' interactions and the effectiveness of our methods. Furthermore, we validate the robustness of these methods and their advantages in speeding up training convergence, reducing inference time requirements",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jihao Gu",
      "Zelin Wang",
      "Yibo Zhang",
      "Ziji Zhang",
      "Ping Gong"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.205": {
    "title": "CoTKR: Chain-of-Thought Enhanced Knowledge Rewriting for Complex Knowledge Graph Question Answering",
    "volume": "main",
    "abstract": "Recent studies have explored the use of Large Language Models (LLMs) with Retrieval Augmented Generation (RAG) for Knowledge Graph Question Answering (KGQA). They typically require rewriting retrieved subgraphs into natural language formats comprehensible to LLMs. However, when tackling complex questions, the knowledge rewritten by existing methods may include irrelevant information, omit crucial details, or fail to align with the question's semantics. To address them, we propose a novel rewriting method CoTKR, Chain- of-Thought Enhanced Knowledge Rewriting, for generating reasoning traces and corresponding knowledge in an interleaved manner, thereby mitigating the limitations of single-step knowledge rewriting. Additionally, to bridge the preference gap between the knowledge rewriter and the question answering (QA) model, we propose a training strategy PAQAF, Preference Alignment from Question Answering Feedback, for leveraging feedback from the QA model to further optimize the knowledge rewriter. We conduct experiments using various LLMs across several KGQA benchmarks. Experimental results demonstrate that, compared with previous knowledge rewriting methods, CoTKR generates the most beneficial knowledge representation for QA models, which significantly improves the performance of LLMs in KGQA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yike Wu",
      "Yi Huang",
      "Nan Hu",
      "Yuncheng Hua",
      "Guilin Qi",
      "Jiaoyan Chen",
      "Jeff Pan"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.206": {
    "title": "MTLS: Making Texts into Linguistic Symbols",
    "volume": "main",
    "abstract": "In linguistics, all languages can be considered as symbolic systems, with each language relying on symbolic processes to associate specific symbols with meanings. In the same language, there is a fixed correspondence between linguistic symbol and meaning. In different languages, universal meanings follow varying rules of symbolization in one-to-one correspondence with symbols. Most work overlooks the properties of languages as symbol systems. In this paper, we shift the focus to the symbolic properties and introduce MTLS: a pre-training method to improve the multilingual capability of models by Making Texts into Linguistic Symbols. Initially, we replace the vocabulary in pre-trained language models by mapping relations between linguistic symbols and semantics. Subsequently, universal semantics within the symbolic system serve as bridges, linking symbols from different languages to the embedding space of the model, thereby enabling the model to process linguistic symbols. To evaluate the effectiveness of MTLS, we conducted experiments on multilingual tasks using BERT and RoBERTa, respectively, as the backbone. The results indicate that despite having just over 12,000 pieces of English data in pre-training, the improvement that MTLS brings to multilingual capabilities is remarkably significant",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenlong Fei",
      "Xiaohua Wang",
      "Min Hu",
      "Qingyu Zhang",
      "Hongbo Li"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.207": {
    "title": "D2R: Dual-Branch Dynamic Routing Network for Multimodal Sentiment Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Chen",
      "Kuntao Li",
      "Weixing Mai",
      "Qiaofeng Wu",
      "Yun Xue",
      "Fenghuan Li"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.208": {
    "title": "A Generic Method for Fine-grained Category Discovery in Natural Language Texts",
    "volume": "main",
    "abstract": "Fine-grained category discovery using only coarse-grained supervision is a cost-effective yet challenging task. Previous training methods focus on aligning query samples with positive samples and distancing them from negatives. They often neglect intra-category and inter-category semantic similarities of fine-grained categories when navigating sample distributions in the embedding space. Furthermore, some evaluation techniques that rely on pre-collected test samples are inadequate for real-time applications. To address these shortcomings, we introduce a method that successfully detects fine-grained clusters of semantically similar texts guided by a novel objective function. The method uses semantic similarities in a logarithmic space to guide sample distributions in the Euclidean space and to form distinct clusters that represent fine-grained categories. We also propose a centroid inference mechanism to support real-time applications. The efficacy of the method is both theoretically justified and empirically confirmed on three benchmark tasks. The proposed objective function is integrated in multiple contrastive learning based neural models. Its results surpass existing state-of-the-art approaches in terms of Accuracy, Adjusted Rand Index and Normalized Mutual Information of the detected fine-grained categories. Code and data are publicly available at https://github.com/changtianluckyforever/F-grained-STAR",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chang Tian",
      "Matthew Blaschko",
      "Wenpeng Yin",
      "Mingzhe Xing",
      "Yinliang Yue",
      "Marie-Francine Moens"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.209": {
    "title": "Toxicity Detection is NOT all you Need: Measuring the Gaps to Supporting Volunteer Content Moderators through a User-Centric Method",
    "volume": "main",
    "abstract": "Extensive efforts in automated approaches for content moderation have been focused on developing models to identify toxic, offensive, and hateful content with the aim of lightening the load for moderators. Yet, it remains uncertain whether improvements on those tasks have truly addressed moderators' needs in accomplishing their work. In this paper, we surface gaps between past research efforts that have aimed to provide automation for aspects of content moderation and the needs of volunteer content moderators, regarding identifying violations of various moderation rules. To do so, we conduct a model review on Hugging Face to reveal the availability of models to cover various moderation rules and guidelines from three exemplar forums. We further put state-of-the-art LLMs to the test, evaluating how well these models perform in flagging violations of platform rules from one particular forum. Finally, we conduct a user survey study with volunteer moderators to gain insight into their perspectives on useful moderation models. Overall, we observe a non trivial gap, as missing developed models and LLMs exhibit moderate to low performance on a significant portion of the rules. Moderators' reports provide guides for future work on developing moderation assistant models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Cao",
      "Lovely-Frances Domingo",
      "Sarah Gilbert",
      "Michelle Mazurek",
      "Katie Shilton",
      "Hal Daumé Iii"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.210": {
    "title": "A User-Centric Multi-Intent Benchmark for Evaluating Large Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) are essential tools that users employ across various scenarios, so evaluating their performance and guiding users in selecting the suitable service is important. Although many benchmarks exist, they mainly focus on specific predefined model abilities, such as world knowledge, reasoning, etc. Based on these ability scores, it is hard for users to determine which LLM best suits their particular needs. To address these issues, we propose to evaluate LLMs from a user-centric perspective and design this benchmark to measure their efficacy in satisfying user needs under distinct intents. Firstly, we collect 1,846 real-world use cases from a user study with 712 participants from 23 countries. This first-hand data helps us understand actual user intents and needs in LLM interactions, forming the User Reported Scenarios (URS) dataset, which is categorized with six types of user intents. Secondly, based on this authentic dataset, we benchmark 10 LLM services with GPT-4-as-Judge. Thirdly, we show that benchmark scores align well with human preference in both real-world experience and pair-wise annotations, achieving Pearson correlations of 0.95 and 0.94, respectively. This alignment confirms that the URS dataset and our evaluation method establish an effective user-centric benchmark. The dataset, code, and process data are publicly available at https://github.com/Alice1998/URS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayin Wang",
      "Fengran Mo",
      "Weizhi Ma",
      "Peijie Sun",
      "Min Zhang",
      "Jian-Yun Nie"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.211": {
    "title": "Decompose and Compare Consistency: Measuring VLMs' Answer Reliability via Task-Decomposition Consistency Comparison",
    "volume": "main",
    "abstract": "Despite tremendous advancements, current state-of-the-art Vision-Language Models (VLMs) are still far from perfect. They tend to hallucinate and may generate biased responses. In such circumstances, having a way to assess the reliability of a given response generated by a VLM is quite useful. Existing methods, such as estimating uncertainty using answer likelihoods or prompt-based confidence generation, often suffer from overconfidence. Other methods use self-consistency comparison but are affected by confirmation biases. To alleviate these, we propose Decompose and Compare Consistency (DeCC) for reliability measurement. By comparing the consistency between the direct answer generated using the VLM's internal reasoning process, and the indirect answers obtained by decomposing the question into sub-questions and reasoning over the sub-answers produced by the VLM, DeCC measures the reliability of VLM's direct answer. Experiments across six vision-language tasks with three VLMs show DeCC's reliability estimation achieves better correlation with task accuracy compared to the existing methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qian Yang",
      "Weixiang Yan",
      "Aishwarya Agrawal"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.212": {
    "title": "Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism",
    "volume": "main",
    "abstract": "Large language models (LLMs) have demonstrated impressive language understanding and generation capabilities, enabling them to answer a wide range of questions across various domains. However, these models are not flawless and often produce responses that contain errors or misinformation. These inaccuracies, commonly referred to as hallucinations, render LLMs unreliable and even unusable in many scenarios. In this paper, our focus is on mitigating the issue of hallucination in LLMs, particularly in the context of question-answering. Instead of attempting to answer all questions, we explore a refusal mechanism that instructs LLMs to refuse to answer challenging questions in order to avoid errors. We then propose a simple yet effective solution called Learn to Refuse (L2R), which incorporates the refusal mechanism to enable LLMs to recognize and refuse to answer questions that they find difficult to address. To achieve this, we utilize a structured knowledge base to represent all the LLM's understanding of the world, enabling it to provide traceable gold knowledge. This knowledge base is separate from the LLM and initially empty. It can be filled with validated knowledge and progressively expanded. When an LLM encounters questions outside its domain, the system recognizes its knowledge scope and determines whether it can answer the question independently. Additionally, we introduce a method for automatically and efficiently expanding the knowledge base of LLMs. Through qualitative and quantitative analysis, we demonstrate that our approach enhances the controllability and reliability of LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lang Cao"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.213": {
    "title": "VGBench: A Comprehensive Benchmark of Vector Graphics Understanding and Generation for Large Language Models",
    "volume": "main",
    "abstract": "In the realm of vision models, the primary mode of representation is using pixels to rasterize the visual world. Yet this is not always the best or unique way to represent visual content, especially for designers and artists who depict the world using geometry primitives such as polygons. Vector graphics (VG), on the other hand, offer a textual representation of visual content, which can be more concise and powerful for content like cartoons, sketches and scientific figures. Recent studies have shown promising results on processing vector graphics with capable Large Language Models (LLMs). However, such works focus solely on qualitative results, understanding, or a specific type of vector graphics. We propose VGBench, a comprehensive benchmark for LLMs on handling vector graphics through diverse aspects, including (a) both visual understanding and generation, (b) evaluation of various vector graphics formats, (c) diverse question types, (d) wide range of prompting techniques, (e) under multiple LLMs and (f) comparison with VLMs on rasterized representations. Evaluating on our collected 4279 understanding and 5845 generation samples, we find that LLMs show strong capability on both aspects while exhibiting less desirable performance on low-level formats (SVG). Both data and evaluation pipeline will be open-sourced",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bocheng Zou",
      "Mu Cai",
      "Jianrui Zhang",
      "Yong Jae Lee"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.214": {
    "title": "What do Large Language Models Need for Machine Translation Evaluation?",
    "volume": "main",
    "abstract": "Leveraging large language models (LLMs) for various natural language processing tasks has led to superlative claims about their performance. For the evaluation of machine translation (MT), existing research shows that LLMs are able to achieve results comparable to fine-tuned multilingual pre-trained language models. In this paper, we explore what translation information, such as the source, reference, translation errors and annotation guidelines, is needed for LLMs to evaluate MT quality. In addition, we investigate prompting techniques such as zero-shot, Chain of Thought (CoT) and few-shot prompting for eight language pairs covering high-, medium- and low-resource languages, leveraging varying LLM variants. Our findings indicate the importance of reference translations for an LLM-based evaluation. While larger models do not necessarily fare better, they tend to benefit more from CoT prompting, than smaller models. We also observe that LLMs do not always provide a numerical score when generating evaluations, which poses a question on their reliability for the task. Our work presents a comprehensive analysis for resource-constrained and training-less LLM-based evaluation of machine translation. We release the accrued prompt templates, code and data publicly for reproducibility",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shenbin Qian",
      "Archchana Sindhujan",
      "Minnie Kabra",
      "Diptesh Kanojia",
      "Constantin Orasan",
      "Tharindu Ranasinghe",
      "Fred Blain"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.215": {
    "title": "Performance-Guided LLM Knowledge Distillation for Efficient Text Classification at Scale",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) face significant challenges at inference time due to their high computational demands. To address this, we present Performance-Guided Knowledge Distillation (PGKD), a cost-effective and high-throughput solution for production text classification applications. PGKD utilizes teacher-student Knowledge Distillation to distill the knowledge of LLMs into smaller, task-specific models. PGKD establishes an active learning routine between the student model and the LLM; the LLM continuously generates new training data leveraging hard-negative mining, student model validation performance, and early-stopping protocols to inform the data generation. By employing a cyclical, performance-aware approach tailored for highly multi-class, sparsely annotated datasets prevalent in industrial text classification, PGKD effectively addresses training challenges and outperforms traditional BERT-base models and other knowledge distillation methods on several multi-class classification datasets. Additionally, cost and latency benchmarking reveals that models fine-tuned with PGKD are up to 130X faster and 25X less expensive than LLMs for inference on the same classification task. While PGKD is showcased for text classification tasks, its versatile framework can be extended to any LLM distillation task, including language generation, making it a powerful tool for optimizing performance across a wide range of AI applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Flavio Palo",
      "Prateek Singhi",
      "Bilal Fadlallah"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.216": {
    "title": "External Knowledge-Driven Argument Mining: Leveraging Attention-Enhanced Multi-Network Models",
    "volume": "main",
    "abstract": "Argument mining (AM) involves the identification of argument relations (AR) between Argumentative Discourse Units (ADUs). The essence of ARs among ADUs is context-dependent and lies in maintaining a coherent flow of ideas, often centered around the relations between discussed entities, topics, themes or concepts. However, these relations are not always explicitly stated; rather, inferred from implicit chains of reasoning connecting the concepts addressed in the ADUs. While humans can infer such background knowledge, machines face challenges when the contextual cues are not explicitly provided. This paper leverages external resources, including WordNet, ConceptNet, and Wikipedia to identify semantic paths (knowledge paths) connecting the concepts discussed in the ADUs to obtain the implicit chains of reasoning. To effectively leverage these paths for AR prediction, we propose attention-based Multi-Network architectures. Various architecture are evaluated on the external resources, and the Wikipedia based configuration attains F-scores of 0.85, 0.84, 0.70, and 0.87, respectively, on four diverse datasets, showing strong performance over the baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Debela Gemechu",
      "Chris Reed"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.217": {
    "title": "C3PA: An Open Dataset of Expert-Annotated and Regulation-Aware Privacy Policies to Enable Scalable Regulatory Compliance Audits",
    "volume": "main",
    "abstract": "The development of tools and techniques to analyze and extract organizations' data habits from privacy policies are critical for scalable regulatory compliance audits. Unfortunately, these tools are becoming increasingly limited in their ability to identify compliance issues and fixes. After all, most were developed using regulation-agnostic datasets of annotated privacy policies obtained from a time before the introduction of landmark privacy regulations such as EU's GDPR and California's CCPA. In this paper, we describe the first open regulation-aware dataset of expert-annotated privacy policies, C3PA (CCPA Privacy Policy Provision Annotations), aimed to address this challenge. C3PA contains over 48K expert-labeled privacy policy text segments associated with responses to CCPA-specific disclosure mandates from 411 unique organizations. We demonstrate that the C3PA dataset is uniquely suited for aiding automated audits of compliance with CCPA-related disclosure mandates",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maaz Musa",
      "Steven Winston",
      "Garrison Allen",
      "Jacob Schiller",
      "Kevin Moore",
      "Sean Quick",
      "Johnathan Melvin",
      "Padmini Srinivasan",
      "Mihailis Diamantis",
      "Rishab Nithyanand"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.218": {
    "title": "M2PT: Multimodal Prompt Tuning for Zero-shot Instruction Learning",
    "volume": "main",
    "abstract": "Multimodal Large Language Models (MLLMs) demonstrate remarkable performance across a wide range of domains, with increasing emphasis on enhancing their zero-shot generalization capabilities for unseen tasks across various modalities. Instruction tuning has emerged as an effective strategy for achieving zero-shot generalization by finetuning pretrained models on diverse multimodal tasks. As the scale of MLLMs continues to grow, parameter-efficient finetuning becomes increasingly critical. However, most existing parameter-efficient approaches focus only on single modalities and often overlook the multimodal characteristics during finetuning. In this work, we introduce a novel Multimodal Prompt Tuning (M2PT) approach for efficient instruction tuning of MLLMs. M2PT effectively integrates visual and textual prompts into the vision encoder and language processor respectively during finetuning, facilitating the extraction and alignment of features across modalities. Empirical results on various multimodal evaluation datasets demonstrate the superior performance of our approach compared to several state-of-the-art baselines. A comprehensive set of ablation studies validates the effectiveness of our prompt design and the efficiency of our approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taowen Wang",
      "Yiyang Liu",
      "James Liang",
      "Junhan Zhao",
      "Yiming Cui",
      "Yuning Mao",
      "Shaoliang Nie",
      "Jiahao Liu",
      "Fuli Feng",
      "Zenglin Xu",
      "Cheng Han",
      "Lifu Huang",
      "Qifan Wang",
      "Dongfang Liu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.219": {
    "title": "Text Grafting: Near-Distribution Weak Supervision for Minority Classes in Text Classification",
    "volume": "main",
    "abstract": "For extremely weak-supervised text classification, pioneer research generates pseudo labels by mining texts similar to the class names from the raw corpus, which may end up with very limited or even no samples for the minority classes. Recent works have started to generate the relevant texts by prompting LLMs using the class names or definitions; however, there is a high risk that LLMs cannot generate in-distribution (i.e., similar to the corpus where the text classifier will be applied) data, leading to ungeneralizable classifiers. In this paper, we combine the advantages of these two approaches and propose to bridge the gap via a novel framework, text grafting, which aims to obtain clean and near-distribution weak supervision for minority classes. Specifically, we first use LLM-based logits to mine masked templates from the raw corpus, which have a high potential for data synthesis into the target minority class. Then, the templates are filled by state-of-the-art LLMs to synthesize near-distribution texts falling into minority classes. Text grafting shows significant improvement over direct mining or synthesis on minority classes. We also use analysis and case studies to comprehend the property of text grafting",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Letian Peng",
      "Yi Gu",
      "Chengyu Dong",
      "Zihan Wang",
      "Jingbo Shang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.220": {
    "title": "Incubating Text Classifiers Following User Instruction with Nothing but LLM",
    "volume": "main",
    "abstract": "In this paper, we aim to generate text classification data given arbitrary class definitions (i.e., user instruction), so one can train a text classifier without any human annotation or raw corpus. Recent advances in large language models (LLMs) lead to pioneer attempts to individually generate texts for each class via prompting. In this paper, we propose Incubator, the first framework that can handle complicated and even mutually dependent classes (e.g., \"TED Talk given by Educator\" and \"Other\"). Specifically, our Incubator is a fine-tuned LLM that takes the instruction of all class definitions as input, and in each inference, it can jointly generate one sample for every class. First, we tune Incubator on the instruction-to-data mappings that we obtained from classification datasets and descriptions on Hugging Face together with in-context augmentation by GPT-4. To emphasize the uniformity and diversity in generations, we refine Incubator by fine-tuning with the cluster centers of semantic textual embeddings of the generated samples. We compare Incubator on various classification tasks with strong baselines such as direct LLM-based inference and training data generation by prompt engineering. Experiments show Incubator is able to (1) outperform previous methods on traditional benchmarks, (2) take label interdependency and user preference into consideration, and (3) enable logical text mining by incubating multiple classifiers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Letian Peng",
      "Zilong Wang",
      "Jingbo Shang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.221": {
    "title": "PTD-SQL: Partitioning and Targeted Drilling with LLMs in Text-to-SQL",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have emerged as powerful tools for Text-to-SQL tasks, exhibiting remarkable reasoning capabilities. Different from tasks such as math word problem and commonsense reasoning, SQL solutions have a relatively fixed pattern. This facilitates the investigation of whether LLMs can benefit from categorical thinking, mirroring how humans acquire knowledge through inductive reasoning based on comparable examples. In this study, we propose that employing query group partitioning allows LLMs to focus on learning the thought processes specific to a single problem type, consequently enhancing their reasoning abilities across diverse difficulty levels and problem categories. Our experiments reveal that multiple advanced LLMs, when equipped with PTD-SQL, can either surpass or match previous state-of-the-art (SOTA) methods on the Spider and BIRD datasets. Intriguingly, models with varying initial performances have exhibited significant improvements mainly at the boundary of their capabilities after targeted drilling, suggesting a parallel with human progress. Code is available at https://github.com/lrlbbzl/PTD-SQL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruilin Luo",
      "Liyuan Wang",
      "Binghuai Lin",
      "Zicheng Lin",
      "Yujiu Yang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.222": {
    "title": "Conditional and Modal Reasoning in Large Language Models",
    "volume": "main",
    "abstract": "The reasoning abilities of large language models (LLMs) are the topic of a growing body of research in AI and cognitive science. In this paper, we probe the extent to which twenty-nine LLMs are able to distinguish logically correct inferences from logically fallacious ones. We focus on inference patterns involving conditionals (e.g., '*If* Ann has a queen, *then* Bob has a jack') and epistemic modals (e.g., ‘Ann *might* have an ace', ‘Bob *must* have a king'). These inferences have been of special interest to logicians, philosophers, and linguists, since they play a central role in the fundamental human ability to reason about distal possibilities. Assessing LLMs on these inferences is thus highly relevant to the question of how much the reasoning abilities of LLMs match those of humans. All the LLMs we tested make some basic mistakes with conditionals or modals, though zero-shot chain-of-thought prompting helps them make fewer mistakes. Even the best performing LLMs make basic errors in modal reasoning, display logically inconsistent judgments across inference patterns involving epistemic modals and conditionals, and give answers about complex conditional inferences that do not match reported human judgments. These results highlight gaps in basic logical reasoning in today's LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wesley Holliday",
      "Matthew Mandelkern",
      "Cedegao Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.223": {
    "title": "Advancing Large Language Model Attribution through Self-Improving",
    "volume": "main",
    "abstract": "Teaching large language models (LLMs) to generate text with citations to evidence sources can mitigate hallucinations and enhance verifiability in information-seeking systems. However, improving this capability requires high-quality attribution data, which is costly and labor-intensive. Inspired by recent advances in self-improvement that enhance LLMs without manual annotation, we present START, a Self-Taught AttRibuTion framework for iteratively improving the attribution capability of LLMs. First, to prevent models from stagnating due to initially insufficient supervision signals, START leverages the model to self-construct synthetic training data for warming up. To further self-improve the model's attribution ability, START iteratively utilizes fine-grained preference supervision signals constructed from its sampled responses to encourage robust, comprehensive, and attributable generation. Experiments on three open-domain question-answering datasets, covering long-form QA and multi-step reasoning, demonstrate significant performance gains of 25.13% on average without relying on human annotations and more advanced models. Further analysis reveals that START excels in aggregating information across multiple sources",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Huang",
      "Xiaocheng Feng",
      "Weitao Ma",
      "Liang Zhao",
      "Yuchun Fan",
      "Weihong Zhong",
      "Dongliang Xu",
      "Qing Yang",
      "Hongtao Liu",
      "Bing Qin"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.224": {
    "title": "AlignCap: Aligning Speech Emotion Captioning to Human Preferences",
    "volume": "main",
    "abstract": "Speech Emotion Captioning (SEC) has gradually become an active research task. The emotional content conveyed through human speech are often complex, and classifying them into fixed categories may not be enough to fully capture speech emotions. Describing speech emotions through natural language may be a more effective approach. However, existing SEC methods often produce hallucinations and lose generalization on unseen speech. To overcome these problems, we propose AlignCap, which Aligning Speech Emotion Captioning to Human Preferences based on large language model (LLM) with two properties: 1) Speech-Text Alignment, which minimizing the divergence between the LLM's response prediction distributions for speech and text inputs using knowledge distillation (KD) Regularization. 2) Human Preference Alignment, where we design Preference Optimization (PO) Regularization to eliminate factuality and faithfulness hallucinations. We also extract emotional clues as a prompt for enriching fine-grained information under KD-Regularization. Experiments demonstrate that AlignCap presents stronger performance to other state-of-the-art methods on Zero-shot SEC task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziqi Liang",
      "Haoxiang Shi",
      "Hanhui Chen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.225": {
    "title": "Interpretability-based Tailored Knowledge Editing in Transformers",
    "volume": "main",
    "abstract": "Language models recognized as a new form of knowledge bases, face challenges of outdated, erroneous, and privacy-sensitive information, necessitating knowledge editing to rectify errors without costly retraining. Existing methods, spanning model's parameters modification, external knowledge integration, and in-context learning, lack in-depth analysis from a model interpretability perspective. Our work explores the instability in in-context learning outcomes, providing insights into its reasons and distinctions from other methods. Leveraging findings on the critical role of feed-forward MLPs in decoder-only models, we propose a tailored knowledge editing method, TailoredKE, that considers the unique information flow of each sample. Model interpretability reveals diverse attribute recall across transformer layers, guiding edits to specific features at different depths and mitigating over-editing issues",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihuai Hong",
      "Aldo Lipani"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.226": {
    "title": "PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Heuristic-based Sampling",
    "volume": "main",
    "abstract": "Prompt optimization aims to find the best prompt to a large language model (LLM) for a given task. LLMs have been successfully used to help find and improve prompt candidates for single-step tasks. However, realistic tasks for agents are multi-step and introduce new challenges: (1) Prompt content is likely to be more extensive and complex, making it more difficult for LLMs to analyze errors, (2) the impact of an individual step is difficult to evaluate, and (3) different people may have varied preferences about task execution. While humans struggle to optimize prompts, they are good at providing feedback about LLM outputs; we therefore introduce a new LLM-driven discrete prompt optimization framework PROMST that incorporates human-designed feedback rules to automatically offer direct suggestions for improvement. We also use an extra learned heuristic model that predicts prompt performance to efficiently sample from prompt candidates. This approach significantly outperforms both human-engineered prompts and several other prompt optimization methods across 11 representative multi-step tasks (an average 10.6%-29.3% improvement to current best methods on five LLMs respectively). We believe our work can serve as a benchmark for automatic prompt optimization for LLM-driven multi-step tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongchao Chen",
      "Jacob Arkin",
      "Yilun Hao",
      "Yang Zhang",
      "Nicholas Roy",
      "Chuchu Fan"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.227": {
    "title": "Empowering Large Language Model for Continual Video Question Answering with Collaborative Prompting",
    "volume": "main",
    "abstract": "In recent years, the rapid increase in online video content has underscored the limitations of static Video Question Answering (VideoQA) models trained on fixed datasets, as they struggle to adapt to new questions or tasks posed by newly available content. In this paper, we explore the novel challenge of VideoQA within a continual learning framework, and empirically identify a critical issue: fine-tuning a large language model (LLM) for a sequence of tasks often results in catastrophic forgetting. To address this, we propose Collaborative Prompting (ColPro), which integrates specific question constraint prompting, knowledge acquisition prompting, and visual temporal awareness prompting. These prompts aim to capture textual question context, visual content, and video temporal dynamics in VideoQA, a perspective underexplored in prior research. Experimental results on the NExT-QA and DramaQA datasets show that ColPro achieves superior performance compared to existing approaches, achieving 55.14% accuracy on NExT-QA and 71.24% accuracy on DramaQA, highlighting its practical relevance and effectiveness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Cai",
      "Zheng Wang",
      "Jianjun Gao",
      "Wenyang Liu",
      "Ye Lu",
      "Runzhong Zhang",
      "Kim-Hui Yap"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.228": {
    "title": "Dissecting Fine-Tuning Unlearning in Large Language Models",
    "volume": "main",
    "abstract": "Fine-tuning-based unlearning methods prevail for erasing targeted harmful, sensitive, or copyrighted information within large language models while preserving overall capabilities. However, the true effectiveness of the methods is unclear. In this paper, we delve into the limitations of fine-tuning-based unlearning through activation patching and parameter restoration experiments. Our findings reveal that these methods alter the model's knowledge retrieval process, rather than genuinely erasing the problematic knowledge embedded in the model parameters. Furthermore, behavioral tests demonstrate that the unlearning mechanisms inevitably impact the global behavior of the models, affecting unrelated knowledge or capabilities. Our work advocates the development of more resilient unlearning techniques for truly erasing knowledge",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihuai Hong",
      "Yuelin Zou",
      "Lijie Hu",
      "Ziqian Zeng",
      "Di Wang",
      "Haiqin Yang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.229": {
    "title": "Dancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models",
    "volume": "main",
    "abstract": "Modern language models (LMs) need to follow human instructions while being faithful; yet, they often fail to achieve both. Here, we provide concrete evidence of a trade-off between instruction following (i.e., follow open-ended instructions) and faithfulness (i.e., ground responses in given context) when training LMs with these objectives. For instance, fine-tuning LLaMA-7B on instruction following datasets renders it less faithful. Conversely, instruction-tuned Vicuna-7B shows degraded performance at following instructions when further optimized on tasks that require contextual grounding. One common remedy is multi-task learning (MTL) with data mixing, yet it remains far from achieving a synergic outcome. We propose a simple yet effective method that relies on Reject-sampling by Self-instruct with Continued Fine-tuning (ReSet), which significantly outperforms vanilla MTL. Surprisingly, we find that less is more, as training ReSet with high-quality, yet substantially smaller data (three-fold less) yields superior results. Our findings offer a better understanding of objective discrepancies in alignment training of LMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengxuan Wu",
      "Yuhao Zhang",
      "Peng Qi",
      "Yumo Xu",
      "Rujun Han",
      "Yian Zhang",
      "Jifan Chen",
      "Bonan Min",
      "Zhiheng Huang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.230": {
    "title": "Where is the signal in tokenization space?",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) are typically shipped with tokenizers that *deterministically* encode text into so-called *canonical* token sequences, to which the LLMs assign probability values.One common assumption is that the probability of a piece of text is the probability of its canonical token sequence.However, the tokenization of a string is not unique: e.g., the Llama2 tokenizer encodes ‘Tokens‘ as ‘[Tok,ens]‘, but ‘[Tok,en,s]‘ also represents the same text.In this paper, we study non-canonical tokenizations.We prove that, given a string, it is computationally hard to find the most likely tokenization for an autoregressive LLM, as well as to compute the marginal probability over all possible tokenizations.We then show how the marginal is, in most cases, indistinguishable from the canonical probability.Surprisingly, we then empirically demonstrate the existence of a significant amount of signal hidden within tokenization space.Notably, by simply aggregating the probabilities of non-canonical tokenizations, we achieve improvements across a range of LLM evaluation benchmarks for a variety of architectures, including transformers and state space models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Renato Geh",
      "Honghua Zhang",
      "Kareem Ahmed",
      "Benjie Wang",
      "Guy Van Den Broeck"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.231": {
    "title": "Private Language Models via Truncated Laplacian Mechanism",
    "volume": "main",
    "abstract": "Recently it has been shown that deep learning models for NLP tasks are prone to attacks that can even reconstruct the verbatim training texts. To prevent privacy leakage, researchers have investigated word-level perturbations, relying on the formal guarantees of differential privacy (DP) in the embedding space. However, many existing approaches either achieve unsatisfactory performance in the high privacy regime when using the Laplacian or Gaussian mechanism, or resort to weaker relaxations of DP that are inferior to the canonical DP in terms of privacy strength. This raises the question of whether a new method for private word embedding can be designed to overcome these limitations. In this paper, we propose a novel private embedding method called the high dimensional truncated Laplacian mechanism. Specifically, we introduce a non-trivial extension of the truncated Laplacian mechanism, which was previously only investigated in one-dimensional space cases. Theoretically, we show that our method has a lower variance compared to the previous private word embedding methods. To further validate its effectiveness, we conduct comprehensive experiments on private embedding and downstream tasks using three datasets. Remarkably, even in the high privacy regime, our approach only incurs a slight decrease in utility compared to the non-private scenario",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianhao Huang",
      "Tao Yang",
      "Ivan Habernal",
      "Lijie Hu",
      "Di Wang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.232": {
    "title": "Estimating Knowledge in Large Language Models Without Generating a Single Token",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniela Gottesman",
      "Mor Geva"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.233": {
    "title": "Consistent Autoformalization for Constructing Mathematical Libraries",
    "volume": "main",
    "abstract": "Autoformalization is the task of automatically translating mathematical content written in natural language to a formal language expression. The growing language interpretation capabilities of Large Language Models (LLMs), including in formal languages, are lowering the barriers for autoformalization. However, LLMs alone are not capable of consistently and reliably delivering autoformalization, in particular as the complexity and specialization of the target domain grows. As the field evolves into the direction of systematically applying autoformalization towards large mathematical libraries, the need to improve syntactic, terminological and semantic control increases. This paper proposes the coordinated use of three mechanisms, most-similar retrieval augmented generation (MS-RAG), denoising steps, and auto-correction with syntax error feedback (Auto-SEF) to improve autoformalization quality. The empirical analysis, across different models, demonstrates that these mechanisms can deliver autoformalizaton results which are syntactically, terminologically and semantically more consistent. These mechanisms can be applied across different LLMs and have shown to deliver improve results across different model types",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lan Zhang",
      "Xin Quan",
      "Andre Freitas"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.234": {
    "title": "When Context Leads but Parametric Memory Follows in Large Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) have demonstrated remarkable progress in leveraging diverse knowledge sources. This study investigates how nine widely used LLMs allocate knowledge between local context and global parameters when answering open-ended questions in knowledge-consistent scenarios. We introduce a novel dataset, WikiAtomic, and systematically vary context sizes to analyze how LLMs prioritize and utilize the provided information and their parametric knowledge in knowledge-consistent scenarios. Additionally, we also study their tendency to hallucinate under varying context sizes. Our findings reveal consistent patterns across models, including a consistent reliance on both contextual (around 70%) and parametric (around 30%) knowledge, and a decrease in hallucinations with increasing context. These insights highlight the importance of more effective context organization and developing models that use input more deterministically for robust performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufei Tao",
      "Adam Hiatt",
      "Erik Haake",
      "Antonie Jetter",
      "Ameeta Agrawal"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.235": {
    "title": "Semantic Training Signals Promote Hierarchical Syntactic Generalization in Transformers",
    "volume": "main",
    "abstract": "Neural networks without hierarchical biases often struggle to learn linguistic rules that come naturally to humans. However, neural networks are trained primarily on form alone, while children acquiring language additionally receive data about meaning. Would neural networks generalize more like humans when trained on both form and meaning? We investigate this by examining if Transformers—neural networks without a hierarchical bias—better achieve hierarchical generalization when trained on both form and meaning compared to when trained on form alone. Our results show that Transformers trained on form and meaning do favor the hierarchical generalization more than those trained on form alone, suggesting that statistical learners without hierarchical biases can leverage semantic training signals to bootstrap hierarchical syntactic generalization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya Yedetore",
      "Najoung Kim"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.236": {
    "title": "When Is Multilinguality a Curse? Language Modeling for 250 High- and Low-Resource Languages",
    "volume": "main",
    "abstract": "Multilingual language models are widely used to extend NLP systems to low-resource languages. However, concrete evidence for the effects of multilinguality on language modeling performance in individual languages remains scarce. Here, we pre-train over 10,000 monolingual and multilingual language models for over 250 languages, including multiple language families that are under-studied in NLP. We assess how language modeling performance in each language varies as a function of (1) monolingual dataset size, (2) added multilingual dataset size, (3) linguistic similarity of the added languages, and (4) model size (up to 45M parameters). We find that in moderation, adding multilingual data improves low-resource language modeling performance, similar to increasing low-resource dataset sizes by up to 33%. Improvements depend on the syntactic similarity of the added multilingual data, with marginal additional effects of vocabulary overlap. However, high-resource languages consistently perform worse in multilingual pre-training scenarios. As dataset sizes increase, adding multilingual data begins to hurt performance for both low-resource and high-resource languages, likely due to limited model capacity (the \"curse of multilinguality\"). These results suggest that massively multilingual pre-training may not be optimal for any languages involved, but that more targeted models can significantly improve performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tyler Chang",
      "Catherine Arnett",
      "Zhuowen Tu",
      "Ben Bergen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.237": {
    "title": "Teaching Embodied Reinforcement Learning Agents: Informativeness and Diversity of Language Use",
    "volume": "main",
    "abstract": "In real-world scenarios, it is desirable for embodied agents to have the ability to leverage human language to gain explicit or implicit knowledge for learning tasks. Despite recent progress, most previous approaches adopt simple low-level instructions as language inputs, which may not reflect natural human communication. We expect human language to be informative (i.e., providing feedback on agents' past behaviors and offering guidance on achieving their future goals) and diverse (i.e., encompassing a wide range of expressions and style nuances). To enable flexibility of language use in teaching agents tasks, this paper studies different types of language inputs in facilitating reinforcement learning (RL) embodied agents. More specifically, we examine how different levels of language informativeness and diversity impact agent learning and inference. Our empirical results based on four RL benchmarks demonstrate that agents trained with diverse and informative language feedback can achieve enhanced generalization and fast adaptation to new tasks. These findings highlight the pivotal role of language use in teaching embodied agents new tasks in an open world",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiajun Xi",
      "Yinong He",
      "Jianing Yang",
      "Yinpei Dai",
      "Joyce Chai"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.238": {
    "title": "MiTTenS: A Dataset for Evaluating Gender Mistranslation",
    "volume": "main",
    "abstract": "Translation systems, including foundation models capable of translation, can produce errors that result in gender mistranslation, and such errors can be especially harmful. To measure the extent of such potential harms when translating into and out of English, we introduce a dataset, MiTTenS, covering 26 languages from a variety of language families and scripts, including several traditionally under-represented in digital resources. The dataset is constructed with handcrafted passages that target known failure patterns, longer synthetically generated passages, and natural passages sourced from multiple domains. We demonstrate the usefulness of the dataset by evaluating both neural machine translation systems and foundation models, and show that all systems exhibit gender mistranslation and potential harm, even in high resource languages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kevin Robinson",
      "Sneha Kudugunta",
      "Romina Stella",
      "Sunipa Dev",
      "Jasmijn Bastings"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.239": {
    "title": "Teaching LLMs to Abstain across Languages via Multilingual Feedback",
    "volume": "main",
    "abstract": "Multilingual LLMs often have knowledge disparities across languages, with larger gaps in under-resourced languages. Teaching LLMs to abstain in the face of knowledge gaps is thus a promising strategy to mitigate hallucinations in multilingual settings. However, previous studies on LLM abstention primarily focus on English; we find that directly applying existing solutions beyond English results in up to 20.5% performance gaps between high and low-resource languages, potentially due to LLMs' drop in calibration and reasoning beyond a few resource-rich languages. To this end, we propose strategies to enhance LLM abstention by learning from multilingual feedback, where LLMs self-reflect on proposed answers in one language by generating multiple feedback items in related languages: we show that this helps identifying the knowledge gaps across diverse languages, cultures, and communities. Extensive experiments demonstrate that our multilingual feedback approach outperforms various strong baselines, achieving up to 9.2% improvement for low-resource languages across three black-box and open models on three datasets, featuring open-book, closed-book, and commonsense QA. Further analysis reveals that multilingual feedback is both an effective and a more equitable abstain strategy to serve diverse language speakers, and cultural factors have great impact on language selection and LLM abstention behavior, highlighting future directions for multilingual and multi-cultural reliable language modeling",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shangbin Feng",
      "Weijia Shi",
      "Yike Wang",
      "Wenxuan Ding",
      "Orevaoghene Ahia",
      "Shuyue Stella Li",
      "Vidhisha Balachandran",
      "Sunayana Sitaram",
      "Yulia Tsvetkov"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.240": {
    "title": "Modular Pluralism: Pluralistic Alignment via Multi-LLM Collaboration",
    "volume": "main",
    "abstract": "While existing alignment paradigms have been integral in developing large language models (LLMs), LLMs often learn an averaged human preference and struggle to model diverse preferences across cultures, demographics, and communities. We propose Modular Pluralism, a modular framework based on multi-LLM collaboration for pluralistic alignment: it \"plugs into\" a base LLM a pool of smaller but specialized community LMs, where models collaborate in distinct modes to flexibility support three modes of pluralism: Overton, steerable, and distributional. Modular Pluralism is uniquely compatible with black-box LLMs and offers the modular control of adding new community LMs for previously underrepresented communities. We evaluate Modular Pluralism with six tasks and four datasets featuring questions/instructions with value-laden and perspective-informed responses. Extensive experiments demonstrate that Modular Pluralism advances the three pluralism objectives across six black-box and open-source LLMs. Further analysis reveals that LLMs are generally faithful to the inputs from smaller community LLMs, allowing seamless patching by adding a new community LM to better cover previously underrepresented communities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shangbin Feng",
      "Taylor Sorensen",
      "Yuhan Liu",
      "Jillian Fisher",
      "Chan Young Park",
      "Yejin Choi",
      "Yulia Tsvetkov"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.241": {
    "title": "StyleRemix: Interpretable Authorship Obfuscation via Distillation and Perturbation of Style Elements",
    "volume": "main",
    "abstract": "Authorship obfuscation, rewriting a text to intentionally obscure the identity of the author, is important yet challenging. Current methods using large language models (LLMs) lack interpretability and controllability, often ignoring author-specific stylistic features, resulting in less robust performance overall.To address this, we develop StyleRemix, an adaptive and interpretable obfuscation method that perturbs specific, fine-grained style elements of the original input text. StyleRemix uses pre-trained Low Rank Adaptation (LoRA) modules to rewrite inputs along various stylistic axes (e.g., formality, length) while maintaining low computational costs. StyleRemix outperforms state-of-the-art baselines and much larger LLMs on an array of domains on both automatic and human evaluation.Additionally, we release AuthorMix, a large set of 30K high-quality, long-form texts from a diverse set of 14 authors and 4 domains, and DiSC, a parallel corpus of 1,500 texts spanning seven style axes in 16 unique directions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jillian Fisher",
      "Skyler Hallinan",
      "Ximing Lu",
      "Mitchell Gordon",
      "Zaid Harchaoui",
      "Yejin Choi"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.242": {
    "title": "I Could've Asked That: Reformulating Unanswerable Questions",
    "volume": "main",
    "abstract": "When seeking information from unfamiliar documents, users frequently pose questions that cannot be answered by the documents. While existing large language models (LLMs) identify these unanswerable questions, they do not assist users in reformulating their questions, thereby reducing their overall utility. We curate CouldAsk, an evaluation benchmark composed of existing and new datasets for document-grounded question answering, specifically designed to study reformulating unanswerable questions. We evaluate state-of-the-art open-source and proprietary LLMs on CouldAsk. The results demonstrate the limited capabilities of these models in reformulating questions. Specifically, GPT-4 and Llama2-7B successfully reformulate questions only 26% and 12% of the time, respectively. Error analysis shows that 62% of the unsuccessful reformulations stem from the models merely rephrasing the questions or even generating identical questions. We publicly release the benchmark and the code to reproduce the experiments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenting Zhao",
      "Ge Gao",
      "Claire Cardie",
      "Alexander Rush"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.243": {
    "title": "STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions",
    "volume": "main",
    "abstract": "Mitigating explicit and implicit biases in Large Language Models (LLMs) has become a critical focus in the field of natural language processing. However, many current methodologies evaluate scenarios in isolation, without considering the broader context or the spectrum of potential biases within each situation. To address this, we introduce the Sensitivity Testing on Offensive Progressions (STOP) dataset, which includes 450 offensive progressions containing 2,700 unique sentences of varying severity that progressively escalate from less to more explicitly offensive. Covering a broad spectrum of 9 demographics and 46 sub-demographics, STOP ensures inclusivity and comprehensive coverage. We evaluate several leading closed- and open-source models, including GPT-4, Mixtral, and Llama 3. Our findings reveal that even the best-performing models detect bias inconsistently, with success rates ranging from 19.3% to 69.8%. Furthermore, we demonstrate how aligning models with human judgments on STOP can improve model answer rates on sensitive tasks such as BBQ, StereoSet, and CrowS-Pairs by up to 191%, while maintaining or even improving performance. STOP presents a novel framework for assessing the complex nature of biases in LLMs, which will enable more effective bias mitigation strategies and facilitates the creation of fairer language models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Robert Morabito",
      "Sangmitra Madhusudan",
      "Tyler McDonald",
      "Ali Emami"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.244": {
    "title": "Hidden Persuaders: LLMs' Political Leaning and Their Influence on Voters",
    "volume": "main",
    "abstract": "Do LLMs have political leanings and are LLMs able to shift our political views? This paper explores these questions in the context of the 2024 U.S. presidential election. Through a voting simulation, we demonstrate 18 open-weight and closed-source LLMs' political preference for Biden over Trump. We show how Biden-leaning becomes more pronounced in instruction-tuned and reinforced models compared to their base versions by analyzing their responses to political questions related to the two nominees. We further explore the potential impact of LLMs on voter choice by recruiting 935 U.S. registered voters. Participants interacted with LLMs (Claude-3, Llama-3, and GPT-4) over five exchanges. Intriguingly, although LLMs were not asked to persuade users to support Biden, about 20% of Trump supporters reduced their support for Trump after LLM interaction. This result is noteworthy given that many studies on the persuasiveness of political campaigns have shown minimal effects in presidential elections. Many users also expressed a desire for further interaction with LLMs on political subjects. Further research on how LLMs affect users' political views is required, as their use becomes more widespread",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujin Potter",
      "Shiyang Lai",
      "Junsol Kim",
      "James Evans",
      "Dawn Song"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.245": {
    "title": "SOUL: Unlocking the Power of Second-Order Optimization for LLM Unlearning",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have highlighted the necessity of effective unlearning mechanisms to comply with data regulations and ethical AI practices. LLM unlearning aims at removing undesired data influences and associated model capabilities without compromising utility beyond the scope of unlearning. While interest in studying LLM unlearning is growing, the impact of the optimizer choice for LLM unlearning remains unexplored. In this work, we shed light on the significance of optimizer selection in LLM unlearning for the first time, establishing a clear connection between second-order optimization and influence unlearning (a classical approach using influence functions to update the model for data influence removal). This insight propels us to develop a second-order optimization-based LLM unlearning framework, termed Second-Order UnLearning (SOUL), which extends the static, one-shot model update using influence unlearning to a dynamic, iterative unlearning process. Our extensive experiments show that SOUL consistently outperforms conventional first-order methods across various unlearning tasks, models, and metrics, indicating that second-order optimization offers an effective and broadly applicable solution for LLM unlearning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinghan Jia",
      "Yihua Zhang",
      "Yimeng Zhang",
      "Jiancheng Liu",
      "Bharat Runwal",
      "James Diffenderfer",
      "Bhavya Kailkhura",
      "Sijia Liu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.246": {
    "title": "When Reasoning Meets Information Aggregation: A Case Study with Sports Narratives",
    "volume": "main",
    "abstract": "Reasoning is most powerful when an LLM accurately aggregates relevant information. We examine the critical role of information aggregation in reasoning by requiring the LLM to analyze sports narratives. To succeed at this task, an LLM must infer points from actions, identify related entities, attribute points accurately to players and teams, and compile key statistics to draw conclusions. We conduct comprehensive experiments with real NBA basketball data and present SportsGen, a new method to synthesize game narratives. By synthesizing data, we can rigorously evaluate LLMs' reasoning capabilities under complex scenarios with varying narrative lengths and density of information. Our findings show that most models, including GPT-4o, often fail to accurately aggregate basketball scores due to frequent scoring patterns. Open-source models like Llama-3 further suffer from significant score hallucinations. Finally, the effectiveness of reasoning is influenced by narrative complexity, information density, and domain-specific terms, highlighting the challenges in analytical reasoning tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yebowen Hu",
      "Kaiqiang Song",
      "Sangwoo Cho",
      "Xiaoyang Wang",
      "Wenlin Yao",
      "Hassan Foroosh",
      "Dong Yu",
      "Fei Liu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.247": {
    "title": "An Analysis of Multilingual FActScore",
    "volume": "main",
    "abstract": "FActScore has gained popularity as a metric to estimate the factuality of long-form texts generated by Large Language Models (LLMs) in English. However, there has not been any work in studying the behavior of FActScore in other languages. This paper studies the limitations of each component in the four-component pipeline of FActScore in the multilingual setting. We introduce a new dataset for FActScore on texts generated by strong multilingual LLMs. Our evaluation shows that LLMs exhibit distinct behaviors in both fact extraction and fact scoring tasks. No LLM produces consistent and reliable FActScore across languages of varying levels of resources. We also find that the knowledge source plays an important role in the quality of the estimated FActScore. Using Wikipedia as the knowledge source may hinder the true FActScore of long-form text due to its limited coverage in medium- and low-resource languages. We also incorporate 3 mitigations to our knowledge source that ultimately improve FActScore estimation across all languages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vu Kim",
      "Michael Krumdick",
      "Varshini Reddy",
      "Franck Dernoncourt",
      "Viet Lai"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.248": {
    "title": "Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models",
    "volume": "main",
    "abstract": "Proprietary LMs such as GPT-4 are often employed to assess the quality of responses from various LMs. However, concerns including transparency, controllability, and affordability strongly motivate the development of open-source LMs specialized in evaluations. On the other hand, existing open evaluator LMs exhibit critical shortcomings: 1) they issue scores that significantly diverge from those assigned by humans, and 2) they lack the flexibility to perform both direct assessment and pairwise ranking, the two most prevalent forms of assessment. Additionally, they do not possess the ability to evaluate based on custom evaluation criteria, focusing instead on general attributes like helpfulness and harmlessness. To address these issues, we introduce Prometheus 2, a more powerful evaluator LM than its predecessor that closely mirrors human and GPT-4 judgements. Moreover, it is capable of processing both direct assessment and pair-wise ranking formats grouped with a user-defined evaluation criteria. On four direct assessment benchmarks and four pairwise ranking benchmarks, Prometheus 2 scores the highest correlation and agreement with humans and proprietary LM judges among all tested open evaluator LMs. Our models, code, and data are all publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seungone Kim",
      "Juyoung Suk",
      "Shayne Longpre",
      "Bill Yuchen Lin",
      "Jamin Shin",
      "Sean Welleck",
      "Graham Neubig",
      "Moontae Lee",
      "Kyungjae Lee",
      "Minjoon Seo"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.249": {
    "title": "RAG-QA Arena: Evaluating Domain Robustness for Long-form Retrieval Augmented Question Answering",
    "volume": "main",
    "abstract": "Question answering based on retrieval augmented generation (RAG-QA) is an important research topic in NLP and has a wide range of real-world applications. However, most existing datasets for this task are either constructed using a single source corpus or consist of short extractive answers, which fall short of evaluating large language model (LLM) based RAG-QA systems on cross-domain generalization. To address these limitations, we create Long-form RobustQA (LFRQA), a new dataset comprising human-written long-form answers that integrate short extractive answers from multiple documents into a single, coherent narrative, covering 26K queries and large corpora across seven different domains. We further propose RAG-QA Arena by directly comparing model-generated answers against LFRQA's answers using LLMs as evaluators. We show via extensive experiments that RAG-QA Arena and human judgments on answer quality are highly correlated. Moreover, only 41.3% of the most competitive LLM's answers are preferred to LFRQA's answers, demonstrating RAG-QA Arena as a challenging evaluation platform for future research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rujun Han",
      "Yuhao Zhang",
      "Peng Qi",
      "Yumo Xu",
      "Jenyuan Wang",
      "Lan Liu",
      "William Yang Wang",
      "Bonan Min",
      "Vittorio Castelli"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.250": {
    "title": "PromptReps: Prompting Large Language Models to Generate Dense and Sparse Representations for Zero-Shot Document Retrieval",
    "volume": "main",
    "abstract": "Utilizing large language models (LLMs) for zero-shot document ranking is done in one of two ways: (1) prompt-based re-ranking methods, which require no further training but are only feasible for re-ranking a handful of candidate documents due to computational costs; and (2) unsupervised contrastive trained dense retrieval methods, which can retrieve relevant documents from the entire corpus but require a large amount of paired text data for contrastive training.In this paper, we propose PromptReps, which combines the advantages of both categories: no need for training and the ability to retrieve from the whole corpus. Our method only requires prompts to guide an LLM to generate query and document representations for effective document retrieval. Specifically, we prompt the LLMs to represent a given text using a single word, and then use the last token's hidden states and the corresponding logits associated with the prediction of the next token to construct a hybrid document retrieval system. The retrieval system harnesses both dense text embedding and sparse bag-of-words representations given by the LLM.Our experimental evaluation on the MSMARCO, TREC deep learning and BEIR zero-shot document retrieval datasets illustrates that this simple prompt-based LLM retrieval method can achieve a similar or higher retrieval effectiveness than state-of-the-art LLM embedding methods that are trained with large amounts of unsupervised data, especially when using a larger LLM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengyao Zhuang",
      "Xueguang Ma",
      "Bevan Koopman",
      "Jimmy Lin",
      "Guido Zuccon"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.251": {
    "title": "Voices Unheard: NLP Resources and Models for Yorùbá Regional Dialects",
    "volume": "main",
    "abstract": "Yoruba—an African language with roughly 47 million speakers—encompasses a continuum with several dialects. Recent efforts to develop NLP technologies for African languages have focused on their standard dialects, resulting in disparities for dialects and varieties for which there are little to no resources or tools. We take steps towards bridging this gap by introducing a new high-quality parallel text and speech corpus; YORULECT across three domains and four regional yoruba dialects. To develop this corpus, we engaged native speakers, traveling to communities where these dialects are spoken, to collect text and speech data. Using our newly created corpus, we conducted extensive experiments on (text) machine translation, automatic speech recognition, and speech-to-text translation. Our results reveal substantial performance disparities between standard yoruba and the other dialects across all tasks. However, we also show that with dialect-adaptive finetuning, we are able to narrow this gap. We believe our dataset and experimental analysis will contribute greatly to developing NLP tools for Yoruba and its dialects, and potentially for other African languages, by improving our understanding of existing challenges and offering a high-quality dataset for further development. We will release YORULECT dataset and models publicly under an open license",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Orevaoghene Ahia",
      "Anuoluwapo Aremu",
      "Diana Abagyan",
      "Hila Gonen",
      "David Adelani",
      "Daud Abolade",
      "Noah Smith",
      "Yulia Tsvetkov"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.252": {
    "title": "ARES: Alternating Reinforcement Learning and Supervised Fine-Tuning for Enhanced Multi-Modal Chain-of-Thought Reasoning Through Diverse AI Feedback",
    "volume": "main",
    "abstract": "Large Multimodal Models (LMMs) excel at comprehending human instructions and demonstrate remarkable results across a broad spectrum of tasks. Reinforcement Learning from Human Feedback (RLHF) and AI Feedback (RLAIF) further refine LLMs by aligning them with specific preferences. These methods primarily use ranking-based feedback for entire generations. With advanced AI models (Teacher), such as GPT-4 and Claude 3 Opus, we can request various types of detailed feedback that are expensive for humans to provide. We propose a two-stage algorithm ARES that Alternates REinforcement Learning (RL) and Supervised Fine-Tuning (SFT). First, we ask the Teacher to score how much each sentence contributes to solving the problem in a Chain-of-Thought (CoT). This sentence-level feedback allows us to consider individual valuable segments, providing more granular rewards for the RL procedure. Second, we ask the Teacher to correct wrong reasoning after the RL stage. The RL procedure requires substantial hyperparameter tuning and often generates errors such as repetitive words and incomplete sentences. With correction feedback, we stabilize the RL fine-tuned model through SFT. We conduct experiments on the multi-modal datasets ScienceQA and A-OKVQA to demonstrate the effectiveness of our proposal. The ARES rationale achieves around 70% win rate compared to baseline models judged by GPT-4o. Additionally, we observe that the improved rationale reasoning leads to a 2.5% increase in inference answer accuracy on average for the multi-modal datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ju-Seung Byun",
      "Jiyun Chun",
      "Jihyung Kil",
      "Andrew Perrault"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.253": {
    "title": "Order of Magnitude Speedups for LLM Membership Inference",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have the promise to revolutionize computing broadly, but their complexity and extensive training data also expose significant privacy vulnerabilities. One of the simplest privacy risks associated with LLMs is their susceptibility to membership inference attacks (MIAs), wherein an adversary aims to determine whether a specific data point was part of the model's training set. Although this is a known risk, state of the art methodologies for MIAs rely on training multiple computationally costly ‘shadow models', making risk evaluation prohibitive for large models. Here we adapt a recent line of work which uses quantile regression to mount membership inference attacks; we extend this work by proposing a low-cost MIA that leverages an ensemble of small quantile regression models to determine if a document belongs to the model's training set or not. We demonstrate the effectiveness of this approach on fine-tuned LLMs of varying families (OPT, Pythia, Llama) and across multiple datasets. Across all scenarios we obtain comparable or improved accuracy compared to state of the art ‘shadow model' approaches, with as little as 6% of their computation budget. We demonstrate increased effectiveness across multi-epoch trained target models, and architecture miss-specification robustness, that is, we can mount an effective attack against a model using a different tokenizer and architecture, without requiring knowledge on the target model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rongting Zhang",
      "Martin Bertran",
      "Aaron Roth"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.254": {
    "title": "VIMI: Grounding Video Generation through Multi-modal Instruction",
    "volume": "main",
    "abstract": "Existing text-to-video diffusion models rely solely on text-only encoders for their pretraining. This limitation stems from the absence of large-scale multimodal prompt video datasets, resulting in a lack of visual grounding and restricting their versatility and application in multimodal integration. To address this, we construct a large-scale multimodal prompt dataset by employing retrieval methods to pair in-context examples with the given text prompts and then utilize a two-stage training strategy to enable diverse video generation tasks within a model. In the first stage, we propose a multimodal conditional video generation framework for pretraining on these augmented datasets, establishing a foundational model for grounded video generation. Secondly, we fine-tune the model from the first stage on various video generation tasks, incorporating multimodal instructions. This process further refines the model's ability to handle diverse inputs and tasks, ensuring seamless integration of multimodal information. After this two-stage training process, VIMI demonstrates multimodal understanding capabilities, producing contextually rich and personalized videos grounded in the provided inputs, as shown in Figure1. Compared to previous subject-driven video generation methods, our generator can synthesize consistent and temporally coherent videos with large motion while retaining the semantic control. Our generator also achieves state-of-the-art text-to-video generation results on UCF101 benchmark",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuwei Fang",
      "Willi Menapace",
      "Aliaksandr Siarohin",
      "Tsai-Shien Chen",
      "Kuan-Chieh Wang",
      "Ivan Skorokhodov",
      "Graham Neubig",
      "Sergey Tulyakov"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.255": {
    "title": "F2RL: Factuality and Faithfulness Reinforcement Learning Framework for Claim-Guided Evidence-Supported Counterspeech Generation",
    "volume": "main",
    "abstract": "Hate speech (HS) on social media exacerbates misinformation and baseless prejudices. Evidence-supported counterspeech (CS) is crucial for correcting misinformation and reducing prejudices through facts. Existing methods for generating evidence-supported CS often lack clear guidance with a core claim for organizing evidence and do not adequately address factuality and faithfulness hallucinations in CS within anti-hate contexts. In this paper, to mitigate the aforementioned, we propose F2RL, a Factuality and Faithfulness Reinforcement Learning framework for generating claim-guided and evidence-supported CS. Firstly, we generate counter-claims based on hate speech and design a self-evaluation mechanism to select the most appropriate one. Secondly, we propose a coarse-to-fine evidence retrieval method. This method initially generates broad queries to ensure the diversity of evidence, followed by carefully reranking the retrieved evidence to ensure its relevance to the claim. Finally, we design a reinforcement learning method with a triplet-based factuality reward model and a multi-aspect faithfulness reward model. The method rewards the generator to encourage greater factuality, more accurate refutation of hate speech, consistency with the claim, and better utilization of evidence. Extensive experiments on three benchmark datasets demonstrate that the proposed framework achieves excellent performance in CS generation, with strong factuality and faithfulness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiyang Wang",
      "Yuchen Pan",
      "Xin Song",
      "Xuechen Zhao",
      "Minghao Hu",
      "Bin Zhou"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.256": {
    "title": "Deciphering Rumors: A Multi-Task Learning Approach with Intent-aware Hierarchical Contrastive Learning",
    "volume": "main",
    "abstract": "Social networks are rife with noise and misleading information, presenting multifaceted challenges for rumor detection. In this paper, from the perspective of human cognitive subjectivity, we introduce the mining of individual latent intentions and propose a novel multi-task learning framework, the Intent-Aware Rumor Detection Network (IRDNet). IRDNet is designed to discern multi-level rumor semantic features and latent user intentions, addressing the challenges of robustness and key feature mining and alignment that plague existing models. In IRDNet, the multi-level semantic extraction module captures sequential and hierarchical features to generate robust semantic representations. The hierarchical contrastive learning module incorporates two complementary strategies, event-level and intent-level, to establish cognitive anchors that uncover the latent intentions of information disseminators. Event-level contrastive learning employs high-quality data augmentation and adversarial perturbations to enhance model robustness. Intent-level contrastive learning leverages the intent encoder to capture latent intent features and optimize consistency within the same intent while ensuring heterogeneity between different intents to clearly distinguish key features from irrelevant elements. Experimental results demonstrate that IRDNet significantly improves the effectiveness of rumor detection and effectively addresses the challenges present in the field of rumor detection",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chang Yang",
      "Peng Zhang",
      "Hui Gao",
      "Jing Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.257": {
    "title": "Visual Prompting in LLMs for Enhancing Emotion Recognition",
    "volume": "main",
    "abstract": "Vision Large Language Models (VLLMs) are transforming the intersection of computer vision and natural language processing; however, the potential of using visual prompts for emotion recognition in these models remains largely unexplored and untapped. Traditional methods in VLLMs struggle with spatial localization and often discard valuable global context. We propose a novel Set-of-Vision prompting (SoV) approach that enhances zero-shot emotion recognition by using spatial information, such as bounding boxes and facial landmarks, to mark targets precisely. SoV improves accuracy in face count and emotion categorization while preserving the enriched image context. Through comprehensive experimentation and analysis of recent commercial or open-source VLLMs, we evaluate the SoV model's ability to comprehend facial expressions in natural environments. Our findings demonstrate the effectiveness of integrating spatial visual prompts into VLLMs for improving emotion recognition performance",
    "checked": true,
    "id": "6d1b37ace2429bb6935d656a043ce6d4484be9de",
    "semantic_title": "visual prompting in llms for enhancing emotion recognition",
    "citation_count": 0,
    "authors": [
      "Qixuan Zhang",
      "Zhifeng Wang",
      "Dylan Zhang",
      "Wenjia Niu",
      "Sabrina Caldwell",
      "Tom Gedeon",
      "Yang Liu",
      "Zhenyue Qin"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.258": {
    "title": "IDEAW: Robust Neural Audio Watermarking with Invertible Dual-Embedding",
    "volume": "main",
    "abstract": "The audio watermarking technique embeds messages into audio and accurately extracts messages from the watermarked audio. Traditional methods develop algorithms based on expert experience to embed watermarks into the time-domain or transform-domain of signals. With the development of deep neural networks, deep learning-based neural audio watermarking has emerged. Compared to traditional algorithms, neural audio watermarking achieves better robustness by considering various attacks during training. However, current neural watermarking methods suffer from low capacity and unsatisfactory imperceptibility. Additionally, the issue of watermark locating, which is extremely important and even more pronounced in neural audio water- marking, has not been adequately studied. In this paper, we design a dual-embedding wa- termarking model for efficient locating. We also consider the impact of the attack layer on the invertible neural network in robustness training, improving the model to enhance both its reasonableness and stability. Experiments show that the proposed model, IDEAW, can withstand various attacks with higher capacity and more efficient locating ability compared to existing methods",
    "checked": true,
    "id": "695553bab1369982f1849f96678b1f70498ba749",
    "semantic_title": "ideaw: robust neural audio watermarking with invertible dual-embedding",
    "citation_count": 0,
    "authors": [
      "Pengcheng Li",
      "Xulong Zhang",
      "Jing Xiao",
      "Jianzong Wang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.259": {
    "title": "Leveraging Conflicts in Social Media Posts: Unintended Offense Dataset",
    "volume": "main",
    "abstract": "In multi-person communications, conflicts often arise. Each individual may have their own perspective, which can differ. Additionally, commonly referenced offensive datasets frequently neglect contextual information and are primarily constructed with a focus on intended offenses. This study suggests that conflicts are pivotal in revealing a broader range of human interactions, including instances of unintended offensive language. This paper proposes a conflict-based data collection method to utilize inter-conflict cues in multi-person communications. By focusing on specific cue posts within conversation threads, our proposed approach effectively identifies relevant instances for analysis. Detailed analyses are provided to showcase the proposed approach efficiently gathers data on subtly offensive content. The experimental results indicate that incorporating elements of conflict into data collection significantly enhances the comprehensiveness and accuracy of detecting offensive language but also enriches our understanding of conflict dynamics in digital communication",
    "checked": true,
    "id": "7df9d17dfc7d67ee439ed3e9ac179e01e33637ad",
    "semantic_title": "leveraging conflicts in social media posts: unintended offense dataset",
    "citation_count": 0,
    "authors": [
      "Che Wei Tsai",
      "Yen-Hao Huang",
      "Tsu-Keng Liao",
      "Didier Estrada",
      "Retnani Latifah",
      "Yi-Shin Chen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.260": {
    "title": "Outcome-Constrained Large Language Models for Countering Hate Speech",
    "volume": "main",
    "abstract": "Automatic counterspeech generation methods have been developed to assist efforts in combating hate speech. Existing research focuses on generating counterspeech with linguistic attributes such as being polite, informative, and intent-driven. However, the real impact of counterspeech in online environments is seldom considered. This study aims to develop methods for generating counterspeech constrained by conversation outcomes and evaluate their effectiveness. We experiment with large language models (LLMs) to incorporate into the text generation process two desired conversation outcomes: low conversation incivility and non-hateful hater reentry. Specifically, we experiment with instruction prompts, LLM finetuning, and LLM reinforcement learning (RL). Evaluation results show that our methods effectively steer the generation of counterspeech toward the desired outcomes. Our analyses, however, show that there are differences in the quality and style depending on the model",
    "checked": true,
    "id": "82cafec3b5af73751ccbde129d8f24a0f2acef1d",
    "semantic_title": "outcome-constrained large language models for countering hate speech",
    "citation_count": 2,
    "authors": [
      "Lingzi Hong",
      "Pengcheng Luo",
      "Eduardo Blanco",
      "Xiaoying Song"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.261": {
    "title": "Multiple Sources are Better Than One: Incorporating External Knowledge in Low-Resource Glossing",
    "volume": "main",
    "abstract": "In this paper, we address the data scarcity problem in automatic data-driven glossing for low-resource languages by coordinating multiple sources of linguistic expertise. We enhance models by incorporating both token-level and sentence-level translations, utilizing the extensive linguistic capabilities of modern LLMs, and incorporating available dictionary resources. Our enhancements lead to an average absolute improvement of 5%-points in word-level accuracy over the previous state of the art on a typologically diverse dataset spanning six low-resource languages. The improvements are particularly noticeable for the lowest-resourced language Gitksan, where we achieve a 10%-point improvement. Furthermore, in a simulated ultra-low resource setting for the same six languages, training on fewer than 100 glossed sentences, we establish an average 10%-point improvement in word-level accuracy over the previous state-of-the-art system",
    "checked": true,
    "id": "4991cbd4fe71769ce0a74b27715488124560b5b5",
    "semantic_title": "multiple sources are better than one: incorporating external knowledge in low-resource glossing",
    "citation_count": 0,
    "authors": [
      "Changbing Yang",
      "Garrett Nicolai",
      "Miikka Silfverberg"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.262": {
    "title": "Adaptive Immune-based Sound-Shape Code Substitution for Adversarial Chinese Text Attacks",
    "volume": "main",
    "abstract": "Adversarial textual examples reveal the vulnerability of natural language processing (NLP) models. Most existing text attack methods are designed for English text, while the robust implementation of the second popular language, i.e., Chinese with 1 billion users, is greatly underestimated. Although several Chinese attack methods have been presented, they either directly transfer from English attacks or adopt simple greedy search to optimize the attack priority, usually leading to unnatural sentences. To address these issues, we propose an adaptive Immune-based Sound-Shape Code (ISSC) algorithm for adversarial Chinese text attacks. Firstly, we leverage the Sound-Shape code to generate natural substitutions, which comprehensively integrate multiple Chinese features. Secondly, we employ adaptive immune algorithm (IA) to determine the replacement order, which can reduce the duplication of population to improve the search ability. Extensive experimental results validate the superiority of our ISSC in producing high-quality Chinese adversarial texts. Our code and data can be found in https://github.com/nohuma/chinese-attack-issc",
    "checked": true,
    "id": "c12ab0c8eb9900510b6e977cd0de37e8b4479d29",
    "semantic_title": "adaptive immune-based sound-shape code substitution for adversarial chinese text attacks",
    "citation_count": 0,
    "authors": [
      "Ao Wang",
      "Xinghao Yang",
      "Chen Li",
      "Bao-di Liu",
      "Weifeng Liu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.263": {
    "title": "Bootstrapped Policy Learning for Task-oriented Dialogue through Goal Shaping",
    "volume": "main",
    "abstract": "Reinforcement learning shows promise in optimizing dialogue policies, but addressing the challenge of reward sparsity remains crucial. While curriculum learning offers a practical solution by strategically training policies from simple to complex, it hinges on the assumption of a gradual increase in goal difficulty to ensure a smooth knowledge transition across varied complexities. In complex dialogue environments without intermediate goals, achieving seamless knowledge transitions becomes tricky. This paper proposes a novel Bootstrapped Policy Learning (BPL) framework, which adaptively tailors progressively challenging subgoal curriculum for each complex goal through goal shaping, ensuring a smooth knowledge transition. Goal shaping involves goal decomposition and evolution, decomposing complex goals into subgoals with solvable maximum difficulty and progressively increasing difficulty as the policy improves. Moreover, to enhance BPL's adaptability across various environments, we explore various combinations of goal decomposition and evolution within BPL, and identify two universal curriculum patterns that remain effective across different dialogue environments, independent of specific environmental constraints. By integrating the summarized curriculum patterns, our BPL has exhibited efficacy and versatility across four publicly available datasets with different difficulty levels",
    "checked": true,
    "id": "55087ab0de0ef5e57d5f441ae5872941bd3b71bb",
    "semantic_title": "bootstrapped policy learning for task-oriented dialogue through goal shaping",
    "citation_count": 0,
    "authors": [
      "Yangyang Zhao",
      "Ben Niu",
      "Mehdi Dastani",
      "Shihan Wang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.264": {
    "title": "PsyGUARD: An Automated System for Suicide Detection and Risk Assessment in Psychological Counseling",
    "volume": "main",
    "abstract": "As awareness of mental health issues grows, online counseling support services are becoming increasingly prevalent worldwide. Detecting whether users express suicidal ideation in text-based counseling services is crucial for identifying and prioritizing at-risk individuals. However, the lack of domain-specific systems to facilitate fine-grained suicide detection and corresponding risk assessment in online counseling poses a significant challenge for automated crisis intervention aimed at suicide prevention. In this paper, we propose PsyGUARD, an automated system for detecting suicide ideation and assessing risk in psychological counseling. To achieve this, we first develop a detailed taxonomy for detecting suicide ideation based on foundational theories. We then curate a large-scale, high-quality dataset called PsySUICIDE for suicide detection. To evaluate the capabilities of automated systems in fine-grained suicide detection, we establish a range of baselines. Subsequently, to assist automated services in providing safe, helpful, and tailored responses for further assessment, we propose to build a suite of risk assessment frameworks. Our study not only provides an insightful analysis of the effectiveness of automated risk assessment systems based on fine-grained suicide detection but also highlights their potential to improve mental health services on online counseling platforms. Code, data, and models are available at https://github.com/qiuhuachuan/PsyGUARD",
    "checked": true,
    "id": "c0bf1f4672b318541df036ab118dc2c127845bbf",
    "semantic_title": "psyguard: an automated system for suicide detection and risk assessment in psychological counseling",
    "citation_count": 0,
    "authors": [
      "Huachuan Qiu",
      "Lizhi Ma",
      "Zhenzhong Lan"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.265": {
    "title": "World to Code: Multi-modal Data Generation via Self-Instructed Compositional Captioning and Filtering",
    "volume": "main",
    "abstract": "Recent advances in Vision-Language Models (VLMs) and the scarcity of high-quality multi-modal alignment data have inspired numerous researches on synthetic VLM data generation. The conventional norm in VLM data construction uses a mixture of specialists in caption and OCR, or stronger VLM APIs and expensive human annotation.In this paper, we present World to Code (W2C), a meticulously curated multi-modal data construction pipeline that organizes the final generation output into a Python code format. The pipeline leverages the VLM itself to extract cross-modal information via different prompts and filter the generated outputs again via a consistency filtering strategy. Experiments have demonstrated the high quality of W2C by improving various existing visual question answering and visual grounding benchmarks across different VLMs. Further analysis also demonstrates that the new code parsing ability of VLMs presents better cross-modal equivalence than the commonly used detail caption ability. Our code is available at https://github.com/foundation-multimodal-models/World2Code",
    "checked": true,
    "id": "1f3a10478cb0b87880833a7860d216e66ebb1de6",
    "semantic_title": "world to code: multi-modal data generation via self-instructed compositional captioning and filtering",
    "citation_count": 0,
    "authors": [
      "Jiacong Wang",
      "Bohong Wu",
      "Haiyong Jiang",
      "Zhou Xun",
      "Xin Xiao",
      "Haoyuan Guo",
      "Jun Xiao"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.266": {
    "title": "DVD: Dynamic Contrastive Decoding for Knowledge Amplification in Multi-Document Question Answering",
    "volume": "main",
    "abstract": "Large language models (LLMs) are widely used in question-answering (QA) systems but often generate information with hallucinations. Retrieval-augmented generation (RAG) offers a potential remedy, yet the uneven retrieval quality and irrelevant contents may distract LLMs.In this work, we address these issues at the generation phase by treating RAG as a multi-document QA task.We propose a novel decoding strategy, Dynamic Contrastive Decoding, which dynamically amplifies knowledge from selected documents during the generation phase. involves constructing inputs batchwise, designing new selection criteria to identify documents worth amplifying, and applying contrastive decoding with a specialized weight calculation to adjust the final logits used for sampling answer tokens. Zero-shot experimental results on ALCE-ASQA, NQ, TQA and PopQA benchmarks show that our method outperforms other decoding strategies. Additionally, we conduct experiments to validate the effectiveness of our selection criteria, weight calculation, and general multi-document scenarios. Our method requires no training and can be integrated with other methods to improve the RAG performance. Our codes will be publicly available at https://github.com/JulieJin-km/Dynamic_Contrastive_Decoding",
    "checked": true,
    "id": "c17dfb4ee6ce8e850a676f2703ae5130efb9bd85",
    "semantic_title": "dvd: dynamic contrastive decoding for knowledge amplification in multi-document question answering",
    "citation_count": 0,
    "authors": [
      "Jing Jin",
      "Houfeng Wang",
      "Hao Zhang",
      "Xiaoguang Li",
      "Zhijiang Guo"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.267": {
    "title": "How Do Humans Write Code? Large Models Do It the Same Way Too",
    "volume": "main",
    "abstract": "Program-of-Thought (PoT) replaces natural language-based Chain-of-Thought (CoT) as the most popular method in Large Language Models (LLMs) mathematical reasoning tasks by utilizing external tool calls to circumvent computational errors. However, our evaluation of the GPT-4 and Llama series reveals that using PoT introduces more reasoning errors, such as incorrect formulas or flawed logic, compared to CoT. To address this issue, we propose Human-Think Language (HTL), which leverages a suite of strategies that help integrate PoT and CoT, encompassing: (1) a new generation paradigm that uses full CoT reasoning to control code generation. (2) Focus Attention, that directs model attention to the CoT reasoning during PoT to generate more logical code. (3) reinforcement learning that utilizes the accuracy of both CoT and PoT responses as rewards to prevent repetitive reasoning steps in LLMs when solving difficult math problems. Our method achieves an average improvement of 6.5% on the Llama-Base model and 4.3% on the Mistral-Base model across 8 mathematical calculation datasets. It also shows significant effectiveness on five out-of-domain datasets by controlling the model's information flow, exhibiting strong transferability. Additionally, HTL shows the most significant improvement in non-mathematical natural language inference task, contributing to a unified reasoning task framework",
    "checked": true,
    "id": "559846635fdc3242898af592808d8e346f1ec00f",
    "semantic_title": "how do humans write code? large models do it the same way too",
    "citation_count": 1,
    "authors": [
      "Long Li",
      "Xuzheng He",
      "Haozhe Wang",
      "Linlin Wang",
      "Liang He"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.268": {
    "title": "Retrospex: Language Agent Meets Offline Reinforcement Learning Critic",
    "volume": "main",
    "abstract": "Large language models (LLMs) possess extensive knowledge and commonsense reasoning capabilities, making them valuable for creating powerful agents. However, existing LLM agent frameworks have not fully utilized past experiences for improvement. This work introduces a new LLM-based agent framework called Retrospex, which addresses this challenge by analyzing past experiences in depth. Unlike previous approaches, Retrospex does not directly integrate experiences into the LLM's context. Instead, it combines the LLM's action likelihood with action values estimated by a Reinforcement Learning (RL) Critic, which is trained on past experiences through an offline \"retrospection\" process. Additionally, Retrospex employs a dynamic action rescoring mechanism that increases the importance of experience-based values for tasks that require more interaction with the environment. We evaluate Retrospex in ScienceWorld, ALFWorld and Webshop environments, demonstrating its advantages over strong baselines",
    "checked": true,
    "id": "74b4eb323020121da84e9f34840d681a2dd86d86",
    "semantic_title": "retrospex: language agent meets offline reinforcement learning critic",
    "citation_count": 0,
    "authors": [
      "Yufei Xiang",
      "Yiqun Shen",
      "Yeqin Zhang",
      "Nguyen Cam-Tu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.269": {
    "title": "Forgetting Curve: A Reliable Method for Evaluating Memorization Capability for Long-Context Models",
    "volume": "main",
    "abstract": "Numerous recent works target to extend effective context length for language models and various methods, tasks and benchmarks exist to measure model's effective memory length. However, through thorough investigations, we find limitations for currently existing evaluations on model's memory. We provide an extensive survey for limitations in this work and propose a new method called forgetting curve to measure the memorization capability of long-context models. We show that forgetting curve has the advantage of being robust to the tested corpus and the experimental settings, of not relying on prompt and can be applied to any model size. We apply our forgetting curve to a large variety of models involving both transformer and RNN/SSM based architectures. Our measurement provides empirical evidence for the effectiveness of transformer extension techniques while raises questions for the effective length of RNN/SSM based models. We also examine the difference between our measurement and existing benchmarks as well as popular metrics for various models",
    "checked": true,
    "id": "e8982f1478986a617ffbeebdc6e0482ce2bc6811",
    "semantic_title": "forgetting curve: a reliable method for evaluating memorization capability for long-context models",
    "citation_count": 0,
    "authors": [
      "Xinyu Liu",
      "Runsong Zhao",
      "Pengcheng Huang",
      "Chunyang Xiao",
      "Bei Li",
      "Jingang Wang",
      "Tong Xiao",
      "JingBo Zhu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.270": {
    "title": "Retrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation",
    "volume": "main",
    "abstract": "Despite the significant progress of large language models (LLMs) in various tasks, they often produce factual errors due to their limited internal knowledge. Retrieval-Augmented Generation (RAG), which enhances LLMs with external knowledge sources, offers a promising solution. However, these methods can be misled by irrelevant paragraphs in retrieved documents. Due to the inherent uncertainty in LLM generation, inputting the entire document may introduce off-topic information, causing the model to deviate from the central topic and affecting the relevance of the generated content. To address these issues, we propose the Retrieve-Plan-Generation (RPG) framework. RPG generates plan tokens to guide subsequent generation in the plan stage. In the answer stage, the model selects relevant fine-grained paragraphs based on the plan and uses them for further answer generation. This plan-answer process is repeated iteratively until completion, enhancing generation relevance by focusing on specific topics. To implement this framework efficiently, we utilize a simple but effective multi-task prompt-tuning method, enabling the existing LLMs to handle both planning and answering. We comprehensively compare RPG with baselines across 5 knowledge-intensive generation tasks, demonstrating the effectiveness of our approach",
    "checked": true,
    "id": "90b337077164112e81ab91e5df1b8e4352b1ff3e",
    "semantic_title": "retrieve-plan-generation: an iterative planning and answering framework for knowledge-intensive llm generation",
    "citation_count": 2,
    "authors": [
      "Yuanjie Lyu",
      "Zihan Niu",
      "Zheyong Xie",
      "Chao Zhang",
      "Tong Xu",
      "Yang Wang",
      "Enhong Chen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.271": {
    "title": "CoEvol: Constructing Better Responses for Instruction Finetuning through Multi-Agent Cooperation",
    "volume": "main",
    "abstract": "In recent years, instruction fine-tuning (IFT) on large language models (LLMs) has garnered considerable attention to enhance model performance on unseen tasks. Attempts have been made on automatic construction and effective selection for IFT data. However, we posit that previous methods have not fully harnessed the potential of LLMs for enhancing data quality. The responses within IFT data could be further enhanced by leveraging the capabilities of LLMs themselves.In this paper, we propose CoEvol, an LLM-based multi-agent cooperation framework for the improvement of responses for instructions. To effectively refine the responses, we develop an iterative framework following a _debate-advise-edit-judge_ paradigm. A two-stage multi-agent debate strategy is further devised to ensure the diversity and reliability of editing suggestions within the framework. Empirically, models equipped with CoEvol outperform competitive baselines evaluated by MT-Bench and AlpacaEval, demonstrating its effectiveness in enhancing instruction-following capabilities for LLMs",
    "checked": true,
    "id": "1e56e3956a86b57d351c305d391cdb65a10c67bb",
    "semantic_title": "coevol: constructing better responses for instruction finetuning through multi-agent cooperation",
    "citation_count": 0,
    "authors": [
      "Renhao Li",
      "Minghuan Tan",
      "Derek Wong",
      "Min Yang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.272": {
    "title": "A Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners",
    "volume": "main",
    "abstract": "This study introduces a hypothesis-testing framework to assess whether large language models (LLMs) possess genuine reasoning abilities or primarily depend on token bias. We go beyond evaluating LLMs on accuracy; rather, we aim to investigate their token bias in solving logical reasoning tasks. Specifically, we develop carefully controlled synthetic datasets, featuring conjunction fallacy and syllogistic problems. Our framework outlines a list of hypotheses where token biases are readily identifiable, with all null hypotheses assuming genuine reasoning capabilities of LLMs. The findings in this study suggest, with statistical guarantee, that most LLMs still struggle with logical reasoning. While they may perform well on classic problems, their success largely depends on recognizing superficial patterns with strong token bias, thereby raising concerns about their actual reasoning and generalization abilities",
    "checked": true,
    "id": "00e88a0c006296e53bb7d4cfc90a134883ad34fd",
    "semantic_title": "a peek into token bias: large language models are not yet genuine reasoners",
    "citation_count": 11,
    "authors": [
      "Bowen Jiang",
      "Yangxinyu Xie",
      "Zhuoqun Hao",
      "Xiaomeng Wang",
      "Tanwi Mallick",
      "Weijie Su",
      "Camillo Taylor",
      "Dan Roth"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.273": {
    "title": "Bayesian Calibration of Win Rate Estimation with LLM Evaluators",
    "volume": "main",
    "abstract": "Recent advances in large language models (LLMs) show the potential of using LLMs as evaluators for assessing the quality of text generations from LLMs. However, applying LLM evaluators naively to compare different systems can lead to unreliable results due to the inaccuracy and intrinsic bias of LLM evaluators. In order to mitigate this problem, we propose two calibration methods, Bayesian Win-Rate Sampling (BWRS) and Bayesian Dawid-Skene, both of which leverage Bayesian inference to more accurately infer the true win rate of generative language models. We empirically validate our methods on six datasets covering story generation, summarization, and instruction following tasks. We show that both our methods are effective in improving the accuracy of win rate estimation using LLMs as evaluators, offering a promising direction for reliable automatic text quality evaluation",
    "checked": true,
    "id": "7c17f46b32a533f314979fba2bbd39d51120eac8",
    "semantic_title": "bayesian calibration of win rate estimation with llm evaluators",
    "citation_count": 0,
    "authors": [
      "Yicheng Gao",
      "Gonghan Xu",
      "Zhe Wang",
      "Arman Cohan"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.274": {
    "title": "MuMath-Code: Combining Tool-Use Large Language Models with Multi-perspective Data Augmentation for Mathematical Reasoning",
    "volume": "main",
    "abstract": "The tool-use Large Language Models (LLMs) that integrate with external Python interpreters have significantly enhanced mathematical reasoning capabilities for open-source LLMs, while tool-free methods chose another track: augmenting math reasoning data. However, a great method to integrate the above two research paths and combine their advantages remains to be explored. In this work, we firstly include new math questions via **mu**lti-perspective data augmenting methods and then synthesize **code**-nested solutions to them. The open LLMs (e.g., Llama-2) are finetuned on the augmented dataset to get the resulting models, **MuMath-Code** (𝜇-Math-Code). During the inference phase, our MuMath-Code generates code and interacts with the external python interpreter to get the execution results. Therefore, MuMath-Code leverages the advantages of both the external tool and data augmentation. To fully leverage the advantages of our augmented data, we propose a two-stage training strategy: In Stage-1, we finetune Llama-2 on pure CoT data to get an intermediate model, which then is trained on the code-nested data in Stage-2 to get the resulting MuMath-Code.Our MuMath-Code-7B achieves 83.8% on GSM8K and 52.4% on MATH, while MuMath-Code-70B model achieves new state-of-the-art performance among open methods—achieving 90.7% on GSM8K and 55.1% on MATH. Extensive experiments validate the combination of tool use and data augmentation, as well as our two-stage training strategy.We release the proposed dataset along with the associated code for public use: https://github.com/youweihao-tal/MuMath-Code",
    "checked": true,
    "id": "512a5c307fdab29112a0f4af5c94a3436632eda1",
    "semantic_title": "mumath-code: combining tool-use large language models with multi-perspective data augmentation for mathematical reasoning",
    "citation_count": 3,
    "authors": [
      "Shuo Yin",
      "Weihao You",
      "Zhilong Ji",
      "Guoqiang Zhong",
      "Jinfeng Bai"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.275": {
    "title": "Seeing the Forest through the Trees: Data Leakage from Partial Transformer Gradients",
    "volume": "main",
    "abstract": "Recent studies have shown that distributed machine learning is vulnerable to gradient inversion attacks, where private training data can be reconstructed by analyzing the gradients of the models shared in training. Previous attacks established that such reconstructions are possible using gradients from all parameters in the entire models. However, we hypothesize that most of the involved modules, or even their sub-modules, are at risk of training data leakage, and we validate such vulnerabilities in various intermediate layers of language models. Our extensive experiments reveal that gradients from a single Transformer layer, or even a single linear component with 0.54% parameters, are susceptible to training data leakage. Additionally, we show that applying differential privacy on gradients during training offers limited protection against the novel vulnerability of data disclosure",
    "checked": true,
    "id": "7ad594b57f16d92bed211442275d846e38ee8735",
    "semantic_title": "seeing the forest through the trees: data leakage from partial transformer gradients",
    "citation_count": 0,
    "authors": [
      "Weijun Li",
      "Qiongkai Xu",
      "Mark Dras"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.276": {
    "title": "RWKV-CLIP: A Robust Vision-Language Representation Learner",
    "volume": "main",
    "abstract": "Contrastive Language-Image Pre-training (CLIP) has significantly improved performance in various vision-language tasks by expanding the dataset with image-text pairs obtained from the web. This paper further explores CLIP from the perspectives of data and model architecture. To mitigate the impact of the noise data and enhance the quality of large-scale image-text data crawled from the internet, we introduce a diverse description generation framework that can leverage Large Language Models (LLMs) to combine and refine information from web-based image-text pairs, synthetic captions, and detection tags. Additionally, we propose RWKV-CLIP, the first RWKV-driven vision-language representation learning model that combines the effective parallel training of transformers with the efficient inference of RNNs. Extensive experiments across different model scales and pre-training datasets demonstrate that RWKV-CLIP is a robust vision-language representation learner and it achieves state-of-the-art performance across multiple downstream tasks, including linear probing, zero-shot classification, and zero-shot image-text retrieval. To facilitate future research, the code and pre-trained models are released at https://github.com/deepglint/RWKV-CLIP",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tiancheng Gu",
      "Kaicheng Yang",
      "Xiang An",
      "Ziyong Feng",
      "Dongnan Liu",
      "Weidong Cai",
      "Jiankang Deng"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.277": {
    "title": "KidLM: Advancing Language Models for Children – Early Insights and Future Directions",
    "volume": "main",
    "abstract": "Recent studies highlight the potential of large language models in creating educational tools for children, yet significant challenges remain in maintaining key child-specific properties such as linguistic nuances, cognitive needs, and safety standards. In this paper, we explore foundational steps toward the development of child-specific language models, emphasizing the necessity of high-quality pre-training data. We introduce a novel user-centric data collection pipeline that involves gathering and validating a corpus specifically written for and sometimes by children. Additionally, we propose a new training objective, Stratified Masking, which dynamically adjusts masking probabilities based on our domain-specific child language data, enabling models to prioritize vocabulary and concepts more suitable for children. Experimental evaluations demonstrate that our model excels in understanding lower grade-level text, maintains safety by avoiding stereotypes, and captures children's unique preferences. Furthermore, we provide actionable insights for future research and development in child-specific language modeling",
    "checked": true,
    "id": "c6cdfa6b103980fecfdf185ef7ab2bc6e61625de",
    "semantic_title": "kidlm: advancing language models for children – early insights and future directions",
    "citation_count": 0,
    "authors": [
      "Mir Tafseer Nayeem",
      "Davood Rafiei"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.278": {
    "title": "Using Language Models to Disambiguate Lexical Choices in Translation",
    "volume": "main",
    "abstract": "In translation, a concept represented by a single word in a source language can have multiple variations in a target language. The task of lexical selection requires using context to identify which variation is most appropriate for a source text. We work with native speakers of nine languages to create DTAiLS, a dataset of 1,377 sentence pairs that exhibit cross-lingual concept variation when translating from English. We evaluate recent LLMs and neural machine translation systems on DTAiLS, with the best-performing model, GPT-4, achieving from 67 to 85% accuracy across languages. Finally, we use language models to generate English rules describing target-language concept variations. Providing weaker models with high-quality lexical rules improves accuracy substantially, in some cases reaching or outperforming GPT-4",
    "checked": true,
    "id": "c5f5d92e20fec8f95f2ad394fdeaf87c0e26dc32",
    "semantic_title": "using language models to disambiguate lexical choices in translation",
    "citation_count": 0,
    "authors": [
      "Josh Barua",
      "Sanjay Subramanian",
      "Kayo Yin",
      "Alane Suhr"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.279": {
    "title": "How Does the Disclosure of AI Assistance Affect the Perceptions of Writing?",
    "volume": "main",
    "abstract": "Recent advances in generative AI technologies like large language models have boosted the incorporation of AI assistance in writing workflows, leading to the rise of a new paradigm of human-AI co-creation in writing. To understand how people perceive writings that are produced under this paradigm, in this paper, we conduct an experimental study to understand whether and how the disclosure of the level and type of AI assistance in the writing process would affect people's perceptions of the writing on various aspects, including their evaluation on the quality of the writing, and their ranking of different writings. Our results suggest that disclosing the AI assistance in the writing process, especially if AI has provided assistance in generating new content, decreases the average quality ratings for both argumentative essays and creative stories. This decrease in the average quality ratings often comes with an increased level of variations in different individuals' quality evaluations of the same writing. Indeed, factors such as an individual's writing confidence and familiarity with AI writing assistants are shown to moderate the impact of AI assistance disclosure on their writing quality evaluations. We also find that disclosing the use of AI assistance may significantly reduce the proportion of writings produced with AI's content generation assistance among the top-ranked writings",
    "checked": true,
    "id": "cad94e9b1c8034c0a4bf74b601676f49f7697864",
    "semantic_title": "how does the disclosure of ai assistance affect the perceptions of writing?",
    "citation_count": 0,
    "authors": [
      "Zhuoyan Li",
      "Chen Liang",
      "Jing Peng",
      "Ming Yin"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.280": {
    "title": "An Unsupervised Approach to Achieve Supervised-Level Explainability in Healthcare Records",
    "volume": "main",
    "abstract": "Electronic healthcare records are vital for patient safety as they document conditions, plans, and procedures in both free text and medical codes. Language models have significantly enhanced the processing of such records, streamlining workflows and reducing manual data entry, thereby saving healthcare providers significant resources. However, the black-box nature of these models often leaves healthcare professionals hesitant to trust them. State-of-the-art explainability methods increase model transparency but rely on human-annotated evidence spans, which are costly. In this study, we propose an approach to produce plausible and faithful explanations without needing such annotations. We demonstrate on the automated medical coding task that adversarial robustness training improves explanation plausibility and introduce AttInGrad, a new explanation method superior to previous ones. By combining both contributions in a fully unsupervised setup, we produce explanations of comparable quality, or better, to that of a supervised approach. We release our code and model weights",
    "checked": true,
    "id": "752c99713b95e14b135d6b9a64f0d27e3a82bcf8",
    "semantic_title": "an unsupervised approach to achieve supervised-level explainability in healthcare records",
    "citation_count": 0,
    "authors": [
      "Joakim Edin",
      "Maria Maistro",
      "Lars Maaløe",
      "Lasse Borgholt",
      "Jakob Havtorn",
      "Tuukka Ruotsalo"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.281": {
    "title": "Crafting Personalized Agents through Retrieval-Augmented Generation on Editable Memory Graphs",
    "volume": "main",
    "abstract": "In the age of mobile internet, user data, often referred to as memories, is continuously generated on personal devices. Effectively managing and utilizing this data to deliver services to users is a compelling research topic. In this paper, we introduce a novel task of crafting personalized agents powered by large language models (LLMs), which utilize a user's smartphone memories to enhance downstream applications with advanced LLM capabilities. To achieve this goal, we introduce EMG-RAG, a solution that combines Retrieval-Augmented Generation (RAG) techniques with an Editable Memory Graph (EMG). This approach is further optimized using Reinforcement Learning to address three distinct challenges: data collection, editability, and selectability. Extensive experiments on a real-world dataset validate the effectiveness of EMG-RAG, achieving an improvement of approximately 10% over the best existing approach. Additionally, the personalized agents have been transferred into a real smartphone AI assistant, which leads to enhanced usability",
    "checked": true,
    "id": "6ea7b51bc1c2865d6af95d93df18687a8de16c7a",
    "semantic_title": "crafting personalized agents through retrieval-augmented generation on editable memory graphs",
    "citation_count": 0,
    "authors": [
      "Zheng Wang",
      "Zhongyang Li",
      "Zeren Jiang",
      "Dandan Tu",
      "Wei Shi"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.282": {
    "title": "EVEDIT: Event-based Knowledge Editing for Deterministic Knowledge Propagation",
    "volume": "main",
    "abstract": "The dynamic nature of real-world information necessitates knowledge editing (KE) in large language models (LLMs). The edited knowledge should propagate and facilitate the deduction of new information based on existing model knowledge. We term the existing related knowledge in LLM serving as the origination of knowledge propagation as \"deduction anchors\". However, current KE approaches, which only operate on (subject, relation, object) triple. We both theoretically and empirically observe that this simplified setting often leads to uncertainty when determining the deduction anchors, causing low confidence in their answers. To mitigate this issue, we propose a novel task of event-based knowledge editing that pairs facts with event descriptions. This task manifests not only a closer simulation of real-world editing scenarios but also a more logically sound setting, implicitly defining the deduction anchor and enabling LLMs to propagate knowledge confidently. We curate a new benchmark dataset Evedit derived from the CounterFact dataset and validate its superiority in improving model confidence. Moreover, while we observe that the event-based setting is significantly challenging for existing approaches, we propose a novel approach Self-Edit that showcases stronger performance, achieving 55.6% consistency improvement while maintaining the naturalness of generation",
    "checked": true,
    "id": "62c1360a6ae51942f77d48ca5af9f8c3eedb551f",
    "semantic_title": "evedit: event-based knowledge editing for deterministic knowledge propagation",
    "citation_count": 0,
    "authors": [
      "Jiateng Liu",
      "Pengfei Yu",
      "Yuji Zhang",
      "Sha Li",
      "Zixuan Zhang",
      "Ruhi Sarikaya",
      "Kevin Small",
      "Heng Ji"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.283": {
    "title": "Modeling Nonnative Sentence Processing with L2 Language Models",
    "volume": "main",
    "abstract": "We study LMs pretrained sequentially on two languages (\"L2LMs\") for modeling nonnative sentence processing. In particular, we pretrain GPT2 on 6 different first languages (L1s), followed by English as the second language (L2). We examine the effect of the choice of pretraining L1 on the model's ability to predict human reading times, evaluating on English readers from a range of L1 backgrounds. Experimental results show that, while all of the LMs' word surprisals improve prediction of L2 reading times, especially for human L1s distant from English, there is no reliable effect of the choice of L2LM's L1. We also evaluate the learning trajectory of a monolingual English LM: for predicting L2 as opposed to L1 reading, it peaks much earlier and immediately falls off, possibly mirroring the difference in proficiency between the native and nonnative populations. Lastly, we provide examples of L2LMs' surprisals, which could potentially generate hypotheses about human L2 reading",
    "checked": true,
    "id": "5fcb5b351375f8844cc1978e8b15d5f46cd4adb3",
    "semantic_title": "modeling nonnative sentence processing with l2 language models",
    "citation_count": 0,
    "authors": [
      "Tatsuya Aoyama",
      "Nathan Schneider"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.284": {
    "title": "From the Least to the Most: Building a Plug-and-Play Visual Reasoner via Data Synthesis",
    "volume": "main",
    "abstract": "We explore multi-step reasoning in vision-language models (VLMs). The problem is challenging, as reasoning data consisting of multiple steps of visual and language processing are barely available. To overcome the challenge, we first introduce a least-to-most visual reasoning paradigm, which interleaves steps of decomposing a question into sub-questions and invoking external tools for resolving sub-questions. Based on the paradigm, we further propose a novel data synthesis approach that can automatically create questions and multi-step reasoning paths for an image in a bottom-up manner. Our approach divides the complex synthesis task into a few simple sub-tasks, and (almost entirely) relies on open-sourced models to accomplish the sub-tasks. Therefore, the entire synthesis process is reproducible and cost-efficient, and the synthesized data is quality guaranteed. With the approach, we construct 50k visual reasoning examples. Then, we develop a visual reasoner through supervised fine-tuning, which is capable of generally enhancing the reasoning abilities of a wide range of existing VLMs in a plug-and-play fashion. Extensive experiments indicate that the visual reasoner can consistently and significantly improve four VLMs on four VQA benchmarks",
    "checked": true,
    "id": "9a8dddfd7e9e975a4d3a63174e9a4a07de389c14",
    "semantic_title": "from the least to the most: building a plug-and-play visual reasoner via data synthesis",
    "citation_count": 4,
    "authors": [
      "Chuanqi Cheng",
      "Jian Guan",
      "Wei Wu",
      "Rui Yan"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.285": {
    "title": "Quality Matters: Evaluating Synthetic Data for Tool-Using LLMs",
    "volume": "main",
    "abstract": "Training large language models (LLMs) for external tool usage is a rapidly expanding field, with recent research focusing on generating synthetic data to address the shortage of available data. However, the absence of systematic data quality checks poses complications for properly training and testing models. To that end, we propose two approaches for assessing the reliability of data for training LLMs to use external tools. The first approach uses intuitive, human-defined correctness criteria. The second approach uses a model-driven assessment with in-context evaluation. We conduct a thorough evaluation of data quality on two popular benchmarks, followed by an extrinsic evaluation that showcases the impact of data quality on model performance. Our results demonstrate that models trained on high-quality data outperform those trained on unvalidated data, even when trained with a smaller quantity of data. These findings empirically support the significance of assessing and ensuring the reliability of training data for tool-using LLMs",
    "checked": true,
    "id": "2f31b29e5901ad224cb94e20e833aee9e0e8331e",
    "semantic_title": "quality matters: evaluating synthetic data for tool-using llms",
    "citation_count": 0,
    "authors": [
      "Shadi Iskander",
      "Sofia Tolmach",
      "Ori Shapira",
      "Nachshon Cohen",
      "Zohar Karnin"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.286": {
    "title": "Cross-Domain Audio Deepfake Detection: Dataset and Analysis",
    "volume": "main",
    "abstract": "Audio deepfake detection (ADD) is essential for preventing the misuse of synthetic voices that may infringe on personal rights and privacy. Recent zero-shot text-to-speech (TTS) models pose higher risks as they can clone voices with a single utterance. However, the existing ADD datasets are outdated, leading to suboptimal generalization of detection models. In this paper, we construct a new cross-domain ADD dataset comprising over 300 hours of speech data that is generated by five advanced zero-shot TTS models. To simulate real-world scenarios, we employ diverse attack methods and audio prompts from different datasets. Experiments show that, through novel attack-augmented training, the Wav2Vec2-large and Whisper-medium models achieve equal error rates of 4.1% and 6.5% respectively. Additionally, we demonstrate our models' outstanding few-shot ADD ability by fine-tuning with just one minute of target-domain data. Nonetheless, neural codec compressors greatly affect the detection accuracy, necessitating further research. Our dataset is publicly available (https://github.com/leolya/CD-ADD)",
    "checked": true,
    "id": "cc7abd01eaa38dc7d1d2ea8bdcea60370749048d",
    "semantic_title": "cross-domain audio deepfake detection: dataset and analysis",
    "citation_count": 2,
    "authors": [
      "Yuang Li",
      "Min Zhang",
      "Mengxin Ren",
      "Xiaosong Qiao",
      "Miaomiao Ma",
      "Daimeng Wei",
      "Hao Yang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.287": {
    "title": "MaPPER: Multimodal Prior-guided Parameter Efficient Tuning for Referring Expression Comprehension",
    "volume": "main",
    "abstract": "Referring Expression Comprehension (REC), which aims to ground a local visual region via natural language, is a task that heavily relies on multimodal alignment. Most existing methods utilize powerful pre-trained models to transfer visual/linguistic knowledge by full fine-tuning. However, full fine-tuning the entire backbone not only breaks the rich prior knowledge embedded in the pre-training, but also incurs significant computational costs. Motivated by the recent emergence of Parameter-Efficient Transfer Learning (PETL) methods, we aim to solve the REC task in an effective and efficient manner. Directly applying these PETL methods to the REC task is inappropriate, as they lack the specific-domain abilities for precise local visual perception and visual-language alignment. Therefore, we propose a novel framework of Multimodal Prior-guided Parameter Efficient Tuning, namely MaPPER. Specifically, MaPPER comprises Dynamic Prior Adapters guided by a aligned prior, and Local Convolution Adapters to extract precise local semantics for better visual perception. Moreover, the Prior-Guided Text module is proposed to further utilize the prior for facilitating the cross-modal alignment. Experimental results on three widely-used benchmarks demonstrate that MaPPER achieves the best accuracy compared to the full fine-tuning and other PETL methods with only 1.41% tunable backbone parameters",
    "checked": true,
    "id": "6947191cc21675b3b1c0c61ac06dfa50403b7a7b",
    "semantic_title": "mapper: multimodal prior-guided parameter efficient tuning for referring expression comprehension",
    "citation_count": 0,
    "authors": [
      "Ting Liu",
      "Zunnan Xu",
      "Yue Hu",
      "Liangtao Shi",
      "Zhiqiang Wang",
      "Quanjun Yin"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.288": {
    "title": "Hierarchical Deconstruction of LLM Reasoning: A Graph-Based Framework for Analyzing Knowledge Utilization",
    "volume": "main",
    "abstract": "Despite the advances in large language models (LLMs), how they use their knowledge for reasoning is not yet well understood.In this study, we propose a method that deconstructs complex real-world questions into a graph, representing each question as a node with predecessors of background knowledge needed to solve the question. We develop the DepthQA dataset, deconstructing questions into three depths: (i) recalling conceptual knowledge, (ii) applying procedural knowledge, and (iii) analyzing strategic knowledge. Based on a hierarchical graph, we quantify forward discrepancy, a discrepancy in LLM performance on simpler sub-problems versus complex questions. We also measure backward discrepancy where LLMs answer complex questions but struggle with simpler ones. Our analysis shows that smaller models exhibit more discrepancies than larger models. Distinct patterns of discrepancies are observed across model capacity and possibility of training data memorization. Additionally, guiding models from simpler to complex questions through multi-turn interactions improves performance across model sizes, highlighting the importance of structured intermediate steps in knowledge reasoning. This work enhances our understanding of LLM reasoning and suggests ways to improve their problem-solving abilities",
    "checked": true,
    "id": "a814b096f45746626cf53ebf146d232e24924edc",
    "semantic_title": "hierarchical deconstruction of llm reasoning: a graph-based framework for analyzing knowledge utilization",
    "citation_count": 1,
    "authors": [
      "Miyoung Ko",
      "Sue Park",
      "Joonsuk Park",
      "Minjoon Seo"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.289": {
    "title": "Aligning Translation-Specific Understanding to General Understanding in Large Language Models",
    "volume": "main",
    "abstract": "Large Language models (LLMs) have exhibited remarkable abilities in understanding complex texts, offering a promising path towards human-like translation performance. However, this study reveals the misalignment between the translation-specific understanding and the general understanding inside LLMs. This understanding misalignment leads to LLMs mistakenly or literally translating some complicated concepts that they accurately comprehend in the general scenarios (e.g., QA). To align the translation-specific understanding to the general one, we propose a novel translation process, DUAT (Difficult words Understanding Aligned Translation), explicitly incorporating the general understanding on the complicated content incurring inconsistent understandings to guide the translation. Specifically, DUAT performs cross-lingual interpretation for the difficult-to-translate words and enhances the translation with the generated interpretations. Furthermore, we reframe the external tools to improve DUAT in detecting difficult words and generating helpful interpretations. We conduct experiments on the self-constructed benchmark Challenge-WMT, consisting of samples that are prone to mistranslation. Human evaluation results on high-resource and low-resource language pairs indicate that DUAT significantly facilitates the understanding alignment, which improves the translation quality (up to +3.85 COMET) and reduces translation literalness by -25% ∼ -51%",
    "checked": true,
    "id": "a23a89855e3af2e6cec7fd4a01e12cacdf6c727f",
    "semantic_title": "aligning translation-specific understanding to general understanding in large language models",
    "citation_count": 0,
    "authors": [
      "Yichong Huang",
      "Baohang Li",
      "Xiaocheng Feng",
      "Wenshuai Huo",
      "Chengpeng Fu",
      "Ting Liu",
      "Bing Qin"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.290": {
    "title": "FOOL ME IF YOU CAN! An Adversarial Dataset to Investigate the Robustness of LMs in Word Sense Disambiguation",
    "volume": "main",
    "abstract": "Word sense disambiguation (WSD) is a key task in natural language processing and lexical semantics. Pre-trained language models with contextualized word embeddings have significantly improved performance in regular WSD tasks. However, these models still struggle with recognizing semantic boundaries and often misclassify homonyms in adversarial context. Therefore, we propose FOOL: FOur-fold Obscure Lexical, a new coarse-grained WSD dataset, which includes four different test sets designed to assess the robustness of language models in WSD tasks. Two sets feature typical WSD scenarios, while the other two include sentences with opposing contexts to challenge the models further.We tested two types of models on the proposed dataset: models with encoders, such as the BERT and T5 series of varying sizes by probing their embeddings, and state-of-the-art large decoder models like GPT-4o and the LlaMA3 family, using zero shot prompting. Across different state-of-the-art language models, we observed a decrease in performance in the latter two sets compared to the first two, with some models being affected more than others. We show interesting findings where small models like T5-large and BERT-large performed better than GPT-4o on Set 3 of the dataset. This indicates that, despite excelling in regular WSD tasks, these models still struggle to correctly disambiguate homonyms in artificial (Set 3) or realistic adversarial contexts (Set 4)",
    "checked": true,
    "id": "7d97f9490a312d9d803799e95d5caf635ea4a3c6",
    "semantic_title": "fool me if you can! an adversarial dataset to investigate the robustness of lms in word sense disambiguation",
    "citation_count": 0,
    "authors": [
      "Mohamad Ballout",
      "Anne Dedert",
      "Nohayr Abdelmoneim",
      "Ulf Krumnack",
      "Gunther Heidemann",
      "Kai-Uwe Kühnberger"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.291": {
    "title": "Concept-skill Transferability-based Data Selection for Large Vision-Language Models",
    "volume": "main",
    "abstract": "Instruction tuning, or supervised finetuning on extensive task-specific data, is necessary for Large Vision-Language Models (LVLMs) to generalize well across a broad range of vision-language (VL) tasks. However, training on large VL datasets can become prohibitively expensive. In this work, we introduce COINCIDE, an effective and scalable data selection technique that uses a small model as a reference model to select visual instruction tuning data for efficient finetuning of a target LVLM, focusing on diversity and transferability. Specifically, we cluster the training data using internal activations from a small model, which identifies VL concept-skill compositions needed by a target LVLM. We then sample data from these diverse clusters by considering their density and transferability, or the ability to transfer well to other concept-skill compositions. This approach ensures the diversity of these compositions, which is vital for LVLM generalization. Extensive experiments demonstrate that COINCIDE achieves superior performance and data selection efficiency against 8 strong baselines on two distinct datasets: LLaVA-1.5 and Vision-Flan. Using only 20% of the LLaVA-1.5 dataset, COINCIDE achieves performance comparable to the LVLM finetuned on the whole dataset, with 70% reduction of the wall-clock running time. On the Vision-Flan dataset, our method achieves superior results with only 16.7% of the training data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaewoo Lee",
      "Boyang Li",
      "Sung Ju Hwang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.292": {
    "title": "LLMs Assist NLP Researchers: Critique Paper (Meta-)Reviewing",
    "volume": "main",
    "abstract": "Claim: This work is not advocating the use of LLMs for paper (meta-)reviewing. Instead, wepresent a comparative analysis to identify and distinguish LLM activities from human activities. Two research goals: i) Enable better recognition of instances when someone implicitly uses LLMs for reviewing activities; ii) Increase community awareness that LLMs, and AI in general, are currently inadequate for performing tasks that require a high level of expertise and nuanced judgment.This work is motivated by two key trends. On one hand, large language models (LLMs) have shown remarkable versatility in various generative tasks such as writing, drawing, and question answering, significantly reducing the time required for many routine tasks. On the other hand, researchers, whose work is not only time-consuming but also highly expertise-demanding, face increasing challenges as they have to spend more time reading, writing, and reviewing papers. This raises the question: how can LLMs potentially assist researchers in alleviating their heavy workload?This study focuses on the topic of LLMs as NLP Researchers, particularly examining the effectiveness of LLMs in assisting paper (meta-)reviewing and its recognizability. To address this, we constructed the ReviewCritique dataset, which includes two types of information: (i) NLP papers (initial submissions rather than camera-ready) with both human-written and LLM-generated reviews, and (ii) each review comes with \"deficiency\" labels and corresponding explanations for individual segments, annotated by experts. Using ReviewCritique, this study explores two threads of research questions: (i) \"LLMs as Reviewers\", how do reviews generated by LLMs compare with those written by humans in terms of quality and distinguishability? (ii) \"LLMs as Metareviewers\", how effectively can LLMs identify potential issues, such as Deficient or unprofessional review segments, within individual paper reviews? To our knowledge, this is the first work to provide such a comprehensive analysis",
    "checked": true,
    "id": "f2c7f69177d7972510e7a026c7fa20c26905a3df",
    "semantic_title": "llms assist nlp researchers: critique paper (meta-)reviewing",
    "citation_count": 6,
    "authors": [
      "Jiangshu Du",
      "Yibo Wang",
      "Wenting Zhao",
      "Zhongfen Deng",
      "Shuaiqi Liu",
      "Renze Lou",
      "Henry Zou",
      "Pranav Narayanan Venkit",
      "Nan Zhang",
      "Mukund Srinath",
      "Haoran Zhang",
      "Vipul Gupta",
      "Yinghui Li",
      "Tao Li",
      "Fei Wang",
      "Qin Liu",
      "Tianlin Liu",
      "Pengzhi Gao",
      "Congying Xia",
      "Chen Xing",
      "Cheng Jiayang",
      "Zhaowei Wang",
      "Ying Su",
      "Raj Shah",
      "Ruohao Guo",
      "Jing Gu",
      "Haoran Li",
      "Kangda Wei",
      "Zihao Wang",
      "Lu Cheng",
      "Surangika Ranathunga",
      "Meng Fang",
      "Jie Fu",
      "Fei Liu",
      "Ruihong Huang",
      "Eduardo Blanco",
      "Yixin Cao",
      "Rui Zhang",
      "Philip Yu",
      "Wenpeng Yin"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.293": {
    "title": "Academics Can Contribute to Domain-Specialized Language Models",
    "volume": "main",
    "abstract": "Commercially available models dominate academic leaderboards. While impressive, this has concentrated research on creating and adapting general-purpose models to improve NLP leaderboard standings for large language models. However, leaderboards collect many individual tasks and general-purpose models often underperform in specialized domains; domain-specific or adapted models yield superior results. This focus on large general-purpose models excludes many academics and draws attention away from areas where they can make important contributions. We advocate for a renewed focus on developing and evaluating domain- and task-specific models, and highlight the unique role of academics in this endeavor",
    "checked": true,
    "id": "2eed49b6661e094789257ea31b418c9aff4b6138",
    "semantic_title": "academics can contribute to domain-specialized language models",
    "citation_count": 0,
    "authors": [
      "Mark Dredze",
      "Genta Winata",
      "Prabhanjan Kambadur",
      "Shijie Wu",
      "Ozan Irsoy",
      "Steven Lu",
      "Vadim Dabravolski",
      "David Rosenberg",
      "Sebastian Gehrmann"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.294": {
    "title": "Beyond Reference: Evaluating High Quality Translations Better than Human References",
    "volume": "main",
    "abstract": "In Machine Translation (MT) evaluations, the conventional approach is to compare a translated sentence against its human-created reference sentence. MT metrics provide an absolute score (e.g., from 0 to 1) to a candidate sentence based on the similarity with the reference sentence. Thus, existing MT metrics give the maximum score to the reference sentence. However, this approach overlooks the potential for a candidate sentence to exceed the reference sentence in terms of quality. In particular, recent advancements in Large Language Models (LLMs) have highlighted this issue, as LLM-generated sentences often exceed the quality of human-written sentences. To address the problem, we introduce the Residual score Metric (ResuMe), which evaluates the relative quality between reference and candidate sentences. ResuMe assigns a positive score to candidate sentences that outperform their reference sentences, and a negative score when they fall short. By adding the residual scores from ResuMe to the absolute scores from MT metrics, it can be possible to allocate higher scores to candidate sentences than what reference sentences are received from MT metrics. Experimental results demonstrate that ResuMe enhances the alignments between MT metrics and human judgments both at the segment-level and the system-level",
    "checked": true,
    "id": "2b981fc8405b6de300601196e10e4192f61764bf",
    "semantic_title": "beyond reference: evaluating high quality translations better than human references",
    "citation_count": 0,
    "authors": [
      "Keonwoong Noh",
      "Seokjin Oh",
      "Woohwan Jung"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.295": {
    "title": "Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement",
    "volume": "main",
    "abstract": "Large language models (LLMs) demonstrate exceptional instruct-following ability to complete various downstream tasks. Although this impressive ability makes LLMs flexible task solvers, their performance in solving tasks also heavily relies on instructions. In this paper, we reveal that LLMs are over-sensitive to lexical variations in task instructions, even when the variations are imperceptible to humans. By providing models with neighborhood instructions, which are closely situated in the latent representation space and differ by only one semantically similar word, the performance on downstream tasks can be vastly different. Following this property, we propose a black-box Combinatorial Optimization framework for Prompt Lexical Enhancement (COPLE). COPLE performs iterative lexical optimization according to the feedback from a batch of proxy tasks, using a search strategy related to word influence. Experiments show that even widely-used human-crafted prompts for current benchmarks suffer from the lexical sensitivity of models, and COPLE recovers the declined model ability in both instruct-following and solving downstream tasks",
    "checked": true,
    "id": "2d77b7203824e617206634277bce7eec2b71a2bd",
    "semantic_title": "unveiling the lexical sensitivity of llms: combinatorial optimization for prompt enhancement",
    "citation_count": 3,
    "authors": [
      "Pengwei Zhan",
      "Zhen Xu",
      "Qian Tan",
      "Jie Song",
      "Ru Xie"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.296": {
    "title": "SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages",
    "volume": "main",
    "abstract": "Southeast Asia (SEA) is a region rich in linguistic diversity and cultural variety, with over 1,300 indigenous languages and a population of 671 million people. However, prevailing AI models suffer from a significant lack of representation of texts, images, and audio datasets from SEA, compromising the quality of AI models for SEA languages. Evaluating models for SEA languages is challenging due to the scarcity of high-quality datasets, compounded by the dominance of English training data, raising concerns about potential cultural misrepresentation. To address these challenges, through a collaborative movement, we introduce SEACrowd, a comprehensive resource center that fills the resource gap by providing standardized corpora in nearly 1,000 SEA languages across three modalities. Through our SEACrowd benchmarks, we assess the quality of AI models on 36 indigenous languages across 13 tasks, offering valuable insights into the current AI landscape in SEA. Furthermore, we propose strategies to facilitate greater AI advancements, maximizing potential utility and resource equity for the future of AI in Southeast Asia",
    "checked": true,
    "id": "ba5284674face6cca3b678fd7a82d691ec29349b",
    "semantic_title": "seacrowd: a multilingual multimodal data hub and benchmark suite for southeast asian languages",
    "citation_count": 3,
    "authors": [
      "Holy Lovenia",
      "Rahmad Mahendra",
      "Salsabil Akbar",
      "Lester James Miranda",
      "Jennifer Santoso",
      "Elyanah Aco",
      "Akhdan Fadhilah",
      "Jonibek Mansurov",
      "Joseph Marvin Imperial",
      "Onno Kampman",
      "Joel Moniz",
      "Muhammad Habibi",
      "Frederikus Hudi",
      "Jann Montalan",
      "Ryan Hadiwijaya",
      "Joanito Lopo",
      "William Nixon",
      "Börje Karlsson",
      "James Jaya",
      "Ryandito Diandaru",
      "Yuze Gao",
      "Patrick Irawan",
      "Bin Wang",
      "Jan Christian Blaise Cruz",
      "Chenxi Whitehouse",
      "Ivan Parmonangan",
      "Maria Khelli",
      "Wenyu Zhang",
      "Lucky Susanto",
      "Reynard Ryanda",
      "Sonny Hermawan",
      "Dan Velasco",
      "Muhammad Kautsar",
      "Willy Hendria",
      "Yasmin Moslem",
      "Noah Flynn",
      "Muhammad Adilazuarda",
      "Haochen Li",
      "Johanes Lee",
      "R. Damanhuri",
      "Shuo Sun",
      "Muhammad Qorib",
      "Amirbek Djanibekov",
      "Wei Qi Leong",
      "Quyet V. Do",
      "Niklas Muennighoff",
      "Tanrada Pansuwan",
      "Ilham Firdausi Putra",
      "Yan Xu",
      "Tai Chia",
      "Ayu Purwarianti",
      "Sebastian Ruder",
      "William Tjhi",
      "Peerat Limkonchotiwat",
      "Alham Aji",
      "Sedrick Keh",
      "Genta Winata",
      "Ruochen Zhang",
      "Fajri Koto",
      "Zheng Xin Yong",
      "Samuel Cahyawijaya"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.297": {
    "title": "Induct-Learn: Short Phrase Prompting with Instruction Induction",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have demonstrated capability in \"instruction induction,\" generating instructions from demonstrations (input-output pairs). However, existing methods often rely on large datasets or numerous examples, which is impractical and costly in real-world scenarios. In this work, we propose a low-cost, task-level framework called Induct-Learn. It induces pseudo instructions from a few demonstrations and a short phrase, adding a CoT process into existing demonstrations. When encountering new problems, the learned pseudo instructions and demonstrations with the pseudo CoT process can be combined into a prompt to guide the LLM's problem-solving process. We validate our approach on the BBH-Induct and Evals-Induct datasets, and the results show that the Induct-Learn framework outperforms state-of-the-art methods. We also exhibit cross-model adaptability and achieve superior performance at a lower cost compared to existing methods",
    "checked": true,
    "id": "468a16eca376b7ef445b671784261de29d354840",
    "semantic_title": "induct-learn: short phrase prompting with instruction induction",
    "citation_count": 0,
    "authors": [
      "Po-Chun Chen",
      "Sheng-Lun Wei",
      "Hen-Hsen Huang",
      "Hsin-Hsi Chen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.298": {
    "title": "Multi-Granularity History and Entity Similarity Learning for Temporal Knowledge Graph Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "d9cbe6cdf9bc397d53f196b5846966960d4336ad",
    "semantic_title": "multi-granularity history and entity similarity learning for temporal knowledge graph reasoning",
    "citation_count": 0,
    "authors": [
      "Shi Mingcong",
      "Chunjiang Zhu",
      "Detian Zhang",
      "Shiting Wen",
      "Li Qing"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.299": {
    "title": "LUQ: Long-text Uncertainty Quantification for LLMs",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capability in a variety of NLP tasks. However, LLMs are also prone to generate nonfactual content. Uncertainty Quantification (UQ) is pivotal in enhancing our understanding of a model's confidence on its generation, thereby aiding in the mitigation of nonfactual outputs. Existing research on UQ predominantly targets short text generation, typically yielding brief, word-limited responses. However, real-world applications frequently necessitate much longer responses. Our study first highlights the limitations of current UQ methods in handling long text generation. We then introduce Luq and its two variations, a series of novel sampling-based UQ approaches specifically designed for long text. Our findings reveal that Luq outperforms existing baseline methods in correlating with the model's factuality scores (negative coefficient of -0.85 observed for Gemini Pro). To further improve the factuality of LLM responses, we propose Luq-Ensemble, a method that ensembles responses from multiple models and selects the response with the lowest uncertainty. The ensembling method greatly improves the response factuality upon the best standalone LLM",
    "checked": true,
    "id": "e7119a3366d4724c11f041306b3f1b9d4b9080f4",
    "semantic_title": "luq: long-text uncertainty quantification for llms",
    "citation_count": 11,
    "authors": [
      "Caiqi Zhang",
      "Fangyu Liu",
      "Marco Basaldella",
      "Nigel Collier"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.300": {
    "title": "Pretraining Data Detection for Large Language Models: A Divergence-based Calibration Method",
    "volume": "main",
    "abstract": "As the scale of training corpora for large language models (LLMs) grows, model developers become increasingly reluctant to disclose details on their data. This lack of transparency poses challenges to scientific evaluation and ethical deployment. Recently, pretraining data detection approaches, which infer whether a given text was part of an LLM's training data through black-box access, have been explored. The Min-K% Prob method, which has achieved state-of-the-art results, assumes that a non-training example tends to contain a few outlier words with low token probabilities. However, the effectiveness may be limited as it tends to misclassify non-training texts that contain many common words with high probabilities predicted by LLMs. To address this issue, we introduce a divergence-based calibration method, inspired by the divergence-from-randomness concept, to calibrate token probabilities for pretraining data detection. We compute the cross-entropy (i.e., the divergence) between the token probability distribution and the token frequency distribution to derive a detection score.We have developed a Chinese-language benchmark, PatentMIA, to assess the performance of detection approaches for LLMs on Chinese text. Experimental results on English-language benchmarks and PatentMIA demonstrate that our proposed method significantly outperforms existing methods. Our code and PatentMIA benchmark are available at https://github.com/zhang-wei-chao/DC-PDD",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weichao Zhang",
      "Ruqing Zhang",
      "Jiafeng Guo",
      "Maarten Rijke",
      "Yixing Fan",
      "Xueqi Cheng"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.301": {
    "title": "Scaling Synthetic Logical Reasoning Datasets with Context-Sensitive Declarative Grammars",
    "volume": "main",
    "abstract": "Logical reasoning remains a challenge for natural language processing, but it can be improved by training language models to mimic theorem provers on procedurally generated problems. Previous work used domain-specific proof generation algorithms, which biases reasoning toward specific proof traces and limits auditability and extensibility. We present a simpler and more general declarative framework with flexible context-sensitive rules binding multiple languages (specifically, simplified English and the TPTP theorem-proving language). We construct first-order logic problems by selecting up to 32 premises and one hypothesis. We demonstrate that using semantic constraints during generation and careful English verbalization of predicates enhances logical reasoning without hurting natural English tasks. Using relatively small DeBERTa-v3 models, we achieve state-of-the-art accuracy on the FOLIO human-authored logic dataset, surpassing GPT-4 in accuracy with or without an external solver by 12%",
    "checked": true,
    "id": "c80457a0579a0dcab914d01ae343c346210cf4e1",
    "semantic_title": "scaling synthetic logical reasoning datasets with context-sensitive declarative grammars",
    "citation_count": 0,
    "authors": [
      "Damien Sileo"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.302": {
    "title": "Improving Spoken Language Modeling with Phoneme Classification: A Simple Fine-tuning Approach",
    "volume": "main",
    "abstract": "Recent progress in Spoken Language Modeling has shown that learning language directly from speech is feasible. Generating speech through a pipeline that operates at the text level typically loses nuances, intonations, and non-verbal vocalizations. Modeling directly from speech opens up the path to more natural and expressive systems. On the other hand, speech-only systems require up to three orders of magnitude more data to catch up to their text-based counterparts in terms of their semantic abilities. We show that fine-tuning speech representation models on phoneme classification leads to more context-invariant representations, and language models trained on these units achieve comparable lexical comprehension to ones trained on hundred times more data",
    "checked": true,
    "id": "e23459636393552d37f54714a0beaa1cb6a5631f",
    "semantic_title": "improving spoken language modeling with phoneme classification: a simple fine-tuning approach",
    "citation_count": 0,
    "authors": [
      "Maxime Poli",
      "Emmanuel Chemla",
      "Emmanuel Dupoux"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.303": {
    "title": "Safely Learning with Private Data: A Federated Learning Framework for Large Language Model",
    "volume": "main",
    "abstract": "Private data, being larger and quality-higher than public data, can greatly improve large language models (LLM). However, due to privacy concerns, this data is often dispersed in multiple silos, making its secure utilization for LLM training a challenge. Federated learning (FL) is an ideal solution for training models with distributed private data, but traditional frameworks like FedAvg are unsuitable for LLM due to their high computational demands on clients. An alternative, split learning, offloads most training parameters to the server while training embedding and output layers locally, making it more suitable for LLM. Nonetheless, it faces significant challenges in security and efficiency. Firstly, the gradients of embeddings are prone to attacks, leading to potential reverse engineering of private data. Furthermore, the server's limitation of handling only one client's training request at a time hinders parallel training, severely impacting training efficiency. In this paper, we propose a Federated Learning framework for LLM, named FL-GLM, which prevents data leakage caused by both server-side and peer-client attacks while improving training efficiency. Specifically, we first place the input block and output block on local client to prevent embedding gradient attacks from server. Secondly, we employ key-encryption during client-server communication to prevent reverse engineering attacks from peer-clients. Lastly, we employ optimization methods like client-batching or server-hierarchical, adopting different acceleration methods based on the actual computational capabilities of the server. Experimental results on NLU and generation tasks demonstrate that FL-GLM achieves comparable metrics to centralized chatGLM model, validating the effectiveness of our federated learning framework",
    "checked": true,
    "id": "8699ca2562722a986ffaea99cc6c94b39567f97b",
    "semantic_title": "safely learning with private data: a federated learning framework for large language model",
    "citation_count": 0,
    "authors": [
      "Jia-Ying Zheng",
      "Hainan Zhang",
      "Lingxiang Wang",
      "Wangjie Qiu",
      "Hong-Wei Zheng",
      "Zhi-Ming Zheng"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.304": {
    "title": "Formality is Favored: Unraveling the Learning Preferences of Large Language Models on Data with Conflicting Knowledge",
    "volume": "main",
    "abstract": "Having been trained on massive pretraining data, large language models have shown excellent performance on many knowledge-intensive tasks. However, pretraining data tends to contain misleading and even conflicting information, and it is intriguing to understand how LLMs handle these noisy data during training. In this study, we systematically analyze LLMs' learning preferences for data with conflicting knowledge. We find that pretrained LLMs establish learning preferences similar to humans, i.e., preferences towards formal texts and texts with fewer spelling errors, resulting in faster learning and more favorable treatment of knowledge in data with such features when facing conflicts. This finding is generalizable across models and languages and is more evident in larger models. An in-depth analysis reveals that LLMs tend to trust data with features that signify consistency with the majority of data, and it is possible to instill new preferences and erase old ones by manipulating the degree of consistency with the majority data",
    "checked": true,
    "id": "efe89dc9fd6bc113460ba886a7a14c82c8da1d38",
    "semantic_title": "formality is favored: unraveling the learning preferences of large language models on data with conflicting knowledge",
    "citation_count": 0,
    "authors": [
      "Jiahuan Li",
      "Yiqing Cao",
      "Shujian Huang",
      "Jiajun Chen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.305": {
    "title": "How Does the Textual Information Affect the Retrieval of Multimodal In-Context Learning?",
    "volume": "main",
    "abstract": "The increase in parameter size of multimodal large language models (MLLMs) introduces significant capabilities, particularly multimodal in-context learning, where MLLMs enhance task performance without updating pre-trained parameters. However, this effectiveness hinges on the appropriate selection of in-context examples, a process currently biased towards visual data, overlooking textual information. More importantly, the area of supervised retrievers for retrieval of multimodal in-context learning, crucial for optimal in-context example selection, continues to be investigated. Our study provides an in-depth evaluation of the impact of textual information on the unsupervised selection of in-context examples in multimodal contexts, uncovering a notable sensitivity of retriever performance to the employed modalities. Based on the above finding, we introduce a novel supervised MLLM prompt retriever MSIER that leverages a trained retriever based on MLLM's confidence to select examples, which enhances multimodal in-context learning efficiency. This approach is validated through extensive testing across three different tasks, demonstrating the method's effectiveness. Additionally, we investigate the influence of modalities on our supervised retrieval method's training and explore the transferability of the supervised prompt retriever. This exploration paves the way for future advancements, highlighting the potential for refined in-context learning in MLLMs through the strategic use of multimodal data. The public code is available at https://github.com/NUS-HPC-AI-Lab/Multimodal-ICL-Retriever",
    "checked": true,
    "id": "2e7faac64082325e70c28b054d4e7f7cd7e50592",
    "semantic_title": "how does the textual information affect the retrieval of multimodal in-context learning?",
    "citation_count": 2,
    "authors": [
      "Yang Luo",
      "Zangwei Zheng",
      "Zirui Zhu",
      "Yang You"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.306": {
    "title": "How Far Can We Extract Diverse Perspectives from Large Language Models?",
    "volume": "main",
    "abstract": "Collecting diverse human opinions is costly and challenging. This leads to a recent trend in exploiting large language models (LLMs) for generating diverse data for potential scalable and efficient solutions. However, the extent to which LLMs can generate diverse perspectives on subjective topics is still unclear. In this study, we explore LLMs' capacity of generating diverse perspectives and rationales on subjective topics such as social norms and argumentative texts. We introduce the problem of extracting maximum diversity from LLMs. Motivated by how humans form opinions based on values, we propose a criteria-based prompting technique to ground diverse opinions. To see how far we can extract diverse perspectives from LLMs, or called diversity coverage, we employ a step-by-step recall prompting to generate more outputs from the model iteratively. Our methods, applied to various tasks, show that LLMs can indeed produce diverse opinions according to the degree of task subjectivity. We also find that LLMs performance of extracting maximum diversity is on par with human",
    "checked": true,
    "id": "56e7bda25b83228f91962d3465fd587cfe8908e1",
    "semantic_title": "how far can we extract diverse perspectives from large language models?",
    "citation_count": 7,
    "authors": [
      "Shirley Hayati",
      "Minhwa Lee",
      "Dheeraj Rajagopal",
      "Dongyeop Kang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.307": {
    "title": "EXPLORA: Efficient Exemplar Subset Selection for Complex Reasoning",
    "volume": "main",
    "abstract": "Answering reasoning-based complex questions over text and hybrid sources, including tables, is a challenging task. Recent advances in large language models (LLMs) have enabled in-context learning (ICL), allowing LLMs to acquire proficiency in a specific task using only a few demonstration samples (exemplars). A critical challenge in ICL is the selection of optimal exemplars, which can be either task-specific (static) or test-example-specific (dynamic). Static exemplars provide faster inference times and increased robustness across a distribution of test examples. In this paper, we propose an algorithm for static exemplar subset selection for complex reasoning tasks. We introduce EXPLORA, a novel exploration method designed to estimate the parameters of the scoring function, which evaluates exemplar subsets without incorporating confidence information. EXPLORA significantly reduces the number of LLM calls to ~11% of those required by state-of-the-art methods and achieves a substantial performance improvement of 12.24%. We open-source our code and data (https://github.com/kiranpurohit/EXPLORA)",
    "checked": true,
    "id": "fa3497822d420a12b29c1734025c8f9fc3dbca87",
    "semantic_title": "explora: efficient exemplar subset selection for complex reasoning",
    "citation_count": 0,
    "authors": [
      "Kiran Purohit",
      "Venktesh V",
      "Raghuram Devalla",
      "Krishna Yerragorla",
      "Sourangshu Bhattacharya",
      "Avishek Anand"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.308": {
    "title": "An LLM Feature-based Framework for Dialogue Constructiveness Assessment",
    "volume": "main",
    "abstract": "Research on dialogue constructiveness assessment focuses on (i) analysing conversational factors that influence individuals to take specific actions, win debates, change their perspectives or broaden their open-mindedness and (ii) predicting constructiveness outcomes following dialogues for such use cases. These objectives can be achieved by training either interpretable feature-based models (which often involve costly human annotations) or neural models such as pre-trained language models (which have empirically shown higher task accuracy but lack interpretability). In this paper we propose an LLM feature-based framework for dialogue constructiveness assessment that combines the strengths of feature-based and neural approaches, while mitigating their downsides. The framework first defines a set of dataset-independent and interpretable linguistic features, which can be extracted by both prompting an LLM and simple heuristics. Such features are then used to train LLM feature-based models. We apply this framework to three datasets of dialogue constructiveness and find that our LLM feature-based models outperform or performs at least as well as standard feature-based models and neural models. We also find that the LLM feature-based model learns more robust prediction rules instead of relying on superficial shortcuts, which often trouble neural models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lexin Zhou",
      "Youmna Farag",
      "Andreas Vlachos"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.309": {
    "title": "Relevance Is a Guiding Light: Relevance-aware Adaptive Learning for End-to-end Task-oriented Dialogue System",
    "volume": "main",
    "abstract": "Retrieving accurate domain knowledge and providing helpful information are crucial in developing an effective end-to-end task-oriented dialogue system (E2ETOD). The field has witnessed numerous methods following the retrieve-then-generate paradigm and training their systems on one specific domain. However, existing approaches still suffer from the Distractive Attributes Problem (DAP): struggling to deal with false but similar knowledge (hard negative entities), which is even more intractable when countless pieces of knowledge from different domains are blended in a real-world scenario. To alleviate DAP, we propose the Relevance-aware Adaptive Learning (ReAL) method, a two-stage training framework that eliminates hard negatives step-by-step and aligns retrieval with generation. In the first stage, we introduce a top-k adaptive contrastive loss and utilize the divergence-driven feedback from the frozen generator to pre-train the retriever. In the second stage, we propose using the metric score distribution as an anchor to align retrieval with generation. Thorough experiments on three benchmark datasets demonstrate ReAL's superiority over existing methods, with extensive analysis validating its strong capabilities of overcoming in- and cross-domain distractions",
    "checked": true,
    "id": "e7a539be7c279b8163b18a813b7c3cf5266f23bf",
    "semantic_title": "relevance is a guiding light: relevance-aware adaptive learning for end-to-end task-oriented dialogue system",
    "citation_count": 0,
    "authors": [
      "Zhanpeng Chen",
      "Zhihong Zhu",
      "Wanshi Xu",
      "Xianwei Zhuang",
      "Yuexian Zou"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.310": {
    "title": "Dialog2Flow: Pre-training Soft-Contrastive Action-Driven Sentence Embeddings for Automatic Dialog Flow Extraction",
    "volume": "main",
    "abstract": "Efficiently deriving structured workflows from unannotated dialogs remains an underexplored and formidable challenge in computational linguistics. Automating this process could significantly accelerate the manual design of workflows in new domains and enable the grounding of large language models in domain-specific flowcharts, enhancing transparency and controllability.In this paper, we introduce Dialog2Flow (D2F) embeddings, which differ from conventional sentence embeddings by mapping utterances to a latent space where they are grouped according to their communicative and informative functions (i.e., the actions they represent). D2F allows for modeling dialogs as continuous trajectories in a latent space with distinct action-related regions. By clustering D2F embeddings, the latent space is quantized, and dialogs can be converted into sequences of region/action IDs, facilitating the extraction of the underlying workflow.To pre-train D2F, we build a comprehensive dataset by unifying twenty task-oriented dialog datasets with normalized per-turn action annotations. We also introduce a novel soft contrastive loss that leverages the semantic information of these actions to guide the representation learning process, showing superior performance compared to standard supervised contrastive loss.Evaluation against various sentence embeddings, including dialog-specific ones, demonstrates that D2F yields superior qualitative and quantitative results across diverse domains",
    "checked": true,
    "id": "b1c4383ee62ca14e539450bd30f2ee4a900aadc7",
    "semantic_title": "dialog2flow: pre-training soft-contrastive action-driven sentence embeddings for automatic dialog flow extraction",
    "citation_count": 0,
    "authors": [
      "Sergio Burdisso",
      "Srikanth Madikeri",
      "Petr Motlicek"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.311": {
    "title": "Words Worth a Thousand Pictures: Measuring and Understanding Perceptual Variability in Text-to-Image Generation",
    "volume": "main",
    "abstract": "Diffusion models are the state of the art in text-to-image generation, but their perceptual variability remains understudied. In this paper, we examine how prompts affect image variability in black-box diffusion-based models. We propose W1KP, a human-calibrated measure of variability in a set of images, bootstrapped from existing image-pair perceptual distances. Current datasets do not cover recent diffusion models, thus we curate three test sets for evaluation. Our best perceptual distance outperforms nine baselines by up to 18 points in accuracy, and our calibration matches graded human judgements 78% of the time. Using W1KP, we study prompt reusability and show that Imagen prompts can be reused for 10-50 random seeds before new images become too similar to already generated images, while Stable Diffusion XL and DALL-E 3 can be reused 50-200 times. Lastly, we analyze 56 linguistic features of real prompts, finding that the prompt's length, CLIP embedding norm, concreteness, and word senses influence variability most. As far as we are aware, we are the first to analyze diffusion variability from a visuolinguistic perspective. Our project page is at http://w1kp.com",
    "checked": true,
    "id": "43d6919485cd520304b4c3572a3828278ea730e4",
    "semantic_title": "words worth a thousand pictures: measuring and understanding perceptual variability in text-to-image generation",
    "citation_count": 0,
    "authors": [
      "Raphael Tang",
      "Crystina Zhang",
      "Lixinyu Xu",
      "Yao Lu",
      "Wenyan Li",
      "Pontus Stenetorp",
      "Jimmy Lin",
      "Ferhan Ture"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.312": {
    "title": "Investigating LLMs as Voting Assistants via Contextual Augmentation: A Case Study on the European Parliament Elections 2024",
    "volume": "main",
    "abstract": "In light of the recent 2024 European Parliament elections, we are investigating if LLMs can be used as Voting Advice Applications (VAAs). We audit MISTRAL and MIXTRAL models and evaluate their accuracy in predicting the stance of political parties based on the latest \"EU and I\" voting assistance questionnaire. Furthermore, we explore alternatives to improve models' performance by augmenting the input context via Retrieval-Augmented Generation (RAG) relying on web search, and Self-Reflection using staged conversations that aim to re-collect relevant content from the model's internal memory. We find that MIXTRAL is highly accurate with an 82% accuracy on average with a significant performance disparity across different political groups (50-95%). Augmenting the input context with expert-curated information can lead to a significant boost of approx. 9%, which remains an open challenge for automated RAG approaches, even considering curated content",
    "checked": true,
    "id": "7e1303dd9296718f44a29577e9feb0d9562075e3",
    "semantic_title": "investigating llms as voting assistants via contextual augmentation: a case study on the european parliament elections 2024",
    "citation_count": 1,
    "authors": [
      "Ilias Chalkidis"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.313": {
    "title": "Adaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning",
    "volume": "main",
    "abstract": "Large language models (LLMs) have shown excellent capability for solving reasoning problems. Existing approaches do not differentiate the question difficulty when designing prompting methods for them. Clearly, a simple method cannot elicit sufficient knowledge from LLMs to answer a hard question. Meanwhile, a sophisticated one will force the LLM to generate redundant or even inaccurate intermediate steps toward a simple question. Consequently, the performance of existing methods fluctuates among various questions.In this work, we propose Adaption-of-Thought (AdoT), an adaptive method to improve LLMs for the reasoning problem, which first measures the question difficulty and then tailors demonstration set construction and difficulty-adapted retrieval strategies for the adaptive demonstration construction. Experimental results on three reasoning tasks prove the superiority of our proposed method, showing an absolute improvement of up to 5.5% on arithmetic reasoning, 7.4% on symbolic reasoning, and 2.3% on commonsense reasoning. Our codes and implementation details are available at: https://github.com/NLPGM/AdoT",
    "checked": true,
    "id": "97126bb7be3797ab21ef938ab886ad66a555aa86",
    "semantic_title": "adaption-of-thought: learning question difficulty improves large language models for reasoning",
    "citation_count": 0,
    "authors": [
      "Mayi Xu",
      "Yongqi Li",
      "Ke Sun",
      "Tieyun Qian"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.314": {
    "title": "LogicST: A Logical Self-Training Framework for Document-Level Relation Extraction with Incomplete Annotations",
    "volume": "main",
    "abstract": "Document-level relation extraction (DocRE) aims to identify relationships between entities within a document. Due to the vast number of entity pairs, fully annotating all fact triplets is challenging, resulting in datasets with numerous false negative samples. Recently, self-training-based methods have been introduced to address this issue. However, these methods are purely black-box and sub-symbolic, making them difficult to interpret and prone to overlooking symbolic interdependencies between relations.To remedy this deficiency, our insight is that symbolic knowledge, such as logical rules, can be used as diagnostic tools to identify conflicts between pseudo-labels. By resolving these conflicts through logical diagnoses, we can correct erroneous pseudo-labels, thus enhancing the training of neural models.To achieve this, we propose **LogicST**, a neural-logic self-training framework that iteratively resolves conflicts and constructs the minimal diagnostic set for updating models. Extensive experiments demonstrate that LogicST significantly improves performance and outperforms previous state-of-the-art methods. For instance, LogicST achieves an increase of **7.94%** in F1 score compared to CAST (Tan et al., 2023a) on the DocRED benchmark (Yao et al., 2019). Additionally, LogicST is more time-efficient than its self-training counterparts, requiring only **10%** of the training time of CAST",
    "checked": true,
    "id": "446c432c0245b0ea08c5bc0d4f522dfaf5ba043c",
    "semantic_title": "logicst: a logical self-training framework for document-level relation extraction with incomplete annotations",
    "citation_count": 0,
    "authors": [
      "Shengda Fan",
      "Yanting Wang",
      "Shasha Mo",
      "Jianwei Niu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.315": {
    "title": "Concept Space Alignment in Multilingual LLMs",
    "volume": "main",
    "abstract": "Multilingual large language models (LLMs) seem to generalize somewhat across languages. We hypothesize this is a result of implicit vector space alignment. Evaluating such alignment, we see that larger models exhibit very high-quality linear alignments between corresponding concepts in different languages. Our experiments show that multilingual LLMs suffer from two familiar weaknesses: generalization works best for languages with similar typology, and for abstract concepts. For some models, e.g., the Llama-2 family of models, prompt-based embeddings align better than word embeddings, but the projections are less linear – an observation that holds across almost all model families, indicating that some of the implicitly learned alignments are broken somewhat by prompt-based methods",
    "checked": true,
    "id": "f2dd6ead79e5eab758993dd7f9b747194f5bf95e",
    "semantic_title": "concept space alignment in multilingual llms",
    "citation_count": 1,
    "authors": [
      "Qiwei Peng",
      "Anders Søgaard"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.316": {
    "title": "Predicting Rewards Alongside Tokens: Non-disruptive Parameter Insertion for Efficient Inference Intervention in Large Language Model",
    "volume": "main",
    "abstract": "Transformer-based large language models (LLMs) exhibit limitations such as generating unsafe responses, unreliable reasoning, etc. Existing inference intervention approaches attempt to mitigate these issues by finetuning additional models to produce calibration signals (such as rewards) that guide the LLM's decoding process. However, this solution introduces substantial time and space overhead due to the separate models required. This work proposes Non-disruptive parameters insertion (Otter), inserting extra parameters into the transformer architecture to predict calibration signals along with the original LLM output. Otter offers state-of-the-art performance on multiple demanding tasks while saving up to 86.5% extra space and 98.5% extra time. Furthermore, Otter seamlessly integrates with existing inference engines, requiring only a one-line code change, and the original model response remains accessible after the parameter insertion",
    "checked": true,
    "id": "0c40ebee64178279d96fb3bbac549c775706574d",
    "semantic_title": "predicting rewards alongside tokens: non-disruptive parameter insertion for efficient inference intervention in large language model",
    "citation_count": 0,
    "authors": [
      "Chenhan Yuan",
      "Fei Huang",
      "Ru Peng",
      "Keming Lu",
      "Bowen Yu",
      "Chang Zhou",
      "Jingren Zhou"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.317": {
    "title": "NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian",
    "volume": "main",
    "abstract": "Norwegian, spoken by only 5 million population, is under-representative within the most impressive breakthroughs in NLP tasks. To the best of our knowledge, there has not yet been a comprehensive evaluation of the existing language models (LMs) on Norwegian generation tasks during the article writing process. To fill this gap, we 1) compiled the existing Norwegian dataset and pre-trained 4 Norwegian Open Language Models varied from parameter scales and architectures, collectively called NorGLM; 2) introduced a comprehensive benchmark, NLEBench, for evaluating natural language generation capabilities in Norwegian, encompassing translation and human annotation. Based on the investigation, we find that: 1) the mainstream, English-dominated LM GPT-3.5 has limited capability in understanding the Norwegian context; 2) the increase in model parameter scales demonstrates limited impact on the performance of downstream tasks when the pre-training dataset is constrained in size; 3) smaller models also demonstrate the reasoning capability through Chain-of-Thought; 4) a multi-task dataset that includes synergy tasks can be used to verify the generalizability of LLMs on natural language understanding and, meanwhile, test the interconnectedness of these NLP tasks. We share our resources and code for reproducibility under a CC BY-NC 4.0 license",
    "checked": true,
    "id": "0ad16e2c1c30d8ed5b63970e5fb3459a08218ea3",
    "semantic_title": "nlebench+norglm: a comprehensive empirical analysis and benchmark dataset for generative language models in norwegian",
    "citation_count": 1,
    "authors": [
      "Peng Liu",
      "Lemei Zhang",
      "Terje Farup",
      "Even Lauvrak",
      "Jon Ingvaldsen",
      "Simen Eide",
      "Jon Atle Gulla",
      "Zhirong Yang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.318": {
    "title": "RSA-Control: A Pragmatics-Grounded Lightweight Controllable Text Generation Framework",
    "volume": "main",
    "abstract": "Despite significant advancements in natural language generation, controlling language models to produce texts with desired attributes remains a formidable challenge. In this work, we introduce RSA-Control, a training-free controllable text generation framework grounded in pragmatics. RSA-Control directs the generation process by recursively reasoning between imaginary speakers and listeners, enhancing the likelihood that target attributes are correctly interpreted by listeners amidst distractors. Additionally, we introduce a self-adjustable rationality parameter, which allows for automatic adjustment of control strength based on context. Our experiments, conducted with two task types and two types of language models, demonstrate that RSA-Control achieves strong attribute control while maintaining language fluency and content consistency. Our code is available at https://github.com/Ewanwong/RSA-Control",
    "checked": true,
    "id": "63161be2bd80005ffe5eff673994978b7ea9a4cb",
    "semantic_title": "rsa-control: a pragmatics-grounded lightweight controllable text generation framework",
    "citation_count": 0,
    "authors": [
      "Yifan Wang",
      "Vera Demberg"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.319": {
    "title": "Scaling Laws Across Model Architectures: A Comparative Analysis of Dense and MoE Models in Large Language Models",
    "volume": "main",
    "abstract": "The scaling of large language models (LLMs) is a critical research area for the efficiency and effectiveness of model training and deployment. Our work investigates the transferability and discrepancies of scaling laws between Dense Models and Mixture of Experts (MoE) models. Through a combination of theoretical analysis and extensive experiments, including consistent loss scaling, optimal batch size/learning rate scaling, and resource allocation strategies scaling, our findings reveal that the power-law scaling framework also applies to MoE Models, indicating that the fundamental principles governing the scaling behavior of these models are preserved, even though the architecture differs. Additionally, MoE Models demonstrate superior generalization, resulting in lower testing losses with the same training compute budget compared to Dense Models. These findings indicate the scaling consistency and transfer generalization capabilities of MoE Models, providing new insights for optimizing MoE Model training and deployment strategies",
    "checked": true,
    "id": "6fdea305b054201a840531ba1f39bb08307a7200",
    "semantic_title": "scaling laws across model architectures: a comparative analysis of dense and moe models in large language models",
    "citation_count": 0,
    "authors": [
      "Siqi Wang",
      "Zhengyu Chen",
      "Bei Li",
      "Keqing He",
      "Min Zhang",
      "Jingang Wang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.320": {
    "title": "Synergizing In-context Learning with Hints for End-to-end Task-oriented Dialog Systems",
    "volume": "main",
    "abstract": "End-to-end Task-Oriented Dialog (TOD) systems typically require extensive training datasets to perform well. In contrast, large language model (LLM) based TOD systems can excel even with limited data due to their ability to learn tasks through in-context exemplars. However, these models lack alignment with the style of responses in training data and often generate comprehensive responses, making it difficult for users to grasp the information quickly. In response, we propose SyncTOD that synergizes LLMs with task-specific hints to improve alignment in low-data settings. SyncTOD employs small auxiliary models to provide hints and select exemplars for in-context prompts. With ChatGPT, SyncTOD achieves superior performance compared to LLM-based baselines and SoTA models in low-data settings, while retaining competitive performance in full-data settings",
    "checked": true,
    "id": "d4dcd6eaab30e47ed3ba526663bdaa99e74a16e4",
    "semantic_title": "synergizing in-context learning with hints for end-to-end task-oriented dialog systems",
    "citation_count": 1,
    "authors": [
      "Vishal Saley",
      "Rocktim Das",
      "Dinesh Raghu",
      "Mausam ."
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.321": {
    "title": "REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering",
    "volume": "main",
    "abstract": "Considering the limited internal parametric knowledge, retrieval-augmented generation (RAG) has been widely used to extend the knowledge scope of large language models (LLMs). Despite the extensive efforts on RAG research, in existing methods, LLMs cannot precisely assess the relevance of retrieved documents, thus likely leading to misleading or even incorrect utilization of external knowledge (i.e., retrieved documents). To address this issue, in this paper, we propose REAR, a RElevance-Aware Retrieval-augmented approach for open-domain question answering (QA). As the key motivation, we aim to enhance the self-awareness regarding the reliability of external knowledge for LLMs, so as to adaptively utilize external knowledge in RAG systems. Specially, we develop a novel architecture for LLM based RAG system, by incorporating a specially designed assessnent module that precisely assesses the relevance of retrieved documents. Furthermore, we propose an improved training method based on bi-granularity relevance fusion and noise-resistant training. By combining the improvements in both architecture and training, our proposed REAR can better utilize external knowledge by effectively perceiving the relevance of retrieved documents. Experiments on four open-domain QA tasks show that REAR significantly outperforms previous a number of competitive RAG approaches. Our codes can be accessed at https://github.com/RUCAIBox/REAR",
    "checked": true,
    "id": "8c4b0f3f69e1ed36f371d91dfab7e0d9f909311c",
    "semantic_title": "rear: a relevance-aware retrieval-augmented framework for open-domain question answering",
    "citation_count": 2,
    "authors": [
      "Yuhao Wang",
      "Ruiyang Ren",
      "Junyi Li",
      "Xin Zhao",
      "Jing Liu",
      "Ji-Rong Wen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.322": {
    "title": "Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA",
    "volume": "main",
    "abstract": "Long-context modeling capabilities of Large Language Models (LLMs) have garnered widespread attention, leading to the emergence of LLMs with ultra-context windows. Meanwhile, benchmarks for evaluating long-context language models are gradually catching up. However, existing benchmarks employ irrelevant noise texts to artificially extend the length of test cases, diverging from the real-world scenarios of long-context applications. To bridge this gap, we propose a novel long-context benchmark, Loong, aligning with realistic scenarios through extended multi-document question answering (QA). Unlike typical document QA, in Loong's test cases, each document is relevant to the final answer, ignoring any document will lead to the failure of the answer. Furthermore, Loong introduces four types of tasks with a range of context lengths: Spotlight Locating, Comparison, Clustering, and Chain of Reasoning, to facilitate a more realistic and comprehensive evaluation of long-context understanding. Extensive experiments indicate that existing long-context language models still exhibit considerable potential for enhancement. Retrieval augmented generation (RAG) achieves poor performance, demonstrating that Loong can reliably assess the model's long-context modeling capabilities",
    "checked": true,
    "id": "9061cc45c64846498f572c9ad2cb14f76324d665",
    "semantic_title": "leave no document behind: benchmarking long-context llms with extended multi-doc qa",
    "citation_count": 15,
    "authors": [
      "Minzheng Wang",
      "Longze Chen",
      "Fu Cheng",
      "Shengyi Liao",
      "Xinghua Zhang",
      "Bingli Wu",
      "Haiyang Yu",
      "Nan Xu",
      "Lei Zhang",
      "Run Luo",
      "Yunshui Li",
      "Min Yang",
      "Fei Huang",
      "Yongbin Li"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.323": {
    "title": "On Mitigating Performance Disparities in Multilingual Speech Recognition",
    "volume": "main",
    "abstract": "How far have we come in mitigating performance disparities across genders in multilingual speech recognition? We compare the impact on gender disparity of different fine-tuning algorithms for automated speech recognition across model sizes, languages and gender. We look at both performance-focused and fairness-promoting algorithms. Across languages, we see slightly better performance for female speakers for larger models regardless of the fine-tuning algorithm. The best trade-off between performance and parity is found using adapter fusion. Fairness-promoting fine-tuning algorithms (Group-DRO and Spectral Decoupling) hurt performance compared to adapter fusion with only slightly better performance parity. LoRA increases disparities slightly. Fairness-mitigating fine-tuning techniques led to slightly higher variance in performance across languages, with the exception of adapter fusion",
    "checked": true,
    "id": "1911784df10bdfa378acb84b7bf402bfc222b5da",
    "semantic_title": "on mitigating performance disparities in multilingual speech recognition",
    "citation_count": 0,
    "authors": [
      "Monorama Swain",
      "Anna Zee",
      "Anders Søgaard"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.324": {
    "title": "Thinking Outside of the Differential Privacy Box: A Case Study in Text Privatization with Language Model Prompting",
    "volume": "main",
    "abstract": "The field of privacy-preserving Natural Language Processing has risen in popularity, particularly at a time when concerns about privacy grow with the proliferation of large language models. One solution consistently appearing in recent literature has been the integration of Differential Privacy (DP) into NLP techniques. In this paper, we take these approaches into critical view, discussing the restrictions that DP integration imposes, as well as bring to light the challenges that such restrictions entail. To accomplish this, we focus on **DP-Prompt**, a recent method for text privatization leveraging language models to rewrite texts. In particular, we explore this rewriting task in multiple scenarios, both with DP and without DP. To drive the discussion on the merits of DP in NLP, we conduct empirical utility and privacy experiments. Our results demonstrate the need for more discussion on the usability of DP in NLP and its benefits over non-DP approaches",
    "checked": true,
    "id": "e0c199318c191bf6dbb46c2a9a425e99e9c977db",
    "semantic_title": "thinking outside of the differential privacy box: a case study in text privatization with language model prompting",
    "citation_count": 0,
    "authors": [
      "Stephen Meisenbacher",
      "Florian Matthes"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.325": {
    "title": "To Preserve or To Compress: An In-Depth Study of Connector Selection in Multimodal Large Language Models",
    "volume": "main",
    "abstract": "In recent years, multimodal large language models (MLLMs) have attracted widespread attention from both industry and academia. Based on the integration position, MLLMs can be categorized into external and internal fusion architectures, with the former being more predominant. However, there remains considerable debate on how to construct the optimal external fusion MLLM architecture, especially regarding the performance of different connectors on tasks with varying granularities. This paper systematically investigates the impact of connectors on MLLM performance. Specifically, we classify connectors into feature-preserving and feature-compressing types. Utilizing a unified classification standard, we categorize sub-tasks from three comprehensive benchmarks, MMBench, MME, and SEED-Bench, into three task types: coarse-grained perception, fine-grained perception, and reasoning, and evaluate the performance from this perspective. Our findings reveal significant performance differences between different types of connectors across various tasks, offering essential guidance for MLLM architecture design and advancing the understanding of MLLM architecture optimization",
    "checked": true,
    "id": "04053dcd96280751344e05231e727e64d4a4e162",
    "semantic_title": "to preserve or to compress: an in-depth study of connector selection in multimodal large language models",
    "citation_count": 0,
    "authors": [
      "Junyan Lin",
      "Haoran Chen",
      "Dawei Zhu",
      "Xiaoyu Shen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.326": {
    "title": "What is \"Typological Diversity\" in NLP?",
    "volume": "main",
    "abstract": "The NLP research community has devoted increased attention to languages beyond English, resulting in considerable improvements for multilingual NLP. However, these improvements only apply to a small subset of the world's languages. An increasing number of papers aspires to enhance generalizable multilingual performance across languages. To this end, linguistic typology is commonly used to motivate language selection, on the basis that a broad typological sample ought to imply generalization across a broad range of languages. These selections are often described as being typologically diverse. In this meta-analysis, we systematically investigate NLP research that includes claims regarding typological diversity. We find there are no set definitions or criteria for such claims. We introduce metrics to approximate the diversity of resulting language samples along several axes and find that the results vary considerably across papers. Crucially, we show that skewed language selection can lead to overestimated multilingual performance. We recommend future work to include an operationalization of typological diversity that empirically justifies the diversity of language samples. To help facilitate this, we release the code for our diversity measures",
    "checked": true,
    "id": "90a3ebfd0aa17589de87f0a17d04d27eab4dd384",
    "semantic_title": "what is \"typological diversity\" in nlp?",
    "citation_count": 1,
    "authors": [
      "Esther Ploeger",
      "Wessel Poelman",
      "Miryam De Lhoneux",
      "Johannes Bjerva"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.327": {
    "title": "The Computational Anatomy of Humility: Modeling Intellectual Humility in Online Public Discourse",
    "volume": "main",
    "abstract": "The ability for individuals to constructively engage with one another across lines of difference is a critical feature of a healthy pluralistic society. This is also true in online discussion spaces like social media platforms. To date, much social media research has focused on preventing ills—like political polarization and the spread of misinformation. While this is important, enhancing the quality of online public discourse requires not just reducing ills, but also, promoting foundational human virtues. In this study, we focus on one particular virtue: \"intellectual humility\" (IH), or acknowledging the potential limitations in one's own beliefs. Specifically, we explore the development of computational methods for measuring IH at scale. We manually curate and validate an IH codebook on 350 posts about religion drawn from subreddits and use them to develop LLM-based models for automating this measurement. Our best model achieves a Macro-F1 score of 0.64 across labels (and 0.70 when predicting IH/IA/Neutral at the coarse level), higher than an expected naive baseline of 0.51 (0.32 for IH/IA/Neutral) but lower than a human annotator-informed upper bound of 0.85 (0.83 for IH/IA/Neutral). Our results both highlight the challenging nature of detecting IH online—opening the door to new directions in NLP research—and also lay a foundation for computational social science researchers interested in analyzing and fostering more IH in online public discourse",
    "checked": true,
    "id": "6474c64ea92bc06ca3d2fe62a79a82a807699c71",
    "semantic_title": "the computational anatomy of humility: modeling intellectual humility in online public discourse",
    "citation_count": 0,
    "authors": [
      "Xiaobo Guo",
      "Neil Potnis",
      "Melody Yu",
      "Nabeel Gillani",
      "Soroush Vosoughi"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.328": {
    "title": "Consistent Bidirectional Language Modelling: Expressive Power and Representational Conciseness",
    "volume": "main",
    "abstract": "The inability to utilise future contexts and the pre-determined left-to-right generation order are major limitations of unidirectional language models. Bidirectionality has been introduced to address those deficiencies. However, a crucial shortcoming of bidirectional language models is the potential inconsistency of their conditional distributions. This fundamental flaw greatly diminishes their applicability and hinders their capability of tractable sampling and likelihood computation. In this work, we introduce a class of bidirectional language models, called latent language models, that are consistent by definition and can be efficiently used both for generation and scoring of sequences. We define latent language models based on the well-understood formalism of bisequential decompositions from automata theory. This formal correspondence allows us to precisely charaterise the abilities and limitations of a subclass of latent language models, called rational language models. As a result, we obtain that latent language models are exponentially more concise and significantly more expressive than unidirectional language models",
    "checked": true,
    "id": "eef9627f8d599b26dd532fd3d4c36e49852f95d4",
    "semantic_title": "consistent bidirectional language modelling: expressive power and representational conciseness",
    "citation_count": 0,
    "authors": [
      "Georgi Shopov",
      "Stefan Gerdjikov"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.329": {
    "title": "Benchmarking Vision Language Models for Cultural Understanding",
    "volume": "main",
    "abstract": "Foundation models and vision-language pre-training have notably advanced Vision Language Models (VLMs), enabling multimodal processing of visual and linguistic data. However, their performance has been typically assessed on general scene understanding - recognizing objects, attributes, and actions - rather than cultural comprehension. This study introduces CulturalVQA, a visual question-answering benchmark aimed at assessing VLM's geo-diverse cultural understanding. We curate a diverse collection of 2,378 image-question pairs with 1-5 answers per question representing cultures from 11 countries across 5 continents. The questions probe understanding of various facets of culture such as clothing, food, drinks, rituals, and traditions. Benchmarking VLMs on CulturalVQA, including GPT-4V and Gemini, reveals disparity in their level of cultural understanding across regions, with strong cultural understanding capabilities for North America while significantly weaker capabilities for Africa. We observe disparity in their performance across cultural facets too, with clothing, rituals, and traditions seeing higher performances than food and drink. These disparities help us identify areas where VLMs lack cultural understanding and demonstrate the potential of CulturalVQA as a comprehensive evaluation set for gauging VLM progress in understanding diverse cultures",
    "checked": true,
    "id": "d838f425c2e5e7b0fabb4ac108fc3f57bb4a85c0",
    "semantic_title": "benchmarking vision language models for cultural understanding",
    "citation_count": 7,
    "authors": [
      "Shravan Nayak",
      "Kanishk Jain",
      "Rabiul Awal",
      "Siva Reddy",
      "Sjoerd Steenkiste",
      "Lisa Hendricks",
      "Karolina Stanczak",
      "Aishwarya Agrawal"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.330": {
    "title": "Methods of Automatic Matrix Language Determination for Code-Switched Speech",
    "volume": "main",
    "abstract": "Code-switching (CS) is the process of speakers interchanging between two or more languages which in the modern world becomes increasingly common. In order to better describe CS speech the Matrix Language Frame (MLF) theory introduces the concept of a Matrix Language, which is the language that provides the grammatical structure for a CS utterance. In this work the MLF theory was used to develop systems for Matrix Language Identity (MLID) determination. The MLID of English/Mandarin and English/Spanish CS text and speech was compared to acoustic language identity (LID), which is a typical way to identify a language in monolingual utterances. MLID predictors from audio show higher correlation with the textual principles than LID in all cases while also outperforming LID in an MLID recognition task based on F1 macro (60%) and correlation score (0.38). This novel approach has identified that non-English languages (Mandarin and Spanish) are preferred over the English language as the ML contrary to the monolingual choice of LID",
    "checked": true,
    "id": "46ace27aa178c0dfa04aa1536793599af7b78daa",
    "semantic_title": "methods of automatic matrix language determination for code-switched speech",
    "citation_count": 0,
    "authors": [
      "Olga Iakovenko",
      "Thomas Hain"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.331": {
    "title": "Analyzing Key Factors Influencing Emotion Prediction Performance of VLLMs in Conversational Contexts",
    "volume": "main",
    "abstract": "Emotional intelligence (EI) in artificial intelligence (AI), which refers to the ability of an AI to understand and respond appropriately to human emotions, has emerged as a crucial research topic. Recent studies have shown that large language models (LLMs) and vision large language models (VLLMs) possess EI and the ability to understand emotional stimuli in the form of text and images, respectively. However, factors influencing the emotion prediction performance of VLLMs in real-world conversational contexts have not been sufficiently explored. This study aims to analyze the key elements affecting the emotion prediction performance of VLLMs in conversational contexts systematically. To achieve this, we reconstructed the MELD dataset, which is based on the popular TV series Friends, and conducted experiments through three sub-tasks: overall emotion tone prediction, character emotion prediction, and contextually appropriate emotion expression selection. We evaluated the performance differences based on various model architectures (e.g., image encoders, modality alignment, and LLMs) and image scopes (e.g., entire scene, person, and facial expression). In addition, we investigated the impact of providing persona information on the emotion prediction performance of the models and analyzed how personality traits and speaking styles influenced the emotion prediction process. We conducted an in-depth analysis of the impact of various other factors, such as gender and regional biases, on the emotion prediction performance of VLLMs. The results revealed that these factors significantly influenced the model performance",
    "checked": true,
    "id": "a7099e192bc375868456e708a66d3670127df30c",
    "semantic_title": "analyzing key factors influencing emotion prediction performance of vllms in conversational contexts",
    "citation_count": 0,
    "authors": [
      "Jaewook Lee",
      "Yeajin Jang",
      "Hongjin Kim",
      "Woojin Lee",
      "Harksoo Kim"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.332": {
    "title": "Context-Aware Assistant Selection for Improved Inference Acceleration with Large Language Models",
    "volume": "main",
    "abstract": "Despite their widespread adoption, large language models (LLMs) remain prohibitive to use under resource constraints, with their ever growing sizes only increasing the barrier for use. One particular issue stems from the high latency associated with auto-regressive generation in LLMs, rendering the largest LLMs difficult to use without advanced computing infrastructure. Assisted decoding, where a smaller draft model guides a larger expert model's generation, has helped alleviate this concern, but remains dependent on alignment between the two models. Thus if the draft model is insufficiently capable on some domain of interest relative to the target model, performance can degrade. Alternatively, one can leverage multiple draft models to better cover the expertise of the target, but when multiple black-box draft models are available, selecting an assistant without details about its construction can be difficult. To better understand this decision making problem, we observe it as a contextual bandit, where a policy must choose a draft model based on a context. We show that even without prior knowledge of the draft models, creating an offline dataset from only outputs of independent draft/target models and training a policy over the alignment of these outputs can accelerate performance on multiple domains as long as an individual draft model is effective. We observe these results hold on various settings with multiple assisted decoding candidates, highlighting its flexibility and the advantageous role that such decision making can play",
    "checked": true,
    "id": "1fb76e69ee4180204f9480853abc8c7bc5d4ddcf",
    "semantic_title": "context-aware assistant selection for improved inference acceleration with large language models",
    "citation_count": 1,
    "authors": [
      "Jerry Huang",
      "Prasanna Parthasarathi",
      "Mehdi Rezagholizadeh",
      "Sarath Chandar"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.333": {
    "title": "Teaching Small Language Models Reasoning through Counterfactual Distillation",
    "volume": "main",
    "abstract": "With the rise of large language models (LLMs), many studies are interested in transferring the reasoning capabilities of LLMs to small language models (SLMs). Previous distillation methods usually utilize the capabilities of LLMs to generate chain-of-thought (CoT) samples and teach SLMs via fine-tuning. However, such a standard distillation approach performs poorly when applied to out-of-distribution (OOD) examples, and the diversity of the generated CoT samples is insufficient. In this work, we propose a novel counterfactual distillation framework. Firstly, we leverage LLMs to automatically generate high-quality counterfactual data. Given an input text example, our method generates a counterfactual example that is very similar to the original input, but its task label has been changed to the desired one. Then, we utilize multi-view CoT to enhance the diversity of reasoning samples. Experiments on four NLP benchmarks show that our approach enhances the reasoning capabilities of SLMs and is more robust to OOD data. We also conduct extensive ablations and sample studies to understand the reasoning capabilities of SLMs",
    "checked": true,
    "id": "7b87eb4a00f1c7d1f45efb929d37204e490a59ee",
    "semantic_title": "teaching small language models reasoning through counterfactual distillation",
    "citation_count": 0,
    "authors": [
      "Tao Feng",
      "Yicheng Li",
      "Li Chenglin",
      "Hao Chen",
      "Fei Yu",
      "Yin Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.334": {
    "title": "Pretraining Language Models Using Translationese",
    "volume": "main",
    "abstract": "In this paper, we explore the utility of Translationese as synthetic data created using machine translation for pre-training language models (LMs) for low-resource languages (LRLs). Our simple methodology consists of translating large amounts of web-crawled monolingual documents (clean) into the LRLs, followed by filtering the translated documents using tiny LMs trained on small but clean LRL data. Taking the case of Indian languages, we pre-train LMs from scratch with 28M and 85M parameters, and then fine-tune them for 5 downstream natural language understanding (NLU) and 4 generative (NLG) tasks. We observe that pre-training on filtered synthetic data leads to relative performance drops of only 0.87% for NLU and 2.35% for NLG, compared to pre-training on clean data, and this gap further diminishes upon the inclusion of a small amount of clean data. We also study the impact of synthetic data filtering and the choice of source language for synthetic data generation. Furthermore, evaluating continually pre-trained larger models like Gemma-2B and Llama-3-8B in few-shot settings, we observe that using synthetic data is competitive with using clean data. Our findings suggest that synthetic data shows promise for bridging the pre-training gap between English and LRLs",
    "checked": true,
    "id": "06fe6f0b7c37a720f00d65766b20b05e9829979a",
    "semantic_title": "pretraining language models using translationese",
    "citation_count": 0,
    "authors": [
      "Meet Doshi",
      "Raj Dabre",
      "Pushpak Bhattacharyya"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.335": {
    "title": "Quantifying the Gaps Between Translation and Native Perception in Training for Multimodal, Multilingual Retrieval",
    "volume": "main",
    "abstract": "There is a scarcity of multilingual vision-language models that properly account for the perceptual differences that are reflected in image captions across languages and cultures. In this work, through a multimodal, multilingual retrieval case study, we quantify the existing lack of model flexibility. We empirically show performance gaps between training on captions that come from native German perception and captions that have been either machine-translated or human-translated from English into German. To address these gaps, we further propose and evaluate caption augmentation strategies. While we achieve mean recall improvements (+1.3), gaps still remain, indicating an open area of future work for the community",
    "checked": true,
    "id": "84be410a6c9dbf2d1ea4b5596674f2195e8ae23a",
    "semantic_title": "quantifying the gaps between translation and native perception in training for multimodal, multilingual retrieval",
    "citation_count": 0,
    "authors": [
      "Kyle Buettner",
      "Adriana Kovashka"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.336": {
    "title": "MTA4DPR: Multi-Teaching-Assistants Based Iterative Knowledge Distillation for Dense Passage Retrieval",
    "volume": "main",
    "abstract": "Although Dense Passage Retrieval (DPR) models have achieved significantly enhanced performance, their widespread application is still hindered by the demanding inference efficiency and high deployment costs. Knowledge distillation is an efficient method to compress models, which transfers knowledge from strong teacher models to weak student models. Previous studies have proved the effectiveness of knowledge distillation in DPR. However, there often remains a significant performance gap between the teacher and the distilled student. To narrow this performance gap, we propose MTA4DPR, a Multi-Teaching-Assistants based iterative knowledge distillation method for Dense Passage Retrieval, which transfers knowledge from the teacher to the student with the help of multiple assistants in an iterative manner; with each iteration, the student learns from more performant assistants and more difficult data. The experimental results show that our 66M student model achieves the state-of-the-art performance among models with same parameters on multiple datasets, and is very competitive when compared with larger, even LLM-based, DPR models",
    "checked": true,
    "id": "f9d58872bace68ea749fd9635919f943bbea78e2",
    "semantic_title": "mta4dpr: multi-teaching-assistants based iterative knowledge distillation for dense passage retrieval",
    "citation_count": 0,
    "authors": [
      "Qixi Lu",
      "Endong Xun",
      "Gongbo Tang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.337": {
    "title": "Fine-Grained Detection of Solidarity for Women and Migrants in 155 Years of German Parliamentary Debates",
    "volume": "main",
    "abstract": "Solidarity is a crucial concept to understand social relations in societies. In this study, we investigate the frequency of (anti-)solidarity towards women and migrants in German parliamentary debates between 1867 and 2022. Using 2,864 manually annotated text snippets, we evaluate large language models (LLMs) like Llama 3, GPT-3.5, and GPT-4. We find that GPT-4 outperforms other models, approaching human annotation accuracy. Using GPT-4, we automatically annotate 18,300 further instances and find that solidarity with migrants outweighs anti-solidarity but that frequencies and solidarity types shift over time. Most importantly, group-based notions of (anti-)solidarity fade in favor of compassionate solidarity, focusing on the vulnerability of migrant groups, and exchange-based anti-solidarity, focusing on the lack of (economic) contribution. This study highlights the interplay of historical events, socio-economic needs, and political ideologies in shaping migration discourse and social cohesion",
    "checked": true,
    "id": "3a513ba56023f328baae1b4327b0cea9e4e574ff",
    "semantic_title": "fine-grained detection of solidarity for women and migrants in 155 years of german parliamentary debates",
    "citation_count": 2,
    "authors": [
      "Aida Kostikova",
      "Dominik Beese",
      "Benjamin Paassen",
      "Ole Pütz",
      "Gregor Wiedemann",
      "Steffen Eger"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.338": {
    "title": "CItruS: Chunked Instruction-aware State Eviction for Long Sequence Modeling",
    "volume": "main",
    "abstract": "Long sequence modeling has gained broad interest as large language models (LLMs) continue to advance. Recent research has identified that a large portion of hidden states within the key-value caches of Transformer models can be discarded (also termed evicted) withoutaffecting the perplexity performance in generating long sequences. However, we show that these methods, despite preserving perplexity performance, often drop information that is important for solving downstream tasks, a problem which we call information neglect. To address this issue, we introduce Chunked Instruction-aware State Eviction (CItruS), a novel modeling technique that integrates the attention preferences useful for a downstream task into the eviction process of hidden states. In addition, we design a method for chunked sequence processing to further improve efficiency. Our training-free method exhibits superior performance on long sequence comprehension and retrieval tasks over several strong baselines under the same memory budget, while preserving language modeling perplexity. The code and data have been released at https://github.com/ybai-nlp/CItruS",
    "checked": true,
    "id": "50c1ac57e25b82387898c9d7ce25e6a6e181d6c5",
    "semantic_title": "citrus: chunked instruction-aware state eviction for long sequence modeling",
    "citation_count": 0,
    "authors": [
      "Yu Bai",
      "Xiyuan Zou",
      "Heyan Huang",
      "Sanxing Chen",
      "Marc-Antoine Rondeau",
      "Yang Gao",
      "Jackie Cheung"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.339": {
    "title": "Story Embeddings — Narrative-Focused Representations of Fictional Stories",
    "volume": "main",
    "abstract": "We present a novel approach to modeling fictional narratives. The proposed model creates embeddings that represent a story such that similar narratives, that is, reformulations of the same story, will result in similar embeddings. We showcase the prowess of our narrative-focused embeddings on various datasets, exhibiting state-of-the-art performance on multiple retrieval tasks. The embeddings also show promising results on a narrative understanding task. Additionally, we perform an annotation-based evaluation to validate that our introduced computational notion of narrative similarity aligns with human perception. The approach can help to explore vast datasets of stories, with potential applications in recommender systems and in the computational analysis of literature",
    "checked": true,
    "id": "482ede29db038954c1469b62db45bff02dc012f7",
    "semantic_title": "story embeddings — narrative-focused representations of fictional stories",
    "citation_count": 0,
    "authors": [
      "Hans Ole Hatzel",
      "Chris Biemann"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.340": {
    "title": "C-LLM: Learn to Check Chinese Spelling Errors Character by Character",
    "volume": "main",
    "abstract": "Chinese Spell Checking (CSC) aims to detect and correct spelling errors in sentences. Despite Large Language Models (LLMs) exhibit robust capabilities and are widely applied in various tasks, their performance on CSC is often unsatisfactory. We find that LLMs fail to meet the Chinese character-level constraints of the CSC task, namely equal length and phonetic similarity, leading to a performance bottleneck. Further analysis reveals that this issue stems from the granularity of tokenization, as current mixed character-word tokenization struggles to satisfy these character-level constraints. To address this issue, we propose C-LLM, a Large Language Model-based Chinese Spell Checking method that learns to check errors Character by Character. Character-level tokenization enables the model to learn character-level alignment, effectively mitigating issues related to character-level constraints. Furthermore, CSC is simplified to replication-dominated and substitution-supplemented tasks. Experiments on two CSC benchmarks demonstrate that C-LLM achieves a 2.1% enhancement in general scenarios and a significant 12% improvement in vertical domain scenarios compared to existing methods, establishing state-of-the-art performance",
    "checked": true,
    "id": "4b6749c981de4c3f519ef18749ecd3059abbff32",
    "semantic_title": "c-llm: learn to check chinese spelling errors character by character",
    "citation_count": 1,
    "authors": [
      "Kunting Li",
      "Yong Hu",
      "Liang He",
      "Fandong Meng",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.341": {
    "title": "PSC: Extending Context Window of Large Language Models via Phase Shift Calibration",
    "volume": "main",
    "abstract": "Rotary Position Embedding (RoPE) is an efficient position encoding approach and is widely utilized in numerous large language models (LLMs). Recently, a lot of methods have been put forward to further expand the context window based on RoPE. The core concept of those methods is to predefine or search for a set of factors to rescale the base frequencies of RoPE. Nevertheless, it is quite a challenge for existing methods to predefine an optimal factor due to the exponential search space. In view of this, we introduce PSC (Phase Shift Calibration), a small module for calibrating the frequencies predefined by existing methods. With the employment of PSC, we demonstrate that many existing methods can be further enhanced, like PI, YaRN, and LongRoPE. We conducted extensive experiments across multiple models and tasks. The results demonstrate that (1) when PSC is enabled, the comparative reductions in perplexity increase as the context window size is varied from 16k, to 32k, and up to 64k. (2) Our approach is broadly applicable and exhibits robustness across a variety of models and tasks",
    "checked": true,
    "id": "eeeb3b416761e29fa0c36d155504814f5a4de805",
    "semantic_title": "psc: extending context window of large language models via phase shift calibration",
    "citation_count": 0,
    "authors": [
      "Wenqiao Zhu",
      "Chao Xu",
      "Lulu Wang",
      "Jun Wu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.342": {
    "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
    "volume": "main",
    "abstract": "Large Vision-Language Model (LVLM) has enhanced the performance of various downstream tasks in visual-language understanding. Most existing approaches encode images and videos into separate feature spaces, which are then fed as inputs to large language models. However, due to the lack of unified tokenization for images and videos, namely misalignment before projection, it becomes challenging for a Large Language Model (LLM) to learn multi-modal interactions from several poor projection layers.In this work, we unify visual representation into the language feature space to advance the foundational LLM towards a unified LVLM. As a result, we establish a simple but robust LVLM baseline, Video-LLaVA, which learns from a mixed dataset of images and videos, mutually enhancing each other.As a result, Video-LLaVA outperforms Video-ChatGPT by 5.8%, 9.9%, 18.6%, and 10.1% on MSRVTT, MSVD, TGIF, and ActivityNet, respectively. Additionally, our Video-LLaVA also achieves superior performances on a broad range of 9 image benchmarks.Notably, extensive experiments demonstrate that Video-LLaVA mutually benefits images and videos within a unified visual representation, outperforming models designed specifically for images or videos. We aim for this work to provide modest insights into the multi-modal inputs for the LLM",
    "checked": true,
    "id": "107fb6eec2febbae12db29bf3e311aaf5680027c",
    "semantic_title": "video-llava: learning united visual representation by alignment before projection",
    "citation_count": 270,
    "authors": [
      "Bin Lin",
      "Yang Ye",
      "Bin Zhu",
      "Jiaxi Cui",
      "Munan Ning",
      "Peng Jin",
      "Li Yuan"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.343": {
    "title": "SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales",
    "volume": "main",
    "abstract": "Large language models (LLMs) often generate inaccurate or fabricated information and generally fail to indicate their confidence, which limits their broader applications. Previous work has elicited confidence from LLMs by direct or self-consistency prompting, or constructing specific datasets for supervised finetuning. The prompting-based approaches have inferior performance, and the training-based approaches are limited to binary or inaccurate group-level confidence estimates. In this work, we present SaySelf, a novel training framework that teaches LLMs to express more fine-grained confidence estimates. In addition, beyond the confidence scores, SaySelf initiates the process of directing LLMs to produce self-reflective rationales that clearly identify gaps in their parametric knowledge and explain their uncertainty. This is achieved by using an LLM to automatically summarize the uncertainties in specific knowledge via natural language. The summarization is based on the analysis of the inconsistency in multiple sampled reasoning chains, and the resulting data is utilized for supervised fine-tuning. Moreover, we utilize reinforcement learning with a meticulously crafted reward function to calibrate the confidence estimates, motivating LLMs to deliver accurate, high-confidence predictions and to penalize overconfidence in erroneous outputs. Experimental results demonstrate the effectiveness of SaySelf in reducing the confidence calibration error and maintaining the task performance. The generated self-reflective rationales are also reasonable and can further contribute to the calibration. The code is made public at https://github.com/xu1868/SaySelf",
    "checked": true,
    "id": "df38798cb99338e1aac9c4dda154c78787f89df3",
    "semantic_title": "sayself: teaching llms to express confidence with self-reflective rationales",
    "citation_count": 13,
    "authors": [
      "Tianyang Xu",
      "Shujin Wu",
      "Shizhe Diao",
      "Xiaoze Liu",
      "Xingyao Wang",
      "Yangyi Chen",
      "Jing Gao"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.344": {
    "title": "Mitigating Frequency Bias and Anisotropy in Language Model Pre-Training with Syntactic Smoothing",
    "volume": "main",
    "abstract": "Language models strongly rely on frequency information because they maximize the likelihood of tokens during pre-training. As a consequence, language models tend to not generalize well to tokens that are seldom seen during training. Moreover, maximum likelihood training has been discovered to give rise to anisotropy: representations of tokens in a model tend to cluster tightly in a high-dimensional cone, rather than spreading out over their representational capacity.Our work introduces a method for quantifying the frequency bias of a language model by assessing sentence-level perplexity with respect to token-level frequency. We then present a method for reducing the frequency bias of a language model by inducing a syntactic prior over token representations during pre-training. Our Syntactic Smoothing method adjusts the maximum likelihood objective function to distribute the learning signal to syntactically similar tokens. This approach results in better performance on infrequent English tokens and a decrease in anisotropy. We empirically show that the degree of anisotropy in a model correlates with its frequency bias",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Richard Diehl Martinez",
      "Zebulon Goriely",
      "Andrew Caines",
      "Paula Buttery",
      "Lisa Beinborn"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.345": {
    "title": "ToxiCloakCN: Evaluating Robustness of Offensive Language Detection in Chinese with Cloaking Perturbations",
    "volume": "main",
    "abstract": "Detecting hate speech and offensive language is essential for maintaining a safe and respectful digital environment. This study examines the limitations of state-of-the-art large language models (LLMs) in identifying offensive content within systematically perturbed data, with a focus on Chinese, a language particularly susceptible to such perturbations. We introduce ToxiCloakCN, an enhanced dataset derived from ToxiCN, augmented with homophonic substitutions and emoji transformations, to test the robustness of LLMs against these cloaking perturbations. Our findings reveal that existing models significantly underperform in detecting offensive content when these perturbations are applied. We provide an in-depth analysis of how different types of offensive content are affected by these perturbations and explore the alignment between human and model explanations of offensiveness. Our work highlights the urgent need for more advanced techniques in offensive language detection to combat the evolving tactics used to evade detection mechanisms",
    "checked": true,
    "id": "84c393d1cd5c0d87f673fadcdfb0a2e99dba30ce",
    "semantic_title": "toxicloakcn: evaluating robustness of offensive language detection in chinese with cloaking perturbations",
    "citation_count": 0,
    "authors": [
      "Yunze Xiao",
      "Yujia Hu",
      "Kenny Choo",
      "Roy Lee"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.346": {
    "title": "Boosting Scientific Concepts Understanding: Can Analogy from Teacher Models Empower Student Models?",
    "volume": "main",
    "abstract": "Analogical reasoning plays a critical role in human cognition, enabling us to understand new concepts by associating them with familiar ones. Previous research in the AI community has mainly focused on identifying and generating analogies and then examining their quality under human evaluation, which overlooks the practical application of these analogies in real-world settings. Inspired by the human education process, in this paper, we propose to investigate how analogies created by teacher language models (LMs) can assist student LMs in understanding scientific concepts, thereby aligning more closely with practical scenarios. Our results suggest that free-form analogies can indeed aid LMs in understanding concepts. Additionally, analogies generated by student LMs can improve their own performance on scientific question answering, demonstrating their capability to use analogies for self-learning new knowledge. Resources are available athttps://github.com/siyuyuan/SCUA",
    "checked": true,
    "id": "7348536e2197f3aa19387c6e5448af4c8f5acb38",
    "semantic_title": "boosting scientific concepts understanding: can analogy from teacher models empower student models?",
    "citation_count": 1,
    "authors": [
      "Siyu Yuan",
      "Cheng Jiayang",
      "Lin Qiu",
      "Deqing Yang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.347": {
    "title": "Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation",
    "volume": "main",
    "abstract": "Ensuring the verifiability of model answers is a fundamental challenge for retrieval-augmented generation (RAG) in the question answering (QA) domain. Recently, self-citation prompting was proposed to make large language models (LLMs) generate citations to supporting documents along with their answers. However, self-citing LLMs often struggle to match the required format, refer to non-existent sources, and fail to faithfully reflect LLMs' context usage throughout the generation. In this work, we present MIRAGE – Model Internals-based RAG Explanations – a plug-and-play approach using model internals for faithful answer attribution in RAG applications. MIRAGE detects context-sensitive answer tokens and pairs them with retrieved documents contributing to their prediction via saliency methods. We evaluate our proposed approach on a multilingual extractive QA dataset, finding high agreement with human answer attribution. On open-ended QA, MIRAGE achieves citation quality and efficiency comparable to self-citation while also allowing for a finer-grained control of attribution parameters. Our qualitative evaluation highlights the faithfulness of MIRAGE's attributions and underscores the promising application of model internals for RAG answer attribution. Code and data released at https://github.com/Betswish/MIRAGE",
    "checked": true,
    "id": "90193735c3a84cf608409007df1bf409fd6635c6",
    "semantic_title": "model internals-based answer attribution for trustworthy retrieval-augmented generation",
    "citation_count": 2,
    "authors": [
      "Jirui Qi",
      "Gabriele Sarti",
      "Raquel Fernández",
      "Arianna Bisazza"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.348": {
    "title": "Do Large Language Models Know How Much They Know?",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have emerged as highly capable systems and are increasingly being integrated into various uses. Nevertheless, the rapid advancement in their deployment trails a comprehensive understanding of their internal mechanisms, as well as a delineation of their capabilities and limitations. A desired characteristic of an intelligent system is its ability to recognize the scope of its own knowledge. To investigate whether LLMs embody this attribute, we develop a benchmark that challenges these models to enumerate all information they possess on specific topics. This benchmark assesses whether the models recall excessive, insufficient, or the precise amount of required information, thereby indicating their awareness of how much they know about the given topic. Our findings reveal that the emergence of this property varies across different architectures and manifests at diverse rates. However, with sufficient scaling, all tested models are ultimately capable of performing this task. The insights gained from this research advance our understanding of LLMs, shedding light on their operational capabilities and contributing to the ongoing exploration of their intricate dynamics",
    "checked": true,
    "id": "0005be24c6ee64d418b8af6f8fdeae1b640f76be",
    "semantic_title": "do large language models know how much they know?",
    "citation_count": 0,
    "authors": [
      "Gabriele Prato",
      "Jerry Huang",
      "Prasanna Parthasarathi",
      "Shagun Sodhani",
      "Sarath Chandar"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.349": {
    "title": "Investigating Mysteries of CoT-Augmented Distillation",
    "volume": "main",
    "abstract": "Eliciting chain of thought (CoT) rationales - sequences of token that convey a \"reasoning\" process has been shown to consistently improve LLM performance on tasks like question answering. More recent efforts have shown that such rationales can also be used for model distillation: Including CoT sequences (elicited from a large \"teacher\" model) in addition to target labels when fine-tuning a small student model yields (often substantial) improvements. In this work we ask: Why and how does this additional training signal help in model distillation? We perform ablations to interrogate this, and report some potentially surprising results. Specifically: (1) Placing CoT sequences after labels (rather than before) realizes consistently better downstream performance – this means that no student \"reasoning\" is necessary at test time to realize gains. (2) When rationales are appended in this way, they need not be coherent reasoning sequences to yield improvements; performance increases are robust to permutations of CoT tokens, for example. In fact, (3) a small number of key tokens are sufficient to achieve improvements equivalent to those observed when full rationales are used in model distillation",
    "checked": true,
    "id": "6d25e201deddad04534e37ccbe0f7566938e10f3",
    "semantic_title": "investigating mysteries of cot-augmented distillation",
    "citation_count": 4,
    "authors": [
      "Somin Wadhwa",
      "Silvio Amir",
      "Byron Wallace"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.350": {
    "title": "SciPrompt: Knowledge-augmented Prompting for Fine-grained Categorization of Scientific Topics",
    "volume": "main",
    "abstract": "Prompt-based fine-tuning has become an essential method for eliciting information encoded in pre-trained language models for a variety of tasks, including text classification. For multi-class classification tasks, prompt-based fine-tuning under low-resource scenarios has resulted in performance levels comparable to those of fully fine-tuning methods. Previous studies have used crafted prompt templates and verbalizers, mapping from the label terms space to the class space, to solve the classification problem as a masked language modeling task. However, cross-domain and fine-grained prompt-based fine-tuning with an automatically enriched verbalizer remains unexplored, mainly due to the difficulty and costs of manually selecting domain label terms for the verbalizer, which requires humans with domain expertise. To address this challenge, we introduce SciPrompt, a framework designed to automatically retrieve scientific topic-related terms for low-resource text classification tasks. To this end, we select semantically correlated and domain-specific label terms within the context of scientific literature for verbalizer augmentation. Furthermore, we propose a new verbalization strategy that uses correlation scores as additional weights to enhance the prediction performance of the language model during model tuning. Our method outperforms state-of-the-art, prompt-based fine-tuning methods on scientific text classification tasks under few and zero-shot settings, especially in classifying fine-grained and emerging scientific topics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiwen You",
      "Kanyao Han",
      "Haotian Zhu",
      "Bertram Ludaescher",
      "Jana Diesner"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.351": {
    "title": "Distilling Knowledge from Text-to-Image Generative Models Improves Visio-Linguistic Reasoning in CLIP",
    "volume": "main",
    "abstract": "Image-text contrastive models like CLIP have wide applications in zero-shot classification, image-text retrieval, and transfer learning. However, they often struggle on compositional visio-linguistic tasks (e.g., attribute-binding or object-relationships) where their performance is no better than random chance. To address this, we introduce SDS-CLIP, a lightweight and sample-efficient distillation method to enhance CLIP's compositional visio-linguistic reasoning. Our approach fine-tunes CLIP using a distillation objective borrowed from large text-to-image generative models like Stable-Diffusion, which are known for their strong visio-linguistic reasoning abilities. On the challenging Winoground benchmark, SDS-CLIP improves the visio-linguistic performance of various CLIP models by up to 7%, while on the ARO dataset, it boosts performance by up to 3%. This work underscores the potential of well-designed distillation objectives from generative models to enhance contrastive image-text models with improved visio-linguistic reasoning capabilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samyadeep Basu",
      "Shell Xu Hu",
      "Maziar Sanjabi",
      "Daniela Massiceti",
      "Soheil Feizi"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.352": {
    "title": "Learning from Natural Language Explanations for Generalizable Entity Matching",
    "volume": "main",
    "abstract": "Entity matching is the task of linking records from different sources that refer to the same real-world entity. Past work has primarily treated entity linking as a standard supervised learning problem. However, supervised entity matching models often do not generalize well to new data, and collecting exhaustive labeled training data is often cost prohibitive. Further, recent efforts have adopted LLMs for this task in few/zero-shot settings, exploiting their general knowledge. But LLMs are prohibitively expensive for performing inference at scale for real-world entity matching tasks.As an efficient alternative, we re-cast entity matching as a conditional generation task as opposed to binary classification. This enables us to \"distill\" LLM reasoning into smaller entity matching models via natural language explanations. This approach achieves strong performance, especially on out-of-domain generalization tests (10.85% F-1) where standalone generative methods struggle. We perform ablations that highlight the importance of explanations, both for performance and model robustness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Somin Wadhwa",
      "Adit Krishnan",
      "Runhui Wang",
      "Byron Wallace",
      "Luyang Kong"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.353": {
    "title": "Do You Know What You Are Talking About? Characterizing Query-Knowledge Relevance For Reliable Retrieval Augmented Generation",
    "volume": "main",
    "abstract": "Language models (LMs) are known to suffer from hallucinations and misinformation. Retrieval augmented generation (RAG) that retrieves verifiable information from an external knowledge corpus to complement the parametric knowledge in LMs provides a tangible solution to these problems. However, the generation quality of RAG is highly dependent on the relevance between a user's query and the retrieved documents. Inaccurate responses may be generated when the query is outside of the scope of knowledge represented in the external knowledge corpus or if the information in the corpus is out-of-date. In this work, we establish a statistical framework that assesses how well a query can be answered by an RAG system by capturing the relevance of knowledge. We introduce an online testing procedure that employs goodness-of-fit (GoF) tests to inspect the relevance of each user query to detect out-of-knowledge queries with low knowledge relevance. Additionally, we develop an offline testing framework that examines a collection of user queries, aiming to detect significant shifts in the query distribution which indicates the knowledge corpus is no longer sufficiently capable of supporting the interests of the users. We demonstrate the capabilities of these strategies through a systematic evaluation on eight question-answering (QA) datasets, the results of which indicate that the new testing framework is an efficient solution to enhance the reliability of existing RAG systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuohang Li",
      "Jiaxin Zhang",
      "Chao Yan",
      "Kamalika Das",
      "Sricharan Kumar",
      "Murat Kantarcioglu",
      "Bradley Malin"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.354": {
    "title": "On the Reliability of Psychological Scales on Large Language Models",
    "volume": "main",
    "abstract": "Recent research has focused on examining Large Language Models' (LLMs) characteristics from a psychological standpoint, acknowledging the necessity of understanding their behavioral characteristics. The administration of personality tests to LLMs has emerged as a noteworthy area in this context. However, the suitability of employing psychological scales, initially devised for humans, on LLMs is a matter of ongoing debate. Our study aims to determine the reliability of applying personality assessments to LLMs, explicitly investigating whether LLMs demonstrate consistent personality traits. Analysis of 2,500 settings per model, including GPT-3.5, GPT-4, Gemini-Pro, and LLaMA-3.1, reveals that various LLMs show consistency in responses to the Big Five Inventory, indicating a satisfactory level of reliability. Furthermore, our research explores the potential of GPT-3.5 to emulate diverse personalities and represent various groups—a capability increasingly sought after in social sciences for substituting human participants with LLMs to reduce costs. Our findings reveal that LLMs have the potential to represent different personalities with specific prompt instructions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jen-tse Huang",
      "Wenxiang Jiao",
      "Man Ho Lam",
      "Eric John Li",
      "Wenxuan Wang",
      "Michael Lyu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.355": {
    "title": "Contrastive Entity Coreference and Disambiguation for Historical Texts",
    "volume": "main",
    "abstract": "Massive-scale historical document collections are crucial for social science research. Despite increasing digitization, these documents typically lack unique cross-document identifiers for individuals mentioned within the texts, as well as individual identifiers from external knowledge bases like Wikipedia/Wikidata. Existing entity disambiguation methods often fall short in accuracy for historical documents, which are replete with individuals not remembered in contemporary knowledge bases. This study makes three key contributions to improve cross-document coreference resolution and disambiguation in historical texts: a massive-scale training dataset replete with hard negatives - that sources over 190 million entity pairs from Wikipedia contexts and disambiguation pages - high-quality evaluation data from hand-labeled historical newswire articles, and trained models evaluated on this historical benchmark. We contrastively train bi-encoder models for coreferencing and disambiguating individuals in historical texts, achieving accurate, scalable performance that identifies out-of-knowledge base individuals. Our approach significantly surpasses other entity disambiguation models on our historical newswire benchmark. Our models also demonstrate competitive performance on modern entity disambiguation benchmarks, particularly on certain news disambiguation datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhishek Arora",
      "Emily Silcock",
      "Melissa Dell",
      "Leander Heldring"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.356": {
    "title": "Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models",
    "volume": "main",
    "abstract": "Recent advances in instruction-tuned Large Vision-Language Models (LVLMs) have imbued the models with the ability to generate high-level, image-grounded explanations with ease. While such capability is largely attributed to the rich world knowledge contained within the Large Language Models (LLMs), our work reveals their shortcomings in fine-grained visual categorization (FGVC) across six different benchmark settings. Most recent state-of-the-art LVLMs such as LLaVa-1.5, InstructBLIP and GPT-4V not only severely deteriorate in terms of classification performance, e.g., average drop of 65.58 in EM for Stanford Dogs for LLaVA-1.5, but also struggle to generate descriptive visual attributes based on a concept that appears within an input image despite their prominent zero-shot image captioning ability. In-depth analyses show that instruction-tuned LVLMs suffer from modality gap, showing discrepancy when given textual and visual inputs that correspond to the same concept. In an effort to further the community's endeavor in this direction, we propose a multiple granularity attribute-centric benchmark and training mixture, Finer, which aims to establish a ground to evaluate LVLMs' fine-grained visual comprehension ability and provide significantly improved explainability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeonghwan Kim",
      "Heng Ji"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.357": {
    "title": "Evaluating LLMs for Targeted Concept Simplification for Domain-Specific Texts",
    "volume": "main",
    "abstract": "One useful application of NLP models is to support people in reading complex text from unfamiliar domains (e.g., scientific articles). Simplifying the entire text makes it understandable but sometimes removes important details. On the contrary, helping adult readers understand difficult concepts in context can enhance their vocabulary and knowledge. In a preliminary human study, we first identify that lack of context and unfamiliarity with difficult concepts is a major reason for adult readers' difficulty with domain-specific text. We then introduce targeted concept simplification, a simplification task for rewriting text to help readers comprehend text containing unfamiliar concepts. We also introduce WikiDomains, a new dataset of 22k definitions from 13 academic domains paired with a difficult concept within each definition. We benchmark the performance of open-source and commercial LLMs and a simple dictionary baseline on this task across human judgments of ease of understanding and meaning preservation. Interestingly, our human judges preferred explanations about the difficult concept more than simplifications of the concept phrase. Further, no single model achieved superior performance across all quality dimensions, and automated metrics also show low correlations with human evaluations of concept simplification (~0.2), opening up rich avenues for research on personalized human reading comprehension support",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sumit Asthana",
      "Hannah Rashkin",
      "Elizabeth Clark",
      "Fantine Huot",
      "Mirella Lapata"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.358": {
    "title": "VLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language Models Alignment",
    "volume": "main",
    "abstract": "As large vision-language models (LVLMs) evolve rapidly, the demand for high-quality and diverse data to align these models becomes increasingly crucial. However, the creation of such data with human supervision proves costly and time-intensive. In this paper, we investigate the efficacy of AI feedback to scale supervision for aligning LVLMs. We introduce VLFeedback, the first large-scale vision-language feedback dataset, comprising over 82K multi-modal instructions and comprehensive rationales generated by off-the-shelf models without human annotations. To evaluate the effectiveness of AI feedback for vision-language alignment, we train Silkie, an LVLM fine-tuned via direct preference optimization on VLFeedback. Silkie showcases exceptional performance regarding helpfulness, visual faithfulness, and safety metrics. It outperforms its base model by 6.9% and 9.5% in perception and cognition tasks, reduces hallucination issues on MMHal-Bench, and exhibits enhanced resilience against red-teaming attacks. Furthermore, our analysis underscores the advantage of AI feedback, particularly in fostering preference diversity to deliver more comprehensive improvements. Our dataset, training code and models are available at https://vlf-silkie.github.io",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Li",
      "Zhihui Xie",
      "Mukai Li",
      "Shunian Chen",
      "Peiyi Wang",
      "Liang Chen",
      "Yazheng Yang",
      "Benyou Wang",
      "Lingpeng Kong",
      "Qi Liu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.359": {
    "title": "Focused Large Language Models are Stable Many-Shot Learners",
    "volume": "main",
    "abstract": "In-Context Learning (ICL) enables large language models (LLMs) to achieve rapid task adaptation by learning from demonstrations. With the increase in available context length of LLMs, recent experiments have shown that the performance of ICL does not necessarily scale well in many-shot (demonstration) settings. We hypothesize that the reason lies in more demonstrations dispersing the model attention from the query, hindering its understanding of key content, which we validate both theoretically and experimentally. Inspired by how humans learn from examples, we propose a training-free method FocusICL, which conducts triviality filtering to avoid attention being diverted by unimportant contents at token-level and operates hierarchical attention to further ensure sufficient attention towards current query at demonstration-level. We also design an efficient hyperparameter searching strategy for FocusICL based on model perplexity of demonstrations. Comprehensive experiments validate that FocusICL achieves an average performance improvement of 5.2% over vanilla ICL and scales well with many-shot demonstrations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peiwen Yuan",
      "Shaoxiong Feng",
      "Yiwei Li",
      "Xinglin Wang",
      "Yueqi Zhang",
      "Chuyi Tan",
      "Boyuan Pan",
      "Heda Wang",
      "Yao Hu",
      "Kan Li"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.360": {
    "title": "Reconsidering Sentence-Level Sign Language Translation",
    "volume": "main",
    "abstract": "Historically, sign language machine translation has been posed as a sentence-level task: datasets consisting of continuous narratives are chopped up and presented to the model as isolated clips. In this work, we explore the limitations of this task framing. First, we survey a number of linguistic phenomena in sign languages that depend on discourse-level context. Then as a case study, we perform the first human baseline for sign language translation that actually substitutes a human into the machine learning task framing, rather than provide the human with the entire document as context. This human baseline—for ASL to English translation on the How2Sign dataset—shows that for 33% of sentences in our sample, our fluent Deaf signer annotators were only able to understand key parts of the clip in light of additional discourse-level context. These results underscore the importance of understanding and sanity checking examples when adapting machine learning to new domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Garrett Tanzer",
      "Maximus Shengelia",
      "Ken Harrenstien",
      "David Uthus"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.361": {
    "title": "GAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities",
    "volume": "main",
    "abstract": "Perceiving and understanding non-speech sounds and non-verbal speech is essential to making decisions that help us interact with our surroundings. In this paper, we propose GAMA, a novel General-purpose Large Audio-Language Model (LALM) with Advanced Audio Understanding and Complex Reasoning Abilities. We build GAMA by integrating an LLM with multiple types of audio representations, including features from a custom Audio Q-Former, a multi-layer aggregator that aggregates features from multiple layers of an audio encoder. We fine-tune GAMA on a large-scale audio-language dataset, which augments it with audio understanding capabilities. Next, we propose CompA-R (Instruction-Tuning for Complex Audio Reasoning), a synthetically generated instruction-tuning (IT) dataset with instructions that require the model to perform complex reasoning on the input audio. We instruction-tune GAMA with CompA-R to endow it with complex reasoning abilities, where we further add a soft prompt as input with high-level semantic evidence by leveraging event tags of the input audio. Finally, we also propose CompA-R-test, a human-labeled evaluation dataset for evaluating the capabilities of LALMs on open-ended audio question-answering that requires complex reasoning. Through automated and expert human evaluations, we show that GAMA outperforms all other LALMs in literature on diverse audio understanding tasks by margins of 1%-84% and demonstrates state-of-the-art performance on deductive reasoning and hallucination evaluation benchmarks. Further, GAMA IT-ed on CompA-R proves to be superior in its complex reasoning capabilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sreyan Ghosh",
      "Sonal Kumar",
      "Ashish Seth",
      "Chandra Kiran Evuru",
      "Utkarsh Tyagi",
      "S Sakshi",
      "Oriol Nieto",
      "Ramani Duraiswami",
      "Dinesh Manocha"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.362": {
    "title": "Verba volant, scripta volant? Don't worry! There are computational solutions for protoword reconstruction",
    "volume": "main",
    "abstract": "We introduce a new database of cognate words and etymons for the five main Romance languages, the most comprehensive one to date. We propose a strong benchmark for the automatic reconstruction of protowords for Romance languages, by applying a set of machine learning models and features on these data. The best results reach 90% accuracy in predicting the protoword of a given cognate set, surpassing existing state-of-the-art results for this task and showing that computational methods can be very useful in assisting linguists with protoword reconstruction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liviu Dinu",
      "Ana Uban",
      "Alina Cristea",
      "Ioan-Bogdan Iordache",
      "Teodor-George Marchitan",
      "Simona Georgescu",
      "Laurentiu Zoicas"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.363": {
    "title": "ChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context",
    "volume": "main",
    "abstract": "While the biases of language models in production are extensively documented, the biases of their guardrails have been neglected. This paper studies how contextual information about the user influences the likelihood of an LLM to refuse to execute a request. By generating user biographies that offer ideological and demographic information, we find a number of biases in guardrail sensitivity on GPT-3.5. Younger, female, and Asian-American personas are more likely to trigger a refusal guardrail when requesting censored or illegal information. Guardrails are also sycophantic, refusing to comply with requests for a political position the user is likely to disagree with. We find that certain identity groups and seemingly innocuous information, e.g., sports fandom, can elicit changes in guardrail sensitivity similar to direct statements of political ideology. For each demographic category and even for American football team fandom, we find that ChatGPT appears to infer a likely political ideology and modify guardrail behavior accordingly",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Victoria Li",
      "Yida Chen",
      "Naomi Saphra"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.364": {
    "title": "Personas as a Way to Model Truthfulness in Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) are trained on vast amounts of text from the internet, which contains both factual and misleading information about the world. While unintuitive from a classic view of LMs, recent work has shown that the truth value of a statement can be elicited from the model's representations. This paper presents an explanation for why LMs appear to know the truth despite not being trained with truth labels. We hypothesize that the pretraining data is generated by groups of (un)truthful agents whose outputs share common features, and they form a (un)truthful persona. By training on this data, LMs can infer and represent the persona in its activation space. This allows the model to separate truth from falsehoods and controls the truthfulness of its generation. We show evidence for the persona hypothesis via two observations: (1) we can probe whether a model's answer will be truthful before it is generated; (2) finetuning a model on a set of facts improves its truthfulness on unseen topics. Next, using arithmetics as a synthetic environment, we show that structures of the pretraining data are crucial for the model to infer the truthful persona. Overall, our findings suggest that models can exploit hierarchical structures in the data to learn abstract concepts like truthfulness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nitish Joshi",
      "Javier Rando",
      "Abulhair Saparov",
      "Najoung Kim",
      "He He"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.365": {
    "title": "Satyrn: A Platform for Analytics Augmented Generation",
    "volume": "main",
    "abstract": "Large language models (LLMs) are capable of producing documents, and retrieval augmented generation (RAG) has shown itself to be a powerful method for improving accuracy without sacrificing fluency. However, not all information can be retrieved from text. We propose an approach that uses the analysis of structured data to generate fact sets that are used to guide generation in much the same way that retrieved documents are used in RAG. This analytics augmented generation (AAG) approach supports the ability to utilize standard analytic techniques to generate facts that are then converted to text and passed to an LLM. We present a neurosymbolic platform, Satyrn, that leverages AAG to produce accurate, fluent, and coherent reports grounded in large scale databases. In our experiments, we find that Satyrn generates reports in which over 86% of claims are accurate while maintaining high levels of fluency and coherence, even when using smaller language models such as Mistral-7B, as compared to GPT-4 Code Interpreter in which just 57% of claims are accurate",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marko Sterbentz",
      "Cameron Barrie",
      "Shubham Shahi",
      "Abhratanu Dutta",
      "Donna Hooshmand",
      "Harper Pack",
      "Kristian Hammond"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.366": {
    "title": "EH-MAM: Easy-to-Hard Masked Acoustic Modeling for Self-Supervised Speech Representation Learning",
    "volume": "main",
    "abstract": "In this paper, we present EH-MAM (Easy-to-Hard adaptive Masked Acoustic Modeling), a novel self-supervised learning approach for speech representation learning. In contrast to the prior methods that use random masking schemes for Masked Acoustic Modeling (MAM), we introduce a novel selective and adaptive masking strategy. Specifically, during SSL training, we progressively introduce harder regions to the model for reconstruction. Our approach automatically selects hard regions and is built on the observation that the reconstruction loss of individual frames in MAM can provide natural signals to judge the difficulty of solving the MAM pre-text task for that frame. To identify these hard regions, we employ a teacher model that first predicts the frame-wise losses and then decides which frames to mask. By learning to create challenging problems, such as identifying harder frames and solving them simultaneously, the model is able to learn more effective representations and thereby acquire a more comprehensive understanding of the speech. Quantitatively, EH-MAM outperforms several state-of-the-art baselines across various low-resource speech recognition and SUPERB benchmarks by 5%-10%. Additionally, we conduct a thorough analysis to show that the regions masked by EH-MAM effectively capture useful context across speech frames",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashish Seth",
      "Ramaneswaran Selvakumar",
      "S Sakshi",
      "Sonal Kumar",
      "Sreyan Ghosh",
      "Dinesh Manocha"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.367": {
    "title": "EPO: Hierarchical LLM Agents with Environment Preference Optimization",
    "volume": "main",
    "abstract": "Long-horizon decision-making tasks present significant challenges for LLM-based agents due to the need for extensive planning over multiple steps. In this paper, we propose a hierarchical framework that decomposes complex tasks into manageable subgoals, utilizing separate LLMs for subgoal prediction and low-level action generation. To address the challenge of creating training signals for unannotated datasets, we develop a reward model that leverages multimodal environment feedback to automatically generate reward signals. We introduce Environment Preference Optimization (EPO), a novel method that generates preference signals from the environment's feedback and uses them to train LLM-based agents. Extensive experiments on ALFRED demonstrate the state-of-the-art performance of our framework, achieving first place on the ALFRED public leaderboard and showcasing its potential to improve long-horizon decision-making in diverse environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Zhao",
      "Haotian Fu",
      "Chen Sun",
      "George Konidaris"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.368": {
    "title": "Detection and Measurement of Syntactic Templates in Generated Text",
    "volume": "main",
    "abstract": "The diversity of text can be measured beyond word-level features, however existing diversity evaluation focuses primarily on word-level features. Here we propose a method for evaluating diversity over syntactic features to characterize general repetition in models, beyond frequent n-grams. Specifically, we define syntactic templates (e.g., strings comprising parts-of-speech) and show that models tend to produce templated text in downstream tasks at a higher rate than what is found in human-reference textsWe find that most (76%) templates in model-generated text can be found in pre-training data (compared to only 35% of human-authored text), and are not overwritten during fine-tuning or alignment processes such as RLHF. The connection between templates in generated text and the pre-training data allows us to analyze syntactic templates in models where we do not have the pre-training data.We also find that templates as features are able to differentiate between models, tasks, and domains, and are useful for qualitatively evaluating common model constructions.Finally, we demonstrate the use of templates as a useful tool for analyzing style memorization of training data in LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chantal Shaib",
      "Yanai Elazar",
      "Junyi Jessy Li",
      "Byron Wallace"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.369": {
    "title": "UOUO: Uncontextualized Uncommon Objects for Measuring Knowledge Horizons of Vision Language Models",
    "volume": "main",
    "abstract": "Smaller-scale Vision-Language Models (VLMs) often claim to perform on par with larger models in general-domain visual grounding and question-answering benchmarks while offering advantages in computational efficiency and storage. However, their ability to handle rare objects, which fall into the long tail of data distributions, is less understood. To rigorously evaluate this aspect, we introduce the \"Uncontextualized Uncommon Objects\" (UOUO) benchmark. This benchmark focuses on systematically testing VLMs with both large and small parameter counts on rare and specialized objects. Our comprehensive analysis reveals that while smaller VLMs maintain competitive performance on common datasets, they significantly underperform on tasks involving uncommon objects. We also propose an advanced, scalable pipeline for data collection and cleaning, ensuring the UOUO benchmark provides high-quality, challenging instances. These findings highlight the need to consider long-tail distributions when assessing the true capabilities of VLMs. Code and project details for UOUO can be found at https://zoezheng126.github.io/UOUO-Website/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Pi",
      "Mingyuan Wu",
      "Jize Jiang",
      "Haozhen Zheng",
      "Beitong Tian",
      "ChengXiang Zhai",
      "Klara Nahrstedt",
      "Zhiting Hu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.370": {
    "title": "Optimized Speculative Sampling for GPU Hardware Accelerators",
    "volume": "main",
    "abstract": "In this work, we optimize speculative sampling for parallel hardware accelerators to improve sampling speed. We notice that substantial portions of the intermediate matrices necessary for speculative sampling can be computed concurrently. This allows us to distribute the workload across multiple GPU threads, enabling simultaneous operations on matrix segments within thread blocks. This results in profiling time improvements ranging from 6% to 13% relative to the baseline implementation, without compromising accuracy. To further accelerate speculative sampling, probability distributions parameterized by softmax are approximated by sigmoid. This approximation approach results in significantly greater relative improvements in profiling time, ranging from 37% to 94%, with a minor decline in accuracy. We conduct extensive experiments on both automatic speech recognition and summarization tasks to validate the effectiveness of our optimization methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dominik Wagner",
      "Seanie Lee",
      "Ilja Baumann",
      "Philipp Seeberger",
      "Korbinian Riedhammer",
      "Tobias Bocklet"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.371": {
    "title": "Personalized Pieces: Efficient Personalized Large Language Models through Collaborative Efforts",
    "volume": "main",
    "abstract": "Personalized large language models (LLMs) aim to tailor interactions, content, and recommendations to individual user preferences. While parameter-efficient fine-tuning (PEFT) methods excel in performance and generalization, they are costly and limit communal benefits when used individually. To this end, we introduce Personalized Pieces (Per-Pcs), a framework that allows users to safely share and assemble personalized PEFT efficiently with collaborative efforts. Per-Pcs involves selecting sharers, breaking their PEFT into pieces, and training gates for each piece. These pieces are added to a pool, from which target users can select and assemble personalized PEFT using their history data. This approach preserves privacy and enables fine-grained user modeling without excessive storage and computation demands. Experimental results show Per-Pcs outperforms non-personalized and PEFT retrieval baselines, offering performance comparable to OPPU with significantly lower resource use across six tasks. Further analysis highlights Per-Pcs's robustness concerning sharer count and selection strategy, pieces sharing ratio, and scalability in computation time and storage space. Per-Pcs's modularity promotes safe sharing, making LLM personalization more efficient, effective, and widely accessible through collaborative efforts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoxuan Tan",
      "Zheyuan Liu",
      "Meng Jiang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.372": {
    "title": "Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning",
    "volume": "main",
    "abstract": "Personalization in large language models (LLMs) is increasingly important, aiming to align the LLMs' interactions, content, and recommendations with individual user preferences. Recent advances have highlighted effective prompt design by enriching user queries with non-parametric knowledge through behavior history retrieval and textual profiles. However, these methods faced limitations due to a lack of model ownership, resulting in constrained customization and privacy issues, and often failed to capture complex, dynamic user behavior patterns. To address these shortcomings, we introduce One PEFT Per User (OPPU), employing personalized parameter-efficient fine-tuning (PEFT) modules to store user-specific behavior patterns and preferences. By plugging in personal PEFT parameters, users can own and use their LLMs individually. OPPU integrates parametric user knowledge in the personal PEFT parameters with non-parametric knowledge from retrieval and profiles, adapting LLMs to user behavior shifts. Experimental results demonstrate that OPPU significantly outperforms existing prompt-based methods across seven diverse tasks in the LaMP benchmark. Further studies reveal OPPU's enhanced capabilities in handling user behavior shifts, modeling users at different activity levels, maintaining robustness across various user history formats, and displaying versatility with different PEFT methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoxuan Tan",
      "Qingkai Zeng",
      "Yijun Tian",
      "Zheyuan Liu",
      "Bing Yin",
      "Meng Jiang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.373": {
    "title": "Unifying Multimodal Retrieval via Document Screenshot Embedding",
    "volume": "main",
    "abstract": "In the real world, documents are organized in different formats and varied modalities. Traditional retrieval pipelines require tailored document parsing techniques and content extraction modules to prepare input for indexing. This process is tedious, prone to errors, and has information loss. To this end, we propose Document Screenshot Embedding (DSE), a novel retrieval paradigm that regards document screenshots as a unified input format, which does not require any content extraction preprocess and preserves all the information in a document (e.g., text, image and layout). DSE leverages a large vision-language model to directly encode document screenshots into dense representations for retrieval. To evaluate our method, we first craft the dataset of Wiki-SS, a 1.3M Wikipedia web page screenshots as the corpus to answer the questions from the Natural Questions dataset. In such a text-intensive document retrieval setting, DSE shows competitive effectiveness compared to other text retrieval methods relying on parsing. For example, DSE outperforms BM25 by 17 points in top-1 retrieval accuracy. Additionally, in a mixed-modality task of slide retrieval, DSE significantly outperforms OCR text retrieval methods by over 15 points in nDCG@10. These experiments show that DSE is an effective document retrieval paradigm for diverse types of documents. Model checkpoints, code, and Wiki-SS collection will be released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xueguang Ma",
      "Sheng-Chieh Lin",
      "Minghan Li",
      "Wenhu Chen",
      "Jimmy Lin"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.374": {
    "title": "Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation",
    "volume": "main",
    "abstract": "Training a unified multilingual model promotes knowledge transfer but inevitably introduces negative interference. Language-specific modeling methods show promise in reducing interference. However, they often rely on heuristics to distribute capacity and struggle to foster cross-lingual transfer via isolated modules. In this paper, we explore intrinsic task modularity within multilingual networks and leverage these observations to circumvent interference under multilingual translation. We show that neurons in the feed-forward layers tend to be activated in a language-specific manner. Meanwhile, these specialized neurons exhibit structural overlaps that reflect language proximity, which progress across layers. Based on these findings, we propose Neuron Specialization, an approach that identifies specialized neurons to modularize feed-forward layers and then continuously updates them through sparse networks. Extensive experiments show that our approach achieves consistent performance gains over strong baselines with additional analyses demonstrating reduced interference and increased knowledge transfer",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaomu Tan",
      "Di Wu",
      "Christof Monz"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.375": {
    "title": "An Audit on the Perspectives and Challenges of Hallucinations in NLP",
    "volume": "main",
    "abstract": "We audit how hallucination in large language models (LLMs) is characterized in peer-reviewed literature, using a critical examination of 103 publications across NLP research. Through the examination of the literature, we identify a lack of agreement with the term ‘hallucination' in the field of NLP. Additionally, to compliment our audit, we conduct a survey with 171 practitioners from the field of NLP and AI to capture varying perspectives on hallucination. Our analysis calls for the necessity of explicit definitions and frameworks outlining hallucination within NLP, highlighting potential challenges, and our survey inputs provide a thematic understanding of the influence and ramifications of hallucination in society",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pranav Narayanan Venkit",
      "Tatiana Chakravorti",
      "Vipul Gupta",
      "Heidi Biggs",
      "Mukund Srinath",
      "Koustava Goswami",
      "Sarah Rajtmajer",
      "Shomir Wilson"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.376": {
    "title": "Discovering Knowledge-Critical Subnetworks in Pretrained Language Models",
    "volume": "main",
    "abstract": "Pretrained language models (LMs) encode implicit representations of knowledge in their parameters. However, localizing these representations and disentangling them from each other remains an open problem. In this work, we investigate whether pretrained language models contain various *knowledge-critical* subnetworks: particular sparse computational subgraphs that can, if removed, precisely suppress specific knowledge the model has memorized. We propose a multi-objective differentiable masking scheme that can be applied to both weights and neurons to discover such subnetworks and show that we can use them to precisely remove specific knowledge from models while minimizing adverse effects on the behavior of the original model. We demonstrate our method on multiple GPT2 variants, uncovering highly sparse subnetworks (98%+ sparsity) that are critical for expressing specific collections of relational knowledge. When these subnetworks are removed, the remaining network maintains most of its initial abilities but struggles to represent the suppressed knowledge",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Deniz Bayazit",
      "Negar Foroutan",
      "Zeming Chen",
      "Gail Weiss",
      "Antoine Bosselut"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.377": {
    "title": "Reconstruct Your Previous Conversations! Comprehensively Investigating Privacy Leakage Risks in Conversations with GPT Models",
    "volume": "main",
    "abstract": "Significant advancements have recently been made in large language models, represented by GPT models.Users frequently have multi-round private conversations with cloud-hosted GPT models for task optimization.Yet, this operational paradigm introduces additional attack surfaces, particularly in custom GPTs and hijacked chat sessions.In this paper, we introduce a straightforward yet potent Conversation Reconstruction Attack.This attack targets the contents of previous conversations between GPT models and benign users, i.e., the benign users' input contents during their interaction with GPT models.The adversary could induce GPT models to leak such contents by querying them with designed malicious prompts.Our comprehensive examination of privacy risks during the interactions with GPT models under this attack reveals GPT-4's considerable resilience.We present two advanced attacks targeting improved reconstruction of past conversations, demonstrating significant privacy leakage across all models under these advanced techniques.Evaluating various defense mechanisms, we find them ineffective against these attacks.Our findings highlight the ease with which privacy can be compromised in interactions with GPT models, urging the community to safeguard against potential abuses of these models' capabilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junjie Chu",
      "Zeyang Sha",
      "Michael Backes",
      "Yang Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.378": {
    "title": "Right for Right Reasons: Large Language Models for Verifiable Commonsense Knowledge Graph Question Answering",
    "volume": "main",
    "abstract": "Knowledge Graph Question Answering (KGQA) methods seek to answer Natural Language questions using the relational information stored in Knowledge Graphs (KGs). With the recent advancements of Large Language Models (LLMs) and their remarkable reasoning abilities, there is a growing trend to leverage them for KGQA. However, existing methodologies have only focused on answering factual questions, e.g., *\"In which city was Silvio Berlusconi's first wife born?\"*, leaving questions involving commonsense reasoning that real-world users may pose more often, e.g., *\"Do I need separate visas to see the Venus of Willendorf and attend the Olympics this summer?\"* unaddressed. In this work, we first observe that existing LLM-based methods for KGQA struggle with hallucination on such questions, especially on queries targeting long-tail entities (e.g., non-mainstream and recent entities), thus hindering their applicability in real-world applications especially since their reasoning processes are not easily verifiable. In response, we propose Right for Right Reasons (R3), a commonsense KGQA methodology that allows for a verifiable reasoning procedure by axiomatically surfacing intrinsic commonsense knowledge of LLMs and grounding every factual reasoning step on KG triples. Through experimental evaluations across three different tasks—question answering, claim verification, and preference matching—our findings showcase R3 as a superior approach, outperforming existing methodologies and notably reducing instances of hallucination and reasoning errors",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Armin Toroghi",
      "Willis Guo",
      "Mohammad Mahdi Abdollah Pour",
      "Scott Sanner"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.379": {
    "title": "Verifiable, Debuggable, and Repairable Commonsense Logical Reasoning via LLM-based Theory Resolution",
    "volume": "main",
    "abstract": "Recent advances in Large Language Models (LLM) have led to substantial interest in their application to commonsense reasoning tasks. Despite their potential, LLMs are susceptible to reasoning errors and hallucinations that may be harmful in use cases where accurate reasoning is critical. This challenge underscores the need for verifiable, debuggable, and repairable LLM reasoning. Recent works have made progress toward verifiable reasoning with LLMs by using them as either (i) a reasoner over an axiomatic knowledge base, or (ii) a semantic parser for use in existing logical inference systems. However, both settings are unable to extract commonsense axioms from the LLM that are not already formalized in the knowledge base, and also lack a reliable method to repair missed commonsense inferences. In this work, we present LLM-TRes, a logical reasoning framework based on the notion of \"theory resolution\" that allows for seamless integration of the commonsense knowledge from LLMs with a verifiable logical reasoning framework that mitigates hallucinations and facilitates debugging of the reasoning procedure as well as repair. We crucially prove that repaired axioms are theoretically guaranteed to be given precedence over flawed ones in our theory resolution inference process. We conclude by evaluating on three diverse language-based reasoning tasks—preference reasoning, deductive reasoning, and causal commonsense reasoning—and demonstrate the superior performance of LLM-TRes vs. state-of-the-art LLM-based reasoning methods in terms of both accuracy and reasoning correctness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Armin Toroghi",
      "Willis Guo",
      "Ali Pesaranghader",
      "Scott Sanner"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.380": {
    "title": "Understanding and Mitigating Language Confusion in LLMs",
    "volume": "main",
    "abstract": "We investigate a surprising limitation of LLMs: their inability to consistently generate text in a user's desired language. We create the Language Confusion Benchmark (LCB) to evaluate such failures, covering 15 typologically diverse languages with existing and newly-created English and multilingual prompts. We evaluate a range of LLMs on monolingual and cross-lingual generation reflecting practical use cases, finding that Llama Instruct and Mistral models exhibit high degrees of language confusion and even the strongest models fail to consistently respond in the correct language. We observe that base and English-centric instruct models are more prone to language confusion, which is aggravated by complex prompts and high sampling temperatures. We find that language confusion can be partially mitigated via few-shot prompting, multilingual SFT and preference tuning. We release our language confusion benchmark, which serves as a first layer of efficient, scalable multilingual evaluation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kelly Marchisio",
      "Wei-Yin Ko",
      "Alexandre Berard",
      "Théo Dehaze",
      "Sebastian Ruder"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.381": {
    "title": "Can Large Language Models Learn Independent Causal Mechanisms?",
    "volume": "main",
    "abstract": "Despite impressive performance on language modelling and complex reasoning tasks, Large Language Models (LLMs) fall short on the same tasks in uncommon settings or with distribution shifts, exhibiting a lack of generalisation ability. By contrast, systems such as causal models, that learn abstract variables and causal relationships, can demonstrate increased robustness against changes in the distribution. One reason for this success is the existence and use of Independent Causal Mechanisms (ICMs) representing high-level concepts that only sparsely interact. In this work, we apply two concepts from causality to learn ICMs within LLMs. We develop a new LLM architecture composed of multiple sparsely interacting language modelling modules. We show that such causal constraints can improve out-of-distribution performance on abstract and causal reasoning tasks. We also investigate the level of independence and domain specialisation and show that LLMs rely on pre-trained partially domain-invariant mechanisms resilient to fine-tuning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gael Gendron",
      "Bao Nguyen",
      "Alex Peng",
      "Michael Witbrock",
      "Gillian Dobbie"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.382": {
    "title": "MirrorStories: Reflecting Diversity through Personalized Narrative Generation with Large Language Models",
    "volume": "main",
    "abstract": "This study explores the effectiveness of Large Language Models (LLMs) in creating personalized \"mirror stories\" that reflect and resonate with individual readers' identities, addressing the significant lack of diversity in literature. We present MirrorStories, a corpus of 1,500 personalized short stories generated by integrating elements such as name, gender, age, ethnicity, reader interest, and story moral. We demonstrate that LLMs can effectively incorporate diverse identity elements into narratives, with human evaluators identifying personalized elements in the stories with high accuracy. Through a comprehensive evaluation involving 26 diverse human judges, we compare the effectiveness of MirrorStories against generic narratives. We find that personalized LLM-generated stories not only outscore generic human-written and LLM-generated ones across all metrics of engagement (with average ratings of 4.22 versus 3.37 on a 5-point scale), but also achieve higher textual diversity while preserving the intended moral. We also provide analyses that include bias assessments and a study on the potential for integrating images into personalized stories",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sarfaroz Yunusov",
      "Hamza Sidat",
      "Ali Emami"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.383": {
    "title": "InterIntent: Investigating Social Intelligence of LLMs via Intention Understanding in an Interactive Game Context",
    "volume": "main",
    "abstract": "Large language models (LLMs) have demonstrated the potential to mimic human social intelligence. However, most studies focus on simplistic and static self-report or performance-based tests, which limits the depth and validity of the analysis. In this paper, we developed a novel framework, InterIntent, to assess LLMs' social intelligence by mapping their ability to understand and manage intentions in a game setting. We focus on four dimensions of social intelligence: situational awareness, self-regulation, self-awareness, and theory of mind. Each dimension is linked to a specific game task: intention selection, intention following, intention summarization, and intention guessing. Our findings indicate that while LLMs exhibit high proficiency in selecting intentions, achieving an accuracy of 88%, their ability to infer the intentions of others is significantly weaker, trailing human performance by 20%. Additionally, game performance correlates with intention understanding, highlighting the importance of the four components towards success in this game. These findings underline the crucial role of intention understanding in evaluating LLMs' social intelligence and highlight the potential of using social deduction games as a complex testbed to enhance LLM evaluation. InterIntent contributes a structured approach to bridging the evaluation gap in social intelligence within multiplayer LLM-based games",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyi Liu",
      "Abhishek Anand",
      "Pei Zhou",
      "Jen-tse Huang",
      "Jieyu Zhao"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.384": {
    "title": "Locating Information Gaps and Narrative Inconsistencies Across Languages: A Case Study of LGBT People Portrayals on Wikipedia",
    "volume": "main",
    "abstract": "To explain social phenomena and identify systematic biases, much research in computational social science focuses on comparative text analyses. These studies often rely on coarse corpus-level statistics or local word-level analyses, mainly in English. We introduce the InfoGap method—an efficient and reliable approach to locating information gaps and inconsistencies in articles at the fact level, across languages. We evaluate InfoGap by analyzing LGBT people's portrayals, across 2.7K biography pages on English, Russian, and French Wikipedias. We find large discrepancies in factual coverage across the languages. Moreover, our analysis reveals that biographical facts carrying negative connotations are more likely to be highlighted in Russian Wikipedia. Crucially, InfoGap both facilitates large scale analyses, and pinpoints local document- and fact-level information gaps, laying a new foundation for targeted and nuanced comparative language analysis at scale",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Farhan Samir",
      "Chan Young Park",
      "Anjalie Field",
      "Vered Shwartz",
      "Yulia Tsvetkov"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.385": {
    "title": "From Local Concepts to Universals: Evaluating the Multicultural Understanding of Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mehar Bhatia",
      "Sahithya Ravi",
      "Aditya Chinchure",
      "EunJeong Hwang",
      "Vered Shwartz"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.386": {
    "title": "Dynamic Multi-Reward Weighting for Multi-Style Controllable Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karin De Langis",
      "Ryan Koo",
      "Dongyeop Kang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.387": {
    "title": "MMNeuron: Discovering Neuron-Level Domain-Specific Interpretation in Multimodal Large Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahao Huo",
      "Yibo Yan",
      "Boren Hu",
      "Yutao Yue",
      "Xuming Hu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.388": {
    "title": "Learning to Extract Structured Entities Using Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haolun Wu",
      "Ye Yuan",
      "Liana Mikaelyan",
      "Alexander Meulemans",
      "Xue Liu",
      "James Hensman",
      "Bhaskar Mitra"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.389": {
    "title": "Efficient LLM Comparative Assessment: A Product of Experts Framework for Pairwise Comparisons",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adian Liusie",
      "Vatsal Raina",
      "Yassir Fathullah",
      "Mark Gales"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.390": {
    "title": "A Survey of AMR Applications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shira Wein",
      "Juri Opitz"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.391": {
    "title": "Beyond Embeddings: The Promise of Visual Table in Visual Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiwu Zhong",
      "Zi-Yuan Hu",
      "Michael Lyu",
      "Liwei Wang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.392": {
    "title": "CareCorpus+: Expanding and Augmenting Caregiver Strategy Data to Support Pediatric Rehabilitation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shahla Farzana",
      "Ivana Lucero",
      "Vivian Villegas",
      "Vera Kaelin",
      "Mary Khetani",
      "Natalie Parde"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.393": {
    "title": "Taylor Unswift: Secured Weight Release for Large Language Models via Taylor Expansion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanchu Wang",
      "Yu-Neng Chuang",
      "Ruixiang Tang",
      "Shaochen Zhong",
      "Jiayi Yuan",
      "Hongye Jin",
      "Zirui Liu",
      "Vipin Chaudhary",
      "Shuai Xu",
      "James Caverlee",
      "Xia Hu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.394": {
    "title": "TimeR4 : Time-aware Retrieval-Augmented Large Language Models for Temporal Knowledge Graph Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinying Qian",
      "Ying Zhang",
      "Yu Zhao",
      "Baohang Zhou",
      "Xuhui Sui",
      "Li Zhang",
      "Kehui Song"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.395": {
    "title": "Knowledge-Centric Hallucination Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangkun Hu",
      "Dongyu Ru",
      "Lin Qiu",
      "Qipeng Guo",
      "Tianhang Zhang",
      "Yang Xu",
      "Yun Luo",
      "Pengfei Liu",
      "Yue Zhang",
      "Zheng Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.396": {
    "title": "Revealing the Parallel Multilingual Learning within Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongyu Mu",
      "Peinan Feng",
      "Zhiquan Cao",
      "Yuzhang Wu",
      "Bei Li",
      "Chenglong Wang",
      "Tong Xiao",
      "Kai Song",
      "Tongran Liu",
      "Chunliang Zhang",
      "JingBo Zhu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.397": {
    "title": "Automatic Instruction Evolving for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weihao Zeng",
      "Can Xu",
      "Yingxiu Zhao",
      "Jian-Guang Lou",
      "Weizhu Chen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.398": {
    "title": "RepEval: Effective Text Evaluation with LLM Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuqian Sheng",
      "Yi Xu",
      "Tianhang Zhang",
      "Zanwei Shen",
      "Luoyi Fu",
      "Jiaxin Ding",
      "Lei Zhou",
      "Xiaoying Gan",
      "Xinbing Wang",
      "Chenghu Zhou"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.399": {
    "title": "Generative Models for Automatic Medical Decision Rule Extraction from Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxin He",
      "Buzhou Tang",
      "Xiaoling Wang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.400": {
    "title": "Encoding and Controlling Global Semantics for Long-form Video Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thong Nguyen",
      "Zhiyuan Hu",
      "Xiaobao Wu",
      "Cong-Duy Nguyen",
      "See-Kiong Ng",
      "Anh Tuan Luu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.401": {
    "title": "Towards Understanding Jailbreak Attacks in LLMs: A Representation Space Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuping Lin",
      "Pengfei He",
      "Han Xu",
      "Yue Xing",
      "Makoto Yamada",
      "Hui Liu",
      "Jiliang Tang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.402": {
    "title": "Enhancing Legal Case Retrieval via Scaling High-quality Synthetic Query-Candidate Pairs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Gao",
      "Chaojun Xiao",
      "Zhenghao Liu",
      "Huimin Chen",
      "Zhiyuan Liu",
      "Maosong Sun"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.403": {
    "title": "Does Large Language Model Contain Task-Specific Neurons?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ran Song",
      "Shizhu He",
      "Shuting Jiang",
      "Yantuan Xian",
      "Shengxiang Gao",
      "Kang Liu",
      "Zhengtao Yu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.404": {
    "title": "Liar, Liar, Logical Mire: A Benchmark for Suppositional Reasoning in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Philipp Mondorf",
      "Barbara Plank"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.405": {
    "title": "Advancing Test-Time Adaptation in Wild Acoustic Test Settings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongfu Liu",
      "Hengguan Huang",
      "Ye Wang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.406": {
    "title": "Learning to Retrieve Iteratively for In-Context Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunmo Chen",
      "Tongfei Chen",
      "Harsh Jhamtani",
      "Patrick Xia",
      "Richard Shin",
      "Jason Eisner",
      "Benjamin Van Durme"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.407": {
    "title": "Taxonomy-guided Semantic Indexing for Academic Paper Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "SeongKu Kang",
      "Yunyi Zhang",
      "Pengcheng Jiang",
      "Dongha Lee",
      "Jiawei Han",
      "Hwanjo Yu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.408": {
    "title": "Python is Not Always the Best Choice: Embracing Multilingual Program of Thoughts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xianzhen Luo",
      "Qingfu Zhu",
      "Zhiming Zhang",
      "Libo Qin",
      "Xuanyu Zhang",
      "Qing Yang",
      "Dongliang Xu",
      "Wanxiang Che"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.409": {
    "title": "Advancing Adversarial Suffix Transfer Learning on Aligned Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongfu Liu",
      "Yuxi Xie",
      "Ye Wang",
      "Michael Shieh"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.410": {
    "title": "Incomplete Utterance Rewriting with Editing Operation Guidance and Utterance Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyu Cao",
      "Peifeng Li",
      "Yaxin Fan",
      "Qiaoming Zhu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.411": {
    "title": "FRoG: Evaluating Fuzzy Reasoning of Generalized Quantifiers in LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiyuan Li",
      "Shichao Sun",
      "Pengfei Liu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.412": {
    "title": "Aligning Large Language Models with Diverse Political Viewpoints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dominik Stammbach",
      "Philine Widmer",
      "Eunjung Cho",
      "Caglar Gulcehre",
      "Elliott Ash"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.413": {
    "title": "You Gotta be a Doctor, Lin\" : An Investigation of Name-Based Bias of Large Language Models in Employment Recommendations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huy Nghiem",
      "John Prindle",
      "Jieyu Zhao",
      "Hal Daumé Iii"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.414": {
    "title": "Extending Context Window of Large Language Models from a Distributional Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingsheng Wu",
      "Yuxuan Gu",
      "Xiaocheng Feng",
      "Weihong Zhong",
      "Dongliang Xu",
      "Qing Yang",
      "Hongtao Liu",
      "Bing Qin"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.415": {
    "title": "Leveraging pre-trained language models for linguistic analysis: A case of argument structure constructions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hakyung Sung",
      "Kristopher Kyle"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.416": {
    "title": "MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lin Xu",
      "Zhiyuan Hu",
      "Daquan Zhou",
      "Hongyu Ren",
      "Zhen Dong",
      "Kurt Keutzer",
      "See-Kiong Ng",
      "Jiashi Feng"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.417": {
    "title": "Position Engineering: Boosting Large Language Models through Positional Information Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan He",
      "Huiqiang Jiang",
      "Zilong Wang",
      "Yuqing Yang",
      "Luna Qiu",
      "Lili Qiu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.418": {
    "title": "Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junying Chen",
      "Chi Gui",
      "Ruyi Ouyang",
      "Anningzhe Gao",
      "Shunian Chen",
      "Guiming Chen",
      "Xidong Wang",
      "Zhenyang Cai",
      "Ke Ji",
      "Xiang Wan",
      "Benyou Wang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.419": {
    "title": "ADELIE: Aligning Large Language Models on Information Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunjia Qi",
      "Hao Peng",
      "Xiaozhi Wang",
      "Bin Xu",
      "Lei Hou",
      "Juanzi Li"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.420": {
    "title": "Unveiling Factual Recall Behaviors of Large Language Models through Knowledge Neurons",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Wang",
      "Yuheng Chen",
      "Wanting Wen",
      "Yu Sheng",
      "Linjing Li",
      "Daniel Zeng"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.421": {
    "title": "Lexically Grounded Subword Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jindřich Libovický",
      "Jindřich Helcl"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.422": {
    "title": "EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhui Li",
      "Fangyun Wei",
      "Chao Zhang",
      "Hongyang Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.423": {
    "title": "Do Text-to-Vis Benchmarks Test Real Use of Visualisations?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hy Nguyen",
      "Xuefei He",
      "Andrew Reeson",
      "Cecile Paris",
      "Josiah Poon",
      "Jonathan Kummerfeld"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.424": {
    "title": "Gold Panning in Vocabulary: An Adaptive Method for Vocabulary Expansion of Domain-Specific LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengyuan Liu",
      "Shihang Wang",
      "Lizhi Qing",
      "Kun Kuang",
      "Yangyang Kang",
      "Changlong Sun",
      "Fei Wu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.425": {
    "title": "Strategic Demonstration Selection for Improved Fairness in LLM In-Context Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyu Hu",
      "Weiru Liu",
      "Mengnan Du"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.426": {
    "title": "Multi-Dialect Vietnamese: Task, Dataset, Baseline Models and Challenges",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nguyen Dinh",
      "Thanh Dang",
      "Luan Thanh Nguyen",
      "Kiet Nguyen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.427": {
    "title": "Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vyas Raina",
      "Adian Liusie",
      "Mark Gales"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.428": {
    "title": "Rethinking the Reversal Curse of LLMs: a Prescription from Human Knowledge Reversal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhicong Lu",
      "Li Jin",
      "Peiguang Li",
      "Yu Tian",
      "Linhao Zhang",
      "Sirui Wang",
      "Guangluan Xu",
      "Changyuan Tian",
      "Xunliang Cai"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.429": {
    "title": "More Than Catastrophic Forgetting: Integrating General Capabilities For Domain-Specific LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengyuan Liu",
      "Yangyang Kang",
      "Shihang Wang",
      "Lizhi Qing",
      "Fubang Zhao",
      "Chao Wu",
      "Changlong Sun",
      "Kun Kuang",
      "Fei Wu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.430": {
    "title": "Muting Whisper: A Universal Acoustic Adversarial Attack on Speech Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vyas Raina",
      "Rao Ma",
      "Charles McGhee",
      "Kate Knill",
      "Mark Gales"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.431": {
    "title": "GENRA: Enhancing Zero-shot Retrieval with Rank Aggregation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Georgios Katsimpras",
      "Georgios Paliouras"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.432": {
    "title": "XplainLLM: A Knowledge-Augmented Dataset for Reliable Grounded Explanations in LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zichen Chen",
      "Jianda Chen",
      "Ambuj Singh",
      "Misha Sra"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.433": {
    "title": "Divide and Conquer Radiology Report Generation via Observation Level Fine-grained Pretraining and Prompt Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanpin Zhou",
      "Huogen Wang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.434": {
    "title": "SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiashuo Sun",
      "Jihai Zhang",
      "Yucheng Zhou",
      "Zhaochen Su",
      "Xiaoye Qu",
      "Yu Cheng"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.435": {
    "title": "UNO Arena for Evaluating Sequential Decision-Making Capability of Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhanyue Qin",
      "Haochuan Wang",
      "Deyuan Liu",
      "Ziyang Song",
      "Cunhang Fan",
      "Zhao Lv",
      "Jinlin Wu",
      "Zhen Lei",
      "Zhiying Tu",
      "Dianhui Chu",
      "Xiaoyan Yu",
      "Dianbo Sui"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.436": {
    "title": "Middleware for LLMs: Tools Are Instrumental for Language Agents in Complex Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Gu",
      "Yiheng Shu",
      "Hao Yu",
      "Xiao Liu",
      "Yuxiao Dong",
      "Jie Tang",
      "Jayanth Srinivasa",
      "Hugo Latapie",
      "Yu Su"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.437": {
    "title": "MORPHEUS: Modeling Role from Personalized Dialogue History by Exploring and Utilizing Latent Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihong Tang",
      "Bo Wang",
      "Dongming Zhao",
      "Jinxiaojia Jinxiaojia",
      "Zhangjijun Zhangjijun",
      "Ruifang He",
      "Yuexian Hou"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.438": {
    "title": "KnowledgeSG: Privacy-Preserving Synthetic Text Generation with Knowledge Distillation from Server",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "WenHao Wang",
      "Xiaoyu Liang",
      "Rui Ye",
      "Jingyi Chai",
      "Siheng Chen",
      "Yanfeng Wang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.439": {
    "title": "DAMRO: Dive into the Attention Mechanism of LVLM to Reduce Object Hallucination",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Gong",
      "Tianshi Ming",
      "Xinpeng Wang",
      "Zhihua Wei"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.440": {
    "title": "Unlocking the Future: Exploring Look-Ahead Planning Mechanistic Interpretability in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyi Men",
      "Pengfei Cao",
      "Zhuoran Jin",
      "Yubo Chen",
      "Kang Liu",
      "Jun Zhao"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.441": {
    "title": "Breaking Language Barriers: Cross-Lingual Continual Pre-Training at Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenzhen Zheng",
      "Wenbo Pan",
      "Xu Xu",
      "Libo Qin",
      "Li Yue",
      "Ming Zhou"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.442": {
    "title": "An Empirical Study of Multilingual Reasoning Distillation for Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Patomporn Payoungkhamdee",
      "Peerat Limkonchotiwat",
      "Jinheon Baek",
      "Potsawee Manakul",
      "Can Udomcharoenchaikit",
      "Ekapol Chuangsuwanich",
      "Sarana Nutanong"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.443": {
    "title": "Can Large Language Models Faithfully Express Their Intrinsic Uncertainty in Words?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gal Yona",
      "Roee Aharoni",
      "Mor Geva"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.444": {
    "title": "Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zorik Gekhman",
      "Gal Yona",
      "Roee Aharoni",
      "Matan Eyal",
      "Amir Feder",
      "Roi Reichart",
      "Jonathan Herzig"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.445": {
    "title": "Bridging Modalities: Enhancing Cross-Modality Hate Speech Detection with Few-Shot In-Context Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming Shan Hee",
      "Aditi Kumaresan",
      "Roy Lee"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.446": {
    "title": "MIND: Multimodal Shopping Intention Distillation from Large Vision-language Models for E-commerce Purchase Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baixuan Xu",
      "Weiqi Wang",
      "Haochen Shi",
      "Wenxuan Ding",
      "Huihao Jing",
      "Tianqing Fang",
      "Jiaxin Bai",
      "Xin Liu",
      "Changlong Yu",
      "Zheng Li",
      "Chen Luo",
      "Qingyu Yin",
      "Bing Yin",
      "Long Chen",
      "Yangqiu Song"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.447": {
    "title": "ECON: On the Detection and Resolution of Evidence Conflicts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Jiayang",
      "Chunkit Chan",
      "Qianqian Zhuang",
      "Lin Qiu",
      "Tianhang Zhang",
      "Tengxiao Liu",
      "Yangqiu Song",
      "Yue Zhang",
      "Pengfei Liu",
      "Zheng Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.448": {
    "title": "Image, Tell me your story!\" Predicting the original meta-context of visual misinformation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathan Tonglet",
      "Marie-Francine Moens",
      "Iryna Gurevych"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.449": {
    "title": "Improving Retrieval-augmented Text-to-SQL with AST-based Ranking and Schema Pruning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhili Shen",
      "Pavlos Vougiouklis",
      "Chenxin Diao",
      "Kaustubh Vyas",
      "Yuanyi Ji",
      "Jeff Pan"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.450": {
    "title": "Mixture-of-Subspaces in Low-Rank Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taiqiang Wu",
      "Jiahao Wang",
      "Zhe Zhao",
      "Ngai Wong"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.451": {
    "title": "PARIKSHA: A Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ishaan Watts",
      "Varun Gumma",
      "Aditya Yadavalli",
      "Vivek Seshadri",
      "Manohar Swaminathan",
      "Sunayana Sitaram"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.452": {
    "title": "LawBench: Benchmarking Legal Knowledge of Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiwei Fei",
      "Xiaoyu Shen",
      "Dawei Zhu",
      "Fengzhe Zhou",
      "Zhuo Han",
      "Alan Huang",
      "Songyang Zhang",
      "Kai Chen",
      "Zhixin Yin",
      "Zongwen Shen",
      "Jidong Ge",
      "Vincent Ng"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.453": {
    "title": "Efficient Performance Tracking: Leveraging Large Language Models for Automated Construction of Scientific Leaderboards",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Furkan Şahinuç",
      "Thy Tran",
      "Yulia Grishina",
      "Yufang Hou",
      "Bei Chen",
      "Iryna Gurevych"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.454": {
    "title": "Efficient Vision-Language pre-training via domain-specific learning for human activities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adrian Bulat",
      "Yassine Ouali",
      "Ricardo Guerrero",
      "Brais Martinez",
      "Georgios Tzimiropoulos"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.455": {
    "title": "Empowering Backbone Models for Visual Text Generation with Input Granularity Control and Glyph-Aware Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenbo Li",
      "Guohao Li",
      "Zhibin Lan",
      "Xue Xu",
      "Wanru Zhuang",
      "Jiachen Liu",
      "Xinyan Xiao",
      "Jinsong Su"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.456": {
    "title": "Evaluating Character Understanding of Large Language Models via Character Profiling from Fictional Works",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinfeng Yuan",
      "Siyu Yuan",
      "Yuhan Cui",
      "Tianhe Lin",
      "Xintao Wang",
      "Rui Xu",
      "Jiangjie Chen",
      "Deqing Yang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.457": {
    "title": "Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shimao Zhang",
      "Changjiang Gao",
      "Wenhao Zhu",
      "Jiajun Chen",
      "Xin Huang",
      "Xue Han",
      "Junlan Feng",
      "Chao Deng",
      "Shujian Huang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.458": {
    "title": "AdaSwitch: Adaptive Switching between Small and Large Agents for Effective Cloud-Local Collaborative Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Sun",
      "Jiayi Wu",
      "Hengyi Cai",
      "Xiaochi Wei",
      "Yue Feng",
      "Bo Wang",
      "Shuaiqiang Wang",
      "Yan Zhang",
      "Dawei Yin"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.459": {
    "title": "CoBa: Convergence Balancer for Multitask Finetuning of Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zi Gong",
      "Hang Yu",
      "Cong Liao",
      "Bingchang Liu",
      "Chaoyu Chen",
      "Jianguo Li"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.460": {
    "title": "mDPO: Conditional Preference Optimization for Multimodal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fei Wang",
      "Wenxuan Zhou",
      "James Y. Huang",
      "Nan Xu",
      "Sheng Zhang",
      "Hoifung Poon",
      "Muhao Chen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.461": {
    "title": "Data Advisor: Dynamic Data Curation for Safety Alignment of Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fei Wang",
      "Ninareh Mehrabi",
      "Palash Goyal",
      "Rahul Gupta",
      "Kai-Wei Chang",
      "Aram Galstyan"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.462": {
    "title": "Language-to-Code Translation with a Single Labeled Example",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaj Bostrom",
      "Harsh Jhamtani",
      "Hao Fang",
      "Sam Thomson",
      "Richard Shin",
      "Patrick Xia",
      "Benjamin Van Durme",
      "Jason Eisner",
      "Jacob Andreas"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.463": {
    "title": "Attribute or Abstain: Large Language Models as Long Document Assistants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan Buchmann",
      "Xiao Liu",
      "Iryna Gurevych"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.464": {
    "title": "FEDKIM: Adaptive Federated Knowledge Injection into Medical Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaochen Wang",
      "Jiaqi Wang",
      "Houping Xiao",
      "Jinghui Chen",
      "Fenglong Ma"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.465": {
    "title": "Retrieved In-Context Principles from Previous Mistakes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Sun",
      "Yong Jiang",
      "Bo Wang",
      "Yingyan Hou",
      "Yan Zhang",
      "Pengjun Xie",
      "Fei Huang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.466": {
    "title": "EmoKnob: Enhance Voice Cloning with Fine-Grained Emotion Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haozhe Chen",
      "Run Chen",
      "Julia Hirschberg"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.467": {
    "title": "VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Liu",
      "Jicheng Wen",
      "Yang Wang",
      "Shengyu Ye",
      "Li Lyna Zhang",
      "Ting Cao",
      "Cheng Li",
      "Mao Yang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.468": {
    "title": "An L* Algorithm for Deterministic Weighted Regular Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Clemente Pasti",
      "Talu Karagöz",
      "Franz Nowak",
      "Anej Svete",
      "Reda Boumasmoud",
      "Ryan Cotterell"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.469": {
    "title": "Towards Verifiable Text Generation with Evolving Memory and Self-Reflection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Sun",
      "Hengyi Cai",
      "Bo Wang",
      "Yingyan Hou",
      "Xiaochi Wei",
      "Shuaiqiang Wang",
      "Yan Zhang",
      "Dawei Yin"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.470": {
    "title": "Pelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition and Program of Thought Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pritish Sahu",
      "Karan Sikka",
      "Ajay Divakaran"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.471": {
    "title": "Resampled Datasets Are Not Enough: Mitigating Societal Bias Beyond Single Attributes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yusuke Hirota",
      "Jerone Andrews",
      "Dora Zhao",
      "Orestis Papakyriakopoulos",
      "Apostolos Modas",
      "Yuta Nakashima",
      "Alice Xiang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.472": {
    "title": "RealVul: Can We Detect Vulnerabilities in Web Applications with LLM?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Di Cao",
      "Yong Liao",
      "Xiuwei Shang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.473": {
    "title": "Unsupervised End-to-End Task-Oriented Dialogue with LLMs: The Power of the Noisy Channel",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brendan King",
      "Jeffrey Flanigan"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.474": {
    "title": "Humans or LLMs as the Judge? A Study on Judgement Bias",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guiming Chen",
      "Shunian Chen",
      "Ziche Liu",
      "Feng Jiang",
      "Benyou Wang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.475": {
    "title": "WPO: Enhancing RLHF with Weighted Preference Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenxuan Zhou",
      "Ravi Agrawal",
      "Shujian Zhang",
      "Sathish Reddy Indurthi",
      "Sanqiang Zhao",
      "Kaiqiang Song",
      "Silei Xu",
      "Chenguang Zhu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.476": {
    "title": "Walking in Others' Shoes: How Perspective-Taking Guides Large Language Models in Reducing Toxicity and Bias",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rongwu Xu",
      "Zian Zhou",
      "Tianwei Zhang",
      "Zehan Qi",
      "Su Yao",
      "Ke Xu",
      "Wei Xu",
      "Han Qiu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.477": {
    "title": "MetaReflection: Learning Instructions for Language Agents using Past Reflections",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Priyanshu Gupta",
      "Shashank Kirtania",
      "Ananya Singha",
      "Sumit Gulwani",
      "Arjun Radhakrishna",
      "Gustavo Soares",
      "Sherry Shi"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.478": {
    "title": "Stepwise Verification and Remediation of Student Reasoning Errors with Large Language Model Tutors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nico Daheim",
      "Jakub Macina",
      "Manu Kapur",
      "Iryna Gurevych",
      "Mrinmaya Sachan"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.479": {
    "title": "On Eliciting Syntax from Language Models via Hashing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiran Wang",
      "Masao Utiyama"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.480": {
    "title": "CliMedBench: A Large-Scale Chinese Benchmark for Evaluating Medical Large Language Models in Clinical Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zetian Ouyang",
      "Yishuai Qiu",
      "Linlin Wang",
      "Gerard De Melo",
      "Ya Zhang",
      "Yanfeng Wang",
      "Liang He"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.481": {
    "title": "The Best Defense is Attack: Repairing Semantics in Textual Adversarial Examples",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heng Yang",
      "Ke Li"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.482": {
    "title": "CSSL: Contrastive Self-Supervised Learning for Dependency Parsing on Relatively Free Word Ordered and Morphologically Rich Low Resource Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pretam Ray",
      "Jivnesh Sandhan",
      "Amrith Krishna",
      "Pawan Goyal"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.483": {
    "title": "Perceptions of Linguistic Uncertainty by Language Models and Humans",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Catarina Belém",
      "Markelle Kelly",
      "Mark Steyvers",
      "Sameer Singh",
      "Padhraic Smyth"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.484": {
    "title": "Explaining and Improving Contrastive Decoding by Extrapolating the Probabilities of a Huge and Hypothetical LM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haw-Shiuan Chang",
      "Nanyun Peng",
      "Mohit Bansal",
      "Anil Ramakrishna",
      "Tagyoung Chung"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.485": {
    "title": "Zero-shot Cross-domain Dialogue State Tracking via Context-aware Auto-prompting and Instruction-following Contrastive Decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyu Dong",
      "Yujie Feng",
      "Zexin Lu",
      "Guangyuan Shi",
      "Xiao-Ming Wu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.486": {
    "title": "Knowledge Conflicts for LLMs: A Survey",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rongwu Xu",
      "Zehan Qi",
      "Zhijiang Guo",
      "Cunxiang Wang",
      "Hongru Wang",
      "Yue Zhang",
      "Wei Xu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.487": {
    "title": "MisinfoEval: Generative AI in the Era of \"Alternative Facts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saadia Gabriel",
      "Liang Lyu",
      "James Siderius",
      "Marzyeh Ghassemi",
      "Jacob Andreas",
      "Asuman Ozdaglar"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.488": {
    "title": "MEANT: Multimodal Encoder for Antecedent Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Irving",
      "Annika Schoene"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.489": {
    "title": "A Thorough Examination of Decoding Methods in the Era of LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chufan Shi",
      "Haoran Yang",
      "Deng Cai",
      "Zhisong Zhang",
      "Yifan Wang",
      "Yujiu Yang",
      "Wai Lam"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.490": {
    "title": "AGRaME: Any-Granularity Ranking with Multi-Vector Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Revanth Gangi Reddy",
      "Omar Attia",
      "Yunyao Li",
      "Heng Ji",
      "Saloni Potdar"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.491": {
    "title": "FIRST: Faster Improved Listwise Reranking with Single Token Decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Revanth Gangi Reddy",
      "JaeHyeok Doo",
      "Yifei Xu",
      "Md Arafat Sultan",
      "Deevya Swain",
      "Avirup Sil",
      "Heng Ji"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.492": {
    "title": "Exploring Nested Named Entity Recognition with Large Language Models: Methods, Challenges, and Insights",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongjin Kim",
      "Jai-Eun Kim",
      "Harksoo Kim"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.493": {
    "title": "ReCaLL: Membership Inference via Relative Conditional Log-Likelihoods",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roy Xie",
      "Junlin Wang",
      "Ruomin Huang",
      "Minxing Zhang",
      "Rong Ge",
      "Jian Pei",
      "Neil Gong",
      "Bhuwan Dhingra"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.494": {
    "title": "Flex Tape Can't Fix That\": Bias and Misinformation in Edited Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karina Halevy",
      "Anna Sotnikova",
      "Badr AlKhamissi",
      "Syrielle Montariol",
      "Antoine Bosselut"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.495": {
    "title": "Revisiting Who's Harry Potter: Towards Targeted Unlearning from a Causal Intervention Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujian Liu",
      "Yang Zhang",
      "Tommi Jaakkola",
      "Shiyu Chang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.496": {
    "title": "LIONs: An Empirically Optimized Approach to Align Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Yu",
      "Qingyang Wu",
      "Yu Li",
      "Zhou Yu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.497": {
    "title": "Jellyfish: Instruction-Tuning Local Large Language Models for Data Preprocessing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haochen Zhang",
      "Yuyang Dong",
      "Chuan Xiao",
      "Masafumi Oyamada"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.498": {
    "title": "A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Zhang",
      "Xiusi Chen",
      "Bowen Jin",
      "Sheng Wang",
      "Shuiwang Ji",
      "Wei Wang",
      "Jiawei Han"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.499": {
    "title": "MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liyan Tang",
      "Philippe Laban",
      "Greg Durrett"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.500": {
    "title": "Beyond Label Attention: Transparency in Language Models for Automated Medical Coding via Dictionary Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "John Wu",
      "David Wu",
      "Jimeng Sun"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.501": {
    "title": "MOSEL: Inference Serving Using Dynamic Modality Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bodun Hu",
      "Le Xu",
      "Jeongyoon Moon",
      "Neeraja Yadwadkar",
      "Aditya Akella"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.502": {
    "title": "From RAG to Riches: Retrieval Interlaced with Sequence Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Palak Jain",
      "Livio Baldini Soares",
      "Tom Kwiatkowski"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.503": {
    "title": "Task Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hsuan Su",
      "Hua Farn",
      "Fan-Yun Sun",
      "Shang-Tse Chen",
      "Hung-yi Lee"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.504": {
    "title": "Learning to Correct for QA Reasoning with Black-box LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaehyung Kim",
      "Dongyoung Kim",
      "Yiming Yang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.505": {
    "title": "AssistantBench: Can Web Agents Solve Realistic and Time-Consuming Tasks?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ori Yoran",
      "Samuel Amouyal",
      "Chaitanya Malaviya",
      "Ben Bogin",
      "Ofir Press",
      "Jonathan Berant"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.506": {
    "title": "PostMark: A Robust Blackbox Watermark for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yapei Chang",
      "Kalpesh Krishna",
      "Amir Houmansadr",
      "John Wieting",
      "Mohit Iyyer"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.507": {
    "title": "Assessing \"Implicit\" Retrieval Robustness of Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyu Shen",
      "Rexhina Blloshmi",
      "Dawei Zhu",
      "Jiahuan Pei",
      "Wei Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.508": {
    "title": "On the Relationship between Truth and Political Bias in Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suyash Fulay",
      "William Brannon",
      "Shrestha Mohanty",
      "Cassandra Overney",
      "Elinor Poole-Dayan",
      "Deb Roy",
      "Jad Kabbara"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.509": {
    "title": "Can Active Label Correction Improve LLM-based Modular AI Systems?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karan Taneja",
      "Ashok Goel"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.510": {
    "title": "Statistical Uncertainty in Word Embeddings: GloVe-V",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrea Vallebueno",
      "Cassandra Handan-Nader",
      "Christopher Manning",
      "Daniel Ho"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.511": {
    "title": "Annotation alignment: Comparing LLM and human annotations of conversational safety",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rajiv Movva",
      "Pang Wei Koh",
      "Emma Pierson"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.512": {
    "title": "DiVERT: Distractor Generation with Variational Errors Represented as Text for Math Multiple-choice Questions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nigel Fernandez",
      "Alexander Scarlatos",
      "Wanyong Feng",
      "Simon Woodhead",
      "Andrew Lan"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.513": {
    "title": "The Factuality Tax of Diversity-Intervened Text-to-Image Generation: Benchmark and Fact-Augmented Intervention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixin Wan",
      "Di Wu",
      "Haoran Wang",
      "Kai-Wei Chang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.514": {
    "title": "CleanGen: Mitigating Backdoor Attacks for Generation Tasks in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuetai Li",
      "Zhangchen Xu",
      "Fengqing Jiang",
      "Luyao Niu",
      "Dinuka Sahabandu",
      "Bhaskar Ramasubramanian",
      "Radha Poovendran"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.515": {
    "title": "Enhancing Reinforcement Learning with Dense Rewards from Language Model Critic",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meng Cao",
      "Lei Shu",
      "Lei Yu",
      "Yun Zhu",
      "Nevan Wichers",
      "Yinxiao Liu",
      "Lei Meng"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.516": {
    "title": "Words Matter: Reducing Stigma in Online Conversations about Substance Use with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Layla Bouzoubaa",
      "Elham Aghakhani",
      "Rezvaneh Rezapour"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.517": {
    "title": "Efficient Sequential Decision Making with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dingyang Chen",
      "Qi Zhang",
      "Yinglun Zhu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.518": {
    "title": "SignCLIP: Connecting Text and Sign Language by Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zifan Jiang",
      "Gerard Sant",
      "Amit Moryossef",
      "Mathias Müller",
      "Rico Sennrich",
      "Sarah Ebling"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.519": {
    "title": "APPLS: Evaluating Evaluation Metrics for Plain Language Summarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Guo",
      "Tal August",
      "Gondy Leroy",
      "Trevor Cohen",
      "Lucy Wang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.520": {
    "title": "Ontologically Faithful Generation of Non-Player Character Dialogues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nathaniel Weir",
      "Ryan Thomas",
      "Randolph d’Amore",
      "Kellie Hill",
      "Benjamin Van Durme",
      "Harsh Jhamtani"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.521": {
    "title": "LLM See, LLM Do: Leveraging Active Inheritance to Target Non-Differentiable Objectives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luísa Shimabucoro",
      "Sebastian Ruder",
      "Julia Kreutzer",
      "Marzieh Fadaee",
      "Sara Hooker"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.522": {
    "title": "RuBLiMP: Russian Benchmark of Linguistic Minimal Pairs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ekaterina Taktasheva",
      "Maxim Bazhukov",
      "Kirill Koncha",
      "Alena Fenogenova",
      "Ekaterina Artemova",
      "Vladislav Mikhailov"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.523": {
    "title": "Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheye Deng",
      "Chunkit Chan",
      "Weiqi Wang",
      "Yuxi Sun",
      "Wei Fan",
      "Tianshi Zheng",
      "Yauwai Yim",
      "Yangqiu Song"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.524": {
    "title": "Toward Compositional Behavior in Neural Models: A Survey of Current Views",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kate McCurdy",
      "Paul Soulos",
      "Paul Smolensky",
      "Roland Fernandez",
      "Jianfeng Gao"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.525": {
    "title": "Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Krista Opsahl-Ong",
      "Michael Ryan",
      "Josh Purtell",
      "David Broman",
      "Christopher Potts",
      "Matei Zaharia",
      "Omar Khattab"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.526": {
    "title": "Reverse-Engineering the Reader",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Kiegeland",
      "Ethan Wilcox",
      "Afra Amini",
      "David Reich",
      "Ryan Cotterell"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.527": {
    "title": "Synchronous Faithfulness Monitoring for Trustworthy Retrieval-Augmented Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Di Wu",
      "Jia-Chen Gu",
      "Fan Yin",
      "Nanyun Peng",
      "Kai-Wei Chang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.528": {
    "title": "Structure Guided Prompt: Instructing Large Language Model in Multi-Step Reasoning by Exploring Graph Structure of the Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kewei Cheng",
      "Nesreen Ahmed",
      "Theodore Willke",
      "Yizhou Sun"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.529": {
    "title": "Less is More: Parameter-Efficient Selection of Intermediate Tasks for Transfer Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Schulte",
      "Felix Hamborg",
      "Alan Akbik"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.530": {
    "title": "The effects of distance on NPI illusive effects in BERT",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "So Lee",
      "Mai Vu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.531": {
    "title": "Enhancing Systematic Decompositional Natural Language Inference Using Informal Logic",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nathaniel Weir",
      "Kate Sanders",
      "Orion Weller",
      "Shreya Sharma",
      "Dongwei Jiang",
      "Zhengping Jiang",
      "Bhavana Dalvi Mishra",
      "Oyvind Tafjord",
      "Peter Jansen",
      "Peter Clark",
      "Benjamin Van Durme"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.532": {
    "title": "Susu Box or Piggy Bank: Assessing Cultural Commonsense Knowledge between Ghana and the US",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christabel Acquaye",
      "Haozhe An",
      "Rachel Rudinger"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.533": {
    "title": "Read Anywhere Pointed: Layout-aware GUI Screen Reading with Tree-of-Lens Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Fan",
      "Lei Ding",
      "Ching-Chen Kuo",
      "Shan Jiang",
      "Yang Zhao",
      "Xinze Guan",
      "Jie Yang",
      "Yi Zhang",
      "Xin Wang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.534": {
    "title": "Ranking Manipulation for Conversational Search Engines",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Pfrommer",
      "Yatong Bai",
      "Tanmay Gautam",
      "Somayeh Sojoudi"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.535": {
    "title": "Fast Forwarding Low-Rank Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adir Rahamim",
      "Naomi Saphra",
      "Sara Kangaslahti",
      "Yonatan Belinkov"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.536": {
    "title": "Precise Model Benchmarking with Only a Few Observations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Riccardo Fogliato",
      "Pratik Patil",
      "Nil-Jana Akpinar",
      "Mathew Monfort"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.537": {
    "title": "Attribute Diversity Determines the Systematicity Gap in VQA",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ian Berlot-Attwell",
      "Kumar Agrawal",
      "Annabelle Carrell",
      "Yash Sharma",
      "Naomi Saphra"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.538": {
    "title": "ArxivDIGESTables: Synthesizing Scientific Literature into Tables using Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Newman",
      "Yoonjoo Lee",
      "Aakanksha Naik",
      "Pao Siangliulue",
      "Raymond Fok",
      "Juho Kim",
      "Daniel Weld",
      "Joseph Chee Chang",
      "Kyle Lo"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.539": {
    "title": "Development of Cognitive Intelligence in Pre-trained Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raj Shah",
      "Khushi Bhardwaj",
      "Sashank Varma"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.540": {
    "title": "Modeling Layout Reading Order as Ordering Relations for Visually-rich Document Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chong Zhang",
      "Yi Tu",
      "Yixi Zhao",
      "Chenshu Yuan",
      "Huan Chen",
      "Yue Zhang",
      "Mingxu Chai",
      "Ya Guo",
      "Huijia Zhu",
      "Qi Zhang",
      "Tao Gui"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.541": {
    "title": "Birdie: Advancing State Space Language Modeling with Dynamic Mixtures of Training Objectives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sam Blouir",
      "Jimmy Smith",
      "Antonios Anastasopoulos",
      "Amarda Shehu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.542": {
    "title": "Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pinzhen Chen",
      "Simon Yu",
      "Zhicheng Guo",
      "Barry Haddow"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.543": {
    "title": "Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheridan Feucht",
      "David Atkinson",
      "Byron Wallace",
      "David Bau"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.544": {
    "title": "TraveLER: A Modular Multi-LMM Agent Framework for Video Question-Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuyi Shang",
      "Amos You",
      "Sanjay Subramanian",
      "Trevor Darrell",
      "Roei Herzig"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.545": {
    "title": "Evaluating the Effectiveness of Large Language Models in Establishing Conversational Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Biswesh Mohapatra",
      "Manav Kapadnis",
      "Laurent Romary",
      "Justine Cassell"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.546": {
    "title": "Unlocking Memorization in Large Language Models with Dynamic Soft Prompting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhepeng Wang",
      "Runxue Bao",
      "Yawen Wu",
      "Jackson Taylor",
      "Cao Xiao",
      "Feng Zheng",
      "Weiwen Jiang",
      "Shangqian Gao",
      "Yanfu Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.547": {
    "title": "If CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reza Esfandiarpoor",
      "Cristina Menghini",
      "Stephen Bach"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.548": {
    "title": "Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowen Zhang",
      "Harold Soh"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.549": {
    "title": "MQuinE: a Cure for \"Z-paradox\" in Knowledge Graph Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Liu",
      "Huang Fang",
      "Yunfeng Cai",
      "Mingming Sun"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.550": {
    "title": "Can Transformers Learn n-gram Language Models?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anej Svete",
      "Nadav Borenstein",
      "Mike Zhou",
      "Isabelle Augenstein",
      "Ryan Cotterell"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.551": {
    "title": "StablePrompt : Automatic Prompt Tuning using Reinforcement Learning for Large Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minchan Kwon",
      "Gaeun Kim",
      "Jongsuk Kim",
      "Haeil Lee",
      "Junmo Kim"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.552": {
    "title": "Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Philippe Laban",
      "Alexander Fabbri",
      "Caiming Xiong",
      "Chien-Sheng Wu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.553": {
    "title": "Multi-pass Decoding for Grammatical Error Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoying Wang",
      "Lingling Mu",
      "Jingyi Zhang",
      "Hongfei Xu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.554": {
    "title": "Into the Unknown Unknowns: Engaged Human Learning through Participation in Language Model Agent Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yucheng Jiang",
      "Yijia Shao",
      "Dekun Ma",
      "Sina Semnani",
      "Monica Lam"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.555": {
    "title": "SCOI: Syntax-augmented Coverage-based In-context Example Selection for Machine Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenming Tang",
      "Zhixiang Wang",
      "Yunfang Wu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.556": {
    "title": "Efficient Temporal Extrapolation of Multimodal Large Language Models with Temporal Grounding Bridge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Wang",
      "Yueqian Wang",
      "Pengfei Wu",
      "Jianxin Liang",
      "Dongyan Zhao",
      "Yang Liu",
      "Zilong Zheng"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.557": {
    "title": "STORYSUMM: Evaluating Faithfulness in Story Summarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Melanie Subbiah",
      "Faisal Ladhak",
      "Akankshya Mishra",
      "Griffin Adams",
      "Lydia Chilton",
      "Kathleen McKeown"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.558": {
    "title": "MMoE: Enhancing Multimodal Models with Mixtures of Multimodal Interaction Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haofei Yu",
      "Zhengyang Qi",
      "Lawrence Jang",
      "Russ Salakhutdinov",
      "Louis-Philippe Morency",
      "Paul Pu Liang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.559": {
    "title": "OmAgent: A Multi-modal Agent Framework for Complex Video Understanding with Task Divide-and-Conquer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lu Zhang",
      "Tiancheng Zhao",
      "Heting Ying",
      "Yibo Ma",
      "Kyusong Lee"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.560": {
    "title": "Enhancing Pre-Trained Generative Language Models with Question Attended Span Extraction on Machine Reading Comprehension",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lin Ai",
      "Zheng Hui",
      "Zizhou Liu",
      "Julia Hirschberg"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.561": {
    "title": "CommonIT: Commonality-Aware Instruction Tuning for Large Language Models via Data Partitions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Rao",
      "Xuebo Liu",
      "Lian Lian",
      "Shengjun Cheng",
      "Yunjie Liao",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.562": {
    "title": "ESC: Efficient Speech Coding with Cross-Scale Residual Vector Quantized Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzhe Gu",
      "Enmao Diao"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.563": {
    "title": "Breaking ReLU Barrier: Generalized MoEfication for Dense Pretrained Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaeseong Lee",
      "Seung-won Hwang",
      "Wonpyo Park",
      "Mingi Ji"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.564": {
    "title": "Detecting Subtle Differences between Human and Model Languages Using Spectrum of Relative Likelihood",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Xu",
      "Yu Wang",
      "Hao An",
      "Zhichen Liu",
      "Yongyuan Li"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.565": {
    "title": "Optimizing Language Models with Fair and Stable Reward Composition in Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahui Li",
      "Hanlin Zhang",
      "Fengda Zhang",
      "Tai-Wei Chang",
      "Kun Kuang",
      "Long Chen",
      "Jun Zhou"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.566": {
    "title": "Fine-grained Pluggable Gradient Ascent for Knowledge Unlearning in Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "XiaoHua Feng",
      "Chaochao Chen",
      "Yuyuan Li",
      "Zibin Lin"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.567": {
    "title": "ARM: An Alignment-and-Replacement Module for Chinese Spelling Check Based on LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changchun Liu",
      "Kai Zhang",
      "Junzhe Jiang",
      "Zirui Liu",
      "Hanqing Tao",
      "Min Gao",
      "Enhong Chen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.568": {
    "title": "On the In-context Generation of Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongtao Jiang",
      "Yuanzhe Zhang",
      "Kun Luo",
      "Xiaowei Yuan",
      "Jun Zhao",
      "Kang Liu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.569": {
    "title": "Atomic Inference for NLI with Generated Facts as Atoms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joe Stacey",
      "Pasquale Minervini",
      "Haim Dubossarsky",
      "Oana-Maria Camburu",
      "Marek Rei"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.570": {
    "title": "Towards Robust Speech Representation Learning for Thousands of Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "William Chen",
      "Wangyou Zhang",
      "Yifan Peng",
      "Xinjian Li",
      "Jinchuan Tian",
      "Jiatong Shi",
      "Xuankai Chang",
      "Soumi Maiti",
      "Karen Livescu",
      "Shinji Watanabe"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.571": {
    "title": "I Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Ren",
      "Biao Wu",
      "Lingqiao Liu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.572": {
    "title": "PreAlign: Boosting Cross-Lingual Transfer by Early Establishment of Multilingual Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahuan Li",
      "Shujian Huang",
      "Aarron Ching",
      "Xinyu Dai",
      "Jiajun Chen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.573": {
    "title": "An image speaks a thousand words, but can everyone listen? On image transcreation for cultural relevance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simran Khanuja",
      "Sathyanarayanan Ramamoorthy",
      "Yueqi Song",
      "Graham Neubig"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.574": {
    "title": "When Parts Are Greater Than Sums: Individual LLM Components Can Outperform Full Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ting-Yun Chang",
      "Jesse Thomason",
      "Robin Jia"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.575": {
    "title": "Multimodal Clickbait Detection by De-confounding Biases Using Causal Representation Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianxing Yu",
      "Shiqi Wang",
      "Han Yin",
      "Zhenlong Sun",
      "Ruobing Xie",
      "Bo Zhang",
      "Yanghui Rao"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.576": {
    "title": "Matryoshka-Adaptor: Unsupervised and Supervised Tuning for Smaller Embedding Dimensions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinsung Yoon",
      "Rajarishi Sinha",
      "Sercan Arik",
      "Tomas Pfister"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.577": {
    "title": "KNN-Instruct: Automatic Instruction Construction with K Nearest Neighbor Deduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianshang Kou",
      "Benfeng Xu",
      "Chiwei Zhu",
      "Zhendong Mao"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.578": {
    "title": "Contextualized Sequence Likelihood: Enhanced Confidence Scores for Natural Language Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Lin",
      "Shubhendu Trivedi",
      "Jimeng Sun"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.579": {
    "title": "MixGR: Enhancing Retriever Generalization for Scientific Domain through Complementary Granularity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fengyu Cai",
      "Xinran Zhao",
      "Tong Chen",
      "Sihao Chen",
      "Hongming Zhang",
      "Iryna Gurevych",
      "Heinz Koeppl"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.580": {
    "title": "CARER - ClinicAl Reasoning-Enhanced Representation for Temporal Health Risk Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tuan Nguyen",
      "Thanh Huynh",
      "Minh Hieu Phan",
      "Quoc Viet Hung Nguyen",
      "Phi Le Nguyen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.581": {
    "title": "In-Dialogues We Learn\": Towards Personalized Dialogue Without Pre-defined Profiles through In-Dialogue Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuanqi Cheng",
      "Quan Tu",
      "Wei Wu",
      "Shuo Shang",
      "Cunli Mao",
      "Zhengtao Yu",
      "Rui Yan"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.582": {
    "title": "Encourage or Inhibit Monosemanticity? Revisit Monosemanticity from a Feature Decorrelation Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanqi Yan",
      "Yanzheng Xiang",
      "Guangyi Chen",
      "Yifei Wang",
      "Lin Gui",
      "Yulan He"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.583": {
    "title": "Enhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Liu",
      "Farima Fatahi Bayat",
      "Lu Wang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.584": {
    "title": "Reasoning Robustness of LLMs to Adversarial Typographical Errors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Esther Gan",
      "Yiran Zhao",
      "Liying Cheng",
      "Mao Yancan",
      "Anirudh Goyal",
      "Kenji Kawaguchi",
      "Min-Yen Kan",
      "Michael Shieh"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.585": {
    "title": "InferAligner: Inference-Time Alignment for Harmlessness through Cross-Model Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengyu Wang",
      "Dong Zhang",
      "Linyang Li",
      "Chenkun Tan",
      "Xinghao Wang",
      "Mozhi Zhang",
      "Ke Ren",
      "Botian Jiang",
      "Xipeng Qiu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.586": {
    "title": "Belief Revision: The Adaptability of Large Language Models Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bryan Wilie",
      "Samuel Cahyawijaya",
      "Etsuko Ishii",
      "Junxian He",
      "Pascale Fung"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.587": {
    "title": "Fisher Information-based Efficient Curriculum Federated Learning with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ji Liu",
      "Jiaxiang Ren",
      "Ruoming Jin",
      "Zijie Zhang",
      "Yang Zhou",
      "Patrick Valduriez",
      "Dejing Dou"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.588": {
    "title": "Bio-RFX: Refining Biomedical Extraction via Advanced Relation Classification and Structural Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minjia Wang",
      "Fangzhou Liu",
      "Xiuxing Li",
      "Bowen Dong",
      "Zhenyu Li",
      "Tengyu Pan",
      "Jianyong Wang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.589": {
    "title": "Decoding Matters: Addressing Amplification Bias and Homogeneity Issue in Recommendations for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keqin Bao",
      "Jizhi Zhang",
      "Yang Zhang",
      "Xinyue Huo",
      "Chong Chen",
      "Fuli Feng"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.590": {
    "title": "LLMs Are Prone to Fallacies in Causal Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nitish Joshi",
      "Abulhair Saparov",
      "Yixin Wang",
      "He He"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.591": {
    "title": "Roleplay-doh: Enabling Domain-Experts to Create LLM-simulated Patients via Eliciting and Adhering to Principles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryan Louie",
      "Ananjan Nandi",
      "William Fang",
      "Cheng Chang",
      "Emma Brunskill",
      "Diyi Yang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.592": {
    "title": "The Lou Dataset - Exploring the Impact of Gender-Fair Language in German Text Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andreas Waldis",
      "Joel Birrer",
      "Anne Lauscher",
      "Iryna Gurevych"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.593": {
    "title": "When Generative Adversarial Networks Meet Sequence Labeling Challenges",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Tong",
      "Ge Chen",
      "Guokai Zheng",
      "Rui Li",
      "Jiang Dazhi"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.594": {
    "title": "Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sungho Ko",
      "Hyunjin Cho",
      "Hyungjoo Chae",
      "Jinyoung Yeo",
      "Dongha Lee"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.595": {
    "title": "Speechworthy Instruction-tuned Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyundong Cho",
      "Nicolaas Jedema",
      "Leonardo Ribeiro",
      "Karishma Sharma",
      "Pedro Szekely",
      "Alessandro Moschitti",
      "Ruben Janssen",
      "Jonathan May"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.596": {
    "title": "Data, Data Everywhere: A Guide for Pretraining Dataset Construction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jupinder Parmar",
      "Shrimai Prabhumoye",
      "Joseph Jennings",
      "Bo Liu",
      "Aastha Jhunjhunwala",
      "Zhilin Wang",
      "Mostofa Patwary",
      "Mohammad Shoeybi",
      "Bryan Catanzaro"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.597": {
    "title": "Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dilara Soylu",
      "Christopher Potts",
      "Omar Khattab"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.598": {
    "title": "Demystifying Verbatim Memorization in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Huang",
      "Diyi Yang",
      "Christopher Potts"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.599": {
    "title": "AmbigNLG: Addressing Task Ambiguity in Instruction for NLG",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ayana Niwa",
      "Hayate Iso"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.600": {
    "title": "Distributional Properties of Subword Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marco Cognetta",
      "Vilém Zouhar",
      "Naoaki Okazaki"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.601": {
    "title": "DataTales: A Benchmark for Real-World Intelligent Data Narration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yajing Yang",
      "Qian Liu",
      "Min-Yen Kan"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.602": {
    "title": "Towards Fast Multilingual LLM Inference: Speculative Decoding and Specialized Drafters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Euiin Yi",
      "Taehyeon Kim",
      "Hongseok Jeung",
      "Du-Seong Chang",
      "Se-Young Yun"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.603": {
    "title": "GlobeSumm: A Challenging Benchmark Towards Unifying Multi-lingual, Cross-lingual and Multi-document News Summarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangfan Ye",
      "Xiachong Feng",
      "Xiaocheng Feng",
      "Weitao Ma",
      "Libo Qin",
      "Dongliang Xu",
      "Qing Yang",
      "Hongtao Liu",
      "Bing Qin"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.604": {
    "title": "Breaking the Curse of Multilinguality with Cross-lingual Expert Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Terra Blevins",
      "Tomasz Limisiewicz",
      "Suchin Gururangan",
      "Margaret Li",
      "Hila Gonen",
      "Noah Smith",
      "Luke Zettlemoyer"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.605": {
    "title": "More Insightful Feedback for Tutoring: Enhancing Generation Mechanisms and Automatic Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wencke Liermann",
      "Jin-Xia Huang",
      "Yohan Lee",
      "Kong Joo Lee"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.606": {
    "title": "Stable Language Model Pre-training by Reducing Embedding Variability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Woojin Chung",
      "Jiwoo Hong",
      "Na Min An",
      "James Thorne",
      "Se-Young Yun"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.607": {
    "title": "What is lost in Normalization? Exploring Pitfalls in Multilingual ASR Model Evaluations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kavya Manohar",
      "Leena Pillai"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.608": {
    "title": "Diversity Over Size: On the Effect of Sample and Topic Sizes for Topic-Dependent Argument Mining Datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Schiller",
      "Johannes Daxenberger",
      "Andreas Waldis",
      "Iryna Gurevych"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.609": {
    "title": "Kiss up, Kick down: Exploring Behavioral Changes in Multi-modal Large Language Models with Assigned Visual Personas",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seungjong Sun",
      "Eungu Lee",
      "Seo Baek",
      "Seunghyun Hwang",
      "Wonbyung Lee",
      "Dongyan Nan",
      "Bernard Jansen",
      "Jang Kim"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.610": {
    "title": "ATM: Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented Generator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junda Zhu",
      "Lingyong Yan",
      "Haibo Shi",
      "Dawei Yin",
      "Lei Sha"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.611": {
    "title": "Dynamic Multi-granularity Attribution Network for Aspect-based Sentiment Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanjiang Chen",
      "Kai Zhang",
      "Feng Hu",
      "Xianquan Wang",
      "Ruikang Li",
      "Qi Liu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.612": {
    "title": "Unlabeled Debiasing in Downstream Tasks via Class-wise Low Variance Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shahed Masoudian",
      "Markus Frohmann",
      "Navid Rekabsaz",
      "Markus Schedl"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.613": {
    "title": "Large Language Models Know What is Key Visual Entity: An LLM-assisted Multimodal Retrieval for VQA",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pu Jian",
      "Donglei Yu",
      "Jiajun Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.614": {
    "title": "Towards Probing Speech-Specific Risks in Large Multimodal Models: A Taxonomy, Benchmark, and Insights",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Yang",
      "Lizhen Qu",
      "Ehsan Shareghi",
      "Reza Haf"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.615": {
    "title": "Self-AMPLIFY: Improving Small Language Models with Self Post Hoc Explanations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Milan Bhan",
      "Jean-Noël Vittaut",
      "Nicolas Chesneau",
      "Marie-Jeanne Lesot"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.616": {
    "title": "What are the Generator Preferences for End-to-end Task-Oriented Dialog System?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wanshi Xu",
      "Xianwei Zhuang",
      "Zhanpeng Chen",
      "Zhihong Zhu",
      "Xuxin Cheng",
      "Yuexian Zou"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.617": {
    "title": "Paraphrase Types Elicit Prompt Engineering Capabilities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan Philip Wahle",
      "Terry Ruas",
      "Yang Xu",
      "Bela Gipp"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.618": {
    "title": "VLEU: a Method for Automatic Evaluation for Generalizability of Text-to-Image Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingtao Cao",
      "Zhang Zheng",
      "Hongru Wang",
      "Kam-Fai Wong"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.619": {
    "title": "Towards Online Continuous Sign Language Recognition and Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ronglai Zuo",
      "Fangyun Wei",
      "Brian Mak"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.620": {
    "title": "Mitigate Extrinsic Social Bias in Pre-trained Language Models via Continuous Prompts Adjustment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiwei Dai",
      "Hengrui Gu",
      "Ying Wang",
      "Xin Wang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.621": {
    "title": "Split and Merge: Aligning Position Biases in LLM-based Evaluators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zongjie Li",
      "Chaozheng Wang",
      "Pingchuan Ma",
      "Daoyuan Wu",
      "Shuai Wang",
      "Cuiyun Gao",
      "Yang Liu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.622": {
    "title": "Integrating Argumentation and Hate-Speech-based Techniques for Countering Misinformation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sougata Saha",
      "Rohini Srihari"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.623": {
    "title": "BPO: Staying Close to the Behavior LLM Creates Better Online LLM Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenda Xu",
      "Jiachen Li",
      "William Yang Wang",
      "Lei Li"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.624": {
    "title": "One2Set + Large Language Model: Best Partners for Keyphrase Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liangying Shao",
      "Liang Zhang",
      "Minlong Peng",
      "Guoqi Ma",
      "Hao Yue",
      "Mingming Sun",
      "Jinsong Su"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.625": {
    "title": "Unlocking Markets: A Multilingual Benchmark to Cross-Market Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Yuan",
      "Yang Deng",
      "Anders Søgaard",
      "Mohammad Aliannejadi"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.626": {
    "title": "ORPO: Monolithic Preference Optimization without Reference Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiwoo Hong",
      "Noah Lee",
      "James Thorne"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.627": {
    "title": "A Multi-Perspective Analysis of Memorization in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowen Chen",
      "Namgi Han",
      "Yusuke Miyao"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.628": {
    "title": "Do LLMs suffer from Multi-Party Hangover? A Diagnostic Approach to Addressee Recognition and Response Selection in Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicolò Penzo",
      "Maryam Sajedinia",
      "Bruno Lepri",
      "Sara Tonelli",
      "Marco Guerini"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.629": {
    "title": "Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haritz Puerto",
      "Martin Tutek",
      "Somak Aditya",
      "Xiaodan Zhu",
      "Iryna Gurevych"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.630": {
    "title": "Unveiling the Role of Pretraining in Direct Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Belen Alastruey",
      "Gerard Gállego",
      "Marta Costa-jussà"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.631": {
    "title": "PCQPR: Proactive Conversational Question Planning with Reflection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shasha Guo",
      "Lizi Liao",
      "Jing Zhang",
      "Cuiping Li",
      "Hong Chen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.632": {
    "title": "CodeAgent: Autonomous Communicative Agents for Code Review",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xunzhu Tang",
      "Kisub Kim",
      "Yewei Song",
      "Cedric Lothritz",
      "Bei Li",
      "Saad Ezzini",
      "Haoye Tian",
      "Jacques Klein",
      "Tegawendé Bissyandé"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.633": {
    "title": "TroL: Traversal of Layers for Large Language and Vision Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Byung-Kwan Lee",
      "Sangyun Chung",
      "Chae Won Kim",
      "Beomchan Park",
      "Yong Man Ro"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.634": {
    "title": "MMTE: Corpus and Metrics for Evaluating Machine Translation Quality of Metaphorical Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shun Wang",
      "Ge Zhang",
      "Han Wu",
      "Tyler Loakman",
      "Wenhao Huang",
      "Chenghua Lin"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.635": {
    "title": "Revisiting Supertagging for faster HPSG parsing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Olga Zamaraeva",
      "Carlos Gómez-Rodríguez"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.636": {
    "title": "Improve Dense Passage Retrieval with Entailment Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lu Dai",
      "Hao Liu",
      "Hui Xiong"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.637": {
    "title": "ToolBeHonest: A Multi-level Hallucination Diagnostic Benchmark for Tool-Augmented Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxiang Zhang",
      "Jing Chen",
      "Junjie Wang",
      "Yaxin Liu",
      "Cheng Yang",
      "Chufan Shi",
      "Xinyu Zhu",
      "Zihao Lin",
      "Hanwen Wan",
      "Yujiu Yang",
      "Tetsuya Sakai",
      "Tian Feng",
      "Hayato Yamana"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.638": {
    "title": "TEMA: Token Embeddings Mapping for Enriching Low-Resource Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rodolfo Zevallos",
      "Núria Bel",
      "Mireia Farrús"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.639": {
    "title": "DECOR: Improving Coherence in L2 English Writing with a Novel Benchmark for Incoherence Detection, Reasoning, and Rewriting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuanming Zhang",
      "Anthony Diaz",
      "Zixun Chen",
      "Qingyang Wu",
      "Kun Qian",
      "Erik Voss",
      "Zhou Yu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.640": {
    "title": "Text2Chart31: Instruction Tuning for Chart Generation with Automatic Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fatemeh Pesaran Zadeh",
      "Juyeon Kim",
      "Jin-Hwa Kim",
      "Gunhee Kim"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.641": {
    "title": "PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christoph Leiter",
      "Steffen Eger"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.642": {
    "title": "Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuai Zhao",
      "Meihuizi Jia",
      "Anh Tuan Luu",
      "Fengjun Pan",
      "Jinming Wen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.643": {
    "title": "Repairs in a Block World: A New Benchmark for Handling User Corrections with Multi-Modal Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Javier Chiyah-Garcia",
      "Alessandro Suglia",
      "Arash Eshghi"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.644": {
    "title": "Beyond the Turn-Based Game: Enabling Real-Time Conversations with Duplex Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinrong Zhang",
      "Yingfa Chen",
      "Shengding Hu",
      "Xu Han",
      "Zihang Xu",
      "Yuanwei Xu",
      "Weilin Zhao",
      "Maosong Sun",
      "Zhiyuan Liu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.645": {
    "title": "Strengthening Structural Inductive Biases by Pre-training to Perform Syntactic Transformations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthias Lindemann",
      "Alexander Koller",
      "Ivan Titov"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.646": {
    "title": "Puzzle Solving using Reasoning of Large Language Models: A Survey",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Panagiotis Giadikiaroglou",
      "Maria Lymperaiou",
      "Giorgos Filandrianos",
      "Giorgos Stamou"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.647": {
    "title": "SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tu Dinh",
      "Carlos Mullov",
      "Leonard Bärmann",
      "Zhaolin Li",
      "Danni Liu",
      "Simon Reiß",
      "Jueun Lee",
      "Nathan Lerzer",
      "Jianfeng Gao",
      "Fabian Peller-Konrad",
      "Tobias Röddiger",
      "Alexander Waibel",
      "Tamim Asfour",
      "Michael Beigl",
      "Rainer Stiefelhagen",
      "Carsten Dachsbacher",
      "Klemens Böhm",
      "Jan Niehues"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.648": {
    "title": "Red Teaming Language Models for Processing Contradictory Dialogues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaofei Wen",
      "Bangzheng Li",
      "Tenghao Huang",
      "Muhao Chen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.649": {
    "title": "Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sander Land",
      "Max Bartolo"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.650": {
    "title": "Reasoning or a Semblance of it? A Diagnostic Study of Transitive Reasoning in LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Houman Mehrafarin",
      "Arash Eshghi",
      "Ioannis Konstas"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.651": {
    "title": "Pragmatic Norms Are All You Need – Why The Symbol Grounding Problem Does Not Apply to LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reto Gubelmann"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.652": {
    "title": "Major Entity Identification: A Generalizable Alternative to Coreference Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kawshik Sundar",
      "Shubham Toshniwal",
      "Makarand Tapaswi",
      "Vineet Gandhi"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.653": {
    "title": "Enhancing High-order Interaction Awareness in LLM-based Recommender Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinfeng Wang",
      "Jin Cui",
      "Fumiyo Fukumoto",
      "Yoshimi Suzuki"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.654": {
    "title": "What Are the Odds? Language Models Are Capable of Probabilistic Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akshay Paruchuri",
      "Jake Garrison",
      "Shun Liao",
      "John Hernandez",
      "Jacob Sunshine",
      "Tim Althoff",
      "Xin Liu",
      "Daniel McDuff"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.655": {
    "title": "MARE: Multi-Aspect Rationale Extractor on Unsupervised Rationale Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Jiang",
      "Junwen Duan",
      "Zhe Qu",
      "Jianxin Wang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.656": {
    "title": "LoRA-Guard: Parameter-Efficient Guardrail Adaptation for Content Moderation of Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hayder Elesedy",
      "Pedro Esperanca",
      "Silviu Vlad Oprea",
      "Mete Ozay"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.657": {
    "title": "A good pun is its own reword\": Can Large Language Models Understand Puns?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhijun Xu",
      "Siyu Yuan",
      "Lingjie Chen",
      "Deqing Yang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.658": {
    "title": "QGEval: Benchmarking Multi-dimensional Evaluation for Question Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiping Fu",
      "Bifan Wei",
      "Jianxiang Hu",
      "Zhongmin Cai",
      "Jun Liu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.659": {
    "title": "Dependency Graph Parsing as Sequence Labeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ana Ezquerro",
      "David Vilares",
      "Carlos Gómez-Rodríguez"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.660": {
    "title": "NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sergei Bogdanov",
      "Alexandre Constantin",
      "Timothée Bernard",
      "Benoit Crabbé",
      "Etienne Bernard"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.661": {
    "title": "Towards a Greek Proverb Atlas: Computational Spatial Exploration and Attribution of Greek Proverbs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "John Pavlopoulos",
      "Panos Louridas",
      "Panagiotis Filos"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.662": {
    "title": "Unraveling Babel: Exploring Multilingual Activation Patterns of LLMs and Their Applications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weize Liu",
      "Yinlong Xu",
      "Hongxia Xu",
      "Jintai Chen",
      "Xuming Hu",
      "Jian Wu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.663": {
    "title": "Advancing Semantic Textual Similarity Modeling: A Regression Framework with Translated ReLU and Smooth K2 Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowen Zhang",
      "Chunping Li"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.664": {
    "title": "Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marc Brinner",
      "Sina Zarrieß"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.665": {
    "title": "Segment Any Text: A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Markus Frohmann",
      "Igor Sterner",
      "Ivan Vulić",
      "Benjamin Minixhofer",
      "Markus Schedl"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.666": {
    "title": "Applying Contrastive Learning to Code Vulnerability Type Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Ji",
      "Su Yang",
      "Hongyu Sun",
      "Yuqing Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.667": {
    "title": "TheoremLlama: Transforming General-Purpose LLMs into Lean4 Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruida Wang",
      "Jipeng Zhang",
      "Yizhen Jia",
      "Rui Pan",
      "Shizhe Diao",
      "Renjie Pi",
      "Tong Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.668": {
    "title": "Multi-Level Cross-Modal Alignment for Speech Relation Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liang Zhang",
      "Zhen Yang",
      "Biao Fu",
      "Ziyao Lu",
      "Liangying Shao",
      "Shiyu Liu",
      "Fandong Meng",
      "Jie Zhou",
      "Xiaoli Wang",
      "Jinsong Su"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.669": {
    "title": "Self-Training for Sample-Efficient Active Learning for Text Classification with Pre-Trained Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christopher Schröder",
      "Gerhard Heyer"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.670": {
    "title": "PANDA: Persona Attributes Navigation for Detecting and Alleviating Overuse Problem in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinsung Kim",
      "Seonmin Koo",
      "Heuiseok Lim"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.671": {
    "title": "The Multilingual Alignment Prism: Aligning Global and Local Preferences to Reduce Harm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aakanksha",
      "Arash Ahmadian",
      "Beyza Ermis",
      "Seraphina Goldfarb-Tarrant",
      "Julia Kreutzer",
      "Marzieh Fadaee",
      "Sara Hooker"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.672": {
    "title": "Subword Segmentation in LLMs: Looking at Inflection and Consistency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marion Marco",
      "Alexander Fraser"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.673": {
    "title": "Explicit, Implicit, and Scattered: Revisiting Event Extraction to Capture Complex Arguments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Omar Sharif",
      "Joseph Gatto",
      "Madhusudan Basak",
      "Sarah Masud Preum"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.674": {
    "title": "Let Me Teach You: Pedagogical Foundations of Feedback for Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Beatriz Borges",
      "Niket Tandon",
      "Tanja Käser",
      "Antoine Bosselut"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.675": {
    "title": "Unknown Claims: Generation of Fact-Checking Training Examples from Unstructured and Structured Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jean-Flavien Bussotti",
      "Luca Ragazzi",
      "Giacomo Frisoni",
      "Gianluca Moro",
      "Paolo Papotti"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.676": {
    "title": "TL-CL: Task And Language Incremental Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shrey Satapara",
      "P. K. Srijith"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.677": {
    "title": "Medical Adaptation of Large Language and Vision-Language Models: Are We Making Progress?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Jeong",
      "Saurabh Garg",
      "Zachary Lipton",
      "Michael Oberst"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.678": {
    "title": "Empowering Multi-step Reasoning across Languages via Program-Aided Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leonardo Ranaldi",
      "Giulia Pucci",
      "Barry Haddow",
      "Alexandra Birch"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.679": {
    "title": "Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Yuan",
      "Lili Zhao",
      "Kai Zhang",
      "Guangting Zheng",
      "Qi Liu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.680": {
    "title": "ControlMath: Controllable Data Generation Promotes Math Generalist Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nuo Chen",
      "Ning Wu",
      "Jianhui Chang",
      "Linjun Shou",
      "Jia Li"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.681": {
    "title": "Where Am I From? Identifying Origin of LLM-generated Content",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liying Li",
      "Yihan Bai",
      "Minhao Cheng"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.682": {
    "title": "ReadMe++: Benchmarking Multilingual Language Models for Multi-Domain Readability Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tarek Naous",
      "Michael Ryan",
      "Anton Lavrouk",
      "Mohit Chandra",
      "Wei Xu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.683": {
    "title": "GlossLM: A Massively Multilingual Corpus and Pretrained Model for Interlinear Glossed Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Ginn",
      "Lindia Tjuatja",
      "Taiqi He",
      "Enora Rice",
      "Graham Neubig",
      "Alexis Palmer",
      "Lori Levin"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.684": {
    "title": "GDTB: Genre Diverse Data for English Shallow Discourse Parsing across Modalities, Text Types, and Domains",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Janet Liu",
      "Tatsuya Aoyama",
      "Wesley Scivetti",
      "Yilun Zhu",
      "Shabnam Behzad",
      "Lauren Levine",
      "Jessica Lin",
      "Devika Tiwari",
      "Amir Zeldes"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.685": {
    "title": "RA2FD: Distilling Faithfulness into Efficient Dialogue Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan Zhu",
      "Yusheng Liao",
      "Chenxin Xu",
      "Yunfeng Guan",
      "Yanfeng Wang",
      "Yu Wang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.686": {
    "title": "Subjective Topic meets LLMs: Unleashing Comprehensive, Reflective and Creative Thinking through the Negation of Negation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangrui Lv",
      "Kaixiong Gong",
      "Jian Liang",
      "Xinyu Pang",
      "Changshui Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.687": {
    "title": "Experimental Contexts Can Facilitate Robust Semantic Property Inference in Language Models, but Inconsistently",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kanishka Misra",
      "Allyson Ettinger",
      "Kyle Mahowald"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.688": {
    "title": "Leveraging Estimated Transferability Over Human Intuition for Model Selection in Text Ranking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Bai",
      "Zhuofan Chen",
      "Zhenzi Li",
      "Hanhua Hong",
      "Jianfei Zhang",
      "Chen Li",
      "Chenghua Lin",
      "Wenge Rong"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.689": {
    "title": "Unveiling In-Context Learning: A Coordinate System to Understand Its Working Mechanism",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anhao Zhao",
      "Fanghua Ye",
      "Jinlan Fu",
      "Xiaoyu Shen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.690": {
    "title": "Self-Powered LLM Modality Expansion for Large Speech-Text Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tengfei Yu",
      "Xuebo Liu",
      "Zhiyi Hou",
      "Liang Ding",
      "Dacheng Tao",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.691": {
    "title": "ABSEval: An Agent-based Framework for Script Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sirui Liang",
      "Baoli Zhang",
      "Jun Zhao",
      "Kang Liu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.692": {
    "title": "Latent Concept-based Explanation of NLP Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuemin Yu",
      "Fahim Dalvi",
      "Nadir Durrani",
      "Marzia Nouri",
      "Hassan Sajjad"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.693": {
    "title": "Decoding with Limited Teacher Supervision Requires Understanding When to Trust the Teacher",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyunjong Ok",
      "Jegwang Ryu",
      "Jaeho Lee"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.694": {
    "title": "Enhancing Data Quality through Simple De-duplication: Navigating Responsible Computational Social Science Research",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yida Mu",
      "Mali Jin",
      "Xingyi Song",
      "Nikolaos Aletras"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.695": {
    "title": "The Mystery of the Pathological Path-star Task for Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arvid Frydenlund"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.696": {
    "title": "Voices in a Crowd: Searching for clusters of unique perspectives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikolas Vitsakis",
      "Amit Parekh",
      "Ioannis Konstas"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.697": {
    "title": "Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character Role-Playing Agent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyan Yu",
      "Tongxu Luo",
      "Yifan Wei",
      "Fangyu Lei",
      "Yiming Huang",
      "Hao Peng",
      "Liehuang Zhu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.698": {
    "title": "SLANG: New Concept Comprehension of Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingrui Mei",
      "Shenghua Liu",
      "Yiwei Wang",
      "Baolong Bi",
      "Xueqi Cheng"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.699": {
    "title": "Towards Interpretable Sequence Continuation: Analyzing Shared Circuits in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Lan",
      "Philip Torr",
      "Fazl Barez"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.700": {
    "title": "Why Does New Knowledge Create Messy Ripple Effects in LLMs?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxin Qin",
      "Zixuan Zhang",
      "Chi Han",
      "Pengfei Yu",
      "Manling Li",
      "Heng Ji"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.701": {
    "title": "Lifelong Event Detection via Optimal Transport",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Viet Dao",
      "Van-Cuong Pham",
      "Quyen Tran",
      "Thanh-Thien Le",
      "Linh Ngo",
      "Thien Nguyen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.702": {
    "title": "SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research Repositories",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ben Bogin",
      "Kejuan Yang",
      "Shashank Gupta",
      "Kyle Richardson",
      "Erin Bransom",
      "Peter Clark",
      "Ashish Sabharwal",
      "Tushar Khot"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.703": {
    "title": "FIRST: Teach A Reliable Large Language Model Through Efficient Trustworthy Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "KaShun Shum",
      "Minrui Xu",
      "Jianshu Zhang",
      "Zixin Chen",
      "Shizhe Diao",
      "Hanze Dong",
      "Jipeng Zhang",
      "Muhammad Raza"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.704": {
    "title": "Domain adapted machine translation: What does catastrophic forgetting forget and why?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Danielle Saunders",
      "Steve DeNeefe"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.705": {
    "title": "Enhancing AI Assisted Writing with One-Shot Implicit Negative Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Towle",
      "Ke Zhou"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.706": {
    "title": "Atomic Self-Consistency for Better Long Form Generations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raghuveer Thirukovalluru",
      "Yukun Huang",
      "Bhuwan Dhingra"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.707": {
    "title": "Global is Good, Local is Bad?\": Understanding Brand Bias in LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mahammed Kamruzzaman",
      "Hieu Nguyen",
      "Gene Kim"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.708": {
    "title": "Optimizing Rare Word Accuracy in Direct Speech Translation with a Retrieval-and-Demonstration Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siqi Li",
      "Danni Liu",
      "Jan Niehues"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.709": {
    "title": "ACE: A LLM-based Negotiation Coaching System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryan Shea",
      "Aymen Kallala",
      "Xin Liu",
      "Michael Morris",
      "Zhou Yu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.710": {
    "title": "TransferTOD: A Generalizable Chinese Multi-Domain Task-Oriented Dialogue System with Transfer Capabilities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming Zhang",
      "Caishuang Huang",
      "Yilong Wu",
      "Shichun Liu",
      "Huiyuan Zheng",
      "Yurui Dong",
      "Yujiong Shen",
      "Shihan Dou",
      "Jun Zhao",
      "Junjie Ye",
      "Qi Zhang",
      "Tao Gui",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.711": {
    "title": "PATIENT-𝜓: Using Large Language Models to Simulate Patients for Training Mental Health Professionals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiyi Wang",
      "Stephanie Milani",
      "Jamie Chiu",
      "Jiayin Zhi",
      "Shaun Eack",
      "Travis Labrum",
      "Samuel Murphy",
      "Nev Jones",
      "Kate Hardy",
      "Hong Shen",
      "Fei Fang",
      "Zhiyu Chen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.712": {
    "title": "DKEC: Domain Knowledge Enhanced Multi-Label Classification for Diagnosis Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xueren Ge",
      "Abhishek Satpathy",
      "Ronald Williams",
      "John Stankovic",
      "Homa Alemzadeh"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.713": {
    "title": "ModSCAN: Measuring Stereotypical Bias in Large Vision-Language Models from Vision and Language Modalities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yukun Jiang",
      "Zheng Li",
      "Xinyue Shen",
      "Yugeng Liu",
      "Michael Backes",
      "Yang Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.714": {
    "title": "Large Language Models Can Self-Correct with Key Condition Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenyu Wu",
      "Qingkai Zeng",
      "Zhihan Zhang",
      "Zhaoxuan Tan",
      "Chao Shen",
      "Meng Jiang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.715": {
    "title": "Learning to Write Rationally: How Information Is Distributed in Non-native Speakers' Essays",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixin Tang",
      "Janet Van Hell"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.716": {
    "title": "Defending Against Social Engineering Attacks in the Age of LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lin Ai",
      "Tharindu Kumarage",
      "Amrita Bhattacharjee",
      "Zizhou Liu",
      "Zheng Hui",
      "Michael Davinroy",
      "James Cook",
      "Laura Cassani",
      "Kirill Trapeznikov",
      "Matthias Kirchner",
      "Arslan Basharat",
      "Anthony Hoogs",
      "Joshua Garland",
      "Huan Liu",
      "Julia Hirschberg"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.717": {
    "title": "Heterogeneous LoRA for Federated Fine-tuning of On-Device Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yae Jee Cho",
      "Luyang Liu",
      "Zheng Xu",
      "Aldi Fahrezi",
      "Gauri Joshi"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.718": {
    "title": "Make Some Noise: Unlocking Language Model Parallel Inference Capability through Noisy Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixuan Wang",
      "Xianzhen Luo",
      "Fuxuan Wei",
      "Yijun Liu",
      "Qingfu Zhu",
      "Xuanyu Zhang",
      "Qing Yang",
      "Dongliang Xu",
      "Wanxiang Che"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.719": {
    "title": "Target-Aware Language Modeling via Granular Data Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ernie Chang",
      "Pin-Jie Lin",
      "Yang Li",
      "Changsheng Zhao",
      "Daeil Kim",
      "Rastislav Rabatin",
      "Zechun Liu",
      "Yangyang Shi",
      "Vikas Chandra"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.720": {
    "title": "SPEED++: A Multilingual Event Extraction Framework for Epidemic Prediction and Preparedness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tanmay Parekh",
      "Jeffrey Kwan",
      "Jiarui Yu",
      "Sparsh Johri",
      "Hyosang Ahn",
      "Sreya Muppalla",
      "Kai-Wei Chang",
      "Wei Wang",
      "Nanyun Peng"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.721": {
    "title": "CoGen: Learning from Feedback with Coupled Comprehension and Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mustafa Omer Gul",
      "Yoav Artzi"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.722": {
    "title": "UNICORN: A Unified Causal Video-Oriented Language-Modeling Framework for Temporal Video-Language Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanhao Xiong",
      "Yixin Nie",
      "Haotian Liu",
      "Boxin Wang",
      "Jun Chen",
      "Rong Jin",
      "Cho-Jui Hsieh",
      "Lorenzo Torresani",
      "Jie Lei"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.723": {
    "title": "Story Morals: Surfacing value-driven narrative schemas using large language models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Hobson",
      "Haiqi Zhou",
      "Derek Ruths",
      "Andrew Piper"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.724": {
    "title": "OATH-Frames: Characterizing Online Attitudes Towards Homelessness with LLM Assistants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaspreet Ranjit",
      "Brihi Joshi",
      "Rebecca Dorn",
      "Laura Petry",
      "Olga Koumoundouros",
      "Jayne Bottarini",
      "Peichen Liu",
      "Eric Rice",
      "Swabha Swayamdipta"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.725": {
    "title": "AnaloBench: Benchmarking the Identification of Abstract and Long-context Analogies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Ye",
      "Andrew Wang",
      "Jacob Choi",
      "Yining Lu",
      "Shreya Sharma",
      "Lingfeng Shen",
      "Vijay Murari Tiyyala",
      "Nicholas Andrews",
      "Daniel Khashabi"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.726": {
    "title": "SciER: An Entity and Relation Extraction Dataset for Datasets, Methods, and Tasks in Scientific Documents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Zhang",
      "Zhijia Chen",
      "Huitong Pan",
      "Cornelia Caragea",
      "Longin Latecki",
      "Eduard Dragut"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.727": {
    "title": "Analysis of Plan-based Retrieval for Grounded Text Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ameya Godbole",
      "Nicholas Monath",
      "Seungyeon Kim",
      "Ankit Singh Rawat",
      "Andrew McCallum",
      "Manzil Zaheer"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.728": {
    "title": "Detecting Errors through Ensembling Prompts (DEEP): An End-to-End LLM Framework for Detecting Factual Errors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alex Chandler",
      "Devesh Surve",
      "Hui Su"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.729": {
    "title": "RLHF Can Speak Many Languages: Unlocking Multilingual Preference Optimization for LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "John Dang",
      "Arash Ahmadian",
      "Kelly Marchisio",
      "Julia Kreutzer",
      "Ahmet Üstün",
      "Sara Hooker"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.730": {
    "title": "Boosting Logical Fallacy Reasoning in LLMs via Logical Structure Tree",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanyuan Lei",
      "Ruihong Huang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.731": {
    "title": "Chain and Causal Attention for Efficient Entity Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Erwan Fagnou",
      "Paul Caillon",
      "Blaise Delattre",
      "Alexandre Allauzen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.732": {
    "title": "BEEAR: Embedding-based Adversarial Removal of Safety Backdoors in Instruction-tuned Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Zeng",
      "Weiyu Sun",
      "Tran Huynh",
      "Dawn Song",
      "Bo Li",
      "Ruoxi Jia"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.733": {
    "title": "A Bayesian Approach to Harnessing the Power of LLMs in Authorship Attribution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengmian Hu",
      "Tong Zheng",
      "Heng Huang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.734": {
    "title": "FAC2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoqiang Wang",
      "Lingfei Wu",
      "Tengfei Ma",
      "Bang Liu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.735": {
    "title": "OpenSep: Leveraging Large Language Models with Textual Inversion for Open World Audio Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tanvir Mahmud",
      "Diana Marculescu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.736": {
    "title": "Language Concept Erasure for Language-invariant Dense Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiqi Huang",
      "Puxuan Yu",
      "Shauli Ravfogel",
      "James Allan"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.737": {
    "title": "Learning Personalized Alignment for Evaluating Open-ended Text Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Danqing Wang",
      "Kevin Yang",
      "Hanlin Zhu",
      "Xiaomeng Yang",
      "Andrew Cohen",
      "Lei Li",
      "Yuandong Tian"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.738": {
    "title": "Large Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure for Jailbreak Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Zhou",
      "Henry Zou",
      "Barbara Di Eugenio",
      "Yang Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.739": {
    "title": "Turn Waste into Worth: Rectifying Top-k Router of MoE",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan Zeng",
      "Qipeng Guo",
      "Zhaoye Fei",
      "Zhangyue Yin",
      "Yunhua Zhou",
      "Linyang Li",
      "Tianxiang Sun",
      "Hang Yan",
      "Dahua Lin",
      "Xipeng Qiu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.740": {
    "title": "Null-Shot Prompting: Rethinking Prompting Large Language Models With Hallucination",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pittawat Taveekitworachai",
      "Febri Abdullah",
      "Ruck Thawonmas"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.741": {
    "title": "CommVQA: Situating Visual Question Answering in Communicative Contexts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nandita Naik",
      "Christopher Potts",
      "Elisa Kreiss"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.742": {
    "title": "Ouroboros: Generating Longer Drafts Phrase by Phrase for Faster Speculative Decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weilin Zhao",
      "Yuxiang Huang",
      "Xu Han",
      "Wang Xu",
      "Chaojun Xiao",
      "Xinrong Zhang",
      "Yewei Fang",
      "Kaihuo Zhang",
      "Zhiyuan Liu",
      "Maosong Sun"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.743": {
    "title": "1+1>2: Can Large Language Models Serve as Cross-Lingual Knowledge Aggregators?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Huang",
      "Chenrui Fan",
      "Yuan Li",
      "Siyuan Wu",
      "Tianyi Zhou",
      "Xiangliang Zhang",
      "Lichao Sun"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.744": {
    "title": "How to Leverage Demonstration Data in Alignment for Large Language Model? A Self-Imitation Learning Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Teng Xiao",
      "Mingxiao Li",
      "Yige Yuan",
      "Huaisheng Zhu",
      "Chao Cui",
      "Vasant Honavar"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.745": {
    "title": "Style-Specific Neurons for Steering LLMs in Text Style Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wen Lai",
      "Viktor Hangya",
      "Alexander Fraser"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.746": {
    "title": "Adaptive Query Rewriting: Aligning Rewriters through Marginal Probability of Conversational Answers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianhua Zhang",
      "Kun Li",
      "Hongyin Luo",
      "Xixin Wu",
      "James Glass",
      "Helen Meng"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.747": {
    "title": "Grasping the Essentials: Tailoring Large Language Models for Zero-Shot Relation Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sizhe Zhou",
      "Yu Meng",
      "Bowen Jin",
      "Jiawei Han"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.748": {
    "title": "DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Huang",
      "Jianwen Luo",
      "Yan Yu",
      "Yitong Zhang",
      "Fangyu Lei",
      "Yifan Wei",
      "Shizhu He",
      "Lifu Huang",
      "Xiao Liu",
      "Jun Zhao",
      "Kang Liu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.749": {
    "title": "Leveraging Context-Aware Prompting for Commit Message Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihua Jiang",
      "Jianwei Chen",
      "Dongning Rao",
      "Guanghui Ye"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.750": {
    "title": "Linguistic Bias in ChatGPT: Language Models Reinforce Dialect Discrimination",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eve Fleisig",
      "Genevieve Smith",
      "Madeline Bossi",
      "Ishita Rustagi",
      "Xavier Yin",
      "Dan Klein"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.751": {
    "title": "Lifelong Knowledge Editing for LLMs with Retrieval-Augmented Continuous Prompt Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qizhou Chen",
      "Taolin Zhang",
      "Xiaofeng He",
      "Dongyang Li",
      "Chengyu Wang",
      "Longtao Huang",
      "Hui Xue’"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.752": {
    "title": "A Learning Rate Path Switching Training Paradigm for Version Updates of Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihao Wang",
      "Shiyu Liu",
      "Jianheng Huang",
      "Wang Zheng",
      "YiXuan Liao",
      "Xiaoxin Chen",
      "Junfeng Yao",
      "Jinsong Su"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.753": {
    "title": "Zero-Shot Cross-Lingual NER Using Phonemic Representations for Low-Resource Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jimin Sohn",
      "Haeji Jung",
      "Alex Cheng",
      "Jooeon Kang",
      "Yilin Du",
      "David Mortensen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.754": {
    "title": "An Analysis and Mitigation of the Reversal Curse",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ang Lv",
      "Kaiyi Zhang",
      "Shufang Xie",
      "Quan Tu",
      "Yuhan Chen",
      "Ji-Rong Wen",
      "Rui Yan"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.755": {
    "title": "Exploring the Practicality of Generative Retrieval on Dynamic Corpora",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaeeun Kim",
      "Soyoung Yoon",
      "Hyunji Lee",
      "Joel Jang",
      "Sohee Yang",
      "Minjoon Seo"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.756": {
    "title": "OneNet: A Fine-Tuning Free Framework for Few-Shot Entity Linking via Large Language Model Prompting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xukai Liu",
      "Ye Liu",
      "Kai Zhang",
      "Kehang Wang",
      "Qi Liu",
      "Enhong Chen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.757": {
    "title": "Don't Just Say \"I don't know\"! Self-aligning Large Language Models for Responding to Unknown Questions with Explanations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Deng",
      "Yong Zhao",
      "Moxin Li",
      "See-Kiong Ng",
      "Tat-Seng Chua"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.758": {
    "title": "Fewer is More: Boosting Math Reasoning with Reinforced Context Pruning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xijie Huang",
      "Li Lyna Zhang",
      "Kwang-Ting Cheng",
      "Fan Yang",
      "Mao Yang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.759": {
    "title": "Large Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fenglin Liu",
      "Zheng Li",
      "Hongjian Zhou",
      "Qingyu Yin",
      "Jingfeng Yang",
      "Xianfeng Tang",
      "Chen Luo",
      "Ming Zeng",
      "Haoming Jiang",
      "Yifan Gao",
      "Priyanka Nigam",
      "Sreyashi Nag",
      "Bing Yin",
      "Yining Hua",
      "Xuan Zhou",
      "Omid Rohanian",
      "Anshul Thakur",
      "Lei Clifton",
      "David Clifton"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.760": {
    "title": "Holistic Automated Red Teaming for Large Language Models through Top-Down Test Case Generation and Multi-turn Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinchuan Zhang",
      "Yan Zhou",
      "Yaxin Liu",
      "Ziming Li",
      "Songlin Hu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.761": {
    "title": "Householder Pseudo-Rotation: A Novel Approach to Activation Editing in LLMs with Direction-Magnitude Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Van-Cuong Pham",
      "Thien Nguyen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.762": {
    "title": "DynamicER: Resolving Emerging Mentions to Dynamic Entities for RAG",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinyoung Kim",
      "Dayoon Ko",
      "Gunhee Kim"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.763": {
    "title": "Preserving Generalization of Language models in Few-shot Continual Relation Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quyen Tran",
      "Nguyen Thanh",
      "Nguyen Anh",
      "Nam Hai",
      "Trung Le",
      "Linh Ngo",
      "Thien Nguyen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.764": {
    "title": "A Systematic Survey and Critical Review on Evaluating Large Language Models: Challenges, Limitations, and Recommendations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Tahmid Rahman Laskar",
      "Sawsan Alqahtani",
      "M Saiful Bari",
      "Mizanur Rahman",
      "Mohammad Abdullah Matin Khan",
      "Haidar Khan",
      "Israt Jahan",
      "Amran Bhuiyan",
      "Chee Wei Tan",
      "Md Rizwan Parvez",
      "Enamul Hoque",
      "Shafiq Joty",
      "Jimmy Huang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.765": {
    "title": "Consecutive Batch Model Editing with HooK Layers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuaiyi Li",
      "Yang Deng",
      "Deng Cai",
      "Hongyuan Lu",
      "Liang Chen",
      "Wai Lam"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.766": {
    "title": "Topic-Oriented Open Relation Extraction with A Priori Seed Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linyi Ding",
      "Jinfeng Xiao",
      "Sizhe Zhou",
      "Chaoqi Yang",
      "Jiawei Han"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.767": {
    "title": "Related Work and Citation Text Generation: A Survey",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangci Li",
      "Jessica Ouyang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.768": {
    "title": "Curriculum Consistency Learning for Conditional Sentence Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liangxin Liu",
      "Xuebo Liu",
      "Lian Lian",
      "Shengjun Cheng",
      "Jun Rao",
      "Tengfei Yu",
      "Hexuan Deng",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.769": {
    "title": "A Systematic Analysis of Large Language Models as Soft Reasoners: The Case of Syllogistic Inferences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leonardo Bertolazzi",
      "Albert Gatt",
      "Raffaella Bernardi"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.770": {
    "title": "Pre-training Cross-lingual Open Domain Question Answering with Large-scale Synthetic Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Jiang",
      "Tom Drummond",
      "Trevor Cohn"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.771": {
    "title": "MOSEL: 950,000 Hours of Speech Data for Open-Source Speech Foundation Model Training on EU Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marco Gaido",
      "Sara Papi",
      "Luisa Bentivogli",
      "Alessio Brutti",
      "Mauro Cettolo",
      "Roberto Gretter",
      "Marco Matassoni",
      "Mohamed Nabih",
      "Matteo Negri"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.772": {
    "title": "Improving Knowledge Graph Completion with Structure-Aware Supervised Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiashi Lin",
      "Lifang Wang",
      "Xinyu Lu",
      "Zhongtian Hu",
      "Wei Zhang",
      "Wenxuan Lu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.773": {
    "title": "Contribution of Linguistic Typology to Universal Dependency Parsing: An Empirical Investigation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Basirat",
      "Navid Hemmati"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.774": {
    "title": "TRoTR: A Framework for Evaluating the Re-contextualization of Text Reuse",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesco Periti",
      "Pierluigi Cassotti",
      "Stefano Montanelli",
      "Nina Tahmasebi",
      "Dominik Schlechtweg"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.775": {
    "title": "Structured Optimal Brain Pruning for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiateng Wei",
      "Quan Lu",
      "Ning Jiang",
      "Siqi Li",
      "Jingyang Xiang",
      "Jun Chen",
      "Yong Liu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.776": {
    "title": "Automatically Generated Definitions and their utility for Modeling Word Meaning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesco Periti",
      "David Alfter",
      "Nina Tahmasebi"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.777": {
    "title": "How Do Your Code LLMs perform? Empowering Code Instruction Tuning with Really Good Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yejie Wang",
      "Keqing He",
      "Dayuan Fu",
      "Zhuoma GongQue",
      "Heyang Xu",
      "Yanxu Chen",
      "Zhexu Wang",
      "Yujia Fu",
      "Guanting Dong",
      "Muxi Diao",
      "Jingang Wang",
      "Mengdi Zhang",
      "Xunliang Cai",
      "Weiran Xu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.778": {
    "title": "MAIR: A Massive Benchmark for Evaluating Instructed Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiwei Sun",
      "Zhengliang Shi",
      "Wu Long",
      "Lingyong Yan",
      "Xinyu Ma",
      "Yiding Liu",
      "Min Cao",
      "Dawei Yin",
      "Zhaochun Ren"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.779": {
    "title": "Rethinking the Evaluation of In-Context Learning for LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guoxin Yu",
      "Lemao Liu",
      "Mo Yu",
      "Yue Yu",
      "Xiang Ao"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.780": {
    "title": "Cluster-Norm for Unsupervised Probing of Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Walter Laurito",
      "Sharan Maiya",
      "Grégoire Dhimoïla",
      "Owen Yeung",
      "Kaarel Hänni"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.781": {
    "title": "Hopping Too Late: Exploring the Limitations of Large Language Models on Multi-Hop Queries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eden Biran",
      "Daniela Gottesman",
      "Sohee Yang",
      "Mor Geva",
      "Amir Globerson"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.782": {
    "title": "Enhancing Training Data Attribution for Large Language Models with Fitting Error Consideration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kangxi Wu",
      "Liang Pang",
      "Huawei Shen",
      "Xueqi Cheng"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.783": {
    "title": "Where am I? Large Language Models Wandering between Semantics and Structures in Long Contexts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seonmin Koo",
      "Jinsung Kim",
      "YoungJoon Jang",
      "Chanjun Park",
      "Heuiseok Lim"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.784": {
    "title": "KARL: Knowledge-Aware Retrieval and Representations aid Retention and Learning in Students",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew Shu",
      "Nishant Balepur",
      "Shi Feng",
      "Jordan Boyd-Graber"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.785": {
    "title": "Large Language Models Can Be Contextual Privacy Protection Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yijia Xiao",
      "Yiqiao Jin",
      "Yushi Bai",
      "Yue Wu",
      "Xianjun Yang",
      "Xiao Luo",
      "Wenchao Yu",
      "Xujiang Zhao",
      "Yanchi Liu",
      "Quanquan Gu",
      "Haifeng Chen",
      "Wei Wang",
      "Wei Cheng"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.786": {
    "title": "A SMART Mnemonic Sounds like \"Glue Tonic\": Mixing LLMs with Student Feedback to Make Mnemonic Learning Stick",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nishant Balepur",
      "Matthew Shu",
      "Alexander Hoyle",
      "Alison Robey",
      "Shi Feng",
      "Seraphina Goldfarb-Tarrant",
      "Jordan Boyd-Graber"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.787": {
    "title": "Mixture-of-Skills: Learning to Optimize Data Usage for Fine-Tuning Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minghao Wu",
      "Thuy-Trang Vu",
      "Lizhen Qu",
      "Reza Haf"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.788": {
    "title": "MolTRES: Improving Chemical Language Representation Learning for Molecular Property Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun-Hyung Park",
      "Yeachan Kim",
      "Mingyu Lee",
      "Hyuntae Park",
      "SangKeun Lee"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.789": {
    "title": "First Heuristic Then Rational: Dynamic Use of Heuristics in Language Model Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yoichi Aoki",
      "Keito Kudo",
      "Tatsuki Kuribayashi",
      "Shusaku Sone",
      "Masaya Taniguchi",
      "Keisuke Sakaguchi",
      "Kentaro Inui"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.790": {
    "title": "Tools Fail: Detecting Silent Errors in Faulty Tools",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jimin Sun",
      "So Yeon Min",
      "Yingshan Chang",
      "Yonatan Bisk"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.791": {
    "title": "Pcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowen Zhang",
      "Chunping Li"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.792": {
    "title": "Cross-lingual Back-Parsing: Utterance Synthesis from Meaning Representation for Zero-Resource Semantic Parsing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Deokhyung Kang",
      "Seonjeong Hwang",
      "Yunsu Kim",
      "Gary Lee"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.793": {
    "title": "Shaking Up VLMs: Comparing Transformers and Structured State Space Models for Vision & Language Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Georgios Pantazopoulos",
      "Malvina Nikandrou",
      "Alessandro Suglia",
      "Oliver Lemon",
      "Arash Eshghi"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.794": {
    "title": "Are LLMs Good Zero-Shot Fallacy Classifiers?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fengjun Pan",
      "Xiaobao Wu",
      "Zongrui Li",
      "Anh Tuan Luu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.795": {
    "title": "The Mystery of In-Context Learning: A Comprehensive Survey on Interpretation and Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxiang Zhou",
      "Jiazheng Li",
      "Yanzheng Xiang",
      "Hanqi Yan",
      "Lin Gui",
      "Yulan He"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.796": {
    "title": "More DWUGs: Extending and Evaluating Word Usage Graph Datasets in Multiple Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dominik Schlechtweg",
      "Pierluigi Cassotti",
      "Bill Noble",
      "David Alfter",
      "Sabine Schulte Im Walde",
      "Nina Tahmasebi"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.797": {
    "title": "Vision-Language Model Fine-Tuning via Simple Parameter-Efficient Modification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming Li",
      "Jike Zhong",
      "Chenxin Li",
      "Liuzhuozheng Li",
      "Nie Lin",
      "Masashi Sugiyama"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.798": {
    "title": "ECIS-VQG: Generation of Entity-centric Information-seeking Questions from Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arpan Phukan",
      "Manish Gupta",
      "Asif Ekbal"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.799": {
    "title": "Distractor Generation in Multiple-Choice Tasks: A Survey of Methods, Datasets, and Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elaf Alhazmi",
      "Quan Sheng",
      "Wei Emma Zhang",
      "Munazza Zaib",
      "Ahoud Alhazmi"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.800": {
    "title": "Evaluating n-Gram Novelty of Language Models Using Rusty-DAWG",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "William Merrill",
      "Noah Smith",
      "Yanai Elazar"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.801": {
    "title": "ASL STEM Wiki: Dataset and Benchmark for Interpreting STEM Articles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kayo Yin",
      "Chinmay Singh",
      "Fyodor Minakov",
      "Vanessa Milan",
      "Hal Daumé Iii",
      "Cyril Zhang",
      "Alex Lu",
      "Danielle Bragg"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.802": {
    "title": "Can Automatic Metrics Assess High-Quality Translations?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sweta Agrawal",
      "António Farinhas",
      "Ricardo Rei",
      "Andre Martins"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.803": {
    "title": "Modeling User Preferences with Automatic Metrics: Creating a High-Quality Preference Dataset for Machine Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sweta Agrawal",
      "José De Souza",
      "Ricardo Rei",
      "António Farinhas",
      "Gonçalo Faria",
      "Patrick Fernandes",
      "Nuno Guerreiro",
      "Andre Martins"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.804": {
    "title": "DC-Instruct: An Effective Framework for Generative Multi-intent Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowen Xing",
      "Lizi Liao",
      "Minlie Huang",
      "Ivor Tsang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.805": {
    "title": "KnowTuning: Knowledge-aware Fine-tuning for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yougang Lyu",
      "Lingyong Yan",
      "Shuaiqiang Wang",
      "Haibo Shi",
      "Dawei Yin",
      "Pengjie Ren",
      "Zhumin Chen",
      "Maarten Rijke",
      "Zhaochun Ren"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.806": {
    "title": "SecCoder: Towards Generalizable and Robust Secure Code Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boyu Zhang",
      "Tianyu Du",
      "Junkai Tong",
      "Xuhong Zhang",
      "Kingsum Chow",
      "Sheng Cheng",
      "Xun Wang",
      "Jianwei Yin"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.807": {
    "title": "Nash CoT: Multi-Path Inference with Preference Equilibrium",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziqi Zhang",
      "Cunxiang Wang",
      "Xiao Xiong",
      "Yue Zhang",
      "Donglin Wang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.808": {
    "title": "Scalable Efficient Training of Large Language Models with Low-dimensional Projected Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingtai Lv",
      "Ning Ding",
      "Kaiyan Zhang",
      "Ermo Hua",
      "Ganqu Cui",
      "Bowen Zhou"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.809": {
    "title": "Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoxue Cheng",
      "Junyi Li",
      "Xin Zhao",
      "Hongzhi Zhang",
      "Fuzheng Zhang",
      "Di Zhang",
      "Kun Gai",
      "Ji-Rong Wen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.810": {
    "title": "Interpretable Composition Attribution Enhancement for Visio-linguistic Compositional Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Li",
      "Zhen Huang",
      "Xinmei Tian",
      "Le Lu",
      "Houqiang Li",
      "Xu Shen",
      "Jieping Ye"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.811": {
    "title": "LLM Task Interference: An Initial Study on the Impact of Task-Switch in Conversational History",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akash Gupta",
      "Ivaxi Sheth",
      "Vyas Raina",
      "Mark Gales",
      "Mario Fritz"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.812": {
    "title": "Social Bias Probing: Fairness Benchmarking for Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marta Marchiori Manerba",
      "Karolina Stanczak",
      "Riccardo Guidotti",
      "Isabelle Augenstein"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.813": {
    "title": "Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhao Yu",
      "Hongming Zhang",
      "Xiaoman Pan",
      "Peixin Cao",
      "Kaixin Ma",
      "Jian Li",
      "Hongwei Wang",
      "Dong Yu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.814": {
    "title": "DynaThink: Fast or Slow? A Dynamic Decision-Making Framework for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiabao Pan",
      "Yan Zhang",
      "Chen Zhang",
      "Zuozhu Liu",
      "Hongwei Wang",
      "Haizhou Li"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.815": {
    "title": "Revisiting Automated Evaluation for Long-form Table Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuqi Wang",
      "Lyuhao Chen",
      "Songcheng Cai",
      "Zhijian Xu",
      "Yilun Zhao"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.816": {
    "title": "Weak Reward Model Transforms Generative Models into Robust Causal Event Extraction Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Italo Silva",
      "Hanqi Yan",
      "Lin Gui",
      "Yulan He"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.817": {
    "title": "Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihan Zhang",
      "Tao Ge",
      "Zhenwen Liang",
      "Wenhao Yu",
      "Dian Yu",
      "Mengzhao Jia",
      "Dong Yu",
      "Meng Jiang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.818": {
    "title": "FinDVer: Explainable Claim Verification over Long and Hybrid-content Financial Documents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilun Zhao",
      "Yitao Long",
      "Tintin Jiang",
      "Chengye Wang",
      "Weiyuan Chen",
      "Hongjun Liu",
      "Xiangru Tang",
      "Yiming Zhang",
      "Chen Zhao",
      "Arman Cohan"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.819": {
    "title": "Extracting Prompts by Inverting LLM Outputs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Collin Zhang",
      "John Morris",
      "Vitaly Shmatikov"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.820": {
    "title": "BiasAlert: A Plug-and-play Tool for Social Bias Detection in LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiting Fan",
      "Ruizhe Chen",
      "Ruiling Xu",
      "Zuozhu Liu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.821": {
    "title": "VHASR: A Multimodal Speech Recognition System With Vision Hotwords",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiliang Hu",
      "Zuchao Li",
      "Ping Wang",
      "Haojun Ai",
      "Lefei Zhang",
      "Hai Zhao"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.822": {
    "title": "A Probability–Quality Trade-off in Aligned Language Models and its Relation to Sampling Adaptors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Naaman Tan",
      "Josef Valvoda",
      "Tianyu Liu",
      "Anej Svete",
      "Yanxia Qin",
      "Min-Yen Kan",
      "Ryan Cotterell"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.823": {
    "title": "Bridging Local Details and Global Context in Text-Attributed Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaoke Wang",
      "Yun Zhu",
      "Wenqiao Zhang",
      "Yueting Zhuang",
      "Liyunfei Liyunfei",
      "Siliang Tang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.824": {
    "title": "Building Resources for Emakhuwa: Machine Translation and News Classification Benchmarks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Felermino Ali",
      "Henrique Lopes Cardoso",
      "Rui Sousa-Silva"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.825": {
    "title": "RepMatch: Quantifying Cross-Instance Similarities in Representation Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Modarres",
      "Sina Abbasi",
      "Mohammad Taher Pilehvar"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.826": {
    "title": "Commonsense Knowledge Editing Based on Free-Text in LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiusheng Huang",
      "Yequan Wang",
      "Jun Zhao",
      "Kang Liu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.827": {
    "title": "A Closer Look at Multidimensional Online Political Incivility",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sagi Pendzel",
      "Nir Lotan",
      "Alon Zoizner",
      "Einat Minkov"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.828": {
    "title": "Leveraging BERT and TFIDF Features for Short Text Clustering via Alignment-Promoting Co-Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zetong Li",
      "Qinliang Su",
      "Shijing Si",
      "Jianxing Yu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.829": {
    "title": "Applying Intrinsic Debiasing on Downstream Tasks: Challenges and Considerations for Machine Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bar Iluz",
      "Yanai Elazar",
      "Asaf Yehudai",
      "Gabriel Stanovsky"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.830": {
    "title": "Unsupervised Named Entity Disambiguation for Low Resource Domains",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Debarghya Datta",
      "Soumajit Pramanik"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.831": {
    "title": "SparseGrad: A Selective Method for Efficient Fine-tuning of MLP Layers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Viktoriia Chekalina",
      "Anna Rudenko",
      "Gleb Mezentsev",
      "Aleksandr Mikhalev",
      "Alexander Panchenko",
      "Ivan Oseledets"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.832": {
    "title": "MoCoKGC: Momentum Contrast Entity Encoding for Knowledge Graph Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingyang Li",
      "Yanru Zhong",
      "Yuchu Qin"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.833": {
    "title": "ActPlan-1K: Benchmarking the Procedural Planning Ability of Visual Language Models in Household Activities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ying Su",
      "Zhan Ling",
      "Haochen Shi",
      "Cheng Jiayang",
      "Yauwai Yim",
      "Yangqiu Song"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.834": {
    "title": "Shortcuts Arising from Contrast: Towards Effective and Lightweight Clean-Label Attacks in Prompt-Based Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaopeng Xie",
      "Ming Yan",
      "Xiwen Zhou",
      "Chenlong Zhao",
      "Suli Wang",
      "Yong Zhang",
      "Joey Zhou"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.835": {
    "title": "GRASS: Compute Efficient Low-Memory LLM Training with Structured Sparse Gradients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aashiq Muhamed",
      "Oscar Li",
      "David Woodruff",
      "Mona Diab",
      "Virginia Smith"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.836": {
    "title": "RaTEScore: A Metric for Radiology Report Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weike Zhao",
      "Chaoyi Wu",
      "Xiaoman Zhang",
      "Ya Zhang",
      "Yanfeng Wang",
      "Weidi Xie"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.837": {
    "title": "HalluMeasure: Fine-grained Hallucination Measurement Using Chain-of-Thought Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shayan Ali Akbar",
      "Md Mosharaf Hossain",
      "Tess Wood",
      "Si-Chi Chin",
      "Erica Salinas",
      "Victor Alvarez",
      "Erwin Cornejo"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.838": {
    "title": "Learning to Rank Salient Content for Query-focused Summarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sajad Sotudeh",
      "Nazli Goharian"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.839": {
    "title": "Are Large Language Models Good Classifiers? A Study on Edit Intent Classification in Scientific Document Revisions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qian Ruan",
      "Ilia Kuznetsov",
      "Iryna Gurevych"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.840": {
    "title": "LitSearch: A Retrieval Benchmark for Scientific Literature Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anirudh Ajith",
      "Mengzhou Xia",
      "Alexis Chevalier",
      "Tanya Goyal",
      "Danqi Chen",
      "Tianyu Gao"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.841": {
    "title": "Open-world Multi-label Text Classification with Extremely Weak Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xintong Li",
      "Jinya Jiang",
      "Ria Dharmani",
      "Jayanth Srinivasa",
      "Gaowen Liu",
      "Jingbo Shang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.842": {
    "title": "LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Toni Liu",
      "Nicolas Boulle",
      "Raphaël Sarfati",
      "Christopher Earls"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.843": {
    "title": "AKEW: Assessing Knowledge Editing in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaobao Wu",
      "Liangming Pan",
      "William Yang Wang",
      "Anh Tuan Luu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.844": {
    "title": "CopyBench: Measuring Literal and Non-Literal Reproduction of Copyright-Protected Text in Language Model Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tong Chen",
      "Akari Asai",
      "Niloofar Mireshghallah",
      "Sewon Min",
      "James Grimmelmann",
      "Yejin Choi",
      "Hannaneh Hajishirzi",
      "Luke Zettlemoyer",
      "Pang Wei Koh"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.845": {
    "title": "Dense X Retrieval: What Retrieval Granularity Should We Use?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tong Chen",
      "Hongwei Wang",
      "Sihao Chen",
      "Wenhao Yu",
      "Kaixin Ma",
      "Xinran Zhao",
      "Hongming Zhang",
      "Dong Yu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.846": {
    "title": "Decoding Susceptibility: Modeling Misbelief to Misinformation Through a Computational Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanchen Liu",
      "Mingyu Ma",
      "Wenna Qin",
      "Azure Zhou",
      "Jiaao Chen",
      "Weiyan Shi",
      "Wei Wang",
      "Diyi Yang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.847": {
    "title": "Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheng Zhao",
      "Yftah Ziser",
      "Shay Cohen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.848": {
    "title": "XDetox: Text Detoxification with Token-Level Toxicity Explanations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Beomseok Lee",
      "Hyunwoo Kim",
      "Keon Kim",
      "Yong Suk Choi"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.849": {
    "title": "Optimizing Chinese Lexical Simplification Across Word Types: A Hybrid Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "ZiHao Xiao",
      "Jiefu Gong",
      "Shijin Wang",
      "Wei Song"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.850": {
    "title": "Control Large Language Models via Divide and Conquer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bingxuan Li",
      "Yiwei Wang",
      "Tao Meng",
      "Kai-Wei Chang",
      "Nanyun Peng"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.851": {
    "title": "Joint Pre-Encoding Representation and Structure Embedding for Efficient and Low-Resource Knowledge Graph Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenyu Qiu",
      "Pengjiang Qian",
      "Chuang Wang",
      "Jian Yao",
      "Li Liu",
      "Fang Wei",
      "Eddie Eddie"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.852": {
    "title": "Improving Discriminative Capability of Reward Models in RLHF Using Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lu Chen",
      "Rui Zheng",
      "Binghai Wang",
      "Senjie Jin",
      "Caishuang Huang",
      "Junjie Ye",
      "Zhihao Zhang",
      "Yuhao Zhou",
      "Zhiheng Xi",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.853": {
    "title": "RoCEL: Advancing Table Entity Linking through Distinctive Row and Column Contexts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanzheng Wang",
      "Yixing Fan",
      "Jiafeng Guo",
      "Ruqing Zhang",
      "Xueqi Cheng"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.854": {
    "title": "Exploring the Role of Reasoning Structures for Constructing Proofs in Multi-Step Natural Language Reasoning with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zi’ou Zheng",
      "Christopher Malon",
      "Martin Min",
      "Xiaodan Zhu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.855": {
    "title": "Efficient Overshadowed Entity Disambiguation by Mitigating Shortcut Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Panuthep Tasawong",
      "Peerat Limkonchotiwat",
      "Potsawee Manakul",
      "Can Udomcharoenchaikit",
      "Ekapol Chuangsuwanich",
      "Sarana Nutanong"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.856": {
    "title": "AppBench: Planning of Multiple APIs from Various APPs for Complex User Instruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongru Wang",
      "Rui Wang",
      "Boyang Xue",
      "Heming Xia",
      "Jingtao Cao",
      "Zeming Liu",
      "Jeff Pan",
      "Kam-Fai Wong"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.857": {
    "title": "Not Everything is All You Need: Toward Low-Redundant Optimization for Large Language Model Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhipeng Chen",
      "Kun Zhou",
      "Xin Zhao",
      "Jingyuan Wang",
      "Ji-Rong Wen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.858": {
    "title": "AudioVSR: Enhancing Video Speech Recognition with Audio Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoda Yang",
      "Xize Cheng",
      "Jiaqi Duan",
      "Hongshun Qiu",
      "Minjie Hong",
      "Minghui Fang",
      "Shengpeng Ji",
      "Jialong Zuo",
      "Zhiqing Hong",
      "Zhimeng Zhang",
      "Tao Jin"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.859": {
    "title": "ECCO: Can We Improve Model-Generated Code Efficiency Without Sacrificing Functional Correctness?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siddhant Waghjale",
      "Vishruth Veerendranath",
      "Zhiruo Wang",
      "Daniel Fried"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.860": {
    "title": "Ladder: A Model-Agnostic Framework Boosting LLM-based Machine Translation to the Next Level",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaopeng Feng",
      "Ruizhe Chen",
      "Yan Zhang",
      "Zijie Meng",
      "Zuozhu Liu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.861": {
    "title": "Re-ReST: Reflection-Reinforced Self-Training for Language Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zi-Yi Dou",
      "Cheng-Fu Yang",
      "Xueqing Wu",
      "Kai-Wei Chang",
      "Nanyun Peng"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.862": {
    "title": "Effective Synthetic Data and Test-Time Adaptation for OCR Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuhao Guan",
      "Cheng Xu",
      "Moule Lin",
      "Derek Greene"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.863": {
    "title": "SRF: Enhancing Document-Level Relation Extraction with a Novel Secondary Reasoning Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fu Zhang",
      "Qi Miao",
      "Jingwei Cheng",
      "Hongsen Yu",
      "Yi Yan",
      "Xin Li",
      "Yongxue Wu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.864": {
    "title": "FineCops-Ref: A new Dataset and Task for Fine-Grained Compositional Referring Expression Comprehension",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junzhuo Liu",
      "Xuzheng Yang",
      "Weiwei Li",
      "Peng Wang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.865": {
    "title": "Exploring the Learning Capabilities of Language Models using LEVERWORLDS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eitan Wagner",
      "Amir Feder",
      "Omri Abend"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.866": {
    "title": "CONTESTS: a Framework for Consistency Testing of Span Probabilities in Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eitan Wagner",
      "Yuli Slavutsky",
      "Omri Abend"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.867": {
    "title": "DocEdit-v2: Document Structure Editing Via Multimodal LLM Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manan Suri",
      "Puneet Mathur",
      "Franck Dernoncourt",
      "Rajiv Jain",
      "Vlad Morariu",
      "Ramit Sawhney",
      "Preslav Nakov",
      "Dinesh Manocha"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.868": {
    "title": "DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tzu-Han Lin",
      "Chen-An Li",
      "Hung-yi Lee",
      "Yun-Nung Chen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.869": {
    "title": "Understanding Slang with LLMs: Modelling Cross-Cultural Nuances through Paraphrasing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ifeoluwa Wuraola",
      "Nina Dethlefs",
      "Daniel Marciniak"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.870": {
    "title": "Unlocking Anticipatory Text Generation: A Constrained Approach for Large Language Models Decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lifu Tu",
      "Semih Yavuz",
      "Jin Qu",
      "Jiacheng Xu",
      "Rui Meng",
      "Caiming Xiong",
      "Yingbo Zhou"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.871": {
    "title": "Re-Reading Improves Reasoning in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaohan Xu",
      "Chongyang Tao",
      "Tao Shen",
      "Can Xu",
      "Hongbo Xu",
      "Guodong Long",
      "Jian-Guang Lou",
      "Shuai Ma"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.872": {
    "title": "Adaptive Axes: A Pipeline for In-domain Social Stereotype Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingcheng Zeng",
      "Mingyu Jin",
      "Rob Voigt"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.873": {
    "title": "ERVQA: A Dataset to Benchmark the Readiness of Large Vision Language Models in Hospital Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sourjyadip Ray",
      "Kushal Gupta",
      "Soumi Kundu",
      "Dr Kasat",
      "Somak Aditya",
      "Pawan Goyal"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.874": {
    "title": "Human-LLM Hybrid Text Answer Aggregation for Crowd Annotations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiyi Li"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.875": {
    "title": "Improve Student's Reasoning Generalizability through Cascading Decomposed CoTs Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengwei Dai",
      "Kun Li",
      "Wei Zhou",
      "Songlin Hu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.876": {
    "title": "Revisiting Supervised Contrastive Learning for Microblog Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junbo Huang",
      "Ricardo Usbeck"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.877": {
    "title": "BaitAttack: Alleviating Intention Shift in Jailbreak Attacks via Adaptive Bait Crafting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Pu",
      "Chaozhuo Li",
      "Rui Ha",
      "Litian Zhang",
      "Lirong Qiu",
      "Xi Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.878": {
    "title": "Images Speak Louder than Words: Understanding and Mitigating Bias in Vision-Language Model from a Causal Mediation Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaotian Weng",
      "Zijun Gao",
      "Jerone Andrews",
      "Jieyu Zhao"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.879": {
    "title": "Mitigating the Language Mismatch and Repetition Issues in LLM-based Machine Translation via Model Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weichuan Wang",
      "Zhaoyi Li",
      "Defu Lian",
      "Chen Ma",
      "Linqi Song",
      "Ying Wei"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.880": {
    "title": "SciAgent: Tool-augmented Language Models for Scientific Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yubo Ma",
      "Zhibin Gou",
      "Junheng Hao",
      "Ruochen Xu",
      "Shuohang Wang",
      "Liangming Pan",
      "Yujiu Yang",
      "Yixin Cao",
      "Aixin Sun"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.881": {
    "title": "Global Reward to Local Rewards: Multimodal-Guided Decomposition for Improving Dialogue Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong Won Lee",
      "Hae Park",
      "Yoon Kim",
      "Cynthia Breazeal",
      "Louis-Philippe Morency"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.882": {
    "title": "Towards Measuring and Modeling \"Culture\" in LLMs: A Survey",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Adilazuarda",
      "Sagnik Mukherjee",
      "Pradhyumna Lavania",
      "Siddhant Singh",
      "Alham Aji",
      "Jacki O’Neill",
      "Ashutosh Modi",
      "Monojit Choudhury"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.883": {
    "title": "ESC-Eval: Evaluating Emotion Support Conversations in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiquan Zhao",
      "Lingyu Li",
      "Shisong Chen",
      "Shuqi Kong",
      "Jiaan Wang",
      "Kexin Huang",
      "Tianle Gu",
      "Yixu Wang",
      "Jian Wang",
      "Liang Dandan",
      "Zhixu Li",
      "Yan Teng",
      "Yanghua Xiao",
      "Yingchun Wang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.884": {
    "title": "Cultural Conditioning or Placebo? On the Effectiveness of Socio-Demographic Prompting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sagnik Mukherjee",
      "Muhammad Adilazuarda",
      "Sunayana Sitaram",
      "Kalika Bali",
      "Alham Aji",
      "Monojit Choudhury"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.885": {
    "title": "Text Fluoroscopy: Detecting LLM-Generated Text through Intrinsic Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Yu",
      "Kejiang Chen",
      "Qi Yang",
      "Weiming Zhang",
      "Nenghai Yu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.886": {
    "title": "Hate Personified: Investigating the role of LLMs in content moderation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sarah Masud",
      "Sahajpreet Singh",
      "Viktor Hangya",
      "Alexander Fraser",
      "Tanmoy Chakraborty"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.887": {
    "title": "Temporally Consistent Factuality Probing for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashutosh Bajpai",
      "Aaryan Goyal",
      "Atif Anwer",
      "Tanmoy Chakraborty"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.888": {
    "title": "A Comparison of Language Modeling and Translation as Multilingual Pretraining Objectives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihao Li",
      "Shaoxiong Ji",
      "Timothee Mickus",
      "Vincent Segonne",
      "Jörg Tiedemann"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.889": {
    "title": "Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prasoon Bajpai",
      "Niladri Chatterjee",
      "Subhabrata Dutta",
      "Tanmoy Chakraborty"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.890": {
    "title": "LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tong Zhu",
      "Xiaoye Qu",
      "Daize Dong",
      "Jiacheng Ruan",
      "Jingqi Tong",
      "Conghui He",
      "Yu Cheng"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.891": {
    "title": "Themis: A Reference-free NLG Evaluation Language Model with Flexibility and Interpretability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Hu",
      "Li Lin",
      "Mingqi Gao",
      "Xunjian Yin",
      "Xiaojun Wan"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.892": {
    "title": "Mitigating Training Imbalance in LLM Fine-Tuning via Selective Parameter Merging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Ju",
      "Ziyi Ni",
      "Xingrun Xing",
      "Zhixiong Zeng",
      "Hanyu Zhao",
      "Siqi Fan",
      "Zheng Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.893": {
    "title": "Generating Demonstrations for In-Context Compositional Generalization in Grounded Language Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sam Spilsbury",
      "Pekka Marttinen",
      "Alexander Ilin"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.894": {
    "title": "FAME: Towards Factual Multi-Task Model Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li Zeng",
      "Yingyu Shan",
      "Zeming Liu",
      "Jiashu Yao",
      "Yuhang Guo"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.895": {
    "title": "MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Renjie Pi",
      "Tianyang Han",
      "Jianshu Zhang",
      "Yueqi Xie",
      "Rui Pan",
      "Qing Lian",
      "Hanze Dong",
      "Jipeng Zhang",
      "Tong Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.896": {
    "title": "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Li",
      "Xiaohan Xu",
      "Tao Shen",
      "Can Xu",
      "Jia-Chen Gu",
      "Yuxuan Lai",
      "Chongyang Tao",
      "Shuai Ma"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.897": {
    "title": "InfiniPot: Infinite Context Processing on Memory-Constrained LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minsoo Kim",
      "Kyuhong Shim",
      "Jungwook Choi",
      "Simyung Chang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.898": {
    "title": "VideoCLIP-XL: Advancing Long Description Understanding for Video CLIP Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiapeng Wang",
      "Chengyu Wang",
      "Kunzhe Huang",
      "Jun Huang",
      "Lianwen Jin"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.899": {
    "title": "CorrSynth - A Correlated Sampling Method for Diverse Dataset Generation from LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suhas Kowshik",
      "Abhishek Divekar",
      "Vijit Malik"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.900": {
    "title": "Defining Knowledge: Bridging Epistemology and Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Constanza Fierro",
      "Ruchira Dhar",
      "Filippos Stamatiou",
      "Nicolas Garneau",
      "Anders Søgaard"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.901": {
    "title": "TKGT: Redefinition and A New Way of Text-to-Table Tasks Based on Real World Demands and Knowledge Graphs Augmented LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peiwen Jiang",
      "Xinbo Lin",
      "Zibo Zhao",
      "Ruhui Ma",
      "Yvonne Chen",
      "Jinhua Cheng"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.902": {
    "title": "Free your mouse! Command Large Language Models to Generate Code to Format Word Documents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shihao Rao",
      "Liang Li",
      "Jiapeng Liu",
      "Guan Weixin",
      "Xiyan Gao",
      "Bing Lim",
      "Can Ma"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.903": {
    "title": "CMR Scaling Law: Predicting Critical Mixture Ratios for Continual Pre-training of Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawei Gu",
      "Zacc Yang",
      "Chuanghao Ding",
      "Rui Zhao",
      "Fei Tan"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.904": {
    "title": "The Instinctive Bias: Spurious Images lead to Illusion in MLLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyang Han",
      "Qing Lian",
      "Rui Pan",
      "Renjie Pi",
      "Jipeng Zhang",
      "Shizhe Diao",
      "Yong Lin",
      "Tong Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.905": {
    "title": "Rationale-Aware Answer Verification by Pairwise Self-Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akira Kawabata",
      "Saku Sugawara"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.906": {
    "title": "On the Robustness of Editing Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinbei Ma",
      "Tianjie Ju",
      "Jiyang Qiu",
      "Zhuosheng Zhang",
      "Hai Zhao",
      "Lifeng Liu",
      "Yulong Wang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.907": {
    "title": "IM-BERT: Enhancing Robustness of BERT through the Implicit Euler Method",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "MiHyeon Kim",
      "Juhyoung Park",
      "YoungBin Kim"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.908": {
    "title": "Distract Large Language Models for Automatic Jailbreak Attack",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeguan Xiao",
      "Yan Yang",
      "Guanhua Chen",
      "Yun Chen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.909": {
    "title": "Exploring Space Efficiency in a Tree-based Linear Model for Extreme Multi-label Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "He-Zhe Lin",
      "Cheng-Hung Liu",
      "Chih-Jen Lin"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.910": {
    "title": "WorryWords: Norms of Anxiety Association for over 44k English Words",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saif Mohammad"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.911": {
    "title": "Finding Blind Spots in Evaluator LLMs with Interpretable Checklists",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sumanth Doddapaneni",
      "Mohammed Khan",
      "Sshubam Verma",
      "Mitesh Khapra"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.912": {
    "title": "LONGAGENT: Achieving Question Answering for 128k-Token-Long Documents through Multi-Agent Collaboration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Zhao",
      "Can Zu",
      "Xu Hao",
      "Yi Lu",
      "Wei He",
      "Yiwen Ding",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.913": {
    "title": "AutoPersuade: A Framework for Evaluating and Explaining Persuasive Arguments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Till Saenger",
      "Musashi Hinck",
      "Justin Grimmer",
      "Brandon Stewart"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.914": {
    "title": "Towards Cross-Cultural Machine Translation with Retrieval-Augmented Generation from Multilingual Knowledge Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simone Conia",
      "Daniel Lee",
      "Min Li",
      "Umar Farooq Minhas",
      "Saloni Potdar",
      "Yunyao Li"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.915": {
    "title": "Exploring the Compositional Deficiency of Large Language Models in Mathematical Reasoning Through Trap Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Zhao",
      "Jingqi Tong",
      "Yurong Mou",
      "Ming Zhang",
      "Qi Zhang",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.916": {
    "title": "Scaling Laws for Linear Complexity Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuyang Shen",
      "Dong Li",
      "Ruitao Leng",
      "Zhen Qin",
      "Weigao Sun",
      "Yiran Zhong"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.917": {
    "title": "Autoregressive Multi-trait Essay Scoring via Reinforcement Learning with Scoring-aware Multiple Rewards",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heejin Do",
      "Sangwon Ryu",
      "Gary Lee"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.918": {
    "title": "Intrinsic Self-correction for Enhanced Morality: An Analysis of Internal Mechanisms and the Superficial Hypothesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangliang Liu",
      "Haitao Mao",
      "Jiliang Tang",
      "Kristen Johnson"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.919": {
    "title": "ATAP: Automatic Template-Augmented Commonsense Knowledge Graph Completion via Pre-Trained Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fu Zhang",
      "Yifan Ding",
      "Jingwei Cheng"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.920": {
    "title": "LM2: A Simple Society of Language Models Solves Complex Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gurusha Juneja",
      "Subhabrata Dutta",
      "Tanmoy Chakraborty"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.921": {
    "title": "Towards a Similarity-adjusted Surprisal Theory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Clara Meister",
      "Mario Giulianelli",
      "Tiago Pimentel"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.922": {
    "title": "Multi-Level Information Retrieval Augmented Generation for Knowledge-based Visual Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adjali Omar",
      "Olivier Ferret",
      "Sahar Ghannay",
      "Hervé Le Borgne"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.923": {
    "title": "Can We Trust the Performance Evaluation of Uncertainty Estimation Methods in Text Summarization?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianfeng He",
      "Runing Yang",
      "Linlin Yu",
      "Changbin Li",
      "Ruoxi Jia",
      "Feng Chen",
      "Ming Jin",
      "Chang-Tien Lu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.924": {
    "title": "Is It Really Long Context if All You Need Is Retrieval? Towards Genuinely Difficult Long Context NLP",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Omer Goldman",
      "Alon Jacovi",
      "Aviv Slobodkin",
      "Aviya Maimon",
      "Ido Dagan",
      "Reut Tsarfaty"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.925": {
    "title": "BPE Gets Picky: Efficient Vocabulary Refinement During Tokenizer Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pavel Chizhov",
      "Catherine Arnett",
      "Elizaveta Korotkova",
      "Ivan Yamshchikov"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.926": {
    "title": "SEGMENT+: Long Text Processing with Short-Context Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Shi",
      "Shuang Li",
      "Kerun Yu",
      "Jinglei Chen",
      "Zujie Liang",
      "Xinhui Wu",
      "Yuxi Qian",
      "Feng Wei",
      "Bo Zheng",
      "Jiaqing Liang",
      "Jiangjie Chen",
      "Yanghua Xiao"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.927": {
    "title": "Explicit Memory Learning with Expectation Maximization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhangyue Yin",
      "Qiushi Sun",
      "Qipeng Guo",
      "Zhiyuan Zeng",
      "Qinyuan Cheng",
      "Xipeng Qiu",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.928": {
    "title": "Closing the Loop: Learning to Generate Writing Feedback via Language Model Simulated Student Revisions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Inderjeet Nair",
      "Jiaye Tan",
      "Xiaotian Su",
      "Anne Gere",
      "Xu Wang",
      "Lu Wang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.929": {
    "title": "Small LLMs Are Weak Tool Learners: A Multi-LLM Agent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weizhou Shen",
      "Chenliang Li",
      "Hongzhan Chen",
      "Ming Yan",
      "Xiaojun Quan",
      "Hehong Chen",
      "Ji Zhang",
      "Fei Huang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.930": {
    "title": "Interpreting Context Look-ups in Transformers: Investigating Attention-MLP Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Clement Neo",
      "Shay Cohen",
      "Fazl Barez"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.931": {
    "title": "Still Not Quite There! Evaluating Large Language Models for Comorbid Mental Health Diagnosis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amey Hengle",
      "Atharva Kulkarni",
      "Shantanu Patankar",
      "Madhumitha Chandrasekaran",
      "Sneha D’silva",
      "Jemima Jacob",
      "Rashmi Gupta"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.932": {
    "title": "The Odyssey of Commonsense Causality: From Foundational Benchmarks to Cutting-Edge Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaobo Cui",
      "Zhijing Jin",
      "Bernhard Schölkopf",
      "Boi Faltings"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.933": {
    "title": "Investigating Large Language Models for Complex Word Identification in Multilingual and Multidomain Setups",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Răzvan-Alexandru Smădu",
      "David-Gabriel Ion",
      "Dumitru-Clementin Cercel",
      "Florin Pop",
      "Mihaela-Claudia Cercel"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.934": {
    "title": "Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jia-Chen Gu",
      "Hao-Xiang Xu",
      "Jun-Yu Ma",
      "Pan Lu",
      "Zhen-Hua Ling",
      "Kai-Wei Chang",
      "Nanyun Peng"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.935": {
    "title": "Are Large Language Models In-Context Personalized Summarizers? Get an iCOPERNICUS Test Done!",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Divya Patel",
      "Pathik Patel",
      "Ankush Chander",
      "Sourish Dasgupta",
      "Tanmoy Chakraborty"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.936": {
    "title": "MediTOD: An English Dialogue Dataset for Medical History Taking with Comprehensive Annotations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vishal Saley",
      "Goonjan Saha",
      "Rocktim Das",
      "Dinesh Raghu",
      "Mausam ."
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.937": {
    "title": "***YesBut***: A High-Quality Annotated Multimodal Dataset for evaluating Satire Comprehension capability of Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhilash Nandy",
      "Yash Agarwal",
      "Ashish Patwa",
      "Millon Das",
      "Aman Bansal",
      "Ankit Raj",
      "Pawan Goyal",
      "Niloy Ganguly"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.938": {
    "title": "Working Memory Identifies Reasoning Limits in Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chunhui Zhang",
      "Yiren Jian",
      "Zhongyu Ouyang",
      "Soroush Vosoughi"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.939": {
    "title": "RAFT: Realistic Attacks to Fool Text Detectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "James Wang",
      "Ran Li",
      "Junfeng Yang",
      "Chengzhi Mao"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.940": {
    "title": "LLM-Evolve: Evaluation for LLM's Evolving Capability on Benchmarks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxuan You",
      "Mingjie Liu",
      "Shrimai Prabhumoye",
      "Mostofa Patwary",
      "Mohammad Shoeybi",
      "Bryan Catanzaro"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.941": {
    "title": "FFN-SkipLLM: A Hidden Gem for Autoregressive Decoding with Adaptive Feed Forward Skipping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ajay Jaiswal",
      "Bodun Hu",
      "Lu Yin",
      "Yeonju Ro",
      "Tianlong Chen",
      "Shiwei Liu",
      "Aditya Akella"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.942": {
    "title": "LLM-based Code-Switched Text Generation for Grammatical Error Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tom Potter",
      "Zheng Yuan"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.943": {
    "title": "Deciphering the Interplay of Parametric and Non-parametric Memory in Retrieval-augmented Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mehrdad Farahani",
      "Richard Johansson"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.944": {
    "title": "On Efficient Language and Vision Assistants for Visually-Situated Natural Language Understanding: What Matters in Reading and Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Geewook Kim",
      "Minjoon Seo"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.945": {
    "title": "Community-Cross-Instruct: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihao He",
      "Minh Chu",
      "Rebecca Dorn",
      "Siyi Guo",
      "Kristina Lerman"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.946": {
    "title": "Mathador-LM: A Dynamic Benchmark for Mathematical Reasoning on Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eldar Kurtic",
      "Amir Moeini",
      "Dan Alistarh"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.947": {
    "title": "Reasoning Paths with Reference Objects Elicit Quantitative Spatial Reasoning in Large Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan-Hong Liao",
      "Rafid Mahmood",
      "Sanja Fidler",
      "David Acuna"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.948": {
    "title": "One Thousand and One Pairs: A \"novel\" challenge for long-context language models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marzena Karpinska",
      "Katherine Thai",
      "Kyle Lo",
      "Tanya Goyal",
      "Mohit Iyyer"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.949": {
    "title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tu Vu",
      "Kalpesh Krishna",
      "Salaheddin Alzubi",
      "Chris Tar",
      "Manaal Faruqui",
      "Yun-Hsuan Sung"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.950": {
    "title": "Do LLMs learn a true syntactic universal?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "John Hale",
      "Miloš Stanojević"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.951": {
    "title": "GDPO: Learning to Directly Align Language Models with Diversity Using GFlowNets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oh Joon Kwon",
      "Daiki Matsunaga",
      "Kee-Eung Kim"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.952": {
    "title": "How Susceptible are Large Language Models to Ideological Manipulation?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Chen",
      "Zihao He",
      "Jun Yan",
      "Taiwei Shi",
      "Kristina Lerman"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.953": {
    "title": "Measuring Psychological Depth in Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fabrice Harel-Canada",
      "Hanyu Zhou",
      "Sreya Muppalla",
      "Zeynep Yildiz",
      "Miryung Kim",
      "Amit Sahai",
      "Nanyun Peng"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.954": {
    "title": "Media Attitude Detection via Framing Analysis with Events and their Relations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jin Zhao",
      "Jingxuan Tu",
      "Han Du",
      "Nianwen Xue"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.955": {
    "title": "Fill In The Gaps: Model Calibration and Generalization with Synthetic Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Ba",
      "Michelle Mancenido",
      "Rong Pan"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.956": {
    "title": "Adaptive Question Answering: Enhancing Language Model Proficiency for Addressing Knowledge Conflicts with Source Citations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sagi Shaier",
      "Ari Kobren",
      "Philip Ogren"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.957": {
    "title": "Granular Privacy Control for Geolocation with Vision Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ethan Mendes",
      "Yang Chen",
      "James Hays",
      "Sauvik Das",
      "Wei Xu",
      "Alan Ritter"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.958": {
    "title": "MedReadMe: A Systematic Study for Fine-grained Sentence Readability in Medical Domain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Jiang",
      "Wei Xu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.959": {
    "title": "MemeCLIP: Leveraging CLIP Representations for Multimodal Meme Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siddhant Bikram Shah",
      "Shuvam Shiwakoti",
      "Maheep Chaudhary",
      "Haohan Wang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.960": {
    "title": "FlipGuard: Defending Preference Alignment against Update Regression with Constrained Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingye Zhu",
      "Yi Liu",
      "Quan Wang",
      "Junbo Guo",
      "Zhendong Mao"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.961": {
    "title": "StorySparkQA: Expert-Annotated QA Pairs with Real-World Knowledge for Children's Story-Based Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaju Chen",
      "Yuxuan Lu",
      "Shao Zhang",
      "Bingsheng Yao",
      "Yuanzhe Dong",
      "Ying Xu",
      "Yunyao Li",
      "Qianwen Wang",
      "Dakuo Wang",
      "Yuling Sun"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.962": {
    "title": "MedCoT: Medical Chain of Thought via Hierarchical Expert",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxiang Liu",
      "Yuan Wang",
      "Jiawei Du",
      "Joey Zhou",
      "Zuozhu Liu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.963": {
    "title": "Varying Sentence Representations via Condition-Specified Routers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyong Lin",
      "Quansen Wang",
      "Zixia Jia",
      "Zilong Zheng"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.964": {
    "title": "Inductive-Deductive Strategy Reuse for Multi-Turn Instructional Dialogues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiao Ou",
      "Jiayu Wu",
      "Che Liu",
      "Fuzheng Zhang",
      "Di Zhang",
      "Kun Gai"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.965": {
    "title": "Information Flow Routes: Automatically Interpreting Language Models at Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Javier Ferrando",
      "Elena Voita"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.966": {
    "title": "A Simple yet Effective Training-free Prompt-free Approach to Chinese Spelling Correction Based on Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Houquan Zhou",
      "Zhenghua Li",
      "Bo Zhang",
      "Chen Li",
      "Shaopeng Lai",
      "Ji Zhang",
      "Fei Huang",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.967": {
    "title": "Representational Analysis of Binding in Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qin Dai",
      "Benjamin Heinzerling",
      "Kentaro Inui"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.968": {
    "title": "CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Erxin Yu",
      "Jing Li",
      "Ming Liao",
      "Siqi Wang",
      "Gao Zuchen",
      "Fei Mi",
      "Lanqing Hong"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.969": {
    "title": "ClimRetrieve: A Benchmarking Dataset for Information Retrieval from Corporate Climate Disclosures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tobias Schimanski",
      "Jingwei Ni",
      "Roberto Martín",
      "Nicola Ranger",
      "Markus Leippold"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.970": {
    "title": "Context-Aware Adapter Tuning for Few-Shot Relation Learning in Knowledge Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liu Ran",
      "Zhongzhou Liu",
      "Xiaoli Li",
      "Yuan Fang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.971": {
    "title": "Zero-Shot Detection of LLM-Generated Text using Token Cohesiveness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shixuan Ma",
      "Quan Wang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.972": {
    "title": "Dual-oriented Disentangled Network with Counterfactual Intervention for Multimodal Intent Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhanpeng Chen",
      "Zhihong Zhu",
      "Xianwei Zhuang",
      "Zhiqi Huang",
      "Yuexian Zou"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.973": {
    "title": "From LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyuan Wang",
      "Zhuohan Long",
      "Zhihao Fan",
      "Zhongyu Wei"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.974": {
    "title": "Symbolic Working Memory Enhances Language Models for Complex Rule Application",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyuan Wang",
      "Zhongyu Wei",
      "Yejin Choi",
      "Xiang Ren"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.975": {
    "title": "LLoCO: Learning Long Contexts Offline",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sijun Tan",
      "Xiuyu Li",
      "Shishir G Patil",
      "Ziyang Wu",
      "Tianjun Zhang",
      "Kurt Keutzer",
      "Joseph Gonzalez",
      "Raluca Popa"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.976": {
    "title": "Don't Forget Your Reward Values: Language Model Alignment via Value-based Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Mao",
      "Feng-Lin Li",
      "Huimin Xu",
      "Wei Zhang",
      "Wang Chen",
      "Anh Tuan Luu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.977": {
    "title": "Mentor-KD: Making Small Language Models Better Multi-step Reasoners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hojae Lee",
      "Junho Kim",
      "SangKeun Lee"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.978": {
    "title": "Are Large Language Models Capable of Generating Human-Level Narratives?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufei Tian",
      "Tenghao Huang",
      "Miri Liu",
      "Derek Jiang",
      "Alexander Spangher",
      "Muhao Chen",
      "Jonathan May",
      "Nanyun Peng"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.979": {
    "title": "MP2D: An Automated Topic Shift Dialogue Generation Framework Leveraging Knowledge Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yerin Hwang",
      "Yongil Kim",
      "Yunah Jang",
      "Jeesoo Bang",
      "Hyunkyung Bae",
      "Kyomin Jung"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.980": {
    "title": "Can Large Language Models Enhance Predictions of Disease Progression? Investigating Through Disease Network Link Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haohui Lu",
      "Usman Naseem"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.981": {
    "title": "Searching for Best Practices in Retrieval-Augmented Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaohua Wang",
      "Zhenghua Wang",
      "Xuan Gao",
      "Feiran Zhang",
      "Yixin Wu",
      "Zhibo Xu",
      "Tianyuan Shi",
      "Zhengyuan Wang",
      "Shizheng Li",
      "Qi Qian",
      "Ruicheng Yin",
      "Changze Lv",
      "Xiaoqing Zheng",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.982": {
    "title": "Moral Foundations of Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marwa Abdulhai",
      "Gregory Serapio-García",
      "Clement Crepy",
      "Daria Valter",
      "John Canny",
      "Natasha Jaques"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.983": {
    "title": "The Zeno's Paradox of ‘Low-Resource' Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hellina Hailu Nigatu",
      "Atnafu Tonja",
      "Benjamin Rosman",
      "Thamar Solorio",
      "Monojit Choudhury"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.984": {
    "title": "Knowledge Planning in Large Language Models for Domain-Aligned Counseling Summarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aseem Srivastava",
      "Smriti Joshi",
      "Tanmoy Chakraborty",
      "Md Shad Akhtar"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.985": {
    "title": "Enhancing Post-Hoc Attributions in Long Document Comprehension via Coarse Grained Answer Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pritika Ramu",
      "Koustava Goswami",
      "Apoorv Saxena",
      "Balaji Vasan Srinivasan"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.986": {
    "title": "From Descriptive Richness to Bias: Unveiling the Dark Side of Generative Image Caption Enrichment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yusuke Hirota",
      "Ryo Hachiuma",
      "Chao-Han Yang",
      "Yuta Nakashima"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.987": {
    "title": "Pruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Deyuan Liu",
      "Zhanyue Qin",
      "Hairu Wang",
      "Zhao Yang",
      "Zecheng Wang",
      "Fangying Rong",
      "Qingbin Liu",
      "Yanchao Hao",
      "Bo Li",
      "Xi Chen",
      "Cunhang Fan",
      "Zhao Lv",
      "Dianhui Chu",
      "Zhiying Tu",
      "Dianbo Sui"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.988": {
    "title": "Embedded Named Entity Recognition using Probing Classifiers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicholas Popovic",
      "Michael Färber"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.989": {
    "title": "Unleashing the Power of Emojis in Texts via Self-supervised Graph Pre-Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhou Zhang",
      "Dongzeng Tan",
      "Jiaan Wang",
      "Yilong Chen",
      "Jiarong Xu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.990": {
    "title": "Data Contamination Can Cross Language Barriers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feng Yao",
      "Yufan Zhuang",
      "Zihao Sun",
      "Sunan Xu",
      "Animesh Kumar",
      "Jingbo Shang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.991": {
    "title": "Automated Essay Scoring: A Reflection on the State of the Art",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengjie Li",
      "Vincent Ng"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.992": {
    "title": "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tian Liang",
      "Zhiwei He",
      "Wenxiang Jiao",
      "Xing Wang",
      "Yan Wang",
      "Rui Wang",
      "Yujiu Yang",
      "Shuming Shi",
      "Zhaopeng Tu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.993": {
    "title": "Unveiling and Consulting Core Experts in Retrieval-Augmented MoE-based LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Zhou",
      "Ping Nie",
      "Yiwen Guo",
      "Haojie Wei",
      "Zhanqiu Zhang",
      "Pasquale Minervini",
      "Ruotian Ma",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.994": {
    "title": "CURE: Context- and Uncertainty-Aware Mental Disorder Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Migyeong Kang",
      "Goun Choi",
      "Hyolim Jeon",
      "Ji Hyun An",
      "Daejin Choi",
      "Jinyoung Han"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.995": {
    "title": "PepRec: Progressive Enhancement of Prompting for Recommendation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yakun Yu",
      "Shi-ang Qi",
      "Baochun Li",
      "Di Niu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.996": {
    "title": "In-Context Compositional Generalization for Large Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuanhao Li",
      "Chenchen Jing",
      "Zhen Li",
      "Mingliang Zhai",
      "Yuwei Wu",
      "Yunde Jia"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.997": {
    "title": "Improving Zero-shot LLM Re-Ranker with Risk Minimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaowei Yuan",
      "Zhao Yang",
      "Yequan Wang",
      "Jun Zhao",
      "Kang Liu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.998": {
    "title": "Game on Tree: Visual Hallucination Mitigation via Coarse-to-Fine View Tree and Game Theory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xianwei Zhuang",
      "Zhihong Zhu",
      "Zhanpeng Chen",
      "Yuxin Xie",
      "Liming Liang",
      "Yuexian Zou"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.999": {
    "title": "Label Confidence Weighted Learning for Target-level Sentence Simplification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Ying Qiu",
      "Jingshen Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1000": {
    "title": "Quantum Recurrent Architectures for Text Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenduan Xu",
      "Stephen Clark",
      "Douglas Brown",
      "Gabriel Matos",
      "Konstantinos Meichanetzidis"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1001": {
    "title": "Tree of Problems: Improving structured problem solving with compositionality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Armel Zebaze",
      "Benoît Sagot",
      "Rachel Bawden"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1002": {
    "title": "What the Harm? Quantifying the Tangible Impact of Gender Bias in Machine Translation with a Human-centered Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Beatrice Savoldi",
      "Sara Papi",
      "Matteo Negri",
      "Ana Guerberof-Arenas",
      "Luisa Bentivogli"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1003": {
    "title": "Seg2Act: Global Context-aware Action Generation for Document Logical Structuring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zichao Li",
      "Shaojie He",
      "Meng Liao",
      "Xuanang Chen",
      "Yaojie Lu",
      "Hongyu Lin",
      "Yanxiong Lu",
      "Xianpei Han",
      "Le Sun"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1004": {
    "title": "Is C4 Dataset Optimal for Pruning? An Investigation of Calibration Data for LLM Pruning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhinav Bandari",
      "Lu Yin",
      "Cheng-Yu Hsieh",
      "Ajay Jaiswal",
      "Tianlong Chen",
      "Li Shen",
      "Ranjay Krishna",
      "Shiwei Liu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1005": {
    "title": "Revisiting the Robustness of Watermarking to Paraphrasing Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saksham Rastogi",
      "Danish Pruthi"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1006": {
    "title": "A Survey of Ontology Expansion for Conversational Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinggui Liang",
      "Yuxia Wu",
      "Yuan Fang",
      "Hao Fei",
      "Lizi Liao"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1007": {
    "title": "Calibrating Language Models with Adaptive Temperature Scaling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Johnathan Xie",
      "Annie Chen",
      "Yoonho Lee",
      "Eric Mitchell",
      "Chelsea Finn"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1008": {
    "title": "Which Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fumiya Uchiyama",
      "Takeshi Kojima",
      "Andrew Gambardella",
      "Qi Cao",
      "Yusuke Iwasawa",
      "Yutaka Matsuo"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1009": {
    "title": "Why do objects have many names? A study on word informativeness in language use and lexical systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eleonora Gualdoni",
      "Gemma Boleda"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1010": {
    "title": "Dual-Space Knowledge Distillation for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Songming Zhang",
      "Xue Zhang",
      "Zengkui Sun",
      "Yufeng Chen",
      "Jinan Xu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1011": {
    "title": "NoiseBench: Benchmarking the Impact of Real Label Noise on Named Entity Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elena Merdjanovska",
      "Ansar Aynetdinov",
      "Alan Akbik"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1012": {
    "title": "On the Universal Truthfulness Hyperplane Inside LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junteng Liu",
      "Shiqi Chen",
      "Yu Cheng",
      "Junxian He"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1013": {
    "title": "PairDistill: Pairwise Relevance Distillation for Dense Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao-Wei Huang",
      "Yun-Nung Chen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1014": {
    "title": "User Inference Attacks on Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikhil Kandpal",
      "Krishna Pillutla",
      "Alina Oprea",
      "Peter Kairouz",
      "Christopher Choquette-Choo",
      "Zheng Xu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1015": {
    "title": "HiFT: A Hierarchical Full Parameter Fine-Tuning Strategy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "YongKang Liu",
      "Yiqun Zhang",
      "Qian Li",
      "Tong Liu",
      "Shi Feng",
      "Daling Wang",
      "Yifei Zhang",
      "Hinrich Schuetze"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1016": {
    "title": "Investigating and Mitigating Object Hallucinations in Pretrained Vision-Language (CLIP) Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufang Liu",
      "Tao Ji",
      "Changzhi Sun",
      "Yuanbin Wu",
      "Aimin Zhou"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1017": {
    "title": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in Fine-tuning LLMs for Simultaneous Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew Raffel",
      "Victor Agostinelli",
      "Lizhong Chen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1018": {
    "title": "ToolPlanner: A Tool Augmented LLM for Multi Granularity Instructions with Path Planning and Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qinzhuo Wu",
      "Wei Liu",
      "Jian Luan",
      "Bin Wang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1019": {
    "title": "Please note that I'm just an AI: Analysis of Behavior Patterns of LLMs in (Non-)offensive Speech Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Esra Dönmez",
      "Thang Vu",
      "Agnieszka Falenska"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1020": {
    "title": "How to Compute the Probability of a Word",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tiago Pimentel",
      "Clara Meister"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1021": {
    "title": "A linguistically-motivated evaluation methodology for unraveling model's abilities in reading comprehension tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elie Antoine",
      "Frederic Bechet",
      "Géraldine Damnati",
      "Philippe Langlais"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1022": {
    "title": "GuardBench: A Large-Scale Benchmark for Guardrail Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elias Bassani",
      "Ignacio Sanchez"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1023": {
    "title": "Generate-on-Graph: Treat LLM as both Agent and KG for Incomplete Knowledge Graph Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yao Xu",
      "Shizhu He",
      "Jiabei Chen",
      "Zihao Wang",
      "Yangqiu Song",
      "Hanghang Tong",
      "Guang Liu",
      "Jun Zhao",
      "Kang Liu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1024": {
    "title": "Language models and brains align due to more than next-word prediction and word-level information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gabriele Merlin",
      "Mariya Toneva"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1025": {
    "title": "LLMEdgeRefine: Enhancing Text Clustering with LLM-Based Boundary Point Refinement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijin Feng",
      "Luyang Lin",
      "Lingzhi Wang",
      "Hong Cheng",
      "Kam-Fai Wong"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1026": {
    "title": "CasiMedicos-Arg: A Medical Question Answering Dataset Annotated with Explanatory Argumentative Structures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ekaterina Sviridova",
      "Anar Yeginbergen",
      "Ainara Estarrona",
      "Elena Cabrio",
      "Serena Villata",
      "Rodrigo Agerri"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1027": {
    "title": "A Simple and Effective L_2 Norm-Based Strategy for KV Cache Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alessio Devoto",
      "Yu Zhao",
      "Simone Scardapane",
      "Pasquale Minervini"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1028": {
    "title": "GOME: Grounding-based Metaphor Binding With Conceptual Elaboration For Figurative Language Illustration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linhao Zhang",
      "Jintao Liu",
      "Li Jin",
      "Hao Wang",
      "Kaiwen Wei",
      "Guangluan Xu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1029": {
    "title": "D3CODE: Disentangling Disagreements in Data across Cultures on Offensiveness Detection and Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aida Mostafazadeh Davani",
      "Mark Diaz",
      "Dylan Baker",
      "Vinodkumar Prabhakaran"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1030": {
    "title": "PALM: Few-Shot Prompt Learning for Audio Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Asif Hanif",
      "Maha Agro",
      "Mohammad Qazi",
      "Hanan Aldarmaki"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1031": {
    "title": "Annotator-Centric Active Learning for Subjective NLP Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michiel Van Der Meer",
      "Neele Falk",
      "Pradeep Murukannaiah",
      "Enrico Liscio"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1032": {
    "title": "On the Proper Treatment of Tokenization in Psycholinguistics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mario Giulianelli",
      "Luca Malagutti",
      "Juan Luis Gastaldi",
      "Brian DuSell",
      "Tim Vieira",
      "Ryan Cotterell"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1033": {
    "title": "Enhanced Hallucination Detection in Neural Machine Translation through Simple Detector Aggregation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anas Himmi",
      "Guillaume Staerman",
      "Marine Picot",
      "Pierre Colombo",
      "Nuno Guerreiro"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1034": {
    "title": "Jailbreaking LLMs with Arabic Transliteration and Arabizi",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mansour Ghanim",
      "Saleh Almohaimeed",
      "Mengxin Zheng",
      "Yan Solihin",
      "Qian Lou"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1035": {
    "title": "Who is better at math, Jenny or Jingzhen? Uncovering Stereotypes in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zara Siddique",
      "Liam Turner",
      "Luis Espinosa-Anke"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1036": {
    "title": "Instruction Matters: A Simple yet Effective Task Selection for Optimized Instruction Tuning of Specific Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changho Lee",
      "Janghoon Han",
      "Seonghyeon Ye",
      "Stanley Jungkyu Choi",
      "Honglak Lee",
      "Kyunghoon Bae"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1037": {
    "title": "Recurrent Alignment with Hard Attention for Hierarchical Text Rating",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenxi Lin",
      "Ren Jiayu",
      "Guoxiu He",
      "Zhuoren Jiang",
      "Haiyan Yu",
      "Xiaomin Zhu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1038": {
    "title": "CHESS: Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junhui He",
      "Shangyu Wu",
      "Weidong Wen",
      "Chun Jason Xue",
      "Qingan Li"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1039": {
    "title": "Semformer: Transformer Language Models with Semantic Planning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongjing Yin",
      "Junran Ding",
      "Kai Song",
      "Yue Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1040": {
    "title": "DocCGen: Document-based Controlled Code Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sameer Pimparkhede",
      "Mehant Kammakomati",
      "Srikanth Tamilselvam",
      "Prince Kumar",
      "Ashok Kumar",
      "Pushpak Bhattacharyya"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1041": {
    "title": "Semantics and Sentiment: Cross-lingual Variations in Emoji Use",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giulio Zhou",
      "Sydelle De Souza",
      "Ella Markham",
      "Oghenetekevwe Kwakpovwe",
      "Sumin Zhao"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1042": {
    "title": "The Emergence of Compositional Languages in Multi-entity Referential Games: from Image to Graph Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Akkerman",
      "Phong Le",
      "Raquel Alhama"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1043": {
    "title": "Transformers are Multi-State RNNs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matanel Oren",
      "Michael Hassid",
      "Nir Yarden",
      "Yossi Adi",
      "Roy Schwartz"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1044": {
    "title": "Evaluating Large Language Models along Dimensions of Language Variation: A Systematik Invesdigatiom uv Cross-lingual Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Niyati Bafna",
      "Kenton Murray",
      "David Yarowsky"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1045": {
    "title": "Fuse to Forget: Bias Reduction and Selective Memorization through Model Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kerem Zaman",
      "Leshem Choshen",
      "Shashank Srivastava"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1046": {
    "title": "Collective Critics for Creative Story Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minwook Bae",
      "Hyounghun Kim"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1047": {
    "title": "Surprise! Uniform Information Density Isn't the Whole Story: Predicting Surprisal Contours in Long-form Discourse",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eleftheria Tsipidi",
      "Franz Nowak",
      "Ryan Cotterell",
      "Ethan Wilcox",
      "Mario Giulianelli",
      "Alex Warstadt"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1048": {
    "title": "Model-based Preference Optimization in Abstractive Summarization without Human Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaepill Choi",
      "Kyubyung Chae",
      "Jiwoo Song",
      "Yohan Jo",
      "Taesup Kim"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1049": {
    "title": "Are Data Augmentation Methods in Named Entity Recognition Applicable for Uncertainty Estimation?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wataru Hashimoto",
      "Hidetaka Kamigaito",
      "Taro Watanabe"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1050": {
    "title": "NeuroTrialNER: An Annotated Corpus for Neurological Diseases and Therapies in Clinical Trial Registries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simona Doneva",
      "Tilia Ellendorff",
      "Beate Sick",
      "Jean-Philippe Goldman",
      "Amelia Cannon",
      "Gerold Schneider",
      "Benjamin Ineichen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1051": {
    "title": "Fool Me Once? Contrasting Textual and Visual Explanations in a Clinical Decision-Support Setting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maxime Kayser",
      "Bayar Menzat",
      "Cornelius Emde",
      "Bogdan Bercean",
      "Alex Novak",
      "Abdalá Morgado",
      "Bartlomiej Papiez",
      "Susanne Gaube",
      "Thomas Lukasiewicz",
      "Oana-Maria Camburu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1052": {
    "title": "Towards Faithful Knowledge Graph Explanation Through Deep Alignment in Commonsense Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weihe Zhai",
      "Arkaitz Zubiaga",
      "Bingquan Liu",
      "Chengjie Sun",
      "Yalong Zhao"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1053": {
    "title": "Generation with Dynamic Vocabulary",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanting Liu",
      "Tao Ji",
      "Changzhi Sun",
      "Yuanbin Wu",
      "Xiaoling Wang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1054": {
    "title": "Argument Relation Classification through Discourse Markers and Adversarial Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michele Contalbo",
      "Francesco Guerra",
      "Matteo Paganelli"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1055": {
    "title": "Getting The Most Out of Your Training Data: Exploring Unsupervised Tasks for Morphological Inflection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhishek Purushothama",
      "Adam Wiemerslage",
      "Katharina Wense"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1056": {
    "title": "Link, Synthesize, Retrieve: Universal Document Linking for Zero-Shot Information Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dae Yon Hwang",
      "Bilal Taha",
      "Harshit Pande",
      "Yaroslav Nechaev"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1057": {
    "title": "Efficient Unseen Language Adaptation for Multilingual Pre-Trained Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Po-Heng Chen",
      "Yun-Nung Chen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1058": {
    "title": "Prove Your Point!: Bringing Proof-Enhancement Principles to Argumentative Essay Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiyu Xiao",
      "Lei Wu",
      "Yuhang Gou",
      "Weinan Zhang",
      "Ting Liu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1059": {
    "title": "TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kate Sanders",
      "Nathaniel Weir",
      "Benjamin Van Durme"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1060": {
    "title": "Unsupervised Extraction of Dialogue Policies from Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Makesh Narsimhan Sreedhar",
      "Traian Rebedea",
      "Christopher Parisien"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1061": {
    "title": "GRIZAL: Generative Prior-guided Zero-Shot Temporal Action Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Onkar Susladkar",
      "Gayatri Deshmukh",
      "Vandan Gorade",
      "Sparsh Mittal"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1062": {
    "title": "Preserving Multi-Modal Capabilities of Pre-trained VLMs for Improving Vision-Linguistic Compositionality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youngtaek Oh",
      "Jae Won Cho",
      "Dong-Jin Kim",
      "In So Kweon",
      "Junmo Kim"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1063": {
    "title": "FoodieQA: A Multimodal Dataset for Fine-Grained Understanding of Chinese Food Culture",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenyan Li",
      "Crystina Zhang",
      "Jiaang Li",
      "Qiwei Peng",
      "Raphael Tang",
      "Li Zhou",
      "Weijia Zhang",
      "Guimin Hu",
      "Yifei Yuan",
      "Anders Søgaard",
      "Daniel Hershcovich",
      "Desmond Elliott"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1064": {
    "title": "A Two-Step Approach for Data-Efficient French Pronunciation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hoyeon Lee",
      "Hyeeun Jang",
      "Jonghwan Kim",
      "Jaemin Kim"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1065": {
    "title": "Exploring Intra and Inter-language Consistency in Embeddings with ICA",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rongzhi Li",
      "Takeru Matsuda",
      "Hitomi Yanaka"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1066": {
    "title": "DetoxLLM: A Framework for Detoxification with Explanations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Tawkat Islam Khondaker",
      "Muhammad Abdul-Mageed",
      "Laks Lakshmanan"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1067": {
    "title": "Comparing a BERT Classifier and a GPT classifier for Detecting Connective Language Across Multiple Social Media",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Josephine Lukito",
      "Bin Chen",
      "Gina Masullo",
      "Natalie Stroud"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1068": {
    "title": "ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yash Akhauri",
      "Ahmed AbouElhamayed",
      "Jordan Dotzel",
      "Zhiru Zhang",
      "Alexander Rush",
      "Safeen Huda",
      "Mohamed Abdelfattah"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1069": {
    "title": "Emotion Granularity from Text: An Aggregate-Level Indicator of Mental Health",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Krishnapriya Vishnubhotla",
      "Daniela Teodorescu",
      "Mallory Feldman",
      "Kristen Lindquist",
      "Saif Mohammad"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1070": {
    "title": "BLSP-Emo: Towards Empathetic Large Speech-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Wang",
      "Minpeng Liao",
      "Zhongqiang Huang",
      "Junhong Wu",
      "Chengqing Zong",
      "Jiajun Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1071": {
    "title": "SynthesizRR: Generating Diverse Datasets with Retrieval Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhishek Divekar",
      "Greg Durrett"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1072": {
    "title": "Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenqi Zhang",
      "Zhenglin Cheng",
      "Yuanyu He",
      "Mengna Wang",
      "Yongliang Shen",
      "Zeqi Tan",
      "Guiyang Hou",
      "Mingqian He",
      "Yanna Ma",
      "Weiming Lu",
      "Yueting Zhuang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1073": {
    "title": "DataNarrative: Automated Data-Driven Storytelling with Visualizations and Texts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammed Saidul Islam",
      "Md Tahmid Rahman Laskar",
      "Md Rizwan Parvez",
      "Enamul Hoque",
      "Shafiq Joty"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1074": {
    "title": "DEM: Distribution Edited Model for Training with Mixed Data Distributions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dhananjay Ram",
      "Aditya Rawal",
      "Momchil Hardalov",
      "Nikolaos Pappas",
      "Sheng Zha"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1075": {
    "title": "Altogether: Image Captioning via Re-aligning Alt-text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hu Xu",
      "Po-Yao Huang",
      "Xiaoqing Tan",
      "Ching-Feng Yeh",
      "Jacob Kahn",
      "Christine Jou",
      "Gargi Ghosh",
      "Omer Levy",
      "Luke Zettlemoyer",
      "Wen-tau Yih",
      "Shang-Wen Li",
      "Saining Xie",
      "Christoph Feichtenhofer"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1076": {
    "title": "VerifyMatch: A Semi-Supervised Learning Paradigm for Natural Language Inference with Confidence-Aware MixUp",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seo Yeon Park",
      "Cornelia Caragea"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1077": {
    "title": "CaT-Bench: Benchmarking Language Model Understanding of Causal and Temporal Dependencies in Plans",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yash Kumar Lal",
      "Vanya Cohen",
      "Nathanael Chambers",
      "Niranjan Balasubramanian",
      "Ray Mooney"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1078": {
    "title": "Mitigating the Impact of Reference Quality on Evaluation of Summarization Systems with Reference-Free Metrics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Théo Gigant",
      "Camille Guinaudeau",
      "Marc Decombas",
      "Frederic Dufaux"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1079": {
    "title": "An Empirical Analysis of the Writing Styles of Persona-Assigned LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manuj Malik",
      "Jing Jiang",
      "Kian Ming Chai"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1080": {
    "title": "Investigating the Role of Instruction Variety and Task Difficulty in Robotic Manipulation Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amit Parekh",
      "Nikolas Vitsakis",
      "Alessandro Suglia",
      "Ioannis Konstas"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1081": {
    "title": "GPT vs RETRO: Exploring the Intersection of Retrieval and Parameter-Efficient Fine-Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aleksander Ficek",
      "Jiaqi Zeng",
      "Oleksii Kuchaiev"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1082": {
    "title": "CoCoST: Automatic Complex Code Generation with Online Searching and Correctness Testing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyi He",
      "Jiaru Zou",
      "Yun Lin",
      "Mengyu Zhou",
      "Shi Han",
      "Zejian Yuan",
      "Dongmei Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1083": {
    "title": "Sequential API Function Calling Using GraphQL Schema",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Avirup Saha",
      "Lakshmi Mandal",
      "Balaji Ganesan",
      "Sambit Ghosh",
      "Renuka Sindhgatta",
      "Carlos Eberhardt",
      "Dan Debrunner",
      "Sameep Mehta"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1084": {
    "title": "The Illusion of Competence: Evaluating the Effect of Explanations on Users' Mental Models of Visual Question Answering Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Judith Sieker",
      "Simeon Junker",
      "Ronja Utescher",
      "Nazia Attari",
      "Heiko Wersing",
      "Hendrik Buschmeier",
      "Sina Zarrieß"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1085": {
    "title": "Re-Evaluating Evaluation for Multilingual Summarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jessica Forde",
      "Ruochen Zhang",
      "Lintang Sutawika",
      "Alham Aji",
      "Samuel Cahyawijaya",
      "Genta Winata",
      "Minghao Wu",
      "Carsten Eickhoff",
      "Stella Biderman",
      "Ellie Pavlick"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1086": {
    "title": "Video-Text Prompting for Weakly Supervised Spatio-Temporal Video Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heng Zhao",
      "Zhao Yinjie",
      "Bihan Wen",
      "Yew-Soon Ong",
      "Joey Zhou"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1087": {
    "title": "A Fast and Sound Tagging Method for Discontinuous Named-Entity Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Caio Corro"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1088": {
    "title": "Factuality of Large Language Models: A Survey",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxia Wang",
      "Minghan Wang",
      "Muhammad Arslan Manzoor",
      "Fei Liu",
      "Georgi Georgiev",
      "Rocktim Das",
      "Preslav Nakov"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1089": {
    "title": "Discovering Biases in Information Retrieval Models Using Relevance Thesaurus as Global Explanation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youngwoo Kim",
      "Razieh Rahimi",
      "James Allan"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1090": {
    "title": "Adaptable Moral Stances of Large Language Models on Sexist Content: Implications for Society and Gender Discourse",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rongchen Guo",
      "Isar Nejadgholi",
      "Hillary Dawkins",
      "Kathleen Fraser",
      "Svetlana Kiritchenko"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1091": {
    "title": "DISCERN: Decoding Systematic Errors in Natural Language for Text Classifiers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rakesh R Menon",
      "Shashank Srivastava"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1092": {
    "title": "IntCoOp: Interpretability-Aware Vision-Language Prompt Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soumya Suvra Ghosal",
      "Samyadeep Basu",
      "Soheil Feizi",
      "Dinesh Manocha"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1093": {
    "title": "Scope-enhanced Compositional Semantic Parsing for DRT",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiulin Yang",
      "Jonas Groschwitz",
      "Alexander Koller",
      "Johan Bos"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1094": {
    "title": "The Generation Gap: Exploring Age Bias in the Value Systems of Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyang Liu",
      "Trisha Maturi",
      "Bowen Yi",
      "Siqi Shen",
      "Rada Mihalcea"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1095": {
    "title": "TempoFormer: A Transformer for Temporally-aware Representations in Change Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Talia Tseriotou",
      "Adam Tsakalidis",
      "Maria Liakata"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1096": {
    "title": "Pron vs Prompt: Can Large Language Models already Challenge a World-Class Fiction Author at Creative Text Writing?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guillermo Marco",
      "Julio Gonzalo",
      "M.Teresa Mateo-Girona",
      "Ramón Santos"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1097": {
    "title": "Evaluating Diversity in Automatic Poetry Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanran Chen",
      "Hannes Gröner",
      "Sina Zarrieß",
      "Steffen Eger"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1098": {
    "title": "Evaluating Short-Term Temporal Fluctuations of Social Biases in Social Media Data and Masked Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Zhou",
      "Danushka Bollegala",
      "Jose Camacho-Collados"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1099": {
    "title": "Delving into Qualitative Implications of Synthetic Data for Hate Speech Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Camilla Casula",
      "Sebastiano Salto",
      "Alan Ramponi",
      "Sara Tonelli"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1100": {
    "title": "Grounding Language in Multi-Perspective Referential Communication",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zineng Tang",
      "Lingjun Mao",
      "Alane Suhr"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1101": {
    "title": "Threshold-driven Pruning with Segmented Maximum Term Weights for Approximate Cluster-based Sparse Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Qiao",
      "Parker Carlson",
      "Shanxiu He",
      "Yingrui Yang",
      "Tao Yang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1102": {
    "title": "Error Analysis of Multilingual Language Models in Machine Translation: A Case Study of English-Amharic Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hizkiel Alemayehu",
      "Hamada Zahera",
      "Axel-Cyrille Ngonga Ngomo"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1103": {
    "title": "MIPD: Exploring Manipulation and Intention In a Novel Corpus of Polish Disinformation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arkadiusz Modzelewski",
      "Giovanni Da San Martino",
      "Pavel Savov",
      "Magdalena Wilczyńska",
      "Adam Wierzbicki"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1104": {
    "title": "Unsupervised Discrete Representations of American Sign Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Artem Abzaliev",
      "Rada Mihalcea"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1105": {
    "title": "Perceptions to Beliefs: Exploring Precursory Inferences for Theory of Mind in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chani Jung",
      "Dongkwan Kim",
      "Jiho Jin",
      "Jiseon Kim",
      "Yeon Seonwoo",
      "Yejin Choi",
      "Alice Oh",
      "Hyunwoo Kim"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1106": {
    "title": "Towards Enhancing Coherence in Extractive Summarization: Dataset and Experiments with LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mihir Parmar",
      "Hanieh Deilamsalehy",
      "Franck Dernoncourt",
      "Seunghyun Yoon",
      "Ryan Rossi",
      "Trung Bui"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1107": {
    "title": "Jump Starting Bandits with LLM-Generated Prior Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Parand Alamdari",
      "Yanshuai Cao",
      "Kevin Wilson"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1108": {
    "title": "Adaptation Odyssey in LLMs: Why Does Additional Pretraining Sometimes Fail to Improve?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fırat Öncel",
      "Matthias Bethge",
      "Beyza Ermis",
      "Mirco Ravanelli",
      "Cem Subakan",
      "Çağatay Yıldız"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1109": {
    "title": "Not All Contexts Are Equal: Teaching LLMs Credibility-aware Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruotong Pan",
      "Boxi Cao",
      "Hongyu Lin",
      "Xianpei Han",
      "Jia Zheng",
      "Sirui Wang",
      "Xunliang Cai",
      "Le Sun"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1110": {
    "title": "Virtual Personas for Language Models via an Anthology of Backstories",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suhong Moon",
      "Marwa Abdulhai",
      "Minwoo Kang",
      "Joseph Suh",
      "Widyadewi Soedarmadji",
      "Eran Behar",
      "David Chan"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1111": {
    "title": "Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nemika Tyagi",
      "Mihir Parmar",
      "Mohith Kulkarni",
      "Aswin Rrv",
      "Nisarg Patel",
      "Mutsumi Nakamura",
      "Arindam Mitra",
      "Chitta Baral"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1112": {
    "title": "Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junlin Wang",
      "Siddhartha Jain",
      "Dejiao Zhang",
      "Baishakhi Ray",
      "Varun Kumar",
      "Ben Athiwaratkun"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1113": {
    "title": "The Empirical Variability of Narrative Perceptions of Social Media Texts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joel Mire",
      "Maria Antoniak",
      "Elliott Ash",
      "Andrew Piper",
      "Maarten Sap"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1114": {
    "title": "Which questions should I answer? Salience Prediction of Inquisitive Questions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yating Wu",
      "Ritika Mangla",
      "Alex Dimakis",
      "Greg Durrett",
      "Junyi Jessy Li"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1115": {
    "title": "Revealing Personality Traits: A New Benchmark Dataset for Explainable Personality Recognition on Dialogues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Sun",
      "Jinming Zhao",
      "Qin Jin"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1116": {
    "title": "Continual Test-time Adaptation for End-to-end Speech Recognition on Noisy Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guan-Ting Lin",
      "Wei Huang",
      "Hung-yi Lee"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1117": {
    "title": "Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sachit Menon",
      "Richard Zemel",
      "Carl Vondrick"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1118": {
    "title": "CodeJudge: Evaluating Code Generation with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weixi Tong",
      "Tianyi Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1119": {
    "title": "Self-Training Large Language and Vision Assistant for Medical Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guohao Sun",
      "Can Qin",
      "Huazhu Fu",
      "Linwei Wang",
      "Zhiqiang Tao"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1120": {
    "title": "SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prakamya Mishra",
      "Zonghai Yao",
      "Parth Vashisht",
      "Feiyun",
      "Beining Wang",
      "Vidhi Mody",
      "Hong Yu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1121": {
    "title": "Defending Jailbreak Prompts via In-Context Adversarial Game",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujun Zhou",
      "Yufei Han",
      "Haomin Zhuang",
      "Kehan Guo",
      "Zhenwen Liang",
      "Hongyan Bao",
      "Xiangliang Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1122": {
    "title": "Detecting Online Community Practices with Large Language Models: A Case Study of Pro-Ukrainian Publics on Twitter",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kateryna Kasianenko",
      "Shima Khanehzar",
      "Stephen Wan",
      "Ehsan Dehghan",
      "Axel Bruns"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1123": {
    "title": "Multilingual Topic Classification in X: Dataset and Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dimosthenis Antypas",
      "Asahi Ushio",
      "Francesco Barbieri",
      "Jose Camacho-Collados"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1124": {
    "title": "MT-Eval: A Multi-Turn Capabilities Evaluation Benchmark for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wai-Chung Kwan",
      "Xingshan Zeng",
      "Yuxin Jiang",
      "Yufei Wang",
      "Liangyou Li",
      "Lifeng Shang",
      "Xin Jiang",
      "Qun Liu",
      "Kam-Fai Wong"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1125": {
    "title": "Updating CLIP to Prefer Descriptions Over Captions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amir Zur",
      "Elisa Kreiss",
      "Karel D’Oosterlinck",
      "Christopher Potts",
      "Atticus Geiger"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1126": {
    "title": "CmdCaliper: A Semantic-Aware Command-Line Embedding Model and Dataset for Security Research",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sian-Yao Huang",
      "Cheng-Lin Yang",
      "Che-Yu Lin",
      "Chun-Ying Huang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1127": {
    "title": "Back to School: Translation Using Grammar Books",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathan Hus",
      "Antonios Anastasopoulos"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1128": {
    "title": "VIEWS: Entity-Aware News Video Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hammad Ayyubi",
      "Tianqi Liu",
      "Arsha Nagrani",
      "Xudong Lin",
      "Mingda Zhang",
      "Anurag Arnab",
      "Feng Han",
      "Yukun Zhu",
      "Xuande Feng",
      "Kevin Zhang",
      "Jialu Liu",
      "Shih-Fu Chang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1129": {
    "title": "Towards Aligning Language Models with Textual Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saüc Lloret",
      "Shehzaad Dhuliawala",
      "Keerthiram Murugesan",
      "Mrinmaya Sachan"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1130": {
    "title": "AMPO: Automatic Multi-Branched Prompt Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng Yang",
      "Yurong Wu",
      "Yan Gao",
      "Zineng Zhou",
      "Bin Zhu",
      "Xiaodi Sun",
      "Jian-Guang Lou",
      "Zhiming Ding",
      "Anbang Hu",
      "Yuan Fang",
      "Yunsong Li",
      "Junyan Chen",
      "Linjun Yang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1131": {
    "title": "DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinglin Lyu",
      "Junhui Li",
      "Yanqing Zhao",
      "Min Zhang",
      "Daimeng Wei",
      "Shimin Tao",
      "Hao Yang",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1132": {
    "title": "DEFT-UCS: Data Efficient Fine-Tuning for Pre-Trained Language Models via Unsupervised Core-Set Selection for Text-Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Devleena Das",
      "Vivek Khetan"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1133": {
    "title": "Unveiling Multi-level and Multi-modal Semantic Representations in the Human Brain using Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuko Nakagi",
      "Takuya Matsuyama",
      "Naoko Koide-Majima",
      "Hiroto Yamaguchi",
      "Rieko Kubo",
      "Shinji Nishimoto",
      "Yu Takagi"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1134": {
    "title": "They are uncultured\": Unveiling Covert Harms and Social Threats in LLM Generated Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Preetam Prabhu Srikar Dammu",
      "Hayoung Jung",
      "Anjali Singh",
      "Monojit Choudhury",
      "Tanu Mitra"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1135": {
    "title": "Multi-expert Prompting Improves Reliability, Safety and Usefulness of Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Do Long",
      "Duong Yen",
      "Anh Tuan Luu",
      "Kenji Kawaguchi",
      "Min-Yen Kan",
      "Nancy Chen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1136": {
    "title": "Will LLMs Replace the Encoder-Only Models in Temporal Relation Classification?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gabriel Roccabruna",
      "Massimo Rizzoli",
      "Giuseppe Riccardi"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1137": {
    "title": "Eliciting In-Context Learning in Vision-Language Models for Videos Through Curated Data Distributional Properties",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keunwoo Yu",
      "Zheyuan Zhang",
      "Fengyuan Hu",
      "Shane Storks",
      "Joyce Chai"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1138": {
    "title": "Waterfall: Scalable Framework for Robust Text Watermarking and Provenance for LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gregory Kang Ruey Lau",
      "Xinyuan Niu",
      "Hieu Dao",
      "Jiangwei Chen",
      "Chuan-Sheng Foo",
      "Bryan Kian Hsiang Low"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1139": {
    "title": "MASIVE: Open-Ended Affective State Identification in English and Spanish",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicholas Deas",
      "Elsbeth Turcan",
      "Ivan Mejia",
      "Kathleen McKeown"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1140": {
    "title": "You Make me Feel like a Natural Question: Training QA Systems on Transformed Trivia Questions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tasnim Kabir",
      "Yoo Yeon Sung",
      "Saptarashmi Bandyopadhyay",
      "Hao Zou",
      "Abhranil Chandra",
      "Jordan Boyd-Graber"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1141": {
    "title": "AlphaLoRA: Assigning LoRA Experts Based on Layer Training Quality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peijun Qing",
      "Chongyang Gao",
      "Yefan Zhou",
      "Xingjian Diao",
      "Yaoqing Yang",
      "Soroush Vosoughi"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1142": {
    "title": "Flee the Flaw: Annotating the Underlying Logic of Fallacious Arguments Through Templates and Slot-filling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Irfan Robbani",
      "Paul Reisert",
      "Surawat Pothong",
      "Naoya Inoue",
      "Camélia Guerraoui",
      "Wenzhi Wang",
      "Shoichi Naito",
      "Jungmin Choi",
      "Kentaro Inui"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1143": {
    "title": "Advancing Social Intelligence in AI Agents: Technical Challenges and Open Questions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leena Mathur",
      "Paul Pu Liang",
      "Louis-Philippe Morency"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1144": {
    "title": "RAt: Injecting Implicit Bias for Text-To-Image Prompt Refinement Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyi Kou",
      "Shichao Pei",
      "Meng Jiang",
      "Xiangliang Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1145": {
    "title": "Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rifki Putri",
      "Faiz Haznitrama",
      "Dea Adhista",
      "Alice Oh"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1146": {
    "title": "Can Language Models Induce Grammatical Knowledge from Indirect Evidence?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miyu Oba",
      "Yohei Oseki",
      "Akiyo Fukatsu",
      "Akari Haga",
      "Hiroki Ouchi",
      "Taro Watanabe",
      "Saku Sugawara"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1147": {
    "title": "Do LLMs Know to Respect Copyright Notice?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jialiang Xu",
      "Shenglan Li",
      "Zhaozhuo Xu",
      "Denghui Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1148": {
    "title": "SpecHub: Provable Acceleration to Multi-Draft Speculative Decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryan Sun",
      "Tianyi Zhou",
      "Xun Chen",
      "Lichao Sun"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1149": {
    "title": "Interventional Speech Noise Injection for ASR Generalizable Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "YeonJoon Jung",
      "Jaeseong Lee",
      "Seungtaek Choi",
      "Dohyeon Lee",
      "Minsoo Kim",
      "Seung-won Hwang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1150": {
    "title": "Rethinking the Role of Proxy Rewards in Language Model Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sungdong Kim",
      "Minjoon Seo"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1151": {
    "title": "Visual Text Matters: Improving Text-KVQA with Visual Text Entity Knowledge-aware Large Multimodal Assistant",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhirama Penamakuri",
      "Anand Mishra"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1152": {
    "title": "Beyond Correlation: Interpretable Evaluation of Machine Translation Metrics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefano Perrella",
      "Lorenzo Proietti",
      "Pere-Lluís Huguet Cabot",
      "Edoardo Barba",
      "Roberto Navigli"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1153": {
    "title": "IFCap: Image-like Retrieval and Frequency-based Entity Filtering for Zero-shot Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soeun Lee",
      "Si-Woo Kim",
      "Taewhan Kim",
      "Dong-Jin Kim"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1154": {
    "title": "Encoding Spreadsheets for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Dong",
      "Jianbo Zhao",
      "Yuzhang Tian",
      "Junyu Xiong",
      "Mengyu Zhou",
      "Yun Lin",
      "José Cambronero",
      "Yeye He",
      "Shi Han",
      "Dongmei Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1155": {
    "title": "Let's discuss! Quality Dimensions and Annotated Datasets for Computational Argument Quality Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rositsa Ivanova",
      "Thomas Huber",
      "Christina Niklaus"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1156": {
    "title": "Automatic sentence segmentation of clinical record narratives in real-world data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongfang Xu",
      "Davy Weissenbacher",
      "Karen O’Connor",
      "Siddharth Rawal",
      "Graciela Hernandez"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1157": {
    "title": "One-to-Many Communication and Compositionality in Emergent Communication",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heeyoung Lee"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1158": {
    "title": "Bayesian Example Selection Improves In-Context Learning for Speech, Text and Visual Modalities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyin Wang",
      "Chao-Han Yang",
      "Ji Wu",
      "Chao Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1159": {
    "title": "Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Weber",
      "Klaudia Thellmann",
      "Jan Ebert",
      "Nicolas Flores-Herr",
      "Jens Lehmann",
      "Michael Fromm",
      "Mehdi Ali"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1160": {
    "title": "Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nisarg Patel",
      "Mohith Kulkarni",
      "Mihir Parmar",
      "Aashna Budhiraja",
      "Mutsumi Nakamura",
      "Neeraj Varshney",
      "Chitta Baral"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1161": {
    "title": "Linear Layer Extrapolation for Fine-Grained Emotion Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mayukh Sharma",
      "Sean O’Brien",
      "Julian McAuley"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1162": {
    "title": "Task Oriented In-Domain Data Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Liang",
      "Xinyu Hu",
      "Simiao Zuo",
      "Yeyun Gong",
      "Qiang Lou",
      "Yi Liu",
      "Shao-Lun Huang",
      "Jian Jiao"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1163": {
    "title": "SciDQA: A Deep Reading Comprehension Dataset over Scientific Papers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shruti Singh",
      "Nandan Sarkar",
      "Arman Cohan"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1164": {
    "title": "Mixture-of-Modules: Reinventing Transformers as Dynamic Assemblies of Modules",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuocheng Gong",
      "Ang Lv",
      "Jian Guan",
      "Wei Wu",
      "Huishuai Zhang",
      "Minlie Huang",
      "Dongyan Zhao",
      "Rui Yan"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1165": {
    "title": "No Culture Left Behind: ArtELingo-28, a Benchmark of WikiArt with Captions in 28 Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youssef Mohamed",
      "Runjia Li",
      "Ibrahim Ahmad",
      "Kilichbek Haydarov",
      "Philip Torr",
      "Kenneth Church",
      "Mohamed Elhoseiny"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1166": {
    "title": "PREDICT: Multi-Agent-based Debate Simulation for Generalized Hate Speech Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Someen Park",
      "Jaehoon Kim",
      "Seungwan Jin",
      "Sohyun Park",
      "Kyungsik Han"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1167": {
    "title": "TokenVerse: Towards Unifying Speech and NLP Tasks via Transducer-based ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shashi Kumar",
      "Srikanth Madikeri",
      "Juan Pablo Zuluaga Gomez",
      "Iuliia Thorbecke",
      "Esaú Villatoro-tello",
      "Sergio Burdisso",
      "Petr Motlicek",
      "Karthik S",
      "Aravind Ganapathiraju"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1168": {
    "title": "ApiQ: Finetuning of 2-Bit Quantized Large Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baohao Liao",
      "Christian Herold",
      "Shahram Khadivi",
      "Christof Monz"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1169": {
    "title": "Memorize Step by Step: Efficient Long-Context Prefilling with Incremental Memory and Decremental Chunk",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan Zeng",
      "Qipeng Guo",
      "Xiaoran Liu",
      "Zhangyue Yin",
      "Wentao Shu",
      "Mianqiu Huang",
      "Bo Wang",
      "Yunhua Zhou",
      "Linlin Li",
      "Qun Liu",
      "Xipeng Qiu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1170": {
    "title": "A Morphology-Based Investigation of Positional Encodings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Poulami Ghosh",
      "Shikhar Vashishth",
      "Raj Dabre",
      "Pushpak Bhattacharyya"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1171": {
    "title": "I love pineapple on pizza != I hate pineapple on pizza: Stance-Aware Sentence Transformers for Opinion Mining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vahid Ghafouri",
      "Jose Such",
      "Guillermo Suarez-Tangil"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1172": {
    "title": "BiasWipe: Mitigating Unintended Bias in Text Classifiers through Model Interpretability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mamta Mamta",
      "Rishikant Chigrupaatii",
      "Asif Ekbal"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1173": {
    "title": "ArMeme: Propagandistic Content in Arabic Memes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Firoj Alam",
      "Abul Hasnat",
      "Fatema Ahmad",
      "Md. Arid Hasan",
      "Maram Hasanain"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1174": {
    "title": "Language is Scary when Over-Analyzed: Unpacking Implied Misogynistic Reasoning with Argumentation Theory-Driven Prompts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arianna Muti",
      "Federico Ruggeri",
      "Khalid Khatib",
      "Alberto Barrón-Cedeño",
      "Tommaso Caselli"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1175": {
    "title": "Thoughts to Target: Enhance Planning for Target-driven Conversation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhonghua Zheng",
      "Lizi Liao",
      "Yang Deng",
      "Ee-Peng Lim",
      "Minlie Huang",
      "Liqiang Nie"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1176": {
    "title": "Scalable Data Ablation Approximations for Language Models through Modular Training and Merging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Clara Na",
      "Ian Magnusson",
      "Ananya Harsh Jha",
      "Tom Sherborne",
      "Emma Strubell",
      "Jesse Dodge",
      "Pradeep Dasigi"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1177": {
    "title": "Exploring Intrinsic Language-specific Subspaces in Fine-tuning Multilingual Neural Machine Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhe Cao",
      "Zhi Qu",
      "Hidetaka Kamigaito",
      "Taro Watanabe"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1178": {
    "title": "Attention Score is not All You Need for Token Importance Indicator in KV Cache Reduction: Value Also Matters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyu Guo",
      "Hidetaka Kamigaito",
      "Taro Watanabe"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1179": {
    "title": "Generative Subgraph Retrieval for Knowledge Graph–Grounded Dialog Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinyoung Park",
      "Minseok Joo",
      "Joo-Kyung Kim",
      "Hyunwoo Kim"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1180": {
    "title": "Adapters Mixup: Mixing Parameter-Efficient Adapters to Enhance the Adversarial Robustness of Fine-tuned Pre-trained Text Classifiers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tuc Nguyen",
      "Thai Le"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1181": {
    "title": "Generalizing Clinical De-identification Models by Privacy-safe Data Augmentation using GPT-4",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Woojin Kim",
      "Sungeun Hahm",
      "Jaejin Lee"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1182": {
    "title": "Connecting the Dots: Evaluating Abstract Reasoning Capabilities of LLMs Using the New York Times Connections Word Game",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prisha Samdarshi",
      "Mariam Mustafa",
      "Anushka Kulkarni",
      "Raven Rothkopf",
      "Tuhin Chakrabarty",
      "Smaranda Muresan"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1183": {
    "title": "GottBERT: a pure German Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raphael Scheible",
      "Johann Frei",
      "Fabian Thomczyk",
      "Henry He",
      "Patric Tippmann",
      "Jochen Knaus",
      "Victor Jaravine",
      "Frank Kramer",
      "Martin Boeker"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1184": {
    "title": "Computational Meme Understanding: A Survey",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Khoi Nguyen",
      "Vincent Ng"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1185": {
    "title": "CoverICL: Selective Annotation for In-Context Learning via Active Graph Coverage",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Costas Mavromatis",
      "Balasubramaniam Srinivasan",
      "Zhengyuan Shen",
      "Jiani Zhang",
      "Huzefa Rangwala",
      "Christos Faloutsos",
      "George Karypis"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1186": {
    "title": "Retrieval-enriched zero-shot image classification in low-resource domains",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicola Dall’Asen",
      "Yiming Wang",
      "Enrico Fini",
      "Elisa Ricci"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1187": {
    "title": "I-AM-G: Interest Augmented Multimodal Generator for Item Personalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xianquan Wang",
      "Likang Wu",
      "Shukang Yin",
      "Zhi Li",
      "Yanjiang Chen",
      "Hufeng Hufeng",
      "Yu Su",
      "Qi Liu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1188": {
    "title": "Twists, Humps, and Pebbles: Multilingual Speech Recognition Models Exhibit Gender Performance Gaps",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giuseppe Attanasio",
      "Beatrice Savoldi",
      "Dennis Fucci",
      "Dirk Hovy"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1189": {
    "title": "Enhancing Language Model Alignment: A Confidence-Based Approach to Label Smoothing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baihe Huang",
      "Hiteshi Sharma",
      "Yi Mao"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1190": {
    "title": "Contrastive Policy Gradient: Aligning LLMs on sequence-level scores in a supervised-friendly fashion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yannis Flet-Berliac",
      "Nathan Grinsztajn",
      "Florian Strub",
      "Eugene Choi",
      "Bill Wu",
      "Chris Cremer",
      "Arash Ahmadian",
      "Yash Chandak",
      "Mohammad Azar",
      "Olivier Pietquin",
      "Matthieu Geist"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1191": {
    "title": "Show and Guide: Instructional-Plan Grounded Vision and Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Diogo Glória-Silva",
      "David Semedo",
      "Joao Magalhaes"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1192": {
    "title": "Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bandhav Veluri",
      "Benjamin Peloquin",
      "Bokai Yu",
      "Hongyu Gong",
      "Shyamnath Gollakota"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1193": {
    "title": "QuBE: Question-based Belief Enhancement for Agentic LLM Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minsoo Kim",
      "Jongyoon Kim",
      "Jihyuk Kim",
      "Seung-won Hwang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1194": {
    "title": "CompAct: Compressing Retrieved Documents Actively for Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chanwoong Yoon",
      "Taewhoo Lee",
      "Hyeon Hwang",
      "Minbyul Jeong",
      "Jaewoo Kang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1195": {
    "title": "An Empirical Analysis on Spatial Reasoning Capabilities of Large Multimodal Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fatemeh Shiri",
      "Xiao-Yu Guo",
      "Mona Far",
      "Xin Yu",
      "Reza Haf",
      "Yuan-Fang Li"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1196": {
    "title": "Synthetic Knowledge Ingestion: Towards Knowledge Refinement and Injection for Enhancing Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxin Zhang",
      "Wendi Cui",
      "Yiran Huang",
      "Kamalika Das",
      "Sricharan Kumar"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1197": {
    "title": "Local Contrastive Editing of Gender Stereotypes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marlene Lutz",
      "Rochelle Choenni",
      "Markus Strohmaier",
      "Anne Lauscher"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1198": {
    "title": "De-Identification of Sensitive Personal Data in Datasets Derived from IIT-CDIP",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefan Larson",
      "Nicole Lima",
      "Santiago Diaz",
      "Amogh Joshi",
      "Siddharth Betala",
      "Jamiu Suleiman",
      "Yash Mathur",
      "Kaushal Prajapati",
      "Ramla Alakraa",
      "Junjie Shen",
      "Temi Okotore",
      "Kevin Leach"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1199": {
    "title": "RAR: Retrieval-augmented retrieval for code generation in low resource languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Avik Dutta",
      "Mukul Singh",
      "Gust Verbruggen",
      "Sumit Gulwani",
      "Vu Le"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1200": {
    "title": "STAR: SocioTechnical Approach to Red Teaming Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Laura Weidinger",
      "John Mellor",
      "Bernat Pegueroles",
      "Nahema Marchal",
      "Ravin Kumar",
      "Kristian Lum",
      "Canfer Akbulut",
      "Mark Diaz",
      "A. Bergman",
      "Mikel Rodriguez",
      "Verena Rieser",
      "William Isaac"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1201": {
    "title": "Do great minds think alike? Investigating Human-AI Complementarity in Question Answering with CAIMIRA",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maharshi Gor",
      "Hal Daumé Iii",
      "Tianyi Zhou",
      "Jordan Boyd-Graber"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1202": {
    "title": "Memory-Efficient Fine-Tuning of Transformers via Token Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antoine Simoulin",
      "Namyong Park",
      "Xiaoyi Liu",
      "Grey Yang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1203": {
    "title": "Unveiling the mystery of visual attributes of concrete and abstract concepts: Variability, nearest neighbors, and challenging categories",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tarun Tater",
      "Sabine Schulte Im Walde",
      "Diego Frassinelli"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1204": {
    "title": "Evaluating Large Language Models on Time Series Feature Understanding: A Comprehensive Taxonomy and Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elizabeth Fons",
      "Rachneet Kaur",
      "Soham Palande",
      "Zhen Zeng",
      "Tucker Balch",
      "Manuela Veloso",
      "Svitlana Vyetrenko"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1205": {
    "title": "Can LLMs Learn Uncertainty on Their Own? Expressing Uncertainty Effectively in A Self-Training Manner",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shudong Liu",
      "Zhaocong Li",
      "Xuebo Liu",
      "Runzhe Zhan",
      "Derek Wong",
      "Lidia Chao",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1206": {
    "title": "Preference-Guided Reflective Sampling for Aligning Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hai Ye",
      "Hwee Tou Ng"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1207": {
    "title": "Metrics for What, Metrics for Whom: Assessing Actionability of Bias Evaluation Metrics in NLP",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pieter Delobelle",
      "Giuseppe Attanasio",
      "Debora Nozza",
      "Su Lin Blodgett",
      "Zeerak Talat"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1208": {
    "title": "Is this the real life? Is this just fantasy? The Misleading Success of Simulating Social Interactions With LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuhui Zhou",
      "Zhe Su",
      "Tiwalayo Eisape",
      "Hyunwoo Kim",
      "Maarten Sap"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1209": {
    "title": "A Simple LLM Framework for Long-Range Video Question-Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ce Zhang",
      "Taixi Lu",
      "Md Mohaiminul Islam",
      "Ziyang Wang",
      "Shoubin Yu",
      "Mohit Bansal",
      "Gedas Bertasius"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1210": {
    "title": "Rebuilding ROME : Resolving Model Collapse during Sequential Model Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akshat Gupta",
      "Sidharth Baskaran",
      "Gopala Anumanchipalli"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1211": {
    "title": "Casablanca: Data and Models for Multidialectal Arabic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bashar Talafha",
      "Karima Kadaoui",
      "Samar Magdy",
      "Mariem Habiboullah",
      "Chafei Chafei",
      "Ahmed El-Shangiti",
      "Hiba Zayed",
      "Mohamedou Tourad",
      "Rahaf Alhamouri",
      "Rwaa Assi",
      "Aisha Alraeesi",
      "Hour Mohamed",
      "Fakhraddin Alwajih",
      "Abdelrahman Mohamed",
      "Abdellah El Mekki",
      "El Moatez Billah Nagoudi",
      "Benelhadj Saadia",
      "Hamzah Alsayadi",
      "Walid Al-Dhabyani",
      "Sara Shatnawi",
      "Yasir Ech-chammakhy",
      "Amal Makouar",
      "Yousra Berrachedi",
      "Mustafa Jarrar",
      "Shady Shehata",
      "Ismail Berrada",
      "Muhammad Abdul-Mageed"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1212": {
    "title": "Safety Arithmetic: A Framework for Test-time Safety Alignment of Language Models by Steering Parameters and Activations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rima Hazra",
      "Sayan Layek",
      "Somnath Banerjee",
      "Soujanya Poria"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1213": {
    "title": "Communicating with Speakers and Listeners of Different Pragmatic Levels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kata Naszadi",
      "Frans Oliehoek",
      "Christof Monz"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1214": {
    "title": "RECANTFormer: Referring Expression Comprehension with Varying Numbers of Targets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bhathiya Hemanthage",
      "Hakan Bilen",
      "Phil Bartie",
      "Christian Dondrup",
      "Oliver Lemon"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1215": {
    "title": "Sprout: Green Generative AI with Carbon-Efficient LLM Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baolin Li",
      "Yankai Jiang",
      "Vijay Gadepally",
      "Devesh Tiwari"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1216": {
    "title": "Do LLMs Plan Like Human Writers? Comparing Journalist Coverage of Press Releases with LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Spangher",
      "Nanyun Peng",
      "Sebastian Gehrmann",
      "Mark Dredze"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1217": {
    "title": "T-FREE: Subword Tokenizer-Free Generative LLMs via Sparse Representations for Memory-Efficient Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Björn Deiseroth",
      "Manuel Brack",
      "Patrick Schramowski",
      "Kristian Kersting",
      "Samuel Weinbach"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1218": {
    "title": "SpeechQE: Estimating the Quality of Direct Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "HyoJung Han",
      "Kevin Duh",
      "Marine Carpuat"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1219": {
    "title": "Assessing and Verifying Task Utility in LLM-Powered Applications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Negar Arabzadeh",
      "Siqing Huo",
      "Nikhil Mehta",
      "Qingyun Wu",
      "Chi Wang",
      "Ahmed Awadallah",
      "Charles Clarke",
      "Julia Kiseleva"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1220": {
    "title": "Dynamic Rewarding with Prompt Optimization Enables Tuning-free Self-Alignment of Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Somanshu Singla",
      "Zhen Wang",
      "Tianyang Liu",
      "Abdullah Ashfaq",
      "Zhiting Hu",
      "Eric Xing"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1221": {
    "title": "Accurate and Data-Efficient Toxicity Prediction when Annotators Disagree",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Harbani Jaggi",
      "Kashyap Coimbatore Murali",
      "Eve Fleisig",
      "Erdem Biyik"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1222": {
    "title": "Adversarial Text Generation using Large Language Models for Dementia Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youxiang Zhu",
      "Nana Lin",
      "Kiran Balivada",
      "Daniel Haehn",
      "Xiaohui Liang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1223": {
    "title": "xCOMET-lite: Bridging the Gap Between Efficiency and Quality in Learned MT Evaluation Metrics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniil Larionov",
      "Mikhail Seleznyov",
      "Vasiliy Viskov",
      "Alexander Panchenko",
      "Steffen Eger"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1224": {
    "title": "The Greatest Good Benchmark: Measuring LLMs' Alignment with Utilitarian Moral Dilemmas",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giovanni Marraffini",
      "Andrés Cotton",
      "Noe Hsueh",
      "Axel Fridman",
      "Juan Wisznia",
      "Luciano Corro"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1225": {
    "title": "FairFlow: Mitigating Dataset Biases through Undecided Learning for Natural Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiali Cheng",
      "Hadi Amiri"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1226": {
    "title": "Style-Shifting Behaviour of the Manosphere on Reddit",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jai Aggarwal",
      "Suzanne Stevenson"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1227": {
    "title": "The Death and Life of Great Prompts: Analyzing the Evolution of LLM Prompts from the Structural Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihan Ma",
      "Xinyue Shen",
      "Yixin Wu",
      "Boyang Zhang",
      "Michael Backes",
      "Yang Zhang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1228": {
    "title": "Holistic Evaluation for Interleaved Text-and-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minqian Liu",
      "Zhiyang Xu",
      "Zihao Lin",
      "Trevor Ashby",
      "Joy Rimchala",
      "Jiaxin Zhang",
      "Lifu Huang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1229": {
    "title": "FOLIO: Natural Language Reasoning with First-Order Logic",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simeng Han",
      "Hailey Schoelkopf",
      "Yilun Zhao",
      "Zhenting Qi",
      "Martin Riddell",
      "Wenfei Zhou",
      "James Coady",
      "David Peng",
      "Yujie Qiao",
      "Luke Benson",
      "Lucy Sun",
      "Alexander Wardle-Solano",
      "Hannah Szabó",
      "Ekaterina Zubova",
      "Matthew Burtell",
      "Jonathan Fan",
      "Yixin Liu",
      "Brian Wong",
      "Malcolm Sailor",
      "Ansong Ni",
      "Linyong Nan",
      "Jungo Kasai",
      "Tao Yu",
      "Rui Zhang",
      "Alexander Fabbri",
      "Wojciech Kryscinski",
      "Semih Yavuz",
      "Ye Liu",
      "Xi Lin",
      "Shafiq Joty",
      "Yingbo Zhou",
      "Caiming Xiong",
      "Rex Ying",
      "Arman Cohan",
      "Dragomir Radev"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1230": {
    "title": "The LLM Effect: Are Humans Truly Using LLMs, or Are They Being Influenced By Them Instead?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Choi",
      "Syeda Sabrina Akter",
      "J.p. Singh",
      "Antonios Anastasopoulos"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1231": {
    "title": "Is Child-Directed Speech Effective Training Data for Language Models?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Steven Feng",
      "Noah Goodman",
      "Michael Frank"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1232": {
    "title": "RevMUX: Data Multiplexing with Reversible Adapters for Efficient LLM Batch Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yige Xu",
      "Xu Guo",
      "Zhiwei Zeng",
      "Chunyan Miao"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1233": {
    "title": "Inference Helps PLMs' Conceptual Understanding: Improving the Abstract Inference Ability with Hierarchical Conceptual Entailment Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juncai Li",
      "Ru Li",
      "Xiaoli Li",
      "Qinghua Chai",
      "Jeff Pan"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1234": {
    "title": "M3Hop-CoT: Misogynous Meme Identification with Multimodal Multi-hop Chain-of-Thought",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gitanjali Kumari",
      "Kirtan Jain",
      "Asif Ekbal"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1235": {
    "title": "GPT-4 Jailbreaks Itself with Near-Perfect Success Using Self-Explanation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Govind Ramesh",
      "Yao Dou",
      "Wei Xu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1236": {
    "title": "RE-RAG: Improving Open-Domain QA Performance and Interpretability with Relevance Estimator in Retrieval-Augmented Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kiseung Kim",
      "Jay-Yoon Lee"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1237": {
    "title": "Evaluating Concurrent Robustness of Language Models Across Diverse Challenge Sets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vatsal Gupta",
      "Pranshu Pandya",
      "Tushar Kataria",
      "Vivek Gupta",
      "Dan Roth"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1238": {
    "title": "Simul-MuST-C: Simultaneous Multilingual Speech Translation Corpus Using Large Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mana Makinae",
      "Yusuke Sakai",
      "Hidetaka Kamigaito",
      "Taro Watanabe"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1239": {
    "title": "Is This a Bad Table? A Closer Look at the Evaluation of Table Generation from Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pritika Ramu",
      "Aparna Garimella",
      "Sambaran Bandyopadhyay"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1240": {
    "title": "On the Fragility of Active Learners for Text Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhishek Ghose",
      "Emma Nguyen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1241": {
    "title": "BMRetriever: Tuning Large Language Models as Better Biomedical Text Retrievers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ran Xu",
      "Wenqi Shi",
      "Yue Yu",
      "Yuchen Zhuang",
      "Yanqiao Zhu",
      "May Dongmei Wang",
      "Joyce Ho",
      "Chao Zhang",
      "Carl Yang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1242": {
    "title": "Comparing Neighbors Together Makes it Easy: Jointly Comparing Multiple Candidates for Efficient and Effective Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonghyun Song",
      "Cheyon Jin",
      "Wenlong Zhao",
      "Andrew McCallum",
      "Jay-Yoon Lee"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1243": {
    "title": "M3D: MultiModal MultiDocument Fine-Grained Inconsistency Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chia-Wei Tang",
      "Ting-Chih Chen",
      "Kiet Nguyen",
      "Kazi Sajeed Mehrab",
      "Alvi Ishmam",
      "Chris Thomas"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1244": {
    "title": "MedAdapter: Efficient Test-Time Adaptation of Large Language Models Towards Medical Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenqi Shi",
      "Ran Xu",
      "Yuchen Zhuang",
      "Yue Yu",
      "Haotian Sun",
      "Hang Wu",
      "Carl Yang",
      "May Dongmei Wang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1245": {
    "title": "EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenqi Shi",
      "Ran Xu",
      "Yuchen Zhuang",
      "Yue Yu",
      "Jieyu Zhang",
      "Hang Wu",
      "Yuanda Zhu",
      "Joyce Ho",
      "Carl Yang",
      "May Dongmei Wang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1246": {
    "title": "SimLLM: Detecting Sentences Generated by Large Language Models Using Similarity between the Generation and its Re-generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hoang-Quoc Nguyen-Son",
      "Minh-Son Dao",
      "Koji Zettsu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1247": {
    "title": "CELLO: Causal Evaluation of Large Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meiqi Chen",
      "Bo Peng",
      "Yan Zhang",
      "Chaochao Lu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1248": {
    "title": "Simultaneous Interpretation Corpus Construction by Large Language Models in Distant Language Pair",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yusuke Sakai",
      "Mana Makinae",
      "Hidetaka Kamigaito",
      "Taro Watanabe"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1249": {
    "title": "Training-free Deep Concept Injection Enables Language Models for Video Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xudong Lin",
      "Manling Li",
      "Richard Zemel",
      "Heng Ji",
      "Shih-Fu Chang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1250": {
    "title": "MIBench: Evaluating Multimodal Large Language Models over Multiple Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haowei Liu",
      "Xi Zhang",
      "Haiyang Xu",
      "Yaya Shi",
      "Chaoya Jiang",
      "Ming Yan",
      "Ji Zhang",
      "Fei Huang",
      "Chunfeng Yuan",
      "Bing Li",
      "Weiming Hu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1251": {
    "title": "ZEBRA: Zero-Shot Example-Based Retrieval Augmentation for Commonsense Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesco Molfese",
      "Simone Conia",
      "Riccardo Orlando",
      "Roberto Navigli"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1252": {
    "title": "ABLE: Personalized Disability Support with Politeness and Empathy Integration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kshitij Mishra",
      "Manisha Burja",
      "Asif Ekbal"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1253": {
    "title": "Language Models as Compilers: Simulating Pseudocode Execution Improves Algorithmic Reasoning in Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyungjoo Chae",
      "Yeonghyeon Kim",
      "Seungone Kim",
      "Kai Ong",
      "Beong-woo Kwak",
      "Moohyeon Kim",
      "Sunghwan Kim",
      "Taeyoon Kwon",
      "Jiwan Chung",
      "Youngjae Yu",
      "Jinyoung Yeo"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1254": {
    "title": "Coffee-Gym: An Environment for Evaluating and Improving Natural Language Feedback on Erroneous Code",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyungjoo Chae",
      "Taeyoon Kwon",
      "Seungjun Moon",
      "Yongho Song",
      "Dongjin Kang",
      "Kai Ong",
      "Beong-woo Kwak",
      "Seonghyeon Bae",
      "Seung-won Hwang",
      "Jinyoung Yeo"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1255": {
    "title": "Improving Minimum Bayes Risk Decoding with Multi-Prompt",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Heineman",
      "Yao Dou",
      "Wei Xu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1256": {
    "title": "Deciphering Cognitive Distortions in Patient-Doctor Mental Health Conversations: A Multimodal LLM-Based Detection and Reasoning Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gopendra Singh",
      "Sai Vemulapalli",
      "Mauajama Firdaus",
      "Asif Ekbal"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1257": {
    "title": "Nearest Neighbor Normalization Improves Multimodal Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Neil Chowdhury",
      "Franklin Wang",
      "Sumedh Shenoy",
      "Douwe Kiela",
      "Sarah Schwettmann",
      "Tristan Thrush"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1258": {
    "title": "Rethinking Pragmatics in Large Language Models: Towards Open-Ended Evaluation and Preference Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengguang Wu",
      "Shusheng Yang",
      "Zhenglun Chen",
      "Qi Su"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1259": {
    "title": "LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for Long-Context Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingfei Zhao",
      "Ruobing Wang",
      "Yukuo Cen",
      "Daren Zha",
      "Shicheng Tan",
      "Yuxiao Dong",
      "Jie Tang"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1260": {
    "title": "Context-aware Watermark with Semantic Balanced Green-red Lists for Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Guo",
      "Zhiliang Tian",
      "Yiping Song",
      "Tianlun Liu",
      "Liang Ding",
      "Dongsheng Li"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1261": {
    "title": "Knowledge Graph Enhanced Large Language Model Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengqi Zhang",
      "Xiaotian Ye",
      "Qiang Liu",
      "Pengjie Ren",
      "Shu Wu",
      "Zhumin Chen"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1262": {
    "title": "‘Quis custodiet ipsos custodes?' Who will watch the watchmen? On Detecting AI-generated peer-reviews",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sandeep Kumar",
      "Mohit Sahu",
      "Vardhan Gacche",
      "Tirthankar Ghosal",
      "Asif Ekbal"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1263": {
    "title": "Mitigating Open-Vocabulary Caption Hallucinations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Assaf Ben-Kish",
      "Moran Yanuka",
      "Morris Alper",
      "Raja Giryes",
      "Hadar Averbuch-Elor"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1264": {
    "title": "Initialization of Large Language Models via Reparameterization to Mitigate Loss Spikes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kosuke Nishida",
      "Kyosuke Nishida",
      "Kuniko Saito"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1265": {
    "title": "ALVIN: Active Learning Via INterpolation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michalis Korakakis",
      "Andreas Vlachos",
      "Adrian Weller"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1266": {
    "title": "Filtered Direct Preference Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tetsuro Morimura",
      "Mitsuki Sakamoto",
      "Yuu Jinnai",
      "Kenshi Abe",
      "Kaito Ariu"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1267": {
    "title": "Instruction Fine-Tuning: Does Prompt Loss Matter?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mathew Huerta-Enochian",
      "Seung Ko"
    ]
  },
  "https://aclanthology.org/2024.emnlp-main.1268": {
    "title": "Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tomás Feith",
      "Akhil Arora",
      "Martin Gerlach",
      "Debjit Paul",
      "Robert West"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.1": {
    "title": "Are LLMs Good Annotators for Discourse-level Event Relation Extraction?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kangda Wei",
      "Aayush Gautam",
      "Ruihong Huang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.2": {
    "title": "Transferability of Syntax-Aware Graph Neural Networks in Zero-Shot Cross-Lingual Semantic Role Labeling",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rachel Devianti",
      "Yusuke Miyao"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.3": {
    "title": "Should Cross-Lingual AMR Parsing go Meta? An Empirical Assessment of Meta-Learning and Joint Learning AMR Parsing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeongwoo Kang",
      "Maximin Coavoux",
      "Cédric Lopez",
      "Didier Schwab"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.4": {
    "title": "General Collaborative Framework between Large Language Model and Experts for Universal Information Extraction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "K Bao",
      "Ning Wang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.5": {
    "title": "SEAVER: Attention Reallocation for Mitigating Distractions in Language Models for Conditional Semantic Textual Similarity Measurement",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baixuan Li",
      "Yunlong Fan",
      "Zhiqiang Gao"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.6": {
    "title": "Search if you don't know! Knowledge-Augmented Korean Grammatical Error Correction with Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seonmin Koo",
      "Jinsung Kim",
      "Chanjun Park",
      "Heuiseok Lim"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.7": {
    "title": "Measuring the Robustness of NLP Models to Domain Shifts",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nitay Calderon",
      "Naveh Porat",
      "Eyal Ben-David",
      "Alexander Chapanin",
      "Zorik Gekhman",
      "Nadav Oved",
      "Vitaly Shalumov",
      "Roi Reichart"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.8": {
    "title": "Text2Model: Text-based Model Induction for Zero-shot Image Classification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ohad Amosy",
      "Tomer Volk",
      "Eilam Shapira",
      "Eyal Ben-David",
      "Roi Reichart",
      "Gal Chechik"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.9": {
    "title": "InsertGNN: A Hierarchical Graph Neural Network for the TOEFL Sentence Insertion Problem",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fang Wu",
      "Stan Li"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.10": {
    "title": "Unleashing Large Language Models' Proficiency in Zero-shot Essay Scoring",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanwoo Lee",
      "Yida Cai",
      "Desong Meng",
      "Ziyang Wang",
      "Yunfang Wu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.11": {
    "title": "DetectBench: Can Large Language Model Detect and Piece Together Implicit Evidence?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhouhong Gu",
      "Lin Zhang",
      "Xiaoxuan Zhu",
      "Jiangjie Chen",
      "Wenhao Huang",
      "Yikai Zhang",
      "Shusen Wang",
      "Zheyu Ye",
      "Yan Gao",
      "Hongwei Feng",
      "Yanghua Xiao"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.12": {
    "title": "Improve Meta-learning for Few-Shot Text Classification with All You Can Acquire from the Tasks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyue Liu",
      "Yunlong Gao",
      "Linlin Zong",
      "Bo Xu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.13": {
    "title": "CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level Granularity",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Moshe Berchansky",
      "Daniel Fleischer",
      "Moshe Wasserblat",
      "Peter Izsak"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.14": {
    "title": "SnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jielin Qiu",
      "Andrea Madotto",
      "Zhaojiang Lin",
      "Paul Crook",
      "Yifan Xu",
      "Babak Damavandi",
      "Xin Dong",
      "Christos Faloutsos",
      "Lei Li",
      "Seungwhan Moon"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.15": {
    "title": "SRAP-Agent: Simulating and Optimizing Scarce Resource Allocation Policy with LLM-based Agent",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiarui Ji",
      "Yang Li",
      "Hongtao Liu",
      "Zhicheng Du",
      "Zhewei Wei",
      "Qi Qi",
      "Weiran Shen",
      "Yankai Lin"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.16": {
    "title": "Ukrainian Resilience: A Dataset for Detection of Help-Seeking Signals Amidst the Chaos of War",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Msvpj Sathvik",
      "Abhilash Dowpati",
      "Srreyansh Sethi"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.17": {
    "title": "Selective Annotation via Data Allocation: These Data Should Be Triaged to Experts for Annotation Rather Than the Model",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Huang",
      "Yang Deng",
      "Wenqiang Lei",
      "Jiancheng Lv",
      "Ido Dagan"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.18": {
    "title": "Document Hashing with Multi-Grained Prototype-Induced Hierarchical Generative Model",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qian Zhang",
      "Qinliang Su",
      "Jiayang Chen",
      "Zhenpeng Song"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.19": {
    "title": "Predictive Multiplicity of Knowledge Graph Embeddings in Link Prediction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuqicheng Zhu",
      "Nico Potyka",
      "Mojtaba Nayyeri",
      "Bo Xiong",
      "Yunjie He",
      "Evgeny Kharlamov",
      "Steffen Staab"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.20": {
    "title": "Temporal Fact Reasoning over Hyper-Relational Knowledge Graphs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zifeng Ding",
      "Jingcheng Wu",
      "Jingpei Wu",
      "Yan Xia",
      "Bo Xiong",
      "Volker Tresp"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.21": {
    "title": "GREEN: Generative Radiology Report Evaluation and Error Notation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sophie Ostmeier",
      "Justin Xu",
      "Zhihong Chen",
      "Maya Varma",
      "Louis Blankemeier",
      "Christian Bluethgen",
      "Arne Md",
      "Michael Moseley",
      "Curtis Langlotz",
      "Akshay Chaudhari",
      "Jean-Benoit Delbrouck"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.22": {
    "title": "XRec: Large Language Models for Explainable Recommendation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiyao Ma",
      "Xubin Ren",
      "Chao Huang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.23": {
    "title": "LLM Questionnaire Completion for Automatic Psychiatric Assessment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gony Rosenman",
      "Talma Hendler",
      "Lior Wolf"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.24": {
    "title": "Disordered-DABS: A Benchmark for Dynamic Aspect-Based Summarization in Disordered Texts",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaobo Guo",
      "Soroush Vosoughi"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.25": {
    "title": "Walia-LLM: Enhancing Amharic-LLaMA by Integrating Task-Specific and Generative Datasets",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Israel Azime",
      "Atnafu Tonja",
      "Tadesse Belay",
      "Mitiku Yohannes Fuge",
      "Aman Wassie",
      "Eyasu Jada",
      "Yonas Chanie",
      "Walelign Sewunetie",
      "Seid Yimam"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.26": {
    "title": "Can Large Language Models Identify Authorship?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baixiang Huang",
      "Canyu Chen",
      "Kai Shu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.27": {
    "title": "TransLLaMa: LLM-based Simultaneous Translation System",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roman Koshkin",
      "Katsuhito Sudoh",
      "Satoshi Nakamura"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.28": {
    "title": "Axis Tour: Word Tour Determines the Order of Axes in ICA-transformed Embeddings",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hiroaki Yamagiwa",
      "Yusuke Takase",
      "Hidetoshi Shimodaira"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.29": {
    "title": "Granularity is crucial when applying differential privacy to text: An investigation for neural machine translation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Doan Vu",
      "Timour Igamberdiev",
      "Ivan Habernal"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.30": {
    "title": "An Open-Source Data Contamination Report for Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yucheng Li",
      "Yunhao Guo",
      "Frank Guerin",
      "Chenghua Lin"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.31": {
    "title": "Few shot chain-of-thought driven reasoning to prompt LLMs for open-ended medical question answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saeel Nachane",
      "Ojas Gramopadhye",
      "Prateek Chanda",
      "Ganesh Ramakrishnan",
      "Kshitij Jadhav",
      "Yatin Nandwani",
      "Dinesh Raghu",
      "Sachindra Joshi"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.32": {
    "title": "Reformatted Alignment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Run-Ze Fan",
      "Xuefeng Li",
      "Haoyang Zou",
      "Junlong Li",
      "Shwai He",
      "Ethan Chern",
      "Jiewen Hu",
      "Pengfei Liu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.33": {
    "title": "Unsupervised Domain Adaptation for Keyphrase Generation using Citation Contexts",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Florian Boudin",
      "Akiko Aizawa"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.34": {
    "title": "SMILE: Single-turn to Multi-turn Inclusive Language Expansion via ChatGPT for Mental Health Support",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huachuan Qiu",
      "Hongliang He",
      "Shuai Zhang",
      "Anqi Li",
      "Zhenzhong Lan"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.35": {
    "title": "DocEE-zh: A Fine-grained Benchmark for Chinese Document-level Event Extraction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minghui Liu",
      "MeiHan Tong",
      "Yangda Peng",
      "Lei Hou",
      "Juanzi Li",
      "Bin Xu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.36": {
    "title": "MalayMMLU: A Multitask Benchmark for the Low-Resource Malay Language",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soon Poh",
      "Sze Jue Yang",
      "Jeraelyn Tan",
      "Lawrence Chieng",
      "Jia Tan",
      "Zhenyu Yu",
      "Foong Mun",
      "Chee Seng Chan"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.37": {
    "title": "Symbolic Prompt Program Search: A Structure-Aware Approach to Efficient Compile-Time Prompt Optimization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tobias Schnabel",
      "Jennifer Neville"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.38": {
    "title": "Learning to Route for Dynamic Adapter Composition in Continual Learning with Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vladimir Araujo",
      "Marie-Francine Moens",
      "Tinne Tuytelaars"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.39": {
    "title": "LLM-supertagger: Categorial Grammar Supertagging via Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinman Zhao",
      "Gerald Penn"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.40": {
    "title": "Editing Conceptual Knowledge for Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaohan Wang",
      "Shengyu Mao",
      "Shumin Deng",
      "Yunzhi Yao",
      "Yue Shen",
      "Lei Liang",
      "Jinjie Gu",
      "Huajun Chen",
      "Ningyu Zhang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.41": {
    "title": "RAG-Studio: Towards In-Domain Adaptation of Retrieval Augmented Generation Through Self-Alignment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kelong Mao",
      "Zheng Liu",
      "Hongjin Qian",
      "Fengran Mo",
      "Chenlong Deng",
      "Zhicheng Dou"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.42": {
    "title": "MMCode: Benchmarking Multimodal Large Language Models for Code Generation with Visually Rich Programming Problems",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaixin Li",
      "Yuchen Tian",
      "Qisheng Hu",
      "Ziyang Luo",
      "Zhiyong Huang",
      "Jing Ma"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.43": {
    "title": "Enabling Discriminative Reasoning in LLMs for Legal Judgment Prediction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenlong Deng",
      "Kelong Mao",
      "Yuyao Zhang",
      "Zhicheng Dou"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.44": {
    "title": "Preserving Pre-trained Representation Space: On Effectiveness of Prefix-tuning for Large Multi-modal Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Donghoon Kim",
      "Gusang Lee",
      "Kyuhong Shim",
      "Byonghyo Shim"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.45": {
    "title": "What Would Happen Next? Predicting Consequences from An Event Causality Graph",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuanhong Zhan",
      "Wei Xiang",
      "Liang Chao",
      "Bang Wang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.46": {
    "title": "Can LLMs Learn From Mistakes? An Empirical Study on Reasoning Tasks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengnan An",
      "Zexiong Ma",
      "Siqi Cai",
      "Zeqi Lin",
      "Nanning Zheng",
      "Jian-Guang Lou",
      "Weizhu Chen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.47": {
    "title": "Temporal Cognitive Tree: A Hierarchical Modeling Approach for Event Temporal Relation Extraction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wanting Ning",
      "Lishuang Li",
      "Xueyang Qin",
      "Yubo Feng",
      "Jingyao Tang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.48": {
    "title": "LongGenBench: Long-context Generation Benchmark",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Liu",
      "Peijie Dong",
      "Xuming Hu",
      "Xiaowen Chu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.49": {
    "title": "RaFe: Ranking Feedback Improves Query Rewriting for RAG",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengyu Mao",
      "Yong Jiang",
      "Boli Chen",
      "Xiao Li",
      "Peng Wang",
      "Xinyu Wang",
      "Pengjun Xie",
      "Fei Huang",
      "Huajun Chen",
      "Ningyu Zhang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.50": {
    "title": "BASES: Large-scale Web Search User Simulation with Large Language Model based Agents",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiyang Ren",
      "Peng Qiu",
      "Yingqi Qu",
      "Jing Liu",
      "Xin Zhao",
      "Hua Wu",
      "Ji-Rong Wen",
      "Haifeng Wang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.51": {
    "title": "Make Large Language Model a Better Ranker",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wen-Shuo Chao",
      "Zhi Zheng",
      "Hengshu Zhu",
      "Hao Liu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.52": {
    "title": "SpeciaLex: A Benchmark for In-Context Specialized Lexicon Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joseph Marvin Imperial",
      "Harish Tayyar Madabushi"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.53": {
    "title": "Devil's Advocate: Anticipatory Reflection for LLM Agents",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Wang",
      "Tao Li",
      "Zhiwei Deng",
      "Dan Roth",
      "Yang Li"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.54": {
    "title": "API Is Enough: Conformal Prediction for Large Language Models Without Logit-Access",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayuan Su",
      "Jing Luo",
      "Hongwei Wang",
      "Lu Cheng"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.55": {
    "title": "Introducing Compiler Semantics into Large Language Models as Programming Language Translators: A Case Study of C to x86 Assembly",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuoming Zhang",
      "Jiacheng Zhao",
      "Chunwei Xia",
      "Zheng Wang",
      "Yunji Chen",
      "Huimin Cui"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.56": {
    "title": "Negating Negatives: Alignment with Human Negative Samples via Distributional Dispreference Optimization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shitong Duan",
      "Xiaoyuan Yi",
      "Peng Zhang",
      "Yan Liu",
      "Zheng Liu",
      "Tun Lu",
      "Xing Xie",
      "Ning Gu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.57": {
    "title": "OffsetBias: Leveraging Debiased Data for Tuning Evaluators",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junsoo Park",
      "Seungyeon Jwa",
      "Ren Meiying",
      "Daeyoung Kim",
      "Sanghyuk Choi"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.58": {
    "title": "Employing Glyphic Information for Chinese Event Extraction with Vision-Language Model",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyi Bao",
      "Jinghang Gu",
      "Zhongqing Wang",
      "Minjie Qiang",
      "Chu-Ren Huang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.59": {
    "title": "Can CLIP Count Stars? An Empirical Study on Quantity Bias in CLIP",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeliang Zhang",
      "Zhuo Liu",
      "Mingqian Feng",
      "Chenliang Xu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.60": {
    "title": "LLM-A*: Large Language Model Enhanced Incremental Heuristic Search on Path Planning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Silin Meng",
      "Yiwei Wang",
      "Cheng-Fu Yang",
      "Nanyun Peng",
      "Kai-Wei Chang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.61": {
    "title": "Guided Knowledge Generation with Language Models for Commonsense Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Wei",
      "Haoran Chen",
      "Hang Yu",
      "Hao Fei",
      "Qian Liu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.62": {
    "title": "BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaisi Guan",
      "Qian Cao",
      "Yuchong Sun",
      "Xiting Wang",
      "Ruihua Song"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.63": {
    "title": "NCPrompt: NSP-Based Prompt Learning and Contrastive Learning for Implicit Discourse Relation Recognition",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuetong Rong",
      "Yijun Mo"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.64": {
    "title": "SAFETY-J: Evaluating Safety with Critique",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixiu Liu",
      "Yuxiang Zheng",
      "Shijie Xia",
      "Jiajun Li",
      "Yi Tu",
      "Chaoling Song",
      "Pengfei Liu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.65": {
    "title": "Improving Demonstration Diversity by Human-Free Fusing for Text-to-SQL",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dingzirui Wang",
      "Longxu Dou",
      "Xuanliang Zhang",
      "Qingfu Zhu",
      "Wanxiang Che"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.66": {
    "title": "A Unified Framework and Dataset for Assessing Societal Bias in Vision-Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashutosh Sathe",
      "Prachi Jain",
      "Sunayana Sitaram"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.67": {
    "title": "Breaking the Boundaries: A Unified Framework for Chinese Named Entity Recognition Across Text and Speech",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinzhong Ning",
      "Yuanyuan Sun",
      "Bo Xu",
      "Zhihao Yang",
      "Ling Luo",
      "Hongfei Lin"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.68": {
    "title": "VGA: Vision GUI Assistant - Minimizing Hallucinations through Image-Centric Fine-Tuning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meng Ziyang",
      "Yu Dai",
      "Zezheng Gong",
      "Shaoxiong Guo",
      "Minglong Tang",
      "Tongquan Wei"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.69": {
    "title": "Understanding the Therapeutic Relationship between Counselors and Clients in Online Text-based Counseling using LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anqi Li",
      "Yu Lu",
      "Nirui Song",
      "Shuai Zhang",
      "Lizhi Ma",
      "Zhenzhong Lan"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.70": {
    "title": "Dynamic Planning for LLM-based Graphical User Interface Automation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaoqing Zhang",
      "Zhuosheng Zhang",
      "Kehai Chen",
      "Xinbei Ma",
      "Muyun Yang",
      "Tiejun Zhao",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.71": {
    "title": "SeRTS: Self-Rewarding Tree Search for Biomedical Retrieval-Augmented Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minda Hu",
      "Licheng Zong",
      "Hongru Wang",
      "Jingyan Zhou",
      "Jingjing Li",
      "Yichen Gao",
      "Kam-Fai Wong",
      "Yu Li",
      "Irwin King"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.72": {
    "title": "Large Language Model-based Human-Agent Collaboration for Complex Task Solving",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xueyang Feng",
      "Zhi-Yuan Chen",
      "Yujia Qin",
      "Yankai Lin",
      "Xu Chen",
      "Zhiyuan Liu",
      "Ji-Rong Wen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.73": {
    "title": "MM-MATH: Advancing Multimodal Math Evaluation with Process Evaluation and Fine-grained Classification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Sun",
      "Yushi Bai",
      "Ji Qi",
      "Lei Hou",
      "Juanzi Li"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.74": {
    "title": "LongAlign: A Recipe for Long Context Alignment of Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yushi Bai",
      "Xin Lv",
      "Jiajie Zhang",
      "Yuze He",
      "Ji Qi",
      "Lei Hou",
      "Jie Tang",
      "Yuxiao Dong",
      "Juanzi Li"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.75": {
    "title": "Let's Ask GNN: Empowering Large Language Model for Graph In-Context Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengyu Hu",
      "Yichuan Li",
      "Zhengyu Chen",
      "Jingang Wang",
      "Han Liu",
      "Kyumin Lee",
      "Kaize Ding"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.76": {
    "title": "CoXQL: A Dataset for Parsing Explanation Requests in Conversational XAI Systems",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qianli Wang",
      "Tatiana Anikina",
      "Nils Feldhus",
      "Simon Ostermann",
      "Sebastian Möller"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.77": {
    "title": "Evaluating Language Model Character Traits",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francis Ward",
      "Zejia Yang",
      "Alex Jackson",
      "Randy Brown",
      "Chandler Smith",
      "Grace Colverd",
      "Louis Thomson",
      "Raymond Douglas",
      "Patrik Bartak",
      "Andrew Rowan"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.78": {
    "title": "Self-Explore: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyeonbin Hwang",
      "Doyoung Kim",
      "Seungone Kim",
      "Seonghyeon Ye",
      "Minjoon Seo"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.79": {
    "title": "R-Judge: Benchmarking Safety Risk Awareness for LLM Agents",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tongxin Yuan",
      "Zhiwei He",
      "Lingzhong Dong",
      "Yiming Wang",
      "Ruijie Zhao",
      "Tian Xia",
      "Lizhen Xu",
      "Binglin Zhou",
      "Fangqi Li",
      "Zhuosheng Zhang",
      "Rui Wang",
      "Gongshen Liu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.80": {
    "title": "EAVE: Efficient Product Attribute Value Extraction via Lightweight Sparse-layer Interaction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li Yang",
      "Qifan Wang",
      "Jianfeng Chi",
      "Jiahao Liu",
      "Jingang Wang",
      "Fuli Feng",
      "Zenglin Xu",
      "Yi Fang",
      "Lifu Huang",
      "Dongfang Liu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.81": {
    "title": "MultiSkill: Evaluating Large Multimodal Models for Fine-grained Alignment Skills",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenran Xu",
      "Senbao Shi",
      "Baotian Hu",
      "Longyue Wang",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.82": {
    "title": "To Forget or Not? Towards Practical Knowledge Unlearning for Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bozhong Tian",
      "Xiaozhuan Liang",
      "Siyuan Cheng",
      "Qingbin Liu",
      "Mengru Wang",
      "Dianbo Sui",
      "Xi Chen",
      "Huajun Chen",
      "Ningyu Zhang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.83": {
    "title": "EchoSight: Advancing Visual-Language Models with Wiki Knowledge",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yibin Yan",
      "Weidi Xie"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.84": {
    "title": "Diversify, Rationalize, and Combine: Ensembling Multiple QA Strategies for Zero-shot Knowledge-based VQA",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miaoyu Li",
      "Haoxin Li",
      "Zilin Du",
      "Boyang Li"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.85": {
    "title": "Reconfidencing LLMs from the Grouping Loss Perspective",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lihu Chen",
      "Alexandre Perez-Lebel",
      "Fabian Suchanek",
      "Gaël Varoquaux"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.86": {
    "title": "Tokenization Falling Short: On Subword Robustness in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yekun Chai",
      "Yewei Fang",
      "Qiwei Peng",
      "Xuhong Li"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.87": {
    "title": "AC-EVAL: Evaluating Ancient Chinese Language Understanding in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuting Wei",
      "Yuanxing Xu",
      "Xinru Wei",
      "Yangsimin Yangsimin",
      "Yangfu Zhu",
      "Yuqing Li",
      "Di Liu",
      "Bin Wu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.88": {
    "title": "MMAR: Multilingual and Multimodal Anaphora Resolution in Instructional Videos",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cennet Oguz",
      "Pascal Denis",
      "Simon Ostermann",
      "Emmanuel Vincent",
      "Natalia Skachkova",
      "Josef Genabith"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.89": {
    "title": "Dealing with Controversy: An Emotion and Coping Strategy Corpus Based on Role Playing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Enrica Troiano",
      "Sofie Labat",
      "Marco Stranisci",
      "Rossana Damiano",
      "Viviana Patti",
      "Roman Klinger"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.90": {
    "title": "MATE: Meet At The Embedding - Connecting Images with Long Texts",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Young Kyun Jang",
      "Junmo Kang",
      "Yong Jae Lee",
      "Donghyun Kim"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.91": {
    "title": "Mixed Distillation Helps Smaller Language Models Reason Better",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li Chenglin",
      "Qianglong Chen",
      "Liangyue Li",
      "Caiyu Wang",
      "Feng Tao",
      "Yicheng Li",
      "Zulong Chen",
      "Yin Zhang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.92": {
    "title": "The SIFo Benchmark: Investigating the Sequential Instruction Following Ability of Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyi Chen",
      "Baohao Liao",
      "Jirui Qi",
      "Panagiotis Eustratiadis",
      "Christof Monz",
      "Arianna Bisazza",
      "Maarten Rijke"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.93": {
    "title": "Optimizing Instruction Synthesis: Effective Exploration of Evolutionary Space with Tree Search",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li Chenglin",
      "Qianglong Chen",
      "Zhi Li",
      "FengTao FengTao",
      "Yicheng Li",
      "Hao Chen",
      "Fei Yu",
      "Yin Zhang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.94": {
    "title": "Suri: Multi-constraint Instruction Following in Long-form Text Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chau Pham",
      "Simeng Sun",
      "Mohit Iyyer"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.95": {
    "title": "Augmenting Black-box LLMs with Medical Textbooks for Biomedical Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yubo Wang",
      "Xueguang Ma",
      "Wenhu Chen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.96": {
    "title": "Exploring Multilingual Concepts of Human Values in Large Language Models: Is Value Alignment Consistent, Transferable and Controllable across Languages?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaoyang Xu",
      "Weilong Dong",
      "Zishan Guo",
      "Xinwei Wu",
      "Deyi Xiong"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.97": {
    "title": "PaCoST: Paired Confidence Significance Testing for Benchmark Contamination Detection in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huixuan Zhang",
      "Yun Lin",
      "Xiaojun Wan"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.98": {
    "title": "UrbanLLM: Autonomous Urban Activity Planning and Management with Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Jiang",
      "Qin Chao",
      "Yile Chen",
      "Xiucheng Li",
      "Shuai Liu",
      "Gao Cong"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.99": {
    "title": "Breaking the Ceiling of the LLM Community by Treating Token Generation as a Classification for Ensembling",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yao-Ching Yu",
      "Chun Chih Kuo",
      "Ye Ziqi",
      "Chang Yucheng",
      "Yueh-Se Li"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.100": {
    "title": "Eliciting Instruction-tuned Code Language Models' Capabilities to Utilize Auxiliary Function for Code Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seonghyeon Lee",
      "Suyeon Kim",
      "Joonwon Jang",
      "HeeJae Chon",
      "Dongha Lee",
      "Hwanjo Yu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.101": {
    "title": "AHP-Powered LLM Reasoning for Multi-Criteria Evaluation of Open-Ended Responses",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaotian Lu",
      "Jiyi Li",
      "Koh Takeuchi",
      "Hisashi Kashima"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.102": {
    "title": "Enhancing Fine-Grained Image Classifications via Cascaded Vision Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Canshi Wei"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.103": {
    "title": "Exploring the Best Practices of Query Expansion with Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Le Zhang",
      "Yihong Wu",
      "Qian Yang",
      "Jian-Yun Nie"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.104": {
    "title": "Chain-of-Rewrite: Aligning Question and Documents for Open-Domain Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chunlei Xin",
      "Yaojie Lu",
      "Hongyu Lin",
      "Shuheng Zhou",
      "Huijia Zhu",
      "Weiqiang Wang",
      "Zhongyi Liu",
      "Xianpei Han",
      "Le Sun"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.105": {
    "title": "MGCL: Multi-Granularity Clue Learning for Emotion-Cause Pair Extraction via Cross-Grained Knowledge Distillation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Yu",
      "Xin Lin",
      "Changqun Li",
      "Shizhou Huang",
      "Liang He"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.106": {
    "title": "Efficient Data Generation for Source-grounded Information-seeking Dialogs: A Use Case for Meeting Transcripts",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lotem Golany",
      "Filippo Galgani",
      "Maya Mamo",
      "Nimrod Parasol",
      "Omer Vandsburger",
      "Nadav Bar",
      "Ido Dagan"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.107": {
    "title": "Visual Question Decomposition on Multimodal Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haowei Zhang",
      "Jianzhe Liu",
      "Zhen Han",
      "Shuo Chen",
      "Bailan He",
      "Volker Tresp",
      "Zhiqiang Xu",
      "Jindong Gu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.108": {
    "title": "ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingming Zhuo",
      "Songyang Zhang",
      "Xinyu Fang",
      "Haodong Duan",
      "Dahua Lin",
      "Kai Chen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.109": {
    "title": "Layer-wise Importance Matters: Less Memory for Better Performance in Parameter-efficient Fine-tuning of Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Yao",
      "Penglei Gao",
      "Lichun Li",
      "Yuan Zhao",
      "Xiaofeng Wang",
      "Wei Wang",
      "Jianke Zhu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.110": {
    "title": "Abstraction-of-Thought Makes Language Models Better Reasoners",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruixin Hong",
      "Hongming Zhang",
      "Xiaoman Pan",
      "Dong Yu",
      "Changshui Zhang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.111": {
    "title": "LLMs Cannot (Yet) Match the Specificity and Simplicity of Online Communities in Long Form Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kris-Fillip Kahl",
      "Tolga Buz",
      "Russa Biswas",
      "Gerard De Melo"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.112": {
    "title": "Automated Tone Transcription and Clustering with Tone2Vec",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Yang",
      "Yiming Wang",
      "ZhiQiang Tang",
      "Jiahong Yuan"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.113": {
    "title": "Multi-dimensional Evaluation of Empathetic Dialogue Responses",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhichao Xu",
      "Jiepu Jiang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.114": {
    "title": "Translation of Multifaceted Data without Re-Training of Machine Translation Systems",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyeonseok Moon",
      "Seungyoon Lee",
      "SeongTae Hong",
      "Seungjun Lee",
      "Chanjun Park",
      "Heuiseok Lim"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.115": {
    "title": "Reward Difference Optimization For Sample Reweighting In Offline RLHF",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiqi Wang",
      "Zhengze Zhang",
      "Rui Zhao",
      "Fei Tan",
      "Nguyen Cam-Tu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.116": {
    "title": "AgentBank: Towards Generalized LLM Agents via Fine-Tuning on 50000+ Interaction Trajectories",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Song",
      "Weimin Xiong",
      "Xiutian Zhao",
      "Dawei Zhu",
      "Wenhao Wu",
      "Ke Wang",
      "Cheng Li",
      "Wei Peng",
      "Sujian Li"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.117": {
    "title": "Are LLMs Aware that Some Questions are not Open-ended?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongjie Yang",
      "Hai Zhao"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.118": {
    "title": "Conditional Language Policy: A General Framework For Steerable Multi-Objective Finetuning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiwen Wang",
      "Rahul Kidambi",
      "Ryan Sullivan",
      "Alekh Agarwal",
      "Christoph Dann",
      "Andrea Michi",
      "Marco Gelmi",
      "Yunxuan Li",
      "Raghav Gupta",
      "Kumar Dubey",
      "Alexandre Rame",
      "Johan Ferret",
      "Geoffrey Cideron",
      "Le Hou",
      "Hongkun Yu",
      "Amr Ahmed",
      "Aranyak Mehta",
      "Leonard Hussenot",
      "Olivier Bachem",
      "Edouard Leurent"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.119": {
    "title": "DALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer's Disease Questions with Scientific Literature",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dawei Li",
      "Shu Yang",
      "Zhen Tan",
      "Jae Baik",
      "Sukwon Yun",
      "Joseph Lee",
      "Aaron Chacko",
      "Bojian Hou",
      "Duy Duong-Tran",
      "Ying Ding",
      "Huan Liu",
      "Li Shen",
      "Tianlong Chen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.120": {
    "title": "Can AI Relate: Testing Large Language Model Response for Mental Health Support",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saadia Gabriel",
      "Isha Puri",
      "Xuhai Xu",
      "Matteo Malgaroli",
      "Marzyeh Ghassemi"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.121": {
    "title": "Towards Robust Extractive Question Answering Models: Rethinking the Training Methodology",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Son Tran",
      "Matt Kretchmar"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.122": {
    "title": "Enhancing Polyglot Voices by Leveraging Cross-Lingual Fine-Tuning in Any-to-One Voice Conversion",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giuseppe Ruggiero",
      "Matteo Testa",
      "Jurgen Walle",
      "Luigi Di Caro"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.123": {
    "title": "IntentionQA: A Benchmark for Evaluating Purchase Intention Comprehension Abilities of Language Models in E-commerce",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenxuan Ding",
      "Weiqi Wang",
      "Sze Kwok",
      "Minghao Liu",
      "Tianqing Fang",
      "Jiaxin Bai",
      "Xin Liu",
      "Changlong Yu",
      "Zheng Li",
      "Chen Luo",
      "Qingyu Yin",
      "Bing Yin",
      "Junxian He",
      "Yangqiu Song"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.124": {
    "title": "Draft on the Fly: Adaptive Self-Speculative Decoding using Cosine Similarity",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Metel",
      "Peng Lu",
      "Boxing Chen",
      "Mehdi Rezagholizadeh",
      "Ivan Kobyzev"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.125": {
    "title": "EconLogicQA: A Question-Answering Benchmark for Evaluating Large Language Models in Economic Sequential Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinzhu Quan",
      "Zefang Liu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.126": {
    "title": "The Base-Rate Effect on LLM Benchmark Performance: Disambiguating Test-Taking Strategies from Benchmark Performance",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyle Moore",
      "Jesse Roberts",
      "Thao Pham",
      "Oseremhen Ewaleifoh",
      "Douglas Fisher"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.127": {
    "title": "Can LLM Graph Reasoning Generalize beyond Pattern Memorization?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yizhuo Zhang",
      "Heng Wang",
      "Shangbin Feng",
      "Zhaoxuan Tan",
      "Xiaochuang Han",
      "Tianxing He",
      "Yulia Tsvetkov"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.128": {
    "title": "Improving Multilingual Instruction Finetuning via Linguistically Natural and Diverse Datasets",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sathish Reddy Indurthi",
      "Wenxuan Zhou",
      "Shamil Chollampatt",
      "Ravi Agrawal",
      "Kaiqiang Song",
      "Lingxiao Zhao",
      "Chenguang Zhu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.129": {
    "title": "ASTE-Transformer: Modelling Dependencies in Aspect-Sentiment Triplet Extraction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Iwo Naglik",
      "Mateusz Lango"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.130": {
    "title": "Faithful and Plausible Natural Language Explanations for Image Classification: A Pipeline Approach",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adam Wojciechowski",
      "Mateusz Lango",
      "Ondrej Dusek"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.131": {
    "title": "SynTQA: Synergistic Table-based Question Answering via Mixture of Text-to-SQL and E2E TQA",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyue Zhang",
      "Anh Tuan Luu",
      "Chen Zhao"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.132": {
    "title": "OpenGraph: Towards Open Graph Foundation Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lianghao Xia",
      "Ben Kao",
      "Chao Huang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.133": {
    "title": "Controlling Risk of Retrieval-augmented Generation: A Counterfactual Prompting Framework",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lu Chen",
      "Ruqing Zhang",
      "Jiafeng Guo",
      "Yixing Fan",
      "Xueqi Cheng"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.134": {
    "title": "Learning to Paraphrase for Alignment with LLM Preference",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junbo Fu",
      "Guoshuai Zhao",
      "Yimin Deng",
      "Yunqi Mi",
      "Xueming Qian"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.135": {
    "title": "Mirror-Consistency: Harnessing Inconsistency in Majority Voting",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyuan Huang",
      "Zhiyuan Ma",
      "Jintao Du",
      "Changhua Meng",
      "Weiqiang Wang",
      "Zhouhan Lin"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.136": {
    "title": "Adaptive Contrastive Decoding in Retrieval-Augmented Generation for Handling Noisy Contexts",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youna Kim",
      "Hyuhng Joon Kim",
      "Cheonbok Park",
      "Choonghyun Park",
      "Hyunsoo Cho",
      "Junyeob Kim",
      "Kang Min Yoo",
      "Sang-goo Lee",
      "Taeuk Kim"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.137": {
    "title": "AnyTrans: Translate AnyText in the Image with Large Scale Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhipeng Qian",
      "Pei Zhang",
      "Baosong Yang",
      "Kai Fan",
      "Yiwei Ma",
      "Derek Wong",
      "Xiaoshuai Sun",
      "Rongrong Ji"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.138": {
    "title": "In-Context Former: Lightning-fast Compressing Context for Large Language Model",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangfeng Wang",
      "Zaiyi Chen",
      "Tong Xu",
      "Zheyong Xie",
      "Yongyi He",
      "Enhong Chen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.139": {
    "title": "How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenhong Zhou",
      "Haiyang Yu",
      "Xinghua Zhang",
      "Rongwu Xu",
      "Fei Huang",
      "Yongbin Li"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.140": {
    "title": "A Coarse-to-Fine Prototype Learning Approach for Multi-Label Few-Shot Intent Detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaotong Zhang",
      "Xinyi Li",
      "Feng Zhang",
      "Zhiyi Wei",
      "Junfeng Liu",
      "Han Liu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.141": {
    "title": "Can Large Language Models Understand DL-Lite Ontologies? An Empirical Study",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keyu Wang",
      "Guilin Qi",
      "Jiaqi Li",
      "Songlin Zhai"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.142": {
    "title": "Enhancing Healthcare LLM Trust with Atypical Presentations Recalibration",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeremy Qin",
      "Bang Liu",
      "Quoc Nguyen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.143": {
    "title": "EvoR: Evolving Retrieval for Code Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongjin Su",
      "Shuyang Jiang",
      "Yuhang Lai",
      "Haoyuan Wu",
      "Boao Shi",
      "Che Liu",
      "Qian Liu",
      "Tao Yu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.144": {
    "title": "Head-wise Shareable Attention for Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zouying Cao",
      "Yifei Yang",
      "Hai Zhao"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.145": {
    "title": "Divide-or-Conquer? Which Part Should You Distill Your LLM?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuofeng Wu",
      "Richard Bai",
      "Aonan Zhang",
      "Jiatao Gu",
      "V.G.Vinod Vydiswaran",
      "Navdeep Jaitly",
      "Yizhe Zhang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.146": {
    "title": "Navigating the Shortcut Maze: A Comprehensive Analysis of Shortcut Learning in Text Classification by Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuqing Zhou",
      "Ruixiang Tang",
      "Ziyu Yao",
      "Ziwei Zhu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.147": {
    "title": "Privacy Evaluation Benchmarks for NLP Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Huang",
      "Yinggui Wang",
      "Cen Chen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.148": {
    "title": "MM-ChatAlign: A Novel Multimodal Reasoning Framework based on Large Language Models for Entity Alignment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuhui Jiang",
      "Yinghan Shen",
      "Zhichao Shi",
      "Chengjin Xu",
      "Wei Li",
      "Huang Zihe",
      "Jian Guo",
      "Yuanzhuo Wang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.149": {
    "title": "Towards Explainable Computerized Adaptive Testing with Large Language Model",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Cheng",
      "GuanHao Zhao",
      "Zhenya Huang",
      "Yan Zhuang",
      "Zhaoyuan Pan",
      "Qi Liu",
      "Xin Li",
      "Enhong Chen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.150": {
    "title": "MC-indexing: Effective Long Document Retrieval via Multi-view Content-aware Indexing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kuicai Dong",
      "Derrick Goh Xin Deik",
      "Yi Lee",
      "Hao Zhang",
      "Xiangyang Li",
      "Cong Zhang",
      "Yong Liu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.151": {
    "title": "PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken Dialogue Systems",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kentaro Mitsui",
      "Koh Mitsuda",
      "Toshiaki Wakatsuki",
      "Yukiya Hono",
      "Kei Sawada"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.152": {
    "title": "Correct after Answer: Enhancing Multi-Span Question Answering with Post-Processing Method",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayi Lin",
      "Chenyang Zhang",
      "Haibo Tong",
      "Dongyu Zhang",
      "Qingqing Hong",
      "Bingxuan Hou",
      "Junli Wang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.153": {
    "title": "Are Large Language Models (LLMs) Good Social Predictors?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiqi Yang",
      "Hang Li",
      "Hongzhi Wen",
      "Tai-Quan Peng",
      "Jiliang Tang",
      "Hui Liu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.154": {
    "title": "Bahasa Harmony: A Comprehensive Dataset for Bahasa Text-to-Speech Synthesis with Discrete Codec Modeling of EnGen-TTS",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Onkar Susladkar",
      "Vishesh Tripathi",
      "Biddwan Ahmed"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.155": {
    "title": "MINERS: Multilingual Language Models as Semantic Retrievers",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Genta Winata",
      "Ruochen Zhang",
      "David Adelani"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.156": {
    "title": "BoolQuestions: Does Dense Retrieval Understand Boolean Logic in Language?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zongmeng Zhang",
      "Jinhua Zhu",
      "Wengang Zhou",
      "Xiang Qi",
      "Peng Zhang",
      "Houqiang Li"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.157": {
    "title": "McCrolin: Multi-consistency Cross-lingual Training for Retrieval Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peerat Limkonchotiwat",
      "Wuttikorn Ponwitayarat",
      "Lalita Lowphansirikul",
      "Potsawee Manakul",
      "Can Udomcharoenchaikit",
      "Ekapol Chuangsuwanich",
      "Sarana Nutanong"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.158": {
    "title": "A Novel Metric for Measuring the Robustness of Large Language Models in Non-adversarial Scenarios",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Ackerman",
      "Ella Rabinovich",
      "Eitan Farchi",
      "Ateret Anaby Tavor"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.159": {
    "title": "Learning Musical Representations for Music Performance Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingjian Diao",
      "Chunhui Zhang",
      "Tingxuan Wu",
      "Ming Cheng",
      "Zhongyu Ouyang",
      "Weiyi Wu",
      "Jiang Gui"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.160": {
    "title": "Transfer Learning for Text Classification via Model Risk Analysis",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujie Sun",
      "Chuyi Fan",
      "Qun Chen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.161": {
    "title": "Typos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sukmin Cho",
      "Soyeong Jeong",
      "Jeongyeon Seo",
      "Taeho Hwang",
      "Jong Park"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.162": {
    "title": "Enhancing Temporal Modeling of Video LLMs via Time Gating",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zi-Yuan Hu",
      "Yiwu Zhong",
      "Shijia Huang",
      "Michael Lyu",
      "Liwei Wang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.163": {
    "title": "AlignedCoT: Prompting Large Language Models via Native-Speaking Demonstrations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhicheng Yang",
      "Yinya Huang",
      "Jing Xiong",
      "Liang Feng",
      "Xiaodan Liang",
      "Yiwei Wang",
      "Jing Tang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.164": {
    "title": "On the Empirical Complexity of Reasoning and Planning in LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liwei Kang",
      "Zirui Zhao",
      "David Hsu",
      "Wee Lee"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.165": {
    "title": "Learning from Mistakes: Iterative Prompt Relabeling for Text-to-Image Diffusion Model Training",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyan Chen",
      "Jiaxin Ge",
      "Tianjun Zhang",
      "Jiaming Liu",
      "Shanghang Zhang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.166": {
    "title": "Are modern neural ASR architectures robust for polysynthetic languages?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eric Le Ferrand",
      "Zoey Liu",
      "Antti Arppe",
      "Emily Prud’hommeaux"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.167": {
    "title": "A Notion of Complexity for Theory of Mind via Discrete World Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "X. Huang",
      "Emanuele La Malfa",
      "Samuele Marro",
      "Andrea Asperti",
      "Anthony Cohn",
      "Michael Wooldridge"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.168": {
    "title": "Learning Dynamic Multi-attribute Interest for Personalized Product Search",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yutong Bai",
      "Zhicheng Dou",
      "Ji-Rong Wen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.169": {
    "title": "Evaluating Automatic Metrics with Incremental Machine Translation Systems",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guojun Wu",
      "Shay Cohen",
      "Rico Sennrich"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.170": {
    "title": "LLM-Based Offline Learning for Embodied Agents via Consistency-Guided Reward Ensemble",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujeong Lee",
      "Sangwoo Shin",
      "Wei-Jin Park",
      "Honguk Woo"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.171": {
    "title": "Self-Renewal Prompt Optimizing with Implicit Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihan Liang",
      "Ben Chen",
      "Zhuoran Ran",
      "Zihan Wang",
      "Huangyu Dai",
      "Yufei Ma",
      "Dehong Gao",
      "Xiaoyan Cai",
      "Libin Yang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.172": {
    "title": "Ruler: A Model-Agnostic Method to Control Generated Length for Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaming Li",
      "Lei Zhang",
      "Yunshui Li",
      "Ziqiang Liu",
      "Yuelin Bai",
      "Run Luo",
      "Longze Chen",
      "Min Yang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.173": {
    "title": "Women Are Beautiful, Men Are Leaders: Gender Stereotypes in Machine Translation and Language Modeling",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matúš Pikuliak",
      "Stefan Oresko",
      "Andrea Hrckova",
      "Marian Simko"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.174": {
    "title": "Recent Trends in Linear Text Segmentation: A Survey",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Iacopo Ghinassi",
      "Lin Wang",
      "Chris Newell",
      "Matthew Purver"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.175": {
    "title": "mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anwen Hu",
      "Haiyang Xu",
      "Jiabo Ye",
      "Ming Yan",
      "Liang Zhang",
      "Bo Zhang",
      "Ji Zhang",
      "Qin Jin",
      "Fei Huang",
      "Jingren Zhou"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.176": {
    "title": "Exploring Question Guidance and Answer Calibration for Visually Grounded Video Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanxing Xu",
      "Yuting Wei",
      "Shuai Zhong",
      "Xinming Chen",
      "Jinsheng Qi",
      "Bin Wu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.177": {
    "title": "LoRAN: Improved Low-Rank Adaptation by a Non-Linear Transformation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinqiao Li",
      "Linqi Song",
      "Hanxu Hou"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.178": {
    "title": "Large Language Models are Limited in Out-of-Context Knowledge Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peng Hu",
      "Changjiang Gao",
      "Ruiqi Gao",
      "Jiajun Chen",
      "Shujian Huang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.179": {
    "title": "BiKT: Enabling Bidirectional Knowledge Transfer Between Pretrained Models and Sequential Downstream Tasks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hang Zeng",
      "Chaoyue Niu",
      "Fan Wu",
      "Shaojie Tang",
      "Leihao Pei",
      "Chengfei Lv",
      "Guihai Chen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.180": {
    "title": "Double-Checker: Large Language Model as a Checker for Few-shot Named Entity Recognition",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Chen",
      "Lili Zhao",
      "Zhi Zheng",
      "Tong Xu",
      "Yang Wang",
      "Enhong Chen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.181": {
    "title": "Scaling Sentence Embeddings with Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ting Jiang",
      "Shaohan Huang",
      "Zhongzhi Luan",
      "Deqing Wang",
      "Fuzhen Zhuang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.182": {
    "title": "Exploring the Relationship between In-Context Learning and Instruction Tuning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanyu Duan",
      "Yixuan Tang",
      "Yi Yang",
      "Ahmed Abbasi",
      "Kar Yan Tam"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.183": {
    "title": "Granular Entity Mapper: Advancing Fine-grained Multimodal Named Entity Recognition and Grounding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziqi Wang",
      "Chen Zhu",
      "Zhi Zheng",
      "Xinhang Li",
      "Tong Xu",
      "Yongyi He",
      "Qi Liu",
      "Ying Yu",
      "Enhong Chen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.184": {
    "title": "JobFair: A Framework for Benchmarking Gender Hiring Bias in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ze Wang",
      "Zekun Wu",
      "Xin Guan",
      "Michael Thaler",
      "Adriano Koshiyama",
      "Skylar Lu",
      "Sachin Beepath",
      "Ediz Ertekin",
      "Maria Perez-Ortiz"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.185": {
    "title": "Contrastive Token Learning with Similarity Decay for Repetition Suppression in Machine Translation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huangyu Dai",
      "Ben Chen",
      "Kaidi Chen",
      "Ying Han",
      "Zihan Liang",
      "Wen Jiang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.186": {
    "title": "A Psycholinguistic Evaluation of Language Models' Sensitivity to Argument Roles",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eun-Kyoung Lee",
      "Sathvik Nair",
      "Naomi Feldman"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.187": {
    "title": "Tending Towards Stability: Convergence Challenges in Small Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Richard Diehl Martinez",
      "Pietro Lesci",
      "Paula Buttery"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.188": {
    "title": "Be a Multitude to Itself: A Prompt Evolution Framework for Red Teaming",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Li",
      "Peiyi Wang",
      "Jingyuan Ma",
      "Di Zhang",
      "Lei Sha",
      "Zhifang Sui"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.189": {
    "title": "Modeling News Interactions and Influence for Financial Market Prediction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengyu Wang",
      "Shay Cohen",
      "Tiejun Ma"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.190": {
    "title": "Multi-Stage Balanced Distillation: Addressing Long-Tail Challenges in Sequence-Level Knowledge Distillation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhang Zhou",
      "Jing Zhu",
      "Paiheng Xu",
      "Xiaoyu Liu",
      "Xiyao Wang",
      "Danai Koutra",
      "Wei Ai",
      "Furong Huang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.191": {
    "title": "Are Large Vision Language Models up to the Challenge of Chart Comprehension and Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammed Saidul Islam",
      "Raian Rahman",
      "Ahmed Masry",
      "Md Tahmid Rahman Laskar",
      "Mir Tafseer Nayeem",
      "Enamul Hoque"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.192": {
    "title": "HoneyComb: A Flexible LLM-Based Agent System for Materials Science",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huan Zhang",
      "Yu Song",
      "Ziyu Hou",
      "Santiago Miret",
      "Bang Liu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.193": {
    "title": "Revealing COVID-19's Social Dynamics: Diachronic Semantic Analysis of Vaccine and Symptom Discourse on Twitter",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeqiang Wang",
      "Jiageng Wu",
      "Yuqi Wang",
      "Wei Xjtlu",
      "Jie Yang",
      "Nishanth Sastry",
      "Jon Johnson",
      "Suparna De"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.194": {
    "title": "Divide and Conquer: Legal Concept-guided Criminal Court View Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Xu",
      "Xiao Wei",
      "Hang Yu",
      "Qian Liu",
      "Hao Fei"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.195": {
    "title": "Data Diversity Matters for Robust Instruction Tuning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Bukharin",
      "Shiyang Li",
      "Zhengyang Wang",
      "Jingfeng Yang",
      "Bing Yin",
      "Xian Li",
      "Chao Zhang",
      "Tuo Zhao",
      "Haoming Jiang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.196": {
    "title": "GE2PE: Persian End-to-End Grapheme-to-Phoneme Conversion",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elnaz Rahmati",
      "Hossein Sameti"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.197": {
    "title": "Characterizing LLM Abstention Behavior in Science QA with Context Perturbations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bingbing Wen",
      "Bill Howe",
      "Lucy Wang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.198": {
    "title": "Plausibly Problematic Questions in Multiple-Choice Benchmarks for Commonsense Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shramay Palta",
      "Nishant Balepur",
      "Peter Rankel",
      "Sarah Wiegreffe",
      "Marine Carpuat",
      "Rachel Rudinger"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.199": {
    "title": "Cost-Efficient Subjective Task Annotation and Modeling through Few-Shot Annotator Adaptation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Preni Golazizian",
      "Alireza Salkhordeh Ziabari",
      "Ali Omrani",
      "Morteza Dehghani"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.200": {
    "title": "EDEN: Empathetic Dialogues for English Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyan Li",
      "Teresa Shao",
      "Zhou Yu",
      "Julia Hirschberg"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.201": {
    "title": "Language Models Still Struggle to Zero-shot Reason about Time Series",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mike Merrill",
      "Mingtian Tan",
      "Vinayak Gupta",
      "Thomas Hartvigsen",
      "Tim Althoff"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.202": {
    "title": "Enhancing Agent Learning through World Dynamics Modeling",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan Sun",
      "Haochen Shi",
      "Marc-Alexandre Côté",
      "Glen Berseth",
      "Xingdi Yuan",
      "Bang Liu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.203": {
    "title": "NormTab: Improving Symbolic Reasoning in LLMs Through Tabular Data Normalization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Nahid",
      "Davood Rafiei"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.204": {
    "title": "Zero-Resource Hallucination Prevention for Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyu Luo",
      "Cao Xiao",
      "Fenglong Ma"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.205": {
    "title": "Measuring and Improving Attentiveness to Partial Inputs with Counterfactuals",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanai Elazar",
      "Bhargavi Paranjape",
      "Hao Peng",
      "Sarah Wiegreffe",
      "Khyathi Chandu",
      "Vivek Srikumar",
      "Sameer Singh",
      "Noah Smith"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.206": {
    "title": "LaRS: Latent Reasoning Skills for Chain-of-Thought Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zifan Xu",
      "Haozhu Wang",
      "Dmitriy Bespalov",
      "Xian Wu",
      "Peter Stone",
      "Yanjun Qi"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.207": {
    "title": "TROPE: TRaining-Free Object-Part Enhancement for Seamlessly Improving Fine-Grained Zero-Shot Image Captioning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joshua Feinglass",
      "Yezhou Yang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.208": {
    "title": "The Craft of Selective Prediction: Towards Reliable Case Outcome Classification - An Empirical Study on European Court of Human Rights Cases",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Santosh T.y.s.s",
      "Irtiza Chowdhury",
      "Shanshan Xu",
      "Matthias Grabmair"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.209": {
    "title": "InfuserKI: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fali Wang",
      "Runxue Bao",
      "Suhang Wang",
      "Wenchao Yu",
      "Yanchi Liu",
      "Wei Cheng",
      "Haifeng Chen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.210": {
    "title": "SummaCoz: A Dataset for Improving the Interpretability of Factual Consistency Detection for Summarization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ge Luo",
      "Weisi Fan",
      "Miaoran Li",
      "Guoruizhe Sun",
      "Runlong Zhang",
      "Chenyu Xu",
      "Forrest Bao"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.211": {
    "title": "Precision or Recall? An Analysis of Image Captions for Training Text-to-Image Generation Model",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng Cheng",
      "Maitreya Patel",
      "Yezhou Yang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.212": {
    "title": "Deciphering the Factors Influencing the Efficacy of Chain-of-Thought: Probability, Memorization, and Noisy Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akshara Prabhakar",
      "Thomas Griffiths",
      "R. McCoy"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.213": {
    "title": "Self-contradictory reasoning evaluation and detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyi Liu",
      "Soumya Sanyal",
      "Isabelle Lee",
      "Yongkang Du",
      "Rahul Gupta",
      "Yang Liu",
      "Jieyu Zhao"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.214": {
    "title": "Incorporating Precedents for Legal Judgement Prediction on European Court of Human Rights Cases",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Santosh T.y.s.s",
      "Mohamed Elganayni",
      "Stanisław Sójka",
      "Matthias Grabmair"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.215": {
    "title": "Molecular Facts: Desiderata for Decontextualization in LLM Fact Verification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anisha Gunjal",
      "Greg Durrett"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.216": {
    "title": "MoleculeQA: A Dataset to Evaluate Factual Accuracy in Molecular Comprehension",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyu Lu",
      "He Cao",
      "Zijing Liu",
      "Shengyuan Bai",
      "Leqing Chen",
      "Yuan Yao",
      "Hai-Tao Zheng",
      "Yu Li"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.217": {
    "title": "Sanitizing Large Language Models in Bug Detection with Data-Flow",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengpeng Wang",
      "Wuqi Zhang",
      "Zian Su",
      "Xiangzhe Xu",
      "Xiangyu Zhang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.218": {
    "title": "Scaling Behavior for Large Language Models regarding Numeral Systems: An Example using Pythia",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhejian Zhou",
      "JIayu Wang",
      "Dahua Lin",
      "Kai Chen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.219": {
    "title": "When and Where Did it Happen? An Encoder-Decoder Model to Identify Scenario Context",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Enrique Noriega-Atala",
      "Robert Vacareanu",
      "Salena Ashton",
      "Adarsh Pyarelal",
      "Clayton Morrison",
      "Mihai Surdeanu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.220": {
    "title": "Enhancing Incremental Summarization with Structured Representations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "EunJeong Hwang",
      "Yichao Zhou",
      "James Wendt",
      "Beliz Gunel",
      "Nguyen Vo",
      "Jing Xie",
      "Sandeep Tata"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.221": {
    "title": "Med-MoE: Mixture of Domain-Specific Experts for Lightweight Medical Vision-Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Songtao Jiang",
      "Tuo Zheng",
      "Yan Zhang",
      "Yeying Jin",
      "Li Yuan",
      "Zuozhu Liu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.222": {
    "title": "Multiple Knowledge-Enhanced Interactive Graph Network for Multimodal Conversational Emotion Recognition",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Geng Tu",
      "Jun Wang",
      "Zhenyu Li",
      "Shiwei Chen",
      "Bin Liang",
      "Xi Zeng",
      "Min Yang",
      "Ruifeng Xu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.223": {
    "title": "AutoRAG-HP: Automatic Online Hyper-Parameter Tuning for Retrieval-Augmented Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jia Fu",
      "Xiaoting Qin",
      "Fangkai Yang",
      "Lu Wang",
      "Jue Zhang",
      "Qingwei Lin",
      "Yubo Chen",
      "Dongmei Zhang",
      "Saravan Rajmohan",
      "Qi Zhang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.224": {
    "title": "Unleashing the Potential of Large Language Models through Spectral Modulation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peng Sun",
      "Yao Zhu",
      "Yunjian Zhang",
      "Xiu Yan",
      "Zizhe Wang",
      "Xiangyang Ji"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.225": {
    "title": "LinguAlchemy: Fusing Typological and Geographical Elements for Unseen Language Generalization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Adilazuarda",
      "Samuel Cahyawijaya",
      "Genta Winata",
      "Ayu Purwarianti",
      "Alham Aji"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.226": {
    "title": "QUEST: Efficient Extreme Multi-Label Text Classification with Large Language Models on Commodity Hardware",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuang Zhou",
      "Junnan Dong",
      "Xiao Huang",
      "Zirui Liu",
      "Kaixiong Zhou",
      "Zhaozhuo Xu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.227": {
    "title": "UniSumEval: Towards Unified, Fine-grained, Multi-dimensional Summarization Evaluation for LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuho Lee",
      "Taewon Yun",
      "Jason Cai",
      "Hang Su",
      "Hwanjun Song"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.228": {
    "title": "Enhancing Arguments Recognition for Financial Mathematical Reasoning over Hybrid Data",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinsu Lim",
      "Yechan Hwang",
      "Young-Jun Lee",
      "Ho-Jin Choi"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.229": {
    "title": "Bi-DCSpell: A Bi-directional Detector-Corrector Interactive Framework for Chinese Spelling Check",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiming Wu",
      "Hanqing Zhang",
      "Richeng Xuan",
      "Dawei Song"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.230": {
    "title": "CLongEval: A Chinese Benchmark for Evaluating Long-Context Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zexuan Qiu",
      "Jingjing Li",
      "Shijue Huang",
      "Xiaoqi Jiao",
      "Wanjun Zhong",
      "Irwin King"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.231": {
    "title": "Guided Profile Generation Improves Personalization with Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiarui Zhang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.232": {
    "title": "mABC: Multi-Agent Blockchain-inspired Collaboration for Root Cause Analysis in Micro-Services Architecture",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Zhang",
      "Hongcheng Guo",
      "Jian Yang",
      "Zhoujin Tian",
      "Yi Zhang",
      "Yan Chaoran",
      "Zhoujun Li",
      "Tongliang Li",
      "Xu Shi",
      "Liangfan Zheng",
      "Bo Zhang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.233": {
    "title": "Taking a Deep Breath: Enhancing Language Modeling of Large Language Models with Sentinel Tokens",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiyao Luo",
      "Suncong Zheng",
      "Heming Xia",
      "Weikang Wang",
      "Yan Lei",
      "Tianyu Liu",
      "Shuang Chen",
      "Zhifang Sui"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.234": {
    "title": "Reward Modeling Requires Automatic Adjustment Based on Data Quality",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Binghai Wang",
      "Rui Zheng",
      "Lu Chen",
      "Zhiheng Xi",
      "Wei Shen",
      "Yuhao Zhou",
      "Dong Yan",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.235": {
    "title": "LOOK-M: Look-Once Optimization in KV Cache for Efficient Multimodal Long-Context Inference",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongwei Wan",
      "Ziang Wu",
      "Che Liu",
      "Jinfa Huang",
      "Zhihong Zhu",
      "Peng Jin",
      "Longyue Wang",
      "Li Yuan"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.236": {
    "title": "The Fall of ROME: Understanding the Collapse of LLMs in Model Editing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wanli Yang",
      "Fei Sun",
      "Jiajun Tan",
      "Xinyu Ma",
      "Du Su",
      "Dawei Yin",
      "Huawei Shen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.237": {
    "title": "OneGen: Efficient One-Pass Unified Generation and Retrieval for LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jintian Zhang",
      "Cheng Peng",
      "Mengshu Sun",
      "Xiang Chen",
      "Lei Liang",
      "Zhiqiang Zhang",
      "Jun Zhou",
      "Huajun Chen",
      "Ningyu Zhang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.238": {
    "title": "Self-Evolution Fine-Tuning for Policy Optimization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruijun Chen",
      "Jiehao Liang",
      "Shiping Gao",
      "Fanqi Wan",
      "Xiaojun Quan"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.239": {
    "title": "Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingyu Yin",
      "Xuzheng He",
      "Chak Tou Leong",
      "Fan Wang",
      "Yanzhao Yan",
      "Xiaoyu Shen",
      "Qiang Zhang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.240": {
    "title": "Adaptive Feature-based Low-Rank Compression of Large Language Models via Bayesian Optimization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixin Ji",
      "Yang Xiang",
      "Juntao Li",
      "Qingrong Xia",
      "Zi Ye",
      "Xinyu Duan",
      "Zhefeng Wang",
      "Kehai Chen",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.241": {
    "title": "Emosical: An Emotion-Annotated Musical Theatre Dataset",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hayoon Kim",
      "Ahyeon Choi",
      "Sungho Lee",
      "Hyun Jung",
      "Kyogu Lee"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.242": {
    "title": "Inference-Time Language Model Alignment via Integrated Value Guidance",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhixuan Liu",
      "Zhanhui Zhou",
      "Yuanfu Wang",
      "Chao Yang",
      "Yu Qiao"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.243": {
    "title": "TongGu: Mastering Classical Chinese Understanding with Knowledge-Grounded Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahuan Cao",
      "Dezhi Peng",
      "Peirong Zhang",
      "Yongxin Shi",
      "Yang Liu",
      "Kai Ding",
      "Lianwen Jin"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.244": {
    "title": "NegotiationToM: A Benchmark for Stress-testing Machine Theory of Mind on Negotiation Surrounding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chunkit Chan",
      "Cheng Jiayang",
      "Yauwai Yim",
      "Zheye Deng",
      "Wei Fan",
      "Haoran Li",
      "Xin Liu",
      "Hongming Zhang",
      "Weiqi Wang",
      "Yangqiu Song"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.245": {
    "title": "A Robust Dual-debiasing VQA Model based on Counterfactual Causal Effect",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingyun Song",
      "Chengkun Yang",
      "Xuanyu Li",
      "Xuequn Shang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.246": {
    "title": "PyramidCodec: Hierarchical Codec for Long-form Music Generation in Audio Domain",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianyi Chen",
      "Zheqi Dai",
      "Zhen Ye",
      "Xu Tan",
      "Qifeng Liu",
      "Yike Guo",
      "Wei Xue"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.247": {
    "title": "Beyond Persuasion: Towards Conversational Recommender System with Credible Explanations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peixin Qin",
      "Chen Huang",
      "Yang Deng",
      "Wenqiang Lei",
      "Tat-Seng Chua"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.248": {
    "title": "Revisiting Query Variation Robustness of Transformer Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tim Hagen",
      "Harrisen Scells",
      "Martin Potthast"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.249": {
    "title": "Revisiting Catastrophic Forgetting in Large Language Model Tuning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyu Li",
      "Liang Ding",
      "Meng Fang",
      "Dacheng Tao"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.250": {
    "title": "M5 – A Diverse Benchmark to Assess the Performance of Large Multimodal Models Across Multilingual and Multicultural Vision-Language Tasks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Florian Schneider",
      "Sunayana Sitaram"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.251": {
    "title": "Divine LLaMAs: Bias, Stereotypes, Stigmatization, and Emotion Representation of Religion in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Flor Plaza-del-Arco",
      "Amanda Curry",
      "Susanna Paoli",
      "Alba Cercas Curry",
      "Dirk Hovy"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.252": {
    "title": "Boosting Large Language Models with Continual Learning for Aspect-based Sentiment Analysis",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuanwen Ding",
      "Jie Zhou",
      "Liang Dou",
      "Qin Chen",
      "Yuanbin Wu",
      "Arlene Chen",
      "Liang He"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.253": {
    "title": "ProTrix: Building Models for Planning and Reasoning over Tables with Sentence Context",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zirui Wu",
      "Yansong Feng"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.254": {
    "title": "Recent Advances in Online Hate Speech Moderation: Multimodality and the Role of Large Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming Shan Hee",
      "Shivam Sharma",
      "Rui Cao",
      "Palash Nandi",
      "Preslav Nakov",
      "Tanmoy Chakraborty",
      "Roy Lee"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.255": {
    "title": "Quantifying Generative Media Bias with a Corpus of Real-world and Generated News Articles",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Filip Trhlík",
      "Pontus Stenetorp"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.256": {
    "title": "OEE-CFC: A Dataset for Open Event Extraction from Chinese Financial Commentary",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qizhi Wan",
      "Changxuan Wan",
      "Rong Hu",
      "Dexi Liu",
      "Xu Wenwu",
      "Kang Xu",
      "Zou Meihua",
      "Liu Tao",
      "Jie Yang",
      "Zhenwei Xiong"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.257": {
    "title": "Graph-tree Fusion Model with Bidirectional Information Propagation for Long Document Classification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sudipta Singha Roy",
      "Xindi Wang",
      "Robert Mercer",
      "Frank Rudzicz"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.258": {
    "title": "BookWorm: A Dataset for Character Description and Analysis",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Argyrios Papoudakis",
      "Mirella Lapata",
      "Frank Keller"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.259": {
    "title": "Leveraging Grammar Induction for Language Understanding and Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jushi Kai",
      "Shengyuan Hou",
      "Yusheng Huang",
      "Zhouhan Lin"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.260": {
    "title": "SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jushi Kai",
      "Tianhang Zhang",
      "Hai Hu",
      "Zhouhan Lin"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.261": {
    "title": "RoQLlama: A Lightweight Romanian Adapted Language Model",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "George-Andrei Dima",
      "Andrei-Marius Avram",
      "Cristian-George Craciun",
      "Dumitru-Clementin Cercel"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.262": {
    "title": "Reference-free Hallucination Detection for Large Vision-Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qing Li",
      "Jiahui Geng",
      "Chenyang Lyu",
      "Derui Zhu",
      "Maxim Panov",
      "Fakhri Karray"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.263": {
    "title": "WavLLM: Towards Robust and Adaptive Speech Large Language Model",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shujie Hu",
      "Long Zhou",
      "Shujie Liu",
      "Sanyuan Chen",
      "Lingwei Meng",
      "Hongkun Hao",
      "Jing Pan",
      "Xunying Liu",
      "Jinyu Li",
      "Sunit Sivasankaran",
      "Linquan Liu",
      "Furu Wei"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.264": {
    "title": "Learning from Implicit User Feedback, Emotions and Demographic Information in Task-Oriented and Document-Grounded Dialogues",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dominic Petrak",
      "Thy Tran",
      "Iryna Gurevych"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.265": {
    "title": "Improving Argument Effectiveness Across Ideologies using Instruction-tuned Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roxanne El Baff",
      "Khalid Khatib",
      "Milad Alshomary",
      "Kai Konen",
      "Benno Stein",
      "Henning Wachsmuth"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.266": {
    "title": "KV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayi Yuan",
      "Hongyi Liu",
      "Shaochen Zhong",
      "Yu-Neng Chuang",
      "Songchen Li",
      "Guanchu Wang",
      "Duy Le",
      "Hongye Jin",
      "Vipin Chaudhary",
      "Zhaozhuo Xu",
      "Zirui Liu",
      "Xia Hu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.267": {
    "title": "An Evaluation Mechanism of LLM-based Agents on Manipulating APIs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bing Liu",
      "Zhou Jianxiang",
      "Dan Meng",
      "Haonan Lu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.268": {
    "title": "Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhao Shi",
      "Zhiqiang Hu",
      "Yi Bin",
      "Junhua Liu",
      "Yang Yang",
      "See-Kiong Ng",
      "Lidong Bing",
      "Roy Lee"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.269": {
    "title": "Navigating the Nuances: A Fine-grained Evaluation of Vision-Language Navigation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zehao Wang",
      "Minye Wu",
      "Yixin Cao",
      "Yubo Ma",
      "Meiqi Chen",
      "Tinne Tuytelaars"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.270": {
    "title": "Re-Invoke: Tool Invocation Rewriting for Zero-Shot Tool Retrieval",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanfei Chen",
      "Jinsung Yoon",
      "Devendra Sachan",
      "Qingze Wang",
      "Vincent Cohen-Addad",
      "Mohammadhossein Bateni",
      "Chen-Yu Lee",
      "Tomas Pfister"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.271": {
    "title": "Rethinking Evaluation Methods for Machine Unlearning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leon Wichert",
      "Sandipan Sikdar"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.272": {
    "title": "Evaluating Moral Beliefs across LLMs through a Pluralistic Framework",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuelin Liu",
      "Yanfei Zhu",
      "Shucheng Zhu",
      "Pengyuan Liu",
      "Ying Liu",
      "Dong Yu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.273": {
    "title": "Knowledge Editing in Language Models via Adapted Direct Preference Optimization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amit Rozner",
      "Barak Battash",
      "Lior Wolf",
      "Ofir Lindenbaum"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.274": {
    "title": "Disentangling Questions from Query Generation for Task-Adaptive Retrieval",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yoonsang Lee",
      "Minsoo Kim",
      "Seung-won Hwang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.275": {
    "title": "Reap the Wild Wind: Detecting Media Storms in Large-Scale News Corpora",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dror Markus",
      "Effi Levi",
      "Tamir Sheafer",
      "Shaul Shenhav"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.276": {
    "title": "A Survey on Natural Language Counterfactual Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongjie Wang",
      "Xiaoqi Qiu",
      "Yu Yue",
      "Xu Guo",
      "Zhiwei Zeng",
      "Yuhong Feng",
      "Zhiqi Shen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.277": {
    "title": "Geneverse: A Collection of Open-source Multimodal Large Language Models for Genomic and Proteomic Research",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyu Liu",
      "Yijia Xiao",
      "Xiao Luo",
      "Hua Xu",
      "Wenjin Zheng",
      "Hongyu Zhao"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.278": {
    "title": "QRMeM: Unleash the Length Limitation through Question then Reflection Memory Mechanism",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Wang",
      "Heyan Huang",
      "Yixin Cao",
      "Jiahao Ying",
      "Wei Tang",
      "Chong Feng"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.279": {
    "title": "LONG2RAG: Evaluating Long-Context & Long-Form Retrieval-Augmented Generation with Key Point Recall",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zehan Qi",
      "Rongwu Xu",
      "Zhijiang Guo",
      "Cunxiang Wang",
      "Hao Zhang",
      "Wei Xu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.280": {
    "title": "IndoCL: Benchmarking Indonesian Language Development Assessment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nankai Lin",
      "Hongyan Wu",
      "Weixiong Zheng",
      "Xingming Liao",
      "Shengyi Jiang",
      "Aimin Yang",
      "Lixian Xiao"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.281": {
    "title": "Context-Driven Index Trimming: A Data Quality Perspective to Enhancing Precision of RALMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kexin Ma",
      "Ruochun Jin",
      "Wang Haotian",
      "Wang Xi",
      "Huan Chen",
      "Yuhua Tang",
      "Qian Wang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.282": {
    "title": "Counter Turing Test (CT2): Investigating AI-Generated Text Detection for Hindi - Ranking LLMs based on Hindi AI Detectability Index (ADI_hi)",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ishan Kavathekar",
      "Anku Rani",
      "Ashmit Chamoli",
      "Ponnurangam Kumaraguru",
      "Amit Sheth",
      "Amitava Das"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.283": {
    "title": "Generating Media Background Checks for Automated Source Critical Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Schlichtkrull"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.284": {
    "title": "In Defense of Structural Sparse Adapters for Concurrent LLM Serving",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junda Su",
      "Zirui Liu",
      "Zeju Qiu",
      "Weiyang Liu",
      "Zhaozhuo Xu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.285": {
    "title": "CONSTRUCTURE: Benchmarking CONcept STRUCTUre REasoning for Multimodal Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiwei Zha",
      "Xiangru Zhu",
      "Yuanyi Xu",
      "Chenghua Huang",
      "Jingping Liu",
      "Zhixu Li",
      "Xuwu Wang",
      "Yanghua Xiao",
      "Bei Yang",
      "Xiaoxiao Xu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.286": {
    "title": "Stanceformer: Target-Aware Transformer for Stance Detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Krishna Garg",
      "Cornelia Caragea"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.287": {
    "title": "Learning Autonomous Driving Tasks via Human Feedbacks with Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunsheng Ma",
      "Xu Cao",
      "Wenqian Ye",
      "Can Cui",
      "Kai Mei",
      "Ziran Wang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.288": {
    "title": "CultureBank: An Online Community-Driven Knowledge Base Towards Culturally Aware Language Technologies",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiyan Shi",
      "Ryan Li",
      "Yutong Zhang",
      "Caleb Ziems",
      "Sunny Yu",
      "Raya Horesh",
      "Rogério Paula",
      "Diyi Yang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.289": {
    "title": "TOOLVERIFIER: Generalization to New Tools via Self-Verification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dheeraj Mekala",
      "Jason Weston",
      "Jack Lanchantin",
      "Roberta Raileanu",
      "Maria Lomeli",
      "Jingbo Shang",
      "Jane Dwivedi-Yu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.290": {
    "title": "FaithScore: Fine-grained Evaluations of Hallucinations in Large Vision-Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liqiang Jing",
      "Ruosen Li",
      "Yunmo Chen",
      "Xinya Du"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.291": {
    "title": "Learning to Ask Informative Questions: Enhancing LLMs with Preference Optimization and Expected Information Gain",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Davide Mazzaccara",
      "Alberto Testoni",
      "Raffaella Bernardi"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.292": {
    "title": "Adversarial Math Word Problem Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roy Xie",
      "Chengxuan Huang",
      "Junlin Wang",
      "Bhuwan Dhingra"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.293": {
    "title": "Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Zhao",
      "Zhe Li",
      "Yige Li",
      "Ye Zhang",
      "Jun Sun"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.294": {
    "title": "Promoting Constructive Deliberation: Reframing for Receptiveness",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gauri Kambhatla",
      "Matthew Lease",
      "Ashwin Rajadesingan"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.295": {
    "title": "A Simple but Effective Approach to Improve Structured Language Model Output for Information Extraction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinghao Li",
      "Rampi Ramprasad",
      "Chao Zhang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.296": {
    "title": "Rater Cohesion and Quality from a Vicarious Perspective",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Deepak Pandita",
      "Tharindu Cyril Weerasooriya",
      "Sujan Dutta",
      "Sarah Luger",
      "Tharindu Ranasinghe",
      "Ashiqur KhudaBukhsh",
      "Marcos Zampieri",
      "Christopher Homan"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.297": {
    "title": "Shall We Team Up: Exploring Spontaneous Cooperation of Competing LLM Agents",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zengqing Wu",
      "Run Peng",
      "Shuyuan Zheng",
      "Qianying Liu",
      "Xu Han",
      "Brian Kwon",
      "Makoto Onizuka",
      "Shaojie Tang",
      "Chuan Xiao"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.298": {
    "title": "Normalized Narrow Jump To Conclusions: Normalized Narrow Shortcuts for Parameter Efficient Early Exit Transformer Prediction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amrit Seshadri"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.299": {
    "title": "From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Melissa Roemmele",
      "Andrew Gordon"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.300": {
    "title": "I Never Said That\": A dataset, taxonomy and baselines on response clarity classification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Konstantinos Thomas",
      "Giorgos Filandrianos",
      "Maria Lymperaiou",
      "Chrysoula Zerva",
      "Giorgos Stamou"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.301": {
    "title": "Immunization against harmful fine-tuning attacks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Domenic Rosati",
      "Jan Wehner",
      "Kai Williams",
      "Lukasz Bartoszcze",
      "Hassan Sajjad",
      "Frank Rudzicz"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.302": {
    "title": "UniMEEC: Towards Unified Multimodal Emotion Recognition and Emotion Cause",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guimin Hu",
      "Zhihong Zhu",
      "Daniel Hershcovich",
      "Lijie Hu",
      "Hasti Seifi",
      "Jiayuan Xie"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.303": {
    "title": "CodeFort: Robust Training for Code Generation Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhao Zhang",
      "Shiqi Wang",
      "Haifeng Qian",
      "Zijian Wang",
      "Mingyue Shang",
      "Linbo Liu",
      "Sanjay Krishna Gouda",
      "Baishakhi Ray",
      "Murali Krishna Ramanathan",
      "Xiaofei Ma",
      "Anoop Deoras"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.304": {
    "title": "MP-RNA: Unleashing Multi-species RNA Foundation Model via Calibrated Secondary Structure Prediction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heng Yang",
      "Ke Li"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.305": {
    "title": "Any Other Thoughts, Hedgehog?\" Linking Deliberation Chains in Collaborative Dialogues",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhijnan Nath",
      "Videep Venkatesha",
      "Mariah Bradford",
      "Avyakta Chelle",
      "Austin Youngren",
      "Carlos Mabrey",
      "Nathaniel Blanchard",
      "Nikhil Krishnaswamy"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.306": {
    "title": "Evaluation of Question Answer Generation for Portuguese: Insights and Datasets",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Felipe Paula",
      "Cassiana Michelin",
      "Viviane Moreira"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.307": {
    "title": "Evolutionary Contrastive Distillation for Language Model Alignment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julian Katz-Samuels",
      "Zheng Li",
      "Hyokun Yun",
      "Priyanka Nigam",
      "Yi Xu",
      "Vaclav Petricek",
      "Bing Yin",
      "Trishul Chilimbi"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.308": {
    "title": "A Fairness-Driven Method for Learning Human-Compatible Negotiation Strategies",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryan Shea",
      "Zhou Yu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.309": {
    "title": "Using RL to Identify Divisive Perspectives Improves LLMs Abilities to Identify Communities on Social Media",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikhil Mehta",
      "Dan Goldwasser"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.310": {
    "title": "Are LLMs Effective Negotiators? Systematic Evaluation of the Multifaceted Capabilities of LLMs in Negotiation Dialogues",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Deuksin Kwon",
      "Emily Weiss",
      "Tara Kulshrestha",
      "Kushal Chawla",
      "Gale Lucas",
      "Jonathan Gratch"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.311": {
    "title": "When Raw Data Prevails: Are Large Language Model Embeddings Effective in Numerical Data Representation for Medical Machine Learning Applications?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanjun Gao",
      "Skatje Myers",
      "Shan Chen",
      "Dmitriy Dligach",
      "Timothy Miller",
      "Danielle Bitterman",
      "Matthew Churpek",
      "Majid Afshar"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.312": {
    "title": "Losing Visual Needles in Image Haystacks: Vision Language Models are Easily Distracted in Short and Long Contexts",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya Sharma",
      "Michael Saxon",
      "William Yang Wang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.313": {
    "title": "Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiazheng Li",
      "Hainiu Xu",
      "Zhaoyue Sun",
      "Yuxiang Zhou",
      "David West",
      "Cesare Aloisi",
      "Yulan He"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.314": {
    "title": "LOCR: Location-Guided Transformer for Optical Character Recognition",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Sun",
      "Dongzhan Zhou",
      "Chen Lin",
      "Conghui He",
      "Wanli Ouyang",
      "Han-Sen Zhong"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.315": {
    "title": "Sing it, Narrate it: Quality Musical Lyrics Translation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuorui Ye",
      "Jinhan Li",
      "Rongwu Xu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.316": {
    "title": "Exploring Automated Keyword Mnemonics Generation with Large Language Models via Overgenerate-and-Rank",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaewook Lee",
      "Hunter McNichols",
      "Andrew Lan"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.317": {
    "title": "Dual-teacher Knowledge Distillation for Low-frequency Word Translation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Guo",
      "Hongying Zan",
      "Hongfei Xu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.318": {
    "title": "A Simple Angle-based Approach for Contrastive Learning of Unsupervised Sentence Representation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yoo Hyun Jeong",
      "Myeongsoo Han",
      "Dong-Kyu Chae"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.319": {
    "title": "Developing a Pragmatic Benchmark for Assessing Korean Legal Language Understanding in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kimyeeun Kimyeeun",
      "Choi Youngrok",
      "Eunkyung Choi",
      "JinHwan Choi",
      "Hai Park",
      "Wonseok Hwang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.320": {
    "title": "Visual Pivoting Unsupervised Multimodal Machine Translation in Low-Resource Distant Language Pairs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Turghun Tayir",
      "Lin Li",
      "Xiaohui Tao",
      "Mieradilijiang Maimaiti",
      "Ming Li",
      "Jianquan Liu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.321": {
    "title": "Scalable Fine-tuning from Multiple Data Sources: A First-Order Approximation Approach",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongyue Li",
      "Ziniu Zhang",
      "Lu Wang",
      "Hongyang Zhang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.322": {
    "title": "In-Context Learning May Not Elicit Trustworthy Reasoning: A-Not-B Errors in Pretrained Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengrui Han",
      "Peiyang Song",
      "Haofei Yu",
      "Jiaxuan You"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.323": {
    "title": "MathFish: Evaluating Language Model Math Reasoning via Grounding in Educational Curricula",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li Lucy",
      "Tal August",
      "Rose Wang",
      "Luca Soldaini",
      "Courtney Allison",
      "Kyle Lo"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.324": {
    "title": "Enhancing Multi-Label Text Classification under Label-Dependent Noise: A Label-Specific Denoising Framework",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengyu Xu",
      "Liping Jing",
      "Jian Yu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.325": {
    "title": "Automatic Reconstruction of Ancient Chinese Pronunciations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhige Huang",
      "Haoan Jin",
      "Mengyue Wu",
      "Kenny Zhu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.326": {
    "title": "Instance-Level Dynamic LoRAs Composition for Cross-Task Generalization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiqi Wang",
      "Shizhu He",
      "Kang Liu",
      "Jun Zhao"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.327": {
    "title": "LongWanjuan: Towards Systematic Measurement for Long Text Quality",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoran Liu",
      "Kai Lv",
      "Qipeng Guo",
      "Hang Yan",
      "Conghui He",
      "Xipeng Qiu",
      "Dahua Lin"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.328": {
    "title": "Large Language Model for Multi-Domain Translation: Benchmarking and Domain CoT Fine-tuning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianxiang Hu",
      "Pei Zhang",
      "Baosong Yang",
      "Jun Xie",
      "Derek Wong",
      "Rui Wang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.329": {
    "title": "TriageAgent: Towards Better Multi-Agents Collaborations for Large Language Model-Based Clinical Triage",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meng Lu",
      "Brandon Ho",
      "Dennis Ren",
      "Xuan Wang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.330": {
    "title": "Generative Deduplication For Socia Media Data Selection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xianming Li",
      "Jing Li"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.331": {
    "title": "Gender Bias in Decision-Making with Large Language Models: A Study of Relationship Conflicts",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sharon Levy",
      "William Adler",
      "Tahilin Karver",
      "Mark Dredze",
      "Michelle Kaufman"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.332": {
    "title": "Evaluating Biases in Context-Dependent Sexual and Reproductive Health Questions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sharon Levy",
      "Tahilin Karver",
      "William Adler",
      "Michelle Kaufman",
      "Mark Dredze"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.333": {
    "title": "Self-Evaluation of Large Language Model based on Glass-box Features",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hui Huang",
      "Yingqi Qu",
      "Jing Liu",
      "Muyun Yang",
      "Bing Xu",
      "Tiejun Zhao",
      "Wenpeng Lu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.334": {
    "title": "FASTTRACK: Reliable Fact Tracing via Clustering and LLM-Powered Evidence Validation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Si Chen",
      "Feiyang Kang",
      "Ning Yu",
      "Ruoxi Jia"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.335": {
    "title": "PKAD: Pretrained Knowledge is All You Need to Detect and Mitigate Textual Backdoor Attacks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Chen",
      "Qi Cao",
      "Kaike Zhang",
      "Xuchao Liu",
      "Huawei Shen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.336": {
    "title": "Merely Judging Metaphor is Not Enough: Research on Reasonable Metaphor Detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Puli Chen",
      "Cheng Yang",
      "Qingbao Huang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.337": {
    "title": "Can we teach language models to gloss endangered languages?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Ginn",
      "Mans Hulden",
      "Alexis Palmer"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.338": {
    "title": "On the token distance modeling ability of higher RoPE attention dimension",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangyu Hong",
      "Che Jiang",
      "Biqing Qi",
      "Fandong Meng",
      "Mo Yu",
      "Bowen Zhou",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.339": {
    "title": "Enhancing Byzantine-Resistant Aggregations with Client Embedding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan Zhang",
      "Hao Zhou",
      "Fandong Meng",
      "Jie Zhou",
      "Xu Sun"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.340": {
    "title": "Exploiting Careful Design of SVM Solution for Aspect-term Sentiment Analysis",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanfeng Liu",
      "Minping Chen",
      "Zhenya Zheng",
      "Zeyi Wen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.341": {
    "title": "Learning to Generate Rules for Realistic Few-Shot Relation Classification: An Encoder-Decoder Approach",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mayank Singh",
      "Eduardo Blanco"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.342": {
    "title": "Plot Twist: Multimodal Models Don't Comprehend Simple Chart Details",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yasaman Razeghi",
      "Ishita Dasgupta",
      "Fangyu Liu",
      "Vinay Ramasesh",
      "Sameer Singh"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.343": {
    "title": "HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huy Nghiem",
      "Hal Daumé Iii"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.344": {
    "title": "Giving Control Back to Models: Enabling Offensive Language Detection Models to Autonomously Identify and Mitigate Biases",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiapeng Liu",
      "Weijie Li",
      "Xiaochao Fan",
      "Wenjun Deng",
      "Liang Yang",
      "Yong Li",
      "Yufeng Diao"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.345": {
    "title": "Toolken+: Improving LLM Tool Usage with Reranking and a Reject Option",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Konstantin Yakovlev",
      "Sergey Nikolenko",
      "Andrey Bout"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.346": {
    "title": "SecureSQL: Evaluating Data Leakage of Large Language Models as Natural Language Interfaces to Databases",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanqi Song",
      "Ruiheng Liu",
      "Shu Chen",
      "Qianhao Ren",
      "Yu Zhang",
      "Yongqi Yu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.347": {
    "title": "Llama SLayer 8B: Shallow Layers Hold the Key to Knowledge Injection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianxiang Chen",
      "Zhentao Tan",
      "Tao Gong",
      "Yue Wu",
      "Qi Chu",
      "Bin Liu",
      "Jieping Ye",
      "Nenghai Yu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.348": {
    "title": "Entity or Relation Embeddings? An Analysis of Encoding Strategies for Relation Extraction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Frank Mtumbuka",
      "Steven Schockaert"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.349": {
    "title": "Self-Consistency Boosts Calibration for Math Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ante Wang",
      "Linfeng Song",
      "Ye Tian",
      "Baolin Peng",
      "Lifeng Jin",
      "Haitao Mi",
      "Jinsong Su",
      "Dong Yu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.350": {
    "title": "Distilling Instruction-following Abilities of Large Language Models with Task-aware Curriculum Planning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanhao Yue",
      "Chengyu Wang",
      "Jun Huang",
      "Peng Wang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.351": {
    "title": "On Creating an English-Thai Code-switched Machine Translation in Medical Domain",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Parinthapat Pengpun",
      "Krittamate Tiankanon",
      "Amrest Chinkamol",
      "Jiramet Kinchagawat",
      "Pitchaya Chairuengjitjaras",
      "Pasit Supholkhan",
      "Pubordee Aussavavirojekul",
      "Chiraphat Boonnag",
      "Kanyakorn Veerakanjana",
      "Hirunkul Phimsiri",
      "Boonthicha Sae-jia",
      "Nattawach Sataudom",
      "Piyalitt Ittichaiwong",
      "Peerat Limkonchotiwat"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.352": {
    "title": "CogGPT: Unleashing the Power of Cognitive Dynamics on Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaojia Lv",
      "Haojie Pan",
      "Zekun Wang",
      "Jiafeng Liang",
      "Yuanxing Liu",
      "Ruiji Fu",
      "Ming Liu",
      "Zhongyuan Wang",
      "Bing Qin"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.353": {
    "title": "Can LLMs Recognize Toxicity? A Structured Investigation Framework and Toxicity Metric",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyukhun Koh",
      "Dohyung Kim",
      "Minwoo Lee",
      "Kyomin Jung"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.354": {
    "title": "Toeing the Party Line: Election Manifestos as a Key to Understand Political Discourse on Twitter",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maximilian Maurer",
      "Tanise Ceron",
      "Sebastian Padó",
      "Gabriella Lapesa"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.355": {
    "title": "UniTabNet: Bridging Vision and Language Models for Enhanced Table Structure Recognition",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenrong Zhang",
      "Shuhang Liu",
      "Pengfei Hu",
      "Jiefeng Ma",
      "Jun Du",
      "Jianshu Zhang",
      "Yu Hu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.356": {
    "title": "PolyWER: A Holistic Evaluation Framework for Code-Switched Speech Recognition",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karima Kadaoui",
      "Maryam Ali",
      "Hawau Toyin",
      "Ibrahim Mohammed",
      "Hanan Aldarmaki"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.357": {
    "title": "A Deep Analysis of the Impact of Multiword Expressions and Named Entities on Chinese-English Machine Translations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huacheng Song",
      "Hongzhi Xu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.358": {
    "title": "SCA: Selective Compression Attention for Efficiently Extending the Context Window of Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huanran Zheng",
      "Wei Zhu",
      "Xiaoling Wang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.359": {
    "title": "FANTAstic SEquences and Where to Find Them: Faithful and Efficient API Call Generation through State-tracked Constrained Decoding and Reranking",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoer Wang",
      "Leonardo Ribeiro",
      "Alexandros Papangelis",
      "Rohan Mukherjee",
      "Tzu-Yen Wang",
      "Xinyan Zhao",
      "Arijit Biswas",
      "James Caverlee",
      "Angeliki Metallinou"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.360": {
    "title": "Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Spyridon Mouselinos",
      "Henryk Michalewski",
      "Mateusz Malinowski"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.361": {
    "title": "AdaMoE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihao Zeng",
      "Yibo Miao",
      "Hongcheng Gao",
      "Hao Zhang",
      "Zhijie Deng"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.362": {
    "title": "Learning from Relevant Subgoals in Successful Dialogs using Iterative Training for Task-oriented Dialog Systems",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Magdalena Kaiser",
      "Patrick Ernst",
      "György Szarvas"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.363": {
    "title": "CLEAR: Can Language Models Really Understand Causal Graphs?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sirui Chen",
      "Mengying Xu",
      "Kun Wang",
      "Xingyu Zeng",
      "Rui Zhao",
      "Shengjie Zhao",
      "Chaochao Lu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.364": {
    "title": "PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gyeongman Kim",
      "Doohyuk Jang",
      "Eunho Yang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.365": {
    "title": "M2QA: Multi-domain Multilingual Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leon Engländer",
      "Hannah Sterz",
      "Clifton Poth",
      "Jonas Pfeiffer",
      "Ilia Kuznetsov",
      "Iryna Gurevych"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.366": {
    "title": "Unveiling the Invisible: Captioning Videos with Metaphors",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abisek Rajakumar Kalarani",
      "Pushpak Bhattacharyya",
      "Sumit Shekhar"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.367": {
    "title": "How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ehsan Doostmohammadi",
      "Oskar Holmström",
      "Marco Kuhlmann"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.368": {
    "title": "RippleCOT: Amplifying Ripple Effect of Knowledge Editing in Language Models via Chain-of-Thought In-Context Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihao Zhao",
      "Yuchen Yang",
      "Yijiang Li",
      "Yinzhi Cao"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.369": {
    "title": "Authorship Obfuscation in Multilingual Machine-Generated Text Detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dominik Macko",
      "Robert Moro",
      "Adaku Uchendu",
      "Ivan Srba",
      "Jason Lucas",
      "Michiharu Yamashita",
      "Nafis Irtiza Tripto",
      "Dongwon Lee",
      "Jakub Simko",
      "Maria Bielikova"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.370": {
    "title": "Comparing Edge-based and Node-based Methods on a Citation Prediction Task",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peter Vickers",
      "Kenneth Church"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.371": {
    "title": "DAdEE: Unsupervised Domain Adaptation in Early Exit PLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Divya Jyoti Bajpai",
      "Manjesh Hanawal"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.372": {
    "title": "LaCo: Large Language Model Pruning via Layer Collapse",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Yang",
      "Zouying Cao",
      "Hai Zhao"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.373": {
    "title": "Llamipa: An Incremental Discourse Parser",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kate Thompson",
      "Akshay Chaturvedi",
      "Julie Hunter",
      "Nicholas Asher"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.374": {
    "title": "Nebula: A discourse aware Minecraft Builder",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akshay Chaturvedi",
      "Kate Thompson",
      "Nicholas Asher"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.375": {
    "title": "Improving Referring Ability for Biomedical Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junfeng Jiang",
      "Fei Cheng",
      "Akiko Aizawa"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.376": {
    "title": "CapEEN: Image Captioning with Early Exits and Knowledge Distillation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Divya Jyoti Bajpai",
      "Manjesh Hanawal"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.377": {
    "title": "LumberChunker: Long-Form Narrative Document Segmentation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "André Duarte",
      "João Marques",
      "Miguel Graça",
      "Miguel Freire",
      "Lei Li",
      "Arlindo Oliveira"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.378": {
    "title": "Exploring the Limits of Fine-grained LLM-based Physics Inference via Premise Removal Interventions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jordan Meadows",
      "Tamsin James",
      "Andre Freitas"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.379": {
    "title": "Unlocking Continual Learning Abilities in Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenyu Du",
      "Shuang Cheng",
      "Tongxu Luo",
      "Zihan Qiu",
      "Zeyu Huang",
      "Ka Chun Cheung",
      "Reynold Cheng",
      "Jie Fu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.380": {
    "title": "On the Rigour of Scientific Writing: Criteria, Analysis, and Insights",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joseph James",
      "Chenghao Xiao",
      "Yucheng Li",
      "Chenghua Lin"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.381": {
    "title": "MMUTF: Multimodal Multimedia Event Argument Extraction with Unified Template Filling",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Philipp Seeberger",
      "Dominik Wagner",
      "Korbinian Riedhammer"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.382": {
    "title": "Not All Preference Pairs Are Created Equal: A Recipe for Annotation-Efficient Iterative Preference Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sen Yang",
      "Leyang Cui",
      "Deng Cai",
      "Xinting Huang",
      "Shuming Shi",
      "Wai Lam"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.383": {
    "title": "Cross-lingual Contextualized Phrase Retrieval",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huayang Li",
      "Deng Cai",
      "Zhi Qu",
      "Qu Cui",
      "Hidetaka Kamigaito",
      "Lemao Liu",
      "Taro Watanabe"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.384": {
    "title": "VideoINSTA: Zero-shot Long Video Understanding via Informative Spatial-Temporal Reasoning with LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruotong Liao",
      "Max Erler",
      "Huiyu Wang",
      "Guangyao Zhai",
      "Gengyuan Zhang",
      "Yunpu Ma",
      "Volker Tresp"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.385": {
    "title": "Self-Constructed Context Decompilation with Fined-grained Alignment Enhancement",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunlong Feng",
      "Dechuan Teng",
      "Yang Xu",
      "Honglin Mu",
      "Xiao Xu",
      "Libo Qin",
      "Qingfu Zhu",
      "Wanxiang Che"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.386": {
    "title": "Efficiently Computing Susceptibility to Context in Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyu Liu",
      "Kevin Du",
      "Mrinmaya Sachan",
      "Ryan Cotterell"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.387": {
    "title": "ESG-Kor: A Korean Dataset for ESG-related Information Extraction and Practical Use Cases",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaeyoung Lee",
      "Geonyeong Son",
      "Misuk Kim"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.388": {
    "title": "Wrong-of-Thought: An Integrated Reasoning Framework with Multi-Perspective Verification and Wrong Information",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongheng Zhang",
      "Qiguang Chen",
      "Jingxuan Zhou",
      "Peng Wang",
      "Jiasheng Si",
      "Jin Wang",
      "Wenpeng Lu",
      "Libo Qin"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.389": {
    "title": "Hope ‘The Paragraph Guy' explains the rest : Introducing MeSum, the Meme Summarizer",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anas Khan",
      "Tanik Saikh",
      "Arpan Phukan",
      "Asif Ekbal"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.390": {
    "title": "Learning Semantic Structure through First-Order-Logic Translation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akshay Chaturvedi",
      "Nicholas Asher"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.391": {
    "title": "A Training Data Recipe to Accelerate A* Search with Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Devaansh Gupta",
      "Boyang Li"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.392": {
    "title": "From Generation to Selection: Findings of Converting Analogical Problem-Solving into Multiple-Choice Questions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Donghyeon Shin",
      "Seungpil Lee",
      "Klea Kovacec",
      "Sundong Kim"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.393": {
    "title": "What's under the hood: Investigating Automatic Metrics on Meeting Summarization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Frederic Kirstein",
      "Jan Philip Wahle",
      "Terry Ruas",
      "Bela Gipp"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.394": {
    "title": "Self-Distillation for Model Stacking Unlocks Cross-Lingual NLU in 200+ Languages",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fabian Schmidt",
      "Philipp Borchert",
      "Ivan Vulić",
      "Goran Glavaš"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.395": {
    "title": "CERD: A Comprehensive Chinese Rhetoric Dataset for Rhetorical Understanding and Generation in Essays",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nuowei Liu",
      "Xinhao Chen",
      "Hongyi Wu",
      "Changzhi Sun",
      "Man Lan",
      "Yuanbin Wu",
      "Xiaopeng Bai",
      "Shaoguang Mao",
      "Yan Xia"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.396": {
    "title": "An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Language Model Inference",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Atsuki Yamaguchi",
      "Aline Villavicencio",
      "Nikolaos Aletras"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.397": {
    "title": "AutoDetect: Towards a Unified Framework for Automated Weakness Detection in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiale Cheng",
      "Yida Lu",
      "Xiaotao Gu",
      "Pei Ke",
      "Xiao Liu",
      "Yuxiao Dong",
      "Hongning Wang",
      "Jie Tang",
      "Minlie Huang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.398": {
    "title": "BAPO: Base-Anchored Preference Optimization for Overcoming Forgetting in Large Language Models Personalization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gihun Lee",
      "Minchan Jeong",
      "Yujin Kim",
      "Hojung Jung",
      "Jaehoon Oh",
      "SangMook Kim",
      "Se-Young Yun"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.399": {
    "title": "Beyond Common Words: Enhancing ASR Cross-Lingual Proper Noun Recognition Using Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rishabh Kumar",
      "Sabyasachi Ghosh",
      "Ganesh Ramakrishnan"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.400": {
    "title": "Few-shot clinical entity recognition in English, French and Spanish: masked language models outperform generative model prompting",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marco Naguib",
      "Xavier Tannier",
      "Aurélie Névéol"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.401": {
    "title": "STTATTS: Unified Speech-To-Text And Text-To-Speech Model",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hawau Toyin",
      "Hao Li",
      "Hanan Aldarmaki"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.402": {
    "title": "From Text Segmentation to Enhanced Representation Learning: A Novel Approach to Multi-Label Classification for Long Texts",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wang Zhang",
      "Xin Wang",
      "Qian Wang",
      "Tao Deng",
      "Xiaoru Wu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.403": {
    "title": "Learning from Imperfect Data: Towards Efficient Knowledge Distillation of Autoregressive Language Models for Text-to-SQL",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qihuang Zhong",
      "Kunfeng Chen",
      "Liang Ding",
      "Juhua Liu",
      "Bo Du",
      "Dacheng Tao"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.404": {
    "title": "ConU: Conformal Uncertainty in Large Language Models with Correctness Coverage Guarantees",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan Wang",
      "Jinhao Duan",
      "Lu Cheng",
      "Yue Zhang",
      "Qingni Wang",
      "Xiaoshuang Shi",
      "Kaidi Xu",
      "Heng Tao Shen",
      "Xiaofeng Zhu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.405": {
    "title": "Irrelevant Alternatives Bias Large Language Model Hiring Decisions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kremena Valkanova",
      "Pencho Yordanov"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.406": {
    "title": "PclGPT: A Large Language Model for Patronizing and Condescending Language Detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongbo Wang",
      "LiMingDa LiMingDa",
      "Junyu Lu",
      "Hebin Xia",
      "Liang Yang",
      "Bo Xu",
      "Ruizhu Liu",
      "Hongfei Lin"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.407": {
    "title": "MultiAgent Collaboration Attack: Investigating Adversarial Attacks in Large Language Model Collaborations via Debate",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alfonso Amayuelas",
      "Xianjun Yang",
      "Antonis Antoniades",
      "Wenyue Hua",
      "Liangming Pan",
      "William Yang Wang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.408": {
    "title": "CEAMC: Corpus and Empirical Study of Argument Analysis in Education via LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yupei Ren",
      "Hongyi Wu",
      "Zhaoguang Long",
      "Shangqing Zhao",
      "Xinyi Zhou",
      "Zheqin Yin",
      "Xinlin Zhuang",
      "Xiaopeng Bai",
      "Man Lan"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.409": {
    "title": "Ada-Instruct: Adapting Instruction Generators for Complex Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wanyun Cui",
      "Qianle Wang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.410": {
    "title": "LINKAGE: Listwise Ranking among Varied-Quality References for Non-Factoid QA Evaluation via LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sihui Yang",
      "Keping Bi",
      "Wanqing Cui",
      "Jiafeng Guo",
      "Xueqi Cheng"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.411": {
    "title": "Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nuo Chen",
      "Zinan Zheng",
      "Ning Wu",
      "Ming Gong",
      "Dongmei Zhang",
      "Jia Li"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.412": {
    "title": "SynthEval: Hybrid Behavioral Testing of NLP Models with Synthetic Evaluation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raoyuan Zhao",
      "Abdullatif Köksal",
      "Yihong Liu",
      "Leonie Weissweiler",
      "Anna Korhonen",
      "Hinrich Schuetze"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.413": {
    "title": "TurkishMMLU: Measuring Massive Multitask Language Understanding in Turkish",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arda Yüksel",
      "Abdullatif Köksal",
      "Lütfi Kerem Senel",
      "Anna Korhonen",
      "Hinrich Schuetze"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.414": {
    "title": "LongForm: Effective Instruction Tuning with Reverse Instructions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abdullatif Köksal",
      "Timo Schick",
      "Anna Korhonen",
      "Hinrich Schuetze"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.415": {
    "title": "Explaining Graph Neural Networks with Large Language Models: A Counterfactual Perspective on Molecule Graphs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinhan He",
      "Zaiyi Zheng",
      "Patrick Soga",
      "Yaochen Zhu",
      "Yushun Dong",
      "Jundong Li"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.416": {
    "title": "Knowledge Mechanisms in Large Language Models: A Survey and Perspective",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengru Wang",
      "Yunzhi Yao",
      "Ziwen Xu",
      "Shuofei Qiao",
      "Shumin Deng",
      "Peng Wang",
      "Xiang Chen",
      "Jia-Chen Gu",
      "Yong Jiang",
      "Pengjun Xie",
      "Fei Huang",
      "Huajun Chen",
      "Ningyu Zhang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.417": {
    "title": "LongHeads: Multi-Head Attention is Secretly a Long Context Processor",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Lu",
      "Xin Zhou",
      "Wei He",
      "Jun Zhao",
      "Tao Ji",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.418": {
    "title": "Crisis counselor language and perceived genuine concern in crisis conversations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Greg Buda",
      "Ignacio Tripodi",
      "Margaret Meagher",
      "Elizabeth Olson"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.419": {
    "title": "Edit-Constrained Decoding for Sentence Simplification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tatsuya Zetsu",
      "Yuki Arase",
      "Tomoyuki Kajiwara"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.420": {
    "title": "Modeling Human Subjectivity in LLMs Using Explicit and Implicit Human Factors in Personas",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Salvatore Giorgi",
      "Tingting Liu",
      "Ankit Aich",
      "Kelsey Isman",
      "Garrick Sherman",
      "Zachary Fried",
      "João Sedoc",
      "Lyle Ungar",
      "Brenda Curtis"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.421": {
    "title": "Multi-Loss Fusion: Angular and Contrastive Integration for Machine-Generated Text Detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Iqra Zahid",
      "Yue Chang",
      "Tharindu Madusanka",
      "Youcheng Sun",
      "Riza Batista-Navarro"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.422": {
    "title": "Intermediate Layer Distillation with the Reused Teacher Classifier: A Study on the Importance of the Classifier of Attention-based Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hang Zhang",
      "Seyyed Mozafari",
      "James Clark",
      "Brett Meyer",
      "Warren Gross"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.423": {
    "title": "Enhancing Large Language Model Based Sequential Recommender Systems with Pseudo Labels Reconstruction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyunsoo Na",
      "Minseok Gang",
      "Youngrok Ko",
      "Jinseok Seol",
      "Sang-goo Lee"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.424": {
    "title": "On the Generalization of Training-based ChatGPT Detection Methods",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Xu",
      "Jie Ren",
      "Pengfei He",
      "Shenglai Zeng",
      "Yingqian Cui",
      "Amy Liu",
      "Hui Liu",
      "Jiliang Tang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.425": {
    "title": "Private prediction for large-scale synthetic text generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kareem Amin",
      "Alex Bie",
      "Weiwei Kong",
      "Alexey Kurakin",
      "Natalia Ponomareva",
      "Umar Syed",
      "Andreas Terzis",
      "Sergei Vassilvitskii"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.426": {
    "title": "Generalists vs. Specialists: Evaluating Large Language Models for Urdu",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samee Arif",
      "Abdul Hameed Azeemi",
      "Agha Ali Raza",
      "Awais Athar"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.427": {
    "title": "Improving Multi-Agent Debate with Sparse Communication Topology",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunxuan Li",
      "Yibing Du",
      "Jiageng Zhang",
      "Le Hou",
      "Peter Grabowski",
      "Yeqing Li",
      "Eugene Ie"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.428": {
    "title": "Evidence Retrieval for Fact Verification using Multi-stage Reranking",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shrikant Malviya",
      "Stamos Katsigiannis"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.429": {
    "title": "Multi-step Problem Solving Through a Verifier: An Empirical Analysis on Model-induced Process Supervision",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihan Wang",
      "Yunxuan Li",
      "Yuexin Wu",
      "Liangchen Luo",
      "Le Hou",
      "Hongkun Yu",
      "Jingbo Shang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.430": {
    "title": "MUSCLE: A Model Update Strategy for Compatible LLM Evolution",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jessica Echterhoff",
      "Fartash Faghri",
      "Raviteja Vemulapalli",
      "Ting-Yao Hu",
      "Chun-Liang Li",
      "Oncel Tuzel",
      "Hadi Pouransari"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.431": {
    "title": "Event-Keyed Summarization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "William Gantt",
      "Alexander Martin",
      "Pavlo Kuchmiichuk",
      "Aaron White"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.432": {
    "title": "The Effect of Sampling Temperature on Problem Solving in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew Renze"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.433": {
    "title": "HiCuLR: Hierarchical Curriculum Learning for Rhetorical Role Labeling of Legal Documents",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Santosh T.y.s.s",
      "Apolline Isaia",
      "Shiyu Hong",
      "Matthias Grabmair"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.434": {
    "title": "Semi-Supervised Reward Modeling via Iterative Self-Training",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei He",
      "Haoxiang Wang",
      "Ziyan Jiang",
      "Alexandros Papangelis",
      "Han Zhao"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.435": {
    "title": "Demonstration Selection Strategies for Numerical Time Series Data-to-Text",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Masayuki Kawarada",
      "Tatsuya Ishigaki",
      "Goran Topić",
      "Hiroya Takamura"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.436": {
    "title": "ALIGN-SIM: A Task-Free Test Bed for Evaluating and Interpreting Sentence Embeddings through Semantic Similarity Alignment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yash Mahajan",
      "Naman Bansal",
      "Eduardo Blanco",
      "Santu Karmaker"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.437": {
    "title": "BIPEFT: Budget-Guided Iterative Search for Parameter Efficient Fine-Tuning of Large Pretrained Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aofei Chang",
      "Jiaqi Wang",
      "Han Liu",
      "Parminder Bhatia",
      "Cao Xiao",
      "Ting Wang",
      "Fenglong Ma"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.438": {
    "title": "In-Context Learning with Iterative Demonstration Selection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengwei Qin",
      "Aston Zhang",
      "Chen Chen",
      "Anirudh Dagar",
      "Wenming Ye"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.439": {
    "title": "On Evaluating Explanation Utility for Human-AI Decision Making in NLP",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fateme Hashemi Chaleshtori",
      "Atreya Ghosal",
      "Alexander Gill",
      "Purbid Bambroo",
      "Ana Marasovic"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.440": {
    "title": "Unsupervised Hierarchical Topic Modeling via Anchor Word Clustering and Path Guidance",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiyuan Liu",
      "Hegang Chen",
      "Chunjiang Zhu",
      "Yanghui Rao"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.441": {
    "title": "GuardEmb: Dynamic Watermark for Safeguarding Large Language Model Embedding Service Against Model Stealing Attack",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liaoyaqi Wang",
      "Minhao Cheng"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.442": {
    "title": "Difficult Task Yes but Simple Task No: Unveiling the Laziness in Multimodal LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sihang Zhao",
      "Youliang Yuan",
      "Xiaoying Tang",
      "Pinjia He"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.443": {
    "title": "Pseudo-Label Enhanced Prototypical Contrastive Learning for Uniformed Intent Discovery",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yimin Deng",
      "Yuxia Wu",
      "Guoshuai Zhao",
      "Li Zhu",
      "Xueming Qian"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.444": {
    "title": "RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xijie Huang",
      "Zechun Liu",
      "Shih-Yang Liu",
      "Kwang-Ting Cheng"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.445": {
    "title": "Can Large Language Models Grasp Legal Theories? Enhance Legal Reasoning with Insights from Multi-Agent Collaboration",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weikang Yuan",
      "Junjie Cao",
      "Zhuoren Jiang",
      "Yangyang Kang",
      "Jun Lin",
      "Kaisong Song",
      "Tianqianjin Lin",
      "Pengwei Yan",
      "Changlong Sun",
      "Xiaozhong Liu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.446": {
    "title": "Retrieval and Reasoning on KGs: Integrate Knowledge Graphs into Large Language Models for Complex Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixin Ji",
      "Kaixin Wu",
      "Juntao Li",
      "Wei Chen",
      "Mingjie Zhong",
      "Xu Jia",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.447": {
    "title": "Insights into LLM Long-Context Failures: When Transformers Know but Don't Tell",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhan Gao",
      "TaiMing Lu",
      "Kuai Yu",
      "Adam Byerly",
      "Daniel Khashabi"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.448": {
    "title": "E2CL: Exploration-based Error Correction Learning for Embodied Agents",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanlin Wang",
      "Chak Tou Leong",
      "Jian Wang",
      "Wenjie Li"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.449": {
    "title": "BERGEN: A Benchmarking Library for Retrieval-Augmented Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Rau",
      "Hervé Déjean",
      "Nadezhda Chirkova",
      "Thibault Formal",
      "Shuai Wang",
      "Stéphane Clinchant",
      "Vassilina Nikoulina"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.450": {
    "title": "Contextualized Graph Representations for Generating Counter-Narratives against Hate Speech",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Selene Baez Santamaria",
      "Helena Gomez Adorno",
      "Ilia Markov"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.451": {
    "title": "Modeling Historical Relevant and Local Frequency Context for Representation-Based Temporal Knowledge Graph Forecasting",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengzhe Zhang",
      "Wei Wei",
      "Rikui Huang",
      "Wenfeng Xie",
      "Dangyang Chen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.452": {
    "title": "Representation Alignment and Adversarial Networks for Cross-lingual Dependency Parsing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ying Li",
      "Jianjian Liu",
      "Zhengtao Yu",
      "Shengxiang Gao",
      "Yuxin Huang",
      "Cunli Mao"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.453": {
    "title": "An Instruction Tuning-Based Contrastive Learning Framework for Aspect Sentiment Quad Prediction with Implicit Aspects and Opinions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Zhang",
      "Yu-N Cheah",
      "Congqing He",
      "Feifan Yi"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.454": {
    "title": "MACAROON: Training Vision-Language Models To Be Your Engaged Partners",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shujin Wu",
      "Yi Fung",
      "Sha Li",
      "Yixin Wan",
      "Kai-Wei Chang",
      "Heng Ji"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.455": {
    "title": "ICL: Iterative Continual Learning for Multi-domain Neural Machine Translation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhibo Man",
      "Kaiyu Huang",
      "Yujie Zhang",
      "Yuanmeng Chen",
      "Yufeng Chen",
      "Jinan Xu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.456": {
    "title": "Mitigating Hallucinations of Large Language Models in Medical Information Extraction via Contrastive Decoding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Derong Xu",
      "Ziheng Zhang",
      "Zhihong Zhu",
      "Zhenxi Lin",
      "Qidong Liu",
      "Xian Wu",
      "Tong Xu",
      "Xiangyu Zhao",
      "Yefeng Zheng",
      "Enhong Chen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.457": {
    "title": "NeuroMax: Enhancing Neural Topic Modeling via Maximizing Mutual Information and Group Topic Regularization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Duy-Tung Pham",
      "Thien Trang Nguyen Vu",
      "Tung Nguyen",
      "Linh Ngo",
      "Duc Nguyen",
      "Thien Nguyen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.458": {
    "title": "LLM Self-Correction with DeCRIM: Decompose, Critique, and Refine for Enhanced Following of Instructions with Multiple Constraints",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Palmeira Ferraz",
      "Kartik Mehta",
      "Yu-Hsiang Lin",
      "Haw-Shiuan Chang",
      "Shereen Oraby",
      "Sijia Liu",
      "Vivek Subramanian",
      "Tagyoung Chung",
      "Mohit Bansal",
      "Nanyun Peng"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.459": {
    "title": "Learning to Plan for Retrieval-Augmented Large Language Models from Knowledge Graphs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junjie Wang",
      "Mingyang Chen",
      "Binbin Hu",
      "Dan Yang",
      "Ziqi Liu",
      "Yue Shen",
      "Peng Wei",
      "Zhiqiang Zhang",
      "Jinjie Gu",
      "Jun Zhou",
      "Jeff Pan",
      "Wen Zhang",
      "Huajun Chen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.460": {
    "title": "Is Compound Aspect-Based Sentiment Analysis Addressed by LLMs?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinhao Bai",
      "Zhixin Han",
      "Yuhua Zhao",
      "Hang Gao",
      "Zhuowei Zhang",
      "Xunzhi Wang",
      "Mengting Hu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.461": {
    "title": "Multilingual Fine-Grained News Headline Hallucination Detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaming Shen",
      "Tianqi Liu",
      "Jialu Liu",
      "Zhen Qin",
      "Jay Pavagadhi",
      "Simon Baumgartner",
      "Michael Bendersky"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.462": {
    "title": "PE: A Poincare Explanation Method for Fast Text Hierarchy Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qian Chen",
      "Dongyang Li",
      "Xiaofeng He",
      "Hongzhao Li",
      "Hongyu Yi"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.463": {
    "title": "Step-level Value Preference Optimization for Mathematical Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guoxin Chen",
      "Minpeng Liao",
      "Chengxi Li",
      "Kai Fan"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.464": {
    "title": "Towards Benchmarking Situational Awareness of Large Language Models:Comprehensive Benchmark, Evaluation and Analysis",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guo Tang",
      "Zheng Chu",
      "Wenxiang Zheng",
      "Ming Liu",
      "Bing Qin"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.465": {
    "title": "Balancing Visual Context Understanding in Dialogue for Image Retrieval",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaohui Wei",
      "Lizi Liao",
      "Xiaoyu Du",
      "Xinguang Xiang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.466": {
    "title": "Mechanistic Understanding and Mitigation of Language Model Non-Factual Hallucinations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Yu",
      "Meng Cao",
      "Jackie Cheung",
      "Yue Dong"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.467": {
    "title": "A Study of Implicit Ranking Unfairness in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Xu",
      "Wenjie Wang",
      "Yuxin Li",
      "Liang Pang",
      "Jun Xu",
      "Tat-Seng Chua"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.468": {
    "title": "Information Parity: Measuring and Predicting the Multilingual Capabilities of Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Tsvetkov",
      "Alon Kipnis"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.469": {
    "title": "Better Call SAUL: Fluent and Consistent Language Model Editing with Generation Regularization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingyang Wang",
      "Lukas Lange",
      "Heike Adel",
      "Jannik Strötgen",
      "Hinrich Schuetze"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.470": {
    "title": "A Semantic Search Engine for Mathlib4",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guoxiong Gao",
      "Haocheng Ju",
      "Jiedong Jiang",
      "Zihan Qin",
      "Bin Dong"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.471": {
    "title": "DyKnow: Dynamically Verifying Time-Sensitive Factual Knowledge in LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seyed Mahed Mousavi",
      "Simone Alghisi",
      "Giuseppe Riccardi"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.472": {
    "title": "Rewarding What Matters: Step-by-Step Reinforcement Learning for Task-Oriented Dialogue",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huifang Du",
      "Shuqin Li",
      "Minghao Wu",
      "Xuejing Feng",
      "Yuan-Fang Li",
      "Haofen Wang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.473": {
    "title": "Assistive Large Language Model Agents for Socially-Aware Negotiation Dialogues",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuncheng Hua",
      "Lizhen Qu",
      "Reza Haf"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.474": {
    "title": "HoLLMwood: Unleashing the Creativity of Large Language Models in Screenwriting via Role Playing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Chen",
      "Xinyu Zhu",
      "Cheng Yang",
      "Chufan Shi",
      "Yadong Xi",
      "Yuxiang Zhang",
      "Junjie Wang",
      "Jiashu Pu",
      "Tian Feng",
      "Yujiu Yang",
      "Rongsheng Zhang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.475": {
    "title": "Advancing Cross-Lingual Entity Alignment with Large Language Models: Tailored Sample Segmentation and Zero-Shot Prompts",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linyan Yang",
      "Jingwei Cheng",
      "Fu Zhang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.476": {
    "title": "Causal Discovery Inspired Unsupervised Domain Adaptation for Emotion-Cause Pair Extraction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuncheng Hua",
      "Yujin Huang",
      "Shuo Huang",
      "Tao Feng",
      "Lizhen Qu",
      "Christopher Bain",
      "Richard Bassed",
      "Reza Haf"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.477": {
    "title": "Large Language Models are Students at Various Levels: Zero-shot Question Difficulty Estimation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jae-Woo Park",
      "Seong-Jin Park",
      "Hyun-Sik Won",
      "Kang-Min Kim"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.478": {
    "title": "Inverse-Q*: Token Level Reinforcement Learning for Aligning Large Language Models Without Preference Data",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Xia",
      "Songyang Gao",
      "Qiming Ge",
      "Zhiheng Xi",
      "Qi Zhang",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.479": {
    "title": "Activation Scaling for Steering and Interpreting Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Niklas Stoehr",
      "Kevin Du",
      "Vésteinn Snæbjarnarson",
      "Robert West",
      "Ryan Cotterell",
      "Aaron Schein"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.480": {
    "title": "LaRA: Large Rank Adaptation for Speech and Text Cross-Modal Learning in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zuhair Shaik",
      "Pradyoth Hegde",
      "Prashant Bannulmath",
      "Deepak T"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.481": {
    "title": "DTS-SQL: Decomposed Text-to-SQL with Small Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammadreza Pourreza",
      "Davood Rafiei"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.482": {
    "title": "MedINST: Meta Dataset of Biomedical Instructions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhan Han",
      "Meng Fang",
      "Zihan Zhang",
      "Yu Yin",
      "Zirui Song",
      "Ling Chen",
      "Mykola Pechenizkiy",
      "Qingyu Chen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.483": {
    "title": "PropTest: Automatic Property Testing for Improved Visual Programming",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaywon Koo",
      "Ziyan Yang",
      "Paola Cascante-Bonilla",
      "Baishakhi Ray",
      "Vicente Ordonez"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.484": {
    "title": "BadFair: Backdoored Fairness Attacks with Group-conditioned Triggers",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi Xue",
      "Qian Lou",
      "Mengxin Zheng"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.485": {
    "title": "Is GPT-4V (ision) All You Need for Automating Academic Data Visualization? Exploring Vision-Language Models' Capability in Reproducing Academic Charts",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhehao Zhang",
      "Weicheng Ma",
      "Soroush Vosoughi"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.486": {
    "title": "Financial Forecasting from Textual and Tabular Time Series",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ross Koval",
      "Nicholas Andrews",
      "Xifeng Yan"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.487": {
    "title": "Learning to Ask Denotative and Connotative Questions for Knowledge-based VQA",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoying Xing",
      "Peixi Xiong",
      "Lei Fan",
      "Yunxuan Li",
      "Ying Wu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.488": {
    "title": "CONTOR: Benchmarking Strategies for Completing Ontologies with Plausible Missing Rules",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Na Li",
      "Thomas Bailleux",
      "Zied Bouraoui",
      "Steven Schockaert"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.489": {
    "title": "Towards Pareto-Efficient RLHF: Paying Attention to a Few High-Reward Samples with Reward Dropout",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changhun Lee",
      "Chiehyeon Lim"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.490": {
    "title": "Weak-to-Strong Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuqing Yang",
      "Yan Ma",
      "Pengfei Liu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.491": {
    "title": "Fine-Tuning Language Models with Differential Privacy through Adaptive Noise Allocation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xianzhi Li",
      "Ran Zmigrod",
      "Zhiqiang Ma",
      "Xiaomo Liu",
      "Xiaodan Zhu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.492": {
    "title": "The Mystery of Compositional Generalization in Graph-based Generative Commonsense Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiyan Fu",
      "Anette Frank"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.493": {
    "title": "AutoHallusion: Automatic Generation of Hallucination Benchmarks for Vision-Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiyang Wu",
      "Tianrui Guan",
      "Dianqi Li",
      "Shuaiyi Huang",
      "Xiaoyu Liu",
      "Xijun Wang",
      "Ruiqi Xian",
      "Abhinav Shrivastava",
      "Furong Huang",
      "Jordan Boyd-Graber",
      "Tianyi Zhou",
      "Dinesh Manocha"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.494": {
    "title": "MetaKP: On-Demand Keyphrase Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Di Wu",
      "Xiaoxian Shen",
      "Kai-Wei Chang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.495": {
    "title": "PSST: A Benchmark for Evaluation-driven Text Public-Speaking Style Transfer",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huashan Sun",
      "Yixiao Wu",
      "Yizhe Yang",
      "Yinghao Li",
      "Jiawei Li",
      "Yuhao Ye",
      "Yang Gao"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.496": {
    "title": "TRACE the Evidence: Constructing Knowledge-Grounded Reasoning Chains for Retrieval-Augmented Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinyuan Fang",
      "Zaiqiao Meng",
      "Craig MacDonald"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.497": {
    "title": "Enable Fast Sampling for Seq2Seq Text Diffusion",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pan Liu",
      "Xiaohua Tian",
      "Zhouhan Lin"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.498": {
    "title": "AlignSum: Data Pyramid Hierarchical Fine-tuning for Aligning with Human Summarization Preference",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Han",
      "Yiming Wang",
      "Rui Wang",
      "Lu Chen",
      "Kai Yu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.499": {
    "title": "CHIRON: Rich Character Representations in Long-Form Narratives",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Gurung",
      "Mirella Lapata"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.500": {
    "title": "Refiner: Restructure Retrieved Content Efficiently to Advance Question-Answering Capabilities",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhonghao Li",
      "Xuming Hu",
      "Aiwei Liu",
      "Kening Zheng",
      "Sirui Huang",
      "Hui Xiong"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.501": {
    "title": "Infrared-LLaVA: Enhancing Understanding of Infrared Images in Multi-Modal Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shixin Jiang",
      "Zerui Chen",
      "Jiafeng Liang",
      "Yanyan Zhao",
      "Ming Liu",
      "Bing Qin"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.502": {
    "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peijie Dong",
      "Lujun Li",
      "Xiang Liu",
      "Zhenheng Tang",
      "Xuebo Liu",
      "Qiang Wang",
      "Xiaowen Chu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.503": {
    "title": "Traffic Light or Light Traffic? Investigating Phrasal Semantics in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Meng",
      "Ye Liu",
      "Lifu Tu",
      "Daqing He",
      "Yingbo Zhou",
      "Semih Yavuz"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.504": {
    "title": "How Far Can In-Context Alignment Go? Exploring the State of In-Context Alignment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heyan Huang",
      "Yinghao Li",
      "Huashan Sun",
      "Yu Bai",
      "Yang Gao"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.505": {
    "title": "Variational Language Concepts for Interpreting Foundation Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hengyi Wang",
      "Shiwei Tan",
      "Zhiqing Hong",
      "Desheng Zhang",
      "Hao Wang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.506": {
    "title": "Exploring the Capability of Multimodal LLMs with Yonkoma Manga: The YManga Dataset and Its Challenging Tasks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Yang",
      "Jingjie Zeng",
      "Liang Yang",
      "Zhihao Yang",
      "Hongfei Lin"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.507": {
    "title": "TWBias: A Benchmark for Assessing Social Bias in Traditional Chinese Large Language Models through a Taiwan Cultural Lens",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hsin-Yi Hsieh",
      "Shih-Cheng Huang",
      "Richard Tsai"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.508": {
    "title": "Unlocking the Potential of Model Merging for Low-Resource Languages",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingxu Tao",
      "Chen Zhang",
      "Quzhe Huang",
      "Tianyao Ma",
      "Songfang Huang",
      "Dongyan Zhao",
      "Yansong Feng"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.509": {
    "title": "PURE: Aligning LLM via Pluggable Query Reformulation for Enhanced Helpfulness",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjin Yao",
      "Yidong Wang",
      "Zhuohao Yu",
      "Rui Xie",
      "Shikun Zhang",
      "Wei Ye"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.510": {
    "title": "MMedAgent: Learning to Use Medical Tools with Multi-modal Agent",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Binxu Li",
      "Tiankai Yan",
      "Yuanting Pan",
      "Jie Luo",
      "Ruiyang Ji",
      "Jiayuan Ding",
      "Zhe Xu",
      "Shilong Liu",
      "Haoyu Dong",
      "Zihao Lin",
      "Yixin Wang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.511": {
    "title": "SALMON: A Structure-Aware Language Model with logicality and densification strategy for Temporal Knowledge Graph Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fu Zhang",
      "Jinghao Lin",
      "Jingwei Cheng"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.512": {
    "title": "Multilingual Contrastive Decoding via Language-Agnostic Layers Skipping",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhao Zhu",
      "Sizhe Liu",
      "Shujian Huang",
      "Shuaijie She",
      "Chris Wendler",
      "Jiajun Chen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.513": {
    "title": "The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bolei Ma",
      "Xinpeng Wang",
      "Tiancheng Hu",
      "Anna-Carolina Haensch",
      "Michael Hedderich",
      "Barbara Plank",
      "Frauke Kreuter"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.514": {
    "title": "Low-Resource Machine Translation through the Lens of Personalized Federated Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Viktor Moskvoretskii",
      "Nazarii Tupitsa",
      "Chris Biemann",
      "Samuel Horváth",
      "Eduard Gorbunov",
      "Irina Nikishina"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.515": {
    "title": "Can Language Models Recognize Convincing Arguments?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paula Rescala",
      "Manoel Ribeiro",
      "Tiancheng Hu",
      "Robert West"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.516": {
    "title": "Knowledge Navigator: LLM-guided Browsing Framework for Exploratory Search in Scientific Literature",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Uri Katz",
      "Mosh Levy",
      "Yoav Goldberg"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.517": {
    "title": "Scalable and Domain-General Abstractive Proposition Segmentation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Javad Hosseini",
      "Yang Gao",
      "Tim Baumgärtner",
      "Alex Fabrikant",
      "Reinald Kim Amplayo"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.518": {
    "title": "Hit the Nail on the Head: Parameter-Efficient Multi-task Tuning via Human Language Intervention",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenxuan Lu",
      "Songhao Jiang",
      "Wang Yijing",
      "Tianning Zang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.519": {
    "title": "LINKED: Eliciting, Filtering and Integrating Knowledge in Large Language Model for Commonsense Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiachun Li",
      "Pengfei Cao",
      "Chenhao Wang",
      "Zhuoran Jin",
      "Yubo Chen",
      "Kang Liu",
      "Xiaojian Jiang",
      "Jiexin Xu",
      "Jun Zhao"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.520": {
    "title": "Beyond Agreement: Diagnosing the Rationale Alignment of Automated Essay Scoring Methods based on Linguistically-informed Counterfactuals",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yupei Wang",
      "Renfen Hu",
      "Zhe Zhao"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.521": {
    "title": "TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Zhang",
      "Chengguang Tang",
      "Dading Chong",
      "Ke Shi",
      "Guohua Tang",
      "Feng Jiang",
      "Haizhou Li"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.522": {
    "title": "Datasets for Multilingual Answer Sentence Selection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matteo Gabburo",
      "Stefano Campese",
      "Federico Agostini",
      "Alessandro Moschitti"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.523": {
    "title": "Active Learning for Abstractive Text Summarization via LLM-Determined Curriculum and Certainty Gain Maximization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongyuan Li",
      "Ying Zhang",
      "Zhen Wang",
      "Shiyin Tan",
      "Satoshi Kosugi",
      "Manabu Okumura"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.524": {
    "title": "Question-guided Knowledge Graph Re-scoring and Injection for Knowledge Graph Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Zhang",
      "Kehai Chen",
      "Xuefeng Bai",
      "Zhao Kang",
      "Quanjiang Guo",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.525": {
    "title": "Achieving Stronger Generation via Simple Contrastive Tuning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhimeng Wang",
      "Pinzheng Wang",
      "Juntao Li",
      "Yibin Chen",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.526": {
    "title": "Forecasting Future International Events: A Reliable Dataset for Text-Based Event Modeling",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daehoon Gwak",
      "Junwoo Park",
      "Minho Park",
      "ChaeHun Park",
      "Hyunchan Lee",
      "Edward Choi",
      "Jaegul Choo"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.527": {
    "title": "QPaug: Question and Passage Augmentation for Open-Domain Question Answering of LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minsang Kim",
      "Cheoneum Park",
      "Seung Baek"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.528": {
    "title": "ICON: Improving Inter-Report Consistency in Radiology Report Generation via Lesion-aware Mixup Augmentation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjun Hou",
      "Yi Cheng",
      "Kaishuai Xu",
      "Yan Hu",
      "Wenjie Li",
      "Jiang Liu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.529": {
    "title": "DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kedi Chen",
      "Qin Chen",
      "Jie Zhou",
      "He Yishen",
      "Liang He"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.530": {
    "title": "ExpertEase: A Multi-Agent Framework for Grade-Specific Document Simplification with Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaijie Mo",
      "Renfen Hu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.531": {
    "title": "Class Name Guided Out-of-Scope Intent Classification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chandan Gautam",
      "Sethupathy Parameswaran",
      "Aditya Kane",
      "Yuan Fang",
      "Savitha Ramasamy",
      "Suresh Sundaram",
      "Sunil Sahu",
      "Xiaoli Li"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.532": {
    "title": "Inference-Time Decontamination: Reusing Leaked Benchmarks for Large Language Model Evaluation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qin Zhu",
      "Qinyuan Cheng",
      "Runyu Peng",
      "Xiaonan Li",
      "Ru Peng",
      "Tengxiao Liu",
      "Xipeng Qiu",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.533": {
    "title": "MultiVerse: Efficient and Expressive Zero-Shot Multi-Task Text-to-Speech",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taejun Bak",
      "Youngsik Eom",
      "SeungJae Choi",
      "Young-Sun Joo"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.534": {
    "title": "RoBERT2VecTM: A Novel Approach for Topic Extraction in Islamic Studies",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sania Aftar",
      "Luca Gagliardelli",
      "Amina Ganadi",
      "Federico Ruozzi",
      "Sonia Bergamaschi"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.535": {
    "title": "Are ELECTRA's Sentence Embeddings Beyond Repair? The Case of Semantic Textual Similarity",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ivan Rep",
      "David Dukić",
      "Jan Šnajder"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.536": {
    "title": "DetectiveNN: Imitating Human Emotional Reasoning with a Recall-Detect-Predict Framework for Emotion Recognition in Conversations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simin Hong",
      "Jun Sun",
      "Taihao Li"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.537": {
    "title": "HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adrián Bazaga",
      "Pietro Lio",
      "Gos Micklem"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.538": {
    "title": "On Diversified Preferences of Large Language Model Alignment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dun Zeng",
      "Yong Dai",
      "Pengyu Cheng",
      "Longyue Wang",
      "Tianhao Hu",
      "Wanshun Chen",
      "Nan Du",
      "Zenglin Xu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.539": {
    "title": "LoRAExit: Empowering Dynamic Modulation of LLMs in Resource-limited Settings using Low-rank Adapters",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiacheng Liu",
      "Peng Tang",
      "Xiaofeng Hou",
      "Chao Li",
      "Pheng-Ann Heng"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.540": {
    "title": "Improving Diversity of Commonsense Generation by Large Language Models via In-Context Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianhui Zhang",
      "Bei Peng",
      "Danushka Bollegala"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.541": {
    "title": "CodeIP: A Grammar-Guided Multi-Bit Watermark for Large Language Models of Code",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Batu Guan",
      "Yao Wan",
      "Zhangqian Bi",
      "Zheng Wang",
      "Hongyu Zhang",
      "Pan Zhou",
      "Lichao Sun"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.542": {
    "title": "StablePT : Towards Stable Prompting for Few-shot Learning via Input Separation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoming Liu",
      "Chen Liu",
      "Zhaohan Zhang",
      "Chengzhengxu Li",
      "Longtian Wang",
      "Yu Lan",
      "Chao Shen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.543": {
    "title": "Natural Evolution-based Dual-Level Aggregation for Temporal Knowledge Graph Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bin Chen",
      "Chunjing Xiao",
      "Fan Zhou"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.544": {
    "title": "Creative and Context-Aware Translation of East Asian Idioms with GPT-4",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kenan Tang",
      "Peiyang Song",
      "Yao Qin",
      "Xifeng Yan"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.545": {
    "title": "Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Angana Borah",
      "Rada Mihalcea"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.546": {
    "title": "Exploring Hint Generation Approaches for Open-Domain Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jamshid Mozafari",
      "Abdelrahman Abdallah",
      "Bhawna Piryani",
      "Adam Jatowt"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.547": {
    "title": "Do LLMs Think Fast and Slow? A Causal Study on Sentiment Analysis",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiheng Lyu",
      "Zhijing Jin",
      "Fernando Gonzalez Adauto",
      "Rada Mihalcea",
      "Bernhard Schölkopf",
      "Mrinmaya Sachan"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.548": {
    "title": "PEDANTS: Cheap but Effective and Interpretable Answer Equivalence",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zongxia Li",
      "Ishani Mondal",
      "Huy Nghiem",
      "Yijun Liang",
      "Jordan Boyd-Graber"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.549": {
    "title": "AgentsCourt: Building Judicial Decision-Making Agents with Court Debate Simulation and Legal Knowledge Augmentation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhitao He",
      "Pengfei Cao",
      "Chenhao Wang",
      "Zhuoran Jin",
      "Yubo Chen",
      "Jiexin Xu",
      "Huaijun Li",
      "Kang Liu",
      "Jun Zhao"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.550": {
    "title": "Editing the Mind of Giants: An In-Depth Exploration of Pitfalls of Knowledge Editing in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng-Hsun Hsueh",
      "Paul Huang",
      "Tzu-Han Lin",
      "Che Liao",
      "Hung-Chieh Fang",
      "Chao-Wei Huang",
      "Yun-Nung Chen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.551": {
    "title": "Improving LLM Attributions with Randomized Path-Integration",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oren Barkan",
      "Yehonatan Elisha",
      "Yonatan Toib",
      "Jonathan Weill",
      "Noam Koenigstein"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.552": {
    "title": "VeriScore: Evaluating the factuality of verifiable claims in long-form text generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixiao Song",
      "Yekyung Kim",
      "Mohit Iyyer"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.553": {
    "title": "Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical Questioning for Socratic Code Debugging",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Priyanka Kargupta",
      "Ishika Agarwal",
      "Dilek Tur",
      "Jiawei Han"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.554": {
    "title": "Tutor-ICL: Guiding Large Language Models for Improved In-Context Learning Performance",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ikhyun Cho",
      "Gaeul Kwon",
      "Julia Hockenmaier"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.555": {
    "title": "Taking a turn for the better: Conversation redirection throughout the course of mental-health therapy",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vivian Nguyen",
      "Sang Jung",
      "Lillian Lee",
      "Thomas Hull",
      "Cristian Danescu-Niculescu-Mizil"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.556": {
    "title": "LLM Explainability via Attributive Masking Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oren Barkan",
      "Yonatan Toib",
      "Yehonatan Elisha",
      "Jonathan Weill",
      "Noam Koenigstein"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.557": {
    "title": "How Entangled is Factuality and Deception in German?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aswathy Velutharambath",
      "Amelie Wuehrl",
      "Roman Klinger"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.558": {
    "title": "Train Once, Use Flexibly: A Modular Framework for Multi-Aspect Neural News Recommendation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andreea Iana",
      "Goran Glavaš",
      "Heiko Paulheim"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.559": {
    "title": "A LLM-based Ranking Method for the Evaluation of Automatic Counter-Narrative Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Irune Zubiaga",
      "Aitor Soroa",
      "Rodrigo Agerri"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.560": {
    "title": "A Survey on Open Information Extraction from Rule-based Model to Large Language Model",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liu Pai",
      "Wenyang Gao",
      "Wenjie Dong",
      "Lin Ai",
      "Ziwei Gong",
      "Songfang Huang",
      "Li Zongsheng",
      "Ehsan Hoque",
      "Julia Hirschberg",
      "Yue Zhang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.561": {
    "title": "Enhancing Tool Retrieval with Iterative Feedback from Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiancheng Xu",
      "Yongqi Li",
      "Heming Xia",
      "Wenjie Li"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.562": {
    "title": "Detecting Temporal Ambiguity in Questions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bhawna Piryani",
      "Abdelrahman Abdallah",
      "Jamshid Mozafari",
      "Adam Jatowt"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.563": {
    "title": "LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seyedarmin Azizi",
      "Souvik Kundu",
      "Massoud Pedram"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.564": {
    "title": "Machine Translation Hallucination Detection for Low and High Resource Languages using Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kenza Benkirane",
      "Laura Gongas",
      "Shahar Pelles",
      "Naomi Fuchs",
      "Joshua Darmon",
      "Pontus Stenetorp",
      "David Adelani",
      "Eduardo Sánchez"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.565": {
    "title": "Navigating Hallucinations for Reasoning of Unintentional Activities",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shresth Grover",
      "Vibhav Vineet",
      "Yogesh Rawat"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.566": {
    "title": "Pruning Foundation Models for High Accuracy without Retraining",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pu Zhao",
      "Fei Sun",
      "Xuan Shen",
      "Pinrui Yu",
      "Zhenglun Kong",
      "Yanzhi Wang",
      "Xue Lin"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.567": {
    "title": "From Pixels to Personas: Investigating and Modeling Self-Anthropomorphism in Human-Robot Dialogues",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Li",
      "Devamanyu Hazarika",
      "Di Jin",
      "Julia Hirschberg",
      "Yang Liu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.568": {
    "title": "DisGeM: Distractor Generation for Multiple Choice Questions with Span Masking",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Devrim Çavuşoğlu",
      "Seçil Şen",
      "Ulaş Sert"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.569": {
    "title": "ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Xu",
      "Xiao Liu",
      "Xinghan Liu",
      "Zhenyu Hou",
      "Yueyan Li",
      "Xiaohan Zhang",
      "Zihan Wang",
      "Aohan Zeng",
      "Zhengxiao Du",
      "Zhao Wenyi",
      "Jie Tang",
      "Yuxiao Dong"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.570": {
    "title": "MobileQuant: Mobile-friendly Quantization for On-device Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fuwen Tan",
      "Royson Lee",
      "Łukasz Dudziak",
      "Shell Xu Hu",
      "Sourav Bhattacharya",
      "Timothy Hospedales",
      "Georgios Tzimiropoulos",
      "Brais Martinez"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.571": {
    "title": "Do *they* mean ‘us'? Interpreting Referring Expression variation under Intergroup Bias",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Venkata Govindarajan",
      "Matianyu Zang",
      "Kyle Mahowald",
      "David Beaver",
      "Junyi Jessy Li"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.572": {
    "title": "A Survey on Detection of LLMs-Generated Content",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xianjun Yang",
      "Liangming Pan",
      "Xuandong Zhao",
      "Haifeng Chen",
      "Linda Petzold",
      "William Yang Wang",
      "Wei Cheng"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.573": {
    "title": "Can LLMs Reason in the Wild with Programs?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Yang",
      "Siheng Xiong",
      "Ali Payani",
      "Ehsan Shareghi",
      "Faramarz Fekri"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.574": {
    "title": "Can Textual Unlearning Solve Cross-Modality Safety Alignment?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Trishna Chakraborty",
      "Erfan Shayegani",
      "Zikui Cai",
      "Nael Abu-Ghazaleh",
      "M. Salman Asif",
      "Yue Dong",
      "Amit Roy-Chowdhury",
      "Chengyu Song"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.575": {
    "title": "VDebugger: Harnessing Execution Feedback for Debugging Visual Programs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xueqing Wu",
      "Zongyu Lin",
      "Songyan Zhao",
      "Te-Lin Wu",
      "Pan Lu",
      "Nanyun Peng",
      "Kai-Wei Chang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.576": {
    "title": "Monotonic Paraphrasing Improves Generalization of Language Model Prompting",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qin Liu",
      "Fei Wang",
      "Nan Xu",
      "Tianyi Yan",
      "Tao Meng",
      "Muhao Chen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.577": {
    "title": "MORL-Prompt: An Empirical Analysis of Multi-Objective Reinforcement Learning for Discrete Prompt Optimization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yasaman Jafari",
      "Dheeraj Mekala",
      "Rose Yu",
      "Taylor Berg-Kirkpatrick"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.578": {
    "title": "Understanding Faithfulness and Reasoning of Large Language Models on Plain Biomedical Summaries",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Biaoyan Fang",
      "Xiang Dai",
      "Sarvnaz Karimi"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.579": {
    "title": "Change Is the Only Constant: Dynamic LLM Slicing based on Layer Redundancy",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Razvan-Gabriel Dumitru",
      "Paul Clotan",
      "Vikas Yadav",
      "Darius Peteleaza",
      "Mihai Surdeanu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.580": {
    "title": "Pruning Multilingual Large Language Models for Multilingual Inference",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hwichan Kim",
      "Jun Suzuki",
      "Tosho Hirasawa",
      "Mamoru Komachi"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.581": {
    "title": "Video Discourse Parsing and Its Application to Multimodal Summarization: A Dataset and Baseline Approaches",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tsutomu Hirao",
      "Naoki Kobayashi",
      "Hidetaka Kamigaito",
      "Manabu Okumura",
      "Akisato Kimura"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.582": {
    "title": "Length Extrapolation of Transformers: A Survey from the Perspective of Positional Encoding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liang Zhao",
      "Xiachong Feng",
      "Xiaocheng Feng",
      "Weihong Zhong",
      "Dongliang Xu",
      "Qing Yang",
      "Hongtao Liu",
      "Bing Qin",
      "Ting Liu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.583": {
    "title": "VPL: Visual Proxy Learning Framework for Zero-Shot Medical Image Diagnosis",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxiang Liu",
      "Tianxiang Hu",
      "Huimin Xiong",
      "Jiawei Du",
      "Yang Feng",
      "Jian Wu",
      "Joey Zhou",
      "Zuozhu Liu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.584": {
    "title": "Word-Conditioned 3D American Sign Language Motion Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lu Dong",
      "Xiao Wang",
      "Ifeoma Nwogu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.585": {
    "title": "TrustAgent: Towards Safe and Trustworthy LLM-based Agents",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenyue Hua",
      "Xianjun Yang",
      "Mingyu Jin",
      "Zelong Li",
      "Wei Cheng",
      "Ruixiang Tang",
      "Yongfeng Zhang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.586": {
    "title": "Enabling Cross-Platform Comparison of Online Communities Using Content and Opinion Similarity",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prasanna Lakkur Subramanyam",
      "Jeng-Yu Chou",
      "Kevin Nam",
      "Brian Levine"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.587": {
    "title": "CNEQ: Incorporating numbers into Knowledge Graph Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xianshu Peng",
      "Wei Wei",
      "Kaihe Xu",
      "Dangyang Chen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.588": {
    "title": "StraGo: Harnessing Strategic Guidance for Prompt Optimization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yurong Wu",
      "Yan Gao",
      "Bin Zhu",
      "Zineng Zhou",
      "Xiaodi Sun",
      "Sheng Yang",
      "Jian-Guang Lou",
      "Zhiming Ding",
      "Linjun Yang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.589": {
    "title": "Learning to Plan by Updating Natural Language",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiduo Guo",
      "Yaobo Liang",
      "Chenfei Wu",
      "Wenshan Wu",
      "Dongyan Zhao",
      "Nan Duan"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.590": {
    "title": "C-ICL: Contrastive In-context Learning for Information Extraction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ying Mo",
      "Jiahao Liu",
      "Jian Yang",
      "Qifan Wang",
      "Shun Zhang",
      "Jingang Wang",
      "Zhoujun Li"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.591": {
    "title": "On the Similarity of Circuits across Languages: a Case Study on the Subject-verb Agreement Task",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Javier Ferrando",
      "Marta Costa-jussà"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.592": {
    "title": "Can LLM be a Personalized Judge?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yijiang Dong",
      "Tiancheng Hu",
      "Nigel Collier"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.593": {
    "title": "Who's Who: Large Language Models Meet Knowledge Conflicts in Practice",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quang Pham",
      "Hoang Ngo",
      "Anh Tuan Luu",
      "Dat Quoc Nguyen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.594": {
    "title": "Unleashing the Potentials of Likelihood Composition for Multi-modal Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shitian Zhao",
      "Renrui Zhang",
      "Xu Luo",
      "Yan Wang",
      "Shanghang Zhang",
      "Peng Gao"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.595": {
    "title": "Automated Peer Reviewing in Paper SEA: Standardization, Evaluation, and Analysis",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianxiang Yu",
      "Zichen Ding",
      "Jiaqi Tan",
      "Kangyang Luo",
      "Zhenmin Weng",
      "Chenghua Gong",
      "Long Zeng",
      "RenJing Cui",
      "Chengcheng Han",
      "Qiushi Sun",
      "Zhiyong Wu",
      "Yunshi Lan",
      "Xiang Li"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.596": {
    "title": "Knowledge-based Consistency Testing of Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sai Rajan",
      "Ezekiel Soremekun",
      "Sudipta Chattopadhyay"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.597": {
    "title": "PRESTO: Progressive Pretraining Enhances Synthetic Chemistry Outcomes",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "He Cao",
      "Yanjun Shao",
      "Zhiyuan Liu",
      "Zijing Liu",
      "Xiangru Tang",
      "Yuan Yao",
      "Yu Li"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.598": {
    "title": "Query Routing for Homogeneous Tools: An Instantiation in the RAG Scenario",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feiteng Mu",
      "Yong Jiang",
      "Liwen Zhang",
      "Liuchu Liuchu",
      "Wenjie Li",
      "Pengjun Xie",
      "Fei Huang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.599": {
    "title": "MobileVLM: A Vision-Language Model for Better Intra- and Inter-UI Understanding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qinzhuo Wu",
      "Weikai Xu",
      "Wei Liu",
      "Tao Tan",
      "Liujian Liujianfeng",
      "Ang Li",
      "Jian Luan",
      "Bin Wang",
      "Shuo Shang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.600": {
    "title": "Schema-Driven Information Extraction from Heterogeneous Tables",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Bai",
      "Junmo Kang",
      "Gabriel Stanovsky",
      "Dayne Freitag",
      "Mark Dredze",
      "Alan Ritter"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.601": {
    "title": "Is There a One-Model-Fits-All Approach to Information Extraction? Revisiting Task Definition Biases",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhao Huang",
      "Qianyu He",
      "Zhixu Li",
      "Jiaqing Liang",
      "Yanghua Xiao"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.602": {
    "title": "PromptIntern: Saving Inference Costs by Internalizing Recurrent Prompt during Large Language Model Fine-tuning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaru Zou",
      "Mengyu Zhou",
      "Tao Li",
      "Shi Han",
      "Dongmei Zhang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.603": {
    "title": "TAP4LLM: Table Provider on Sampling, Augmenting, and Packing Semi-structured Data for Large Language Model Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Sui",
      "Jiaru Zou",
      "Mengyu Zhou",
      "Xinyi He",
      "Lun Du",
      "Shi Han",
      "Dongmei Zhang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.604": {
    "title": "In2Core: Leveraging Influence Functions for Coreset Selection in Instruction Finetuning of Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ayrton Joaquin",
      "Bin Wang",
      "Zhengyuan Liu",
      "Philippe Muller",
      "Nicholas Asher",
      "Brian Lim",
      "Nancy Chen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.605": {
    "title": "How Personality Traits Influence Negotiation Outcomes? A Simulation based on Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yin Jou Huang",
      "Rafik Hadfi"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.606": {
    "title": "Introducing Spatial Information and a Novel Evaluation Scheme for Open-Domain Live Commentary Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Erica Shimomoto",
      "Edison Marrese-Taylor",
      "Ichiro Kobayashi",
      "Hiroya Takamura",
      "Yusuke Miyao"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.607": {
    "title": "Retrieving, Rethinking and Revising: The Chain-of-Verification Can Improve Retrieval Augmented Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bolei He",
      "Nuo Chen",
      "Xinran He",
      "Lingyong Yan",
      "Zhenkai Wei",
      "Jinchang Luo",
      "Zhen-Hua Ling"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.608": {
    "title": "Detecting Machine-Generated Long-Form Content with Latent-Space Variables",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufei Tian",
      "Zeyu Pan",
      "Nanyun Peng"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.609": {
    "title": "Learning to Match Representations is Better for End-to-End Task-Oriented Dialog System",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wanshi Xu",
      "Xuxin Cheng",
      "Zhihong Zhu",
      "Zhanpeng Chen",
      "Yuexian Zou"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.610": {
    "title": "ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable Safety Detectors",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhexin Zhang",
      "Yida Lu",
      "Jingyuan Ma",
      "Di Zhang",
      "Rui Li",
      "Pei Ke",
      "Hao Sun",
      "Lei Sha",
      "Zhifang Sui",
      "Hongning Wang",
      "Minlie Huang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.611": {
    "title": "BiasDora: Exploring Hidden Biased Associations in Vision-Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chahat Raj",
      "Anjishnu Mukherjee",
      "Aylin Caliskan",
      "Antonios Anastasopoulos",
      "Ziwei Zhu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.612": {
    "title": "MoE-I2: Compressing Mixture of Experts Models through Inter-Expert Pruning and Intra-Expert Low-Rank Decomposition",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Yang",
      "Yang Sui",
      "Jinqi Xiao",
      "Lingyi Huang",
      "Yu Gong",
      "Yuanlin Duan",
      "Wenqi Jia",
      "Miao Yin",
      "Yu Cheng",
      "Bo Yuan"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.613": {
    "title": "Multimodal Misinformation Detection by Learning from Synthetic Data with Multimodal LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fengzhu Zeng",
      "Wenqian Li",
      "Wei Gao",
      "Yan Pang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.614": {
    "title": "Exploring Design Choices for Building Language-Specific LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Atula Tejaswi",
      "Nilesh Gupta",
      "Eunsol Choi"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.615": {
    "title": "Promoting Data and Model Privacy in Federated Learning through Quantized LoRA",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhu JianHao",
      "Changze Lv",
      "Xiaohua Wang",
      "Muling Wu",
      "Wenhao Liu",
      "Tianlong Li",
      "Zixuan Ling",
      "Cenyuan Zhang",
      "Xiaoqing Zheng",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.616": {
    "title": "Intended Target Identification for Anomia Patients with Gradient-based Selective Augmentation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jongho Kim",
      "Romain Storaï",
      "Seung-won Hwang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.617": {
    "title": "Fine-tuning Smaller Language Models for Question Answering over Financial Documents",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karmvir Phogat",
      "Sai Akhil Puranam",
      "Sridhar Dasaratha",
      "Chetan Harsha",
      "Shashishekar Ramakrishna"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.618": {
    "title": "Beyond Fine-tuning: Unleashing the Potential of Continuous Pretraining for Clinical LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Clement Christophe",
      "Tathagata Raha",
      "Svetlana Maslenkova",
      "Muhammad Umar Salman",
      "Praveenkumar Kanithi",
      "Marco Pimentel",
      "Shadab Khan"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.619": {
    "title": "MedCare: Advancing Medical LLMs through Decoupling Clinical Alignment and Knowledge Aggregation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yusheng Liao",
      "Shuyang Jiang",
      "Zhe Chen",
      "Yu Wang",
      "Yanfeng Wang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.620": {
    "title": "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoxiang Wang",
      "Wei Xiong",
      "Tengyang Xie",
      "Han Zhao",
      "Tong Zhang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.621": {
    "title": "Code Membership Inference for Detecting Unauthorized Data Use in Code Pre-trained Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng Zhang",
      "Hui Li",
      "Rongrong Ji"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.622": {
    "title": "Learning When to Retrieve, What to Rewrite, and How to Respond in Conversational QA",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nirmal Roy",
      "Leonardo Ribeiro",
      "Rexhina Blloshmi",
      "Kevin Small"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.623": {
    "title": "Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weize Chen",
      "Chenfei Yuan",
      "Jiarui Yuan",
      "Yusheng Su",
      "Chen Qian",
      "Cheng Yang",
      "Ruobing Xie",
      "Zhiyuan Liu",
      "Maosong Sun"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.624": {
    "title": "Learning to Use Tools via Cooperative and Interactive Agents",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengliang Shi",
      "Shen Gao",
      "Xiuyi Chen",
      "Yue Feng",
      "Lingyong Yan",
      "Haibo Shi",
      "Dawei Yin",
      "Pengjie Ren",
      "Suzan Verberne",
      "Zhaochun Ren"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.625": {
    "title": "STARD: A Chinese Statute Retrieval Dataset Derived from Real-life Queries by Non-professionals",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weihang Su",
      "Yiran Hu",
      "Anzhe Xie",
      "Qingyao Ai",
      "Quezi Bing",
      "Ning Zheng",
      "Yun Liu",
      "Weixing Shen",
      "Yiqun Liu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.626": {
    "title": "What if...?: Thinking Counterfactual Keywords Helps to Mitigate Hallucination in Large Multi-modal Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junho Kim",
      "Kim Yeonju",
      "Yong Man Ro"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.627": {
    "title": "MELT: Materials-aware Continued Pre-training for Language Model Adaptation to Materials Science",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junho Kim",
      "Yeachan Kim",
      "Jun-Hyung Park",
      "Yerim Oh",
      "Suho Kim",
      "SangKeun Lee"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.628": {
    "title": "PDF-to-Tree: Parsing PDF Text Blocks into a Tree",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Zhang",
      "Zhihao Zhang",
      "Wenbin Lai",
      "Chong Zhang",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.629": {
    "title": "Seeing Through VisualBERT: A Causal Adventure on Memetic Landscapes",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dibyanayan Bandyopadhyay",
      "Mohammed Hasanuzzaman",
      "Asif Ekbal"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.630": {
    "title": "Cross-Lingual Unlearning of Selective Knowledge in Multilingual Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minseok Choi",
      "Kyunghyun Min",
      "Jaegul Choo"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.631": {
    "title": "LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation Capabilities Beyond 100 Languages",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinquan Lu",
      "Wenhao Zhu",
      "Lei Li",
      "Yu Qiao",
      "Fei Yuan"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.632": {
    "title": "Enhancing Emotion-Cause Pair Extraction in Conversations via Center Event Detection and Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Botao Wang",
      "Keke Tang",
      "Peican Zhu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.633": {
    "title": "Light-weight Fine-tuning Method for Defending Adversarial Noise in Pre-trained Medical Vision-Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xu Han",
      "Linghao Jin",
      "Xuezhe Ma",
      "Xiaofeng Liu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.634": {
    "title": "Together We Can: Multilingual Automatic Post-Editing for Low-Resource Languages",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sourabh Deoghare",
      "Diptesh Kanojia",
      "Pushpak Bhattacharyya"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.635": {
    "title": "CERT-ED: Certifiably Robust Text Classification for Edit Distance",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoqun Huang",
      "Neil Marchant",
      "Olga Ohrimenko",
      "Benjamin Rubinstein"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.636": {
    "title": "Ask-before-Plan: Proactive Language Agents for Real-World Planning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Zhang",
      "Yang Deng",
      "Zifeng Ren",
      "See-Kiong Ng",
      "Tat-Seng Chua"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.637": {
    "title": "From Complex to Simple: Enhancing Multi-Constraint Complex Instruction Following Ability of Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qianyu He",
      "Jie Zeng",
      "Qianxi He",
      "Jiaqing Liang",
      "Yanghua Xiao"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.638": {
    "title": "FlowBench: Revisiting and Benchmarking Workflow-Guided Planning for LLM-based Agents",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruixuan Xiao",
      "Wentao Ma",
      "Ke Wang",
      "Yuchuan Wu",
      "Junbo Zhao",
      "Haobo Wang",
      "Fei Huang",
      "Yongbin Li"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.639": {
    "title": "Mental Disorder Classification via Temporal Representation of Text",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raja Kumar",
      "Kishan Maharaj",
      "Ashita Saxena",
      "Pushpak Bhattacharyya"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.640": {
    "title": "Beyond Single-Audio: Advancing Multi-Audio Processing in Audio Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Chen",
      "Xianghu Yue",
      "Xiaoxue Gao",
      "Chen Zhang",
      "Luis Fernando D’Haro",
      "Robby Tan",
      "Haizhou Li"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.641": {
    "title": "Multimodal Procedural Planning via Dual Text-Image Prompting",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujie Lu",
      "Pan Lu",
      "Zhiyu Chen",
      "Wanrong Zhu",
      "Xin Wang",
      "William Yang Wang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.642": {
    "title": "Functionality learning through specification instructions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pedro Henrique Luz De Araujo",
      "Benjamin Roth"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.643": {
    "title": "DictDis: Dictionary Constrained Disambiguation for Improved NMT",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ayush Maheshwari",
      "Preethi Jyothi",
      "Ganesh Ramakrishnan"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.644": {
    "title": "Fighting Randomness with Randomness: Mitigating Optimisation Instability of Fine-Tuning using Delayed Ensemble and Noisy Interpolation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Branislav Pecher",
      "Jan Cegin",
      "Robert Belanec",
      "Jakub Simko",
      "Ivan Srba",
      "Maria Bielikova"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.645": {
    "title": "Rethinking Code Refinement: Learning to Judge Code Efficiency",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minju Seo",
      "Jinheon Baek",
      "Sung Ju Hwang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.646": {
    "title": "Selection-p: Self-Supervised Task-Agnostic Prompt Compression for Faithfulness and Transferability",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tsz Ting Chung",
      "Leyang Cui",
      "Lemao Liu",
      "Xinting Huang",
      "Shuming Shi",
      "Dit-Yan Yeung"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.647": {
    "title": "Adaptive Token Biaser: Knowledge Editing via Biasing Key Entities",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baolong Bi",
      "Shenghua Liu",
      "Yiwei Wang",
      "Lingrui Mei",
      "Hongcheng Gao",
      "Yilong Xu",
      "Xueqi Cheng"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.648": {
    "title": "Improving Factual Consistency of News Summarization by Contrastive Preference Optimization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huawen Feng",
      "Yan Fan",
      "Xiong Liu",
      "Ting-En Lin",
      "Zekun Yao",
      "Yuchuan Wu",
      "Fei Huang",
      "Yongbin Li",
      "Qianli Ma"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.649": {
    "title": "AlanaVLM: A Multimodal Embodied AI Foundation Model for Egocentric Video Understanding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alessandro Suglia",
      "Claudio Greco",
      "Katie Baker",
      "Jose Part",
      "Ioannis Papaioannou",
      "Arash Eshghi",
      "Ioannis Konstas",
      "Oliver Lemon"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.650": {
    "title": "Platform-Invariant Topic Modeling via Contrastive Learning to Mitigate Platform-Induced Bias",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minseo Koo",
      "Doeun Kim",
      "Sungwon Han",
      "Sungkyu Park"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.651": {
    "title": "MAVEN-FACT: A Large-scale Event Factuality Detection Dataset",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chunyang Li",
      "Hao Peng",
      "Xiaozhi Wang",
      "Yunjia Qi",
      "Lei Hou",
      "Bin Xu",
      "Juanzi Li"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.652": {
    "title": "Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kranti Ch",
      "Sherzod Hakimov",
      "David Schlangen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.653": {
    "title": "Make Compound Sentences Simple to Analyze: Learning to Split Sentences for Aspect-based Sentiment Analysis",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongsik Seo",
      "Sungwon Song",
      "Ryang Heo",
      "Jieyong Kim",
      "Dongha Lee"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.654": {
    "title": "LLMs-as-Instructors: Learning from Errors Toward Automating Model Improvement",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahao Ying",
      "Mingbao Lin",
      "Yixin Cao",
      "Wei Tang",
      "Bo Wang",
      "Qianru Sun",
      "Xuanjing Huang",
      "Shuicheng Yan"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.655": {
    "title": "ITER: Iterative Transformer-based Entity Recognition and Relation Extraction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Moritz Hennen",
      "Florian Babl",
      "Michaela Geierhos"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.656": {
    "title": "Zero-shot Persuasive Chatbots with LLM-Generated Strategies and Information Retrieval",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kazuaki Furumai",
      "Roberto Legaspi",
      "Julio Romero",
      "Yudai Yamazaki",
      "Yasutaka Nishimura",
      "Sina Semnani",
      "Kazushi Ikeda",
      "Weiyan Shi",
      "Monica Lam"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.657": {
    "title": "Logits Reranking via Semantic Labels for Hard Samples in Text Classification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peijie Huang",
      "Junbao Huang",
      "Yuhong Xu",
      "Weizhen Li",
      "Xisheng Xiao"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.658": {
    "title": "Scaling Laws for Fact Memorization of Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyu Lu",
      "Xiaonan Li",
      "Qinyuan Cheng",
      "Kai Ding",
      "Xuanjing Huang",
      "Xipeng Qiu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.659": {
    "title": "Breaking the Script Barrier in Multilingual Pre-Trained Language Models with Transliteration-Based Post-Training Alignment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Orgest Xhelili",
      "Yihong Liu",
      "Hinrich Schuetze"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.660": {
    "title": "Leveraging Web-Crawled Data for High-Quality Fine-Tuning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Zhou",
      "Chenglin Jiang",
      "Wei Shen",
      "Xiao Zhou",
      "Xiaonan He"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.661": {
    "title": "Designing Logic Pattern Templates for Counter-Argument Logical Structure Analysis",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shoichi Naito",
      "Wenzhi Wang",
      "Paul Reisert",
      "Naoya Inoue",
      "Camélia Guerraoui",
      "Kenshi Yamaguchi",
      "Jungmin Choi",
      "Irfan Robbani",
      "Surawat Pothong",
      "Kentaro Inui"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.662": {
    "title": "Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhua Cheng",
      "Weiwei Zhang",
      "Haihao Shen",
      "Yiyang Cai",
      "Xin He",
      "Lv Kaokao",
      "Yi Liu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.663": {
    "title": "Using LLMs to simulate students' responses to exam questions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luca Benedetto",
      "Giovanni Aradelli",
      "Antonia Donvito",
      "Alberto Lucchetti",
      "Andrea Cappelli",
      "Paula Buttery"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.664": {
    "title": "HSDreport: Heart Sound Diagnosis with Echocardiography Reports",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihan Zhao",
      "Pingjie Wang",
      "Liudan Zhao",
      "Yuchen Yang",
      "Ya Zhang",
      "Kun Sun",
      "Xin Sun",
      "Xin Zhou",
      "Yu Wang",
      "Yanfeng Wang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.665": {
    "title": "Repairing Catastrophic-Neglect in Text-to-Image Diffusion Models via Attention-Guided Feature Enhancement",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan Chang",
      "Mingyang Li",
      "Junjie Wang",
      "Yi Liu",
      "Qing Wang",
      "Yang Liu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.666": {
    "title": "Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeonghun Yeo",
      "Seunghee Han",
      "Minsu Kim",
      "Yong Man Ro"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.667": {
    "title": "MDCR: A Dataset for Multi-Document Conditional Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peter Baile Chen",
      "Yi Zhang",
      "Chunwei Liu",
      "Sejal Gupta",
      "Yoon Kim",
      "Mike Cafarella"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.668": {
    "title": "Will LLMs Sink or Swim? Exploring Decision-Making Under Pressure",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyusik Kim",
      "Hyeonseok Jeon",
      "Jeongwoo Ryu",
      "Bongwon Suh"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.669": {
    "title": "Zero-shot Commonsense Reasoning over Machine Imagination",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyuntae Park",
      "Yeachan Kim",
      "Jun-Hyung Park",
      "SangKeun Lee"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.670": {
    "title": "A Framework of Knowledge Graph-Enhanced Large Language Model Based on Question Decomposition and Atomic Retrieval",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yading Li",
      "Dandan Song",
      "Changzhi Zhou",
      "Yuhang Tian",
      "Hao Wang",
      "Ziyi Yang",
      "Shuhao Zhang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.671": {
    "title": "Vanessa: Visual Connotation and Aesthetic Attributes Understanding Network for Multimodal Aspect-based Sentiment Analysis",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luwei Xiao",
      "Rui Mao",
      "Xulang Zhang",
      "Liang He",
      "Erik Cambria"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.672": {
    "title": "Consistent Document-level Relation Extraction via Counterfactuals",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Modarressi",
      "Abdullatif Köksal",
      "Hinrich Schuetze"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.673": {
    "title": "Enhancing Learning-Based Binary Code Similarity Detection Model through Adversarial Training with Multiple Function Variants",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lichen Jia",
      "Chenggang Wu",
      "Bowen Tang",
      "Peihua Zhang",
      "Zihan Jiang",
      "Yang Yang",
      "Ning Liu",
      "Jingfeng Zhang",
      "Zhe Wang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.674": {
    "title": "Ask the experts: sourcing a high-quality nutrition counseling dataset through Human-AI collaboration",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simone Balloccu",
      "Ehud Reiter",
      "Karen Li",
      "Rafael Sargsyan",
      "Vivek Kumar",
      "Diego Reforgiato",
      "Daniele Riboni",
      "Ondrej Dusek"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.675": {
    "title": "HealthAlignSumm : Utilizing Alignment for Multimodal Summarization of Code-Mixed Healthcare Dialogues",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akash Ghosh",
      "Arkadeep Acharya",
      "Sriparna Saha",
      "Gaurav Pandey",
      "Dinesh Raghu",
      "Setu Sinha"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.676": {
    "title": "Revisiting the Impact of Pursuing Modularity for Code Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Deokyeong Kang",
      "KiJung Seo",
      "Taeuk Kim"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.677": {
    "title": "A Decoding Algorithm for Length-Control Summarization Based on Directed Acyclic Transformers",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenyang Huang",
      "Hao Zhou",
      "Cameron Jen",
      "Kangjie Zheng",
      "Osmar Zaiane",
      "Lili Mou"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.678": {
    "title": "R2AG: Incorporating Retrieval Information into Retrieval Augmented Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fuda Ye",
      "Shuangyin Li",
      "Yongqi Zhang",
      "Lei Chen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.679": {
    "title": "Not (yet) the whole story: Evaluating Visual Storytelling Requires More than Measuring Coherence, Grounding, and Repetition",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya Kaushik Surikuchi",
      "Raquel Fernández",
      "Sandro Pezzelle"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.680": {
    "title": "Gender Identity in Pretrained Language Models: An Inclusive Approach to Data Creation and Probing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Urban Knupleš",
      "Agnieszka Falenska",
      "Filip Miletić"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.681": {
    "title": "Vorbești Românește?\" A Recipe to Train Powerful Romanian LLMs with English Instructions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mihai Masala",
      "Denis Ilie-Ablachim",
      "Alexandru Dima",
      "Dragos Georgian Corlatescu",
      "Miruna-Andreea Zavelca",
      "Ovio Olaru",
      "Simina-Maria Terian",
      "Andrei Terian",
      "Marius Leordeanu",
      "Horia Velicu",
      "Marius Popescu",
      "Mihai Dascalu",
      "Traian Rebedea"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.682": {
    "title": "Generalized Measures of Anticipation and Responsivity in Online Language Processing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mario Giulianelli",
      "Andreas Opedal",
      "Ryan Cotterell"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.683": {
    "title": "Towards Effective Counter-Responses: Aligning Human Preferences with Strategies to Combat Online Trolling",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huije Lee",
      "Hoyun Song",
      "Jisu Shin",
      "Sukmin Cho",
      "SeungYoon Han",
      "Jong Park"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.684": {
    "title": "Soda-Eval: Open-Domain Dialogue Evaluation in the age of LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "John Mendonça",
      "Isabel Trancoso",
      "Alon Lavie"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.685": {
    "title": "A Comprehensive Survey of Hallucination in Large Language, Image, Video and Audio Foundation Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pranab Sahoo",
      "Prabhash Meharia",
      "Akash Ghosh",
      "Sriparna Saha",
      "Vinija Jain",
      "Aman Chadha"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.686": {
    "title": "Predicting generalization performance with correctness discriminators",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuekun Yao",
      "Alexander Koller"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.687": {
    "title": "FastMem: Fast Memorization of Prompt Improves Context Awareness of Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyi Zhu",
      "Shuochen Liu",
      "Yu Yu",
      "Bo Tang",
      "Yibo Yan",
      "Zhiyu Li",
      "Feiyu Xiong",
      "Tong Xu",
      "Matthew Blaschko"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.688": {
    "title": "Towards More Robust NLP System Evaluation: Handling Missing Scores in Benchmarks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anas Himmi",
      "Ekhine Irurozki",
      "Nathan Noiry",
      "Stephan Clémençon",
      "Pierre Colombo"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.689": {
    "title": "Mixed-Session Conversation with Egocentric Memory",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jihyoung Jang",
      "Taeyoung Kim",
      "Hyounghun Kim"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.690": {
    "title": "CSLM: A Framework for Question Answering Dataset Generation through Collaborative Small Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Wang",
      "Yang Liu",
      "Lingchen Wang",
      "An Xiao"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.691": {
    "title": "Large Language Models Can Not Perform Well in Understanding and Manipulating Natural Language at Both Character and Word Levels?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yidan Zhang",
      "Zhenan He"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.692": {
    "title": "Virtual Context Enhancing Jailbreak Attacks with Special Token Injection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuqi Zhou",
      "Lin Lu",
      "Ryan Sun",
      "Pan Zhou",
      "Lichao Sun"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.693": {
    "title": "Think Twice Before Trusting: Self-Detection for Large Language Models through Comprehensive Answer Reflection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Moxin Li",
      "Wenjie Wang",
      "Fuli Feng",
      "Fengbin Zhu",
      "Qifan Wang",
      "Tat-Seng Chua"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.694": {
    "title": "Automating Easy Read Text Segmentation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jesus Calleja",
      "Thierry Etchegoyhen",
      "Antonio David Ponce Martínez"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.695": {
    "title": "Position Paper: Data-Centric AI in the Age of Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyi Xu",
      "Zhaoxuan Wu",
      "Rui Qiao",
      "Arun Verma",
      "Yao Shu",
      "Jingtan Wang",
      "Xinyuan Niu",
      "Zhenfeng He",
      "Jiangwei Chen",
      "Zijian Zhou",
      "Gregory Kang Ruey Lau",
      "Hieu Dao",
      "Lucas Agussurja",
      "Rachael Hwee Ling Sim",
      "Xiaoqiang Lin",
      "Wenyang Hu",
      "Zhongxiang Dai",
      "Pang Wei Koh",
      "Bryan Kian Hsiang Low"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.696": {
    "title": "MATHWELL: Generating Educational Math Word Problems Using Teacher Annotations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bryan Christ",
      "Jonathan Kropko",
      "Thomas Hartvigsen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.697": {
    "title": "Resilience of Large Language Models for Noisy Instructions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bin Wang",
      "Chengwei Wei",
      "Zhengyuan Liu",
      "Geyu Lin",
      "Nancy Chen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.698": {
    "title": "LLM-TOPLA: Efficient LLM Ensemble by Maximising Diversity",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Selim Tekin",
      "Fatih Ilhan",
      "Tiansheng Huang",
      "Sihao Hu",
      "Ling Liu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.699": {
    "title": "Augmenting Reasoning Capabilities of LLMs with Graph Structures in Knowledge Base Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhang Tian",
      "Dandan Song",
      "Zhijing Wu",
      "Changzhi Zhou",
      "Hao Wang",
      "Jun Yang",
      "Jing Xu",
      "Ruanmin Cao",
      "HaoYu Wang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.700": {
    "title": "Creative Problem Solving in Large Language and Vision Models - What Would it Take?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lakshmi Nair",
      "Evana Gizzi",
      "Jivko Sinapov"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.701": {
    "title": "Cross-Lingual Multi-Hop Knowledge Editing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditi Khandelwal",
      "Harman Singh",
      "Hengrui Gu",
      "Tianlong Chen",
      "Kaixiong Zhou"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.702": {
    "title": "Android in the Zoo: Chain-of-Action-Thought for GUI Agents",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiwen Zhang",
      "Jihao Wu",
      "Teng Yihua",
      "Minghui Liao",
      "Nuo Xu",
      "Xiao Xiao",
      "Zhongyu Wei",
      "Duyu Tang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.703": {
    "title": "Self-Recognition in Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tim Davidson",
      "Viacheslav Surkov",
      "Veniamin Veselovsky",
      "Giuseppe Russo",
      "Robert West",
      "Caglar Gulcehre"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.704": {
    "title": "Beyond Accuracy Optimization: Computer Vision Losses for Large Language Model Fine-Tuning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniele Rege Cambrin",
      "Giuseppe Gallipoli",
      "Irene Benedetto",
      "Luca Cagliero",
      "Paolo Garza"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.705": {
    "title": "The Shape of Word Embeddings: Quantifying Non-Isometry with Topological Data Analysis",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ondřej Draganov",
      "Steven Skiena"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.706": {
    "title": "Towards Robust Evaluation of Unlearning in LLMs via Data Transformations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhinav Joshi",
      "Shaswati Saha",
      "Divyaksh Shukla",
      "Sriram Vema",
      "Harsh Jhamtani",
      "Manas Gaur",
      "Ashutosh Modi"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.707": {
    "title": "Numbers Matter! Bringing Quantity-awareness to Retrieval Systems",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Satya Almasian",
      "Milena Bruseva",
      "Michael Gertz"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.708": {
    "title": "Stark: Social Long-Term Multi-Modal Conversation with Persona Commonsense Knowledge",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Young-Jun Lee",
      "Dokyong Lee",
      "Junyoung Youn",
      "Kyeong-Jin Oh",
      "Byungsoo Ko",
      "Jonghwan Hyeon",
      "Ho-Jin Choi"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.709": {
    "title": "Dual-Phase Accelerated Prompt Optimization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muchen Yang",
      "Moxin Li",
      "Yongle Li",
      "Zijun Chen",
      "Chongming Gao",
      "Junqi Zhang",
      "Yangyang Li",
      "Fuli Feng"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.710": {
    "title": "ChartInsights: Evaluating Multimodal Large Language Models for Low-Level Chart Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Wu",
      "Lutao Yan",
      "Leixian Shen",
      "Yunhai Wang",
      "Nan Tang",
      "Yuyu Luo"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.711": {
    "title": "Communicate to Play: Pragmatic Reasoning for Efficient Cross-Cultural Communication",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Isadora White",
      "Sashrika Pandey",
      "Michelle Pan"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.712": {
    "title": "SAFARI: Cross-lingual Bias and Factuality Detection in News Media and News Articles",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dilshod Azizov",
      "Zain Mujahid",
      "Hilal AlQuabeh",
      "Preslav Nakov",
      "Shangsong Liang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.713": {
    "title": "CantTalkAboutThis: Aligning Language Models to Stay on Topic in Dialogues",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Makesh Narsimhan Sreedhar",
      "Traian Rebedea",
      "Shaona Ghosh",
      "Jiaqi Zeng",
      "Christopher Parisien"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.714": {
    "title": "An LLM-Enabled Knowledge Elicitation and Retrieval Framework for Zero-Shot Cross-Lingual Stance Identification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruike Zhang",
      "Yuan Tian",
      "Penghui Wei",
      "Daniel Zeng",
      "Wenji Mao"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.715": {
    "title": "TuringQ: Benchmarking AI Comprehension in Theory of Computation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pardis Zahraei",
      "Ehsaneddin Asgari"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.716": {
    "title": "Learning to Refine with Fine-Grained Natural Language Feedback",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manya Wadhwa",
      "Xinyu Zhao",
      "Junyi Jessy Li",
      "Greg Durrett"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.717": {
    "title": "Implicit Personalization in Language Models: A Systematic Study",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhijing Jin",
      "Nils Heil",
      "Jiarui Liu",
      "Shehzaad Dhuliawala",
      "Yahang Qi",
      "Bernhard Schölkopf",
      "Rada Mihalcea",
      "Mrinmaya Sachan"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.718": {
    "title": "When the Misidentified Adverbial Phrase Functions as a Complement",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yige Chen",
      "Kyuwon Kim",
      "KyungTae Lim",
      "Jungyeul Park",
      "Chulwoo Park"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.719": {
    "title": "Unveiling Implicit Table Knowledge with Question-Then-Pinpoint Reasoner for Insightful Table Summarization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kwangwook Seo",
      "Jinyoung Yeo",
      "Dongha Lee"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.720": {
    "title": "Few-shot Prompting for Pairwise Ranking: An Effective Non-Parametric Retrieval Model",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nilanjan Sinhababu",
      "Andrew Parry",
      "Debasis Ganguly",
      "Debasis Samanta",
      "Pabitra Mitra"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.721": {
    "title": "Self-training Language Models for Arithmetic Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marek Kadlčík",
      "Michal Štefánik"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.722": {
    "title": "PPTC-R benchmark: Towards Evaluating the Robustness of Large Language Models for PowerPoint Task Completion",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zekai Zhang",
      "Yiduo Guo",
      "Yaobo Liang",
      "Dongyan Zhao",
      "Nan Duan"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.723": {
    "title": "Efficient Pointwise-Pairwise Learning-to-Rank for News Recommendation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nithish Kannen",
      "Yao Ma",
      "Gerrit Van Den Burg",
      "Jean Baptiste Faddoul"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.724": {
    "title": "Fast Matrix Multiplications for Lookup Table-Quantized LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Guo",
      "William Brandon",
      "Radostin Cholakov",
      "Jonathan Ragan-Kelley",
      "Eric Xing",
      "Yoon Kim"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.725": {
    "title": "Distance-aware Calibration for Pre-trained Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alberto Gasparin",
      "Gianluca Detommaso"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.726": {
    "title": "Language Models are Surprisingly Fragile to Drug Names in Biomedical Benchmarks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jack Gallifant",
      "Shan Chen",
      "Pedro Moreira",
      "Nikolaj Munch",
      "Mingye Gao",
      "Jackson Pond",
      "Leo Anthony Celi",
      "Hugo Aerts",
      "Thomas Hartvigsen",
      "Danielle Bitterman"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.727": {
    "title": "To Err Is Human, but Llamas Can Learn It Too",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Agnes Luhtaru",
      "Taido Purason",
      "Martin Vainikko",
      "Maksym Del",
      "Mark Fishel"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.728": {
    "title": "PizzaCommonSense: A Dataset for Commonsense Reasoning about Intermediate Steps in Cooking Recipes",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aissatou Diallo",
      "Antonis Bikakis",
      "Luke Dickens",
      "Anthony Hunter",
      "Rob Miller"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.729": {
    "title": "Enhancing Discourse Dependency Parsing with Sentence Dependency Parsing: A Unified Generative Method Based on Code Representation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zizhuo Shen",
      "Yanqiu Shao",
      "Wei Li"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.730": {
    "title": "Knowing When You Don't Know\": A Multilingual Relevance Assessment Dataset for Robust Retrieval-Augmented Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nandan Thakur",
      "Luiz Bonifacio",
      "Crystina Zhang",
      "Odunayo Ogundepo",
      "Ehsan Kamalloo",
      "David Alfonso-Hermelo",
      "Xiaoguang Li",
      "Qun Liu",
      "Boxing Chen",
      "Mehdi Rezagholizadeh",
      "Jimmy Lin"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.731": {
    "title": "Diverse and Effective Synthetic Data Generation for Adaptable Zero-Shot Dialogue State Tracking",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "James Finch",
      "Jinho Choi"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.732": {
    "title": "Can We Instruct LLMs to Compensate for Position Bias?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meiru Zhang",
      "Zaiqiao Meng",
      "Nigel Collier"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.733": {
    "title": "Textual Dataset Distillation via Language Model Embedding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yefan Tao",
      "Luyang Kong",
      "Andrey Kan",
      "Laurent Callot"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.734": {
    "title": "TARA: Token-level Attribute Relation Adaptation for Multi-Attribute Controllable Text Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilin Cao",
      "Jiahao Zhao",
      "Ruike Zhang",
      "Hanyi Zou",
      "Wenji Mao"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.735": {
    "title": "AuriSRec: Adversarial User Intention Learning in Sequential Recommendation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junjie Zhang",
      "Ruobing Xie",
      "Wenqi Sun",
      "Leyu Lin",
      "Xin Zhao",
      "Ji-Rong Wen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.736": {
    "title": "Denoising Rationalization for Multi-hop Fact Verification via Multi-granular Explainer",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiasheng Si",
      "Yingjie Zhu",
      "Wenpeng Lu",
      "Deyu Zhou"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.737": {
    "title": "README: Bridging Medical Jargon and Lay Understanding for Patient Education through Data-Centric NLP",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zonghai Yao",
      "Nandyala Siddharth Kantu",
      "Guanghao Wei",
      "Hieu Tran",
      "Zhangqi Duan",
      "Sunjae Kwon",
      "Zhichao Yang",
      "Hong Yu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.738": {
    "title": "Pre-trained Language Models Return Distinguishable Probability Distributions to Unfaithfully Hallucinated Texts",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taehun Cha",
      "Donghun Lee"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.739": {
    "title": "Cognitive Bias in Decision-Making with LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jessica Echterhoff",
      "Yao Liu",
      "Abeer Alessa",
      "Julian McAuley",
      "Zexue He"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.740": {
    "title": "Problem-Oriented Segmentation and Retrieval: Case Study on Tutoring Conversations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rose Wang",
      "Pawan Wirawarn",
      "Kenny Lam",
      "Omar Khattab",
      "Dorottya Demszky"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.741": {
    "title": "Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kang He",
      "Yinghan Long",
      "Kaushik Roy"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.742": {
    "title": "Can't Remember Details in Long Documents? You Need Some R&R",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Devanshu Agrawal",
      "Shang Gao",
      "Martin Gajek"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.743": {
    "title": "HumVI: A Multilingual Dataset for Detecting Violent Incidents Impacting Humanitarian Aid",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hemank Lamba",
      "Anton Abilov",
      "Ke Zhang",
      "Elizabeth Olson",
      "Henry Dambanemuya",
      "João Bárcia",
      "David Batista",
      "Christina Wille",
      "Aoife Cahill",
      "Joel Tetreault",
      "Alejandro Jaimes"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.744": {
    "title": "Improving Quotation Attribution with Fictional Character Embeddings",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaspard Michel",
      "Elena Epure",
      "Romain Hennequin",
      "Christophe Cerisara"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.745": {
    "title": "Robust Text Classification: Analyzing Prototype-Based Networks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhivar Sourati",
      "Darshan Deshpande",
      "Filip Ilievski",
      "Kiril Gashteovski",
      "Sascha Saralajew"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.746": {
    "title": "GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shilong Li",
      "Yancheng He",
      "Hangyu Guo",
      "Xingyuan Bu",
      "Ge Bai",
      "Jie Liu",
      "Jiaheng Liu",
      "Xingwei Qu",
      "Yangguang Li",
      "Wanli Ouyang",
      "Wenbo Su",
      "Bo Zheng"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.747": {
    "title": "Compare without Despair: Reliable Preference Evaluation with Generation Separability",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sayan Ghosh",
      "Tejas Srinivasan",
      "Swabha Swayamdipta"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.748": {
    "title": "LoRASC: Expressive and Generalizable Low-rank Adaptation for Large Models via Slow Cascaded Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siwei Li",
      "Yifan Yang",
      "Yifei Shen",
      "Fangyun Wei",
      "Zongqing Lu",
      "Lili Qiu",
      "Yuqing Yang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.749": {
    "title": "SQFT: Low-cost Model Adaptation in Low-precision Sparse Foundation Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juan Munoz",
      "Jinjie Yuan",
      "Nilesh Jain"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.750": {
    "title": "Securing Multi-turn Conversational Language Models From Distributed Backdoor Attacks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Terry Tong",
      "Qin Liu",
      "Jiashu Xu",
      "Muhao Chen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.751": {
    "title": "InternalInspector I2: Robust Confidence Estimation in LLMs through Internal States",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Beigi",
      "Ying Shen",
      "Runing Yang",
      "Zihao Lin",
      "Qifan Wang",
      "Ankith Mohan",
      "Jianfeng He",
      "Ming Jin",
      "Chang-Tien Lu",
      "Lifu Huang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.752": {
    "title": "All You Need is Attention: Lightweight Attention-based Data Augmentation for Text Classification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junehyung Kim",
      "Sungjae Hwang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.753": {
    "title": "Adversarial Attacks on Parts of Speech: An Empirical Study in Text-to-Image Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "G Shahariar",
      "Jia Chen",
      "Jiachen Li",
      "Yue Dong"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.754": {
    "title": "Enhancing Alignment using Curriculum Learning & Ranked Preferences",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pulkit Pattnaik",
      "Rishabh Maheshwary",
      "Kelechi Ogueji",
      "Vikas Yadav",
      "Sathwik Tejaswi Madhusudhan"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.755": {
    "title": "Multi-Target Cross-Lingual Summarization: a novel task and a language-neutral approach",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Diogo Pernes",
      "Gonçalo Correia",
      "Afonso Mendes"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.756": {
    "title": "Tab2Text - A framework for deep learning with tabular data",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tong Lin",
      "Jason Yan",
      "David Jurgens",
      "Sabina Tomkins"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.757": {
    "title": "More Bang for your Context: Virtual Documents for Question Answering over Long Documents",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yosi Mass",
      "Boaz Carmeli",
      "Asaf Yehudai",
      "Assaf Toledo",
      "Nathaniel Mills"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.758": {
    "title": "Out-of-Distribution Detection through Soft Clustering with Non-Negative Kernel Regression",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aryan Gulati",
      "Xingjian Dong",
      "Carlos Hurtado",
      "Sarath Shekkizhar",
      "Swabha Swayamdipta",
      "Antonio Ortega"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.759": {
    "title": "Synthetic Multimodal Question Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ian Wu",
      "Sravan Jayanthi",
      "Vijay Viswanathan",
      "Simon Rosenberg",
      "Sina Pakazad",
      "Tongshuang Wu",
      "Graham Neubig"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.760": {
    "title": "Lost in Translation: Chemical Language Models and the Misunderstanding of Molecule Structures",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Veronika Ganeeva",
      "Andrey Sakhovskiy",
      "Kuzma Khrabrov",
      "Andrey Savchenko",
      "Artur Kadurin",
      "Elena Tutubalina"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.761": {
    "title": "HyQE: Ranking Contexts with Hypothetical Query Embeddings",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weichao Zhou",
      "Jiaxin Zhang",
      "Hilaf Hasson",
      "Anu Singh",
      "Wenchao Li"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.762": {
    "title": "Model Merging and Safety Alignment: One Bad Model Spoils the Bunch",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hasan Hammoud",
      "Umberto Michieli",
      "Fabio Pizzati",
      "Philip Torr",
      "Adel Bibi",
      "Bernard Ghanem",
      "Mete Ozay"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.763": {
    "title": "Large Language Models Are Challenged by Habitat-Centered Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sadaf Ghaffari",
      "Nikhil Krishnaswamy"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.764": {
    "title": "How to Train Your Fact Verifier: Knowledge Transfer with Multimodal Open Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaeyoung Lee",
      "Ximing Lu",
      "Jack Hessel",
      "Faeze Brahman",
      "Youngjae Yu",
      "Yonatan Bisk",
      "Yejin Choi",
      "Saadia Gabriel"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.765": {
    "title": "Benchmarking Machine Translation with Cultural Awareness",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Binwei Yao",
      "Ming Jiang",
      "Tara Bobinac",
      "Diyi Yang",
      "Junjie Hu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.766": {
    "title": "Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is Needed?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tannon Kew",
      "Florian Schottmann",
      "Rico Sennrich"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.767": {
    "title": "Temperature-Centric Investigation of Speculative Decoding with Knowledge Distillation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siru Ouyang",
      "Shuohang Wang",
      "Minhao Jiang",
      "Ming Zhong",
      "Donghan Yu",
      "Jiawei Han",
      "Yelong Shen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.768": {
    "title": "Generate then Refine: Data Augmentation for Zero-shot Intent Detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "I-Fan Lin",
      "Faegheh Hasibi",
      "Suzan Verberne"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.769": {
    "title": "Unleashing the Power of Large Language Models in Zero-shot Relation Extraction via Self-Prompting",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyi Liu",
      "Yang Li",
      "Jiang Li",
      "Shan Yang",
      "Yunshi Lan"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.770": {
    "title": "What is the value of templates?\" Rethinking Document Information Extraction Datasets for LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ran Zmigrod",
      "Pranav Shetty",
      "Mathieu Sibue",
      "Zhiqiang Ma",
      "Armineh Nourbakhsh",
      "Xiaomo Liu",
      "Manuela Veloso"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.771": {
    "title": "What Matters in Memorizing and Recalling Facts? Multifaceted Benchmarks for Knowledge Probing in Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Zhao",
      "Naoki Yoshinaga",
      "Daisuke Oba"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.772": {
    "title": "On Leakage of Code Generation Evaluation Datasets",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexandre Matton",
      "Tom Sherborne",
      "Dennis Aumiller",
      "Elena Tommasone",
      "Milad Alizadeh",
      "Jingyi He",
      "Raymond Ma",
      "Maxime Voisin",
      "Ellen Gilsenan-McMahon",
      "Matthias Gallé"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.773": {
    "title": "The Language of Trauma: Modeling Traumatic Event Descriptions Across Domains with Explainable AI",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miriam Schirmer",
      "Tobias Leemann",
      "Gjergji Kasneci",
      "Jürgen Pfeffer",
      "David Jurgens"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.774": {
    "title": "Auto-Evolve: Enhancing Large Language Model's Performance via Self-Reasoning Framework",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Krishna Aswani",
      "Huilin Lu",
      "Pranav Patankar",
      "Priya Dhalwani",
      "Xue Tan",
      "Jayant Ganeshmohan",
      "Simon Lacasse"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.775": {
    "title": "V-DPO: Mitigating Hallucination in Large Vision Language Models via Vision-Guided Direct Preference Optimization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxi Xie",
      "Guanzhen Li",
      "Xiao Xu",
      "Min-Yen Kan"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.776": {
    "title": "Exploring the Potential of Multimodal LLM with Knowledge-Intensive Multimodal ASR",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minghan Wang",
      "Yuxia Wang",
      "Thuy-Trang Vu",
      "Ehsan Shareghi",
      "Reza Haf"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.777": {
    "title": "Better Alignment with Instruction Back-and-Forth Translation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thao Nguyen",
      "Jeffrey Li",
      "Sewoong Oh",
      "Ludwig Schmidt",
      "Jason Weston",
      "Luke Zettlemoyer",
      "Xian Li"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.778": {
    "title": "AliGATr: Graph-based layout generation for form understanding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Armineh Nourbakhsh",
      "Zhao Jin",
      "Siddharth Parekh",
      "Sameena Shah",
      "Carolyn Rose"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.779": {
    "title": "Attribute Controlled Fine-tuning for Large Language Models: A Case Study on Detoxification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Meng",
      "Ninareh Mehrabi",
      "Palash Goyal",
      "Anil Ramakrishna",
      "Aram Galstyan",
      "Richard Zemel",
      "Kai-Wei Chang",
      "Rahul Gupta",
      "Charith Peris"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.780": {
    "title": "SciDoc2Diagrammer-MAF: Towards Generation of Scientific Diagrams from Documents guided by Multi-Aspect Feedback Refinement",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ishani Mondal",
      "Zongxia Li",
      "Yufang Hou",
      "Anandhavelu Natarajan",
      "Aparna Garimella",
      "Jordan Boyd-Graber"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.781": {
    "title": "TinyStyler: Efficient Few-Shot Text Style Transfer with Authorship Embeddings",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zachary Horvitz",
      "Ajay Patel",
      "Kanishk Singh",
      "Chris Callison-Burch",
      "Kathleen McKeown",
      "Zhou Yu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.782": {
    "title": "Can LLMs Understand the Implication of Emphasized Sentences in Dialogue?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guan-Ting Lin",
      "Hung-yi Lee"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.783": {
    "title": "Why do LLaVA Vision-Language Models Reply to Images in English?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Musashi Hinck",
      "Carolin Holtermann",
      "Matthew Olson",
      "Florian Schneider",
      "Sungduk Yu",
      "Anahita Bhiwandiwalla",
      "Anne Lauscher",
      "Shao-Yen Tseng",
      "Vasudev Lal"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.784": {
    "title": "Preference Tuning For Toxicity Mitigation Generalizes Across Languages",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaochen Li",
      "Zheng Xin Yong",
      "Stephen Bach"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.785": {
    "title": "Calibrating Long-form Generations From Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yukun Huang",
      "Yixin Liu",
      "Raghuveer Thirukovalluru",
      "Arman Cohan",
      "Bhuwan Dhingra"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.786": {
    "title": "Train Once, Deploy Anywhere: Matryoshka Representation Learning for Multimodal Recommendation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yueqi Wang",
      "Zhenrui Yue",
      "Huimin Zeng",
      "Dong Wang",
      "Julian McAuley"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.787": {
    "title": "Exploring Quantization for Efficient Pre-Training of Transformer Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kamran Chitsaz",
      "Quentin Fournier",
      "Goncalo Mordido",
      "Sarath Chandar"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.788": {
    "title": "Multilingual Synopses of Movie Narratives: A Dataset for Vision-Language Story Understanding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yidan Sun",
      "Jianfei Yu",
      "Boyang Li"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.789": {
    "title": "MVP-Bench: Can Large Vision-Language Models Conduct Multi-level Visual Perception Like Humans?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanzhen Li",
      "Yuxi Xie",
      "Min-Yen Kan"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.790": {
    "title": "Topic Modeling: Contextual Token Embeddings Are All You Need",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dimo Angelov",
      "Diana Inkpen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.791": {
    "title": "Dense Passage Retrieval: Is it Retrieving?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Reichman",
      "Larry Heck"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.792": {
    "title": "Margin Matching Preference Optimization: Enhanced Model Alignment with Granular Feedback",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyuyoung Kim",
      "Ah Seo",
      "Hao Liu",
      "Jinwoo Shin",
      "Kimin Lee"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.793": {
    "title": "AfriInstruct: Instruction Tuning of African Languages for Diverse Tasks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kosei Uemura",
      "Mahe Chen",
      "Alex Pejovic",
      "Chika Maduabuchi",
      "Yifei Sun",
      "En-Shiun Lee"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.794": {
    "title": "LLMs as Collaborator: Demands-Guided Collaborative Retrieval-Augmented Generation for Commonsense Knowledge-Grounded Open-Domain Dialogue Systems",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiong Yu",
      "Sixing Wu",
      "Jiahao Chen",
      "Wei Zhou"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.795": {
    "title": "ClaimVer: Explainable Claim-Level Verification and Evidence Attribution of Text Through Knowledge Graphs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Preetam Prabhu Srikar Dammu",
      "Himanshu Naidu",
      "Mouly Dewan",
      "YoungMin Kim",
      "Tanya Roosta",
      "Aman Chadha",
      "Chirag Shah"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.796": {
    "title": "Empirical Prior for Text Autoencoders",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongjing Yin",
      "Wenyang Gao",
      "Haodong Wu",
      "Jianhao Yan",
      "Yue Zhang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.797": {
    "title": "Pedagogical Alignment of Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shashank Sonkar",
      "Kangqi Ni",
      "Sapana Chaudhary",
      "Richard Baraniuk"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.798": {
    "title": "Reference-based Metrics Disprove Themselves in Question Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bang Nguyen",
      "Mengxia Yu",
      "Yun Huang",
      "Meng Jiang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.799": {
    "title": "Regression Aware Inference with LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michal Lukasik",
      "Harikrishna Narasimhan",
      "Aditya Menon",
      "Felix Yu",
      "Sanjiv Kumar"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.800": {
    "title": "R3-NL2GQL: A Model Coordination and Knowledge Graph Alignment Approach for NL2GQL",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhang Zhou",
      "Yu He",
      "Siyu Tian",
      "Yuchen Ni",
      "Zhangyue Yin",
      "Xiang Liu",
      "Chuanjun Ji",
      "Sen Liu",
      "Xipeng Qiu",
      "Guangnan Ye",
      "Hongfeng Chai"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.801": {
    "title": "Updating Large Language Models' Memories with Time Constraints",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Wu",
      "Yuqi Bu",
      "Yi Cai",
      "Tao Wang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.802": {
    "title": "DLoRA: Distributed Parameter-Efficient Fine-Tuning Solution for Large Language Model",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Gao",
      "Sai Qian Zhang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.803": {
    "title": "Cross-modality Information Check for Detecting Jailbreaking in Multimodal Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Xu",
      "Xiuyuan Qi",
      "Zhan Qin",
      "Wenjie Wang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.804": {
    "title": "Attacks against Abstractive Text Summarization Models through Lead Bias and Influence Functions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Poojitha Thota",
      "Shirin Nilizadeh"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.805": {
    "title": "One Model is All You Need: ByT5-Sanskrit, a Unified Model for Sanskrit NLP Tasks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Nehrdich",
      "Oliver Hellwig",
      "Kurt Keutzer"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.806": {
    "title": "NALA: an Effective and Interpretable Entity Alignment Method",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuanhao Xu",
      "Jingwei Cheng",
      "Fu Zhang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.807": {
    "title": "ConTReGen: Context-driven Tree-structured Retrieval for Open-domain Long-form Text Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kashob Kumar Roy",
      "Pritom Saha Akash",
      "Kevin Chang",
      "Lucian Popa"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.808": {
    "title": "Aligners: Decoupling LLMs and Alignment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lilian Ngweta",
      "Mayank Agarwal",
      "Subha Maity",
      "Alex Gittens",
      "Yuekai Sun",
      "Mikhail Yurochkin"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.809": {
    "title": "TOWER: Tree Organized Weighting for Evaluating Complex Instructions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Noah Ziems",
      "Zhihan Zhang",
      "Meng Jiang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.810": {
    "title": "Extractive Medical Entity Disambiguation with Memory Mechanism and Memorized Entity Information",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guobiao Zhang",
      "Xueping Peng",
      "Tao Shen",
      "Guodong Long",
      "Jiasheng Si",
      "Libo Qin",
      "Wenpeng Lu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.811": {
    "title": "QEFT: Quantization for Efficient Fine-Tuning of LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changhun Lee",
      "Jun-gyu Jin",
      "YoungHyun Cho",
      "Eunhyeok Park"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.812": {
    "title": "Skills-in-Context: Unlocking Compositionality in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaao Chen",
      "Xiaoman Pan",
      "Dian Yu",
      "Kaiqiang Song",
      "Xiaoyang Wang",
      "Dong Yu",
      "Jianshu Chen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.813": {
    "title": "DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLMs Jailbreakers",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xirui Li",
      "Ruochen Wang",
      "Minhao Cheng",
      "Tianyi Zhou",
      "Cho-Jui Hsieh"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.814": {
    "title": "Can LLMs Replace Clinical Doctors? Exploring Bias in Disease Diagnosis by Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yutian Zhao",
      "Huimin Wang",
      "Yuqi Liu",
      "Wu Suhuang",
      "Xian Wu",
      "Yefeng Zheng"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.815": {
    "title": "BLADE: Benchmarking Language Model Agents for Data-Driven Science",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ken Gu",
      "Ruoxi Shang",
      "Ruien Jiang",
      "Keying Kuang",
      "Richard-John Lin",
      "Donghe Lyu",
      "Yue Mao",
      "Youran Pan",
      "Teng Wu",
      "Jiaqian Yu",
      "Yikun Zhang",
      "Tianmai Zhang",
      "Lanyi Zhu",
      "Mike Merrill",
      "Jeffrey Heer",
      "Tim Althoff"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.816": {
    "title": "Phonetic and Lexical Discovery of Canine Vocalization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Theron Wang",
      "Xingyuan Li",
      "Chunhao Zhang",
      "Mengyue Wu",
      "Kenny Zhu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.817": {
    "title": "Audio-Based Linguistic Feature Extraction for Enhancing Multi-lingual and Low-Resource Text-to-Speech",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youngjae Kim",
      "Yejin Jeon",
      "Gary Lee"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.818": {
    "title": "LexC-Gen: Generating Data for Extremely Low-Resource Languages with Large Language Models and Bilingual Lexicons",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheng Xin Yong",
      "Cristina Menghini",
      "Stephen Bach"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.819": {
    "title": "Beyond Demographics: Aligning Role-playing LLM-based Agents Using Human Belief Networks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yun-Shiuan Chuang",
      "Krirk Nirunwiroj",
      "Zach Studdiford",
      "Agam Goyal",
      "Vincent Frigo",
      "Sijia Yang",
      "Dhavan Shah",
      "Junjie Hu",
      "Timothy Rogers"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.820": {
    "title": "PRoDeliberation: Parallel Robust Deliberation for End-to-End Spoken Language Understanding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Trang Le",
      "Daniel Lazar",
      "Suyoun Kim",
      "Shan Jiang",
      "Duc Le",
      "Adithya Sagar",
      "Aleksandr Livshits",
      "Ahmed Aly",
      "Akshat Shrivastava"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.821": {
    "title": "Downstream Trade-offs of a Family of Text Watermarks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anirudh Ajith",
      "Sameer Singh",
      "Danish Pruthi"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.822": {
    "title": "Knowledge-Aware Reasoning over Multimodal Semi-structured Tables",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suyash Mathur",
      "Jainit Bafna",
      "Kunal Kartik",
      "Harshita Khandelwal",
      "Manish Shrivastava",
      "Vivek Gupta",
      "Mohit Bansal",
      "Dan Roth"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.823": {
    "title": "Representational Isomorphism and Alignment of Multilingual Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Di Wu",
      "Yibin Lei",
      "Andrew Yates",
      "Christof Monz"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.824": {
    "title": "SWAG: Storytelling With Action Guidance",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathan Pei",
      "Zeeshan Patel",
      "Karim El-Refai",
      "Tianle Li"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.825": {
    "title": "Random Label Forests: An Ensemble Method with Label Subsampling For Extreme Multi-Label Problems",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng-Wei Chen",
      "Chih-Jen Lin"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.826": {
    "title": "Active Listening: Personalized Question Generation in Open-Domain Social Conversation with User Model Based Prompting",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kevin Bowden",
      "Yue Fan",
      "Winson Chen",
      "Wen Cui",
      "Davan Harrison",
      "Xin Wang",
      "Marilyn Walker"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.827": {
    "title": "Query-based Cross-Modal Projector Bolstering Mamba Multimodal LLM",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "SooHwan Eom",
      "Jay Shim",
      "Gwanhyeong Koo",
      "Haebin Na",
      "Mark Hasegawa-Johnson",
      "Sungwoong Kim",
      "Chang Yoo"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.828": {
    "title": "LLM as a metric critic for low resource relation identification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhe Yang",
      "Yi Huang",
      "Yaqin Chen",
      "Xiaoting Wu",
      "Junlan Feng",
      "Chao Deng"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.829": {
    "title": "Experience as Source for Anticipation and Planning: Experiential Policy Learning for Target-driven Recommendation Dialogues",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huy Dao",
      "Yang Deng",
      "Khanh-Huyen Bui",
      "Dung Le",
      "Lizi Liao"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.830": {
    "title": "Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-checkers",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxia Wang",
      "Revanth Gangi Reddy",
      "Zain Mujahid",
      "Arnav Arora",
      "Aleksandr Rubashevskii",
      "Jiahui Geng",
      "Osama Mohammed Afzal",
      "Liangming Pan",
      "Nadav Borenstein",
      "Aditya Pillai",
      "Isabelle Augenstein",
      "Iryna Gurevych",
      "Preslav Nakov"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.831": {
    "title": "Open-RAG: Enhanced Retrieval Augmented Reasoning with Open-Source Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shayekh Islam",
      "Md Asib Rahman",
      "K S M Tozammel Hossain",
      "Enamul Hoque",
      "Shafiq Joty",
      "Md Rizwan Parvez"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.832": {
    "title": "Cactus: Towards Psychological Counseling Conversations using Cognitive Behavioral Theory",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suyeon Lee",
      "Sunghwan Kim",
      "Minju Kim",
      "Dongjin Kang",
      "Dongil Yang",
      "Harim Kim",
      "Minseok Kang",
      "Dayi Jung",
      "Min Kim",
      "Seungbeen Lee",
      "Kyong-Mee Chung",
      "Youngjae Yu",
      "Dongha Lee",
      "Jinyoung Yeo"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.833": {
    "title": "TextLap: Customizing Language Models for Text-to-Layout Planning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Chen",
      "Ruiyi Zhang",
      "Yufan Zhou",
      "Jennifer Healey",
      "Jiuxiang Gu",
      "Zhiqiang Xu",
      "Changyou Chen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.834": {
    "title": "Data-driven Coreference-based Ontology Building",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shir Ashury Tahan",
      "Amir Cohen",
      "Nadav Cohen",
      "Yoram Louzoun",
      "Yoav Goldberg"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.835": {
    "title": "Retrieving Contextual Information for Long-Form Question Answering using Weak Supervision",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Philipp Christmann",
      "Svitlana Vakulenko",
      "Ionut Sorodoc",
      "Bill Byrne",
      "Adrià Gispert"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.836": {
    "title": "Persuasiveness of Generated Free-Text Rationales in Subjective Decisions: A Case Study on Pairwise Argument Ranking",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohamed Elaraby",
      "Diane Litman",
      "Xiang Li",
      "Ahmed Magooda"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.837": {
    "title": "Semantic Token Reweighting for Interpretable and Controllable Text Embeddings in CLIP",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eunji Kim",
      "Kyuhong Shim",
      "Simyung Chang",
      "Sungroh Yoon"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.838": {
    "title": "DYNAMICQA: Tracing Internal Knowledge Conflicts in Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sara Marjanovic",
      "Haeun Yu",
      "Pepa Atanasova",
      "Maria Maistro",
      "Christina Lioma",
      "Isabelle Augenstein"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.839": {
    "title": "LLMs to Replace Crowdsourcing For Parallel Data Creation? The Case of Text Detoxification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniil Moskovskiy",
      "Sergey Pletenev",
      "Alexander Panchenko"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.840": {
    "title": "Efficient Active Learning with Adapters",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daria Galimzianova",
      "Leonid Sanochkin"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.841": {
    "title": "How You Prompt Matters! Even Task-Oriented Constraints in Instructions Affect LLM-Generated Text Detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryuto Koike",
      "Masahiro Kaneko",
      "Naoaki Okazaki"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.842": {
    "title": "Seeing the Big through the Small\": Can LLMs Approximate Human Judgment Distributions on NLI from a Few Explanations?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Beiduo Chen",
      "Xinpeng Wang",
      "Siyao Peng",
      "Robert Litschko",
      "Anna Korhonen",
      "Barbara Plank"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.843": {
    "title": "Language Models in Dialogue: Conversational Maxims for Human-AI Interactions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Erik Miehling",
      "Manish Nagireddy",
      "Prasanna Sattigeri",
      "Elizabeth Daly",
      "David Piorkowski",
      "John Richards"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.844": {
    "title": "LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration in Evolving Environments",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruirui Chen",
      "Weifeng Jiang",
      "Chengwei Qin",
      "Ishaan Rawal",
      "Cheston Tan",
      "Dongkyu Choi",
      "Bo Xiong",
      "Bo Ai"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.845": {
    "title": "Self-supervised Preference Optimization: Enhance Your Language Model with Preference Degree Awareness",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Li",
      "Haojing Huang",
      "Yujia Zhang",
      "Pengfei Xu",
      "Xi Chen",
      "Rui Song",
      "Lida Shi",
      "Jingwen Wang",
      "Hao Xu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.846": {
    "title": "Mitigating Hallucination in Fictional Character Role-Play",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nafis Sadeq",
      "Zhouhang Xie",
      "Byungkyu Kang",
      "Prarit Lamba",
      "Xiang Gao",
      "Julian McAuley"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.847": {
    "title": "I'm sure you're a real scholar yourself: Exploring Ironic Content Generation by Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pier Balestrucci",
      "Silvia Casola",
      "Soda Marem Lo",
      "Valerio Basile",
      "Alessandro Mazzei"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.848": {
    "title": "Enhancing Temporal Sensitivity and Reasoning for Time-Sensitive Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wanqi Yang",
      "Yanda Li",
      "Meng Fang",
      "Ling Chen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.849": {
    "title": "Minimal Yet Big Impact: How AI Agent Back-channeling Enhances Conversational Engagement through Conversation Persistence and Context Richness",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jin Yea Jang",
      "Saim Shin",
      "Gahgene Gweon"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.850": {
    "title": "Large Language Models for Propaganda Span Annotation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maram Hasanain",
      "Fatema Ahmad",
      "Firoj Alam"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.851": {
    "title": "Style-Compress: An LLM-Based Prompt Compression Framework Considering Task-Specific Styles",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Pu",
      "Tianxing He",
      "Xiaojun Wan"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.852": {
    "title": "POSIX: A Prompt Sensitivity Index For Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anwoy Chatterjee",
      "H S V N S Kowndinya Renduchintala",
      "Sumit Bhatia",
      "Tanmoy Chakraborty"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.853": {
    "title": "Capturing Minds, Not Just Words: Enhancing Role-Playing Language Models with Personality-Indicative Data",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiting Ran",
      "Xintao Wang",
      "Rui Xu",
      "Xinfeng Yuan",
      "Jiaqing Liang",
      "Yanghua Xiao",
      "Deqing Yang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.854": {
    "title": "Local and Global Decoding in Text Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Gareev",
      "Thomas Hofmann",
      "Ezhilmathi Krishnasamy",
      "Tiago Pimentel"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.855": {
    "title": "LEGOBench: Scientific Leaderboard Generation Benchmark",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shruti Singh",
      "Shoaib Alam",
      "Husain Malwat",
      "Mayank Singh"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.856": {
    "title": "H-LegalKI: A Hierarchical Legal Knowledge Integration Framework for Legal Community Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Jiang",
      "Ziyu Guan",
      "Jie Zhao",
      "Wei Zhao",
      "Jiaqi Yang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.857": {
    "title": "Identifying Factual Inconsistencies in Summaries: Grounding LLM Inference via Task Taxonomy",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liyan Xu",
      "Zhenlin Su",
      "Mo Yu",
      "Jin Xu",
      "Jinho Choi",
      "Jie Zhou",
      "Fei Liu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.858": {
    "title": "Long Sequence Modeling with Attention Tensorization: From Sequence to Tensor Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aosong Feng",
      "Rex Ying",
      "Leandros Tassiulas"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.859": {
    "title": "BanglaTLit: A Benchmark Dataset for Back-Transliteration of Romanized Bangla",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Fahim",
      "Fariha Shifat",
      "Fabiha Haider",
      "Deeparghya Barua",
      "Md Sourove",
      "Md Ishmam",
      "Md Bhuiyan"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.860": {
    "title": "Finding the Optimal Byte-Pair Encoding Merge Operations for Neural Machine Translation in a Low-Resource Setting",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kristine Adlaon",
      "Nelson Marcos"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.861": {
    "title": "Can Machines Resonate with Humans? Evaluating the Emotional and Empathic Comprehension of LMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Arslan Manzoor",
      "Yuxia Wang",
      "Minghan Wang",
      "Preslav Nakov"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.862": {
    "title": "EU DisinfoTest: a Benchmark for Evaluating Language Models' Ability to Detect Disinformation Narratives",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Witold Sosnowski",
      "Arkadiusz Modzelewski",
      "Kinga Skorupska",
      "Jahna Otterbacher",
      "Adam Wierzbicki"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.863": {
    "title": "Adaptive BPE Tokenization for Enhanced Vocabulary Adaptation in Finetuning Pretrained Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gunjan Balde",
      "Soumyadeep Roy",
      "Mainack Mondal",
      "Niloy Ganguly"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.864": {
    "title": "From Reading to Compressing: Exploring the Multi-document Reader for Prompt Compression",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eunseong Choi",
      "Sunkyung Lee",
      "Minjin Choi",
      "Jun Park",
      "Jongwuk Lee"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.865": {
    "title": "Knowledge-Guided Dynamic Modality Attention Fusion Framework for Multimodal Sentiment Analysis",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Feng",
      "Yuming Lin",
      "Lihua He",
      "You Li",
      "Liang Chang",
      "Ya Zhou"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.866": {
    "title": "LexMatcher: Dictionary-centric Data Curation for LLM-based Machine Translation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongjing Yin",
      "Jiali Zeng",
      "Yafu Li",
      "Fandong Meng",
      "Yue Zhang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.867": {
    "title": "SARCAT: Generative Span-Act Guided Response Generation using Copy-enhanced Target Augmentation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeong-Doo Lee",
      "Hyeongjun Choi",
      "Beomseok Hong",
      "Youngsub Han",
      "Byoung-Ki Jeon",
      "Seung-Hoon Na"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.868": {
    "title": "Does Context Help Mitigate Gender Bias in Neural Machine Translation?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Harritxu Gete",
      "Thierry Etchegoyhen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.869": {
    "title": "A Critical Look at Meta-evaluating Summarisation Evaluation Metrics",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Dai",
      "Sarvnaz Karimi",
      "Biaoyan Fang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.870": {
    "title": "LLMs for Generating and Evaluating Counterfactuals: A Comprehensive Study",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Van Bach Nguyen",
      "Paul Youssef",
      "Christin Seifert",
      "Jörg Schlötterer"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.871": {
    "title": "Unlocking Black-Box Prompt Tuning Efficiency via Zeroth-Order Optimization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heshen Zhan",
      "Congliang Chen",
      "Tian Ding",
      "Ziniu Li",
      "Ruoyu Sun"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.872": {
    "title": "Unveiling Narrative Reasoning Limits of Large Language Models with Trope in Movie Synopses",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hung-Ting Su",
      "Ya-Ching Hsu",
      "Xudong Lin",
      "Xiang-Qian Shi",
      "Yulei Niu",
      "Han-Yuan Hsu",
      "Hung-yi Lee",
      "Winston Hsu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.873": {
    "title": "Unveiling the Flaws: Exploring Imperfections in Synthetic Data and Mitigation Strategies for Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Chen",
      "Yupeng Zhang",
      "Bingning Wang",
      "Xin Zhao",
      "Ji-Rong Wen",
      "Weipeng Chen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.874": {
    "title": "CED: Comparing Embedding Differences for Detecting Out-of-Distribution and Hallucinated Text",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hakyung Lee",
      "Keon-Hee Park",
      "Hoyoon Byun",
      "Jeyoon Yeom",
      "Jihee Kim",
      "Gyeong-Moon Park",
      "Kyungwoo Song"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.875": {
    "title": "CHAmbi: A New Benchmark on Chinese Ambiguity Challenges for Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qin Zhang",
      "Sihan Cai",
      "Jiaxu Zhao",
      "Mykola Pechenizkiy",
      "Meng Fang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.876": {
    "title": "Analyzing Context Contributions in LLM-based Machine Translation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emmanouil Zaranis",
      "Nuno Guerreiro",
      "Andre Martins"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.877": {
    "title": "ARTS: Assessing Readability & Text Simplicity",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Björn Engelmann",
      "Christin Kreutz",
      "Fabian Haak",
      "Philipp Schaer"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.878": {
    "title": "AXCEL: Automated eXplainable Consistency Evaluation using LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "P Aditya Sreekar",
      "Sahil Verma",
      "Suransh Chopra",
      "Abhishek Persad",
      "Sarik Ghazarian",
      "Narayanan Sadagopan"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.879": {
    "title": "Prospector: Improving LLM Agents with Self-Asking and Trajectory Ranking",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Byoungjip Kim",
      "Youngsoo Jang",
      "Lajanugen Logeswaran",
      "Geon-Hyeong Kim",
      "Yu Jin Kim",
      "Honglak Lee",
      "Moontae Lee"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.880": {
    "title": "Characterizing Text Datasets with Psycholinguistic Features",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marcio Monteiro",
      "Charu Karakkaparambil James",
      "Marius Kloft",
      "Sophie Fellenz"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.881": {
    "title": "Talking the Talk Does Not Entail Walking the Walk: On the Limits of Large Language Models in Lexical Entailment Recognition",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Candida Greco",
      "Lucio La Cava",
      "Andrea Tagarelli"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.882": {
    "title": "Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Debjit Paul",
      "Robert West",
      "Antoine Bosselut",
      "Boi Faltings"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.883": {
    "title": "Self-training Large Language Models through Knowledge Detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yeo Wei Jie",
      "Teddy Ferdinan",
      "Przemyslaw Kazienko",
      "Ranjan Satapathy",
      "Erik Cambria"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.884": {
    "title": "VE-KD: Vocabulary-Expansion Knowledge-Distillation for Training Smaller Domain-Specific Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengju Gao",
      "Tomohiro Yamasaki",
      "Kazunori Imoto"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.885": {
    "title": "Adaptive Contrastive Search: Uncertainty-Guided Decoding for Open-Ended Text Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Esteban Garces Arias",
      "Julian Rodemann",
      "Meimingwei Li",
      "Christian Heumann",
      "Matthias Aßenmacher"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.886": {
    "title": "SSP: Self-Supervised Prompting for Cross-Lingual Transfer to Low-Resource Languages using Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vipul Rathore",
      "Aniruddha Deb",
      "Ankish Chandresh",
      "Parag Singla",
      "Mausam ."
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.887": {
    "title": "Re-examining Sexism and Misogyny Classification with Annotator Attitudes",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aiqi Jiang",
      "Nikolas Vitsakis",
      "Tanvi Dinkar",
      "Gavin Abercrombie",
      "Ioannis Konstas"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.888": {
    "title": "When \"A Helpful Assistant\" Is Not Really Helpful: Personas in System Prompts Do Not Improve Performances of Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingqian Zheng",
      "Jiaxin Pei",
      "Lajanugen Logeswaran",
      "Moontae Lee",
      "David Jurgens"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.889": {
    "title": "Towards Efficient Visual-Language Alignment of the Q-Former for Visual Reasoning Tasks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sungkyung Kim",
      "Adam Lee",
      "Junyoung Park",
      "Andrew Chung",
      "Jusang Oh",
      "Jay-Yoon Lee"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.890": {
    "title": "Modeling Gender and Dialect Bias in Automatic Speech Recognition",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Camille Harris",
      "Chijioke Mgbahurike",
      "Neha Kumar",
      "Diyi Yang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.891": {
    "title": "Are Large Language Models Consistent over Value-laden Questions?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jared Moore",
      "Tanvi Deshpande",
      "Diyi Yang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.892": {
    "title": "xTower: A Multilingual LLM for Explaining and Correcting Translation Errors",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marcos Treviso",
      "Nuno Guerreiro",
      "Sweta Agrawal",
      "Ricardo Rei",
      "José Pombal",
      "Tania Vaz",
      "Helena Wu",
      "Beatriz Silva",
      "Daan Stigt",
      "Andre Martins"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.893": {
    "title": "LAMBDA: Large Language Model-Based Data Augmentation for Multi-Modal Machine Translation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yusong Wang",
      "Dongyuan Li",
      "Jialun Shen",
      "Yicheng Xu",
      "Mingkun Xu",
      "Kotaro Funakoshi",
      "Manabu Okumura"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.894": {
    "title": "Evaluating Differentially Private Synthetic Data Generation in High-Stakes Domains",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Krithika Ramesh",
      "Nupoor Gandhi",
      "Pulkit Madaan",
      "Lisa Bauer",
      "Charith Peris",
      "Anjalie Field"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.895": {
    "title": "Dual Process Masking for Dialogue Act Recognition",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yeo Jin Kim",
      "Halim Acosta",
      "Wookhee Min",
      "Jonathan Rowe",
      "Bradford Mott",
      "Snigdha Chaturvedi",
      "James Lester"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.896": {
    "title": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joao Monteiro",
      "Étienne Marcotte",
      "Pierre-Andre Noel",
      "Valentina Zantedeschi",
      "David Vazquez",
      "Nicolas Chapados",
      "Christopher Pal",
      "Perouz Taslakian"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.897": {
    "title": "Pioneering Reliable Assessment in Text-to-Image Knowledge Editing: Leveraging a Fine-Grained Dataset and an Innovative Criterion",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hengrui Gu",
      "Kaixiong Zhou",
      "Yili Wang",
      "Ruobing Wang",
      "Xin Wang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.898": {
    "title": "DEFT: Distribution-guided Efficient Fine-Tuning for Human Alignment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liang Zhu",
      "Feiteng Fang",
      "Yuelin Bai",
      "Longze Chen",
      "Zhexiang Zhang",
      "Minghuan Tan",
      "Min Yang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.899": {
    "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Utkarsh Saxena",
      "Gobinda Saha",
      "Sakshi Choudhary",
      "Kaushik Roy"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.900": {
    "title": "ACCEPT: Adaptive Codebook for Composite and Efficient Prompt Tuning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu-Chen Lin",
      "Wei-Hua Li",
      "Jun-cheng Chen",
      "Chu-Song Chen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.901": {
    "title": "Beyond Perplexity: Multi-dimensional Safety Evaluation of LLM Compression",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhichao Xu",
      "Ashim Gupta",
      "Tao Li",
      "Oliver Bentham",
      "Vivek Srikumar"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.902": {
    "title": "One-to-many testing for code generation from (just) natural language",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mansi Uniyal",
      "Mukul Singh",
      "Gust Verbruggen",
      "Sumit Gulwani",
      "Vu Le"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.903": {
    "title": "A Unified Framework for Model Editing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akshat Gupta",
      "Dev Sajnani",
      "Gopala Anumanchipalli"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.904": {
    "title": "M3SciQA: A Multi-Modal Multi-Document Scientific QA Benchmark for Evaluating Foundation Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuhan Li",
      "Ziyao Shangguan",
      "Yilun Zhao",
      "Deyuan Li",
      "Yixin Liu",
      "Arman Cohan"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.905": {
    "title": "Probing the Capacity of Language Model Agents to Operationalize Disparate Experiential Context Despite Distraction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sonny George",
      "Chris Sypherd",
      "Dylan Cashman"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.906": {
    "title": "Knowledge-Centric Templatic Views of Documents",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Isabel Cachola",
      "Silviu Cucerzan",
      "Allen Herring",
      "Vuksan Mijovic",
      "Erik Oveson",
      "Sujay Kumar Jauhar"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.907": {
    "title": "Shoes-ACOSI: A Dataset for Aspect-Based Sentiment Analysis with Implicit Opinion Extraction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joseph Peper",
      "Wenzhao Qiu",
      "Ryan Bruggeman",
      "Yi Han",
      "Estefania Chehade",
      "Lu Wang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.908": {
    "title": "Socratic Human Feedback (SoHF): Expert Steering Strategies for LLM Code Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Subramanian Chidambaram",
      "Li Li",
      "Min Bai",
      "Xiaopeng Li",
      "Kaixiang Lin",
      "Xiong Zhou",
      "Alex Williams"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.909": {
    "title": "Large Language Models Know What To Say But Not When To Speak",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Umair",
      "Vasanth Sarathy",
      "Jan Ruiter"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.910": {
    "title": "Towards Explainable Chinese Native Learner Essay Fluency Assessment: Dataset, Tasks, and Method",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinshu Shen",
      "Hongyi Wu",
      "Yadong Zhang",
      "Man Lan",
      "Xiaopeng Bai",
      "Shaoguang Mao",
      "Yuanbin Wu",
      "Xinlin Zhuang",
      "Li Cai"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.911": {
    "title": "CoCoHD: Congress Committee Hearing Dataset",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arnav Hiray",
      "Yunsong Liu",
      "Mingxiao Song",
      "Agam Shah",
      "Sudheer Chava"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.912": {
    "title": "Student Data Paradox and Curious Case of Single Student-Tutor Model: Regressive Side Effects of Training LLMs for Personalized Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shashank Sonkar",
      "Naiming Liu",
      "Richard Baraniuk"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.913": {
    "title": "MalAlgoQA: Pedagogical Evaluation of Counterfactual Reasoning in Large Language Models and Implications for AI in Education",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shashank Sonkar",
      "Naiming Liu",
      "MyCo Le",
      "Richard Baraniuk"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.914": {
    "title": "Sonnet or Not, Bot? Poetry Evaluation for Large Models and Datasets",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Melanie Walsh",
      "Maria Antoniak",
      "Anna Preus"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.915": {
    "title": "Merge to Learn: Efficiently Adding Skills to Language Models with Model Merging",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jacob Morrison",
      "Noah Smith",
      "Hannaneh Hajishirzi",
      "Pang Wei Koh",
      "Jesse Dodge",
      "Pradeep Dasigi"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.916": {
    "title": "To Ask LLMs about English Grammaticality, Prompt Them in a Different Language",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shabnam Behzad",
      "Amir Zeldes",
      "Nathan Schneider"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.917": {
    "title": "Enhancing Short-Text Topic Modeling with LLM-Driven Context Expansion and Prefix-Tuned VAEs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pritom Saha Akash",
      "Kevin Chang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.918": {
    "title": "Targeted Multilingual Adaptation for Low-resource Language Families",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "C. Downey",
      "Terra Blevins",
      "Dhwani Serai",
      "Dwija Parikh",
      "Shane Steinert-Threlkeld"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.919": {
    "title": "A Pointer Network-based Approach for Joint Extraction and Detection of Multi-Label Multi-Class Intents",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ankan Mullick",
      "Sombit Bose",
      "Abhilash Nandy",
      "Gajula Chaitanya",
      "Pawan Goyal"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.920": {
    "title": "Cost-Performance Optimization for Processing Low-Resource Language Tasks Using Commercial LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arijit Nag",
      "Animesh Mukherjee",
      "Niloy Ganguly",
      "Soumen Chakrabarti"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.921": {
    "title": "Advancing Vision-Language Models with Adapter Ensemble Strategies",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Bai",
      "Handong Zhao",
      "Zhe Lin",
      "Ajinkya Kale",
      "Jiuxiang Gu",
      "Tong Yu",
      "Sungchul Kim",
      "Yun Fu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.922": {
    "title": "Who Wrote When? Author Diarization in Social Media Discussions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benedikt Boenninghoff",
      "Henry Hosseini",
      "Robert Nickel",
      "Dorothea Kolossa"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.923": {
    "title": "Controlled Transformation of Text-Attributed Graphs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nidhi Vakil",
      "Hadi Amiri"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.924": {
    "title": "Misinformation with Legal Consequences (MisLC): A New Task Towards Harnessing Societal Harm of Misinformation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chu Fei Luo",
      "Radin Shayanfar",
      "Rohan Bhambhoria",
      "Samuel Dahan",
      "Xiaodan Zhu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.925": {
    "title": "CASE: Efficient Curricular Data Pre-training for Building Assistive Psychology Expert Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sarthak Harne",
      "Monjoy Choudhury",
      "Madhav Rao",
      "T Srikanth",
      "Seema Mehrotra",
      "Apoorva Vashisht",
      "Aarushi Basu",
      "Manjit Sodhi"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.926": {
    "title": "Explicit Inductive Inference using Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyang Liu",
      "Tianyi Li",
      "Liang Cheng",
      "Mark Steedman"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.927": {
    "title": "Less is More: Making Smaller Language Models Competent Subgraph Retrievers for Multi-hop KGQA",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenyu Huang",
      "Guancheng Zhou",
      "Hongru Wang",
      "Pavlos Vougiouklis",
      "Mirella Lapata",
      "Jeff Pan"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.928": {
    "title": "Evaluating Gender Bias of LLMs in Making Morality Judgements",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Divij Bajaj",
      "Yuanyuan Lei",
      "Jonathan Tong",
      "Ruihong Huang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.929": {
    "title": "A Study of Parameter Efficient Fine-tuning by Learning to Efficiently Fine-Tune",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taha Ceritli",
      "Savas Ozkan",
      "Jeongwon Min",
      "Eunchung Noh",
      "Cho Min",
      "Mete Ozay"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.930": {
    "title": "Explaining Mixtures of Sources in News Articles",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Spangher",
      "James Youn",
      "Matt DeButts",
      "Nanyun Peng",
      "Emilio Ferrara",
      "Jonathan May"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.931": {
    "title": "LLM generated responses to mitigate the impact of hate speech",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jakub Podolak",
      "Szymon Łukasik",
      "Paweł Balawender",
      "Jan Ossowski",
      "Jan Piotrowski",
      "Katarzyna Bakowicz",
      "Piotr Sankowski"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.932": {
    "title": "Locally Measuring Cross-lingual Lexical Alignment: A Domain and Word Level Perspective",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taelin Karidi",
      "Eitan Grossman",
      "Omri Abend"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.933": {
    "title": "SaSR-Net: Source-Aware Semantic Representation Network for Enhancing Audio-Visual Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyu Yang",
      "Yiyang Nan",
      "Lisen Dai",
      "Zhenwen Liang",
      "Yapeng Tian",
      "Xiangliang Zhang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.934": {
    "title": "Grounding Partially-Defined Events in Multimodal Data",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kate Sanders",
      "Reno Kriz",
      "David Etter",
      "Hannah Recknor",
      "Alexander Martin",
      "Cameron Carpenter",
      "Jingyang Lin",
      "Benjamin Van Durme"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.935": {
    "title": "How Does Quantization Affect Multilingual LLMs?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kelly Marchisio",
      "Saurabh Dash",
      "Hongyu Chen",
      "Dennis Aumiller",
      "Ahmet Üstün",
      "Sara Hooker",
      "Sebastian Ruder"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.936": {
    "title": "Presentations are not always linear! GNN meets LLM for Text Document-to-Presentation Transformation with Attribution",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Himanshu Maheshwari",
      "Sambaran Bandyopadhyay",
      "Aparna Garimella",
      "Anandhavelu Natarajan"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.937": {
    "title": "Domain Adaptation via Prompt Learning for Alzheimer's Detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shahla Farzana",
      "Natalie Parde"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.938": {
    "title": "SPINACH: SPARQL-Based Information Navigation for Challenging Real-World Questions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shicheng Liu",
      "Sina Semnani",
      "Harold Triedman",
      "Jialiang Xu",
      "Isaac Zhao",
      "Monica Lam"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.939": {
    "title": "Navigating Noisy Feedback: Enhancing Reinforcement Learning with Error-Prone Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhan Lin",
      "Shuyang Shi",
      "Yue Guo",
      "Behdad Chalaki",
      "Vaishnav Tadiparthi",
      "Ehsan Moradi Pari",
      "Simon Stepputtis",
      "Joseph Campbell",
      "Katia Sycara"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.940": {
    "title": "On the Limited Generalization Capability of the Implicit Reward Model Induced by Direct Preference Optimization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yong Lin",
      "Skyler Seto",
      "Maartje Ter Hoeve",
      "Katherine Metcalf",
      "Barry-John Theobald",
      "Xuan Wang",
      "Yizhe Zhang",
      "Chen Huang",
      "Tong Zhang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.941": {
    "title": "Gazelle: An Instruction Dataset for Arabic Writing Assistance",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samar Magdy",
      "Fakhraddin Alwajih",
      "Sang Yun Kwon",
      "Reem Abdel-Salam",
      "Muhammad Abdul-Mageed"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.942": {
    "title": "Extrinsic Evaluation of Cultural Competence in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaily Bhatt",
      "Fernando Diaz"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.943": {
    "title": "BLASER 2.0: a metric for evaluation and quality estimation of massively multilingual speech and text translation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Dale",
      "Marta Costa-jussà"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.944": {
    "title": "Multi-label Sequential Sentence Classification via Large Language Model",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengfei Lan",
      "Lecheng Zheng",
      "Shufan Ming",
      "Halil Kilicoglu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.945": {
    "title": "Multi-trait User Simulation with Adaptive Decoding for Conversational Task Assistants",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rafael Ferreira",
      "David Semedo",
      "Joao Magalhaes"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.946": {
    "title": "VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kun Qian",
      "Shunji Wan",
      "Claudia Tang",
      "Youzhi Wang",
      "Xuanming Zhang",
      "Maximillian Chen",
      "Zhou Yu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.947": {
    "title": "Gloss2Text: Sign Language Gloss translation using LLMs and Semantically Aware Label Smoothing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pooya Fayyazsanavi",
      "Antonios Anastasopoulos",
      "Jana Kosecka"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.948": {
    "title": "Structured Chain-of-Thought Prompting for Few-Shot Generation of Content-Grounded QA Conversations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Arafat Sultan",
      "Jatin Ganhotra",
      "Ramón Astudillo"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.949": {
    "title": "Gradient Localization Improves Lifelong Pretraining of Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jared Fernandez",
      "Yonatan Bisk",
      "Emma Strubell"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.950": {
    "title": "PFA-ERC: Psuedo-Future Augmented Dynamic Emotion Recognition in Conversations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tanmay Khule",
      "Rishabh Agrawal",
      "Apurva Narayan"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.951": {
    "title": "Textless Speech-to-Speech Translation With Limited Parallel Data",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anuj Diwan",
      "Anirudh Srinivasan",
      "David Harwath",
      "Eunsol Choi"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.952": {
    "title": "The Overlooked Repetitive Lengthening Form in Sentiment Analysis",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Wang",
      "Eduard Dragut"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.953": {
    "title": "Remember This Event That Year? Assessing Temporal Information and Understanding in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Himanshu Beniwal",
      "Dishant Patel",
      "Kowsik D",
      "Hritik Ladia",
      "Ankit Yadav",
      "Mayank Singh"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.954": {
    "title": "Hop, skip, jump to Convergence: Dynamics of Learning Rate Transitions for Improved Training of Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shreyas Subramanian",
      "Vignesh Ganapathiraman",
      "Corey Barrett"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.955": {
    "title": "FactAlign: Long-form Factuality Alignment of Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao-Wei Huang",
      "Yun-Nung Chen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.956": {
    "title": "HyperLoRA: Efficient Cross-task Generalization via Constrained Low-Rank Adapters Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuancheng Lv",
      "Lei Li",
      "Shitou Zhang",
      "Gang Chen",
      "Fanchao Qi",
      "Ningyu Zhang",
      "Hai-Tao Zheng"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.957": {
    "title": "Inference and Verbalization Functions During In-Context Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyi Tao",
      "Xiaoyin Chen",
      "Nelson Liu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.958": {
    "title": "Debate as Optimization: Adaptive Conformal Prediction and Diverse Retrieval for Event Extraction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sijia Wang",
      "Lifu Huang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.959": {
    "title": "MiRAGeNews: Multimodal Realistic AI-Generated News Detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runsheng Huang",
      "Liam Dugan",
      "Yue Yang",
      "Chris Callison-Burch"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.960": {
    "title": "Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meiqi Chen",
      "Yixin Cao",
      "Yan Zhang",
      "Chaochao Lu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.961": {
    "title": "Large Language Models are In-context Teachers for Knowledge Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiachen Zhao",
      "Zonghai Yao",
      "Zhichao Yang",
      "Hong Yu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.962": {
    "title": "SocialGaze: Improving the Integration of Human Social Norms in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anvesh Rao Vijjini",
      "Rakesh R Menon",
      "Jiayi Fu",
      "Shashank Srivastava",
      "Snigdha Chaturvedi"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.963": {
    "title": "Narrative-of-Thought: Improving Temporal Reasoning of Large Language Models via Recounted Narratives",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinliang Frederick Zhang",
      "Nicholas Beauchamp",
      "Lu Wang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.964": {
    "title": "Auto-Intent: Automated Intent Discovery and Self-Exploration for Large Language Model Web Agents",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaekyeom Kim",
      "Dong-Ki Kim",
      "Lajanugen Logeswaran",
      "Sungryull Sohn",
      "Honglak Lee"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.965": {
    "title": "See Detail Say Clear: Towards Brain CT Report Generation via Pathological Clue-driven Representation Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengxin Zheng",
      "Junzhong Ji",
      "Yanzhao Shi",
      "Xiaodan Zhang",
      "Liangqiong Qu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.966": {
    "title": "P-FOLIO: Evaluating and Improving Logical Reasoning with Abundant Human-Written Reasoning Chains",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simeng Han",
      "Aaron Yu",
      "Rui Shen",
      "Zhenting Qi",
      "Martin Riddell",
      "Wenfei Zhou",
      "Yujie Qiao",
      "Yilun Zhao",
      "Semih Yavuz",
      "Ye Liu",
      "Shafiq Joty",
      "Yingbo Zhou",
      "Caiming Xiong",
      "Dragomir Radev",
      "Rex Ying",
      "Arman Cohan"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.967": {
    "title": "TRIP NEGOTIATOR: A Travel Persona-aware Reinforced Dialogue Generation Model for Personalized Integrative Negotiation in Tourism",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Priyanshu Priya",
      "Desai Yasheshbhai",
      "Ratnesh Joshi",
      "Roshni Ramnani",
      "Anutosh Maitra",
      "Shubhashis Sengupta",
      "Asif Ekbal"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.968": {
    "title": "Chain of Condition: Construct, Verify and Solve Conditions for Conditional Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiuheng Lin",
      "Yuxuan Lai",
      "Yansong Feng"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.969": {
    "title": "Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu-Min Tseng",
      "Yu-Chao Huang",
      "Teng-Yun Hsiao",
      "Wei-Lin Chen",
      "Chao-Wei Huang",
      "Yu Meng",
      "Yun-Nung Chen"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.970": {
    "title": "ToxiCraft: A Novel Framework for Synthetic Generation of Harmful Information",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheng Hui",
      "Zhaoxiao Guo",
      "Hang Zhao",
      "Juanyong Duan",
      "Congrui Huang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.971": {
    "title": "Look Who's Talking Now: Covert Channels From Biased LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Silva",
      "Frederic Sala",
      "Ryan Gabrys"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.972": {
    "title": "ValueScope: Unveiling Implicit Norms and Values via Return Potential Model of Social Interactions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chan Young Park",
      "Shuyue Stella Li",
      "Hayoung Jung",
      "Svitlana Volkova",
      "Tanu Mitra",
      "David Jurgens",
      "Yulia Tsvetkov"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.973": {
    "title": "Unraveling the Truth: Do VLMs really Understand Charts? A Deep Dive into Consistency and Robustness",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Srija Mukhopadhyay",
      "Adnan Qidwai",
      "Aparna Garimella",
      "Pritika Ramu",
      "Vivek Gupta",
      "Dan Roth"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.974": {
    "title": "Fine-Tuning Language Models on Multiple Datasets for Citation Intention Classification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeren Shui",
      "Petros Karypis",
      "Daniel Karls",
      "Mingjian Wen",
      "Saurav Manchanda",
      "Ellad Tadmor",
      "George Karypis"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.975": {
    "title": "TransferCVLM: Transferring Cross-Modal Knowledge for Vision-Language Modeling",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongha Choi",
      "Jung-jae Kim",
      "Hyunju Lee"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.976": {
    "title": "Fast Streaming Transducer ASR Prototyping via Knowledge Distillation with Whisper",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Iuliia Thorbecke",
      "Juan Pablo Zuluaga Gomez",
      "Esaú Villatoro-tello",
      "Shashi Kumar",
      "Pradeep Rangappa",
      "Sergio Burdisso",
      "Petr Motlicek",
      "Karthik S",
      "Aravind Ganapathiraju"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.977": {
    "title": "Reasoning Paths Optimization: Learning to Reason and Explore From Diverse Paths",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yew Ken Chia",
      "Guizhen Chen",
      "Weiwen Xu",
      "Anh Tuan Luu",
      "Soujanya Poria",
      "Lidong Bing"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.978": {
    "title": "Uncertainty Calibration for Tool-Using Language Agents",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Liu",
      "Zi-Yi Dou",
      "Yixin Wang",
      "Nanyun Peng",
      "Yisong Yue"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.979": {
    "title": "Personalized Video Comment Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xudong Lin",
      "Ali Zare",
      "Shiyuan Huang",
      "Ming-Hsuan Yang",
      "Shih-Fu Chang",
      "Li Zhang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.980": {
    "title": "Solving for X and Beyond: Can Large Language Models Solve Complex Math Problems with More-Than-Two Unknowns?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kuei-Chun Kao",
      "Ruochen Wang",
      "Cho-Jui Hsieh"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.981": {
    "title": "MedLogic-AQA: Enhancing Medicare Question Answering with Abstractive Models Focusing on Logical Structures",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aizan Zafar",
      "Kshitij Mishra",
      "Asif Ekbal"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.982": {
    "title": "EmbodiedBERT: Cognitively Informed Metaphor Detection Incorporating Sensorimotor Information",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Li",
      "Bo Peng",
      "Yu-Yin Hsu",
      "Chu-Ren Huang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.983": {
    "title": "PositionID: LLMs can Control Lengths, Copy and Paste with Explicit Positional Awareness",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Noah Wang",
      "Feiyu Duan",
      "Yibo Zhang",
      "Wangchunshu Zhou",
      "Ke Xu",
      "Wenhao Huang",
      "Jie Fu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.984": {
    "title": "SedarEval: Automated Evaluation using Self-Adaptive Rubrics",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan Fan",
      "Weinong Wang",
      "Xing W",
      "Debing Zhang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.985": {
    "title": "Towards One-to-Many Visual Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huishan Ji",
      "Qingyi Si",
      "Zheng Lin",
      "Yanan Cao",
      "Weiping Wang"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.986": {
    "title": "Document-level Causal Relation Extraction with Knowledge-guided Binary Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zimu Wang",
      "Lei Xia",
      "Wei Xjtlu",
      "Xinya Du"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.987": {
    "title": "Block-Diagonal Orthogonal Relation and Matrix Entity for Knowledge Graph Embedding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihua Zhu",
      "Hidetoshi Shimodaira"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.988": {
    "title": "When Compression Meets Model Compression: Memory-Efficient Double Compression for Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weilan Wang",
      "Yu Mao",
      "Tang Dongdong",
      "Du Hongchao",
      "Nan Guan",
      "Chun Jason Xue"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.989": {
    "title": "BiMediX: Bilingual Medical Mixture of Experts LLM",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sara Pieri",
      "Sahal Shaji Mullappilly",
      "Fahad Khan",
      "Rao Anwer",
      "Salman Khan",
      "Timothy Baldwin",
      "Hisham Cholakkal"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.990": {
    "title": "Improving Adversarial Robustness in Vision-Language Models with Architecture and Prompt Design",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rishika Bhagwatkar",
      "Shravan Nayak",
      "Pouya Bashivan",
      "Irina Rish"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.991": {
    "title": "Zero-Shot Fact Verification via Natural Logic and Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marek Strong",
      "Rami Aly",
      "Andreas Vlachos"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.992": {
    "title": "Robust AI-Generated Text Detection by Restricted Embeddings",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kristian Kuznetsov",
      "Eduard Tulchinskii",
      "Laida Kushnareva",
      "German Magai",
      "Serguei Barannikov",
      "Sergey Nikolenko",
      "Irina Piontkovskaya"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.993": {
    "title": "CROWD: Certified Robustness via Weight Distribution for Smoothed Classifiers against Backdoor Attack",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siqi Sun",
      "Procheta Sen",
      "Wenjie Ruan"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.994": {
    "title": "MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingfan Zhang",
      "Yi Zhao",
      "Dan Chen",
      "Xing Tian",
      "Huanran Zheng",
      "Wei Zhu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.995": {
    "title": "LLM Tropes: Revealing Fine-Grained Values and Opinions in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dustin Wright",
      "Arnav Arora",
      "Nadav Borenstein",
      "Srishti Yadav",
      "Serge Belongie",
      "Isabelle Augenstein"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.996": {
    "title": "PythonSaga: Redefining the Benchmark to Evaluate Code Generating LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ankit Yadav",
      "Himanshu Beniwal",
      "Mayank Singh"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.997": {
    "title": "Efficient and Interpretable Grammatical Error Correction with Mixture of Experts",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Qorib",
      "Alham Aji",
      "Hwee Tou Ng"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.998": {
    "title": "Dial BeInfo for Faithfulness: Improving Factuality of Information-Seeking Dialogue via Behavioural Fine-Tuning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Evgeniia Razumovskaia",
      "Ivan Vulić",
      "Pavle Marković",
      "Tomasz Cichy",
      "Qian Zheng",
      "Tsung-Hsien Wen",
      "Paweł Budzianowski"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.999": {
    "title": "Unified Active Retrieval for Retrieval Augmented Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qinyuan Cheng",
      "Xiaonan Li",
      "Shimin Li",
      "Qin Zhu",
      "Zhangyue Yin",
      "Yunfan Shao",
      "Linyang Li",
      "Tianxiang Sun",
      "Hang Yan",
      "Xipeng Qiu"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.1000": {
    "title": "Mitigating Catastrophic Forgetting in Language Transfer via Model Merging",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anton Alexandrov",
      "Veselin Raychev",
      "Mark Mueller",
      "Ce Zhang",
      "Martin Vechev",
      "Kristina Toutanova"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.1001": {
    "title": "ATQ: Activation Transformation forWeight-Activation Quantization of Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yundong Gai",
      "Ping Li"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.1002": {
    "title": "Stochastic Fine-Tuning of Language Models Using Masked Gradients",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Akbar-Tajari",
      "Mohammad Taher Pilehvar"
    ]
  },
  "https://aclanthology.org/2024.findings-emnlp.1003": {
    "title": "To Know or Not To Know? Analyzing Self-Consistency of Large Language Models under Ambiguity",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anastasiia Sedova",
      "Robert Litschko",
      "Diego Frassinelli",
      "Benjamin Roth",
      "Barbara Plank"
    ]
  }
}